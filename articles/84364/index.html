<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Akinator and Mathematics</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On Habr√©, the Akinator theme has already surfaced several times, including with the tag I don‚Äôt know how it works . I stumbled upon it recently and, o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Akinator and Mathematics</h1><div class="post__text post__text-html js-mediator-article">  On Habr√©, the Akinator theme has already <a href="http://habrahabr.ru/blogs/artificial_intelligence/80595/">surfaced</a> several times, including with the tag <a href="http://habrahabr.ru/blogs/i_am_clever/45336/">I don‚Äôt know how it works</a> .  I stumbled upon it recently and, of course, was delighted.  Then, like many others, I thought, ‚ÄúBut how does it work?‚Äù I did not find the answer to this question, and therefore I set myself the goal of writing a program similar in functionality, having figured out what was happening. <br><a name="habracut"></a><br><h4>  Functional requirements </h4><br>  The first step is to find out what the words ‚Äúsimilar in functionality to a program‚Äù actually mean.  Thinking a little, I highlighted the following requirements: <br><ul><li>  <b>The program must be trained.</b>  It is clear that you can not teach the program to recognize several hundred thousand characters by manually entering the answers to the questions for each of them.  Rather, it is theoretically possible, but we will look for more beautiful solutions.  The obvious alternative to this approach is to learn on the go, using the answers of users.  This is our program should be able to. </li><li>  <b>The program must forgive errors.</b>  Obviously, users' opinions on the answers to some questions can vary considerably.  A simple example: a desperate homophobe and a simple man guessing Brad Pitt.  To the question ‚ÄúIs your character sexy?‚Äù The first one will probably answer in the negative, while most people do otherwise.  However, this small discrepancy should in no way prevent our program from finding out the truth. </li><li>  <b>The program should choose the questions wisely.</b>  There are many strategies that determine the questions that need to be asked.  For example, you can ask all the questions at all (only a lot of them).  You can ask random questions, but even then, most likely, the answer will have to go very long.  And you can try to choose the next question so that you will find out as much information as possible when answering it.  That is what we will try to do. </li></ul><br><h4>  Algorithms </h4><br>  If it were not for the forgiveness of mistakes, it would have been quite easy to achieve the desired.  For example, it would be possible to store a tree of answers to questions, in which internal vertices correspond to questions, and sheets - to answers.  The process of the game would then look like a descent from the root to one of the sheets.  However, this algorithm will not cope with the forgiveness of errors.  Yes, and issues of balancing the tree arise. <br><br>  In a sense, wood is a very ‚Äúmechanistic‚Äù, ‚Äúmachine‚Äù way of playing, extremely unstable to the slightest inaccuracies.  We need to play like a rational person would play.  Those who are more or less familiar with the theory of probability should know that it has the so-called <a href="http://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">Bayesian interpretation</a> , as well as the <a href="http://sexdrugsandappliedscience.com/blog/2009/11/10/bayesian-approach-introduction.html">Bayesian approach</a> based on it.  The basis of this approach is the description of knowledge using distributions of random variables with the subsequent transformation of a priori knowledge into a posteriori based on observations using the famous <a href="http://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25B0">Bayes formula</a> .  Moreover, this approach is the only generalization of the classical algebra of logic in case of uncertainty (you can read about it, for example, <a href="http://www.amazon.com/Probability-Theory-Logic-Science-Vol/dp/0521592712">here</a> ).  This leads many scientists to believe that the Bayesian approach is the standard of rational thinking.  Well, we just need it.  Let's try to apply it to our problem. <br><br><h4>  Bayesian model </h4><br>  So, we recall the Bayesian formula: P (A | B) = P (B | A) P (A) / P (B).  And now in words.  Suppose we need to evaluate the probability that event A occurred, provided that event B exactly happened (that is, we were guaranteed to watch it; this is why B is often called observation).  According to the Bayes formula, this probability is proportional to the product of the other two.  The first of these, P (B | A), is called <a href="http://ru.wikipedia.org/wiki/%25D0%25A4%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F_%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B4%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BE%25D0%25B1%25D0%25B8%25D1%258F">likelihood</a> and shows how likely B is to occur, provided that A. A second factor, P (A), is the so-called <a href="http://ru.science.wikia.com/wiki/%25D0%2590%25D0%25BF%25D1%2580%25D0%25B8%25D0%25BE%25D1%2580%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">prior probability of</a> A, that is, the probability that it will happen in principle (regardless of B).  In fact, this probability reflects the information that we knew about A before we learned about what happened B. In the denominator of the formula, there is also a value P (B), which in this case just plays the role of the normalization factor and can be ignored. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Using this formula in the context of the question game is pretty easy.  Let's assume that Ai is an event of the form ‚Äúyou have made an object i‚Äù, where i can be either Spock or the Virgin Mary.  Since B is an observation on Ai, it would be natural to assume that B consists of answers to questions.  The only option that I see here is to present B in the form of a joint event: ‚ÄúA1 was answered to question Q1, ..., Ak was answered to question Qk‚Äù.  Then P (Ai | B) will show for the object i the probability that it was he who thought it (taking into account that the user gave answers to k questions).  This is exactly the value that interests us.  If you select an object with a maximum value of P (Ai | B), you can, if the value of P (Ai | B) is large enough, try using it as a guess. <br><br>  The prior probability P (Ai) can be considered as a special case of P (Ai | B) with k = 0.  In other words, this is the probability that the player made an object i, provided that no questions were asked, and we don‚Äôt know anything at all.  On the one hand, it would be possible to give all objects equal to P (Ai), since  it's fair.  On the other hand, Barack Obama will most likely be thinking much more often than Holden Caulfield.  Therefore, <b>all other things being equal</b> (that is, when we cannot distinguish objects), Obama should be chosen.  Consequently, the natural estimate of P (Ai) will be the ratio of the number of games, when X was made, to their total number. <br><br>  Likelihood P (B | Ai) also receives a convenient interpretation.  Only before you need to use one little trick - to assume the conditional independence of answers to the questions under the condition Ai (somewhat crude, but very convenient for us to simplify).  Translated into Russian, this means that, by hypothesis, the probability P (B | Ai) can be written as a product (by j) of the probabilities P (Bj | Ai), where Bj is an event of the form ‚ÄúAj was answered to the question Qj‚Äù.  P (Bj | Ai) in this case will be the ratio of the number of times when, when the object i was thought of, Qj was answered by Aj by the number of times when, when the object i was thought of, in principle, the question Qj was asked.  In order to avoid zero and uncertain probabilities, I propose to additionally assume that initially each of the questions each of the answers was given once for each question.  That is, if the question Qj has never been asked about the object i, P (Bj | Ai) will be 1 / Nj, where Nj is the number of answers to the question Qj (I, by the way, used one and all the same 4 answer choices: ‚Äúyes‚Äù, ‚Äúno‚Äù, ‚Äúdo not know‚Äù and ‚Äúthe question does not make sense‚Äù). <br><br>  Let's sum up the intermediate result.  We found a simple formula that maps a set of question / answer pairs and some entity into the probability that it was this entity that was made with the given answers to the questions.  Having recalculated this probability for all objects in our database, after answering a new question, we can see which of them are more similar to the proposed object at the moment.  Moreover, the training of our model is implemented quite simply: you just need to store information for each entity in the database about what questions were asked about it and how many responses each type gave to users.  After each game, this information can be updated based on the user's responses.  Also, to take into account the ‚Äúpopularity‚Äù of a person, you need to store in the database the number of times that a person was made up. <br><br><h4>  Question selection, information and entropy </h4><br>  Well, it remains only to understand which questions are best to ask.  Naturally, you need to ask those questions that give more information.  But can we somehow measure this information?  It turns out that yes.  For this you can use the concept of <a href="http://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BD%25D1%2584%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D1%2586%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%258D%25D0%25BD%25D1%2582%25D1%2580%25D0%25BE%25D0%25BF%25D0%25B8%25D1%258F">informational entropy</a> .  Speaking roughly, but understandable, then informational entropy is such a characteristic of the distribution of a random variable (measured, like information, in bits), which shows how far we are not sure how much this random variable will take.  For example, if a random variable takes the value 1 with a probability of 0.99, and the value 0 - with a probability of 0.01, then the entropy of such a distribution will be very close to zero.  If the random variable takes, for example, the values ‚Äã‚Äãof 0 and 1 with equal probabilities of 0.5 (heads or tails), then the entropy of such a random variable will be equal to 1 bit (this is just the amount of information we need to get to eliminate the uncertainty). <br><br>  Okay, let's choose every time the question, the answer to which will most strongly reduce the entropy of the distribution P (Ai | B), which is exactly responsible for our knowledge of who the player has thought of.  Another problem immediately arises here: generally speaking, different answers to the same question can reduce entropy in different ways.  What to do?  It is proposed to find the question for which the <b>expected</b> decrease in entropy will be maximum.  The expected decrease in entropy shows how much "on average" entropy decreases if we ask a question.  In order not to write here a few more paragraphs of the text, I will give a formula by which this value can be calculated.  Those who want to easily understand why she has this look.  So, every time you need to ask such a question j, for which the value of H [P (Ai | B, &lt;Qj, Yes&gt;)] P (&lt;Qj, Yes&gt;) + ... + H [P (Ai | B, &lt;Qj, No&gt;)] P (&lt;Qj, No&gt;) is minimal.  Here, H [P] denotes the entropy of the probability distribution P, and by "&lt;Qj, Ans&gt;" - the event "the question Qj is answered by Ans".  The value of P (&lt;Qj, Ans&gt;) can be easily found by the <a href="http://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25BE%25D1%2580%25D0%25BC%25D1%2583%25D0%25BB%25D0%25B0_%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">formula of total probability</a> , summing it, due to all known objects.  That is, P (&lt;Qj, Ans&gt;) = sum (i) P (&lt;Qj, Ans&gt; | Ai) P (Ai | B). <br><br>  It turns out that this approach allows you to quickly reject irrelevant questions, focusing on the most important.  In a sense, this method is a generalization of the method of "dividing in half" in a probabilistic formulation.  See how it all works together, you can on the video below. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/-RZCKnudOoM%3Ffeature%3Doembed&amp;xid=25657,15700021,15700186,15700191,15700253,15700256,15700259&amp;usg=ALkJrhipKc-KIRWQVBG-cjL-IESkHwgBMQ" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  Total </h4><br>  I hope the information from this short article seemed interesting to one of you.  In fact, first of all this article should draw your attention to the power of (albeit the simplest) mathematics in solving poorly formalized problems.  Using the Bayesian approach to probability theory and information theory in your programs can make them more rational, but at the same time more humane :) </div><p>Source: <a href="https://habr.com/ru/post/84364/">https://habr.com/ru/post/84364/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../84357/index.html">SVN: Six months later</a></li>
<li><a href="../84359/index.html">iPhone development: Integrating In-App Purchases</a></li>
<li><a href="../84360/index.html">Project to optimize the distribution of incoming Voip calls</a></li>
<li><a href="../84361/index.html">Cubestormer robot, collects the Rubik's cube in 12 seconds</a></li>
<li><a href="../84362/index.html">Where do you go for packaging from household and computer equipment?</a></li>
<li><a href="../84367/index.html">NOMOBILE.RU at MWC-2010. Day zero and day one. How it was</a></li>
<li><a href="../84368/index.html">Troubleshooting xdebug with php version 5.3</a></li>
<li><a href="../84369/index.html">43 nude video cards from Voodoo5 6000 to Radeon HD 5970</a></li>
<li><a href="../84377/index.html">ASA: network address translation troubles. Part 2. Static broadcast</a></li>
<li><a href="../84379/index.html">The idea of ‚Äã‚Äãan interactive service to help in remembering foreign words</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>