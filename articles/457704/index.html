<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How do GPUs handle branching</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="About the article 
 This post is a small note intended for programmers who want to learn more about how the GPU handles branching. It can be considere...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How do GPUs handle branching</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/f33/e39/6cbf33e39c393986a3a26bd44b9777e8.png" alt="image"></div><br><h2>  About the article </h2><br>  This post is a small note intended for programmers who want to learn more about how the GPU handles branching.  It can be considered as an introduction to this topic.  I recommend looking at [ <a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">1</a> ], [ <a href="http://cs149.stanford.edu/winter19/">2</a> ] and [ <a href="https://t.co/QG28evt7QR">8</a> ] to get started, to get an idea of ‚Äã‚Äãhow the model of GPU execution looks in general, because we will consider only one particular detail.  For curious readers at the end of the post have all the links.  If you find any errors, please contact me. <br><br><h2>  Content </h2><br><ul><li>  About the article </li><li>  Content </li><li>  Vocabulary </li><li>  How does a GPU core differ from a CPU core? </li><li>  What is consistency / discrepancy? </li><li>  Execution mask processing examples <ul><li>  Fictional ISA </li><li>  AMD GCN ISA </li><li>  AVX512 </li></ul></li><li>  How to deal with the discrepancy? </li><li>  Links </li></ul><a name="habracut"></a><br><h2>  Vocabulary </h2><br><ul><li>  GPU - Graphics processing unit, graphics processor </li><li>  Flynn's classification <br><ul><li>  SIMD - Single instruction multiple data, single command stream, multiple data stream </li><li>  SIMT - Single instruction multiple threads, single command stream, multiple threads </li></ul></li><li>  Wave (SIMD) - a stream running in SIMD mode </li><li>  Line (lane) - a separate data stream in the model SIMD </li><li>  SMT - Simultaneous multi-threading, simultaneous multithreading (Intel Hyper-threading) [ <a href="http://cs149.stanford.edu/winter19/">2</a> ] <br><ul><li>  Multiple threads use shared kernel computing resources. </li></ul></li><li>  IMT - Interleaved multi-threading, alternating multithreading [ <a href="http://cs149.stanford.edu/winter19/">2</a> ] <br><ul><li>  Multiple threads share common computational resources of the kernel, but only one is executed per clock. </li></ul></li><li>  BB - Basic Block, basic block - linear sequence of instructions with a single transition at the end </li><li>  ILP - Instruction Level Parallelism, instruction-level parallelism [ <a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">3</a> ] </li><li>  ISA - Instruction Set Architecture, command / instruction set architecture </li></ul><br>  In my post I will adhere to this invented classification.  It roughly resembles how a modern GPU is organized. <br><br><blockquote><code>: <br> GPU -+ <br> |-  0 -+ <br> | |-  0 + <br> | | |-  0 <br> | | |-  1 <br> | | |- ... <br> | | +-  Q-1 <br> | | <br> | |- ... <br> | +-  M-1 <br> | <br> |- ... <br> +-  N-1 <br> <br> *  -  SIMD <br> <br>  : <br>  + <br> |-  0 <br> |- ... <br> +-  N-1</code> </blockquote> <br>  Other names: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  The kernel may be called CU, SM, EU </li><li>  A wave can be called a wavefront, a hardware flow (HW thread), warp, a context </li><li>  A line can be called a program thread (SW thread) </li></ul><br><h2>  How does a GPU core differ from a CPU core? </h2><br>  Any current generation of GPU cores is less powerful than CPUs: simple ILP / multi-issue [ <a href="https://arxiv.org/pdf/1804.06826.pdf">6</a> ] and pre-selection [ <a href="https://arxiv.org/pdf/1509.02308%26amp%3Bved%3D0ahUKEwifl_P9rt7LAhXBVxoKHRsxDIYQFgg_MAk%26amp%3Busg%3DAFQjCNGchkZRzkueGqHEz78QnmcIVCSXvg%26amp%3Bsig2%3DIdzxfrzQgNv8yq7e1mkeVg">5</a> ], no prediction or prediction of transitions / returns.  All this, along with tiny caches, frees up a rather large area on the crystal, which is filled with many cores.  Memory load / storage mechanisms are able to handle channel widths an order of magnitude more (this does not apply to integrated / mobile GPUs) than regular CPUs, but you have to pay for this with high latency.  To hide the delays, the GPU uses SMT [ <a href="http://cs149.stanford.edu/winter19/">2</a> ] - while one wave is idle, others use free kernel computing resources.  Typically, the number of waves processed by a single core depends on the registers used and is determined dynamically by allocating a fixed register file [ <a href="https://t.co/QG28evt7QR">8</a> ].  Hybrid - Dynamic-Static [ <a href="https://arxiv.org/pdf/1804.06826.pdf">6</a> ] [ <a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> 4.4] scheduling instructions execution.  SIMT kernels running in SIMD mode achieve high FLOPS values ‚Äã‚Äã(FLoating-point Operations Per Second, flops, number of floating point operations per second). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb8/059/770/fb8059770b3653b65c5e2cc30f5fee16.png" alt="Figure 1"><br><br>  <i>Legend of the chart.</i>  <i>Black is inactive, white is active, gray is disabled, blue is idle, red is pending</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/12e/fc7/5d9/12efc75d90a064410b1632c2be710235.png"></div><br>  <i>Figure 1. 4: 2 execution history</i> <br><br>  The image shows the history of the execution mask, where the time from left to right is plotted on the x axis, and the line identifier going from top to bottom on the y axis.  If this is not yet clear to you, then return to the picture after reading the following sections. <br><br>  This is an illustration of how the execution history of a GPU kernel may look like in a fictional configuration: four waves share one sampler and two ALUs.  The wave planner in each cycle releases two instructions from two waves.  When a wave is idle when performing a memory access or a long ALU operation, the scheduler switches to another pair of waves, so that the ALU are constantly occupied by almost 100%. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/433/54a/ea2/43354aea2bb0a351048a196808d6a06a.png"></div><br>  <i>Figure 2. 4: 1 execution history</i> <br><br>  An example with the same load, but this time in each cycle of the instruction only one wave releases.  Notice that the second ALU is starving. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cb/f33/e39/6cbf33e39c393986a3a26bd44b9777e8.png"></div><br>  <i>Figure 3. 4: 4 execution history</i> <br><br>  This time, four instructions are issued for each cycle.  Note that the ALU is now too many requests, so the two waves are almost constantly waiting (in fact, this is the error of the scheduling algorithm). <br><br>  <strong><em>Update</em></strong> For more information about the difficulties of scheduling instructions, see [ <a href="http://www.joshbarczak.com/blog/%3Fp%3D823">12</a> ]. <br><br>  In the real world, GPUs have different kernel configurations: some can have up to 40 waves per core and 4 ALUs, others have fixed 7 waves and 2 ALUs.  It all depends on many factors and is determined by the painstaking process of architecture simulation. <br><br>  In addition, a real ALD SIMD may have a narrower width than the waves it serves, and then it takes several cycles to process one issued instruction;  the multiplier is called the chime length [ <a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">3</a> ]. <br><br><h2>  What is consistency / discrepancy? </h2><br>  Let's take a look at the following code snippet: <br><br><h6>  Example 1 </h6><br><pre> <code class="cpp hljs">uint lane_id = get_lane_id(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (lane_id &amp; <span class="hljs-number"><span class="hljs-number">1</span></span>) { <span class="hljs-comment"><span class="hljs-comment">// Do smth } // Do some more</span></span></code> </pre> <br>  Here we see a stream of instructions in which the execution path depends on the identifier of the line being executed.  Obviously, different lines have different meanings.  What should happen?  There are different approaches to solving this problem [ <a href="https://hal.archives-ouvertes.fr/hal-00622654/document">4</a> ], but in the end, they all do about the same thing.  One of these approaches is the execution mask, which I will consider.  This approach was used in the Nvidia GPU to Volta and on the AMD GCN GPU.  The basic meaning of the execution mask is that we store a bit for each line in the wave.  If the corresponding line execution bit is 0, then no registers will be affected for the next instruction issued.  In fact, the line should not feel the influence of all executed instructions, because its execution bit is 0. This works as follows: the wave travels through the control flow graph in the depth-first search order, keeping the history of the selected transitions until the bits are set.  I think it is better to show it by example. <br><br>  Suppose we have a wave width of 8. Here‚Äôs what the execution mask looks like for a code snippet: <br><br><h6>  Example 1. Execution mask history </h6><br><pre> <code class="cpp hljs"> <span class="hljs-comment"><span class="hljs-comment">// execution mask uint lane_id = get_lane_id(); // 11111111 if (lane_id &amp; 1) { // 11111111 // Do smth // 01010101 } // Do some more // 11111111</span></span></code> </pre> <br>  Now let's look at more complex examples: <br><br><h6>  Example 2 </h6><br><pre> <code class="cpp hljs">uint lane_id = get_lane_id(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (uint i = lane_id; i &lt; <span class="hljs-number"><span class="hljs-number">16</span></span>; i++) { <span class="hljs-comment"><span class="hljs-comment">// Do smth }</span></span></code> </pre> <br><h6>  Example 3 </h6><br><pre> <code class="cpp hljs">uint lane_id = get_lane_id(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (lane_id &lt; <span class="hljs-number"><span class="hljs-number">16</span></span>) { <span class="hljs-comment"><span class="hljs-comment">// Do smth } else { // Do smth else }</span></span></code> </pre> <br>  You may notice that history is necessary.  When using the execution mask approach, the equipment typically uses some kind of stack.  The naive approach is to store a stack of tuples (exec_mask, address) and add convergence instructions that remove the mask from the stack and change the instruction pointer for the wave.  In this case, the wave will have enough information to bypass the entire CFG for each line. <br><br>  In terms of performance, only a couple of cycles are required to process a control flow instruction because of all this data storage.  And do not forget that the stack has a limited depth. <br><br>  <strong><em>Update.</em></strong>  Thanks to <a href="https://twitter.com/craigkolb">@craigkolb,</a> I read an article [ <a href="https://tangentvector.wordpress.com/2013/04/12/a-digression-on-divergence/">13</a> ] in which it is noted that the AMD GCN fork / join instructions first choose a path from a smaller number of threads [ <a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> 4.6], which ensures the sufficiency of the stack depth of the masks equal to log2. <br><br>  <strong><em>Update.</em></strong>  Obviously, it is almost always possible to embed everything in a shader / structure CFG in a shader, and therefore store the entire history of execution masks in registers and plan for traversing / converging CFG statically [ <a href="https://github.com/rAzoR8/EuroLLVM19">15</a> ].  After reviewing the LLVM backend for AMDGPU, I did not find any evidence of processing stacks that are constantly being released by the compiler. <br><br><h3>  Hardware support for the execution mask </h3><br>  Now take a look at these control flow graphs from Wikipedia: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f9/f04/a1a/5f9f04a1a89b36ad0908dee0e90542f3.png"></div><br>  <i>Figure 4. Some of the types of control flow graphs</i> <br><br>  What is the minimum set of instructions for managing masks that we need to handle all cases?  Here‚Äôs how it looks in my artificial ISA with implicit parallelization, explicit mask management and fully dynamic synchronization of data conflicts: <br><br><pre> <code class="cpp hljs">push_mask BRANCH_END ; Push current mask <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> reconvergence pointer pop_mask ; Pop mask <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> jump to reconvergence instruction mask_nz r0.x ; Set execution bit, pop mask <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> all bits are zero ; Branch instruction is more complicated ; Push current mask <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> reconvergence ; <span class="hljs-function"><span class="hljs-function">Push mask </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">for</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r0.x == </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">for</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">else</span></span></span><span class="hljs-function"> block, </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">if</span></span></span><span class="hljs-function"> any lane takes the path </span></span>; <span class="hljs-function"><span class="hljs-function">Set mask </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">with</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(r0.x != </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">, fallback to </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">else</span></span></span><span class="hljs-function"> in </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">case</span></span></span><span class="hljs-function"> no bit is 1 br_push r0.x, ELSE, CONVERGE</span></span></code> </pre> <br>  Let's take a look at how case d) may look. <br><br><pre> <code class="cpp hljs">A: br_push r0.x, C, D B: C: mask_nz r0.y jmp B D: ret</code> </pre> <br>  I am not an expert in analyzing control flows or designing ISA, so I‚Äôm sure that there is a case that my artificial ISA cannot handle, but this is not important, because a structured CFG should be enough for everyone. <br><br>  <strong><em>Update.</em></strong>  You can read more about GCN support for control flow instructions here: [ <a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> ] ch.4, and about implementing LLVM - here: [ <a href="https://github.com/rAzoR8/EuroLLVM19">15</a> ]. <br><br>  Conclusion: <br><br><ul><li>  The discrepancy is an emerging difference in the execution paths chosen by different lines of the same wave. </li><li>  Consistency - no discrepancy. </li></ul><br><h2>  Execution mask processing examples </h2><br><h3>  Fictional ISA </h3><br>  I compiled the previous code snippets in my artificial ISA and ran them on the simulator in SIMD32.  See how it handles the execution mask. <br><br>  <strong><em>Update.</em></strong>  Note that the artificial simulator always chooses the true path, and this is not the best way. <br><br><h6>  Example 1 </h6><br><pre> <code class="lisp hljs"><span class="hljs-comment"><span class="hljs-comment">; uint lane_id = get_lane_id(); mov r0.x, lane_id ; if (lane_id &amp; 1) { push_mask BRANCH_END and r0.y, r0.x, u(1) mask_nz r0.y LOOP_BEGIN: ; // Do smth pop_mask ; pop mask and reconverge BRANCH_END: ; // Do some more ret</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/d40/30c/fdb/d4030cfdb754b9a663c03ae46b31efc7.png" alt="Figure 5"><br><br>  <i>Figure 5. Execution history of example 1</i> <br><br>  Have you noticed the black area?  This time wasted.  Some lines are waiting for others to complete the iteration. <br><br><h6>  Example 2 </h6><br><pre> <code class="lisp hljs"><span class="hljs-comment"><span class="hljs-comment">; uint lane_id = get_lane_id(); mov r0.x, lane_id ; for (uint i = lane_id; i &lt; 16; i++) { push_mask LOOP_END ; Push the current mask and the pointer to reconvergence instruction LOOP_PROLOG: lt.u32 r0.y, r0.x, u(16) ; r0.y &lt;- r0.x &lt; 16 add.u32 r0.x, r0.x, u(1) ; r0.x &lt;- r0.x + 1 mask_nz r0.y ; exec bit &lt;- r0.y != 0 - when all bits are zero next mask is popped LOOP_BEGIN: ; // Do smth jmp LOOP_PROLOG LOOP_END: ; // } ret</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/fe3/259/b17/fe3259b179e832553900c7cb22487e03.png" alt="Figure 6"><br><br>  <i>Figure 6. Execution history of example 2</i> <br><br><h6>  Example 3 </h6><br><pre> <code class="lisp hljs"> mov r0.x, lane_id lt.u32 r0.y, r0.x, u(<span class="hljs-number"><span class="hljs-number">16</span></span>) <span class="hljs-comment"><span class="hljs-comment">; if (lane_id &lt; 16) { ; Push (current mask, CONVERGE) and (else mask, ELSE) ; Also set current execution bit to r0.y != 0 br_push r0.y, ELSE, CONVERGE THEN: ; // Do smth pop_mask ; } else { ELSE: ; // Do smth else pop_mask ; } CONVERGE: ret</span></span></code> </pre> <br><img src="https://habrastorage.org/getpro/habr/post_images/27e/d1b/2a9/27ed1b2a99db4ab917d661946ed7c705.png" alt="Figure 7"><br><br>  <i>Figure 7. Execution history of Example 3</i> <br><br><h3>  AMD GCN ISA </h3><br>  <strong><em>Update.</em></strong>  GCN also uses explicit processing of masks, more about this can be found here: [ <a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">11</a> 4.x].  I decided to show a few examples with their ISA, thanks to the <a href="http://shader-playground.timjones.io/">shader-playground</a> it is easy to do.  Maybe someday I will find a simulator and manage to get diagrams. <br><br>  Note that the compiler is smart, so you can get other results.  I tried to trick the compiler so that it doesn‚Äôt optimize my branching by putting the pointer-follow cycles there and then cleaning up the assembler code;  I am not a GCN specialist, so several important <code>nop</code> can be skipped. <br><br>  Also note that the S_CBRANCH_I / G_FORK and S_CBRANCH_JOIN instructions are not used in these fragments because they are simple and the compiler does not support them.  Therefore, unfortunately, it was not possible to consider the stack of masks.  If you know how to force the compiler to issue stack processing, please let me know. <br><br>  <strong><em>Update.</em></strong>  See this <a href="https://youtu.be/8K8ClHoZzHw">talk</a> <a href="https://twitter.com/SiNGUL4RiTY">@ SiNGUL4RiTY</a> about implementing vectorized control flow in the LLVM backend used by AMD. <br><br><h6>  Example 1 </h6><br><pre> <code class="lisp hljs"><span class="hljs-comment"><span class="hljs-comment">; uint lane_id = get_lane_id(); ; GCN uses 64 wave width, so lane_id = thread_id &amp; 63 ; There are scalar s* and vector v* registers ; Executon mask does not affect scalar or branch instructions v_mov_b32 v1, 0x00000400 ; 1024 - group size v_mad_u32_u24 v0, s12, v1, v0 ; thread_id calculation v_and_b32 v1, 63, v0 ; if (lane_id &amp; 1) { v_and_b32 v2, 1, v0 s_mov_b64 s[0:1], exec ; Save the execution mask v_cmpx_ne_u32 exec, v2, 0 ; Set the execution bit s_cbranch_execz ELSE ; Jmp if all exec bits are zero ; // Do smth ELSE: ; } ; // Do some more s_mov_b64 exec, s[0:1] ; Restore the execution mask s_endpgm</span></span></code> </pre> <br><h6>  Example 2 </h6><br><pre> <code class="lisp hljs"><span class="hljs-comment"><span class="hljs-comment">; uint lane_id = get_lane_id(); v_mov_b32 v1, 0x00000400 v_mad_u32_u24 v0, s8, v1, v0 ; Not sure why s8 this time and not s12 v_and_b32 v1, 63, v0 ; LOOP PROLOG s_mov_b64 s[0:1], exec ; Save the execution mask v_mov_b32 v2, v1 v_cmp_le_u32 vcc, 16, v1 s_andn2_b64 exec, exec, vcc ; Set the execution bit s_cbranch_execz LOOP_END ; Jmp if all exec bits are zero ; for (uint i = lane_id; i &lt; 16; i++) { LOOP_BEGIN: ; // Do smth v_add_u32 v2, 1, v2 v_cmp_le_u32 vcc, 16, v2 s_andn2_b64 exec, exec, vcc ; Mask out lanes which are beyond loop limit s_cbranch_execnz LOOP_BEGIN ; Jmp if non zero exec mask LOOP_END: ; // } s_mov_b64 exec, s[0:1] ; Restore the execution mask s_endpgm</span></span></code> </pre> <br><h6>  Example 3 </h6><br><pre> <code class="lisp hljs"><span class="hljs-comment"><span class="hljs-comment">; uint lane_id = get_lane_id(); v_mov_b32 v1, 0x00000400 v_mad_u32_u24 v0, s12, v1, v0 v_and_b32 v1, 63, v0 v_and_b32 v2, 1, v0 s_mov_b64 s[0:1], exec ; Save the execution mask ; if (lane_id &lt; 16) { v_cmpx_lt_u32 exec, v1, 16 ; Set the execution bit s_cbranch_execz ELSE ; Jmp if all exec bits are zero ; // Do smth ; } else { ELSE: s_andn2_b64 exec, s[0:1], exec ; Inverse the mask and &amp; with previous s_cbranch_execz CONVERGE ; Jmp if all exec bits are zero ; // Do smth else ; } CONVERGE: s_mov_b64 exec, s[0:1] ; Restore the execution mask ; // Do some more s_endpgm</span></span></code> </pre> <br><h3>  AVX512 </h3><br>  <strong><em>Update.</em></strong>  <a href="https://twitter.com/tom_forsyth">@tom_forsyth</a> pointed out to me that the AVX512 extension also has explicit mask handling, so here are some examples.  Read more about this in [ <a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">14</a> ], 15.x and 15.6.1.  It's not exactly a GPU, but it still has a real 32-bit SIMD16.  The code snippets were created using ISPC (‚Äìtarget = avx512knl-i32x16) <a href="https://godbolt.org/z/kwrr1y">godbolt</a> and have been extensively reworked, so they may not be 100% correct. <br><br><h6>  Example 1 </h6><br><pre> <code class="lisp hljs"> <span class="hljs-comment"><span class="hljs-comment">; Imagine zmm0 contains 16 lane_ids ; AVXZ512 comes with k0-k7 mask registers ; Usage: ; op reg1 {k[7:0]}, reg2, reg3 ; k0 can not be used as a predicate operand, only k1-k7 ; if (lane_id &amp; 1) { vpslld zmm0 {k1}, zmm0, 31 ; zmm0[i] = zmm0[i] &lt;&lt; 31 kmovw eax, k1 ; Save the execution mask vptestmd k1 {k1}, zmm0, zmm0 ; k1[i] = zmm0[i] != 0 kortestw k1, k1 je ELSE ; Jmp if all exec bits are zero ; // Do smth ; Now k1 contains the execution mask ; We can use it like this: ; vmovdqa32 zmm1 {k1}, zmm0 ELSE: ; } kmovw k1, eax ; Restore the execution mask ; // Do some more ret</span></span></code> </pre> <br><h6>  Example 2 </h6><br><pre> <code class="lisp hljs"> <span class="hljs-comment"><span class="hljs-comment">; Imagine zmm0 contains 16 lane_ids kmovw eax, k1 ; Save the execution mask vpcmpltud k1 {k1}, zmm0, 16 ; k1[i] = zmm0[i] &lt; 16 kortestw k1, k1 je LOOP_END ; Jmp if all exec bits are zero vpternlogd zmm1 {k1}, zmm1, zmm1, 255 ; zmm1[i] = -1 ; for (uint i = lane_id; i &lt; 16; i++) { LOOP_BEGIN: ; // Do smth vpsubd zmm0 {k1}, zmm0, zmm1 ; zmm0[i] = zmm0[i] + 1 vpcmpltud k1 {k1}, zmm0, 16 ; masked k1[i] = zmm0[i] &lt; 16 kortestw k1, k1 jne LOOP_BEGIN ; Break if all exec bits are zero LOOP_END: ; // } kmovw k1, eax ; Restore the execution mask ; // Do some more ret</span></span></code> </pre> <br><h6>  Example 3 </h6><br><pre> <code class="lisp hljs"> <span class="hljs-comment"><span class="hljs-comment">; Imagine zmm0 contains 16 lane_ids ; if (lane_id &amp; 1) { vpslld zmm0 {k1}, zmm0, 31 ; zmm0[i] = zmm0[i] &lt;&lt; 31 kmovw eax, k1 ; Save the execution mask vptestmd k1 {k1}, zmm0, zmm0 ; k1[i] = zmm0[i] != 0 kortestw k1, k1 je ELSE ; Jmp if all exec bits are zero THEN: ; // Do smth ; } else { ELSE: kmovw ebx, k1 andn ebx, eax, ebx kmovw k1, ebx ; mask = ~mask &amp; old_mask kortestw k1, k1 je CONVERGE ; Jmp if all exec bits are zero ; // Do smth else ; } CONVERGE: kmovw k1, eax ; Restore the execution mask ; // Do some more ret</span></span></code> </pre> <br><h2>  How to deal with the discrepancy? </h2><br>  I tried to create a simple but complete illustration of how inefficiency arises due to the combination of diverging lines. <br><br>  Provide a simple code snippet: <br><br><pre> <code class="lisp hljs">uint thread_id = get_thread_id()<span class="hljs-comment"><span class="hljs-comment">; uint iter_count = memory[thread_id]; for (uint i = 0; i &lt; iter_count; i++) { // Do smth }</span></span></code> </pre> <br>  Let's create 256 threads and measure their execution duration: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7c2/2da/c5d/7c22dac5d75a77156a60c510a655309a.png"></div><br>  <i>Figure 8. Runtime of diverging threads</i> <br><br>  The x axis is the identifier of the program flow, the y axis is the clock cycles;  different columns show how much time is wasted in grouping streams with different wavelengths compared to single-threaded execution. <br><br>  The wave execution time is equal to the maximum execution time among the lines contained in it.  You can see that the performance drops dramatically already with SIMD8, and further expansion simply makes it a little worse. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a0d/c9a/be6/a0dc9abe6a7a0e12515d4c88c36aaa21.png" alt="Figure 9"></div><br>  <i>Figure 9. Execution time of matched threads</i> <br><br>  This picture shows the same columns, but this time the number of iterations is sorted by the thread identifiers, that is, streams with a similar number of iterations are transmitted to the same wavelength. <br><br>  For this example, execution is potentially about half as fast. <br><br>  Of course, the example is too simple, but I hope that you understand the meaning: the discrepancy in the implementation stems from the discrepancy of the data, so the CFG should be simple, and the data should be consistent. <br><br>  For example, if you are writing a ray tracer, you may be given an advantage of grouping rays with the same direction and position, because they are more likely to travel along the same nodes in BVH.  For details, see [ <a href="https://www.eecis.udel.edu/~cavazos/cisc879-spring2012/papers/a3-han.pdf">10</a> ] and other related articles. <br><br>  It is also worth mentioning that there are techniques for dealing with divergence and at the hardware level, for example, Dynamic Warp Formation [ <a href="http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/fung07_dynamicwarp.pdf">7</a> ] and the predicted performance for small branches. <br><br><h1>  Links </h1><br>  [1] <a href="https://fgiesen.wordpress.com/2011/07/09/a-trip-through-the-graphics-pipeline-2011-index/">A trip through the Graphics Pipeline</a> <br><br>  [2] <a href="http://cs149.stanford.edu/winter19/">Kayvon Fatahalian: PARALLEL COMPUTING</a> <br><br>  [3] <a href="https://www.elsevier.com/books/computer-architecture/hennessy/978-0-12-811905-1">Computer Architecture A Quantitative Approach</a> <br><br>  [4] <a href="https://hal.archives-ouvertes.fr/hal-00622654/document">Stack-less SIMT reconvergence at low cost</a> <br><br>  [5] <a href="https://arxiv.org/pdf/1509.02308%26amp%3Bved%3D0ahUKEwifl_P9rt7LAhXBVxoKHRsxDIYQFgg_MAk%26amp%3Busg%3DAFQjCNGchkZRzkueGqHEz78QnmcIVCSXvg%26amp%3Bsig2%3DIdzxfrzQgNv8yq7e1mkeVg">Dissecting GPU memory hierarchy through microbenchmarking</a> <br><br>  [6] <a href="https://arxiv.org/pdf/1804.06826.pdf">Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking</a> <br><br>  [7] <a href="http://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/fung07_dynamicwarp.pdf">Dynamic Warp Formation and Scheduling for Efficient GPU Control Flow</a> <br><br>  [8] <a href="https://t.co/QG28evt7QR">Maurizio Cerrato: GPU Architectures</a> <br><br>  [9] <a href="https://aschrein.github.io/guppy/">Toy GPU simulator</a> <br><br>  [10] <a href="https://www.eecis.udel.edu/~cavazos/cisc879-spring2012/papers/a3-han.pdf">Reducing Branch Divergence in GPU Programs</a> <br><br>  [11] <a href="https://developer.amd.com/wp-content/resources/Vega_Shader_ISA_28July2017.pdf">‚ÄúVega‚Äù Instruction Set Architecture</a> <br><br>  [12] <a href="http://www.joshbarczak.com/blog/%3Fp%3D823">Joshua Barczak: Simulating Shader Execution for GCN</a> <br><br>  [13] <a href="https://tangentvector.wordpress.com/2013/04/12/a-digression-on-divergence/">Tangent Vector: A Digression on Divergence</a> <br><br>  [14] <a href="https://software.intel.com/sites/default/files/managed/39/c5/325462-sdm-vol-1-2abcd-3abcd.pdf">Intel 64 and IA-32 Architectures Software Developer's Manual</a> <br><br>  [15] <a href="https://github.com/rAzoR8/EuroLLVM19">Vectorizing Divergent Control-Flow for SIMD Applications</a> </div><p>Source: <a href="https://habr.com/ru/post/457704/">https://habr.com/ru/post/457704/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457696/index.html">Danger of using multi-character constants</a></li>
<li><a href="../457698/index.html">Experiment: use a proxy as a tool to combat DoS attacks</a></li>
<li><a href="../4577/index.html">Google strikes back. Personal search engine with Google CSE</a></li>
<li><a href="../457700/index.html">Node.js authentication guide without passport.js and third-party services</a></li>
<li><a href="../457702/index.html">Using the KOMPAS-3D API ‚Üí Lesson 16 ‚Üí Control Characters</a></li>
<li><a href="../457706/index.html">Robot is testing SAP ERP</a></li>
<li><a href="../457710/index.html">The amazing capabilities of neural networks in 2019</a></li>
<li><a href="../457712/index.html">How Verizon and BGP Optimizer made a big offline</a></li>
<li><a href="../457718/index.html">HyperCard, the lost link in the evolution of the Web</a></li>
<li><a href="../457722/index.html">SQL: working time task: debriefing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>