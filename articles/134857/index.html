<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Viola Jones in her own skin, part 2. - Emotion? - OMG, Yes !!!</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello everyone again! I decided to immediately try to release two articles, almost at the same time, so as not to interrupt the narrative chain, since...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Viola Jones in her own skin, part 2. - Emotion? - OMG, Yes !!!</h1><div class="post__text post__text-html js-mediator-article">  Hello everyone again!  I decided to immediately try to release two articles, almost at the same time, so as not to interrupt the narrative chain, since  <a href="http://habrahabr.ru/blogs/image_processing/135244/">The beginning of this article is</a> very important! <br>  So, many were waiting for examples of my program and an explanation of its work in terms of writing code.  I am telling consistently so that everyone can repeat it on his computer.  Pay more attention to the abundant comments in the code, they have power!  And do not be afraid of a mega-small scroll, because  a lot of information.  Relocate to a place with good internet, there are a lot of schemes and photos in the article! <a name="habracut"></a><br><br><h4>  Project development </h4><br>  As a result, <a href="http://habrahabr.ru/blogs/image_processing/135244/">in the previous part</a> , a project template was developed with the necessary headers connected and with the OpenCV library hooked up.  Let's see what type of work still needs to be done and how to do it. <br><br>  Also, <a href="http://habrahabr.ru/blogs/algorithm/133909/">hot on the heels</a> , it turned out that you can sketch out approximate schemes of work. <br>  standard <br><img src="https://habrastorage.org/storage2/e78/9ee/fca/e789eefcae81898ed5915770cfda3a78.jpg"><br>  and Viola-Jones's modified method of working with photography. <br><img src="https://habrastorage.org/storage2/85d/cdd/11d/85dcdd11dd52f34f31743ef5af88d176.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Pre-training Classifiers </h5><br>  Since <a href="http://habrahabr.ru/blogs/algorithm/133826/">classifiers</a> are used in the emotion recognition module, a sequence of actions for their training and training has been developed. <br><br>  So, in order to <b>train a classifier</b> , you will need: <br><br>  ‚Ä¢ Preparing development tools, connecting the OpenCV library; <br>  ‚Ä¢ Data preparation.  Suitable for use: <br>  - Test sets of "positive" and "negative" images (positive and negative images).  ‚ÄúPositive‚Äù images contain an object of interest (face, mouth, nose, and so on), <br>  "Negative" images contain only the background (background); <br>  - .vec file, easily complemented, containing the same sets of images, only in a reduced size; <br>  - The base of ready-made images, a <a href="http://www.face-rec.org/databases/">lot of</a> them.  For example, <a href="http://itl.nist.gov/iad/humanid/feret/">FERET</a> , used by the OpenCV developers; <br>  ‚Ä¢ Crop positive images to the desired part, gluing in a row; <br>  ‚Ä¢ Preparation of natural test images in the form of ‚Äúobject of interest in the background‚Äù; <br>  ‚Ä¢ Selection of regions of interest ( <a href="http://robocraft.ru/blog/computervision/289.html">ROI</a> ) on a given set of natural test images, in essence, setting the coordinates of the place where the object of interest is located; <br>  ‚Ä¢ Saving to a new data set; <br>  ‚Ä¢ Creation of samples (training samples); <br>  ‚Ä¢ Training.  At this stage: <br>  - Training classifier Haar; <br>  - Create XML file. <br><br>  The collected image sets are collected in the data folder, and in the haarcascades folder, the learning results of the cascades in the form of Haar classifiers in XML format are added. <br>  For each classifier, expressed as an entity with subordinate attributes, you must create a separate XML-storage.  <a href="http://www.w3.org/DOM/">DOM</a> (Document Object Model), or in Russian, the object model of the document of such a classifier in general form is presented below.  The root element is Storage, the length of the tree itself is 7. <br><br><img src="https://habrastorage.org/storage2/163/f53/f38/163f53f385570b154fa751d6d33ea37a.jpg"><br><br>  There are many levels in a cascade - this can be seen in the example of the first level of the classifier: <br><br><img src="https://habrastorage.org/storage2/c3c/f41/e26/c3cf41e26d5fbfdd9d1a3790467ecb63.jpg"><br><br>  This example shows what information is stored in the XML used.  This is information about the classifier (haarcascade) and its size (size), level used (stage), predecessor, or parent of this level (parent), next level, or follower (next), the tree (tree) and its root node ( root node) with parameters (threshold, left, right).  Next, information about the features themselves (features) is built up in the nodes of this tree, which are specified by rectangles with parameters (rects) with a certain angle of inclination (tilted).  In a more specific example, it is necessary to split the levels and trees in a general sense, therefore the length of such a tree is equal to 9. <br><br><h6>  Classifier file structure </h6><br>  As a result of the work of the program, the output should be xml-files with the structure shown in the listing below.  All xml-files will be identical in structure and DOM-model. <br><pre><code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">storage</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">classifier1</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">size</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">size</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">stages</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-comment"><span class="hljs-comment">&lt;!-- #of stage --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">trees</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-comment"><span class="hljs-comment">&lt;!-- #of tree --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-comment"><span class="hljs-comment">&lt;!-- root node --&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">feature</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">rects</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">rects</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tilted</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">tilted</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">feature</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">threshold</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">threshold</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">left_val</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">left_val</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">right_val</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">right_val</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">trees</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">stage_threshold</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">stage_threshold</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">parent</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">parent</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">next</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">next</span></span></span><span class="hljs-tag">&gt;</span></span> ‚Ä¶ <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">stages</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">_</span></span></span><span class="hljs-tag">&gt;</span></span><span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">classifier1</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">storage</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br><br><h6>  Preparation of new data sets </h6><br>  In order to train a classifier, we need large sets of images of ‚Äúpositive‚Äù, and large sets of images of ‚Äúnegative‚Äù, roughly speaking, the desired object and its background.  It is important that the sets are of the same size, since glueing occurs and the coordinates where the object is located, that is, those coordinates that the classifier memorizes and thus learns, are indicated.  When gluing a new set of images is obtained.  An example of such a "glued" image from my database: <br><br><img src="https://habrastorage.org/storage2/ff1/c5c/a31/ff1c5ca318db4fd623afb09064bea32a.jpg"><br><br>  Such samples are created using the createsamples.cpp file, which in turn loads cvhaartrainig.h to work.  I took them from the standard OpenCV library and ‚Äúfinished‚Äù for my needs.  There are a lot of parameters to be specified when creating such samples, presented in the following listing are used by me, but this is not all parameters: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> reateTrainingSamples( <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">char</span></span>* filename,<span class="hljs-comment"><span class="hljs-comment">//   ,    ,       const char* imgfilename,//    int bgcolor, //   int bgthreshold,//  ,   8-  const char* bgfilename, //   int count, int invert = 20,//      int maxintensitydev = 40,//        double maxxangle = 1.1,//      X double maxyangle = 1.1, //      Y double maxzangle = 0.5, //      Z int showsamples = 0,//  ,       int winwidth = 24,//        int winheight = 24//        );</span></span></code> </pre><br><br>  As a result, a .dat-file is also created immediately, where we can view the following information: the resulting image is assigned a name according to the coordinates of the marked features for further processing.  The numbers in the example mean that the number of objects found is 5 and instances of the object are found in a rectangle with coordinates {30, 49}, and 469 and 250 in width and height: <br><br> <code>../../ kalian2.png/0005_0030_0049_0469_0250.jpg 5 30 49 469 250</code> <br> <br>  Creation is also possible through a <a href="http://filext.com/file-extension/VEC">.vec</a> file, which is used in OpenCV, and which is used to represent images, contains the name of the file and can be extended with additional code.  You can make this add information through addVec to the vec-file: <br><br><pre> <code class="cpp hljs">addVec (oldvecname, newvecname, samplwidth, samplheight);<span class="hljs-comment"><span class="hljs-comment">//     ,  newvecname      ,   ,   oldvecname,         .</span></span></code> </pre><br><br><h6>  Parameters and stages of training classifier </h6><br>  The next stage after the creation of samples is the stage of training the classifier, and the parameters I specify are as follows: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">void</span></span> reateCascadeClassifier( <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">char</span></span>* dirname,<span class="hljs-comment"><span class="hljs-comment">// ,       const char* vecfilename,// vec-       const char* bgfilename, //  ,   int npos,//   int nneg, //   int nstages,//     int numprecalculated,// ,     int numsplits,//  ,     : 1 ‚Äì , 2   -  float minhitrate = 0.995F,//         float maxfalsealarm = 0.5F,//        float weightfraction = 0.95F,//   ,         90 int mode = 3,// ,     ,    . 0,  BASIC ‚Äì  ,     -,    ,     2,  ALL    ,   45 ,   3            int symmetric = 1,// ,         ,    0,     int equalweights = 1,//    0,          int winwidth = 24,//  int winheight = 24,//  int boosttype = 3,//   ,  0 ‚Äì Discrete AdaBoost, 1 ‚Äì Real AdaBoost, 2 ‚Äì LogitBoost, 3 ‚Äì Gentle AdaBoost int stumperror = 0 );//  ,    Discrete AdaBoost</span></span></code> </pre><br><br>  The result of this stage are ready-to-use, trained classifiers.  In the module that I am developing, a lot of classifiers will be used (for each facial feature that distinguishes this emotion), which I prepared in advance, because  classifiers are trained for a long time.  I taught them on different computers.  <b>Each classifier takes about three days with the weak computing power of a computer!</b>  So I advise, if there is a deadline, then take up the job now: <br><pre> <code class="cpp hljs">‚Ä¢ face_identify_classifier.xml ‚Äì   , ‚Ä¢ eyes_identify_classifier.xml ‚Äì   , ‚Ä¢ eyebrows_identify_classifier.xml ‚Äì   , ‚Ä¢ mouth_identify_classifier.xml ‚Äì   , ‚Ä¢ nose_identify_classifier.xml ‚Äì   , ... ‚Ä¢ happysmile_kalian_classifier.xml ‚Äì    , ‚Ä¢ happyeyes_kalian_classifier.xml ‚Äì   ,  , ‚Ä¢ happyeyebrows_kalian_classifier.xml ‚Äì     , ‚Ä¢ surprisingeyes_kalian_classifier.xml ‚Äì     ,    , ‚Ä¢ openingmouth_kalian_classifier.xml ‚Äì      , ‚Ä¢ surprisingeyebrows_kalian_classifier.xml ‚Äì   ¬´¬ª  (         )</code> </pre><br><br><h5>  Apparatus for automatic recognition of emotions </h5><br><br>  As soon as the source image is available, the module automatically recognizes the desired objects on it.  The sequence of actions looks very simple, but behind this lies the difficulty of implementing these steps. <br>  Let us examine the <i>init ()</i> function, where images are loaded, resources are initialized, the database is connected to, and an image is prepared for further processing: <br><br><ul><li>  Initially, using the special function <i>cvLoad (), the</i> created classifiers for detection and recognition are loaded, I have them located in the haarcascades folder of my project (... \ haarcascades \).  The following is the creation of storage: <code>storage = cvCreateMemStorage(0);</code>  : <br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//  ,  ,   ,   void init(){//  ,       //     haarcascades cascade = (CvHaarClassifierCascade*)cvLoad( "haarcascades/mouth_disugst_classifier.xml" );//  //cascade1 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/ eyes_identify_classifier.xml" );//  cascade1 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/face_identify_classifier.xml" );//  cascade7 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_frontalface_alt_tree.xml" );//  cascade2 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_eye.xml" );//    //cascade3 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_mcs_nose.xml" );//  cascade3 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_kalian_nose.xml" );//  cascade4 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_profileface.xml" );// //  cascade5 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/smiled_01.xml" );//1 cascade8 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/smiled_02.xml" );//2 cascade9 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/smiled_03.xml" );//3 cascade10 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/smiled_04.xml" );//4 cascade6 = (CvHaarClassifierCascade*)cvLoad( "haarcascades/haarcascade_mcs_eyepair_big.xml" );// ... //   storage = cvCreateMemStorage(0);</span></span></code> </pre><br></li><li>  Using the function <code>cvNamedWindow();</code>  A window appears in which the results of operations with the image and on the image will appear. </li><li>  If you use a camera, you will need a code to capture from the camera: <br><br><pre> <code class="cpp hljs"> <span class="hljs-comment"><span class="hljs-comment">//     CvCapture* capture = cvCreateCameraCapture(CV_CAP_ANY); //cvCaptureFromCAM( 0 ); assert( capture ); //cvSetCaptureProperty(capture, CV_CAP_PROP_FRAME_WIDTH, 640);//1280); //cvSetCaptureProperty(capture, CV_CAP_PROP_FRAME_HEIGHT, 480);//960); //      double width = cvGetCaptureProperty(capture, CV_CAP_PROP_FRAME_WIDTH); double height = cvGetCaptureProperty(capture, CV_CAP_PROP_FRAME_HEIGHT); printf("[i] %.0f x %.0f\n", width, height ); IplImage* frame=0; cvNamedWindow("capture", CV_WINDOW_AUTOSIZE); printf("[i] press Enter for capture image and Esc for quit!\n\n"); int counter=0; char filename[512]; while(true){ //   frame = cvQueryFrame( capture ); //  cvShowImage("capture", frame); char c = cvWaitKey(33); if (c == 27) { //  ESC break; } else if(c == 13) { // Enter //     sprintf(filename, "Image%d.jpg", counter); printf("[i] capture... %s\n", filename); cvSaveImage(filename, frame); counter++; }</span></span></code> </pre></li><li>  The program includes a cycle in which the image or an array of images is loaded into memory: <br><br><pre> <code class="cpp hljs"> image = cvLoadImage( array_1[j], <span class="hljs-number"><span class="hljs-number">1</span></span> ); <span class="hljs-built_in"><span class="hljs-built_in">printf</span></span>(<span class="hljs-string"><span class="hljs-string">"\n[i] Image loaded\n"</span></span>, image); assert( image!= <span class="hljs-number"><span class="hljs-number">0</span></span> );</code> </pre><br><br>  If the data is stored in the database, then at this stage, the connection of the required <i>mysql.h</i> header to the database is made, the preliminary connection and loading of data from the database into the memory if necessary: <br><br><pre> <code class="cpp hljs"> <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;mysql.h&gt; ‚Ä¶ //     void comlinePrint(char *); ‚Ä¶ //   MYSQL conn; //    MYSQL_RES *res; //     MYSQL_ROW row; //    if(!mysql_init(&amp;conn)) comlinePrint("Error:   MySql - \n"); //      if(!mysql_real_connect(&amp;conn,"localhost","kalian","","user",0,NULL,0)) comlinePrint("Error:    SQL \n"); //   ,   //    if(mysql_query(&amp;conn, "SET NAMES 'cp1251'") != 0) { //     -  //    comlinePrint ("Error:   \n"); exit(1); } //    if(mysql_query(&amp;conn,"INSERT INTO `user` VALUES(NULL, '', '','', 22, '', '','') ") != 0) { //      -  //    comlinePrint ("Error:      \n"); exit(1); } void comlinePrint(char * str) { printf(stderr, str); exit(1); } //       mysql_close(&amp;conn);}</span></span></span></span></code> </pre></li><li>  Next, the image is resized.  The width and height of the image are checked and, depending on the conditions, the image is enlarged or reduced by n times: <br><br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(image-&gt;width&gt;<span class="hljs-number"><span class="hljs-number">700</span></span> &amp;&amp; image-&gt;height&gt;<span class="hljs-number"><span class="hljs-number">700</span></span>){ resPic = cvCreateImage( cvSize(image-&gt;width/<span class="hljs-number"><span class="hljs-number">1.5</span></span>, image-&gt;height/<span class="hljs-number"><span class="hljs-number">1.5</span></span>), image-&gt;depth, image-&gt;nChannels ); cvResize(image, resPic, <span class="hljs-number"><span class="hljs-number">1</span></span>); cvShowImage(<span class="hljs-string"><span class="hljs-string">"resizing"</span></span>, resPic); <span class="hljs-built_in"><span class="hljs-built_in">printf</span></span> (<span class="hljs-string"><span class="hljs-string">"\n[i] Image has been resized"</span></span>, image); comlinePrint(); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(image-&gt;width&lt;<span class="hljs-number"><span class="hljs-number">200</span></span> &amp;&amp; image-&gt;height&lt;<span class="hljs-number"><span class="hljs-number">200</span></span>){ resPicMal = cvCreateImage( cvSize(image-&gt;width*<span class="hljs-number"><span class="hljs-number">2</span></span>, image-&gt;height*<span class="hljs-number"><span class="hljs-number">2</span></span>), image-&gt;depth, image-&gt;nChannels ); cvResize(image, resPicMal, <span class="hljs-number"><span class="hljs-number">1</span></span>); cvShowImage(<span class="hljs-string"><span class="hljs-string">"resizing"</span></span>, resPicMal);</code> </pre></li></ul><br><br>  Then the image falls into the main loop of the module.  The main work of my developed method of recognition is divided into functions that go in this order: <br><br><ol><li>  imageGray (); // convert the original image to halftone <br><img src="https://habrastorage.org/storage2/1d1/412/99e/1d141299e4af8177be070c883309cf8a.png"></li><li>  SobelPreparation (resPic, dst2); // initial processing by Sobel operator <br><img src="https://habrastorage.org/storage2/f05/dfb/384/f05dfb3843f66786dad38ae6d66d7042.png"></li><li>  OtsuThresholdPreparation (resPic, ots1, ots2); // binarization by the Otsu method <br><img src="https://habrastorage.org/storage2/356/cbe/32f/356cbe32fe10db965cc4f430d23f261b.png"></li><li>  ImageCannyPreparation (resPic, dst); // selection of borders by the Kanni detector (the result is shown in the screenshot from cvSub) </li><li>  dilate (); // thickening of the found boundaries * - can be not used, according to the results of the program's tests using this function, in most cases the results do not improve, and sometimes even worse than without it </li><li>  cvSub (gray, img1, img1); // subtraction of the binary from the original <br><img src="https://habrastorage.org/storage2/bd1/748/f34/bd1748f34a5150b3782e36f7198dc9df.png"></li><li>  detect_and_draw (resPic); // identification of the face and its features in the image </li><li>  recognizeResult (resPic); // calculation of emotion </li><li>  cvShowImage (&amp; resPic); // recognition result </li><li>  (cvWaitKey (1000)); // waiting for a key to be pressed </li><li>  cvReleaseImage (&amp; resPic) // freeing memory resources </li></ol><br><br><h5>  Customizable parameters for detecting faces and their features </h5><br><br><ul><li>  Image preparation functions I am sure you will write without problems yourself, since  Implementing functions in OpenCV have the same names as the names of the algorithms used, so only screenshots of the program at each stage are given.  Although I may attach the code at the very end. </li><li>  The search for objects in the module is performed using the function <i>detect_and_draw (resPic)</i> , in the body of which the array is initially set, which contains colors for the rectangles with which faces and features will be highlighted.  Then, using the <i>cvHaarDetectObjects ()</i> function, all the sections in the picture that correspond to the faces and features of people's faces are returned to the sequence of <i>objects</i> .  In code, it looks like this: <br><br><pre> <code class="cpp hljs">CvSeq* objects1 = cvHaarDetectObjects( img, cascade1, storage, <span class="hljs-number"><span class="hljs-number">1.1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>|CV_HAAR_DO_CANNY_PRUNING, cvSize( <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span> ));<span class="hljs-comment"><span class="hljs-comment">// /*   CvSeq: CvSeq* cvHaarDetectObjects( const CvArr* image,// ,     CvHaarClassifierCascade* cascade,//  CvMemStorage* storage,//   double scale_factor = 1.1,//   int min_neighbors = 4,//   int flags = 0|CV_HAAR_DO_CANNY_PRUNING,//   CvSize min_size = cvSize(0,0)//   ); */</span></span></code> </pre><br><br>  The function of the function, and the result of the work of the entire module depends on the parameters <i>cvHaarDetectObjects ()</i> : <br><br>  <b>The minimum adjacent threshold</b> is how intense each part of the face is.  It is necessary to configure manually, because without a threshold the detector generates many of the same recognitions in one place.  <i>Usually isolated detections are false</i> , so it makes sense <i>to discard them</i> .  It also makes sense to <i>combine multiple detections for each face area into a single entity</i> . <br>  OpenCV makes these both conditions before returning the list of detected objects. <br><br>  The merge step first groups the rectangles that contain a large number of overlaps, and then finds the averaged rectangle for the group. <br><br>  Then all the rectangles in the group are replaced with the calculated average rectangle. <br>  There are small groups between isolated rectangles and large groups, which can be both individuals and false detections. <br><br>  The minimum adjacent threshold sets the clipping level for dropping or saving groups of rectangles depending on how many raw detections are in the group.  By default, this parameter is three, which means merging a group of three or more rectangles and discarding groups with fewer rectangles.  <i>If there is a noticeable lack of recognition of a large number of faces and facial features</i> , then the <i>threshold value should be reduced to a smaller number</i> .  If it is set to zero, the function will return a complete list of raw Haar classifier detections. <br><br>  To make the <b>minimum scale</b> larger than the default, you can set a new value for this parameter.  If you decide to use a value that is different from the standard, then you need to make sure that the standard proportions remain (ratio of width to height).  In this case, the aspect ratio will be 1: 1. <br><br>  The fourth parameter of the function determines how fast OpenCV will zoom in for facial detections along with each pass taken through the image.  Setting a larger value makes the detector work faster (due to fewer passes of the scanning window), but if the minimum scale is too high, then there is a possibility of too fast transitions between scales and <i>omission of faces</i> .  By default, in OpenCV, this parameter is set to 1.1, or in other words, the scale increases with a factor of 1.1 (10%) with each pass. <br><br>  The flag variable of the cvHaarDetectObject () function can take several values: <br> <code>- 0; <br> - CV_HAAR_DO_CANNY_PRUNING; <br> - CV_HAAR_FIND_BIGGEST_OBJECT; <br> - CV_HAAR_DO_ROUGH_SEARCH; <br> - CV_HAAR_DO_CANNY_PRUNING; <br> - CV_HAAR_SCALE_IMAGE.</code> <br> <br>  If <i>the ‚Äúpractical cropping‚Äù flag is</i> selected (0 | CV_HAAR_DO_CANNY_PRUNING), then the detector skips those areas of the image that are unlikely to have a face inside, which reduces the computation time and possibly eliminates some false detections.  Skipped areas are identified by a detector that detects such ‚Äúunnecessary‚Äù edges across the entire image, before launching the facial detector.  Again, the choice of setting this type of flag is a compromise between choosing the speed and finding more faces.  Setting the flag will increase the processing speed, but may lead to the omission of some individuals. <br><br>  The minimum size for which the search is performed, you need to set as small as possible, since what scale of the face will be - not defined.  By default, it is (0, 0), which means the use of such a scale, which is recorded in the classifier xml-file. <br><br>  An example of poor recognition of facial features at the stage of a poorly trained classifier with incorrectly selected search parameters: <br><br><img src="https://habrastorage.org/storage2/5e9/048/3a2/5e90483a243c46c4e9729d695382112d.png"><br><br>  An example of good facial recognition at the stage of a well-trained classifier: <br><br><img src="https://habrastorage.org/storage2/651/b48/dd3/651b48dd302163d7b296879cdfea6eba.png"><br><br></li><li>  Functions <i>recognizeResult (resPic);</i>  information is transmitted, what type of classifier worked in the function <i>detect_and_draw (resPic);</i>  in other words, which object was found.  And if the classifiers found satisfy the conditions of the type of emotion, then information is output that the emotion is defined. <br><br>  For example, if the following classifiers worked: <br><pre> <code class="cpp hljs">‚Ä¢ happysmile_kalian_classifier.xml, ‚Ä¢ happyeyes_kalian_classifier.xml, ‚Ä¢ happyeyebrows_kalian_classifier.xml</code> </pre><br>  - this is the emotion of <b>joy</b> . <br><img src="https://habrastorage.org/storage2/e72/20b/a45/e7220ba457f74ab91b918cd15638dd9a.png"><br><br>  <u>- Emotion?</u>  <u>- Yes !!!</u> </li></ul><br><br><h4>  Testing module and algorithm </h4><br>  To test the performance and speed of the algorithm, tests were carried out with a full EmotionDB database (15 images - no longer needed for the test) and a simple array of images in a local directory (10 images).  These images in the database and in the array have a different size, there are color, and there are black and white photographs, with different numbers of people on them. <br>  For the tests, the built-in file OpenCV <i>performance.cpp is used</i> , where all data is calculated and the <a href="http://gim.unmc.edu/dxtests/ROC1.htm">ROC curve is</a> plotted.  An example of the calculation of stages or stages of the classifier: <br><br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>* numclassifiers = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span>[cascade-&gt;count]; numclassifiers[<span class="hljs-number"><span class="hljs-number">0</span></span>] = cascade-&gt;stage_classifier[<span class="hljs-number"><span class="hljs-number">0</span></span>].count; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>( i = <span class="hljs-number"><span class="hljs-number">1</span></span>; i &lt; cascade-&gt;count; i++ ) { numclassifiers[i] = numclassifiers[i<span class="hljs-number"><span class="hljs-number">-1</span></span>] + cascade-&gt;stage_classifier[i].count; }</code> </pre><br><br>  Although there are tilted faces in the test image set, the algorithm does not skip and identify such faces, but detection works at a lower speed than is possible in the simple case of a non-tilted face.  In conditions of insufficient illumination and large fusion with the background of the desired objects, as well as interfering noise and noise in the image, the module works slower than with normal images, along with the standard Viola-Jones algorithm. <br>  In conditions of poor visibility of the object (mixing with the background), the module copes with recognition better by using the proposed mechanism of preliminary image processing. <br>      ,       -      OpenCV   -       : <br>    <br><br><table><tbody><tr><th> / </th><th> Viola-Jones OpenCV </th><th> EmoRec (prep. + Viola-Jones + OpenCV) </th></tr><tr><td>   (10 ) </td><td> 89%, ~7  (7475 ms) </td><td> 93%, ~7  (7412 ms) </td></tr><tr><td>  EmotionDB (15 ) </td><td> 92%, ~12  (12167 ms) </td><td> 94%, ~12  (11870 ms) </td></tr><tr><td> * ‚Äî % ,   (),       </td></tr></tbody></table><br><br><h4>  findings </h4><br> C         ,      ,         .     .  ,    ,            ,     . <br><br>  :       (~250-300 ms),    .     400 ms  .      ,           . <br><br><h4>  ,     </h4><br><ol><li>       Viola Jones   ? </li><li>      ? </li><li>  ? </li><li>     .           ,    .             .        ? </li></ol><br><br>  Thanks to all!  !      !   <a href="https://habrahabr.ru/users/skynoname/" class="user_link">SkyNoName</a> <br><br> <b>PS</b>       !    ¬´¬ª,    -  ‚Ä¶ </div><p>Source: <a href="https://habr.com/ru/post/134857/">https://habr.com/ru/post/134857/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../134851/index.html">Autoconfiguration system for VoIP devices on the knee</a></li>
<li><a href="../134852/index.html">Failure at DCU MSU</a></li>
<li><a href="../134853/index.html">Global Toolkit Update</a></li>
<li><a href="../134854/index.html">SkypeTab a week later + resentment to the Unity post</a></li>
<li><a href="../134856/index.html">Competition of scientific curators of design work of students on nanotechnology</a></li>
<li><a href="../134859/index.html">2GIS for iPhone</a></li>
<li><a href="../134860/index.html">Safe and comfortable access to the Internet or how to protect your network from Internet threats without inconvenience</a></li>
<li><a href="../134861/index.html">Information security tools and where tar</a></li>
<li><a href="../134862/index.html">Breaking with the sequel</a></li>
<li><a href="../134863/index.html">First steps in Python programming</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>