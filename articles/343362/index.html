<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Broadcast h264 video without transcoding and delay</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is no secret that when controlling aircraft, video transmission from the device itself to the ground is often used. Usually this opportunity is pro...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Broadcast h264 video without transcoding and delay</h1><div class="post__text post__text-html js-mediator-article">  It is no secret that when controlling aircraft, video transmission from the device itself to the ground is often used.  Usually this opportunity is provided by the manufacturers of UAVs themselves.  However, what to do if the drone is assembled by yourself? <br><br>  We and our Swiss partners from Helvetis were faced with the task of streaming live video from a webcam from a low-powered embedded device on the drone via WiFi to a Windows tablet.  Ideally, we would like: <br><br><ul><li>  delay &lt;0.3s; </li><li>  low CPU utilization on the embedded system (less than 10% per core); </li><li>  Resolution at least 480p (better than 720p). </li></ul><br>  It would seem that could go wrong? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/if/hi/ja/ifhijazwly7o2cb0h27satruiku.jpeg"><br><a name="habracut"></a><br>  So, we stopped at the following list of equipment: <br><br><ul><li>  Minnowboard 2-core, Atom E3826 @ 1.4 GHz, OS: Ubuntu 16.04 </li><li>  ELP USB100W04H webcam that supports multiple formats (YUV, MJPEG, H264) </li><li>  Windows tablet ASUS VivoTab Note 8 </li></ul><br><h1>  Attempts to get by with standard solutions. </h1><br><h3>  Simple solution with Python + OpenCV </h3><br>  First, we tried using a <a href="http://www.chioka.in/python-live-video-streaming-example/">simple Python script</a> that, using OpenCV, received frames from the camera, compressed them using JPEG, and sent them over HTTP to the client application. <br><br><div class="spoiler">  <b class="spoiler_title">Http mjpg streaming to python</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flask, render_template, Response <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2 <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">VideoCamera</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(object)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.video = cv2.VideoCapture(<span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__del__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> self.video.release() <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_frame</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> success, image = self.video.read() ret, jpeg = cv2.imencode(<span class="hljs-string"><span class="hljs-string">'.jpg'</span></span>, image) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> jpeg.tobytes() app = Flask(__name__) @app.route(<span class="hljs-string"><span class="hljs-string">'/'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">index</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> render_template(<span class="hljs-string"><span class="hljs-string">'index.html'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(camera)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: frame = camera.get_frame() <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> (<span class="hljs-string"><span class="hljs-string">b'--frame\r\n'</span></span> <span class="hljs-string"><span class="hljs-string">b'Content-Type: image/jpeg\r\n\r\n'</span></span> + frame + <span class="hljs-string"><span class="hljs-string">b'\r\n\r\n'</span></span>) @app.route(<span class="hljs-string"><span class="hljs-string">'/video_feed'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">video_feed</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> Response(gen(VideoCamera()), mimetype=<span class="hljs-string"><span class="hljs-string">'multipart/x-mixed-replace; boundary=frame'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: app.run(host=<span class="hljs-string"><span class="hljs-string">'0.0.0.0'</span></span>, debug=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br></div></div><br>  This approach turned out to be (almost) working.  As a viewing application, you could use any web browser.  However, we immediately noticed that the frame rate was lower than expected, and the CPU utilization level on Minnowboard was constantly at 100%.  The embedded device simply could not cope with real-time frame encoding.  From the advantages of this solution, it is worth noting a very small delay in the transmission of 480p video with a frequency of no more than 10 frames per second. <br><br>  During the search, a webcam was detected, which, in addition to uncompressed YUV frames, could produce frames in MJPEG format.  It was decided to use this useful feature to reduce the load on the CPU and find a way to transfer video without transcoding. <br><br><h3>  FFmpeg / VLC </h3><br>  First of all, we tried everybody‚Äôs favorite open-source ffmpeg combine, which allows, among other things, to read a video stream from a UVC device, encode it and transmit.  After a short dive into the manual, the command line keys were found, which allowed to receive and transmit a compressed MJPEG video stream without transcoding. <br><br><pre> <code class="bash hljs">ffmpeg -f v4l2 -s 640x480 -input_format mjpeg -i /dev/video0 -c:v copy -f mjpeg udp://ip:port</code> </pre><br>  CPU utilization was low.  Delighted, we eagerly opened the stream in the ffplay player ... To our disappointment, the video delay level was absolutely unacceptable (about 2 - 3 seconds).  After trying everything <a href="https://trac.ffmpeg.org/wiki/StreamingGuide">from here</a> and going through the Internet, we still could not achieve a positive result and decided to abandon ffmpeg. <br><br>  After the failure of ffmpeg, it was the turn of the VLC media player, or rather the console utility cvlc.  VLC by default uses a bunch of buffers that, on the one hand, help to achieve a smooth image, but on the other hand, give a serious delay of a few seconds.  Having suffered a great deal, we selected the parameters with which the streaming looked tolerable enough, i.e.  the delay was not very long (about 0.5 s), there was no transcoding, and the client showed the video fairly smoothly (it was necessary, however, the client left a small buffer of 150 ms). <br><br>  This is the summary line for cvlc: <br><br><pre> <code class="bash hljs">cvlc -v v4l2:///dev/video0:chroma=<span class="hljs-string"><span class="hljs-string">"MJPG"</span></span>:width=640:height=480:fps=30 --sout=<span class="hljs-string"><span class="hljs-string">"#rtp{sdp=rtsp://:port/live,caching=0}"</span></span> --rtsp-timeout=-1 --sout-udp-caching=0 --network-caching=0 --live-caching=0</code> </pre><br>  Unfortunately, the video did not work quite stably, and a delay of 0.5 s was unacceptable for us. <br><br><h3>  Mjpg-streamer </h3><br>  Having come across <a href="http://petrkout.com/electronics/low-latency-0-4-s-video-streaming-from-raspberry-pi-mjpeg-streamer-opencv/">an article</a> about practically our task, we decided to try mjpg-streamer.  Tried, liked it!  Absolutely no change turned out to use mjpg-streamer for our needs without significant delay in video at 480p resolution. <br><br>  Against the background of previous failures, we were happy for quite a long time, but then we wanted more.  Namely: a little less to hammer the channel and improve the quality of the video up to 720p. <br><br><h1>  H264 streaming </h1><br>  To reduce the download of the channel, we decided to change the used codec to h264 (having found a suitable web-camera in our stocks).  Mjpg-streamer did not have h264 support, so it was decided to modify it.  During development, we used two cameras with a built-in h264 codec, manufactured by Logitech and ELP.  As it turned out, the content of the h264 stream in these cameras was significantly different. <br><br><h3>  Chambers and flow pattern </h3><br>  The h264 stream consists of several types of network abstraction layer packets.  Our cameras generated 5 types of packages: <br><br><ul><li>  Picture parameter set (PPS) </li><li>  Sequence parameter set (SPS) </li><li>  Coded slice layer without partitioning, IDR picture </li><li>  Coded slice layer without partitioning, non-IDR picture </li><li>  Coded slice data partition </li></ul><br>  IDR (Instantaneous decoding refresh) - package containing the encoded image.  At the same time all the necessary data for decoding the image are in this package.  This package is necessary for the decoder to start generating an image.  Usually the first frame of any video compressed h264 - IDR picture. <br><br>  Non-IDR is a packet containing an encoded image containing references to other frames.  The decoder is not able to recover the image on one Non-IDR frame without the presence of other packets. <br><br>  In addition to the IDR frame, the decoder needs PPS and SPS packets to decode the image.  These packages contain metadata about the image and frame stream. <br><br>  Based on the mjpg-streamer code, we used the API V4L2 (video4linux2) to read the data from the cameras.  As it turned out, one ‚Äúframe‚Äù of the video contained several NAL packets. <br><br>  It is in the content of the ‚Äúframes‚Äù that the essential difference between the cameras was found.  We used the <a href="https://github.com/aizvorski/h264bitstream">h264bitstream</a> library to parse the stream.  There are <a href="https://github.com/shi-yan/H264Naked">standalone utilities</a> that allow you to view the contents of a stream. <br><br><img src="https://habrastorage.org/webt/59/f0/a0/59f0a061ae1e2427496288.jpeg"><br><br>  The Logitech camera frame stream consisted mainly of non-IDR frames, besides being divided into several data partitions.  Every 30 seconds, the camera generated a packet containing an IDR picture, SPS and PPS.  Since the decoder needs an IDR package in order to start decoding the video, this situation did not suit us right away.  Unfortunately, it turned out that there is no adequate way to establish the period with which the camera generates IDR packets.  Therefore, we had to abandon the use of this camera. <br><br><img src="https://habrastorage.org/webt/59/f0/a0/59f0a061cb2de667322895.jpeg"><br><br>  The ELP camera was much more convenient.  Each frame we received contained PPS and SPS packets.  In addition, the camera generated an IDR packet every 30 frames (~ 1s period).  It suited us quite well and we opted for this camera. <br><br><h3>  Implementing a broadcast server based on mjpg-streamer </h3><br>  The basis of the server part, it was decided to take the above mjpg-streamer.  Its architecture made it easy to add new input and output plugins.  We started by adding a plug-in to read the h264 stream from the device.  An existing http plugin was chosen as the output plugin. <br><br>  In V4L2, it was enough to indicate that we want to receive frames in the V4L2_PIX_FMT_H264 format in order to start receiving the h264 stream. Since an IDR frame is needed for decoding a stream, we parsed the stream and waited for the IDR frame.  The client application stream was sent via HTTP starting from this frame. <br><br>  On the client side, we decided to use libavformat and libavcodec from the ffmpeg project to read and decode the h264 stream.  In the first test prototype, streaming over the network, splitting it into frames and decoding was assigned to ffmpeg, converting the resulting decoded image from NV12 format to RGB and displaying was done on OpenCV. <br><br>  The first tests showed that this method of video broadcasting is efficient, but there is a significant delay (about 1 second).  Our suspicion fell on the http protocol, so it was decided to use UDP to transmit packets. <br><br>  Since we did not need to support existing protocols like RTP, we implemented our simplest <s>bicycle</s> protocol, in which the NAL packets of the h264 stream were transmitted within the UDP datagram.  After a little refinement of the receiving part, we were pleasantly surprised by the low latency of the video on the desktop PC.  However, the very first tests on a mobile device showed that software decoding of h264 is not a fad of mobile processors.  The tablet just did not have time to process frames in real time. <br><br>  Since the Atom Z3740 processor used on our tablet supports Quick Sync Video (QSV) technology, we tried using the QSV h264 decoder from libavcodec.  To our surprise, he not only did not improve the situation, but also increased the delay to 1.5 seconds even on a powerful desktop PC!  However, this approach did significantly reduce the load on the CPU. <br><br>  Having tried various decoder configuration options in ffmpeg, it was decided to abandon libavcodec and use the <a href="https://software.intel.com/en-us/media-sdk">Intel Media SDK</a> directly. <br><br>  The first surprise for us was the horror, which is proposed to immerse the person who decided to develop using the Media SDK.  The official <a href="https://github.com/Intel-Media-SDK/samples">example</a> offered to developers is a powerful combine that can do everything, but which is difficult to understand.  Fortunately, we found like-minded people on the <a href="https://software.intel.com/en-us/forums/intel-media-sdk">Intel forums</a> who were also dissatisfied with an example.  They found old, but more easily digestible <a href="https://software.intel.com/en-us/articles/media-sdk-tutorials-for-client-and-server">tutorials</a> .  Based on the simple_2_decode example, we got the following code. <br><br><div class="spoiler">  <b class="spoiler_title">Stream Decoding with Intel Media SDK</b> <div class="spoiler_text"><pre> <code class="cpp hljs">mfxStatus sts = MFX_ERR_NONE; <span class="hljs-comment"><span class="hljs-comment">//     h264 mfxBitstream mfx_bitstream; memset(&amp;mfx_bitstream, 0, sizeof(_mfxBS)); mfx_bitstream.MaxLength = 1 * 1024 * 1024; // 1MB mfx_bitstream.Data = new mfxU8[mfx_bitstream.MaxLength]; //     UDP StreamReader *reader = new StreamReader(/*...*/); MFXVideoDECODE *mfx_dec; mfxVideoParam mfx_video_params; MFXVideoSession session; mfxFrameAllocator *mfx_allocator; //   MFX mfxIMPL impl = MFX_IMPL_AUTO; mfxVersion ver = { { 0, 1 } }; session.Init(sts, &amp;ver); if (sts &lt; MFX_ERR_NONE) return 0; // :( //  ,   AVC (h.264) mfx_dec = new MFXVideoDECODE(session); memset(&amp;mfx_video_params, 0, sizeof(mfx_video_params)); mfx_video_params.mfx.CodecId = MFX_CODEC_AVC; //     mfx_video_params.IOPattern = MFX_IOPATTERN_OUT_SYSTEM_MEMORY; //       mfx_video_params.AsyncDepth = 1; //     reader-&gt;ReadToBitstream(&amp;mfx_bitstream); sts = mfx_dec-&gt;DecodeHeader(&amp;mfx_bitstream, &amp;mfx_video_params); if (sts &lt; MFX_ERR_NONE) return 0; // :( //      mfxFrameAllocRequest request; memset(&amp;request, 0, sizeof(request)); sts = mfx_dec-&gt;QueryIOSurf(&amp;mfx_video_params, &amp;request); if (sts &lt; MFX_ERR_NONE) return 0; // :( mfxU16 numSurfaces = request.NumFrameSuggested; //          32 mfxU16 width = (mfxU16)MSDK_ALIGN32(request.Info.Width); mfxU16 height = (mfxU16)MSDK_ALIGN32(request.Info.Height); // NV12 -  YUV 4:2:0, 12    mfxU8 bitsPerPixel = 12; mfxU32 surfaceSize = width * height * bitsPerPixel / 8; //          mfxU8* surfaceBuffers = new mfxU8[surfaceSize * numSurfaces]; //      mfxFrameSurface1** pmfxSurfaces = new mfxFrameSurface1*[numSurfaces]; for(int i = 0; i &lt; numSurfaces; i++) { pmfxSurfaces[i] = new mfxFrameSurface1; memset(pmfxSurfaces[i], 0, sizeof(mfxFrameSurface1)); memcpy(&amp;(pmfxSurfaces[i]-&gt;Info), &amp;(_mfxVideoParams.mfx.FrameInfo), sizeof(mfxFrameInfo)); pmfxSurfaces[i]-&gt;Data.Y = &amp;surfaceBuffers[surfaceSize * i]; pmfxSurfaces[i]-&gt;Data.U = pmfxSurfaces[i]-&gt;Data.Y + width * height; pmfxSurfaces[i]-&gt;Data.V = pmfxSurfaces[i]-&gt;Data.U + 1; pmfxSurfaces[i]-&gt;Data.Pitch = width; } sts = mfx_dec-&gt;Init(&amp;mfx_video_params); if (sts &lt; MFX_ERR_NONE) return 0; // :( mfxSyncPoint syncp; mfxFrameSurface1* pmfxOutSurface = NULL; mfxU32 nFrame = 0; //    while (reader-&gt;IsActive() &amp;&amp; (MFX_ERR_NONE &lt;= sts || MFX_ERR_MORE_DATA == sts || MFX_ERR_MORE_SURFACE == sts)) { //      if (MFX_WRN_DEVICE_BUSY == sts) Sleep(1); if (MFX_ERR_MORE_DATA == sts) reader-&gt;ReadToBitstream(mfx_bitstream); if (MFX_ERR_MORE_SURFACE == sts || MFX_ERR_NONE == sts) { nIndex = GetFreeSurfaceIndex(pmfxSurfaces, numSurfaces); if (nIndex == MFX_ERR_NOT_FOUND) break; } //   //    NAL-      sts = mfx_dec-&gt;DecodeFrameAsync(mfx_bitstream, pmfxSurfaces[nIndex], &amp;pmfxOutSurface, &amp;syncp); //   if (MFX_ERR_NONE &lt; sts &amp;&amp; syncp) sts = MFX_ERR_NONE; //     if (MFX_ERR_NONE == sts) sts = session.SyncOperation(syncp, 60000); if (MFX_ERR_NONE == sts) { //  ! mfxFrameInfo* pInfo = &amp;pmfxOutSurface-&gt;Info; mfxFrameData* pData = &amp;pmfxOutSurface-&gt;Data; //     NV12 //  Y: pData-&gt;Y,   //  UV: pData-UV,   2     Y } } //   </span></span></code> </pre><br></div></div><br>  After implementing video decoding using the Media SDK, we faced a similar situation - the video delay was 1.5 seconds.  Desperate, we turned to the forums and found tips that were supposed to reduce the delay in decoding video. <br><br>  The h264 decoder from the Media SDK accumulates frames before issuing the decoded image.  It was found that if the ‚Äúend of stream‚Äù flag is set in the data structure transmitted to the decoder (mfxBitstream), the delay is reduced to ~ 0.5 seconds: <br><br><pre> <code class="cpp hljs">mfx_bitstream.DataFlag = MFX_BITSTREAM_EOS;</code> </pre><br>  Further experimentally, it was found that the decoder holds 5 frames in the queue, even if the end of stream flag is set.  As a result, we had to add the code that simulated the ‚Äúfinal end of the stream‚Äù and forced the decoder to output frames from this queue: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>( no_frames_in_queue ) sts = mfx_dec-&gt;DecodeFrameAsync(mfx_bitstream, pmfxSurfaces[nIndex], &amp;pmfxOutSurface, &amp;syncp); <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> sts = mfx_dec-&gt;DecodeFrameAsync(<span class="hljs-number"><span class="hljs-number">0</span></span>, pmfxSurfaces[nIndex], &amp;pmfxOutSurface, &amp;syncp); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (sts == MFX_ERR_MORE_DATA) { no_frames_in_queue = <span class="hljs-literal"><span class="hljs-literal">true</span></span>; }</code> </pre><br>  After that, the delay level went down to the acceptable, i.e.  imperceptible gaze. <br><br><h1>  findings </h1><br>  Starting the task of broadcasting video in real time, we very much hoped to use existing solutions and do without our bicycles. <br><br>  Our main hope was such video giants as FFmpeg and VLC.  Despite the fact that they seem to be able to do what we need (to transmit video without transcoding), we were not able to remove the delay resulting from the transmission of video. <br><br>  Almost accidentally stumbling upon the mjpg-streamer project, we were fascinated by its simplicity and precise work in translating video in the MJPG format.  If you suddenly need to transfer this particular format, then we strongly recommend using it.  It is no coincidence that it was on its basis that we implemented our decision. <br><br>  As a result of the development, we got a fairly lightweight solution for transmitting video without delay, not demanding to the resources of either the transmitting or the receiving parties.  In the task of decoding video, the Intel Media Media SDK helped us a lot, even if we had to use a bit of force to make it give frames without buffering. </div><p>Source: <a href="https://habr.com/ru/post/343362/">https://habr.com/ru/post/343362/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343348/index.html">3 Unusual Linux Networking Cases</a></li>
<li><a href="../343350/index.html">‚ÄúDo you want to be a system architect? There is only light and purity ... "</a></li>
<li><a href="../343352/index.html">How we did a huge amount of communication for a rather big security structure</a></li>
<li><a href="../343356/index.html">Interpolation polynomial on arbitrary functions</a></li>
<li><a href="../343358/index.html">We collected the user activity in WPF</a></li>
<li><a href="../343364/index.html">Simplest RESTful service on Kotlin and Spring boot</a></li>
<li><a href="../343366/index.html">UX-design: common misconceptions and myths</a></li>
<li><a href="../343368/index.html">Accelerate Ansible</a></li>
<li><a href="../343370/index.html">EPAM + universities: how we work with universities in Russia</a></li>
<li><a href="../343372/index.html">Actual man month</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>