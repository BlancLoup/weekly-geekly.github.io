<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Familiarity with the Ceph repository in pictures</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Cloud file storage continues to gain popularity, and their demands continue to grow. Modern systems are no longer able to fully meet all these require...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Familiarity with the Ceph repository in pictures</h1><div class="post__text post__text-html js-mediator-article">  Cloud file storage continues to gain popularity, and their demands continue to grow.  Modern systems are no longer able to fully meet all these requirements without a significant expenditure of resources to support and scale these systems.  By system, I mean a cluster with some level of data access.  Reliability of storage and high availability is important for the user, so that files can always be easily and quickly received, and the risk of data loss tends to zero.  In turn, for suppliers and administrators of such storage, simplicity of support, scalability and low cost of hardware and software components are important. <br><br><h1>  Meet Ceph </h1><br>  Ceph is an open source software-defined distributed file system, devoid of bottlenecks and single points of failure, which is a cluster of nodes that perform various functions, provides storage and replication of data, as well as load distribution, which guarantees high availability and reliability.  The system is free, although developers can provide paid support.  No special equipment is required. <br><br><img src="https://habrastorage.org/files/0d4/f57/8fe/0d4f578fe9044ab4bb094a8cc17d9280.gif">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When any disk, node or group of nodes fails, Ceph will not only ensure data integrity, but will also restore lost copies on other nodes until the failed nodes or disks are replaced with working ones.  At the same time, a rebuild occurs without a second downtime and is transparent to customers. <br><a name="habracut"></a><br><img src="https://habrastorage.org/files/11f/3a0/5fd/11f3a05fd17a4adea5e2c815b23558b1.gif"><br><br><h1>  Node roles and demons </h1><br>  Since the system is software defined and works on top of standard file systems and network layers, you can take a bunch of different servers, stuff them with different disks of different sizes, connect all this happiness with some kind of network (better fast) and raise the cluster.  You can plug into these servers on the second network card, and connect them with a second network to speed up the inter-server data exchange.  And experiments with settings and schemes can be easily carried out even in a virtual environment.  My experience of experiments shows that the longest in this process is the installation of the OS.  If we have three servers with disks and a configured network, it will take 5-10 minutes to raise a working cluster with default settings (if everything is done correctly). <br><br><img src="https://habrastorage.org/files/5af/2fe/56e/5af2fe56e7764e998d18a85d0b0f71da.png"><br><br>  On top of the operating system, there are Ceph daemons that perform various cluster roles.  Thus, a single server can act, for example, both as a monitor (MON), and as a data warehouse (OSD).  In the meantime, another server can act as a data warehouse and as a metadata server (MDS).  In large clusters, daemons run on separate machines, but in small clusters, where the number of servers is very limited, some servers can perform two or three roles at once.  Depends on server power and the roles themselves.  Of course, everything will work faster on separate servers, but it is not always possible to implement it.  A cluster can even be assembled from one machine and just one disk, and it will work.  Another conversation that it would not make sense.  It should be noted that due to software determinability, the storage can be raised even on top of a RAID or iSCSI device, but in most cases this will also not make sense. <br><br>  The documentation lists 3 types of daemons: <br><br><ul><li>  Mon - monitor daemon </li><li>  OSD - storage daemon </li><li>  MDS - Metadata Server (required only when using CephFS) </li></ul><br>  The initial cluster can be created from several machines, combining cluster roles on them.  Then, with the growth of the cluster and the addition of new servers, some roles can be duplicated on other machines or completely transferred to separate servers. <br><br><img src="https://habrastorage.org/files/7e2/b2a/18a/7e2b2a18af5d4b60a76e9d6ff31f8035.gif"><br><br><h1>  Storage structure </h1><br>  For a start, short and incomprehensible.  A cluster can have one or many data pools of different purposes and with different settings.  Pools are divided into placement groups.  In the placement groups are stored objects that are accessed by customers.  At this the logical level ends and the physical begins, because each placement group has one main disk and several replica disks (how much depends on the pool replication factor).  In other words, at the logical level, an object is stored in a particular placement group, and at the physical level - on disks that are assigned to it.  In this case, the disks can physically be located on different nodes or even in different data centers. <br><br><img src="https://habrastorage.org/files/654/6aa/686/6546aa686a7f4c0bbcada5d74eb8ad96.png"><br><br>  Further detail &amp; understandable. <br><br><h1>  Replication factor (RF) </h1><br>  Replication factor is the level of data redundancy.  The number of copies of data that will be stored on different disks.  The variable size is responsible for this parameter.  The replication factor can be different for each pool, and can be changed on the fly.  In general, in Ceph almost all parameters can be changed on the fly, instantly receiving a cluster response.  At first, we can have size = 2, and in this case, the pool will store two copies of one piece of data on different disks.  This parameter of the pool can be changed to size = 3, and at the same time the cluster will begin to redistribute the data, decomposing another copy of the existing data on the disks, without stopping the work of clients. <br><br><h1>  Pool </h1><br>  A pool is a logical abstract container for storing user data.  Any data is stored in the pool as objects.  Multiple pools can be spread on the same disks (and maybe in different ways how to configure) using different sets of placement groups.  Each pool has a number of customizable parameters: replication factor, number of placement groups, the minimum number of live replicas of the object required for operation, etc. Each pool can have its own replication policy (by city, data center, rack or even disk).  For example, a pool for hosting may have a replication factor of size = 3, and a failure zone will be data centers.  And then Ceph will ensure that each piece of data has one copy in three data centers.  In the meantime, the pool for virtual machines may have a replication factor of size = 2, and the level of failure will already be the server rack.  And in this case, the cluster will store only two copies.  At the same time, if we have two racks with a storage of virtual images in one data center, and two racks in another, the system will not pay attention to data centers, and both copies of data can fly into one data center, but guaranteed to be in different racks, as we wanted . <br><br><h1>  Placement Group (PG) </h1><br>  Placement groups are such a link between the physical storage layer (disks) and the logical organization of data (pools). <br><br>  Each object at the logical level is stored in a specific place-group.  At the physical level, it lies in the right number of copies on different physical disks, which are included in this placement group (in fact, not disks, but OSD, but usually one OSD is one disk, and for simplicity I will call it drive, although I remind you, it may be a RAID array or an iSCSI device).  With the replication factor size = 3, each placement group includes three disks.  But at the same time, each disc is in a multitude of placement groups, and for some groups it will be primary, for others - a replica.  If OSD is included, for example, into three placement groups, then when such an OSD falls, placement groups will exclude it from work, and in its place each placement group will select a working OSD and spread the data on it.  Using this mechanism, a fairly uniform distribution of data and load is achieved.  This is a very simple and at the same time flexible solution. <br><br><img src="https://habrastorage.org/files/911/f51/eb4/911f51eb4cc8452f9a39226edb9ecd55.gif"><br><br><h1>  Monitors </h1><br>  A monitor is a daemon that acts as a coordinator from which the cluster begins.  As soon as we have at least one working monitor, we have a Ceph cluster.  The monitor stores information about the health and condition of the cluster, exchanging various maps with other monitors.  Clients access monitors to find out which OSDs to write / read data on.  When a new storage is deployed, the monitor (or several) is created first.  A cluster can live on one monitor, but it is recommended to make 3 or 5 monitors, in order to avoid the fall of the entire system due to the fall of a single monitor.  The main thing is that the number of these should be odd in order to avoid situations of a split consciousness (split-brain).  Monitors operate in a quorum, so if more than half of the monitors fall, the cluster will be locked to prevent data mismatch. <br><br><h1>  OSD (Object Storage Device) </h1><br>  OSD is a storage unit that stores the data itself and processes customer requests, exchanging data with other OSDs.  This is usually a disk.  And usually a separate OSD daemon is responsible for each OSD, which can be run on any machine on which this disk is installed.  This is the second thing that needs to be added to the cluster, when deployed.  One monitor and one OSD - the minimum set in order to raise the cluster and start using it.  If there are 12 disks spinning on the server for storage, then the same OSD daemons will be launched on it.  Clients work directly with the OSD themselves, bypassing the bottlenecks, and thus achieving load distribution.  The client always writes the object to the primary OSD for some group placement, and further on this OSD synchronizes data with the other (secondary) OSD from the same placement group.  Confirmation of a successful record can be sent to the client immediately after writing to the primary OSD, or maybe after reaching the minimum number of records (pool parameter min_size).  For example, if the replication factor size = 3, and min_size = 2, then confirmation of successful recording will be sent to the client when the object is written to at least two of the three OSDs (including the primary one). <br><br>  With different options for setting these parameters, we will observe different behavior. <br><br>  If size = 3 and min_size = 2: everything will be fine as long as 2 out of 3 OSD placement groups are alive.  When only 1 live OSD remains, the cluster will freeze the operations of this placement group until at least one more OSD comes to life. <br><br>  If size = min_size, then the placement group will be blocked when any OSD that is part of it is dropped.  And due to the high level of data blurring, the majority of crashes of at least one OSD will result in freezing of the entire or almost the entire cluster.  Therefore, the size parameter should always be at least one item larger than the min_size parameter. <br><br>  If size = 1, the cluster will work, but the death of any OSD will mean irretrievable data loss.  Ceph allows you to set this parameter to one, but even if the administrator does this for a specific purpose for a short time, he takes the risk. <br><br>  An OSD disk consists of two parts: the log and the data itself.  Accordingly, the data is first written to the log, then to the data section.  On the one hand, this provides additional reliability and some optimization, and on the other hand, an additional operation that affects performance.  The issue of log performance will be discussed below. <br><br><h1>  CRUSH algorithm </h1><br>  The mechanism of decentralization and distribution is based on the so-called CRUSH algorithm (Controlled Replicated Under Scalable Hashing), which plays an important role in the system architecture.  This algorithm allows you to uniquely determine the location of an object based on the object name hash and a specific map, which is formed based on the physical and logical structures of the cluster (data centers, halls, rows, racks, nodes, disks).  The map does not include location information.  Each client determines the path to the data himself, using the CRUSH algorithm and the current map, which he first asks for the monitor.  When you add a disk or server crashes, the map is updated. <br><br>  Thanks to determinism, two different clients will find the same unique path to one object on their own, eliminating the need to keep all these paths on some servers, synchronizing them with each other, giving a huge overload to the storage as a whole. <br><br>  Example: <br><br><img src="https://habrastorage.org/files/722/13c/799/72213c7996e544c99b51628852e365e8.gif"><br><br>  A client wants to write some object1 to pool1.  To do this, he looks at the card of placement groups, which the monitor has kindly provided him before, and sees that Pool1 is divided into 10 placement groups.  Then, using the CRUSH algorithm, which accepts the object name and the total number of placement groups in Pool1 as input, the placement group ID is calculated.  Following the map, the client understands that this OS placement group has three OSDs (say, their numbers: 17, 9, and 22), the first of which is primary, which means the client will write to him.  By the way, there are three of them, because replication factor size = 3 is set in this pool.  After the object is successfully written to OSD_17, the client‚Äôs work is finished (this is if the pool parameter is min_size = 1), and OSD_17 replicates this object to OSD_9 and OSD_22 assigned to this placement group.  It is important to understand that this is a simplified explanation of the operation of the algorithm. <br><br>  By default, our CRUSH card is flat, all nodes are in the same space.  However, you can easily turn this plane into a tree by distributing servers in racks, racks in rows, rows in halls, halls in data centers, and data centers in different cities and planets, indicating what level to consider the zone of failure.  Using this new map, Ceph will more intelligently distribute data, taking into account the individual characteristics of the organization, preventing the sad consequences of a fire in a data center or a meteorite falling on a whole city.  Moreover, thanks to this flexible mechanism, it is possible to create additional layers, both at the upper levels (data centers and cities), and at the lower levels (for example, additional division into groups of disks within one server). <br><br><h1>  Caching </h1><br>  Ceph provides several ways to increase cluster performance with cache methods. <br><br>  <strong>Primary-affinity</strong> <br>  Each OSD has several weights, and one of them is responsible for which OSD in the placement group will be primary.  And, as we found out earlier, the client writes the data on the primary OSD.  So, you can add to the cluster a pack of SSD drives, making them always primary, reducing the weight of primary-affinity HDD drives to zero.  And then the recording will always be carried out first on a fast disk, and then slowly replicate to slow ones.  This method is the wrong, but the easiest to implement.  The main disadvantage is that one copy of the data will always be on the SSD and it will take a lot of such disks to fully cover the replication.  Although someone used this method in practice, I rather mentioned it in order to tell about the possibility of controlling the priority of a recording. <br><br>  <strong>Removal of journals on SSD</strong> <br>  In general, the lion's share of performance depends on the OSD logs.  When writing, the daemon first writes the data to the log, and then to the storage itself.  This is always true, except when using BTRFS as a file system on OSD, which can do this in parallel thanks to the copy-on-write technique, but I still do not understand how ready it is for industrial use.  Each OSD has its own log, and by default it is on the same disk as the data itself.  However, logs from four or five drives can be brought to one SSD, not bad speeding up write operations.  The method is not very flexible and convenient, but quite simple.  The disadvantage of the method is that during the departure of the SSD with the magazine, we will lose several OSDs at once, which is not very pleasant and introduces additional difficulties into all further support, which scales with the growth of the cluster. <br><br>  <strong>Cash Tiring</strong> <br>  The orthodoxy of this method is in its flexibility and scalability.  The scheme is such that we have a pool with cold data and a pool with hot data.  With frequent access to the object, it heats up and enters the hot pool, which consists of fast SSDs.  Then, if the object cools, it falls into a cold pool with slow HDDs.  This scheme allows you to easily change the SSD in the hot pool, which in turn can be of any size, because the parameters of heating and cooling are regulated. <br><br><img src="https://habrastorage.org/files/8f3/da1/f05/8f3da1f05866456da59f00c8de84bc82.gif"><br><br><h1>  From the client's point of view </h1><br>  Ceph provides the client with various options for accessing data: a block device, a file system, or object storage. <br><br><img src="https://habrastorage.org/files/fd4/84b/bcf/fd484bbcf6a44973b4c371a958bc382b.jpg"><br><br>  <strong>Block Device (RBD, Rados Block Device)</strong> <br>  Ceph allows you to create a block RBD device in the data pool, and later mount it on operating systems that support it (at the time of writing, there were only various linux distributions, however FreeBSD and VMWare are also working in this direction).  If the client does not support RBD (for example, Windows), then an intermediate iSCSI-target with RBD support (for example, tgt-rbd) can be used.  In addition, such a block device supports snapshots. <br><br>  <strong>CephFS file system</strong> <br>  A client can mount a CephFS file system if it has linux with kernel version 2.6.34 or newer.  If the kernel version is older, then you can mount it through FUSE (Filesystem in User Space).  In order for clients to connect Ceph as a file system, you need to raise at least one metadata server (MDS) in a cluster <br><br>  <strong>Object Gateway</strong> <br>  Using the RGW Gateway (RADOS Gateway), customers can be given the opportunity to use storage through a RESTful Amazon S3 or OpenStack Swift compatible API. <br><br>  <strong>Other...</strong> <br>  All these data access levels operate on top of the RADOS level.  The list can be supplemented by developing your data access layer using the librados API (through which the access layers listed above work).  At the moment there are C, Python, Ruby, Java and PHP bindings <br><br>  RADOS (Reliable Autonomic Distributed Object Store), in a nutshell, this is a layer of interaction between customers and the cluster. <br><br>  Wikipedia says that Ceph itself is written in C ++ and Python, and Canonical, CERN, Cisco, Fujitsu, Intel, Red Hat, SanDisk, and SUSE take part in the development. <br><br><h1>  Impressions </h1><br>  Why did I write all this and draw pictures?  Then that despite all these advantages, Ceph is either not very popular, or people eat it secretly, judging by the amount of information about it on the Internet. <br><br>  The fact that Ceph is flexible, simple and convenient, we found out.  The cluster can be raised on any hardware in a normal network, spending a minimum of time and effort, while Ceph will take care of data integrity by taking the necessary measures in case of iron failures.  The fact that Ceph is flexible, simple and scalable converges many points of view.  However, reviews of performance are very diverse.  Maybe someone did not cope with the logs, someone summed up the network and delays in I / O operations.  That is, making the cluster work is easy, but making it work quickly is perhaps more difficult.  Therefore, I appeal to IT professionals who have experience using Ceph in production.  Share in comments about your negative impressions. <br><br>  <strong>Links</strong> <br><blockquote>  <a href="http://ceph.com/">Ceph website</a> <br>  <a href="https://en.wikipedia.org/wiki/Ceph_(software)">Wikipedia</a> <br>  <a href="http://docs.ceph.com/docs/">Documentation</a> <br>  <a href="https://github.com/ceph">Github</a> <br>  <a href="https://www.amazon.com/Ceph-Cookbook-Karan-Singh/dp/1784393509">Ceph Recipe Book</a> <br>  <a href="https://www.amazon.com/Learning-Ceph-Karan-Singh/dp/1783985623">Learning Ceph Book</a> <br>  <a href="https://habrahabr.ru/post/315646/">Ceph on VMWare in 10 minutes</a> <br>  <a href="http://learningos.ru/intceph">Ceph Intensive in Russian</a> <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/313644/">https://habr.com/ru/post/313644/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../313632/index.html">GitLab on open source project management policy</a></li>
<li><a href="../313636/index.html">Linux print server with AD integration</a></li>
<li><a href="../313638/index.html">On the performance of named pipes in multi-process applications</a></li>
<li><a href="../313640/index.html">‚ÄúMonitoring .NET application performance: approaches and tools,‚Äù an interview with Dina Goldstein</a></li>
<li><a href="../313642/index.html">Microsoft unveiled an update to its Cognitive Toolkit.</a></li>
<li><a href="../313646/index.html">List of free transactional, hosting and marketing email-providers</a></li>
<li><a href="../313648/index.html">Machine Learning and Intel Xeon: Tencent In-Game Purchasing Advisory System</a></li>
<li><a href="../313652/index.html">What's New in vSphere 6.5: Security</a></li>
<li><a href="../313654/index.html">Implementing the undo / redo model for a complex document</a></li>
<li><a href="../313656/index.html">UX / UI track program at MBLTdev 16 conference</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>