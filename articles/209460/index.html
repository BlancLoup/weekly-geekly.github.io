<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating reliable iSCSI storage on Linux, part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part two 

 Prelude 
 Today I will tell you how I created a low-cost, fault-tolerant iSCSI storage from two Linux-based servers to serve the needs of ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating reliable iSCSI storage on Linux, part 1</h1><div class="post__text post__text-html js-mediator-article">  <a href="https://habr.com/post/209666/">Part two</a> <br><br><h4>  Prelude </h4><br>  Today I will tell you how I created a low-cost, fault-tolerant iSCSI storage from two Linux-based servers to serve the needs of a VMWare vSphere cluster.  There were similar articles ( <a href="http://habrahabr.ru/company/depocomputers/blog/130573/">for example</a> ), but my approach is somewhat different, and the solutions (the same heartbeat and iscsitarget) used there are already outdated. <br><br>  The article is intended for sufficiently experienced administrators who are not afraid of the phrase ‚Äúpatch and compile the kernel‚Äù, although some parts could be simplified and dispensed with no compilation at all, but I will write as I did myself.  Some simple things I will skip so as not to inflate the material.  The purpose of this article is rather to show general principles, and not to put everything in steps. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Introductory </h4><br>  My requirements were simple: create a cluster for running virtual machines that does not have a single point of failure.  And as a bonus, the repository had to be able to encrypt the data so that the enemies, having dragged the server away, would not get to them. <br><br>  VSphere was chosen as the hypervisor as the most well-established and complete product, and iSCSI as the protocol, as not requiring additional financial investments in the form of FC or FCoE switches.  With open source SAS targets, it‚Äôs pretty tight, if not worse, so this option was also rejected. <br><br>  Left storage.  Different branded solutions from leading vendors were discarded due to the high cost of both their own and the licenses for synchronous replication.  So we will do it ourselves, at the same time and learn. <br><br>  As the software was selected: <br><ul><li>  Debian Wheezy + LTS core 3.10 </li><li>  iSCSI target <a href="http://scst.sourceforge.net/">SCST</a> </li><li>  <a href="http://www.drbd.org/">DRBD</a> for replication </li><li>  <a href="http://clusterlabs.org/">Pacemaker</a> for cluster resource management and monitoring </li><li>  DM-Crypt kernel subsystem for encryption (AES-NI instructions in the processor will help us a lot) </li></ul><br>  As a result, such a simple scheme was born in a brief torment: <br><img src="https://habrastorage.org/getpro/habr/post_images/802/cdc/f7e/802cdcf7eddbe7bd05b1dd6486af53f4.png" alt="image"><a name="habracut"></a><br>  It shows that each of the servers has 10 gigabit interfaces (2 built-in and 4 on additional network cards).  6 of them are connected to the stack of switches (3 to each), and the remaining 4 - to the server-neighbor. <br>  Then replication through DRBD will go with them.  Replication cards can be replaced with 10-Gbps if you wish, but I had these on hand, so "I blinded him from what was." <br><br>  Thus, the sudden death of any of the cards will not lead to complete inoperability of any of the subsystems. <br><br>  Since the main task of these storages is reliable storage of big data (file server, mail archives, etc.), servers with 3.5 "disks were selected: <br><ul><li>  The <a href="http://www.supermicro.nl/products/chassis/3U/836/SC836E26-R1200.cfm">Supermicro SC836E26-R1200 case</a> on 16 3.5 "disks </li><li>  Motherboard <a href="http://www.supermicro.nl/products/motherboard/Xeon/C600/X9SRi-F.cfm">Supermicro X9SRi-F</a> </li><li>  <a href="http://ark.intel.com/products/64594">Intel E5-2620 processor</a> </li><li>  4 x 8GB DDR3 ECC memory </li><li>  <a href="http://www.lsi.com/products/raid-controllers/pages/megaraid-sas-9271-8i.aspx">LSI 9271-8i</a> RAID controller with supercapacitor for emergency cache flush to flash module </li><li>  16 disks <a href="http://www.seagate.com/internal-hard-drives/enterprise-hard-drives/hdd/enterprise-capacity-3-5-hdd/">Seagate Constellation ES.3 3Tb SAS</a> </li><li>  2 x 4-port <a href="http://www.intel.com/content/www/us/en/network-adapters/gigabit-network-adapters/ethernet-server-adapter-i350.html">Intel Ethernet I350-T4</a> network cards </li></ul><br><h4>  For the cause </h4><br><h5>  Discs </h5><br>  I created two RAID10 arrays of 8 disks on each of the servers. <br>  I decided to refuse RAID6 because  there was plenty of space, and the performance of RAID10 on random access tasks is higher.  Plus, the rebuild time is lower and the load goes only on one disk, and not on the whole array at once. <br><br>  In general, here everyone decides for himself. <br><br><h5>  Network part </h5><br>  With the iSCSI protocol, it‚Äôs pointless to use Bonding / Etherchannel to speed up its work. <br>  The reason is simple - it uses hash functions to distribute packets across channels, so it is very difficult to find such IP / MAC addresses so that a packet from IP1 to IP2 goes on one channel, and from IP1 to IP3 on another. <br>  On cisco, there is even a command that allows you to see which of the Etherchannel interfaces the packet will fly off: <br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># test etherchannel load-balance interface port-channel 1 ip 10.1.1.1 10.1.1.2 Would select Gi2/1/4 of Po1</span></span></code> </pre> <br>  Therefore, for our purposes, it is much better to use several paths to the LUN, which we will configure. <br><br>  On the switch, I created 6 VLANs (one for each external interface of the server): <br><pre> <code class="bash hljs">stack-3750x<span class="hljs-comment"><span class="hljs-comment"># sh vlan | i iSCSI 24 iSCSI_VLAN_1 active 25 iSCSI_VLAN_2 active 26 iSCSI_VLAN_3 active 27 iSCSI_VLAN_4 active 28 iSCSI_VLAN_5 active 29 iSCSI_VLAN_6 active</span></span></code> </pre><br>  Interfaces were made trunk for universality and something else will be seen later: <br><pre> <code class="bash hljs">interface GigabitEthernet1/0/11 description VMSTOR1-1 switchport trunk encapsulation dot1q switchport mode trunk switchport nonegotiate flowcontrol receive desired spanning-tree portfast trunk end</code> </pre><br>  The MTU on the switch should be set to maximum in order to reduce the server load (larger packet -&gt; fewer packets per second -&gt; fewer interrupts are generated).  In my case, this is 9198: <br><pre> <code class="bash hljs">(config)<span class="hljs-comment"><span class="hljs-comment"># system mtu jumbo 9198</span></span></code> </pre><br>  ESXi does not support MTU greater than 9000, so there is still some margin. <br><br>  Each VLAN-u was chosen address space, for simplicity, which looks like this: 10.1.  <b>VLAN_ID</b> .0 / 24 (for example - 10.1.24.0/24).  With a shortage of addresses, you can keep within smaller subnets, but it is more convenient. <br><br>  Each LUN will be represented by a separate iSCSI target, so each target has been selected ‚Äúcommon‚Äù cluster addresses, which will be raised on the node serving this target at the moment: 10.1.  <b>VLAN_ID</b> .10 and 10.1.  <b>VLAN_ID</b> .20 <br><br>  The servers will also have permanent addresses for management, in my case it is 10.1.0.100/24 ‚Äã‚Äãand .200 (in a separate VLAN) <br><br><h4>  Soft </h4><br>  So, here we install on both Debian servers in the minimum form, I will not dwell on this in detail. <br><br><h5>  Build Packages </h5><br>  I spent the assembly on a separate virtual machine so as not to litter the server with compilers and sources. <br>  To build a kernel for Debian, it‚Äôs enough to install a build-essential meta-package, and perhaps something else trivially, I don‚Äôt remember exactly. <br><br>  Download the latest kernel 3.10 with <a href="http://www.kernel.org/">kernel.org</a> : and unpack: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd /usr/src/ # wget https://www.kernel.org/pub/linux/kernel/v3.x/linux-3.10.27.tar.xz # tar xJf linux-3.10.27.tar.xz</span></span></code> </pre><br>  Next, download the latest revision of the stable SCST branch via SVN, generate a patch for our kernel version and apply it: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># svn checkout svn://svn.code.sf.net/p/scst/svn/branches/2.2.x scst-svn # cd scst-svn # scripts/generate-kernel-patch 3.10.27 &gt; ../scst.patch # cd linux-3.10.27 # patch -Np1 -i ../scst.patch</span></span></code> </pre><br>  Now let's build the iscsi-scstd daemon: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd scst-svn/iscsi-scst/usr # make</span></span></code> </pre><br>  The resulting <b>iscsi-scstd</b> will need to be put on the server, for example in <b>/ opt / scst</b> <br><br>  Next, configure the kernel for your server. <br>  Enable encryption (if needed). <br><br>  Do not forget to enable these options for SCST and DRBD: <br><pre> <code class="bash hljs">CONFIG_CONNECTOR=y CONFIG_SCST=y CONFIG_SCST_DISK=y CONFIG_SCST_VDISK=y CONFIG_SCST_ISCSI=y CONFIG_SCST_LOCAL=y</code> </pre><br>  Putting it in the form of a .deb package (for this you need to install the fakeroot, kernel-package and at the same time debhelper packages): <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># fakeroot make-kpkg clean prepare # fakeroot make-kpkg --us --uc --stem=kernel-scst --revision=1 kernel_image</span></span></code> </pre><br>  At the output we get the package <b>kernel-scst-image-3.10.27_1_amd64.deb</b> <br><br>  Next, build the package for DRBD: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># wget http://oss.linbit.com/drbd/8.4/drbd-8.4.4.tar.gz # tar xzf drbd-8.4.4.tar.gz # cd drbd-8.4.4 # dh_make --native --single    Enter</span></span></code> </pre><br>  We change the debian / rules file to the following state (there is a standard file, but it does not collect kernel modules): <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/make -f #     export KDIR="/usr/src/linux-3.10.27" override_dh_installdocs: &lt;  ,     &gt; override_dh_installchangelogs: &lt;  &gt; override_dh_auto_configure: ./configure \ --prefix=/usr \ --localstatedir=/var \ --sysconfdir=/etc \ --with-pacemaker \ --with-utils \ --with-km \ --with-udev \ --with-distro=debian \ --without-xen \ --without-heartbeat \ --without-legacy_utils \ --without-rgmanager \ --without-bashcompletion %: dh $@</span></span></code> </pre><br>  In the Makefile.in file, we will correct the SUBDIRS variable, remove <b>documentation</b> from it, otherwise the package will not pack up with the abuse on the documentation. <br><br>  We collect: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dpkg-buildpackage -us -uc -b</span></span></code> </pre><br>  <b>Get the drbd_8.4.4_amd64.deb</b> package <br><br>  Everything, it is necessary to collect more like nothing, copy both packages to the servers and install: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dpkg -i kernel-scst-image-3.10.27_1_amd64.deb # dpkg -i drbd_8.4.4_amd64.deb</span></span></code> </pre><br><br><h5>  Server setup </h5><br><h6>  Network </h6><br>  My interfaces were renamed to <b>/etc/udev/rules.d/70-persistent-net.rules</b> as follows: <br>  <b>int1-6</b> go to the switch, and <b>drbd1-4 go</b> to the neighboring server. <br><br>  <b>/ etc / network / interfaces</b> has an extremely frightening look, which even in a nightmare will not dream: <br><pre> <code class="bash hljs">auto lo iface lo inet loopback <span class="hljs-comment"><span class="hljs-comment"># Interfaces auto int1 iface int1 inet manual up ip link set int1 mtu 9000 up down ip link set int1 down auto int2 iface int2 inet manual up ip link set int2 mtu 9000 up down ip link set int2 down auto int3 iface int3 inet manual up ip link set int3 mtu 9000 up down ip link set int3 down auto int4 iface int4 inet manual up ip link set int4 mtu 9000 up down ip link set int4 down auto int5 iface int5 inet manual up ip link set int5 mtu 9000 up down ip link set int5 down auto int6 iface int6 inet manual up ip link set int6 mtu 9000 up down ip link set int6 down # Management interface auto int1.2 iface int1.2 inet manual up ip link set int1.2 mtu 1500 up down ip link set int1.2 down vlan_raw_device int1 auto int2.2 iface int2.2 inet manual up ip link set int2.2 mtu 1500 up down ip link set int2.2 down vlan_raw_device int2 auto int3.2 iface int3.2 inet manual up ip link set int3.2 mtu 1500 up down ip link set int3.2 down vlan_raw_device int3 auto int4.2 iface int4.2 inet manual up ip link set int4.2 mtu 1500 up down ip link set int4.2 down vlan_raw_device int4 auto int5.2 iface int5.2 inet manual up ip link set int5.2 mtu 1500 up down ip link set int5.2 down vlan_raw_device int5 auto int6.2 iface int6.2 inet manual up ip link set int6.2 mtu 1500 up down ip link set int6.2 down vlan_raw_device int6 auto bond_vlan2 iface bond_vlan2 inet static address 10.1.0.100 netmask 255.255.255.0 gateway 10.1.0.1 slaves int1.2 int2.2 int3.2 int4.2 int5.2 int6.2 bond-mode active-backup bond-primary int1.2 bond-miimon 100 bond-downdelay 200 bond-updelay 200 mtu 1500 # iSCSI auto int1.24 iface int1.24 inet manual up ip link set int1.24 mtu 9000 up down ip link set int1.24 down vlan_raw_device int1 auto int2.25 iface int2.25 inet manual up ip link set int2.25 mtu 9000 up down ip link set int2.25 down vlan_raw_device int2 auto int3.26 iface int3.26 inet manual up ip link set int3.26 mtu 9000 up down ip link set int3.26 down vlan_raw_device int3 auto int4.27 iface int4.27 inet manual up ip link set int4.27 mtu 9000 up down ip link set int4.27 down vlan_raw_device int4 auto int5.28 iface int5.28 inet manual up ip link set int5.28 mtu 9000 up down ip link set int5.28 down vlan_raw_device int5 auto int6.29 iface int6.29 inet manual up ip link set int6.29 mtu 9000 up down ip link set int6.29 down vlan_raw_device int6 # DRBD bonding auto bond_drbd iface bond_drbd inet static address 192.168.123.100 netmask 255.255.255.0 slaves drbd1 drbd2 drbd3 drbd4 bond-mode balance-rr mtu 9216</span></span></code> </pre><br>  Since we want to have fault tolerance and server management, we use the military trick: in <b>active-backup</b> bonding, we collect not the interfaces themselves, but VLAN-subinterfaces.  Thus, the server will be available as long as at least one interface is running.  This is redundant, but the puruet would not be pas.  And at the same time, the same interfaces can be freely used for iSCSI traffic. <br><br>  For replication, the <b>bond_drbd</b> interface was <b>created</b> in <b>balance-rr</b> mode, in which packets are sent stupidly sequentially across all interfaces.  He was assigned an address from the gray network / 24, but one could get by and / 30 or / 31 because  there will be only two hosts. <br><br>  Since this sometimes leads to the arrival of packets out of turn, we increase the buffer of extraordinary packets in <b>/etc/sysctl.conf</b> .  Below I will give the whole file, which options I will not explain, for a very long time.  You can independently read if desired. <br><pre> <code class="bash hljs">net.ipv4.tcp_reordering = 127 net.core.rmem_max = 33554432 net.core.wmem_max = 33554432 net.core.rmem_default = 16777216 net.core.wmem_default = 16777216 net.ipv4.tcp_rmem = 131072 524288 33554432 net.ipv4.tcp_wmem = 131072 524288 33554432 net.ipv4.tcp_no_metrics_save = 1 net.ipv4.tcp_window_scaling = 1 net.ipv4.tcp_timestamps = 0 net.ipv4.tcp_sack = 0 net.ipv4.tcp_dsack = 0 net.ipv4.tcp_fin_timeout = 15 net.core.netdev_max_backlog = 300000 vm.min_free_kbytes = 720896</code> </pre><br>  According to the test results, the replication interface produces somewhere <b>3.7 Gbit / s</b> , which is quite acceptable. <br><br>  Since our server is multi-core, and network cards and a RAID controller are able to separate interrupt handling across several queues, a script was written that binds the interrupts to the cores: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/perl -w use strict; use warnings; my $irq = 77; my $ifs = 11; my $queues = 6; my $skip = 1; my @tmpl = ("0", "0", "0", "0", "0", "0"); print "Applying IRQ affinity...\n"; for(my $if = 0; $if &lt; $ifs; $if++) { for(my $q = 0; $q &lt; $queues; $q++, $irq++) { my @tmp = @tmpl; $tmp[$q] = 1; my $mask = join("", @tmp); my $hexmask = bin2hex($mask); #print $irq . " -&gt; " . $hexmask . "\n"; open(OUT, "&gt;/proc/irq/".$irq."/smp_affinity"); print OUT $hexmask."\n"; close(OUT); } $irq += $skip; } sub bin2hex { my ($bin) = @_; return sprintf('%x', oct("0b".scalar(reverse($bin)))); }</span></span></code> </pre><br><h6>  Discs </h6><br>  Before exporting the disks, we encrypt them and save the master keys for every fireman: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cryptsetup luksFormat --cipher aes-cbc-essiv:sha256 --hash sha256 /dev/sdb # cryptsetup luksFormat --cipher aes-cbc-essiv:sha256 --hash sha256 /dev/sdc # cryptsetup luksHeaderBackup /dev/sdb --header-backup-file /root/header_sdb.bin # cryptsetup luksHeaderBackup /dev/sdc --header-backup-file /root/header_sdc.bin</span></span></code> </pre><br>  The password must be written on the inside of the skull and never forget, and hide the key backups to far. <br>  It should be borne in mind that after changing the password on the sections, the backup of the master key can be decrypted with the old password. <br><br>  Next, a script was written to simplify decryption: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/perl -w use strict; use warnings; use IO::Prompt; my %crypto_map = ( '1bd1f798-d105-4150-841b-f2751f419efc' =&gt; 'VM_STORAGE_1', 'd7fcdb2b-88fd-4d19-89f3-5bdf3ddcc456' =&gt; 'VM_STORAGE_2' ); my $i = 0; my $passwd = prompt('Password: ', '-echo' =&gt; '*'); foreach my $dev (sort(keys(%crypto_map))) { $i++; if(-e '/dev/mapper/'.$crypto_map{$dev}) { print "Mapping '".$crypto_map{$dev}."' already exists, skipping\n"; next; } my $ret = system('echo "'.$passwd.'" | /usr/sbin/cryptsetup luksOpen /dev/disk/by-uuid/'.$dev.' '.$crypto_map{$dev}); if($ret == 0) { print $i . ' Crypto mapping '.$dev.' =&gt; '.$crypto_map{$dev}.' added successfully' . "\n"; } else { print $i . ' Failed to add mapping '.$dev.' =&gt; '.$crypto_map{$dev} . "\n"; exit(1); } }</span></span></code> </pre><br>  The script works with the UUIDs of the disks in order to always uniquely identify the disk in the system without reference to <b>/ dev / sd *</b> . <br><br>  The encryption speed depends on the processor frequency and the number of cores, and the recording is parallelized better than the reading.  You can check the speed with which the server encrypts using the following simple method: <br><pre> <code class="bash hljs">  ,       ,     <span class="hljs-comment"><span class="hljs-comment"># echo "0 268435456 zero" | dmsetup create zerodisk        # cryptsetup --cipher aes-cbc-essiv:sha256 --hash sha256 create zerocrypt /dev/mapper/zerodisk Enter passphrase: &lt; &gt;  : # dd if=/dev/zero of=/dev/mapper/zerocrypt bs=1M count=16384 16384+0 records in 16384+0 records out 17179869184 bytes (17 GB) copied, 38.3414 s, 448 MB/s # dd of=/dev/null if=/dev/mapper/zerocrypt bs=1M count=16384 16384+0 records in 16384+0 records out 17179869184 bytes (17 GB) copied, 74.5436 s, 230 MB/s</span></span></code> </pre><br>  As you can see, the speeds are not so hot, but they will rarely be achieved in practice, because  the random nature of access usually prevails. <br><br>  For comparison, the results of the same test on the new <b>Xeon E3-1270 v3</b> on the <b>Haswell</b> core: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/zero of=/dev/mapper/zerocrypt bs=1M count=16384 16384+0 records in 16384+0 records out 17179869184 bytes (17 GB) copied, 11.183 s, 1.5 GB/s # dd of=/dev/null if=/dev/mapper/zerocrypt bs=1M count=16384 16384+0 records in 16384+0 records out 17179869184 bytes (17 GB) copied, 19.4902 s, 881 MB/s</span></span></code> </pre><br>  Here, here it is much more fun.  Frequency is the decisive factor, apparently. <br>  And if you deactivate AES-NI, it will be several times slower. <br><br><h6>  DRBD </h6><br>  Configuring replication, configs from both ends should be 100% identical. <br><br>  <b>/etc/drbd.d/global_common.conf</b> <br><pre> <code class="bash hljs">global { usage-count no; } common { protocol B; handlers { } startup { wfc-timeout 10; } disk { c-plan-ahead 0; al-extents 6433; resync-rate 400M; disk-barrier no; disk-flushes no; disk-drain yes; } net { sndbuf-size 1024k; rcvbuf-size 1024k; max-buffers 8192; <span class="hljs-comment"><span class="hljs-comment"># x PAGE_SIZE max-epoch-size 8192; # x PAGE_SIZE unplug-watermark 8192; timeout 100; ping-int 15; ping-timeout 60; # x 0.1sec connect-int 15; timeout 50; # x 0.1sec verify-alg sha1; csums-alg sha1; data-integrity-alg crc32c; cram-hmac-alg sha1; shared-secret "ultrasuperdupermegatopsecretpassword"; use-rle; } }</span></span></code> </pre><br>  Here the most interesting parameter is the protocol, let's compare them. <br><br>  Recording is considered successful if the block was recorded ... <br><ul><li>  <b>A</b> - to local disk and hit the local send buffer </li><li>  <b>B</b> - to the local disk and hit the remote receive buffer </li><li>  <b>C</b> - to local and remote disk </li></ul><br>  The slowest (read - high latency) and, at the same time, reliable is <b>C</b> , and I chose a middle ground. <br><br>  Next comes the definition of the resources with which the DRBD and the nodes involved in their replication operate. <br><br>  <b>/etc/drbd.d/VM_STORAGE_1.res</b> <br><pre> <code class="bash hljs">resource VM_STORAGE_1 { device /dev/drbd0; disk /dev/mapper/VM_STORAGE_1; meta-disk internal; on vmstor1 { address 192.168.123.100:7801; } on vmstor2 { address 192.168.123.200:7801; } }</code> </pre><br>  <b>/etc/drbd.d/VM_STORAGE_2.res</b> <br><pre> <code class="bash hljs">resource VM_STORAGE_2 { device /dev/drbd1; disk /dev/mapper/VM_STORAGE_2; meta-disk internal; on vmstor1 { address 192.168.123.100:7802; } on vmstor2 { address 192.168.123.200:7802; } }</code> </pre><br>  Each resource has its own port. <br><br>  Now we initialize the metadata of the DRBD resources and activate them; this needs to be done on each server: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm create-md VM_STORAGE_1 # drbdadm create-md VM_STORAGE_2 # drbdadm up VM_STORAGE_1 # drbdadm up VM_STORAGE_2</span></span></code> </pre><br>  Next, you need to select a single server (you can have your own for each resource) and determine that it is the main one and the primary synchronization will go to another one: <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># drbdadm primary --force VM_STORAGE_1 # drbdadm primary --force VM_STORAGE_2</span></span></code> </pre><br>  Everything went, went, synchronization began. <br>  Depending on the size of the arrays and the speed of the network, it will take a long or very long time. <br><br>  You can watch the progress with the watch command <b>-n0.1 cat / proc / drbd</b> , it pacifies and tunes philosophically. <br>  In principle, the devices can already be used in the synchronization process, but I advise you to relax :) <br><br><h4>  The end of the first part </h4><br>  For one part, I think that's enough.  And so much information to absorb. <br><br>  In the second part I will talk about setting up the cluster manager and ESXi hosts to work with this product. </div><p>Source: <a href="https://habr.com/ru/post/209460/">https://habr.com/ru/post/209460/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../209446/index.html">We study the internal kitchen of the Linux kernel using / proc for quick diagnosis and problem solving</a></li>
<li><a href="../209450/index.html">The future of IT within companies: less, better, cheaper</a></li>
<li><a href="../209452/index.html">US authorities seized another 29655 bitcoins ($ 28 million) from Silk Road servers</a></li>
<li><a href="../209454/index.html">Laboratories for penetration testing "Test.lab"</a></li>
<li><a href="../209458/index.html">Google showed the dynamics of the popularity of music trends over the past 64 years</a></li>
<li><a href="../209462/index.html">CSS animation tricks: instant changes, negative delays, transform-origin animation and more</a></li>
<li><a href="../209464/index.html">Objective-D - an alternative or addition to Objective-C</a></li>
<li><a href="../209466/index.html">Technological startup: what to do if money is no longer a problem?</a></li>
<li><a href="../209470/index.html">Raspberry PI and JAVA: a closer look</a></li>
<li><a href="../209472/index.html">The movement of attention based on the continuously accumulated experience of perception, as the basis of the proposed approach to the design of a strong AI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>