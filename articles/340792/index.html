<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A short course of machine learning or how to create a neural network to solve the scoring problem</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We often hear such verbal constructions as ‚Äúmachine learning‚Äù, ‚Äúneural networks‚Äù. These expressions are already tightly embedded in the public conscio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A short course of machine learning or how to create a neural network to solve the scoring problem</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/59/ed/b2/59edb292efcac960883973.jpeg" alt="image"><br><br>  <i>We often hear such verbal constructions as ‚Äúmachine learning‚Äù, ‚Äúneural networks‚Äù.</i>  <i>These expressions are already tightly embedded in the public consciousness and are most often associated with pattern and speech recognition, with the generation of human-like text.</i>  <i>In fact, machine learning algorithms can solve many different types of problems, including helping small businesses, online publishing, and anything.</i>  <i>In this article I will explain how to create a neural network that is able to solve the real business problem of creating a scoring model.</i>  <i>We will consider all the stages: from data preparation to creating a model and assessing its quality.</i> <br><br>  Questions that are discussed in the article: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ‚Ä¢ How to collect and prepare data for building a model? <br>  ‚Ä¢ What is a neural network and how is it arranged? <br>  ‚Ä¢ How to write your neural network from scratch? <br>  ‚Ä¢ How to properly train a neural network based on available data? <br>  ‚Ä¢ How to interpret the model and its results? <br>  ‚Ä¢ How to correctly assess the quality of the model? <br><a name="habracut"></a><cut></cut><br><blockquote>  "The question of whether a computer can think is no more interesting, <br>  than the question of whether a submarine can swim. " <br>  Edsger Vibe Dijkstra </blockquote><br>  In many companies, sales managers communicate with potential customers, give them demonstrations, talk about the product.  They give, so to speak, their soul for the hundredth time to be torn apart by those who may have fallen into their hands completely by accident.  Often, customers do not understand enough what they need, or what the product can give them.  Communicating with such clients brings neither pleasure nor profit.  And the most unpleasant thing is that due to time constraints, you can not pay enough attention to a really important client and miss the deal. <br><br>  I am a software mathematician in the Serpstat seo analytics service.  Recently, I received an interesting task to improve the scoring model that already exists and works with us, re-evaluating the factors that influence the success of the sale.  Scoring was considered based on the survey of our clients, and each item, depending on the answer to the question, contributed a certain number of points to the total score.  All these points for different questions were placed on the basis of statistical hypotheses.  The scoring model was used, as time went on, the data was collected and one day came to me.  Now that I had a sufficient sample, I could safely build hypotheses using machine learning algorithms. <br><br>  I will tell you how we built our scoring model.  This is a real case with real data, with all the difficulties and limitations that we encountered in a real business.  So, first things first. <br><br>  We will dwell on all stages of work: <br><br>  ‚ñ∏ Data collection <br>  ‚ñ∏ Preprocessing <br>  ‚ñ∏ Building a model <br>  ‚ñ∏ Quality analysis and model interpretation <br><br>  Consider the design, creation and training of a neural network.  I describe all this, solving a real scoring problem, and constantly reinforce a new theory with an example. <br><br><h2>  Data collection </h2><br>  First you need to understand what questions will represent the client (or just an object) in the future model.  We approach the task seriously, since on its basis a further process is built.  First, it is necessary not to miss the important signs that describe the object, and second, to create stringent criteria for deciding on a sign.  Based on experience, I can distinguish three categories of questions: <br><br><ol><li>  Booleans (bicategorial), the answer to which is: Yes or No (1 or 0).  For example, the answer to the question: does the client have an account? </li><li>  Categorical, the answer to which is a specific class.  Usually there are more than two classes (multi-categorical), otherwise the question can be reduced to a Boolean one.  For example, color: red, green or blue. </li><li>  Quantitative, the answers to which are the numbers characterizing a specific measure.  For example, the number of hits per month: fifteen. </li></ol><br>  Why do I dwell on it in such detail?  Usually, when considering a classical problem solved by machine learning algorithms, we deal only with numerical data.  For example, the recognition of black and white handwritten numbers from the image of 20 by 20 pixels.  In this example, 400 numbers (describing the brightness of a black and white pixel) represent one example from a sample.  In general, the data need not be numeric.  The fact is that when building a model, you need to understand what types of questions the algorithm can deal with.  For example: a decision tree is trained on all types of questions, and a neural network accepts only numerical input data and is trained only on quantitative attributes.  Does this mean that we have to give up some questions for the sake of a better model?  Not at all, just need to properly prepare the data. <br><br>  The data should have the following classical structure: feature vector for each i-th client X <sup>(i)</sup> = {x <sup>(i)</sup> <sub>1</sub> , x <sup>(i)</sup> <sub>2</sub> , ..., x <sup>(i)</sup> <sub>n</sub> } and the class Y <sup>(i)</sup> - category showing whether he bought or not.  For example: client <sup>(3)</sup> = {green, bitter, 4.14, yes} - bought. <br><br>  Based on the above, we will try to provide a data format with types of questions for further preparation: <br><table><tbody><tr><th>  class: <br>  (category) <br></th><th>  Colour: <br>  (category) <br></th><th>  taste: <br>  (category) <br></th><th>  weight: <br>  (number) <br></th><th>  solid: <br>  (bool) <br></th></tr><tr><td>  - </td><td>  red </td><td>  sour </td><td>  4.23 </td><td>  Yes </td></tr><tr><td>  - </td><td>  green </td><td>  bitter </td><td>  3.15 </td><td>  not </td></tr><tr><td>  + </td><td>  green </td><td>  bitter </td><td>  4.14 </td><td>  Yes </td></tr><tr><td>  + </td><td>  blue </td><td>  sweet </td><td>  4.38 </td><td>  not </td></tr><tr><td>  - </td><td>  green </td><td>  salty </td><td>  3.62 </td><td>  not </td></tr></tbody></table><br>  <i>Table 1 - Example of training sample data before preprocessing</i> <br><br><h2>  Preprocessing </h2><br>  Once the data is collected, they need to be prepared.  This stage is called preprocessing.  The main task of preprocessing is to display data in a format suitable for learning the model.  There are three main manipulations with data at the preprocessing stage: <br><br><ol><li>  Creating a vector feature space where the examples of the training sample will live.  In essence, this is the process of bringing all the data into numerical form.  This saves us from categorical, Boolean and other non-numeric types. </li><li>  <a href="https://en.wikipedia.org/wiki/Normalization_(statistics)">Data normalization.</a>  The process by which we achieve, for example, that the average value of each attribute for all data is zero, and the variance is single.  Here is the most classic example of data normalization: X = (X - Œº) / œÉ <br><div class="spoiler">  <b class="spoiler_title">normalization function</b> <div class="spoiler_text"><pre><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">normalize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(X)</span></span></span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (XX.mean())/X.std()</code> </pre> </div></div></li><li>  Changing the dimension of the vector space.  If the vector space of features is too large (millions of features) or small (less than ten), then you can apply methods to increase or decrease the dimension of space: <br><ul><li>  To increase the dimension, you can use part of the training sample as <a href="https://en.wikipedia.org/wiki/Kernel_method">reference points</a> , adding the distance to these points in the feature vector.  This method often leads to the fact that in spaces of higher dimensionality the sets become linearly separable, and this simplifies the task of classification. </li><li>  To reduce the dimension most often use PCA.  The main objective of <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">the principal component method</a> is the search for new linear combinations of features along which the dispersion of the values ‚Äã‚Äãof the projections of the elements of the training sample is maximized. </li></ul></li></ol><br>  One of the most important tricks in constructing a vector space is the method of representation as a number of categorical and boolean types.  Meet: <a href="https://en.wikipedia.org/wiki/One-hot">One-Hot</a> (Rus. Unitary Code).  The main idea of ‚Äã‚Äãsuch a coding is to represent a categorical feature as a vector in a vector space with a dimension corresponding to the number of possible categories.  The value of the coordinates of this category is taken as one, and all other coordinates are reset.  With boolean values, everything is quite simple, they turn into real units or zeros. <br><br>  For example, the sampling element can be either bitter, or sweet, or salty, or sour, or umami (meat).  Then the One-Hot encoding will be the following: bitter = (1, 0, 0, 0, 0), sweet = (0, 1, 0, 0, 0), salty = (0, 0, 1, 0, 0), acid = (0, 0, 0, 1, 0), umami = (0, 0, 0, 0, 1).  If you have a question why there are five tastes, not four, then read this <a href="https://en.wikipedia.org/wiki/Taste">article about the taste sensory system</a> , well, this has nothing to do with scoring, and we will use four, confining ourselves to the old classification. <br><br>  Now we have learned to turn categorical signs into ordinary numerical vectors, and this is very useful.  After all the manipulations on the data, we get a training sample that is suitable for any model.  In our case, after applying the unitary encoding and normalization, the data looks like this: <br><table><tbody><tr><th>  class: </th><th>  red: </th><th>  green: </th><th>  blue: </th><th>  bitter: </th><th>  sweet: </th><th>  salti: </th><th>  sour: </th><th>  weight: </th><th>  solid: </th></tr><tr><td>  0 </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  one </td><td>  0.23 </td><td>  one </td></tr><tr><td>  0 </td><td>  0 </td><td>  one </td><td>  0 </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  -0.85 </td><td>  0 </td></tr><tr><td>  one </td><td>  0 </td><td>  one </td><td>  0 </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  0.14 </td><td>  one </td></tr><tr><td>  one </td><td>  0 </td><td>  0 </td><td>  one </td><td>  0 </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0.38 </td><td>  0 </td></tr><tr><td>  0 </td><td>  0 </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  one </td><td>  0 </td><td>  -0.48 </td><td>  0 </td></tr></tbody></table><br>  <i>Table 2 - Example of training sample data after preprocessing</i> <br><br>  It can be said that preprocessing is the process of mapping the data that we understand to a less convenient form for a person, but to a favorite form. <br><br>  The scoring formula most often represents the following linear model: <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d21d02e616211003.png" alt="image"><br><br>  Where, k is the question number in the questionnaire, w <sub>k</sub> is the coefficient of the contribution of the answer to this k-th question to the total scoring, | w |  - the number of questions (or coefficients), x <sub>k</sub> - the answer to this question.  At the same time, questions can be any, as we have discussed: boolean (yes or no, 1 or 0), numerical (for example, height = 175) or categorical, but presented in the form of a unitary encoding (green from the list: red, green or blue = [0, 1, 0]).  At the same time, we can assume that categorical questions break down into as many Boolean ones as there are categories in the response options (for example: client red? Client green? Client blue?). <br><br><h2>  Model selection </h2><br>  Now the most important: the choice of the model.  Today, there are many machine learning algorithms on the basis of which you can build a scoring model: Decision Tree (decision tree), KNN (k-nearest neighbors method), SVM (support vector method), NN (neural network).  And the choice of model should be based on what we want from it.  First, how much the decisions that influenced the model results should be understandable.  In other words, how important it is for us to be able to interpret the structure of the model. <br><br><img src="https://habrastorage.org/webt/59/ee/de/59eedec7e5d46365793835.png" alt="image"><br>  <i>Fig.</i>  <i>1 - Dependence of machine learning algorithm flexibility and interpretability of the resulting model</i> <br><br>  In addition, not all models are easy to build, some require very specific skills and very, very powerful hardware.  But the most important thing is the implementation of the constructed model.  It happens that the business process is already established, and the introduction of a complex model is simply impossible.  Or a linear model is required, in which customers, when answering questions, receive positive or negative points depending on the answer option.  Sometimes, on the contrary, there is the possibility of implementation, and even requires a complex model that takes into account very unobvious combinations of input parameters, which finds the relationship between them.  So, what to choose? <br><br>  In choosing a machine learning algorithm, we stopped at a neural network.  Why?  First, there are now many cool frameworks, such as TensorFlow, Theano.  They make it possible to very deeply and seriously customize the architecture and parameters of training.  Secondly, the ability to change the device model from a single-layer neural network, which, by the way, is well interpreted, to a multi-layer one, which has an excellent ability to find non-linear dependencies, changing only a couple of lines of code.  In addition, a trained single-layer neural network can be turned into a classic additive scoring model that adds points for answers to various survey questions, but more on that later. <br><br>  Now a little theory.  If for you such things as neuron, activation function, loss function, gradient descent and the method of back propagation of error are native words, then you can safely skip it all.  If not, welcome to the short course of artificial neural networks. <br><br><h1>  Short course of artificial neural networks </h1><br>  To begin with, artificial neural networks (ANN) are mathematical models of the organization of real biological neural networks (BNS).  But unlike mathematical models of BNS, the ANN does not require an exact description of all chemical and physical processes, such as the description of the ‚Äúignition‚Äù of the action potential (AP), the work of neurotransmitters, ion channels, secondary mediators, transporter proteins, and others. the work of real BNS only at a functional, not at the physical level. <br><br>  The basic element of a neural network is a neuron.  Let's try to make the simplest functional mathematical model of a neuron.  To do this, we describe in general terms the functioning of a biological neuron. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cf17969289986661.png" alt="image"><br>  <i>Fig.</i>  <i>2 - Typical structure of a biological neuron</i> <br><br>  As we can see, the structure of a biological neuron can be simplified to the following: dendrites, the body of the neuron and axon.  Dendrites are branching processes that collect information from the entrance to a neuron (this may be external information from receptors, for example, from a cone in the case of color or internal information from another neuron).  In that case, if the incoming information activated a neuron (in the biological case, the potential became higher than some threshold), an excitation wave (PD) is generated, which propagates through the neuron's body membrane, and then through the axon, by emitting a neurotransmitter, transmits a signal to other nerve cells. cells or tissues. <br><br>  Based on this, Warren McCulloch and Walter Pitts in 1943 proposed a model of a mathematical neuron.  And in 1958, Frank Rosenblatt, based on the McCulloch-Pitts neuron, created a computer program, and then a physical device, the perceptron.  This is how the history of artificial neural networks began.  Now consider the structural model of the neuron, with which we will deal further. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cf901a6675451364.png" alt="image"><br>  <i>Fig.</i>  <i>3 - The model of the mathematical neuron McCullock-Pitts</i> <br><br>  Where: <br><br><ol><li>  X is the input vector of parameters.  Vector (column) of numbers (biol. Degree of activation of different receptors), which came to the input of the neuron. <br>  W is the weights vector (in the general case, the weights matrix), numerical values ‚Äã‚Äãthat change in the learning process (biol. Training based on synaptic plasticity, the neuron learns to respond correctly to signals from its receptors). </li><li>  Adder is a functional block of a neuron that adds all input parameters multiplied by their respective weights. </li><li>  The activation function of the neuron is the dependence of the value of the output of the neuron on the value coming from the adder. </li><li>  The following neurons, where the value from the output of the given neuron is fed to one of the set of their own inputs (this layer may be absent if this neuron is the last, terminal). </li></ol><br><div class="spoiler">  <b class="spoiler_title">math neuron implementation</b> <div class="spoiler_text"><pre> <code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np def neuron(<span class="hljs-title"><span class="hljs-title">x</span></span>, <span class="hljs-title"><span class="hljs-title">w</span></span>): z = np.dot(<span class="hljs-title"><span class="hljs-title">w</span></span>, <span class="hljs-title"><span class="hljs-title">x</span></span>) output = activation(<span class="hljs-title"><span class="hljs-title">z</span></span>) return output</code> </pre> </div></div><br>  Then classical artificial neural networks are assembled from these minimal structural units.  The following terminology has been adopted: <br><br><ul><li>  The input (receptor) layer is a vector of parameters (features).  This layer does not consist of neurons.  We can say that this is digital information, taken by receptors from the "external" world.  In our case, this is customer information.  The layer contains as many elements as the input parameters (plus the bias-term needed to shift the activation threshold). </li><li>  An associative (hidden) layer is a deep structure capable of memorizing examples, finding complex correlations and non-linear dependencies, and building abstractions and generalizations.  In general, this is not even a layer, but a multitude of layers between input and output.  It can be said that each layer prepares a new (higher-level) feature vector for the next layer.  This layer is responsible for the appearance in the learning process of high-level abstractions.  The structure contains as many neurons and layers as you please, and may even be absent (in the case of the classification of linearly separable sets). </li><li>  The output layer is a layer, each neuron of which is responsible for a specific class.  The output of this layer can be interpreted as a function of the probability distribution of the belonging of an object to different classes.  A layer contains as many neurons as there are classes in the training set.  If there are two classes, then you can use two output neurons or limit it to just one.  In this case, one neuron is still responsible only for one class, but if it gives values ‚Äã‚Äãclose to zero, then the element of the sample according to its logic should belong to another class. </li></ul><br><img src="https://habrastorage.org/webt/59/ed/fc/59edfc8497712414118124.jpeg" alt="image"><br>  <i>Fig.</i>  <i>4 - Classical neural network topology, with input (receptor), output, class-making, and associative (hidden) layer</i> <br><br>  Due to the presence of hidden associative layers, an artificial neural network is able to build hypotheses based on finding complex dependencies.  For example, for convolutional neural networks that recognize images, the brightness values ‚Äã‚Äãof the pixels of the image will be fed to the input layer, and the output layer will contain neurons responsible for specific classes (human, machine, tree, house, etc.). ‚ÄúReceptors‚Äù of hidden layers will start ‚Äúby themselves‚Äù to appear (specialize) neurons, excited from straight lines, different inclination, then reacting to angles, squares, circles, primitive patterns: alternating stripes, geometric mesh patterns  nty  Closer to the output layers are neurons that react, for example, to the eye, the wheel, the nose, the wing, the leaf, the face, etc. <br><br><img src="https://habrastorage.org/webt/59/ea/02/59ea02bbcad6a843046464.png" alt="image"><br>  <i>Fig.</i>  <i>5 - The formation of hierarchical associations in the process of learning convolutional neural network</i> <br><br>  Conducting a biological analogy, I would like to refer to the words of the remarkable neurophysiologist Vyacheslav Albertovich Dubynin concerning the speech model: <br><blockquote>  ‚ÄúOur brain is able to create, generate words that summarize the words of a lower level.  Say, bunny, ball, cubes, doll - toys;  toys, clothes, furniture are objects;  and objects, houses, people are objects of the environment.  And so a little more, and we get to abstract philosophical concepts, mathematical, physical.  That is, speech generalization is a very important property of our associative parietal cortex, and it, in addition, is multi-layered and allows the speech model of the external world to form as integrity.  At some point, it turns out that nerve impulses are able to move very actively along this speech model, and we call this movement the proud word ‚Äúthinking‚Äù. </blockquote><br>  Lots of theory ?!  But there is good news: in the simplest case, the entire neural network can be represented by a single neuron!  Moreover, even one neuron often copes well with a task, especially when it comes to recognizing the class of an object in space in which the objects of these classes are linearly separable.  Often, linear separability can be achieved by increasing the dimension of space, as described above, and be limited to just one neuron.  But sometimes it is easier to add a couple of hidden layers to a neural network and not require a linear separability from the sample. <br><br><img src="https://habrastorage.org/webt/59/ed/fc/59edfc8487ba4200424745.jpeg" alt="image"><br>  <i>Fig.</i>  <i>6 - Linearly separable sets and linearly non-separable sets</i> <br><br>  Well, now let's describe all this formally.  At the input of the neuron we have a vector of parameters.  In our case, these are the results of the customer survey presented in the numerical form X <sup>(i)</sup> = {x <sup>(i)</sup> <sub>1</sub> , x <sup>(i)</sup> <sub>2</sub> , ..., x <sup>(i)</sup> <sub>n</sub> }.  In addition, each client is assigned a Y <sup>(i)</sup> class, which characterizes the success of the lead (1 or 0).  The neural network, in fact, must find the optimal separating hypersurface in the vector space, the dimension of which corresponds to the number of features.  Learning a neural network in this case is finding such values ‚Äã‚Äã(coefficients) of the matrix of weights W for which the neuron responsible for the class will produce values ‚Äã‚Äãclose to one in those cases if the customer buys and values ‚Äã‚Äãclose to zero if not. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d209069206186309.png" alt="image"><br><br>  As can be seen from the formula, the result of the neuron's work is the activation function (often denoted by) of the sum of the product of the input parameters and the coefficients sought in the learning process.  Let us understand what is the activation function. <br><br>  Since any real values ‚Äã‚Äãcan be input to a neural network, and the coefficients of the weights matrix can also be anything, and the result of the sum of their products can be any real number from minus to plus infinity.  Each element of the training set has a class value relative to this neuron (zero or one).  It is desirable to obtain a value from the neuron in the same range from zero to one, and decide on the class, depending on what this value is closer to.  It is even better to interpret this value as the probability that an element belongs to this class.  So, we need such a monotone smooth function that will display elements from the set of real numbers in the range from zero to one.  The so-called sigmoid is great for this position. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d047a90016245563.png" alt="image"><br>  <i>Fig.</i>  <i>7 - Logistic curve graph, one of the most classical representatives of the class sigmoid</i> <br><br><div class="spoiler">  <b class="spoiler_title">activation function</b> <div class="spoiler_text"><pre> <code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">activation</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(z)</span></span></span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-z))</code> </pre> </div></div><br>  By the way, in real biological neurons such a continuous activation function was not realized.  In our cells with you there is a potential of rest, which averages -70mV.  If information is sent to the neuron, then the activated receptor opens ion channels connected with it, which leads to an increase or decrease in potential in the cell.  It is possible to draw an analogy between the strength of the reaction to the activation of the receptor and that obtained in the learning process with a single coefficient of the weight matrix.  As soon as the potential reaches a value of -50mV, PD occurs, and the excitation wave travels along an axon to the presynaptic ending, throwing the neurotransmitter into the inter-synaptic medium.  That is, the real biological activation is stepwise, not smooth: the neuron is either activated or not.  This shows how mathematically we are free to build our models.  Taking from nature the basic principle of distributed computing and learning, we are able to build a computational graph consisting of elements with any desired properties.  In our example, we want to get continuous, rather than discrete values ‚Äã‚Äãfrom the neuron.  Although in the general case, the activation function may be different. <br><br>  Here is the most important thing to extract from what was written above: <i>‚ÄúNeural network training (synaptic training) should be reduced to an optimal selection of weights matrix coefficients in order to minimize the allowed error.‚Äù</i> In the case of a single-layer neural network, these coefficients can be interpreted as the contribution of element parameters to the probability of belonging to a specific class. <br><br>  The result of the work of a neural network is called a hypothesis (English hypothesis).  Denote by h (X), showing the dependence of the hypothesis on the input characteristics (parameters) of the object.  Why a hypothesis?  So historically.  Personally, I like this term.  As a result, we want the hypotheses of the neural network to correspond to reality as much as possible (real classes of objects).  Actually here the basic idea of ‚Äã‚Äãlearning by experience is born.  Now we need a measure describing the quality of the neural network.  This functionality is usually called the "loss function" (born loss function).  The functional is usually denoted by J (W), showing its dependence on the coefficients of the weights matrix.  The less functionality, the less often our neural network is mistaken and the better it is.  It is to minimize this functionality and learning is reduced.  Depending on the coefficients of the weights matrix, the neural network may have different accuracy.  The learning process is a movement along the loss functional hypersurface, the purpose of which is to minimize this functional. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cb1a711477559952.png" alt="image"><br>  <i>Fig.</i>  <i>8 - The learning process, as a gradient descent to the local minimum of the loss functional</i> <br><br>  Usually the weights matrix coefficients are initialized randomly.  In the process of learning, the coefficients change.  The graph shows two different iterative learning paths as a change in the coefficients w <sub>1</sub> and w <sub>2 of the</sub> matrix of weights of the neural network initialized in the neighborhood. <br><br>    ,   .     ,     :  ()     .    .   ‚Äî     ,      .       ,      .          .         ,          . <br><br><h3>   </h3><br>       :    ‚Äî ,   ‚Äî , ¬´  ¬ª   ‚Äî   (     ,     ).      .       () ,   : <br><br><ol><li>   ( )      -1  1.    ,      ,   .     P ‚Äî population or parents. <br><div class="spoiler"> <b class="spoiler_title">    </b> <div class="spoiler_text"><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random def generate_population(p, w_size): population = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(p): model = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(w_size + <span class="hljs-number"><span class="hljs-number">1</span></span>): # +<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b (bias term) model.append(<span class="hljs-number"><span class="hljs-number">2</span></span> * random.random() - <span class="hljs-number"><span class="hljs-number">1</span></span>) # random initialization <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> b <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> w population.append(model) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.<span class="hljs-keyword"><span class="hljs-keyword">array</span></span>(population)</code> </pre> </div></div></li><li>   . , -   ,    ()   . :     ,             -0.1  0.1. <br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="hljs lua">def mutation(genom, t=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, m=<span class="hljs-number"><span class="hljs-number">0.1</span></span>): mutant = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> gen <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> genom: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-built_in"><span class="hljs-built_in">random</span></span>.<span class="hljs-built_in"><span class="hljs-built_in">random</span></span>() &lt;= t: gen += m*(<span class="hljs-number"><span class="hljs-number">2</span></span>*<span class="hljs-built_in"><span class="hljs-built_in">random</span></span>.<span class="hljs-built_in"><span class="hljs-built_in">random</span></span>() <span class="hljs-number"><span class="hljs-number">-1</span></span>) mutant.append(gen) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> mutant</code> </pre> </div></div></li><li>    ,   ,         (    ‚Äî    ,   ‚Äî   ).     . <br><div class="spoiler"> <b class="spoiler_title">  </b> <div class="spoiler_text"><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-function">def </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">accuracy</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">X, Y, model</span></span></span><span class="hljs-function">): A</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span> m = len(Y) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, <span class="hljs-function"><span class="hljs-function">y </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">in</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">enumerate</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">Y</span></span></span><span class="hljs-function">): A +</span></span>= (<span class="hljs-number"><span class="hljs-number">1</span></span>/m)*(y*(<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">if</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">neuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">X[i], model</span></span></span><span class="hljs-function">) &gt;</span></span>= <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>)+(<span class="hljs-number"><span class="hljs-number">1</span></span>-y)*(<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">if</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">neuron</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">X[i], model</span></span></span><span class="hljs-function">) &gt;</span></span>= <span class="hljs-number"><span class="hljs-number">0.5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> A</code> </pre> </div></div></li><li> ¬´  ¬ª  P  .     2,     . :        80%. <br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">selection</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(offspring, population)</span></span></span></span>: offspring.sort() population = [kid[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> kid <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> offspring[<span class="hljs-symbol"><span class="hljs-symbol">:len</span></span>(population)]] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> population</code> </pre> </div></div></li></ol><br><div class="spoiler"> <b class="spoiler_title">  </b> <div class="spoiler_text"><pre> <code class="hljs pgsql">def evolution(population, X_in, Y, number_of_generations, children): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(number_of_generations): X = [[<span class="hljs-number"><span class="hljs-number">1</span></span>]+[v.tolist()] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X_in] offspring = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> genom <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> population: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(children): child = mutation(genom) child_loss = <span class="hljs-number"><span class="hljs-number">1</span></span> - accuracy(X_in, Y, child) # <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> child_loss = binary_crossentropy(X, Y, child) <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> better offspring.append([child_loss, child]) population = selection(offspring, population) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> population</code> </pre> </div></div><br>    . ,    -,  œÑ ‚Äî    Œº ‚Äî  .            œÑ,         (  -0.1  0.1)   Œº.      . <br><br>  ,           ,      ,         ,   ,          .      .      ,          .           . <br><br>             : <br><blockquote> ¬´  ‚Äî     ,  ‚Äî  .  ,    ,     ‚Äî   .   ,          .   ,       .¬ª </blockquote><br><br><h3>        </h3><br>       ,     .               ,    .    ,  ,        ,   ,    ,     .       ,    . <br><br>   ,            .     ,    ,       ,    ‚Äî .     ,    ,          ,            . ,     ,          ,    . <br><br>    ,      , ,       ,        ,      ,     ,     . <br><br><img src="https://habrastorage.org/webt/59/ee/14/59ee14a9ce201843532518.jpeg" alt="image"><br>  <i>Fig.</i> <i>9 ‚Äî   ,     : 1)         ( ), 2)          ( )</i> <br><br>        .   ,  Y   i-     m     ,  ,           . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d1df361245170450.png" alt="image"><br><br> ,     ,     <a href="https://en.wikipedia.org/wiki/Cross_entropy"> </a> (. cross entropy). C    ,           . <br><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="hljs lua">def binary_crossentropy(X, Y, model): # loss <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">J</span></span></span><span class="hljs-function"> = 0 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">m</span></span></span><span class="hljs-function"> = </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">len</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Y)</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, y <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(Y): J += -(<span class="hljs-number"><span class="hljs-number">1</span></span>/m)*(y*np.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>(neuron(X[i], model))+(<span class="hljs-number"><span class="hljs-number">1.</span></span>-y)*np.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>(<span class="hljs-number"><span class="hljs-number">1.</span></span>-neuron(X[i], model))) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> J</code> </pre> </div></div><br>      ,      ,     ,  - ,  .   ,      ,   ,    .      ‚Äî .   ,      .   ()    ,      () . ,       : <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d1af2dd761028034.png" alt="image"><br><br>     . -,        ,   Œ± (     ),     .  ,   ,    W,   ,        ,     ,     . <br><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-function">def </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gradient_descent</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">model, X_in, Y, number_of_iteratons=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">500</span></span></span></span><span class="hljs-function"><span class="hljs-params">, learning_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span></span><span class="hljs-function">): X</span></span> = [[<span class="hljs-number"><span class="hljs-number">1</span></span>]+[v.tolist()] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> X_in] m = len(Y) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">for</span></span></span><span class="hljs-function"> it </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">in</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">range</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">number_of_iteratons</span></span></span><span class="hljs-function">): new_model</span></span> = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, <span class="hljs-function"><span class="hljs-function">w </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">in</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">enumerate</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">model</span></span></span><span class="hljs-function">): error</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, <span class="hljs-function"><span class="hljs-function">x </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">in</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">enumerate</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">X</span></span></span><span class="hljs-function">): error +</span></span>= (<span class="hljs-number"><span class="hljs-number">1</span></span>/m) * (neuron(X[i], model) - Y[i]) * X[i][j] w_new = w - learning_rate * error new_model.append(w_new) model = new_model model_loss = binary_crossentropy(X, Y, model) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> </div></div><br> <a href="https://en.wikipedia.org/wiki/Backpropagation">   </a>         .          .          . ,     ,     . <br><br><h3>    </h3><br> ,            ‚Äî  .    ,      ,      50%. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cf78e0f240964795.png" alt="image"><br>  <i>Fig.</i> <i>10 ‚Äî      </i> <br><br> ,    ,    .  ,    bias ,      (  ). ,      ,   , - ,    (   ‚Äî ),      42 . <br><br>        0.5    42 ,  0.5    .    ,      0.5      0.5  ‚Äî . ,        -  .         ,       ,                  x,            . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cfb3707213565119.png" alt="image"><br>  <i>Fig.</i> <i>11 ‚Äî          ¬´¬ª  ,    </i> <br><br>      ,      bias-term   .    f(x) ,   42,    42    f(x-42).        ,  ,   0.25,    f(0.25(x-24)).  , : <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d19d6c3035544555.png" alt="image"><br><br>        w = 0.25,   b = -10.5.    ,  b      (w <sub>0</sub> =b)   ,          (x <sub>0</sub> =1). ,   ¬´¬ª     45 ,   x <sup>(15)</sup> = {x <sup>(15)</sup> <sub>0</sub> , x <sup>(15)</sup> <sub>1</sub> } = [1, 30],      68%.           ¬´ ¬ª. , ,         .          (w <sub>0</sub> =b  w <sub>1</sub> ). <br><br><img src="https://habrastorage.org/webt/59/e9/d5/59e9d5695ded6015256031.gif" alt="image"><br> <i> 1 ‚Äî      </i> <br><br>          ,        .           ,     . <br><br><img src="https://habrastorage.org/webt/59/e9/d5/59e9d5805ee0e906711578.gif" alt="image"><br> <i> 2 ‚Äî     </i> <br><br>      .         .              ¬´¬ª,     .    ,                   ¬´¬ª   . <br><br><img src="https://habrastorage.org/webt/59/e9/d5/59e9d5b8005f7346637351.gif" alt="image"><br> <i> 3 ‚Äî      </i> <br><br>            ,    ¬´¬ª‚Äù                 . <br><br><img src="https://habrastorage.org/webt/59/e9/d5/59e9d594082c0282377834.gif" alt="image"><br> <i> 4 ‚Äî     </i> <br><br>            : <br><br><ul><li>   x; </li><li> ,       a; </li><li> ,   bias b; </li><li> ,            w; </li><li>  h <sub>w,b</sub> (x) ‚Äî    . </li></ul><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cf1733c436467192.png" alt="image"><br>  <i>Fig.</i> <i>12 ‚Äî     </i> <br><br>       <a href="http://playground.tensorflow.org/">TensorFlow</a> .       ,     .     ¬´¬ª,           (  bias,   bias  ).   ,    <a href="https://en.wikipedia.org/wiki/Linear_separability"> </a>       ,  ()  .        . <br><br><img src="https://habrastorage.org/webt/59/e9/ec/59e9ec0c5a580106863325.gif" alt="image"><br> <i> 5 ‚Äî      </i> <br><br>      ,              ,            .         ,      .            . <br><br><img src="https://habrastorage.org/webt/59/e9/ec/59e9ec18e3480308937731.gif" alt="image"><br> <i> 6 ‚Äî       </i> <br><br>   ,         ,     .  ,       () .    ,    . <br><br><img src="https://habrastorage.org/webt/59/e9/ec/59e9ec14af3e5684819941.gif" alt="image"><br> <i> 7 ‚Äî       </i> <br><br>      .    ,   .       ‚Äî  .    .    ()      . <br><br>      ‚Äî    ,     . <br><br><img src="https://habrastorage.org/webt/59/e9/d6/59e9d65f9cf57757114894.gif" alt="image"><br> <i> 8 ‚Äî     ¬´ ¬ª</i> <br><br>        .        ¬´ ¬ª,           .    ,        .        <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> (rectified linear unit)     ,     (  <a href="https://en.wikipedia.org/wiki/Softmax_function)">softmax</a> -)   . <br><br>  ,  ,        ,       .        ,       ,          . <br><br><h1>  Model training </h1><br>       ,   ,     .     ,        .    5%,   ‚Äî 95%.      ?    95% , ,    . <br> ,        ,      ,         .     ¬´¬ª   ,      . <br><br> ,     20,000     1,000 ,         500      .       .      ,      . <br><br>     ,      :     ,  70%  ,   30%   ,       . <br><br><h2>    </h2><br>  ,     .     : <br><br><ul><li> TP (True Positive) ‚Äî .  ,   ,   . </li><li> FP (False Positive) ‚Äî .  ,   ,    .      .    ,    ,    ,   ‚Äî   - . </li><li> FN (False Negative) ‚Äî .  ,    ,     (  ).      .        ,       . </li><li> TN (True Negative) ‚Äî .  ,    ,    . </li></ul><br>         ,   (. precision)   (. recall),      . <br><br><img src="https://habrastorage.org/webt/59/f1/06/59f106b9e156d163176381.png"><br>  <i>Fig.</i> <i>13 ‚Äî     </i> <br><br>   ,              -,    .    ,    .           . <br><br><h3>   </h3><br>    ‚Äî    (. Accuracy).         ,    .    ,     ,         . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d17644e393489643.png" alt="image"><br><br><h3>    </h3><br>  (. precision)         ,      . ,  ,   115,      37,    0.33.  (. recall)           . ,      37,     43.     0.88. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cb31194244354746.png" alt="image"><br>  <i>Fig.</i> <i>14 ‚Äî    confusion matrix</i> <br><br><h3> F- </h3><br>   F- (. F1 score) ‚Äî     .   ,    . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01d16a768781075145.png" alt="image"><br><br>    ,    . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cb8d177595707754.png" alt="image"><br>  <i>Fig.</i> <i>15 ‚Äî        </i> <br><br>    ,           (. recall).    88%  ,   12%.      36%   ,     64%.           ,   .         ,       ,    . <br><br><h2>   </h2><br>      ,    ,   ,      .      ()    ,  ,   ,         .          ,      . <br><br>           (  ): <br><br><img src="https://habrastorage.org/webt/59/ee/20/59ee2017166de475541126.jpeg" alt="image"><br><br> ,  ,    ,    . , ¬´¬ª      ,        .            . <br><br>             . ,       (    ‚Äî     )  0.5     (    ).                 .           .     ,      . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01c8ed04c652848757.jpeg" alt="image"><br>  <i>Fig.</i> <i>16 ‚Äî   ()    ()      </i> <br><br>    ,         .  (       )  64%. <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01c912284999341332.jpeg" alt="image"><br>  <i>Fig.</i> <i>17 ‚Äî   ()    ()      </i> <br><br>   ,              .  ,    ,    ,  ‚Äî .  (       )  88%. <br><br><h2>  Result </h2><br>           ,      .  ,      ,   ,    ,     . <br><br>                  ,           . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01c98a6d5652455073.jpeg" alt="image"><br>  <i>Fig.</i> <i>18 ‚Äî   ()    ()       </i> <br><br>  ,    ,          87%    .  :      77%. ,       10%    .           : 23%  24%      .   ,        . <br><br><img src="https://habrastorage.org/webt/59/ea/01/59ea01cbdfbc2169054812.png" alt="image"><br>  <i>Fig.</i> <i>19 ‚Äî       </i> <br><br> ,    : <br><br><ul><li>     data-mining. </li><li>        ,    . </li><li>         . </li><li>        . </li><li>                 . </li><li> ,           -. </li></ul><br>     ,   ,    . </div><p>Source: <a href="https://habr.com/ru/post/340792/">https://habr.com/ru/post/340792/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../340780/index.html">Programming for 3CX in C #: use the 3CX Call Control API in the Call Flow Designer development environment</a></li>
<li><a href="../340782/index.html">Fake cryptocurrency trading applications from Google Play were stolen by data</a></li>
<li><a href="../340784/index.html">Yandex. Blitz. 12 algorithmic problems of the qualifying round and their analysis</a></li>
<li><a href="../340786/index.html">EdHack 2017 results - AR and VR in education</a></li>
<li><a href="../340788/index.html">How to integrate with Cisco security solutions? Overview of two dozen APIs available to all</a></li>
<li><a href="../340794/index.html">Workshops at FrontFest - cross-platform applications on Angular, 3D games on Canvas and backends on Node.js</a></li>
<li><a href="../340798/index.html">The best UX and web design by Behance users</a></li>
<li><a href="../340800/index.html">The release of OWASP Top 10 2017 RC 2</a></li>
<li><a href="../340806/index.html">‚ÄúThe main challenge is personnel shortage‚Äù is a panel discussion about the selection of data teams. Data Science Week 2017</a></li>
<li><a href="../340808/index.html">Fantastic processors and where they live - the juice from the new lines of HPE, Dell EMC and Lenovo</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>