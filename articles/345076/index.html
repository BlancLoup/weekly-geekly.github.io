<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Key-value for storing metadata in storage. Testing embedded databases</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On November 7-8, 2017, at the Highload ++ conference, researchers at the Radiks Lab presented a report ‚ÄúCluster Metadata: The Race of Key-Value-Heroes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Key-value for storing metadata in storage. Testing embedded databases</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/hh/zt/po/hhztpoljw20zmj_ukxawyhag4yq.jpeg"><br><br>  On November 7-8, 2017, at the <b>Highload ++</b> conference, researchers at the Radiks Lab presented a report ‚ÄúCluster Metadata: The Race of Key-Value-Heroes‚Äù. <br><br>  In this article, we presented the main report material relating to testing key-value databases.  ‚ÄúWhy should they be tested by the storage vendor?‚Äù You ask.  The task arose in connection with the problem of storing metadata.  Such features as deduplication, tearing, thin provisioning (thin provisioning), log-structured recording, run counter to the direct addressing mechanism ‚Äî there is a need to store a large amount of service information. <br><a name="habracut"></a><br><h2>  Introduction </h2><br>  Suppose that all storage space is divided into pages of size X bytes.  Each of them has its own address - LBA (8 Bytes).  For each such page we want to store Y bytes of metadata.  Thus, we obtain a set of lba -&gt; metadata correspondences.  How many such matches will we have?  It all depends on how much data we store. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/t-/wq/jr/t-wqjrmncxdmfalupmwy7lq-knq.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">1. Metadata</font></i> <br><br>  For example, when X = 4KB, Y = 16 bytes.  We get the following table: <br><br>  <i><font color="#99999">Table 1. The ratio of storage volume and metadata</font></i> <br><table><tbody><tr><th>  The amount of data per node </th><th>  Number of keys per node </th><th>  Metadata volume </th></tr><tr><td>  512TB </td><td>  137 billion </td><td>  3TB </td></tr><tr><td>  64TB </td><td>  17 billion </td><td>  384GB </td></tr><tr><td>  4TB </td><td>  1 billion </td><td>  22.3GB </td></tr></tbody></table><br>  The volume of metadata is quite large, so keeping metadata in RAM is not possible (or it is not economically feasible).  In this regard, the question arises of storing metadata, and with maximum access performance. <br><br><h2>  Metadata storage options </h2><br><ol><li>  <b>Key-value DB.</b>  lba - key, metadata - value </li><li>  <b>Direct Addressing.</b>  Do not store lba = N * Y metadata, not N * (8 + Y) B </li></ol><br>  What is direct addressing?  This is when we simply place our metadata on the drive in order, starting with the very first sector of the drive.  At the same time, we don‚Äôt even have to write down to which lba the metadata correspond, since everything is in ascending order of lba. <br><br><img src="https://habrastorage.org/webt/xt/ew/b8/xtewb889tkpq3-u8wxgaiox-gmw.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">2. The principle of direct addressing</font></i> <br><br>  In Figure 2, pba1 is the physical sector (512B) of the drive, where we store metadata, and lba1 ... lba32 (there are 32 of them, because 512 / 16 = 32) are the addresses of the pages with which this metadata corresponds, and these we do not need to store addresses. <br><br><h2>  Analysis of the workload in the storage system </h2><br>  Based on our experience of workloads, we decide what requirements for delays and throughput we need. <br><br>  Workload in the <b>media industry</b> : <br><br><ul><li>  NLE (non-linear editing) - reading and writing several large files in parallel. </li><li>  VOD (video on demand) - reading multiple streams, sometimes with jumps.  It is possible to write in parallel in several streams. </li><li>  Transcoding - 16‚Äì128 KB random R / W 50/50. </li></ul><br>  <b>Enterprise</b> workload: <br><br><ul><li>  8‚Äì64 KB IO. </li><li>  Random read / write approximately 50/50. </li><li>  Periodically Seq Read and Write to hundreds of threads (Boot, Virus Scan). </li></ul><br>  Workload in <b>high performance computing (HPC)</b> : <br><br><ul><li>  16/32 KB IO. </li><li>  Alternate read / write in hundreds and thousands of threads. </li></ul><br>  From the presented workloads in various sectors of the market we will form the final performance requirements for All-Flash DSS: <br><br><ol><li>  Show: <br>  ‚Ä¢ Latency (latency) 1‚Äì2ms (99.99% percentile) for flash. <br>  ‚Ä¢ From 20GB / s, from 300‚Äì500k IOPS. </li><li>  On the following loads: <br>  ‚Ä¢ Random R / W. <br>  ‚Ä¢ Ratio 50/50. <br>  ‚Ä¢ Block size 8‚Äì64K. </li></ol><br>  To choose which database of the dozens of existing ones will suit us, it is necessary to conduct tests on selected loads.  So we can understand how a database deals with a certain amount of data, which provides latency / throughput. <br><br>  What difficulties arise at the start? <br><br><ol><li>  We'll have to select only a few databases for tests - everything will fail to test.  How to choose these few?  Only subjectively. </li><li>  Ideally, you should carefully configure each database before testing, which can take an enormous amount of time.  Therefore, we decided to first look at what numbers can be obtained in the standard configuration, and then decide how promising the testing is. </li><li>  It is rather difficult to make tests completely objective and create the same conditions for each of the databases due to fundamental differences between the databases. </li><li>  Few benchmarks or difficult to find.  What we need is unified benchmarks that can be used to test any (or almost any) key-value database, or at least some of the most interesting ones.  When testing different databases with different benchmarks, objectivity suffers. </li></ol><br><h2>  Types of key-value database </h2><br>  There are two types of key-value databases: <br><br><ol><li>  <b>Embedded</b> - they are also called "engines".  In fact, this is a library that you can connect to in your code and use its functions. </li><li>  <b>Dedicated</b> - you can also call them ‚Äúdatabase server‚Äù, ‚ÄúNoSQL database‚Äù.  These are separate processes that can most often be accessed by sockets.  Usually have more features than embedded.  For example, replication. </li></ol><br>  In this article we will consider testing embedded key-value databases (hereinafter, we will call them "engines"). <br><br><h2>  What to test? </h2><br>  The first option that can be found is <a href="https://github.com/brianfrankcooper/YCSB/wiki">YCSB</a> . <br><br>  Features YCSB: <br><ul><li>  This benchmark is a kind of industry standard, they trust it. </li><li>  Workloads can be easily configured in configuration files. </li><li>  Written in Java.  In this case, it is a minus, because  Java is not very fast, and this can introduce distortion into the test results.  In addition, the engines are mainly written in C / C ++.  This makes it difficult to write the driver YCSB &lt;-&gt; engine. </li></ul><br>  The second option is a benchmark for <a href="https://github.com/pmwkaa/ioarena">ioarena</a> engines. <br><ul><li>  Written in C. </li><li>  Few workloads.  Of those that interest us, there is only Random Read.  We had to write in the code we need workloads. </li></ul><br><br><img src="https://habrastorage.org/webt/49/6m/po/496mpoqmpvxiiv-jc8utxest76o.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">3. Testing options</font></i> <br><br>  As a result, ioarena is selected for the engines, and YCSB is selected for the selected ones. <br><br>  In addition to the workloads in ioarena, we added an option (-a), which allows you to specify the number of operations performed per thread and the number of keys in the database separately when running. <br><br>  All changes to the ioarena code can be found on <a href="https://github.com/ragruslan/ioarena">GitHub</a> . <br><br><h2>  Test parameters of the key-value database </h2><br>  The main workload that we are interested in is Mix50 / 50.  We also decided to look at RR, Mix70 / 30 and Mix30 / 70 in order to understand which databases more ‚Äúlove‚Äù this or that workload. <br><br><h3>  Testing method </h3><br>  We test in 3 stages: <br><br><ol><li>  Database filling - we fill in 1 database flow to the required number of keys. <br>  1.1 Reset caches!  Otherwise, the tests will be dishonest: the database usually writes data on top of the file system, so the operating system cache works.  It is important to discard it before each test. </li><li>  Tests for 32 threads - we run workloads <br>  2.1 Random Read <br>  ‚Ä¢ We reset caches! <br>  2.2 Mix70 / 30 <br>  ‚Ä¢ We reset caches! <br>  2.3 Mix50 / 50 <br>  ‚Ä¢ We reset caches! <br>  2.4 Mix30 / 70 <br>  ‚Ä¢ We reset caches! </li><li>  Tests for 256 threads. <br>  3.1 Same as for 32 threads. </li></ol><br><h3>  What are we measuring? </h3><br><ul><li>  Throughput / throughput (IOPS / RPS - who loves which notation). </li><li>  Latency (msec) latency: <br>  ‚Ä¢ Min. <br>  ‚Ä¢ Max. <br>  ‚Ä¢ The mean square value is a more significant value than the arithmetic mean, because  takes into account the standard deviation. <br>  ‚Ä¢ Percentile 99.99. </li></ul><br><h3>  Test environment </h3><br>  Configuration: <br><table><tbody><tr><td>  CPU: </td><td>  2x Intel Xeon E5-2620 v4 2.10GHz </td></tr><tr><td>  RAM: </td><td>  16GB </td></tr><tr><td>  Disk: </td><td>  [2x] NVMe HGST SN100 1.5TB </td></tr><tr><td>  OS: </td><td>  CentOS Linux 7.2 kernel 3.11 </td></tr><tr><td>  FS: </td><td>  EXT4 </td></tr></tbody></table><br>  It is important to note here that such a small amount of RAM is taken not by chance.  Thus, the base will not be able to fit completely into the cache on tests with 1 billion keys. <br><br>  The amount of available RAM was not physically regulated, but programmatically - part of it was artificially filled with a Python script, and the remainder was free for the database and caches. <br><br>  In some tests there was a different amount of available memory - this will be discussed separately. <br><br>  NVMe in tests used one. <br><br><h3>  Write reliability </h3><br>  An important point is the reliability mode of writing data to disk.  From this very much depends on the recording speed and the probability / volume of losses in case of failures. <br><br>  In general, there are 3 modes: <br><br><ul><li>  <b>Sync</b> - fair write to disk before answering the ‚ÄúOK‚Äù user for a write request.  In the event of a failure, everything remains in place until the last committed transaction. </li><li>  <b>Lazy</b> - we write data to the buffer, we answer the user with ‚ÄúOK‚Äù, and after a short period of time we buffer the buffer to disk.  In case of failure, we may lose some recent changes. </li><li>  <b>Nosync</b> - we don‚Äôt <b>flush</b> data to disk and just write them to the buffer so that sometime (not fundamentally when) to flush the buffer to disk.  In this mode there can be big losses in case of failure. </li></ul><br>  By performance, the difference is approximately the same (for example, MDBX engine): <br><br><ul><li>  <b>Sync = 10k IOPS</b> </li><li>  <b>Lazy = 40k IOPS</b> </li><li>  <b>Nosync = 300k IOPS</b> </li></ul><br>  The numbers here are ONLY for an approximate understanding of the difference between modes. <br><br>  As a result, for tests, the lazy mode was chosen as the most balanced.  Exceptions will be discussed separately. <br><br><h2>  We test the embedded key-value database </h2><br>  To test the ‚Äúengines‚Äù we performed two test options: for 1 billion keys and for 17 billion keys. <br><br>  Selected "engines": <br><br><ul><li>  <a href="http://rocksdb.org/">RocksDB</a> - everyone knows about it, this is a database from Facebook.  LSM index. </li><li>  <a href="http://www.wiredtiger.com/">WiredTiger</a> - MongoDB engine.  LSM index.  Read <a href="http://source.wiredtiger.com/2.9.3/architecture.html">here</a> . </li><li>  <a href="http://sophia.systems/">Sophia</a> - this engine has its own custom index, which has something in common with LSM-trees, B-trees.  You can <a href="http://sophia.systems/v2.2/arch/v12.html">read here</a> . </li><li>  <a href="https://github.com/leo-yuriev/libmdbx">MDBX</a> - fork LMDB with improvements in reliability and performance.  B + tree as an index. </li></ul><br><h2>  Test results.  1 billion keys </h2><br><h3>  Filling </h3><br><br><img src="https://habrastorage.org/webt/fu/so/3i/fuso3izacoxi3mc5neprp9fk8bk.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">4.1.</font></i>  <i><font color="#99999">Dependence of the current speed (IOPS) on the number of keys (the x-axis - millions of keys).</font></i> <br><br>  Here are all the engines in the standard configuration.  Reliability mode Lazy, except MDBX.  His record is too slow in Lazy, so the Nosync mode was chosen for him, otherwise the filling will take too long.  However, it is clear that from some point the recording speed still drops to about the speed level of the Sync mode. <br><br>  What can be seen on this chart? <br><br>  <b>First</b> , something happens to RocksDB after 800 million keys.  Unfortunately, the reasons for what is happening have not been clarified. <br><br><img src="https://habrastorage.org/webt/qe/a7/gh/qea7ghxq1muwc0n9pukl1ehprtw.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">4.2.</font></i>  <i><font color="#99999">Dependence of the current speed (IOPS) on the number of keys (the x-axis - millions of keys).</font></i> <br><br>  <b>Second:</b> MDBX didn‚Äôt tolerate the moment when data became more than the available memory. <br><br><img src="https://habrastorage.org/webt/ea/uo/ic/eauoicnolko90t1t4999s0yd-ci.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">4.3.</font></i>  <i><font color="#99999">Dependence of the current speed (IOPS) on the number of keys (the x-axis - millions of keys).</font></i> <br><br>  Then you can look at the maximum delay graph.  It also shows that RocksDB started flights after 800 million keys. <br><br><img src="https://habrastorage.org/webt/ec/l7/3e/ecl73eqpoidgbkew-hgdmlfogio.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">5 Maximum latency</font></i> <br><br>  Below is a graph of the average quadratic delay value.  You can also see the very same borders for RocksDB and MDBX. <br><br><img src="https://habrastorage.org/webt/b8/fb/f2/b8fbf2rwgxj-lguux5uzqsnszvk.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">6.1.</font></i>  <i><font color="#99999">RMS Latency</font></i> <br><br><img src="https://habrastorage.org/webt/2p/kn/ja/2pknjar-czpdnkpiijpjkfrv_xk.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">6.2.</font></i>  <i><font color="#99999">RMS Latency</font></i> <br><br><h3>  Tests </h3><br>  Unfortunately, Sophia showed low results in all tests.  Most likely, she does not ‚Äúlove‚Äù many streams (ie, 32 or more). <br><br>  WiredTiger first showed very poor performance - at the level of 30 IOPS.  It turned out that he has an important parameter cache_size, which by default is set to 500MB.  After installing it on 8GB (or even 4GB), everything becomes much better. <br><br>  For the sake of interest, a test was conducted with the same amount of data, but with the amount of available memory&gt; 100GB.  In this case, MDBX with a margin goes forward in the reading test. <br><br><img src="https://habrastorage.org/webt/z2/ps/xm/z2psxmqrel3wi7khnezxqzdbots.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">7. 100% Read</font></i> <br><br>  When adding a record to workload, we get a strong drop in MDBX (which is expected, because the speed was low when filling).  WiredTiger grew, and RocksDB slowed down. <br><br><img src="https://habrastorage.org/webt/jz/5q/pn/jz5qpnahkl2j-xozh0oizhp-xjy.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">8. Mix 70% / 30%</font></i> <br><br>  The trend is preserved. <br><br><img src="https://habrastorage.org/webt/em/jt/bs/emjtbschp6bethuksaqqkuttsoa.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">9. Mix 50% / 50%</font></i> <br><br>  When there are a lot of recordings, WiredTiger begins to overtake RocksDB on a small number of threads. <br><br><img src="https://habrastorage.org/webt/d1/m7/5x/d1m75xpytjws-rirnyj-wvyl-hk.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">10. Mix 30% / 70%</font></i> <br><br>  Now you can look at the graphics delays.  The columns show the minimum and maximum delays, the orange bar shows the average quadratic delay, and the red bar shows the percentile 99.99. <br><br>  The green bar is about 2 ms.  That is, we want the percentile to be no higher than the green bar.  In this case, we do not get this (logarithmic scale). <br><br><img src="https://habrastorage.org/webt/q-/pj/yy/q-pjyyecdue0nl4rkwpwobuz4wc.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">11. Latency Read</font></i> <br><br><img src="https://habrastorage.org/webt/za/p2/qg/zap2qgk8t2ut0fybixpqruzvags.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">12. Latency 50% / 50%</font></i> <br><br><h2>  Test results.  17 billion keys </h2><br><h3>  Filling </h3><br>  Tests for 17 billion keys were conducted only on RocksDB and WiredTiger, because  they were the leaders in tests for 1 billion keys. <br><br>  WiredTiger started having strange attacks, but in general it shows itself quite well on filling, plus there is no degradation with increasing data. <br><br>  But RocksDB eventually went below 100k IOPS.  Thus, in the test for 1 billion keys, we did not see the whole picture, so it is important to conduct tests on volumes comparable to real ones! <br><br><img src="https://habrastorage.org/webt/y7/an/6l/y7an6lbuflngv6wldqdnth9ibdw.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">13. Productivity 17 billion keys</font></i> <br><br>  The dotted line shows the mean square delay.  It can be seen that the maximum delay for WiredTiger is higher, and the average quadratic delay is lower than that of RocksDB. <br><br><img src="https://habrastorage.org/webt/ot/uw/aj/otuwaj8ev6fspshm9venlcucmie.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">14. Latency 17 billion keys</font></i> <br><br><h3>  Tests </h3><br>  WiredTiger had the same trouble as last time - it showed about 30 IOPS per read, even with cache_size = 8GB.  It was decided to further increase the value of the cache_size parameter, but this did not help either: even with 96GB, the speed did not rise above several thousand IOPS, although the allocated memory was not even full. <br><br>  When adding a record to the workload, WiredTiger traditionally rises. <br><br><img src="https://habrastorage.org/webt/kp/ju/hs/kpjuhsawq-fzacnsh-v5rzdsneg.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">15. Performance 100% Read</font></i> <br><br><img src="https://habrastorage.org/webt/av/kr/lq/avkrlqhglxjwxnalulzusfsgm9o.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">16. Productivity 70% / 30%</font></i> <br><br><img src="https://habrastorage.org/webt/gc/pk/8x/gcpk8xopidenksuuhic7cv-q6am.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">17. Productivity 30% / 70%</font></i> <br><br><img src="https://habrastorage.org/webt/xw/ad/ij/xwadijn05fvgst4tkjrijha2d88.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">18. Productivity 50% / 50%</font></i> <br><br><img src="https://habrastorage.org/webt/fk/2e/jk/fk2ejkdngv3tj_u5-ivyumvlfcs.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">19. Latency 100% Read</font></i> <br><br><img src="https://habrastorage.org/webt/en/pm/nt/enpmntpmhcvbmx3kkbn8ymov0jo.png"><br>  <i><font color="#99999">Fig.</font></i>  <i><font color="#99999">20. Latency 50% / 50%</font></i> <br><br><h2>  findings </h2><br>  From what has been said above, we can draw the following conclusions: <br><br>  For a base of 1 billion keys: <br><br><ul><li>  Record + few threads =&gt; <b>WiredTiger</b> </li><li>  Record + many threads =&gt; <b>RocksDB</b> </li><li>  Read + DATA&gt; RAM =&gt; <b>RocksDB</b> </li><li>  Read + DATA &lt;RAM =&gt; <b>MDBX</b> </li></ul><br>  From which it follows that: <br><br><ul><li>  Mix50 / 50 + many threads + DATA&gt; RAM =&gt; <b>RocksDB</b> </li></ul><br>  For a base of 17 billion keys: a clear lead from <b>RocksDB</b> . <br><br>  This is the situation with embedded engines.  In the next article we will talk about the indicators of the selected key-value databases and draw conclusions about benchmarks. </div><p>Source: <a href="https://habr.com/ru/post/345076/">https://habr.com/ru/post/345076/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../345064/index.html">First look at the RPG: it turns out, this is not only role-playing games</a></li>
<li><a href="../345066/index.html">How we overcame iron obstacles in test automation</a></li>
<li><a href="../345070/index.html">On the wave of mining. New virus distributed through Facebook</a></li>
<li><a href="../345072/index.html">Good news for the webmaster: Hamster Marketplace placed an offer on the RunCPA site</a></li>
<li><a href="../345074/index.html">How not to drown in the landing: the history of the creation of the Japanese CarPrice</a></li>
<li><a href="../345078/index.html">IT forecasts for 2018: 8 infrastructure trends</a></li>
<li><a href="../345080/index.html">Hippocratic Oath or how to protect information in medical institutions</a></li>
<li><a href="../345082/index.html">How to analyze the tone of tweets using machine learning in PHP</a></li>
<li><a href="../345084/index.html">Voxxed Days Minsk</a></li>
<li><a href="../345086/index.html">Fast and secure OS for web surfing with impregnable media, easily changeable by the user.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>