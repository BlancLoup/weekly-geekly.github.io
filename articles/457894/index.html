<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Work with Proxmox Cluster: Installation, Network Configuration, ZFS, Common Problem Solving</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Over the past few years, I have been working very closely with Proxmox clusters: many clients need their own infrastructure where they can develop the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Work with Proxmox Cluster: Installation, Network Configuration, ZFS, Common Problem Solving</h1><div class="post__text post__text-html js-mediator-article">  Over the past few years, I have been working very closely with Proxmox clusters: many clients need their own infrastructure where they can develop their project.  That is why I can tell you about the most common mistakes and problems that you may also face.  In addition, we of course configure a cluster of three nodes from scratch. <br><img src="https://habrastorage.org/webt/jz/j-/lq/jzj-lqgwozo7rze1o8ij7bvzday.png"><br><a name="habracut"></a><br>  Proxmox cluster can consist of two or more servers.  The maximum number of nodes in a cluster is 32 pieces.  Our own cluster will consist of three nodes on a multicast (in the article I will also describe how to raise a cluster on a unique one ‚Äî this is important if you base your cluster infrastructure on Hetzner or OVH, for example).  In short, the multicast allows data transfer to several nodes simultaneously.  When multicasting, we can not think about the number of nodes in the cluster (focusing on the restrictions above). <br><br>  The cluster itself is built on an internal network (it is important that IP addresses are on the same subnet), the same Hetzner and OVH have the ability to cluster nodes in different data centers using Virtual Switch (Hetzner) and vRack (OVH) technology - about Virtual Switch we'll also talk in the article.  If your hosting provider does not have similar technologies in operation, then you can use OVS (Open Virtual Switch), which is natively supported by Proxmox, or use VPN.  However, I recommend in this case to use a unicast with a small number of nodes - situations often arise where the cluster simply ‚Äúcollapses‚Äù on the basis of such a network infrastructure and has to be restored.  Therefore, I try to use OVH and Hetzner in my work - I observed fewer such incidents, but first of all, study the hosting provider that you will be hosting: does it have alternative technology, what solutions does it offer, does multicast support, and so on . <br><br><h3>  Install Proxmox </h3><br>  Proxmox can be installed in two ways: ISO-installer and installation through the shell.  We choose the second method, so install Debian on the server. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's proceed directly to installing Proxmox on each server.  Installation is extremely simple and is described in the official documentation here. <br><br>  Add the Proxmox repository and the key of this repository: <br><br><pre><code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">"deb http://download.proxmox.com/debian/pve stretch pve-no-subscription"</span></span> &gt; /etc/apt/sources.list.d/pve-install-repo.list wget http://download.proxmox.com/debian/proxmox-ve-release-5.x.gpg -O /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg chmod +r /etc/apt/trusted.gpg.d/proxmox-ve-release-5.x.gpg <span class="hljs-comment"><span class="hljs-comment"># optional, if you have a changed default umask</span></span></code> </pre> <br>  We update the repositories and the system itself: <br><br><pre> <code class="bash hljs">apt update &amp;&amp; apt dist-upgrade</code> </pre> <br>  After a successful upgrade, install the necessary Proxmox packages: <br><br><pre> <code class="bash hljs">apt install proxmox-ve postfix open-iscsi</code> </pre> <br>  <b>Note</b> : during the installation, Postfix and grub will be configured - one of them may fail with an error.  Perhaps this will be due to the fact that the hostname does not resolve the name.  Edit the hosts entry and run apt-get update <br><br>  From now on, we can log in to the Proxmox web interface at https: // &lt;external ip-address&gt;: 8006 (you will encounter an untrusted certificate during connection). <br><br><img src="https://habrastorage.org/webt/e_/cg/mv/e_cgmvs9rrh3qwq0su222v2j0iw.png"><br>  <b>Image 1.</b> Proxmox node web interface <br><br><h3>  Installing Nginx and Let's Encrypt Certificate </h3><br>  I do not really like the situation with the certificate and IP address, so I suggest to install Nginx and configure Let's Encrypt certificate.  I will not describe Nginx installation, I will leave only important files for Let's encrypt the certificate: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/nginx/snippets/letsencrypt.conf</b> <div class="spoiler_text"><pre> <code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">location</span></span><span class="hljs-regexp"><span class="hljs-regexp"> ^~</span></span> /.well-known/acme-challenge/ { <span class="hljs-attribute"><span class="hljs-attribute">allow</span></span> all; <span class="hljs-attribute"><span class="hljs-attribute">root</span></span> /var/lib/letsencrypt/; <span class="hljs-attribute"><span class="hljs-attribute">default_type</span></span> <span class="hljs-string"><span class="hljs-string">"text/plain"</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">try_files</span></span> <span class="hljs-variable"><span class="hljs-variable">$uri</span></span> =<span class="hljs-number"><span class="hljs-number">404</span></span>; }</code> </pre><br><br></div></div><br>  The command to issue an SSL certificate: <br><br><pre> <code class="bash hljs">certbot certonly --agree-tos --email sos@livelinux.info --webroot -w /var/lib/letsencrypt/ -d proxmox1.domain.name</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Site configuration in NGINX</b> <div class="spoiler_text"><pre> <code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">upstream</span></span> proxmox1.domain.name { <span class="hljs-attribute"><span class="hljs-attribute">server</span></span> <span class="hljs-number"><span class="hljs-number">127.0.0.1:8006</span></span>; } <span class="hljs-section"><span class="hljs-section">server</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">listen</span></span> <span class="hljs-number"><span class="hljs-number">80</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">server_name</span></span> proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">include</span></span> snippets/letsencrypt.conf; <span class="hljs-attribute"><span class="hljs-attribute">return</span></span> <span class="hljs-number"><span class="hljs-number">301</span></span> https://<span class="hljs-variable"><span class="hljs-variable">$host</span></span><span class="hljs-variable"><span class="hljs-variable">$request_uri</span></span>; } <span class="hljs-section"><span class="hljs-section">server</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">listen</span></span> <span class="hljs-number"><span class="hljs-number">443</span></span> ssl; <span class="hljs-attribute"><span class="hljs-attribute">server_name</span></span> proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">access_log</span></span> /var/log/nginx/proxmox1.domain.name.access.log; <span class="hljs-attribute"><span class="hljs-attribute">error_log</span></span> /var/log/nginx/proxmox1.domain.name.<span class="hljs-literal"><span class="hljs-literal">error</span></span>.log; <span class="hljs-attribute"><span class="hljs-attribute">include</span></span> snippets/letsencrypt.conf; <span class="hljs-attribute"><span class="hljs-attribute">ssl_certificate</span></span> /etc/letsencrypt/live/proxmox1.domain.name/fullchain.pem; <span class="hljs-attribute"><span class="hljs-attribute">ssl_certificate_key</span></span> /etc/letsencrypt/live/proxmox1.domain.name/privkey.pem; <span class="hljs-attribute"><span class="hljs-attribute">location</span></span> / { <span class="hljs-attribute"><span class="hljs-attribute">proxy_pass</span></span> https://proxmox1.domain.name; <span class="hljs-attribute"><span class="hljs-attribute">proxy_next_upstream</span></span> <span class="hljs-literal"><span class="hljs-literal">error</span></span> timeout invalid_header http_500 http_502 http_503 http_504; <span class="hljs-attribute"><span class="hljs-attribute">proxy_redirect</span></span> <span class="hljs-literal"><span class="hljs-literal">off</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_buffering</span></span> <span class="hljs-literal"><span class="hljs-literal">off</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> Host <span class="hljs-variable"><span class="hljs-variable">$host</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> X-Real-IP <span class="hljs-variable"><span class="hljs-variable">$remote_addr</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">proxy_set_header</span></span> X-Forwarded-For <span class="hljs-variable"><span class="hljs-variable">$proxy_add_x_forwarded_for</span></span>; }</code> </pre> <br></div></div><br>  Do not forget after installing the SSL certificate to put it on auto-update via cron: <br><br><pre> <code class="bash hljs">0 */12 * * * /usr/bin/certbot -a \! -d /run/systemd/system &amp;&amp; perl -e <span class="hljs-string"><span class="hljs-string">'sleep int(rand(3600))'</span></span> &amp;&amp; certbot -q renew --renew-hook <span class="hljs-string"><span class="hljs-string">"systemctl reload nginx"</span></span></code> </pre> <br>  Fine!  Now we can access our domain via HTTPS. <br><br>  <b>Note</b> : to disable the subscription info window, run this command: <br><br><pre> <code class="bash hljs">sed -i.bak <span class="hljs-string"><span class="hljs-string">"s/data.status !== 'Active'/false/g"</span></span> /usr/share/javascript/proxmox-widget-toolkit/proxmoxlib.js &amp;&amp; systemctl restart pveproxy.service</code> </pre> <br>  <b>Network settings</b> <br><br>  Before connecting to the cluster, we configure the network interfaces on the hypervisor.  It should be noted that the configuration of the rest of the nodes is no different, except for the IP addresses and server names, so I will not duplicate their settings. <br><br>  We will create a network bridge for the internal network so that our virtual machines (in my version there will be an LXC container for convenience) firstly, be connected to the internal network of the hypervisor and can interact with each other.  Secondly, we will add a bridge for the external network a bit later so that the virtual machines have their external IP address.  Accordingly, the containers will be currently behind NAT'om with us. <br><br>  There are two ways to work with the Proxmox network configuration: through the web interface or through the / etc / network / interfaces configuration file.  In the first option, you will need to restart the server (or you can just rename the interfaces.new file to interfaces and restart the networking service via systemd).  If you are just starting the configuration and there are no virtual machines or LXC containers yet, then it is advisable to restart the hypervisor after the changes. <br><br>  Now create a network bridge called vmbr1 in the network tab in the Proxmox web panel. <br><br><img src="https://habrastorage.org/webt/i3/6k/wp/i36kwpe0ky3khngufngwifulwcs.png"><br>  <b>Figure 2.</b> Network interfaces proxmox1 nodes <br><br><img src="https://habrastorage.org/webt/ro/k6/tg/rok6tgyuqyvte_dswvl-0xgvbxe.png"><br>  <b>Image 3.</b> Creating a network bridge <br><br><img src="https://habrastorage.org/webt/kx/xu/kg/kxxukgzgym97cjezlvrczgtji8g.png"><br>  <b>Image 4.</b> Configure the vmbr1 network configuration <br><br>  Setup is extremely simple - we need vmbr1 in order for instances to access the Internet. <br><br>  Now we restart our hypervisor and check if the interface has been created: <br><br><img src="https://habrastorage.org/webt/cx/b9/ga/cxb9ga2zhwn0fefphugyihuj6fg.png"><br>  <b>Figure 5.</b> Network interface vmbr1 in the output of the ip a command <br><br>  Note: I already have an interface ens19 - this is an interface with an internal network, on the basis of which a cluster will be created. <br><br>  Repeat these steps on the remaining two hypervisors, and then proceed to the next step ‚Äî preparing the cluster. <br><br>  Also, an important step now is to enable packet forwarding - without it, instances will not get access to the external network.  Open the sysctl.conf file and change the value of the net.ipv4.ip_forward parameter to 1, and then enter the following command: <br><br><pre> <code class="bash hljs">sysctl -p</code> </pre> <br>  In the output you should see the directive net.ipv4.ip_forward (if you haven‚Äôt changed it before) <br><br>  <b>Configure Proxmox Cluster</b> <br><br>  We now turn directly to the cluster.  Each node must rezolvit themselves and other nodes on the internal network, for this you need to change the values ‚Äã‚Äãin the hosts records as follows (each node must contain a record of the others): <br><br><pre> <code class="bash hljs">172.30.0.15 proxmox1.livelinux.info proxmox1 172.30.0.16 proxmox2.livelinux.info proxmox2 172.30.0.17 proxmox3.livelinux.info proxmox3</code> </pre><br>  You also need to add the public keys of each node to the rest - this is required to create a cluster. <br><br>  Create a cluster through the web panel: <br><br><img src="https://habrastorage.org/webt/vl/rm/rh/vlrmrhkpwn5dle9gcnomfueoega.png"><br>  <b>Image 6.</b> Creating a cluster via the web interface <br><br>  After creating the cluster, we need to get information about it.  Go to the same tab of the cluster and click the button ‚ÄúJoin Information‚Äù: <br><br><img src="https://habrastorage.org/webt/gj/ur/t2/gjurt2tqr_pgtlfsxv7l3hrz398.png"><br>  <b>Figure 7.</b> Information about the created cluster <br><br>  This information is useful to us at the time of joining the second and third nodes in the cluster.  Connect to the second node and in the Cluster tab, click the ‚ÄúJoin Cluster‚Äù button: <br><br><img src="https://habrastorage.org/webt/fo/8u/zh/fo8uzhx-lzxfyqkapqdqsfuoalq.png"><br>  <b>Figure 8.</b> Connecting the node to the cluster <br><br>  Let us consider the details of the connection: <br><br><ol><li>  <b>Peer Address:</b> IP address of the first server (to the one to which we connect) </li><li>  <b>Password:</b> first server password </li><li>  <b>Fingerprint:</b> we get this value from the cluster information </li></ol><br><img src="https://habrastorage.org/webt/l4/zp/eo/l4zpeodynxiuqubl1fjc4b9iona.png"><br>  <b>Figure 9.</b> Cluster state after connecting the second node <br><br>  The second node is successfully connected!  However, this is not always the case.  If you perform the steps incorrectly or network problems arise, the connection to the cluster will fail, and the cluster itself will be ‚Äúbroken up‚Äù.  The best solution is to disconnect the node from the cluster, delete all the information about the cluster itself, then restart the server and check the previous steps.  How is it safe to disconnect the node from the cluster?  First, remove it from the cluster on the first server: <br><br><pre> <code class="bash hljs">pvecm del proxmox2</code> </pre> <br>  After that, the node will be disconnected from the cluster.  Now go to the broken node and disable the following services on it: <br><br><pre> <code class="bash hljs">systemctl stop pvestatd.service systemctl stop pvedaemon.service systemctl stop pve-cluster.service systemctl stop corosync systemctl stop pve-cluster</code> </pre><br>  Proxmox cluster stores information about itself in the sqlite database, it also needs to be cleared: <br><br><pre> <code class="bash hljs">sqlite3 /var/lib/pve-cluster/config.db delete from tree <span class="hljs-built_in"><span class="hljs-built_in">where</span></span> name = <span class="hljs-string"><span class="hljs-string">'corosync.conf'</span></span>; .quit</code> </pre><br>  The data on the boletus has been successfully deleted.  Delete the remaining files, for this you need to run the cluster file system in standalone mode: <br><br><pre> <code class="bash hljs">pmxcfs -l rm /etc/pve/corosync.conf rm /etc/corosync/* rm /var/lib/corosync/* rm -rf /etc/pve/nodes/*</code> </pre><br>  We restart the server (this is optional, but we will re-insure: all services should be started and work correctly. To not miss anything, we are restarting).  After switching on, we will get a blank node without any information about the previous cluster and we can start connecting again. <br><br><h3>  Installing and configuring ZFS </h3><br>  ZFS is a file system that can be used with Proxmox.  Using it, you can afford to replicate data to another hypervisor, migrate a virtual machine / LXC container, access the LXC container from a host system, and so on.  Installing it is quite simple, let's proceed to the analysis.  Three SSD drives are available on my servers, which we will combine into a RAID array. <br><br>  Add repositories: <br><br><pre> <code class="bash hljs">nano /etc/apt/sources.list.d/stretch-backports.list deb http://deb.debian.org/debian stretch-backports main contrib deb-src http://deb.debian.org/debian stretch-backports main contrib nano /etc/apt/preferences.d/90_zfs Package: libnvpair1linux libuutil1linux libzfs2linux libzpool2linux spl-dkms zfs-dkms zfs-test zfsutils-linux zfsutils-linux-dev zfs-zed Pin: release n=stretch-backports Pin-Priority: 990</code> </pre><br>  Update the list of packages: <br><br><pre> <code class="bash hljs">apt update</code> </pre> <br>  Install the required dependencies: <br><br><pre> <code class="bash hljs"> apt install --yes dpkg-dev linux-headers-$(uname -r) linux-image-amd64</code> </pre> <br>  Install ZFS itself: <br><br><pre> <code class="bash hljs">apt-get install zfs-dkms zfsutils-linux</code> </pre> <br>  If you get a fusermount: fuse device not found error in the future, try 'modprobe fuse' first, then run the following command: <br><br><pre> <code class="bash hljs">modprobe fuse</code> </pre> <br>  Now proceed directly to the setting.  First we need to format the SSD and configure them via parted: <br><br><div class="spoiler">  <b class="spoiler_title">Configure / dev / sda</b> <div class="spoiler_text"><pre> <code class="bash hljs">parted /dev/sda (parted) <span class="hljs-built_in"><span class="hljs-built_in">print</span></span> Model: ATA SAMSUNG MZ7LM480 (scsi) Disk /dev/sda: 480GB Sector size (logical/physical): 512B/512B Partition Table: msdos Disk Flags: Number Start End Size Type File system Flags 1 1049kB 4296MB 4295MB primary raid 2 4296MB 4833MB 537MB primary raid 3 4833MB 37,0GB 32,2GB primary raid (parted) mkpart Partition <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>? primary/extended? primary File system <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>? [ext2]? zfs Start? 33GB End? 480GB Warning: You requested a partition from 33,0GB to 480GB (sectors 64453125..937500000). The closest location we can manage is 37,0GB to 480GB (sectors 72353792..937703087). Is this still acceptable to you? Yes/No? yes</code> </pre><br></div></div><br>  Similar actions need to be made for other disks.  After all the disks are prepared, proceed to the next step: <br><br>  zpool create -f -o ashift = 12 rpool / dev / sda4 / dev / sdb4 / dev / sdc4 <br><br>  We choose ashift = 12 for performance reasons - this is the recommendation of zfsonlinux itself, read more about this in their wiki: <a href="https://github.com/zfsonlinux/zfs/wiki/faq">github.com/zfsonlinux/zfs/wiki/faq#performance-considerations</a> <br><br>  Let's apply some settings for ZFS: <br><br><pre> <code class="bash hljs">zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> atime=off rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> compression=lz4 rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> dedup=off rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> snapdir=visible rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> primarycache=all rpool zfs <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> aclinherit=passthrough rpool zfs inherit acltype rpool zfs get -r acltype rpool zfs get all rpool | grep compressratio</code> </pre><br>  Now we need to calculate some variables to calculate zfs_arc_max, I do this as follows: <br><br><pre> <code class="bash hljs">mem =`free --giga | grep Mem | awk <span class="hljs-string"><span class="hljs-string">'{print $2}'</span></span>` partofmem=$((<span class="hljs-variable"><span class="hljs-variable">$mem</span></span>/10)) <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> &gt; /sys/module/zfs/parameters/zfs_arc_max grep c_max /proc/spl/kstat/zfs/arcstats zfs create rpool/data cat &gt; /etc/modprobe.d/zfs.conf &lt;&lt; EOL options zfs zfs_arc_max=<span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> EOL <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$setzfscache</span></span> &gt; /sys/module/zfs/parameters/zfs_arc_max grep c_max /proc/spl/kstat/zfs/arcstats</code> </pre> <br>  At the moment, the pool has been successfully created, we also created a subpul data.  You can check the status of your pool with the zpool status command.  This action must be performed on all hypervisors, and then proceed to the next step. <br><br>  Now add ZFS to Proxmox.  Go to the settings of the datacenter (namely him, and not a separate node) in the ‚ÄúStorage‚Äù section, click on the ‚ÄúAdd‚Äù button and select the ‚ÄúZFS‚Äù option, after which we will see the following parameters: <br><br>  ID: The name of the stack.  I gave him the name local-zfs <br>  ZFS Pool: We created rpool / data, and add it here. <br>  Nodes: specify all available nodes <br><br>  This command creates a new pool with selected disks.  At each hypervisor, a new storage should appear under the name local-zfs, after which you can migrate your virtual machines from local storage to ZFS. <br><br><h3>  Replicating instances to a neighboring hypervisor </h3><br>  In a Proxmox cluster, there is the ability to replicate data from one hypervisor to another: this option allows you to switch an instance from one server to another.  The data will be relevant at the time of the last synchronization - its time can be set when creating replication (15 minutes is standardly set).  There are two ways to migrate an instance to another Proxmox node: manual and automatic.  Let's first consider the manual version, and at the end I will provide you with a Python script that will allow you to create a virtual machine on an available hypervisor when one of the hypervisors is unavailable. <br><br>  To create replication, go to the Proxmox web panel and create a virtual machine or LXC container.  In the previous paragraphs, we set up a vmbr1 bridge with NAT, which will allow us to go to the external network.  I will create an LXC container with MySQL, Nginx and PHP-FPM with a test site to test replication work.  Below is a step by step instruction. <br><br>  We load the appropriate template (go to storage -&gt; Content -&gt; Templates), an example in the screenshot: <br><br><img src="https://habrastorage.org/webt/sd/bd/57/sdbd579lmmzxefsigiivaftvpce.png"><br>  <b>Image 10.</b> Local storage with templates and VM images <br><br>  Click the ‚ÄúTemplates‚Äù button and load the container LXC template we need: <br><br><img src="https://habrastorage.org/webt/qx/ug/he/qxughewqdsfniccmaamka9idie0.png"><br>  <b>Figure 11.</b> Selecting and loading a template <br><br>  Now we can use it when creating new LXC containers.  Select the first hypervisor and click the ‚ÄúCreate CT‚Äù button in the upper right corner: we will see the panel for creating a new instance.  The installation steps are quite simple and I will only provide the configuration file for this LXC container: <br><br><pre> <code class="bash hljs">arch: amd64 cores: 3 memory: 2048 nameserver: 8.8.8.8 net0: name=eth0,bridge=vmbr1,firewall=1,gw=172.16.0.1,hwaddr=D6:60:C5:39:98:A0,ip=172.16.0.2/24,<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>=veth ostype: centos rootfs: <span class="hljs-built_in"><span class="hljs-built_in">local</span></span>:100/vm-100-disk-1.raw,size=10G swap: 512 unprivileged:</code> </pre><br>  Container successfully created.  You can connect to LXC containers via the pct enter command, I also added an SSH hypervisor key before installing to connect directly via SSH (there are some minor problems with the terminal display in PCT).  I prepared the server and installed all the necessary server applications there, now you can proceed to creating replication. <br><br>  We click on the LXC container and go to the ‚ÄúReplication‚Äù tab, where we create the replication parameter using the ‚ÄúAdd‚Äù button: <br><br><img src="https://habrastorage.org/webt/ub/ac/si/ubacsivqghyu5w9np8dlnjdqe3g.png"><br>  <b>Figure 12.</b> Creating replication in Proxmox interface <br><br><img src="https://habrastorage.org/webt/ea/mb/48/eamb489i0yqndxdcknvr2f1vefi.png"><br>  <b>Figure 13.</b> Replication job creation window <br><br>  I created the task to replicate the container to the second node, as seen in the following screenshot, the replication was successful - pay attention to the ‚ÄúStatus‚Äù field, it notifies you of the replication status, you should also pay attention to the ‚ÄúDuration‚Äù field to know how long the data is replicated. <br><br><img src="https://habrastorage.org/webt/wr/hd/t7/wrhdt7uk4szufdqrvboovxwr6t0.png"><br>  <b>Picture 14.</b> List of VM syncs <br><br>  Now we will try to migrate the car to the second node using the ‚ÄúMigrate‚Äù button <br><br>  The migration of the container begins, the log can be viewed in the task list - there will be our migration.  After that, the container will be moved to the second node. <br><br>  <b>‚ÄúHost Key Verification Failed‚Äù Error</b> <br><br>  Sometimes when setting up a cluster, a similar problem may arise - it prevents the machines from migrating and creating replication, which negates the advantages of cluster solutions.  To correct this error, delete the known_hosts file and connect via SSH to the conflicting node: <br><br><pre> <code class="bash hljs">/usr/bin/ssh -o <span class="hljs-string"><span class="hljs-string">'HostKeyAlias=proxmox2'</span></span> root@172.30.0.16</code> </pre><br>  Accept the Hostkey and try entering this command, it should connect you to the server: <br><br><pre> <code class="bash hljs">/usr/bin/ssh -o <span class="hljs-string"><span class="hljs-string">'BatchMode=yes'</span></span> -o <span class="hljs-string"><span class="hljs-string">'HostKeyAlias=proxmox2'</span></span> root@172.30.0.16</code> </pre><br><h3>  Features of network settings on Hetzner </h3><br>  Go to the Robot panel and click on the ‚ÄúVirtual Switches‚Äù button.  On the next page, you will see the Virtual Switch interface creation and management panel: you first need to create it, and then ‚Äúconnect‚Äù the dedicated servers to it.  In the search, we add the necessary servers to connect - they do not need to be rebooted, just have to wait up to 10-15 minutes when the connection to the Virtual Switch is active. <br><br>  After adding the servers to the Virtual Switch, we connect to the servers via the web panel and open the network interface configuration files, where we create a new network interface: <br><br><pre> <code class="bash hljs">auto enp4s0.4000 iface enp4s0.4000 inet static address 10.1.0.11/24 mtu 1400 vlan-raw-device enp4s0</code> </pre> <br>  Let's take a closer look at what it is.  At its core, this is a VLAN that connects to a single physical interface called enp4s0 (it may differ from you), with the VLAN number indicated ‚Äî this is the Virtual Switch number you created in the Hetzner Robot web panel.  You can specify any address, as long as it is local. <br><br>  I note that you should configure enp4s0 as usual, in fact, it should contain the external IP address that was issued to your physical server.  Repeat these steps on other hypervisors, and then reboot the networking service onto them, ping to the next node at the IP address of the Virtual Switch.  If the ping was successful, then you have successfully established a connection between the servers via the Virtual Switch. <br><br>  I will also attach the sysctl.conf configuration file, it will be needed if you have problems with forwarding packet and other network parameters: <br><br><pre> <code class="bash hljs">net.ipv6.conf.all.disable_ipv6=0 net.ipv6.conf.default.disable_ipv6 = 0 net.ipv6.conf.all.forwarding=1 net.ipv4.conf.all.rp_filter=1 net.ipv4.tcp_syncookies=1 net.ipv4.ip_forward=1 net.ipv4.conf.all.send_redirects=0</code> </pre><br>  <b>Adding an IPv4 Subnet to Hetzner</b> <br><br>  Before starting work, you need to order a subnet in Hetzner, you can do this through the Robot panel. <br><br>  Create a network bridge with an address that will be from this subnet.  Configuration example: <br><br><pre> <code class="bash hljs">auto vmbr2 iface vmbr2 inet static address ip-address netmask 29 bridge-ports none bridge-stp off bridge-fd 0</code> </pre> <br>  Now go to the virtual machine settings in Proxmox and create a new network interface that will be attached to the bridge vmbr2.  I use the LXC container, its configuration can be changed immediately in Proxmox.  Final configuration for Debian: <br><br><pre> <code class="bash hljs">auto eth0 iface eth0 inet static address ip-address netmask 26 gateway bridge-address</code> </pre> <br>  Please note: I have specified 26 masks, not 29 - this is required for the network to work on the virtual machine. <br><br>  <b>Adding an IPv4 Address to Hetzner</b> <br><br>  The situation with a single IP address is different - usually Hetzner gives us an additional address from the server's subnet.  This means that instead of vmbr2 we need to use vmbr0, but at the moment we don‚Äôt have one.  The bottom line is that vmbr0 must contain the IP address of the iron server (that is, use the address used by the physical network interface enp2s0).  The address must be moved to vmbr0, for this the following configuration will do (I advise you to order KVM in order to resume network operation in case of anything): <br><br><pre> <code class="bash hljs">auto enp2s0 iface enp2s0 inet manual auto vmbr0 iface vmbr0 inet static address ip-address netmask 255.255.255.192 gateway ip-gateway bridge-ports enp2s0 bridge-stp off bridge-fd 0</code> </pre><br>  Restart the server if possible (if not, restart the networking service), then check the network interfaces via ip a: <br><br><pre> <code class="bash hljs">2: enp2s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast master vmbr0 state UP group default qlen 1000 link/ether 44:8a:5b:2c:30:c2 brd ff:ff:ff:ff:ff:ff</code> </pre><br>  As you can see here, enp2s0 is connected to vmbr0 and does not have an IP address, since it was reassigned to vmbr0. <br><br>  Now in the settings of the virtual machine, add the network interface that will be connected to vmbr0.  As gateway, specify the address attached to vmbr0. <br><br><h3>  At the end </h3><br>  I hope that this article will be useful to you when you configure the Proxmox cluster in Hetzner.  If time permits, then I will expand the article and add instructions for OVH - there is also not everything obvious, as it seems at first glance.  The material turned out to be quite voluminous, if you find errors, then please write in the comments, I will correct them.  Thank you all for your attention. <br><br>  <i>Author: Ilya Andreev, edited by Alexey Zhadan and the Live Linux team</i> </div><p>Source: <a href="https://habr.com/ru/post/457894/">https://habr.com/ru/post/457894/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457876/index.html">Translation: IEEE 802.15.4z Standard. What awaits us in the future?</a></li>
<li><a href="../457884/index.html">Sovereign Online - Clarifying Orders</a></li>
<li><a href="../457886/index.html">Two-factor authentication on the site using a USB token. Now for Linux</a></li>
<li><a href="../457888/index.html">Mutation Testing: Testing Tests</a></li>
<li><a href="../457892/index.html">Professor who beat roulette</a></li>
<li><a href="../457896/index.html">Zimbra and server overload protection</a></li>
<li><a href="../4579/index.html">Digital TV: investments set the stage for growth</a></li>
<li><a href="../45790/index.html">Create your own magnetic prototype</a></li>
<li><a href="../457900/index.html">Federal Communications Commission Fights Meteorologists Again</a></li>
<li><a href="../457902/index.html">Mitap for Data Science</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>