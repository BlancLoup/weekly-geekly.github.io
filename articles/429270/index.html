<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Adult journalism: from Russia to the Kremlin</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Analysis of publications Lenta.ru for 18 years (from September 1999 to December 2017) using python, sklearn, scipy, XGBoost, pymorphy2, nltk, gensim, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Adult journalism: from Russia to the Kremlin</h1><div class="post__text post__text-html js-mediator-article"><p>  <i>Analysis of publications Lenta.ru for 18 years (from September 1999 to December 2017) using python, sklearn, scipy, XGBoost, pymorphy2, nltk, gensim, MongoDB, Keras and TensorFlow.</i> </p><br><p><img src="https://habrastorage.org/webt/lb/v5/f5/lbv5f5apvshbatlpergzpimrl3k.png"></p><br><p>  The study used data from the post " <a href="https://habr.com/post/343838/">Analyze this - Lenta.ru</a> " user <a href="https://habr.com/users/ildarchegg/" class="user_link">ildarchegg</a> .  The author has kindly provided 3 gigabytes of articles in a convenient format, and I decided that this is a great opportunity to test some text processing methods.  At the same time, if you're lucky, learn something new about Russian journalism, society and in general. </p><a name="habracut"></a><br><h4 id="soderzhanie">  Content: </h4><br><ul><li>  <a href="https://habr.com/ru/post/429270/">MongoDB to import json into python</a> </li><li>  <a href="https://habr.com/ru/post/429270/">Clearing and normalizing text</a> </li><li>  <a href="https://habr.com/ru/post/429270/">Tag Cloud</a> </li><li>  <a href="https://habr.com/ru/post/429270/">LDA-based thematic modeling</a> </li><li>  <a href="https://habr.com/ru/post/429270/">Popularity Prediction: XGBClassifier, LogisticRegression, Embedding &amp; LSTM</a> </li><li>  <a href="https://habr.com/ru/post/429270/">Explore Objects with Word2Vec</a> </li></ul><br><h4 id="MongoDB">  MongoDB to import json into python </h4><br><p>  Unfortunately, json with texts was a bit broken, uncritical for me, but python refused to work with the file.  Therefore, at first I imported into MongoDB, and only then through MongoClient from the pymongo library I loaded the array and saved it to csv piece by piece. </p><br><p>  From the comments: 1. I had to start the database with the sudo service mongod start command - there are other options, but they did not work;  2. mongoimport - a separate application, from the mongo console does not start, only from the terminal. </p><br><p>  The data gaps are evenly distributed over the years.  I do not plan to use the period of less than a year, I hope, it will not affect the correctness of the conclusions. </p><br><p><img src="https://habrastorage.org/webt/w2/yh/w-/w2yhw-uvllvzihu6n7p4jqppudo.png"></p><br><h4 id="">  Clearing and normalizing text </h4><br><p>  Before directly analyzing the array, you need to bring it to standard form: remove special characters, translate text into lower case (pandas string methods did a great job), remove stop words (stopwords.words ('russian') from nltk.corpus), return the words to normal form using lemmatization (pymorphy2.MorphAnalyzer). </p><br><p>  It was not without flaws, for example, Dmitry Peskov turned into ‚Äúdmitry‚Äù and ‚Äúsand‚Äù, but on the whole I was satisfied with the result. </p><br><h4 id="">  Tag Cloud </h4><br><p>  As a seed, let's see what publications are in the most general form.  We will display the 50 most frequent words that journalists used Tapes from 1999 to 2017, in the form of a tag cloud. </p><br><p><img src="https://habrastorage.org/webt/yx/zq/77/yxzq77iexwyali-vhggbrd2c8xc.png"></p><br><p>  ‚ÄúRia Novosti‚Äù (the most popular source), ‚Äúbillion dollar‚Äù and ‚Äúmillion dollar‚Äù (financial topics), ‚Äúpresent‚Äù (speech circulation, typical of all news sites), ‚Äúlaw enforcement agency‚Äù and ‚Äúcriminal case‚Äù (criminal news ), ‚ÄúPrime Minister‚Äù and ‚ÄúVladimir Putin‚Äù (politics) are quite expected style and themes for a news portal. </p><br><h4 id="">  LDA-based thematic modeling </h4><br><p>  We calculate the most popular topics for each year using LDA from gensim.  LDA (thematic modeling of the Dirichlet latent placement method) automatically reveals hidden topics (a set of words that occur together and most often) based on the observed word frequencies in the articles. </p><br><p>  The cornerstone of domestic journalism was Russia, Putin, the United States. </p><br><p>  In some years, this topic was diluted by the Chechen war (from 1999 to 2000), September 11 - in 2001, by Iraq (from 2002 to 2004).  From 2008 to 2009, the economy took the first place: interest, company, dollar, ruble, billion, million.  In 2011, they often wrote about Gaddafi. </p><br><p>  From 2014 to 2017  in Russia began and continues the years of Ukraine.  The peak came in 2015, then the trend began to decline, but still continues to hold high. </p><br><p><img src="https://habrastorage.org/webt/x5/ou/nb/x5ounbamg03xt43t74xjpv2veuo.png"></p><br><p>  Interesting, of course, but nothing that I wouldn‚Äôt know or guess about. </p><br><p>  Let's change a bit of the approach - let's highlight the top topics for all the time and see how their ratio has changed from year to year, that is, let's study the evolution of topics. </p><br><p>  The most interpreted option was Top-5: </p><br><ol><li>  Crime (male, police, occur, detain, policeman); </li><li>  Politics (Russia, Ukraine, President, USA, chapter); </li><li>  Culture (spinner, purulent, instagram, ramming - yes, this is our culture, although this topic turned out to be quite mixed); </li><li>  Sports (match, team, game, club, athlete, championship); </li><li>  Science (scientist, space, satellite, planet, cell). </li></ol><br><p>  Next, take each article and see with what probability it relates to a particular topic, as a result, all materials will be divided into five groups. </p><br><p>  The policy turned out the most popular - under 80% of all publications.  However, the peak of popularity of political materials was passed in 2014, now their share is declining, and the contribution to the information agenda of Crime and Sports is growing. </p><br><p><img src="https://habrastorage.org/webt/-6/ok/fz/-6okfzsonsezo8atfj0ip45s3qi.png"></p><br><p>  Check the adequacy of thematic models using the subheadings indicated by the editors.  The top sub-headings have been more or less correctly distinguished since 2013. </p><br><p><img src="https://habrastorage.org/webt/fo/k5/rb/fok5rbn-mvzpxgo9w28sytl5hai.png"></p><br><p>  There are no particular contradictions: the policy is stagnating in 2017, Football and the Accidents are growing, Ukraine is still in the trend, with a peak in 2015 </p><br><h4 id="">  Popularity Prediction: XGBClassifier, LogisticRegression, Embedding &amp; LSTM </h4><br><p>  Let us try to understand whether the text can predict the popularity of an article on the Ribbon, and on what this popularity generally depends.  As a target variable, I took the number of reposts on Facebook for 2017. </p><br><p>  3 thousand articles for 2017 did not have any repost on Fb - they were assigned the class ‚Äúunpopular‚Äù, 3 thousand materials with the largest number of reposts received the ‚Äúmost popular‚Äù label. </p><br><p>  The text (6 thousand publications for 2017) was divided into unograms and digrams (tokens, both single and two-word phrases) and a matrix was built, where the columns are tokens, rows are articles, and at the intersection is relative the frequency of words in the article.  Used functions from sklearn - CountVectorizer and TfidfTransformer. </p><br><p>  The prepared data were fed to the input XGBClassifier (classifier based on gradient boosting from the xgboost library), which after 13 minutes of searching the hyperparameters (GridSearchCV with cv = 3) produced an accuracy of 76% on the test. </p><br><p><img src="https://habrastorage.org/webt/3c/tb/cm/3ctbcmj-qlhaneknvvaegc8050g.png"></p><br><p>  Then I used the usual logistic regression (sklearn.linear_model.LogisticRegression) and after 17 seconds I got an accuracy of 81%. </p><br><p>  Once again, I am convinced that linear methods are best for classifying texts, provided that the data are carefully prepared. </p><br><p>  As a fashion, I tested a little neural networks.  Translated words into numbers using one_hot from keras, brought all articles to the same length (the pad_sequences function from keras) and fed LSTM (convolutional neural network, using TensorFlow backend) to the input through the Embedding layer (to reduce the dimension and speed up processing time). </p><br><p>  The network worked for 2 minutes and showed accuracy on the test of 70%.  It‚Äôs not the limit at all, but there‚Äôs no point in bothering much. </p><br><p>  In general, all methods gave relatively little accuracy.  Experience shows that classification algorithms work well with a variety of stylistics, - on author's materials, in other words.  Lenta.ru has such materials, but there are very few of them - less than 2%. </p><br><p><img src="https://habrastorage.org/webt/od/d_/mh/odd_mhkigi6sw5oub6s4ltsk3rc.png"></p><br><p>  The main array is written using neutral news vocabulary.  And the popularity of news is determined not by the text itself or even the topic as such, but by their affiliation to the upward informational trend. </p><br><p>  For example, quite a lot of popular articles cover events in Ukraine, the least popular of this topic almost do not concern. </p><br><p><img src="https://habrastorage.org/webt/pq/um/fq/pqumfqsa0w8bvl-ewjyoqzg5c2a.png"></p><br><h4 id="Word2Vec">  Explore Objects with Word2Vec </h4><br><p>  As a conclusion, I wanted to conduct a sentiment analysis - to understand how journalists are among the most popular objects that they mention in their articles, whether their attitude changes with time. </p><br><p> But I have no marked data, and the search for semantic thesauruses is unlikely to work correctly, since the news vocabulary is rather neutral, stingy with emotions.  Therefore, I decided to focus on the context in which the objects are mentioned. </p><br><p>  He took Ukraine (2015 vs 2017) and Putin (2000 vs 2017) as a test.  I chose the articles in which they are mentioned, translated the text into a multidimensional vector space (Word2Vec from gensim.models) and projected onto a two-dimensional one using the Main Components method. </p><br><p>  After the visualization of the pictures turned out to be epic, no less than the tapestry from Bayeux.  Cut out the necessary clusters to simplify perception, as I could, sorry for the "jackals." </p><br><p><img src="https://habrastorage.org/webt/5l/qv/si/5lqvsi1-fxwsfw7slm9vz_uioqs.png"><br><img src="https://habrastorage.org/webt/ue/lx/hs/uelxhs5y09xjqederrghlvsq-3c.png"></p><br><p>  What noticed. </p><br><p>  Putin of the 2000 model always appeared in the context of Russia and delivered his addresses personally.  In 2017, the President of the Russian Federation became a leader (whatever that means) and distanced himself from the country, now he is, judging by the context, a representative of the Kremlin, who communicates with the world through his press secretary. </p><br><p>  Ukraine-2015 in the Russian media - war, battles, explosions;  mentioned impersonal (Kiev declared, Kiev began).  Ukraine 2017 appears primarily in the context of negotiations between officials, and these individuals have specific names. </p><br><h3>  ... </h3><br><p>  It can take a long time to interpret the information received, but I think this is offtopic on this resource.  Those interested can look at their own.  Code and data attached. </p><br><p>  <a href="https://github.com/Myonin/silentio.su/blob/master/topic_model_texts_lenta_ru.ipynb">Link to the script</a> </p><br><p>  <a href="https://drive.google.com/file/d/1NlFuOjOt0oQ9Mx70Z7ZvfOsB3-1fCALp/view">Data Link</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/429270/">https://habr.com/ru/post/429270/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429260/index.html">Our way to the centralized storage of logs</a></li>
<li><a href="../429262/index.html">We invite you to the autumn DIYorDIE Meetup November 17</a></li>
<li><a href="../429264/index.html">Lithium-Ion UPS Time: Fire Hazard or a Safe Step into the Future?</a></li>
<li><a href="../429266/index.html">What salaries for IT specialists are offered by My Circle employers, data for May-October 2018</a></li>
<li><a href="../429268/index.html">Giant spider and minotaur on the streets of toulouse</a></li>
<li><a href="../429272/index.html">A brief snapshot of React and Redux functional web development: Chapter 1. Welcome to React</a></li>
<li><a href="../429274/index.html">Startup: how to live up to the round</a></li>
<li><a href="../429276/index.html">Variational autocoders: theory and working code</a></li>
<li><a href="../429278/index.html">Promising magnetic recording technology MAMR: what awaits us in the near future?</a></li>
<li><a href="../429280/index.html">As a vulnerability in REG.RU allowed to get registration data of any domain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>