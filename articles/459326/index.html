<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Auto-scaling and resource management in Kubernetes (review and video report)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On April 27, at the conference Strike 2019 , in the DevOps section, the report ‚ÄúAutoscaling and resource management in Kubernetes‚Äù was heard. It tells...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Auto-scaling and resource management in Kubernetes (review and video report)</h1><div class="post__text post__text-html js-mediator-article">  On April 27, at the conference <a href="https://nastachku.ru/archive/2019/index.php">Strike 2019</a> , in the DevOps section, the report ‚ÄúAutoscaling and resource management in Kubernetes‚Äù was heard.  It tells you how to use K8s to ensure high availability of applications and ensure their maximum performance. <br><br><img src="https://habrastorage.org/webt/ol/sv/vf/olsvvfwmfrorzavctm_ipdufuxo.jpeg"><br><br>  By tradition, we are glad to present a <a href="https://www.youtube.com/watch%3Fv%3D10ZR-fbyuSY%26list%3DPL1mJ-PkCYnmB9vljnjxCMP3dlxQY3Dfcq"><b>video with the report</b></a> (44 minutes, much more informative than the article) and the main squeeze in text form.  Go! <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's sort the topic of the report by words and start from the end. <br><br><h2>  Kubernetes </h2><br>  Let us have Docker-containers on the host.  What for?  To ensure repeatability and isolation, which in turn allows you to simply and well deploy, CI / CD.  We have a lot of such machines with containers. <br><br>  What does Kubernetes give in this case? <br><br><ol><li>  We stop thinking about these machines and start working with the ‚Äúcloud‚Äù, a <b>cluster of containers</b> or pods (groups of containers). </li><li>  Moreover, we do not even think about individual pods, but we manage even more groups.  Such <b>high-level primitives</b> allow us to say that there is a template for starting a certain workload, but the necessary number of instances to start it.  If we subsequently change the pattern, all instances will be changed. </li><li>  Using the <b>declarative API,</b> instead of executing a sequence of specific commands, we describe the ‚Äúworld device‚Äù (in YAML), which is created by Kubernetes.  And again: when the description is changed, its display will also change. </li></ol><br><h2>  Resource management </h2><br><h3>  CPU </h3><br>  Let us run on the server nginx, php-fpm and mysql.  These services will actually have even more working processes, each of which requires computational resources: <br><br><img src="https://habrastorage.org/webt/yu/v6/t8/yuv6t8a5q6txbhi25fe5mwv8f4k.png"><br>  <i>(the numbers on the slide are ‚Äúparrots‚Äù, the abstract need of each process for computing power)</i> <br><br>  To work with this, it is logical to combine the processes by groups (for example, all nginx processes into one ‚Äúnginx‚Äù group).  A simple and obvious way to do this is to place each group in a container: <br><br><img src="https://habrastorage.org/webt/yu/en/fr/yuenfrw6vzvexx1xrkvy3dfgw3o.png"><br><br>  To continue, you need to remember what a container is (in Linux).  Their appearance was made possible thanks to three key capabilities in the kernel that have been implemented for a long time: <a href="http://man7.org/linux/man-pages/man7/capabilities.7.html">capabilities</a> , <a href="http://man7.org/linux/man-pages/man7/namespaces.7.html">namespaces,</a> and <a href="http://man7.org/linux/man-pages/man7/cgroups.7.html">cgroups</a> .  Other technologies contributed to the further development (including convenient Docker type ‚Äúshells‚Äù): <br><br><img src="https://habrastorage.org/webt/f-/nf/ua/f-nfuaos1_9xdwyblt2em4yp-gs.png"><br><br>  In the context of the report, we are only interested in <b>cgroups</b> , because the control groups are the part of the functionality of containers (Docker, etc.) that implements resource management.  The processes combined into groups, as we wanted, are the control groups. <br><br>  Let's return to the CPU needs of these processes, and now - the process groups: <br><br><img src="https://habrastorage.org/webt/_s/cl/7v/_scl7v6nsak1ieo-dipaz9sgb_a.png"><br>  <i>(I repeat that all numbers are an abstract expression of the need for resources)</i> <br><br>  At the same time, the CPU itself has a finite resource <i>(in the example it is 1000)</i> , which everyone may lack (the sum of the needs of all groups is 150 + 850 + 460 = 1460).  What will happen in this case? <br><br>  The kernel begins to distribute resources and does it ‚Äúhonestly,‚Äù giving the same amount of resources to each group.  But in the first case there are more of them (333&gt; 150), so the surplus (333-150 = 183) remains in reserve, which is also equally distributed between two other containers: <br><br><img src="https://habrastorage.org/webt/nm/qv/te/nmqvteopou65lsc_ku2b0-qmbco.gif"><br><br>  As a result: the first container had enough resources, the second one was not enough, the third one was not enough.  This is the result of the actions of an <b>"honest" scheduler in Linux</b> - <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS</a> .  Its operation can be adjusted by assigning the <b>weight to</b> each of the containers.  For example: <br><br><img src="https://habrastorage.org/webt/z1/c_/_b/z1c__bumb8hy1k5zdrkcwo_aak0.gif"><br><br>  Let's look at the case of lack of resources in the second container (php-fpm).  All container resources are shared equally among the processes.  As a result, the master process works well, and all workers slow down, getting less than half of what they need: <br><br><img src="https://habrastorage.org/webt/9z/wi/d2/9zwid2g1znmdoslkl9bf9cogrnq.gif"><br><br>  This is how the CFS scheduler works.  Weights that we assign to containers will be called <b>requests</b> later.  Why so - see further. <br><br>  Take a look at the whole situation from the other side.  As you know, all roads lead to Rome, and in the case of a computer - to the CPU.  CPU alone, many tasks - you need a traffic light.  The easiest way to manage resources is ‚Äútraffic lights‚Äù: they gave one process a fixed access time to the CPU, then the next, and so on. <br><br><img src="https://habrastorage.org/webt/vf/af/qe/vfafqespiii6mnts87i4amfdyku.gif"><br><br>  This approach is called <i>hard limiting</i> .  Keep it simple as <b>limits</b> .  However, if you distribute limits to all containers, a problem arises: mysql was driving along the road and at some point his need for a CPU was over, but all other processes had to wait while the CPU was <b>idle</b> . <br><br><img src="https://habrastorage.org/webt/7o/1m/ps/7o1mps5khnoqkfywizrusxtug1q.png"><br><br>  Returning to the Linux kernel and its interaction with the CPU, the overall picture is as follows: <br><br><img src="https://habrastorage.org/webt/m7/xz/x8/m7xzx8brcdgbihu8qchb63rwjwc.png"><br><br>  Cgroup has two settings - in fact, these are two simple ‚Äútwists‚Äù that allow you to define: <br><br><ol><li>  container weight (request) is <b>shares</b> ; </li><li>  The percentage of total CPU time for working on container tasks (limits) is <b>quota</b> . </li></ol><br><h3>  What is the CPU measure? </h3><br>  There are different ways: <br><br><ol><li>  No one knows what <i>parrots</i> are - you need to negotiate each time. </li><li>  <i>The percentages are</i> clearer, but relative: 50% of the server with 4 cores and 20 cores are completely different things. </li><li>  You can use the already mentioned <i>weights</i> that Linux knows, but they are also relative. </li><li>  The most adequate option is to measure computing resources in <i>seconds</i> .  Those.  in seconds of processor time relative to real time seconds: given out 1 second of processor time to 1 real second - this is one whole CPU core. </li></ol><br>  To make it even easier to say, they began to measure directly in the <i>cores</i> , meaning that the very time of the CPU is relatively real.  Since Linux understands weights, and not such CPU time / cores, I needed a mechanism to translate from one to another. <br><br>  Consider a simple example with a server with 3 CPU cores, where three pods will select such weights (500, 1000 and 1500) that are easily converted into the corresponding parts of the cores allocated to them (0.5, 1 and 1.5). <br><br><img src="https://habrastorage.org/webt/mz/vl/1x/mzvl1xmzbtvlwgsqtf1-_n_rhns.png"><br><br>  If you take the second server, where there will be twice as many cores (6), and place the same pods there, the distribution of cores can be easily calculated by simple multiplication by 2 (1, 2 and 3, respectively).  But an important moment occurs when the fourth pod appears on this server, the weight of which, for convenience, is 3000. It takes some CPU resources (half of the cores), and they are recalculated from the other pods (halved): <br><br><img src="https://habrastorage.org/webt/p3/1t/nd/p31tndrejrchgwgbnpk53q5qnig.gif"><br><br><h3>  Kubernetes and CPU resources </h3><br>  In Kubernetes, CPU resources are usually measured in <b>milli-kernels</b> , i.e.  0.001 kernels are taken as the base weight.  <i>(The same thing in Linux / cgroups terminology is called CPU share, although, to be more precise, 1000 milliadr = 1024 CPU shares.)</i> K8s ensures that there are no more pods on the server than there are CPU resources for the sum of weights all pod'ov. <br><br>  How does this happen?  When adding a server to the Kubernetes cluster, it is reported how many CPU cores it has.  And when creating a new pod, the Kubernetes scheduler knows how many cores this pod will need.  Thus, pod will be defined on the server where there are enough cores. <br><br>  What happens if request is <b>not</b> specified (i.e., the number of cores it needs is not defined for pod)?  Let's see how Kubernetes considers resources at all. <br><br>  With pod, you can specify both requests (CFS scheduler) and limits (remember the traffic light?): <br><br><ul><li>  If they are equal, then the pod is assigned a QoS class <b>guaranteed</b> .  Such amount of kernels always available for it is guaranteed. </li><li>  If request is less than the limit, the QoS class is <b>burstable</b> .  Those.  we expect that pod, for example, always uses 1 core, however this value is not a restriction for it: <i>sometimes</i> pod can use more (when there are free resources on the server for this). </li><li>  There is also the <b>best effort</b> QoS class - those pods for which the request is not specified belong to it.  Resources are given to them last. </li></ul><br><h3>  Memory </h3><br>  With memory, the situation is similar, but slightly different - after all, the nature of these resources is different.  In general, the analogy is: <br><br><img src="https://habrastorage.org/webt/hw/zu/ja/hwzuja_vf0ojiz8uai-hhtn23ys.png"><br><br>  Let's see how requests are implemented in memory.  Let the pods live on the server, changing the consumed memory, until one of them becomes so large that the memory runs out.  In this case, the OOM killer appears and kills the largest process: <br><br><img src="https://habrastorage.org/webt/mg/i0/at/mgi0atkc5o5m0xo-crxt3augbnm.gif"><br><br>  It does not always suit us, so it is possible to regulate which processes are important for us and should not be killed.  To do this, use the <b>oom_score_adj</b> parameter. <br><br>  Let us return to the CPU QoS classes and draw an analogy with the oom_score_adj values, which determine memory consumption priorities for pods: <br><br><ul><li>  The lowest value of the pod's oom_score_adj - -998 - means that such a pod should be killed at the very last, <b>guaranteed</b> . </li><li>  The highest - 1000 is the <b>best effort</b> , such pods are killed before anyone else. </li><li>  To calculate the remaining values ‚Äã‚Äã( <b>burstable</b> ) there is a formula, the essence of which boils down to the fact that the more pod requested resources, the less chance that he will be killed. </li></ul><br><img src="https://habrastorage.org/webt/sc/yo/rm/scyorm9zwn_lltxminknyv-cbhy.png"><br><br>  The second "twist" - <b>limit_in_bytes</b> - for limits.  Everything is easier with it: we simply assign the maximum amount of memory to be issued, and here (unlike the CPU) there is no question what to measure its (memory). <br><br><h3>  Total </h3><br>  Each pod in Kubernetes is assigned <code>requests</code> and <code>limits</code> - both parameters for CPU and for memory: <br><br><ol><li>  based on requests, the Kubernetes scheduler works, which distributes the pods across servers; </li><li>  based on all parameters, the pod'a QoS class is determined; </li><li>  based on CPU requests, relative weights are calculated; </li><li>  based on CPU requests, a CFS scheduler is configured; </li><li>  based on the memory requests, the OOM killer is configured; </li><li>  based on CPU limits, the ‚Äútraffic light‚Äù is configured; </li><li>  based on the memory limits, the limit on the cgroup is configured. </li></ol><br><img src="https://habrastorage.org/webt/dr/1i/0_/dr1i0_troiqlcg4q_ki2fytefs0.png"><br><br>  In general, this picture answers all the questions about how the bulk of resource management in Kubernetes takes place. <br><br><h2>  Autoscale </h2><br><h3>  K8s cluster-autoscaler </h3><br>  Imagine that the entire cluster is already occupied and a new pod has to be created.  While the pod cannot appear, it hangs in <i>Pending</i> status.  To make it appear, we can connect a new server to the cluster or ... put a cluster-autoscaler to do it for us: order a virtual machine from the cloud provider (by API request) and connect it to the cluster, after which the pod will be added . <br><br><img src="https://habrastorage.org/webt/zu/va/dq/zuvadqhlpycxkqaw5q35bmr08_e.gif"><br><br>  This is the autoscaling of the Kubernetes cluster, which works great (in our experience).  However, as elsewhere, here is not without nuances ... <br><br>  While we were increasing the size of the cluster, everything was fine, but what happens when the cluster <b>becomes free</b> ?  The problem is that migrating pods (for freeing hosts) is very technically difficult and expensive in terms of resources.  A different approach works at Kubernetes. <br><br>  Consider a cluster of 3 servers in which there is a Deployment.  He has 6 pods: now it's 2 for each server.  We for some reason wanted to shut down one of the servers.  To do this, use the command <code>kubectl drain</code> , which: <br><br><ul><li>  prohibit sending new pods to this server; </li><li>  will remove existing pods on the server. </li></ul><br>  Since Kubernetes keeps track of the number of pods (6), he will simply <b>re-create</b> them on other nodes, but not on a disconnecting one, since it is already marked as inaccessible for hosting new pods.  This is the fundamental mechanics for Kubernetes. <br><br><img src="https://habrastorage.org/webt/l8/dw/jf/l8dwjfv1yyszva-knkk6p0hls_s.gif"><br><br>  However, there is a nuance.  In a similar situation for StatefulSet (instead of Deployment), the actions will be different.  Now we already have a stateful application - for example, three pods with MongoDB, one of which had some kind of problem (the data got corrupted or another error that does not allow the pod to start correctly).  And we again decide to disable one server.  What will happen? <br><br><img src="https://habrastorage.org/webt/a1/dx/ck/a1dxckkad2wpckrygftdzcjbuyq.gif"><br><br>  MongoDB <i>could</i> die, because it needs a quorum: for a cluster of three installations, at least two must function.  However, this <i>does not happen</i> - thanks to the <b>PodDisruptionBudget</b> .  This parameter defines the minimum required number of working pods.  Knowing that one of the pods with MongoDB is no longer working, and seeing that the MongoDB in PodDisruptionBudget is set to <code>minAvailable: 2</code> , Kubernetes will not allow to remove the pod. <br><br>  Bottom line: in order for the displacement (and in fact, re-creation) of pods to work correctly when the cluster is released, you need to configure the PodDisruptionBudget. <br><br><h3>  Horizontal scaling </h3><br>  Consider another situation.  There is an application running as Deployment in Kubernetes.  At its pods (for example, there are three of them) user traffic comes, and we measure a certain indicator in them (say, the load on the CPU).  When the load increases, we fix it on schedule and increase the number of pods for distribution of requests. <br><br>  Today in Kubernetes, this does not have to be done manually: an automatic increase / decrease in the number of pods is set up depending on the values ‚Äã‚Äãof the measured load parameters. <br><br><img src="https://habrastorage.org/webt/kj/fm/_t/kjfm_tu0u83c4mthfjayisabme0.gif"><br><br>  The main questions here are <b>what to measure</b> and <b>how to interpret</b> the values ‚Äã‚Äãobtained (to make a decision about changing the number of pods).  You can measure a lot: <br><br><img src="https://habrastorage.org/webt/h-/tw/a8/h-twa8kqe49av8gwxwxeoadyalc.png"><br><br>  How to do it technically - collect metrics, etc.  - I talked in detail in the report about <a href="https://habr.com/ru/company/flant/blog/412901/">Monitoring and Kubernetes</a> .  And the main advice for choosing the optimal parameters - <b>experiment</b> ! <br><br>  There is a <a href="http://www.brendangregg.com/usemethod.html">method USE</a> <i>(Utilization Saturation and Errors</i> ), the meaning of which is as follows.  Based on what does it make sense to scale, for example, php-fpm?  Based on the fact that workers are running out, this is <i>utilization</i> .  And if the workers have ended and new connections are not accepted, this is already the <i>saturation</i> .  Both of these parameters need to be measured, and depending on the values ‚Äã‚Äãand scaling. <br><br><h2>  Instead of conclusion </h2><br>  The report has a continuation: about vertical scaling and how to properly select resources.  I will tell you about this in future videos on <a href="https://www.youtube.com/c/%25D0%25A4%25D0%25BB%25D0%25B0%25D0%25BD%25D1%2582">our YouTube</a> - subscribe not to miss! <br><br><h2>  Video and slides </h2><br>  Video from the performance (44 minutes): <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/10ZR-fbyuSY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Presentation of the report: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/https://translate" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  PS </h2><br>  Other reports about Kubernetes in our blog: <br><br><ul><li>  ‚Äú <a href="https://habr.com/company/flant/blog/449096/">We are expanding and complementing Kubernetes</a> ‚Äù <i>(Andrey Polovov; April 8, 2019 at Saint HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/431500/">Databases and Kubernetes</a> ‚Äù <i>(Dmitry Stolyarov; November 8, 2018 in HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/412901/">Monitoring and Kubernetes</a> ‚Äù <i>(Dmitry Stolyarov; May 28, 2018 at RootConf)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/345116/">Best CI / CD practices with Kubernetes and GitLab</a> ‚Äù <i>(Dmitry Stolyarov; November 7, 2017 on HighLoad ++)</i> ; </li><li>  ‚Äú <a href="https://habr.com/company/flant/blog/331188/">Our experience with Kubernetes in small projects</a> ‚Äù <i>(Dmitry Stolyarov; June 6, 2017 at RootConf)</i> . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/459326/">https://habr.com/ru/post/459326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../459316/index.html">Hydra 2019: free broadcast of the first hall and a little about what will be at the conference</a></li>
<li><a href="../459318/index.html">TypeScript and short sprints. How we did the frontend interview variability tool</a></li>
<li><a href="../45932/index.html">Summarizing the experience of the three stages</a></li>
<li><a href="../459320/index.html">Kubernetes Operator in Python without frameworks and SDK</a></li>
<li><a href="../459322/index.html">Publisher Peter. Summer Sale</a></li>
<li><a href="../459328/index.html">Best in class in terms of price-quality ratio - Mpow A5 (059)</a></li>
<li><a href="../45933/index.html">26.8 million unique internet users per week</a></li>
<li><a href="../459330/index.html">Bitrix for the programmer and manager: love and hate</a></li>
<li><a href="../459334/index.html">YouTrack 2019.2: system-wide banner, improvements to the page with a list of tasks, new search options and more</a></li>
<li><a href="../459336/index.html">Live and learn. Part 1. School and vocational guidance</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>