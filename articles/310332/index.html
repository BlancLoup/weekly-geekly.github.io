<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DetectNet: Deep Neural Network for Object Detection at DIGITS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi Habr. Recently, I really like to read articles on the topic of deep learning, convolutional networks, image processing, etc. Indeed, there are very...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DetectNet: Deep Neural Network for Object Detection at DIGITS</h1><div class="post__text post__text-html js-mediator-article"><p>  <em>Hi Habr.</em>  <em>Recently, I really like to read articles on the topic of deep learning, convolutional networks, image processing, etc.</em>  <em>Indeed, there are very cool articles here that amaze and inspire you to your own "more modest" feats.</em>  <em>So, I want to bring to the attention of the Russian-speaking public a translation of an <a href="https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/">article</a> from Nvidia, written on August 11, 2016, in which their new tool, DIGITS and DetectNet, for detecting objects in images is presented.</em>  <em>The original article, of course, may seem at first a bit of advertising, and DetectNet does not represent anything ‚Äúrevolutionary‚Äù, but the combination of the DIGITS tool and the DetectNet network, it seems to me, can be interesting for everyone.</em> </p><br><p>  Today, using the <a href="https://developer.nvidia.com/digits">NVIDIA Deep Learning GPU Training System</a> (DIGITS), research analysts have at their disposal all the power of <em>deep learning</em> for solving the most common tasks in this area, such as: data preparation, definition of a convolutional network, parallel training of several models , observation of the learning process in real time, as well as the selection of the best model.  The fully interactive DIGITS tool relieves you from programming and debugging and you only do network design and training. </p><br><a name="habracut"></a><br><p>  Version DIGITS 4 introduces a new approach to object detection, which allows you to train networks to find objects (such as people, vehicles, or pedestrians) in images and define <em>bounding boxes</em> around objects.  Read the <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/">Deep Learning for Object Detection with DIGITS article</a> for more details on the method. </p><br><img src="https://habrastorage.org/getpro/habr/post_images/10d/0c7/44f/10d0c744f972b46b23dc78af962c0f8f.jpg" alt="Figure 1. The result of the DetectNet network for vehicle detection"><br><p>  <em>Figure 1. The result of the DetectNet network for vehicle detection</em> </p><br><p>  For quick mastering of the DIGITS method, the tool includes an illustrative example of a neural network model called DetectNet.  In fig.  Figure 1 shows the result of a DetectNet network trained to detect vehicles in aerial photography. </p><br><h3>  DetectNet data format </h3><br><p>  The input data of the training sample for the problem of classifying images are ordinary images (usually small and containing one object) and class labels (usually an integer class identifier or a string class name).  On the other hand, for the object detection problem, more information is needed for training.  The images from the input to the training sample for DetectNet are larger and contain several objects, and for each object in the image, the label must contain not only information about the class to which the object belongs, but also the location of the corners of its bounding box.  In this case, the naive choice of the label format with varying length and dimensionality leads to the fact that the definition of the <em>loss function</em> can be difficult, since the number of objects in the training image can vary. </p><br><p>  DetectNet solves this problem using a fixed three-dimensional label format, which allows you to work with images of any size and a different number of objects present.  This view of the input to DetectNet was "inspired" by the work of <a href="http://arxiv.org/abs/1506.02640">[Redmon et al.</a>  <a href="http://arxiv.org/abs/1506.02640">2015]</a> . </p><br><p>  In fig.  Figure 2 shows a diagram of image processing from a training sample with markup for DetectNet training.  At the beginning, a fixed grid with a size slightly smaller than the smallest object that we want to detect is superimposed on the original image.  Further, each lattice square is marked with the following information: the <em>class of the object</em> located in the lattice square, and <em>the pixel coordinates of the</em> corners of the bounding box relative to the center of the lattice square.  In the event that no object falls into the lattice square, then a special ‚Äúdontcare‚Äù class is used to preserve the fixed data format.  An additional value of "coverage" is also added to the input data format, taking the values ‚Äã‚Äã0 or 1 to indicate whether the object is present in the grid square or not.  In the case when several objects fall into one lattice square, DetectNet selects the object that occupies the largest number of pixels.  If the number of pixels is the same, then the object with the smallest ordinate ( <em>OY</em> ) of the bounding box is selected.  Such a selection of objects is not critical for aerial photography, but it makes sense for images with a horizon, for example, images from a DVR, where the object with the smallest ordinate of the bounding box is closer to the camera. </p><br><img src="https://habrastorage.org/getpro/habr/post_images/e73/e2e/a01/e73e2ea01516a487d8b27d1b76ed2f2c.png" alt="Figure 2. DetectNet input view"><br><p>  <em>Figure 2. DetectNet input view</em> </p><br><p>  Thus, the purpose of training the DetectNet network is to predict a similar presentation of data for a given image.  Or, in other words, DetectNet must predict for each lattice square whether an object is present in it, and also calculate the relative coordinates of the corners of the bounding box. </p><br><h3>  DetectNet network architecture </h3><br><p>  DetectNet's neural network has five parts, defined in the Caffe framework network model file.  In fig.  Figure 3 shows the architecture of the DetectNet network used during training.  It can be divided into 3 important processes: </p><br><ol><li>  Images and tags of the training set are fed to the input of the data layer.  Next, the on-the-fly conversion layer produces data addition. </li><li>  A fully convolutional network (FCN) produces feature extraction and prediction of object classes and bounding rectangles over lattice squares. </li><li>  The loss functions, at the same time, consider the error in the problems of predicting the coverage of the object and the corners of the bounding rectangles by the lattice squares. </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/169/a27/f78169a278aa0f01c5dd21fd653c24af.png" alt="Figure 3. DetectNet network structure for learning"><br><p>  <em>Figure 3. DetectNet network structure for learning</em> </p><br><p>  In fig.  Figure 4 shows the DetectNet architecture for verification, with two additional important processes: </p><br><ol><li>  Cluster the predicted bounding boxes for the final set. </li><li>  Calculate a simplified mAP (mean Average Precision) metric to measure the model's performance across the entire test sample. </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/b01/c03/13f/b01c0313f0b0ce8ddbd8e6cd0fda8aea.png" alt="Figure 4. DetectNet network structure for verification"><br><p>  <em>Figure 4. DetectNet network structure for verification</em> </p><br><p> You can change the size of the lattice square for the tutorial marks by setting the <em>stride</em> in pixels for the <code>detectnet_groundtruth_param</code> layer.  For example, </p><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">detectnet_groundtruth_param</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">stride</span></span>: <span class="hljs-number"><span class="hljs-number">16</span></span> scale_cvg: <span class="hljs-number"><span class="hljs-number">0.4</span></span> gridbox_type: GRIDBOX_MIN min_cvg_len: <span class="hljs-number"><span class="hljs-number">20</span></span> coverage_type: RECTANGULAR image_size_x: <span class="hljs-number"><span class="hljs-number">1024</span></span> image_size_y: <span class="hljs-number"><span class="hljs-number">512</span></span> obj_norm: true crop_bboxes: false }</code> </pre> <br><p>  In the parameters of this layer you can also specify the size of the training images <code>(image_size_x, image_size_y)</code> .  Thus, when these parameters are indicated, images that fall into the input of the DetectNet network during training are cropped at random by these sizes.  This can be useful if your training sample consists of very large images in which the objects to be detected are very small. </p><br><p>  The parameters of the layer that supplements the on-the-fly input data are defined in <code>detectnet_augmentation_param</code> .  For example, </p><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">detectnet_augmentation_param</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">crop_prob</span></span>: <span class="hljs-number"><span class="hljs-number">1.0</span></span> shift_x: <span class="hljs-number"><span class="hljs-number">32</span></span> shift_y: <span class="hljs-number"><span class="hljs-number">32</span></span> scale_prob: <span class="hljs-number"><span class="hljs-number">0.4</span></span> scale_min: <span class="hljs-number"><span class="hljs-number">0.8</span></span> scale_max: <span class="hljs-number"><span class="hljs-number">1.2</span></span> flip_prob: <span class="hljs-number"><span class="hljs-number">0.5</span></span> rotation_prob: <span class="hljs-number"><span class="hljs-number">0.0</span></span> max_rotate_degree: <span class="hljs-number"><span class="hljs-number">5.0</span></span> hue_rotation_prob: <span class="hljs-number"><span class="hljs-number">0.8</span></span> hue_rotation: <span class="hljs-number"><span class="hljs-number">30.0</span></span> desaturation_prob: <span class="hljs-number"><span class="hljs-number">0.8</span></span> desaturation_max: <span class="hljs-number"><span class="hljs-number">0.8</span></span> }</code> </pre> <br><p>  The data augmentation procedure plays an important role in successfully learning a highly sensitive and accurate object detector using DetectNet.  The parameters from <code>detectnet_augmentation_param</code> define various random transformations (offset, reflections, etc.) over the training set.  Such transformations of the input data lead to the fact that the network never processes the same image twice, and thus becomes more resistant to retraining and the natural change in the shape of objects from the test sample. </p><br><p>  The FCN subnetwork in DetectNet has a structure similar to GoogLeNet without an input data layer, a final <em>pool</em> layer and output layers <a href="">[Szegedy et al.</a>  <a href="">2014]</a> .  This approach allows DetectNet to use the already trained GoogLeNet model, reduce training time and improve the accuracy of the full model.  A fully convolutional network (FCN) is a convolutional neural network without fully connected layers.  This means that the network can receive images of various sizes as inputs and normally read the response using the sliding window technique in increments.  The output is a multidimensional array of real values ‚Äã‚Äãthat can be superimposed on the input image, similar to input labels and a square grid from DetectNet.  As a result, GoogLeNet without the final <em>pool</em> layer is a kind of convolutional neural network with a sliding window of size 555 x 555 pixels and a step of 16 pixels <a href="https://habr.com/ru/post/310332/">[1]</a> . </p><br><p>  DetectNet uses a linear combination of two independent loss functions to create the final loss function and optimization.  The first loss function <code>coverage_loss</code> is a quadratic error in all squares of the source data grid between the present and the predicted object coverage: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db6/f98/b6a/db6f98b6a7deaba9c86a46f2ab61b014.png"></div><br><p>  The second function of <code>bbox_loss</code> is the average error between the true and predicted angles of the bounding rectangle over all lattice squares: </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d0c/03a/dd5/d0c03add5ac988f9cb4bbe84f8162ed3.png"></div><br><p>  The Caffe framework minimizes the weighted sum of the values ‚Äã‚Äãof these loss functions. </p><br><h3>  DetectNet network output </h3><br><p>  The last layers of the DetectNet network filter and cluster the set of generated bounding boxes for grid squares.  To do this, use the <a href="http://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html">groupRectangles</a> algorithm from the OpenCV library.  The filtering of the bounding boxes is performed by the threshold method according to the value of the predicted coverage of the object.  The threshold value is set by the <code>gridbox_cvg_threshold</code> parameter in the DetectNet model prototxt file.  Clustering of bounding boxes is performed using the equivalence criterion of rectangles, which combines figures of similar location and size.  The similarity of rectangles is determined by the variable <code>eps</code> : at zero, the rectangles are not merged, and if the value tends to plus infinity, all the rectangles fall into one cluster.  After the rectangles are clustered, threshold filtering of small clusters occurs with the threshold specified by the <code>gridbox_rect_thresh</code> parameter, and the remaining clusters are considered to be middle rectangles, which are written to the output list.  The clustering method is implemented by a function in Python and is called in Caffe via the ‚ÄúPython Layers‚Äú interface.  The parameters of the <code>groupRectangles</code> algorithm are specified through the <code>cluster</code> layer in the DetectNet network model file. </p><br><p>  In DetectNet, the ‚ÄúPython Layers‚Äù interface is also used to calculate and display a simplified mean-metrics mAP (calculated from the final set of bounding boxes).  For the predicted and present bounding rectangles, the <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D1%258D%25D1%2584%25D1%2584%25D0%25B8%25D1%2586%25D0%25B8%25D0%25B5%25D0%25BD%25D1%2582_%25D0%2596%25D0%25B0%25D0%25BA%25D0%25BA%25D0%25B0%25D1%2580%25D0%25B0"><em>Intersection over Union (IoU)</em></a> value is calculated - the ratio of the intersection area of ‚Äã‚Äãthe rectangles to the sum of their areas.  When using a threshold value (default 0.7) for IoU, the predicted rectangle can be categorized as true positive or false positive predictions.  In the case when the IoU value for a pair of rectangles does not exceed the threshold value, the predicted rectangle falls into the category of false-negative predictions - the object was not detected.  Thus, DetectNet's simplified mAP metric is calculated as the product of <em>accuracy</em> (precision is the ratio of true-positive to the sum of true-positive and false-positive) by the <em>measure of completeness</em> (recall is the ratio of true-positive to the sum of true-positive and true-negative). </p><br><div class="spoiler">  <b class="spoiler_title">Note</b>  <b class="spoiler_title">per.</b>  <b class="spoiler_title">Useful image from Wikipedia about accuracy and completeness</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/8e0/caa/39e/8e0caa39ec8bc12f03fb4e059f47bfa0.png"></div></div><br><p>  This metric is a convenient characteristic of DetectNet's sensitivity to the detection of training sample objects, discarding false results, and the accuracy of the resulting bounding rectangles.  More information about the analysis of object detection errors can be found in the article <a href="http://dhoiem.cs.illinois.edu/publications/eccv2012_detanalysis_derek.pdf">[Hoiem et al.</a>  <a href="http://dhoiem.cs.illinois.edu/publications/eccv2012_detanalysis_derek.pdf">2012]</a> . </p><br><h3>  Learning effectiveness and results </h3><br><p>  The main advantage of the DetectNet network for the object detection problem is the efficiency with which objects are detected, and the accuracy of the generated bounding rectangles.  The presence of a fully convolutional network (FCN) allows DetectNet to be more effective in comparison with the use of a neural network based classifier on a sliding window.  This avoids unnecessary computation associated with overlapping windows.  This approach with a single neural network architecture is also simpler and more elegant for solving detection tasks. </p><br><p>  Learning the DetectNet network in DIGITS 4 with <a href="https://github.com/NVIDIA/caffe">Nvidia Caffe 0.15.7</a> and cuDNN RC 5.1 on a sample of 307 training and 24 test images, measuring 1536 x 1024 pixels, takes 63 minutes using a single Titan X graphics card. </p><br><p>  DetectNet's object detection time on images 1536 x 1024 pixels, with a grid size of 16 pixels, in the previous configuration (one TitanX, Nvidia Caffe 0.15.7, cuDNN RC 5.1) takes 41 ms (approximately 24 fps). </p><br><h3>  First steps with DetectNet </h3><br><p>  If you want to try DetectNet on your own data, you can download <a href="https://developer.nvidia.com/digits">DIGITS 4</a> .  A step-by-step demonstration of the process of detecting objects in DIGITS is presented <a href="https://github.com/NVIDIA/DIGITS/tree/master/examples/object-detection">here</a> . <br>  Read also the post <a href="https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits">Deep Learning for Object Detection with DIGITS</a> for an introduction to the method of using the object detection functionality in DIGITS 4. </p><br><p>  If you are interested in the pros and cons of various approaches using deep learning in the object detection task, see the <a href="http://on-demand.gputechconf.com/gtc/2016/video/S6389.html">Jon Barker</a> presentation <a href="http://on-demand.gputechconf.com/gtc/2016/video/S6389.html">at GTC 2016</a> </p><br><p>  Other in-depth training materials, including webinars, etc., can be found at the <a href="https://www.nvidia.com/dli">NVIDIA Deep Learning Institute</a> . </p><br><h4>  Links </h4><br><blockquote>  Hoiem, D., Chodpathumwan, Y., and Dai, Q. 2012. Diagnosing Error in Object Detectors.  Computer Vision - ECCV 2012, Springer Berlin Heidelberg, 340‚Äì353. <br><br>  Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. 2015. You Only Look Once: Unified, Real-Time Object Detection.  arXiv [cs.CV].  <a href="http://arxiv.org/abs/1506.02640">http://arxiv.org/abs/1506.02640</a> . <br><br>  Szegedy, C., Liu, W., Jia, Y., et al.  2014. Going Deeper with Convolutions.  arXiv [cs.CV].  <a href="">http://arxiv.org/abs/1409.4842</a> . </blockquote><br><h3>  Additional information and links from the translator </h3><br><p>  The original article can be found <a href="https://devblogs.nvidia.com/parallelforall/detectnet-deep-neural-network-object-detection-digits/">here</a> .  The DIGITS project is open source and can be found <a href="https://github.com/NVIDIA/DIGITS">here</a> .  The DetectNet prototxt file can be found <a href="https://github.com/NVIDIA/caffe/blob/caffe-0.15/examples/kitti/detectnet_network.prototxt">here</a> or as an image <a href="">here</a> . </p><br><h4>  About installing DIGITS </h4><br><p>  Detailed installation instructions can be found <a href="">here</a> . <br>  The DIGITS application needs Caffe to run it, or rather <a href="https://github.com/NVIDIA/caffe">NVidia fork</a> , where the Python layers needed for DetectNet are added.  This "no problem" fork can be installed on Mac OSX and Ubuntu.  With Windows, the problem is that the <em>windows</em> branch from BVLC / Caffe is missing from the fork, so as the authors themselves write: <a href="">DIGITS for Windows does not support DetectNet</a> .  Thus, on Windows, you can install BVLC / Caffe and run "standard" networks. </p><br><h4>  Notes </h4><br><blockquote><a name="ref1"></a>  [1] CNN with a receptive field of 555 x 555 pixels and a stride of 16 pixels. <br></blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/310332/">https://habr.com/ru/post/310332/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310320/index.html">The language of arithmetic expressions with the let construction as dsl on Kotlin</a></li>
<li><a href="../310324/index.html">Rag I limp or have power?</a></li>
<li><a href="../310326/index.html">UWP beginner: Window title in applications (VB.NET + C #)</a></li>
<li><a href="../310328/index.html">Database Performance Testing with tSQLt and SQLQueryStress</a></li>
<li><a href="../310330/index.html">Azure-IaaS-Digest number 10 (August-September)</a></li>
<li><a href="../310334/index.html">Introducing Apache Ignite: First Steps</a></li>
<li><a href="../310336/index.html">The main characteristics of a quality code</a></li>
<li><a href="../310338/index.html">About fundamental mistakes in the design of programming languages</a></li>
<li><a href="../310340/index.html">Hermitage - the solution to your problems with storing and processing images</a></li>
<li><a href="../310344/index.html">How I left InDesign for LaTeX</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>