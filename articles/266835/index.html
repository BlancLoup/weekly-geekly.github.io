<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Inventing servers - Open Compute Project</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Launched in 2011, the Facebook project called the Open Compute Project (OCP) involves the creation of open standards and hardware architectures for bu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Inventing servers - Open Compute Project</h1><div class="post__text post__text-html js-mediator-article">  Launched in 2011, the Facebook project called the <a href="http://opencompute.org/">Open Compute Project</a> (OCP) involves the creation of open standards and hardware architectures for building energy efficient and cost-effective data centers.  OCP began as a Facebook data center hardware project in Prineville, Oregon.  As a result, Facebook decided to make the architecture open, including server boards, power supplies, server chassis and racks.  The company released OCP specifications with recommendations for compact and energy-efficient rack-mount server architecture and cooling methods. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b64/2c3/8dc/b642c38dcd18434888d3a17fbba462b2.jpg"></div><br><br>  Under the cat, we will look at the details of what these servers consist of, how they work and what it gives. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a>  The Open Compute Project was born thanks to Facebook technical director <a href="http://www.youtube.com/watch%3Fv%3DVIWK_NyCO14">Frank Frankowski</a> .  It was he who launched the initiative that allowed the industry community not only to get acquainted with the Facebook data center project in Oregon, but also to take part in the further development of the new architecture.  The ultimate goal is to improve data centers, to form an ecosystem, to create more efficient and cost-effective servers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/037/7ee/9d9/0377ee9d94730a39af6b1abe17073b82.png"></div><br>  In general, the idea resembles the community of software developers Open Source, which creates and improves their products.  The project was so interesting that it was supported by large companies.  OCP has more than a hundred and fifty members. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86d/828/fbd/86d828fbdbe8331e37d0fc078a61063d.jpg"><br><br>  Server and storage architectures are created in accordance with <a href="http://www.opencompute.org/wiki/Server/SpecsAndDesigns">the</a> OCP Open Rack <a href="http://www.opencompute.org/wiki/Server/SpecsAndDesigns">specifications</a> , covering such hardware components as motherboards and power supply components.  The project also involves the development of standards, in particular, management standards.  Last year, OCP added new members.  Now IBM, Microsoft, Yandex, Box.net and many other well-known companies are participating in Open Compute. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cc/d11/43e/8ccd1143edf37f61cf58f7ab2783f601.jpg"><br><br><h2>  <font color="#ff6f00">Make it cheaper, and then you lose</font> </h2><br>  If the OCP architecture becomes the de facto standard for data centers, it can simplify system deployment and management.  But the main thing is the savings, allowing to provide customers with cheaper services and thereby win in a highly competitive market.  OCP aims to increase MTBF, increase server density, ease of maintenance with access from the cold corridor, improve energy efficiency, which is especially important for companies operating thousands of servers. <br><br>  For example, Facebook's data center in Oregon consumes 38% less electricity than other data centers of the company, it uses a non-killer adiabatic cooling system, and the PUE value reaches 1.07-1.08 (and this is without water cooling) while the industry average is about 1.5.  At the same time, capital expenditures were reduced by a quarter. <br><br>  Open Compute servers have a lightweight design and can operate at elevated temperatures.  They are much lighter, but larger than a normal server - the case height is 1.5U instead of 1U.  They have high radiators and more efficient fans.  The modular design of the Open Compute server simplifies access to all of its components ‚Äî processors, disks, network cards, and memory modules.  No tools are required to maintain it. <br><br>  By 2014, OCP specifications covered an entire pool of ‚Äúopen‚Äù hardware ‚Äî from servers to data center infrastructure.  The number of companies using OCP has also grown.  As it turned out, many OCP innovations are suitable not only for large data centers, but also for use in solutions for private / public clouds. <br><br>  This year, Facebook introduced several more developments within the OCP project.  In particular, Yosemite is working on a ‚Äúserver on a chip‚Äù jointly with Intel, while Accton and Broadcom are participating in the <a href="http://www.itnews.com.au/news/facebook-to-share-first-open-compute-switch-the-wedge-388499">Wedge switch</a> design project. <br><br><h2>  <font color="#ff6f00">From Freedom to Leopard</font> </h2><br>  The introduction of the technologies created within the project gave Facebook the opportunity to save more than $ 2 billion over three years.  However, it should be noted that such significant savings were achieved by optimizing the software, and for each type of application five typical platforms were developed. <br><br>  Today, these are basically variants of the Facebook Leopard platform on Xeon E5 processors.  It became the development of servers of previous generations, such as the <a href="http://www.datanami.com/2013/02/21/facebook_pushes_open_hardware_drive/">Windmill system</a> released in 2012 based on the Intel Sandy Bridge-EP processor and the AMD Opteron 6200/6300 processor. <br><br><table><tbody><tr><td colspan="7"><h3>  Generations of OCP servers from Facebook </h3></td></tr><tr><td><br></td><td>  Freedom (Intel) <br></td><td>  Freedom (AMD) <br></td><td>  Windmill (Intel) <br></td><td>  Watermark (AMD) <br></td><td>  Winterfell <br></td><td>  Leopard <br></td></tr><tr><td>  <b>Platform</b> <br></td><td>  Westmere-ep <br></td><td>  Interlagos <br></td><td>  Sandy Bridge-EP <br></td><td>  Interlagos <br></td><td>  Sandy Bridge-EP / Ivy-Bridge EP <br></td><td>  Haswell-ep <br></td></tr><tr><td>  <b>Chipset</b> <br></td><td>  5500 <br></td><td>  SR5650 / <br>  SP5100 <br></td><td>  C602 <br></td><td>  SR5650 / <br>  SR5670 / <br>  SR5690 <br></td><td>  C602 <br></td><td>  C226 <br></td></tr><tr><td>  <b>Models</b> <br></td><td>  X5500 / <br>  X5600 <br></td><td>  Opteron 6200/6300 <br></td><td>  E5-2600 <br></td><td>  Opteron 6200/6300 <br></td><td>  E5-2600 v1 / v2 <br></td><td>  E5-2600v3 <br></td></tr><tr><td>  <b>Sockets</b> <br></td><td>  2 <br></td><td>  2 <br></td><td>  2 <br></td><td>  2 <br></td><td>  2 <br></td><td>  2 <br></td></tr><tr><td>  <b>Thermopacket, W</b> <br></td><td>  95 <br></td><td>  85 <br></td><td>  115 <br></td><td>  85 <br></td><td>  115 <br></td><td>  145 <br></td></tr><tr><td>  <b>RAM to socket</b> <br></td><td>  3x DDR3 <br></td><td>  12x DDR3 <br></td><td>  8x DDR3 <br></td><td>  8x DDR3 <br></td><td>  8x DDR3 <br></td><td>  8x DDR4 <br>  / NVDIMM <br></td></tr><tr><td>  <b>~ Server node width (inches)</b> <br></td><td>  21 <br></td><td>  21 <br></td><td>  eight <br></td><td>  21 <br></td><td>  6.5 <br></td><td>  6.5 <br></td></tr><tr><td>  <b>Form Factor (U)</b> <br></td><td>  1.5 <br></td><td>  1.5 <br></td><td>  1.5 <br></td><td>  1.5 <br></td><td>  2 <br></td><td>  2 <br></td></tr><tr><td>  <b>Number of fans per node</b> <br></td><td>  four <br></td><td>  four <br></td><td>  2 <br></td><td>  four <br></td><td>  2 <br></td><td>  2 <br></td></tr><tr><td>  <b>Fan Size (mm)</b> <br></td><td>  60 <br></td><td>  60 <br></td><td>  60 <br></td><td>  60 <br></td><td>  80 <br></td><td>  80 <br></td></tr><tr><td>  <b>The number of compartments for disks (3.5 '')</b> <br></td><td>  6 <br></td><td>  6 <br></td><td>  6 <br></td><td>  6 <br></td><td>  one <br></td><td>  one <br></td></tr><tr><td>  <b>Disk interface</b> <br></td><td>  SATA II <br></td><td>  SATA II <br></td><td>  SATA III <br></td><td>  SATA III <br></td><td>  SATA III / RAID HBA <br></td><td>  SATA III <br>  / M.2 <br></td></tr><tr><td>  <b>The number of DIMM slots per socket</b> <br></td><td>  9 <br></td><td>  12 <br></td><td>  9 <br></td><td>  12 <br></td><td>  eight <br></td><td>  eight <br></td></tr><tr><td>  <b>DDRX generation</b> <br></td><td>  3 <br></td><td>  3 <br></td><td>  3 <br></td><td>  3 <br></td><td>  3 <br></td><td>  four <br></td></tr><tr><td>  <b>Ethernet</b> <br></td><td>  1 GbE fix <br></td><td>  2 GbE fix <br></td><td>  2 GbE fix  + PCIe mezzanine <br></td><td>  2 GbE fix <br></td><td>  1GbE fix  + 8x PCIe Mezzanine <br></td><td>  8x PCIe <br>  Mezzanine <br></td></tr><tr><td>  <b>Where deployed</b> <br></td><td>  Oregon <br></td><td>  Oregon <br></td><td>  Sweden <br></td><td>  Sweden <br></td><td>  Pennsylvania <br></td><td>  ? <br></td></tr><tr><td>  <b>PSU model</b> <br></td><td>  PowerOne SPAFCBK- 01G <br></td><td>  PowerOne SPAFCBK- 01G <br></td><td>  Powerone <br></td><td>  Powerone <br></td><td>  - </td><td>  - </td></tr><tr><td>  <b>PSU number</b> <br></td><td>  one <br></td><td>  one <br></td><td>  one <br></td><td>  one <br></td><td>  - </td><td>  - </td></tr><tr><td>  <b>PSU power (W)</b> <br></td><td>  450 <br></td><td>  450 <br></td><td>  450 <br></td><td>  450 <br></td><td>  - </td><td>  - </td></tr><tr><td>  <b>Number of nodes</b> <br></td><td>  one <br></td><td>  one <br></td><td>  2 <br></td><td>  2 <br></td><td>  3 <br></td><td>  3 <br></td></tr><tr><td>  <b>BMC</b> <br></td><td>  No (Intel RMM) <br></td><td>  Not <br></td><td>  No (Intel RMM) <br></td><td>  Not <br></td><td>  No (Intel RMM) <br></td><td>  Yes (Aspeed AST1250 <br>  w 1GB <br>  Samsung DDR3 DIMM K4B1G1646G- BCH9) <br></td></tr></tbody></table><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/34b/71c/58f/34b71c58f82ec58a0ac206f28a4bec66.jpg"></div><br>  One of the problems with Freedom servers is the lack of a backup power supply unit PSU.  Adding a PSU to each server would mean an increase not only of CAPEX, but also of OPEX, since in active / passive mode the passive PSU still consumes electricity. <br><br>  It was logical to group the power supply of several servers in the chassis.  This is reflected in the architecture of the racks Open Rack v1, where the power supplies are located on the "shelves" of 12.5V DC, supplying the "zones" of 4.2 kW each. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b6a/b61/b3e/b6ab61b3e21c7845b868631fea417e86.png"></div><br>  Each zone has its own power shelf (height 3OU, OpenUnits, 1OU = 48 mm).  Power supplies are reserved under the 5 + 1 scheme and occupy a total of 10OU per rack.  When the power consumption is low, part of the PSU is automatically turned off, allowing the rest to work with optimal load. <br><br>  Improvements touched and power distribution system.  There are no power cables to disconnect each time a server is serviced.  Power is supplied by vertical power tires in each zone.  When the server moves into the rack, a power connector is plugged into the back of it.  A separate 2OU compartment is allocated for switches. <br><br>  <a href="http://www.opencompute.org/wiki/Open_Rack">Open Rack</a> specifications suggest the creation of racks 48U high, which contributes to the improvement of air circulation in the equipment and simplifies the access to equipment for technicians.  The Open Rack's rack width is 24 inches, but the equipment bay is 21 inches wide ‚Äî 2 inches wider than a regular rack.  This allows you to install three motherboards in the chassis or five 3.5-inch disks. <br><br><h2>  <font color="#ff6f00">OCP Knox and others</font> </h2><br>  Open Rack v1 required new server design.  Using the Freedom chassis without a PSU left a lot of empty space, and simply filling it with a 3.5 "HDD would be wasteful, and for most of the Facebook loads, so many disks were not required. A solution similar to power supplies was chosen. The disks were grouped and moved out of the server nodes. So the Knox storage system was born. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/185/c43/43f/185c4343f995d37a7d6a4fe7eab9d64e.jpg"></div><br>  Generally speaking, OCP Knox is a regular JBOD disk shelf, created under Open Rack.  HBA neighboring Winterfell server nodes are connected to it.  It differs from the standard 19 "constructive in that it can hold 30 3.5" disk drives and is very easy to maintain.  To replace the disc, the tray moves forward, the corresponding compartment opens, the disc is replaced, and everything moves back. <br><br>  Seagate has developed its own ‚ÄúEthernet storage device‚Äù specification, known as Seagate Kinetic.  These drives were an object storage that is connected directly to the data network.  A new <a href="http://www.rnt.de/en/open-bigfoot-storage-object.html">BigFoot Storage Object Open</a> chassis was also developed with these disks and 12 10GbE ports in the 2OU package. <br><br>  On Facebook, for a similar purpose, they created the Honey Badger system, a Knox modification for storing images.  It is equipped with Panther + compute nodes based on the Intel Avoton SoC (C2350 and C2750) with four DDR3 SODIMM slots and mSATA / M.2 SATA3 interfaces. <br><br>  Such a system can work without a head node - usually Winterfell servers (Leopard servers are not planning to use Facebook from Knox).  A slightly modified version of Knox was used as archive storage.  Disks and fans in it are started only when it is required. <br><br>  Another version of the archival system from Facebook using OpenRack intervenes 24 stores with 36 cartridge-containers, 12 Blu-ray discs in each.  That is, the total capacity reaches 1.26 PB.  And stored Blu-ray discs up to 50 years or more.  The system works like a jukebox. <br><br><h2>  <font color="#ff6f00">Winterfell Servers</font> </h2><br>  The Freedom chassis without a PSU is essentially just a motherboard, a fan, and a boot disk.  Facebook engineers have created a more compact form factor - Winterfell.  It resembles the Supermicro dual server node, but three such nodes can be placed on the ORv1 shelf.  One 2OU Winterfell node contains a modified Windmill motherboard, a power bus connector and a backplane for connecting power cables and fans to the motherboard.  You can install a full-size x16 PCIe card and a x8 half-size card, as well as a x8 PCIe network interface card on the motherboard.  The boot disk is connected via SATA or mSATA. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5e6/47e/673/5e647e673bb503ffe6a180988cfeae6c.png"><br><br><h2>  <font color="#ff6f00">Open Rack v2</font> </h2><br>  In the process of deploying ORv1, it became clear that the three power zones with three tires each are redundant - so much power is simply not required.  There is a new version - Open Rack v2 with two power zones instead of three and one bus per zone.  And the height of the compartment for switches has grown to 3OU. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8cf/caa/4fa/8cfcaa4fa3dedd77fe042cb67a2097f5.png"><br><br>  Changes in nutrition led to incompatibility with Winterfell, so a new project appeared - Project Cubby.  Cubby is in fact a sort of Supermicro TwinServer chassis, but instead of the two PSUs built into the server module, the power bus is used.  This design will use three power supply units (2 + 1) 3.3 kW per supply area instead of six.  Each power zone provides 6.3 kW of power.  The bottom of the rack can contain three batteries - Battery Backup Units (BBU) in case of a power failure. <br><br><h2>  <font color="#ff6f00">Leopard Servers</font> </h2><br>  So, Leopard.  This is the latest Windmill update with the Intel C226 chipset and support for up to two E5-2600v3 Haswell Xeon processors. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/010/089/dc3/010089dc334dd323147513c38fc7b8a5.jpg"><br><br>  Enlarged CPU heatsinks and good airflow allow you to use processors with a thermal pack up to 145 W, that is, the entire Xeon family, excluding the 160-watt E5-2687W v3.  Each processor has 8 DIMM channels available, and DDR4 allows <b>in the future to</b> use 128 GB memory modules, which will give up to 2 TB of RAM - more than enough for Facebook.  You can also use NVDIMM modules (DIMM flash memory) and Facebook is testing this option. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bf4/37f/af7/bf437faf7004b8535b3ae493bcbc3df7.jpg"><br><br>  Other changes include the absence of an external PCIe connector, support for a mezzanine card with two QSFP +, an mSATA / M.2 slot for SATA / NVMe drives and 8 more PCIe lanes for an additional card - there are only 24 of them. There is no SAS connector - Leopard is not used as head node for Knox. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cc4/136/f54/cc4136f5475401d85374f72b17b3fdb9.jpg"><br><br>  An important addition is the control controller (Baseboard Management Controller, BMC).  This is an Aspeed AST1250 controller with IPMI and Serial Over Lan access.  BMC allows you to remotely update CPLD, VR, BMC and UEFI firmware.  Power control for PSU load control is also provided. <br><br><h2>  <font color="#ff6f00">Certified Solutions</font> </h2><br>  OCP equipment is usually made to order.  But there are also "retail" versions of Leopard.  Manufacturers offer their modifications.  Examples include cloud-based Quanta QCT servers and WiWynn systems with an increased number of disks.  HP, Microsoft and Dell also did not stand aside. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/45d/ea4/93c/45dea493c6d4d9019aaec299a166dd82.png"><br><br>  With the increase in the number of Open Compute equipment suppliers, it has become necessary to ensure that they follow the accepted specifications.  There are two <a href="http://www.opencompute.org/wiki/Certification_FAQ">certifications</a> - OCP Ready and OCP Certified.  The first means that the equipment meets specifications and can operate in an OCP environment.  The second is assigned to special testing organizations.  There are just two of them - at the University of Texas at San Antonio ( <a href="http://www.utsa.edu/">UTSA</a> ) in the USA and at the Industrial Technology Research Institute ( <a href="https://www.itri.org.tw/eng/">ITRI</a> ) in Taiwan.  <a href="http://www.wiwynn.com/">WiWynn</a> and <a href="http://www.quantatw.com/Quanta/english/product/subsidiary_qct.aspx">Quanta QCT</a> were the first vendors to certify their equipment. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a5/07b/949/3a507b94925520a6dfcc18c20dfd950d.jpg"><br><br>  OCP innovations are gradually being introduced in data centers and standardized, becoming available to a wide range of customers. <br>  Open technologies allow customers to offer any combination of compute nodes, storage systems and switches in a rack, to use ready-made or own components.  At the same time there is no hard link to the switching systems - you can use switches from any vendor. <br><br><h2>  <font color="#ff6f00">Microsoft Innovation</font> </h2><br>  Microsoft presented detailed specifications of its Open Compute servers and even revealed the source code of infrastructure management software with server diagnostics, cooling monitoring and power functions.  She contributed to OCP by developing the Open Cloud Server architecture.  These servers are optimized to work with Windows Server and are built in accordance with the high requirements for availability, scalability and efficiency, which makes the Windows Azure cloud platform. <br><br>  According to Microsoft, the server cost has been reduced by almost 40%, energy efficiency has increased by 15%, and infrastructure can be deployed 50% faster. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cc0/b31/ad1/cc0b31ad1eb38e713e7079b1b4169ee1.png" align="left">  At the Open Compute Project (OCP) Summit forum in the US, Microsoft showed another interesting development - the distributed UPS technology called <a href="http://blogs.technet.com/b/server-cloud/archive/2015/03/10/microsoft-reinvents-datacenter-power-backup-with-new-open-compute-project-specification.aspx">Local Energy Storage (LES)</a> .  It is a combination of power module and battery compatible with the Open CloudServer chassis (OCS) v2.  New LES modules are interchangeable with previous PSUs.  Depending on the data center topology and power backup requirements, you can choose which type of PSU to use.  Usually, UPSs are placed in a separate room, and lead batteries are used to back up power to IT equipment.  For several reasons, this solution is ineffective.  Large areas are occupied, energy is lost due to AC / AC and AC / DC conversions (direct / alternating current).  Double conversion and battery charging increases the data center PUE by up to 17%.  Reliability decreases, operating costs increase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2ad/63f/9ca/2ad63f9caa7356254aadc57071ad471d.jpg"><br><br>  Switching to adiabatic cooling can reduce costs and simplify operations. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e19/daf/d27/e19dafd27133c03c286a9bdc58720800.png"><br><br>  But how to improve the power distribution system and UPS?  Is it possible to radically simplify everything here?  Microsoft decided to abandon a separate room for UPS and move power modules closer to the IT load, at the same time integrating the battery system with IT management.  So LES appeared. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/695/458/eca/695458eca789890570dfc037dd134331.png"><br><br>  In the LES topology, the PSU design was changed - components such as batteries, a battery management controller, and a low-voltage charger were added. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8c1/60e/812/8c160e812ddc662f11976b9c939764ab.png"><br><br>  Batteries are used lithium-ion, as in electric transport.  Thus, the LES developers took the standard PSU elements, conventional batteries, and connected them in one module.  What does this give?  According to Microsoft: <br><br><ul><li>  Up to five times the cost is reduced compared to traditional UPS, the power supply system in the data center is greatly simplified, and the function of energy storage is performed by commercially available batteries. <br></li><li>  Moving the battery to the server eliminates the 9% loss typical of conventional UPS systems.  In lithium-ion batteries, only 2% is lost in charge, while in lead batteries - up to 8% and 1% for power supply.  As a result, PUE decreases. <br></li><li>  The data center areas are reduced by 25%, and this is a radical saving in capital costs. <br></li><li>  Maintenance is greatly simplified - LES modules are easily replaced, no acid.  The consequences of failure are minimized and localized. <br></li></ul><br>  Innovations of the Open Compute Project are changing the data center and cloud services market, standardizing and cheapening the development of server solutions.  This is just one of many examples.  In the next series of materials you will learn about other interesting solutions. <br><br><img src="https://habrastorage.org/files/6a0/635/cc7/6a0635cc7a454f06b35dd766d27374a2.png" align="left">  We at <a href="http://www.hostkey.ru/">Khostkay</a> also make the servers themselves for renting them out as dedicated servers - traditional solutions do not allow technologically to offer low-budget machines to customers.  After many attempts, the 4U platform codenamed <a href="https://www.dropbox.com/s/ni37nja0fq3z8zk/aero10.pdf%3Fdl%3D0">Aero10</a> went into the preliminary series: the solutions on this platform fully satisfy the need for <a href="http://www.hostkey.ru/dedicated/ru-micro/">microservers</a> on 2-4 core processors. <br><br><img src="https://habrastorage.org/files/462/2ef/0c6/4622ef0c681c4aeaa35953c17229b505.jpg"><br><br>  We use traditional mini-ITX motherboards, the rest of the development is completely ours - from the case to power distribution electronics and control circuits - everything is done locally in Moscow.  We still use Chinese power supplies - <a href="http://www.meanwell.com/search/RSP-1000/rsp-1000-spec.pdf">MeanWell RSP1000-12</a> at 12V with load sharing and hot swapping. <br><br>  The platform implements dedicated micro-servers on Celeron J1800 2x2.4Ghz, Celeron J1900 4x2.0Ghz, i3-4360 2x3.7Ghz and flagship i7-4790 4x3.6Ghz processors.  In the middle segment, we make servers based on the E3-1230v3 with a remote control module on the same platform using <a href="https://market.yandex.ru/product/10532300%3Fncrnd%3D2854">ASUS P9D-I</a> motherboards. <br><br>  Using this solution allows us to reduce the cost of capital expenditures by up to 50%, save up to 20-30% of electricity and develop this solution further.  All this allows us to offer our clients competitive prices that are possible on the Russian market in the absence of crediting, leasing, installments for 3 years and other instruments available to <a href="http://www.hostkey.ru/dedicated/euro/new/">us in the Netherlands</a> . <br>  The project already has a solution for 19 blades mini-ITX in the same 4U and we are constantly working to improve the power and cooling system.  In the future, for the operation of such servers, a traditional data center will not be required. <br>  In the near future we will tell about this platform and our other developments in detail, subscribe and follow the news. </div><p>Source: <a href="https://habr.com/ru/post/266835/">https://habr.com/ru/post/266835/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266823/index.html">At the junction of ERP and ECM: how we automated the logistics process</a></li>
<li><a href="../266827/index.html">New Russian K-Meleon 75 for Real Pro</a></li>
<li><a href="../266829/index.html">Paradigm of situationally oriented programming</a></li>
<li><a href="../266831/index.html">PE (Portable Executable): On Stranger Tides</a></li>
<li><a href="../266833/index.html">15 year old code and newspaper ad</a></li>
<li><a href="../266837/index.html">Testing on Android: Robolectric + Jenkins + Ja–°o–°o</a></li>
<li><a href="../266839/index.html">Working with lighting in Unity - theory and practice</a></li>
<li><a href="../266843/index.html">Preparing Nexus Player (FUGU) for working with SoCWatch</a></li>
<li><a href="../266845/index.html">Armory 1.2: Workspaces and switching between projects</a></li>
<li><a href="../266847/index.html">Mikrotik: small utility. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>