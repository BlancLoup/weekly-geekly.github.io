<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Transformer - new neural network architecture for working with strings</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Necessary preface: I decided to try the modern format of carrying light to the masses and try to stream on YouTube about deep learning. 


 In particu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Transformer - new neural network architecture for working with strings</h1><div class="post__text post__text-html js-mediator-article"><p>  Necessary preface: I decided to try the modern format of carrying light to the masses and try to stream on YouTube about deep learning. </p><br><p>  In particular, at some point I was asked to tell about attention, and for this you need to tell about machine translation, and about sequence to sequence, and about applying to pictures, and so on.  The result was such a stream for an hour: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/k63pDjKV3Ew" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  I understand from other posts that c video is taken to post his transcript.  Let me better talk about what is not in the video - about the new neural network architecture for working with sequences based on attention.  And if you need an additional background about machine translation, current approaches, where attention, etc. came from, you watch the video, okay? </p><br><p>  The new architecture is called Transformer, it was developed in Google, described in the article Attention Is All You Need ( <a href="https://arxiv.org/abs/1706.03762">arxiv</a> ) and there is a <a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html">post</a> about it <a href="https://research.googleblog.com/2017/08/transformer-novel-neural-network.html">on the Google Research Blog</a> (not very detailed, but with pictures). </p><br><p>  Go. </p><a name="habracut"></a><br><h1 id="sverh-kratkoe-soderzhanie-predyduschih-seriy">  Over-summary of previous episodes </h1><br><p>  The task of machine translation in deep learning comes down to working with sequences (as well as many other tasks): we train a model that can receive a sentence as an input for a sequence of words and produce a sequence of words in another language.  In the current approaches, inside the model there is usually an encoder and a decoder - the encoder converts the words of the input sentence into one or more vectors in a certain space, the decoder - generates a sequence of words in another language from these vectors. <br>  The standard architectures for an encoder are RNN or CNN, for a decoder it is most often RNN.  Further development has put the mechanism of attention on this scheme and about it is better to see stream. </p><br><p>  And now a new architecture is proposed to solve this problem, which is neither RNN nor CNN. </p><br><p><img src="https://habrastorage.org/webt/59/f0/44/59f04410c0e56192990801.png"><br>  Here is the main picture.  What's in it that! </p><br><h1 id="enkoder-i-multi-head-attention-layer">  Encoder and Multi-head attention layer </h1><br><p>  Let us first consider an encoder, that is, the part of the network that receives words at the input and outputs some embeddings that correspond to the words that will be used by the decoder. </p><br><p>  Here it is specifically: <br><img src="https://habrastorage.org/webt/59/f0/44/59f0440f9359b881372094.png"><br>  The idea is that each word in parallel passes through the layers shown in the picture. <br>  Some of them are standard fully-connected layers, some are shortcut connections like in ResNet (where in the Add picture). </p><br><p>  But the new interesting thing in these layers is Multi-head attention.  This is a special new layer that allows each input vector to interact with other words through the attention mechanism, instead of transmitting the hidden state as in RNN or neighboring words as in CNN. </p><br><p><img src="https://habrastorage.org/webt/59/f0/44/59f0440f1109b864893781.png"><br>  Query vectors are given as input, and several Key and Value pairs (in practice, Key and Value are always the same vector).  Each of them is transformed by a learning linear transformation, and then the scalar product Q is calculated with all K in turn, the result of these scalar products is run through softmax, and with the resulting weights, all the vectors V are summed into a single vector.  This wording of attention is very close to previous works where attention is used. </p><br><p>  The only thing that they complement it is that there are several such interests in parallel (their number in the picture is indicated by h), i.e.  several linear transformations and parallel scalar products / weighted sums.  And then the result of all these parallel interests is concatenated, once again it is run through the learning linear transformation and goes to the output. <br>  But in general, each such module receives as input a Query vector and a set of vectors for Key and Value, and returns one vector of the same size as each of the inputs. </p><br><blockquote>  <em>It is not clear what it gives.</em>  <em>In standard attention, ‚Äúintuition‚Äù is clear - the network ‚Äúattention‚Äù tries to match one word to another in a sentence if they are close to something.</em>  <em>And this is one network.</em>  <em>Here, the same thing, but a bunch of networks in parallel?</em>  <em>And they do the same thing, but the output concludes?</em>  <em>But then what is the point, will they learn exactly the same thing?</em> <br>  Not.  If there is a need to pay attention to several aspects of words, then this gives the network the opportunity to do this. <br>  Such a trick is used quite often - it turns out that there are enough stupidly different initial random weights to push different layers in different directions. <br><br>  <em>What are several aspects of words?</em> <br>  For example, a word has features about its meaning and pro grammatical. <br>  I would like to get vectors corresponding to the neighbors in terms of the semantic component and the grammatical one. </blockquote><p>  Since the output of such a block produces a vector of the same size as it was at the input, this block can be inserted into the network several times, adding network depth.  In practice, they use a combination of Multi-head attention, the residual layer and the fully-connected layer 6 times, then this is such a deep enough network. </p><br><p>  The last thing to say is that one of the features of each word is positional encoding ‚Äî that is,  his position in the sentence.  For example, from this, when processing words, it is easy to ‚Äúpay attention‚Äù to adjacent words if they are important. <br>  They use as a feature a vector of the same size as the word vector, and which contains sine and cosine from a position with different periods, so that they say it is easy to pay attention to the relative offset by choosing a coordinate with the desired period. <br>  Tried instead, embeddingings of the positions were also taught and it turned out the same thing as with the sinuses. </p><br><p>  They also have LayerNormalization ( <a href="https://arxiv.org/abs/1607.06450">arxiv</a> ) <a href="https://arxiv.org/abs/1607.06450">stuck</a> .  This is the normalization procedure, which normalizes the outputs from all the neurons in the trainer inside each sample (unlike each neuron, separately inside the batch, as in Batch Normalization, apparently because they did not like BN). </p><br><blockquote>  <em>They say that BN in recurrent networks does not work, since the normalization statistics for different sentences of one batch only spoils everything, but does not help, because the sentences are all of different lengths and all that.</em>  <em>In this architecture, too, is this effect expected and harmful to BN?</em> <br>  Why they did not take BN is an interesting question, the article does not particularly comment.  It seems to have been successful attempts to use with RNN for example in speech recognition.  Deep Speech 2 by Baidu ( <a href="https://arxiv.org/abs/1512.02595">arxiv</a> ), AFAIR </blockquote><p>  Let's try to summarize the work of the encoder on points. </p><br><p>  Encoder operation: </p><br><ol><li> Embeddingings are made for all words of a sentence (a vector of the same dimension).  For example, let it be a sentence <code>I am stupid</code> .  In the embedding is added the position of the word in the sentence. </li><li>  The vector of the first word and the vector of the second word ( <code>I</code> , <code>am</code> ) are taken, fed to a single-layer network with one output, which gives the degree of their similarity (scalar value).  This scalar value is multiplied by the vector of the second word, receiving its some "weakened" by the value of similarity copy. </li><li>  Instead of the second word, the third word is submitted and the same thing is done as in paragraph 2.  with the same network with the same weights (for vectors <code>I</code> , <code>stupid</code> ). </li><li>  By doing the same for all the remaining words of the sentence, their "weakened" (weighted) copies are obtained, which express the degree of their similarity to the first word.  Then these all weighted vectors are added to each other, getting one resultant vector of dimension of one embedding: <br> <code>output=am * weight(I, am) + stupid * weight(I, stupid)</code> <br>  This is the mechanism of the "usual" attention. </li><li>  Since the assessment of the similarity of words in just one way (according to one criterion) is considered insufficient, the same thing (p.2-4) is repeated several times with other weights.  Type one one attention can determine the similarity of words on the semantic load, the other on grammatical, the rest somehow, etc. </li><li>  At the exit of p.5.  several vectors are obtained, each of which is a weighted sum of all the other words of the sentence as to their similarity to the first word ( <code>I</code> ).  We conclude this rector in one. </li><li>  Then another layer of the linear transformation is put, which reduces the dimension of the result of item 6.  to the dimension of the vector of one embedding.  It turns out a certain representation of the first word of the sentence, composed of the weighted vectors of all the other words of the sentence. </li><li><p>  The same process is done for all other words in a sentence. </p><br></li><li>  Since the dimension of output is the same, then you can do the same thing again (p.2-8), but instead of the original embeddingdings of words, take what you get after going through this Multi-head attention, and take the neural networks of attenuation inside with other weights (weights between layers are not common).  And you can do a lot of such layers (for Google 6).  However, between the first and second layer, a full meshed layer and residual compound are added to add nets of expressiveness. </li></ol><br><p>  In a blog they have about this process being visualized by a beautiful gif - for now look only at the encoding part: <br><img src="https://habrastorage.org/getpro/habr/post_images/7d8/a4f/e67/7d8a4fe677ff3afae1bb5dc19762fe69.gif" alt="image"></p><br><p>  As a result, for each word we get the final output - embedding, on which the decoder will look. </p><br><h1 id="perehodim-k-dekoderu">  Go to the decoder </h1><br><p><img src="https://habrastorage.org/webt/59/f0/44/59f0440f7d88f805415140.png"><br>  The decoder is also launched one word at a time, receives the last word as input, and must output the following (at the first iteration, it receives a special token <code>&lt;start&gt;</code> ). </p><br><p>  The decoder has two different types of using Multi-head attention: </p><br><ul><li>  The first is the ability to refer to the vectors of past decoded words, as well as it was in the encoding process (but not all can be addressed, but only to those already decoded). </li><li>  The second is the ability to access the encoder output.  In this case, the Query is the input vector in the decoder, and the Key / Value pairs are the final encoder embeddings, where again the same vector goes both as a key and as a value (but the linear transformations within the attention module are different for them) <br>  In the middle there is just the FC layer, again the same residual connections and layer normalization. </li></ul><br><p>  And all this is repeated again 6 times, where the output of the previous block goes to the next input. </p><br><p>  Finally, at the end of the network is the usual softmax, which gives the probabilities of words.  Sampling from it is the result, that is, the next word in the sentence.  We give it to the input of the next launch of the decoder and the process repeats until the decoder issues a token <code>&lt;end of sentence&gt;</code> . </p><br><p>  Of course, this is all end-to-end differentiable, as usual. <br>  Now you can see the whole gif. <br><img src="https://habrastorage.org/getpro/habr/post_images/7d8/a4f/e67/7d8a4fe677ff3afae1bb5dc19762fe69.gif" alt="image"></p><br><p>  During encoding, each vector interacts with all others.  During decoding, each next word interacts with the previous ones and with the encoder vectors. </p><br><h1 id="rezultaty">  results </h1><br><p>  And this good decently improves the state of the art on machine translation. <br><img src="https://habrastorage.org/getpro/habr/post_images/89e/a76/f16/89ea76f1640a2b3ef4e4e085cf3e280e.png" alt="image"></p><br><p>  2 points BLEU - this is quite serious, especially since at these values ‚Äã‚ÄãBLEU is worse correlated with how much a person likes the translation. </p><br><p>  In general, the main innovation is the use of the self-attention mechanism to interact with other words in a sentence instead of RNN or CNN mechanisms. <br>  They theorize that it helps, because the network can equally easily access any information, regardless of the length of the context ‚Äî accessing the last word or the word 10 steps back is equally easy. <br>  It is easier to learn from this and it is possible to carry out calculations in parallel, in contrast to the RNN, where you need to take each step in turn. <br>  They also tried the same architecture for Constituency Parsing, that is, grammatical analysis, and everything worked out too. </p><br><p>  I haven‚Äôt yet seen confirmation that Transformer is already being used in the Google Translate production (although it‚Äôs necessary to think it‚Äôs being used), but the use in Yandex.Translate was mentioned in <a href="https://vk.com/video-11283947_456239228">an interview with Anton Frolov from Yandex</a> (just in case the timestamp is 32:40). </p><br><p>  What can I say - well done and Google, and Yandex!  It's very cool that new architectures are emerging, and that attention is no longer just a standard part for improving RNN, but gives an opportunity to take a fresh look at the problem.  So you see and get to the memory as a standard piece get. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/341240/">https://habr.com/ru/post/341240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../341230/index.html">Paul Graham: My idols</a></li>
<li><a href="../341232/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ286 (October 23 - 29, 2017)</a></li>
<li><a href="../341234/index.html">And you are not too hurried to expel the "hamburger" from your application?</a></li>
<li><a href="../341236/index.html">PHP Digest number 119 (October 10 - 29, 2017)</a></li>
<li><a href="../341238/index.html">Testing photohosting for digital wear (21 pieces)</a></li>
<li><a href="../341242/index.html">When virtual reality goes too far</a></li>
<li><a href="../341244/index.html">Digital events in Moscow from October 30 to November 5</a></li>
<li><a href="../341246/index.html">On November 18‚Äì19, 2017 in Moscow in the co-working center "Atmosphere" the first in Russia Legal Tech Hackathon will be held</a></li>
<li><a href="../341250/index.html">About toilet paper, DevOps and 582 banks</a></li>
<li><a href="../341252/index.html">Vulnerabilities in Oschadbank: getting a full name of a client by phone number, enumerating card numbers, problems in payment terminals</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>