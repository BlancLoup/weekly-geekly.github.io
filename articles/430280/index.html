<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>VotingClassifier in scikit-learn: building and optimizing an ensemble of classification models</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As part of the implementation of a large task on Sentiment Analysis (analysis of reviews), I decided to devote some time to additional study of its se...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>VotingClassifier in scikit-learn: building and optimizing an ensemble of classification models</h1><div class="post__text post__text-html js-mediator-article">  As part of the implementation of a large task on Sentiment Analysis (analysis of reviews), I decided to devote some time to additional study of its separate element - using the VotingClassifier from sklearn.ensemble as a tool for building an ensemble of classification models and improving the final quality of predictions.  Why is this important and what are the nuances? <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/pu/ka/uy/pukauyhbdf34xacms3hlnnm4jok.jpeg"><br><br>  It often happens that in the course of solving an applied problem of analyzing data, it is not immediately obvious (or not at all) what training model is best suited.  One solution may be to choose the most popular and / or intuitively suitable model based on the nature of the data available.  In this case, the parameters of the selected model are optimized (for example, via GridSearchCV) and it is used in the work.  Another approach may be the use of an ensemble of models, when the results of several of them are involved in the formation of the final result.  I‚Äôll just say that the purpose of the article is not to describe the advantages of using an ensemble of models or the principles of its construction (you can read about it <a href="https://habr.com/company/ods/blog/324402/">here</a> ), but rather in one specific applied approach to solving the problem using a specific example and analysis of the nuances arising in the course of such a solution. <br><br>  <u>Setting a global problem is the following</u> : there are only <u>100</u> reviews on mobile phones as a test sample, and we need a pre-trained model that, with these 100 reviews, will show the best result ‚Äî that is, determine whether a review is positive or negative.  An additional complexity, as follows from the conditions of the problem, is the absence of a training sample.  To overcome this difficulty with the help of the Beautiful Soup library, 10,000 reviews of mobile phones and ratings from one of the Russian sites were successfully sparred. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>Skipping the stages of parsing, preprocessing data and studying their original structure</i> , we are going to the moment when there are: <br><br><ul><li>  a training sample of 10,000 phone reviews, each feedback is marked binary (positive or negative).  The markup of receiving the definition of reviews with grades 1-3 as negative and grades 4-5 as positive. </li><li>  using Count Vectorizer, data is presented in a form that is suitable for training classifier models </li></ul><br>  <u>How to decide which model will work best?</u>  We do not have the ability to manually iterate through the models, since  A test sample of only 100 reviews creates a huge risk that some model will simply better fit this test sample, but if you use it on an additional sample hidden from us or in a ‚Äúbattle‚Äù, the result will be below average. <br><br>  To solve this problem, <b>in the Scikit-learn library there is a VotingClassifier module</b> , which is an excellent tool for using several machine learning models that are not similar among themselves, and combining them into one classifier.  This reduces the risk of retraining, as well as misinterpretation of the results of any one single model.  <u>The VotingClassifier module is imported by the following command</u> : <br> <code>from sklearn.ensemble import VotingClassifier</code> <br> <br>  Practical details when working with this module: <br><br>  1) The first and most important is how a separate prediction of the combined classifier is obtained after receiving the predictions from each of its constituent models.  Among the VotingClassifier parameters there is a <i>voting</i> parameter with two possible values: 'hard' and 'soft'. <br><br>  1.1) In the first case, the final answer of the combined classifier will correspond to the ‚Äúopinion‚Äù of the majority of its members.  For example, your combined classifier uses data from three different models.  Two of them on a specific observation predict a response ‚Äúpositive feedback‚Äù, the third - ‚Äúnegative feedback‚Äù.  Thus, for this observation, the final prediction will be a ‚Äúpositive feedback‚Äù, since we have 2 - ‚Äúfor‚Äù and 1 ‚Äúagainst‚Äù. <br><br>  1.2) In the second case, i.e.  when using the parameter 'soft', the <i>voting</i> parameter goes to full ‚Äúvoting‚Äù and weighting model predictions for <u>each</u> class, so the final answer of the combined classifier is argmax of the sum of the predicted probabilities.  <b>IMPORTANT!</b>  To be able to use this ‚Äúvoting‚Äù method, <b>each</b> classifier from within your ensemble must support the <b>predict_proba ()</b> method in <b>order</b> to obtain a quantitative estimate of the probability of entering each of the classes.  Please note that not all models of classifiers support this method and, accordingly, can be used within the VotingClassifier when using the method of weighted probability (Soft Voting). <br><br>  <u>Let us consider an example</u> : there are three classifiers and two classes of reviews: positive and negative.  Each classifier through the method predict_proba will give a certain value of probability (p), with which a specific observation is assigned to it to class 1 and, accordingly, with probability (1-p) to class two.  The combined classifier after receiving a response from each of the models weighs the estimates obtained and gives the final result, obtained as <p><math> </math> $$ display $$ max (w1 * p1 + w2 * p1 + w3 * p1, w1 * p2 + w2 * p2 + w3 * p3) $$ display $$ </p>  where w1, w2, w3 are the weights of your classifier that are included in the ensemble, have equal weights by default, and p1, p2 is the score for belonging to class 1 or class 2 of each of them.  Note also that the weights of classifiers using Soft Vote can be changed using the weights parameter, so the module call should look like this: <br>  <code>... = VotingClassifier(estimators=[('..', clf1), ('..', clf2), ('...', clf3)], voting='soft', weights=[*,*,*])</code> , where asterisks can be specified the required weight for each model. <br><br>  2) The ability to <u>simultaneously</u> use the module VotingClassifier and GridSearch to optimize the hyperparameters of each of the classifiers in the ensemble. <br><br>  When you plan to use an ensemble and you want the models included in it to be optimized, you can use GridSearch already in the combined classifier.  And the code below shows how you can work with the models included in it (Logistic regression, naive Bayes, stochastic gradient descent) while remaining within the combined classifier (VotingClassifier): <br><br><pre> <code class="plaintext hljs">clf1 = LogisticRegression() clf2 = MultinomialNB() clf3 = SGDClassifier(max_iter=1000, loss='log') eclf = VotingClassifier(estimators=[ ('lr', clf1), ('nb', clf2),('sgd', clf3)], voting='hard') #      (hard voting), . . 1.1 &lt;b&gt;params = {'lr__C' : [0.5,1,1.5], 'lr__class_weight': [None,'balanced'], 'nb__alpha' : [0.1,1,2], 'sgd__penalty' : ['l2', 'l1'], 'sgd__alpha': [0.0001,0.001,0.01]} &lt;/b&gt; #       ,  ,     grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5, scoring='accuracy', n_jobs=-1) grid = grid.fit(data_messages_vectorized, df_texts['Binary_Rate']) #    ,      5     </code> </pre><br>  Thus, the params dictionary must be defined in such a way that when accessing it via GridSearch, it is possible to determine which of the models in the ensemble is the parameter whose value is to be optimized. <br><br>  That's all you need to know to fully use the VotingClassifier tool as a way to build an ensemble of models and optimize it.  Let's look at the results: <br><br><pre> <code class="plaintext hljs"> print grid.best_params_ {'lr__class_weight': 'balanced', 'sgd__penalty': 'l1', 'nb__alpha': 1, 'lr__C': 1, 'sgd__alpha': 0.001}</code> </pre><br>  The optimal values ‚Äã‚Äãof the parameters are found, it remains to compare the results for the ensemble of classifiers (VotingClassifier) ‚Äã‚Äãwith the optimal parameters, cross-validate the training set and compare the models with the optimal parameters and the ensemble consisting of them: <br><br><pre> <code class="plaintext hljs">for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Naive Bayes', 'SGD', 'Ensemble_HardVoting']): scores = cross_val_score(clf, data_messages_vectorized, df_texts['Binary_Rate'], cv=3, scoring='accuracy') print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))</code> </pre><br>  The final result: <br><br>  Accuracy: 0.75 (¬± 0.02) [Logistic Regression] <br>  Accuracy: 0.79 (¬± 0.02) [Naive Bayes] <br>  Accuracy: 0.79 (¬± 0.02) [SGD] <br>  Accuracy: 0.79 (¬± 0.02) [Ensemble_HardVoting] <br><br>  As can be seen, the models showed themselves somewhat differently in the training sample (with standard parameters, this difference was more noticeable).  In this case, the total value (for the accuracy metric) of the ensemble does not have to exceed the best value of the models included in it, since  The ensemble is more likely to be a more stable model that can show ¬± similar results on a test sample and in ‚Äúcombat‚Äù, and thus reduce the risk of retraining, fitting a training sample, and other related classifiers of problems.  Good luck in solving applied problems and thank you for your attention! <br><br>  PS Considering the specifics and rules of publication in the sandbox, I cannot provide a link to github and the source code for the analysis given in this article, as well as references to Kaggle, in the framework of the InClass competition which provided a test set and tools for testing models on it.  I can only say that this ensemble significantly broke the baseline and took a worthy place on the leaderboard after checking on the test set.  I hope in the following publications I can share. </div><p>Source: <a href="https://habr.com/ru/post/430280/">https://habr.com/ru/post/430280/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../430270/index.html">Test it: how do we determine which tests to run on pull requests?</a></li>
<li><a href="../430272/index.html">"Monsters in games or 15 cm is enough to attack"</a></li>
<li><a href="../430274/index.html">7 more cool PC games for learning English</a></li>
<li><a href="../430276/index.html">The ruinous mistake of newbies in game dev</a></li>
<li><a href="../430278/index.html">Conference in Budapest (October 29-31) Data Crunch</a></li>
<li><a href="../430282/index.html">9 out of 10 people agree to earn less on more meaningful work</a></li>
<li><a href="../430284/index.html">The digest of interesting materials for the mobile # 275 developer (November 12 - 18)</a></li>
<li><a href="../430286/index.html">Details of the promiscuous and dark side of pirated games for the Nintendo Switch</a></li>
<li><a href="../430290/index.html">Attempt to predict the fourth iteration of the SpaceX BFR project</a></li>
<li><a href="../430292/index.html">Electronic Frontier Foundation: network performance of police car license scanners in the US - 0.5%</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>