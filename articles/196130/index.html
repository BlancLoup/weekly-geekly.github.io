<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Procedural audio</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The time spent at the computer comes down mainly to ‚Äúlooking at the screen‚Äù: whether a person writes code, watches a movie, plays or finds out news. J...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Procedural audio</h1><div class="post__text post__text-html js-mediator-article">  The time spent at the computer comes down mainly to ‚Äúlooking at the screen‚Äù: whether a person writes code, watches a movie, plays or finds out news.  Just because the main feeling of a person is sight.  And it largely determines the perception of reality, the mechanisms of interaction with it, and guides the development of many technologies. <br>  The usual example of the evolution of iron and software over the past few decades: compare Wolfenstein 3D (1992) and Crysis 3 (2013).  How advanced and complicated computer graphics during this time is difficult to underestimate. <br>  But alas, this cannot be said about sound.  How to correct a situation, and there will be article. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/22c/701/e07/22c701e07f19d98dc3fb2a32cb9d8f8b.jpg" alt="image"><br><h6>  <i>Image from page <a href="http://peripheriques.free.fr/blog/index.php%3F/past/2010-pure-data-read-as-pure-data/">peripheriques.free.fr/blog/index.php?/past/2010-pure-data-read-as-pure-data</a></i> </h6><br><a name="habracut"></a><br>  The basis of the article is the material of lectures, which periodically reads by Andy Farnell (Andy Farnell).  In these lectures, he sets out the essence of his vision of the development and use of computational audio (or procedural audio, procedural / computational audio) and gives a number of interesting examples that I would like to share with the community. <br>  A few words about Andy: sound designer, programmer, professor at a number of universities, an audio engineer in the past.  His professional career was once strongly influenced by the demoscene, in particular the work of The Black Lotus.  Andy is also the author of the book ‚ÄúDesigning Sound‚Äù, which contains a lot of useful theory on sound physics, modeling and psychoacoustics, and a wide variety of practical exercises written in the Pure Data visual programming language. <br><br>  The article turned out to be a little more than expected, so if it's interesting to just listen to the results - go directly to the section with examples. <br>  If, on the contrary, you want to delve more into the details, watch the entire lecture, Andy tells a lot of interesting things in it. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  A bit of history </h4><br>  So, the possibilities of computer graphics have reached the detailed modeling of objects and their interactions in real time.  All this is due to the use of many technologies and concepts, including individual graphics cores and more optimal image-building algorithms.  This makes visualization adaptive with respect to system resources without apparent loss of quality.  The use of a dynamic level of detail of the visual space significantly saves resources (the Mipmapping method, for example).  But naturally, it did not appear immediately.  Already mentioned game Wolfenstein 3D looked terrible, if anyone remembers.  Among some experts then there was the opinion that the photographic approach a la Myst (1993) will remain forever.  But the low power of computers and the terrible results did not stop the enthusiasts. <br><br>  In parallel with the development of graphics, serious sound synthesis in applications was not particularly thought of, since computer audio "for lack of attention" was still at the embryonic stage.  Agree, even today when a person hears the word "synthesizer", the first association is likely to be a man with a brilliant head of hair, extracting ridiculous sounds from a plastic parallelepiped.  Although now there are synthesizers capable of creating melodious, complex and beautiful sounds of a completely different nature (say, Massive from Native Instruments or Zebra from U-he, which was <a href="http://habrahabr.ru/post/194670">recently written on Habr√©</a> ).  Interestingly, when I hear the words ‚Äúfirst person shooter,‚Äù I don‚Äôt have associations with huge pixels and eight-bit color, even though I‚Äôm not a gamer at all. <br><br><h4>  Current situation </h4><br>  Audio technologies in non-sound-oriented applications are not adaptable and flexible.  The approach to creating sound is often based on pre-prepared data, and this creates a number of serious limitations.  With a linear increase in the number of objects in the simulated space, the number of their possible interactions grows nonlinearly (n (n-1) / 2).  This does not allow creating a realistic sound field, because in the general case it is impossible to predict all possible types of interaction and prepare sounds for each of them.  In addition, regardless of the importance and proximity of the object on the stage, the resources spent on playback of the audio associated with the object do not change at all.  As was the original audio file, say, 44.1 kHz 24 bits, so it plays.  In the approach to infinity, this approach is ineffective.  In this case, the default argument against the use of computational audio is that the performance of modern conventional computers is supposedly not enough.  As will become clear further, this is not true for a long time.  All examples of this lecture were calculated live on an ordinary gigahertz laptop, using a small part of the resources. <br><br>  Unlike data-based design, we can use a dynamic level of sound detail in procedural audio.  Turn on and off individual blocks of models and use the psychoacoustic features of human perception of sound, removing complex details of the signal or replacing them with something simpler, if necessary.  For example, simulating the sound of rain, it is natural to begin with the fall of individual drops.  Over time, when the density of falling drops becomes quite high, the noise of rain is perfectly approximated by the usual filtered pink noise - this is a kind of audio Mitmapping. <br><br><h4>  Procedural audio development process </h4><br>  Procedural audio involves a well-structured design process, applying the same principles as when writing programs: modularity and reuse.  Such an organized approach has many positive consequences.  I‚Äôll say right away that it doesn‚Äôt kill the creative component of the designer‚Äôs work at all, since the sound design stack has several levels of abstraction, and the designer can work at the level at which he is comfortable.  In favor of this, Andy gives an analogy with the practicality of the OSI seven-tier model and suggests the following general structure of the stack: <br><br><ol><li>  Behavior (Behavior) </li><li>  Model (Model) </li><li>  Method </li><li>  Implementation </li></ol><br>  Behavior, as you can guess, is the behavior that causes the sound.  There is no particular difficulty. <br><br>  Simulation, on the other hand, is the most difficult stage.  Here it is important to understand not only the physics of the process, but also the peculiarities of our perception of sound.  Do not go too deep into the structure of the object.  In the end, although the performance of the desktops is high, you can always write a redundantly complex model that cannot be calculated in real time (some <a href="http://habrahabr.ru/post/61888/">remember the</a> exact simulation of the sound of water, which took several hours to create in a few seconds).  But even for very complex models there are sufficient capacities: according to Andy, a research team from Queen Mary University of London showed that CUDA technology can be very successfully used in computational audio and produce excellent results (unfortunately, I did not find a description of the results). <br>  As David Beck (sound designer, author of the excellent book ‚ÄúThe Csound Book‚Äù) says, the realism of sound is not at all connected with extreme detailing and accuracy, but with what can be called ‚Äúacoustic viability‚Äù, that is, how well the physical parameters relate to the audible sound.  If the perceived seems sensible to us, we intuitively think that "it sounds right." <br>  One of the most important concepts in physical modeling is the ‚Äúkey process‚Äù (signature process) ‚Äîthe process on which the sound produced mostly depends.  For walking, for example, the support reaction is important.  It, in turn, depends on the force applied by the foot on a certain area.  Depending on the type of walking, most of the weight can be shifted to the heel, and maybe to the toe.  From a structural point of view, the leg can be represented as three joints.  Plus, two legs, and they are in phase correlation.  And so on.  The model is complicated.  But ultimately it depends entirely on only one parameter - the speed of the object, and it is elementary to control this parameter at a high level of abstraction. <br>  In the general case, in modeling it is necessary to use a weighted approach, understand how something sounds outside, and, depending on the needs, resort to physical specificity (phenomenal approach, physically informed / contextualised models).  This sometimes implies a departure from the realistic concept towards hyperrealism.  For example, in life a gun does not sound as expressive as it used to be heard in games.  Hyperrealism is often perceived by gamers better than realism. <br><br>  The method is a palette of sound design techniques that is used to synthesize a particular sound.  For example, you can use additive synthesis or frequency modulation instead.  This is a separate big topic, which in this article should not go deep.  It is important to note that methods are often interchangeable. <br><br>  In terms of implementation in general and in detail, there are also many interesting points. <br>  One of the possible ways to implement the following.  Each simulated object has its own methods.  Some of these methods relate to sounds produced by the object.  At the physical level, we can say that the object reacts to the impulse that is transmitted to it when it collides with another object.  Depending on the parameters of the pulse and the properties of objects and the sound is generated.  To do this, each object must have its own ‚Äúimpulse key‚Äù (impulse signature), which determines the acoustic response of the object to the impulse. <br>  In the implementation layer, there is also substitutability.  Dan Stoll - also an expert in audio programming - took as a basis examples from Andy‚Äôs book (up to and including methods) and rewrote them in SuperCollider language.  The result was of the same quality. <br><br><h4>  More pluses </h4><br>  As I said, modularity and reuse have a lot of positive effects. <br>  Let's look at this example.  We have created a Turbine class object that generates the noise of an airplane engine.  The aircraft has two engines.  We create the second object of our class.  Now, for a person who is virtually in the pilot's cabin, we use the sound generated by these two objects, but pass them through the spatial processing module (psycho-acoustics in action: a person will hear on the left and right behind one engine) and hang impulse on them cockpit characteristic (impulse response is a unique characteristic of any space; roughly speaking, this is how the walls of a room reflect the different spectral components of sound, this creates the effect of being in the room).  And for a person who is passing an airplane along the runway, the original generated sound can be simply passed through the module that simulates the Doppler effect, and the approach / delete module (if desired, you can add reflected sound from the runway). <br>  Another positive consequence of this approach is scalability.  For example, the same ‚ÄúCylinder‚Äù class, depending on the settings of the parameters, will simulate a tin can, a big drum, and a piece of a huge pipe.  Or use the same intermittent friction module for the squeak of the door hinge, the creaking of leather on furniture, the squeal of tires, or for rubbing a scrub on glass. <br><br>  One of the most important aspects is financial.  Andy told an interesting story from his experience.  His friend, a sound engineer, recorded the sound of a real aircraft engine for an aircraft simulator.  To do this, it was necessary to take microphones for several thousand euros, rent the engine itself for a day, buy fuel for it, pay for the day of work of a qualified service engineer, pay for the hangar rental and pay for medical insurance for the whole team.  The sound producer was engaged in so-called.  analytical recording - that is, the recording of individual components and processes in the engine: the ignition sequence, specific resonant vibrations of various cavities, the noise of the rotor brushes after cessation of fuel supply, and other similar details.  The output was a few tens of gigabytes of high-quality recordings.  Andy, having carefully analyzed these records, made a model that weighs less than 1 KB and is easily calculated in real time on a netbook.  The sound, naturally, was very realistic. <br><br><h4>  Perspectives </h4><br>  A few years ago, in 2005, Andy tried to find like-minded people in the field of game development, but, according to him, this industry from the inside turned out to be much more conservative than it is customary to think.  In addition to the fact that people do not really think about alternatives, even if you bring all the arguments for - very few people want to change the settled game development process and lose money in case of an unsuccessful experiment.  In addition, there is no proven and effective way to create procedural audio, since there are very few pioneers.  But they are still there, and lately there are more of them.  For example, there is the game <a href="http://www.pugsluvbeats.com/">Pugs Luv Beats</a> , the main feature of which is in procedural music, depending on the gameplay.  The audio engine is also implemented on Pure Data. <br>  Of course, it is difficult to start using procedural audio, but the libraries of modules and components will gradually become saturated, and in the long term the process of creating sound design will go faster and produce better results. <br>  There is no need to fear that this approach will supplant the traditional sound design - computational audio will complement the existing set of tools, and not replace it entirely.  It is not difficult to imagine how many specializations this will create and how it will expand the market.  Moreover, there are now gradations between recorded sounds and procedural audio (for example, grain dictionaries dictionaries can be used - microsamples of the original sound, from which sound is algorithmically collected during the game). <br><br>  How far can such an approach to sound go?  Gradually, many new niches appear in interactive art and synchronized media.  Probably, it will be possible to buy a disc not with a musical recording, but with an algorithm that generates it (to some extent it <a href="http://www.1bitsymphony.com/">is already possible now</a> ).  However, living sound and human recording will not completely replace it, but it will significantly expand and enrich the experience of perception of art in general.  Again, this is not a bipolar thing at all, a whole range of full-fledged approaches using procedural audio comes to mind.  As, for example, in computer graphics, after fully modeled movements, over time, they switched over to models whose movement is recorded from live actors.  It's just another way of recording reality. <br><br><h5>  Examples </h5><br>  In order of increasing sound complexity: <br><ul><li>  walking and running on gravel <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D50m53s">(@ 50: 53)</a> </li><li>  intermittent friction parameterized for door hinge <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D48m55s">(@ 48: 55)</a> </li><li>  a switch <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D54m7s">(@ 54: 07)</a> and a clock based on it <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D55m50s">(@ 55: 50)</a> </li><li>  car engine <a href="http://youtu.be/-Ucv7EXwnCM%3Ft%3D12m5s">(from another video)</a> with tire squeal (intermittent friction module) <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D53m08s">(@ 53: 08)</a> </li><li>  electric motor <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h01m49s">(@ 1: 01: 49)</a> and a robot from several such modules <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h02m15s">(@ 1: 02: 15)</a> </li><li>  propeller blades <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h03m53s">(@ 1: 03: 53)</a> </li><li>  fan in fan shaft (blades + electric motor + pulse characteristic of a shaft) <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h05m34s">(@ 1: 05: 34)</a> </li><li>  wind <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h7m36s">(@ 1: 07: 36)</a> , rain <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h9m14s">(@ 1: 09: 14)</a> , window <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D46m50s">(@ 46: 50)</a> , all three together, "sitting by the window during a storm" <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h10m15s">(@ 1: 10: 15)</a> </li><li>  ‚ÄúVoice module‚Äù for robots <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h11m37s">(@ 1: 11: 37)</a> , a lion and a cow on the same basis <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h12m28s">(@ 1: 12: 28)</a> </li><li>  the flame <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h13m41s">(@ 1: 13: 41)</a> , the very engine of the aircraft (turbine + flame) <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h16m01s">(@ 1: 16: 01)</a> </li><li>  helicopter on the ground (using a propeller model) <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h19m38s">(@ 1: 19: 38)</a> and flying (with an approximation / removal module) <a href="http://youtu.be/sp83-Pq7TyQ%3Ft%3D1h25m02s">(@ 1: 25: 02)</a> </li></ul><br><br>  Materials: <br><ol><li>  Andy Farnell Lecture in London, 2013: <a href="http://www.youtube.com/watch%3Fv%3Dsp83-Pq7TyQ">www.youtube.com/watch?v=sp83-Pq7TyQ</a> (the same material, but in five parts and in the worst quality: <a href="http://www.youtube.com/watch%3Fv%3DDc04hDcy3lo">1</a> , <a href="http://www.youtube.com/watch%3Fv%3D_nBmNEPl0XY">2</a> , <a href="http://www.youtube.com/watch%3Fv%3DqUfOKBMlcuY">3</a> , <a href="http://www.youtube.com/watch%3Fv%3DkwK7OSkg4Gs">4</a> , <a href="http://www.youtube.com/watch%3Fv%3D-Ucv7EXwnCM">5</a> ) </li><li>  A large excerpt from the book "Designing Sound" in the public domain: <a href="http://aspress.co.uk/ds/pdf/pd_intro.pdf">aspress.co.uk/ds/pdf/pd_intro.pdf</a> </li><li>  Interview with Andy on Designingsound.org: <a href="http://designingsound.org/2012/01/procedural-audio-interview-with-andy-farnell/">designingsound.org/2012/01/procedural-audio-interview-with-andy-farnell</a> </li><li>  Interview with the creators of Pugs Luv Beats ibid: <a href="http://designingsound.org/2012/01/the-sound-of-pugs-luv-beats/">designingsound.org/2012/01/the-sound-of-pugs-luv-beats</a> </li><li>  Study of the effectiveness and quality of audio modeling at Queen Mary University of London: <a href="http://www.eecs.qmul.ac.uk/~josh/documents/HendryReiss-AES129.pdf">http://www.eecs.qmul.ac.uk/~josh/documents/HendryReiss-AES129.pdf</a> </li><li>  Pure Data: <a href="http://puredata.info/">puredata.info</a> </li><li>  I also want to mention the remarkable Survey of Music Technology course from a practical and cultural point of view: <a href="https://class.coursera.org/musictech-001">class.coursera.org/musictech-001</a> and the <a href="https://class.coursera.org/musictech-001">ChucK</a> course starting soon: <a href="https://www.coursera.org/course/chuck101">www.coursera.org/course/chuck101</a> </li></ol><br></div><p>Source: <a href="https://habr.com/ru/post/196130/">https://habr.com/ru/post/196130/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../196110/index.html">Edward Hands - C ++</a></li>
<li><a href="../196112/index.html">Winning startups: Where are they now?</a></li>
<li><a href="../196114/index.html">‚ÄúEvaluate the quality of service‚Äù: Motivator, Management Tool or Demotivator?</a></li>
<li><a href="../196126/index.html">IMotion - 3D motion controller with tactile feedback</a></li>
<li><a href="../196128/index.html">Windows + Tmux + Cygwin</a></li>
<li><a href="../196132/index.html">The Large Hadron Collider with my own eyes</a></li>
<li><a href="../196134/index.html">How VPN is arranged through SSTP</a></li>
<li><a href="../196136/index.html">Initial setup of Tomcat and its registration with NetBeans</a></li>
<li><a href="../196138/index.html">Explanatory head phraseological dictionary - with explanations</a></li>
<li><a href="../196140/index.html">Game Maker AI - How to start?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>