<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AlphaGo on the fingers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="So, while our new masters are resting, let me try to tell you how AlphaGo works. The post implies some familiarity of the reader with the subject - yo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AlphaGo on the fingers</h1><div class="post__text post__text-html js-mediator-article">  So, while our new masters are resting, let me try to tell you how AlphaGo works.  The post implies some familiarity of the reader with the subject - you need to know the difference between Fan Hui and Lee Sedol, and to superficially understand how neural networks work. <br><a name="habracut"></a><br>  <em>Disclaimer: the post is written on the basis of fairly edited chat logs <a href="http://closedcircles.com/%3Finvite%3Dc5c16d7fe2c18045aeea4e82ff05e05d6ef1c00d">closedcircles.com</a> , hence the style of presentation, and the availability of clarifying questions</em> <br><br>  As everyone knows, computers played badly in Go because there are so many possible moves and the search space is so large that direct bust helps a little. <br>  The best programs use the so-called <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo Tree Search</a> - a search on the tree with the evaluation of the nodes through the so-called rollouts, that is, quick simulations of the result of the game from the position in the node. <br><br>  AlphaGo complements this search for tree-based deep-learning evaluation functions to optimize the search space.  The article originally appeared in Nature (and it is there for paywall), but you can find it on the Internet.  For example here - <a href="https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf">https://gogameguru.com/i/2016/03/deepmind-mastering-go.pdf</a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  First, let's talk about the composite pieces, and then how they are combined. </h1><br><h3>  Step 1: we train the neural network that learns to predict the moves of people - SL-policy network </h3><br>  We take 160K of fairly high-level online games available and train the neural network that predicts the next move of a person in position. <br>  The network architecture is just 12 convolution layers with non-linearity and softmax per cell at the end.  Such depth is generally comparable to the networks for processing images of the past generation (Google Inception-v1, VGG, all these things) <br>  The important point is that the neural network is given for input: <br><br><img src="https://habrastorage.org/files/85b/8de/94b/85b8de94b0d441878c0f1a6faa71c2bc.png" alt="image"><br><br>  For each cell, 48 features are given for input, they are all in the table (each dimension is a binary feature) <br>  The set is interesting.  At first glance, it seems that the network needs to be given only if there is a stone in the cell and if so, which one.  But figs there! <br>  There are trivially computed features like "the number of degrees of freedom of a stone", or "the number of stones that will be taken by this move" <br>  There are also formally unimportant features like "how long has it been a move" <br>  And even a special feature for the frequent appearance of "ladder capture / ladder escape" - a potentially long sequence of forced moves. <br><br>  <em>and what is "always 1" and "always 0"?</em> <br>  They simply to finish quantity of features to multiple 4th, it seems to me. <br><br>  And on this all the grid learns to predict human moves.  Predicts with an accuracy of 57% and this must be treated with caution - the purpose of the prediction, the human move, is still ambiguous. <br>  The authors show, however, that even small improvements in accuracy strongly affect the strength in the game (comparing grids of different capacities) <br><br>  Separate from SL-policy, they train fast rollout policy - a very fast strategy, which is simply a linear classifier. <br>  They give her more advanced features at the entrance. <br><img src="https://habrastorage.org/files/5cb/135/75d/5cb13575d76f49348538c73dc2970132.png" alt="image"><br>  That is, it is given features in the form of pre-prepared patterns. <br>  It is much worse than a model with a deep network, but it is super-fast.  How it is used - it will be clear further <br><br><h3>  Step 2: we train policy even better through playing with ourselves (reinforcement learning) - RL-policy network </h3><br>  We choose an adversary from a pool of past versions of the network by chance (so as not to overfit herself), play the game with him to the end by simply choosing the most likely move from network prediction, again without any sorting. <br><br>  The only reward is the actual result of the game, won or lost. <br>  After the reward is known, we calculate how to move the weights - we lose the game again and at each turn we move the weights that affect the choice of the selected position, along the gradient in + or in - depending on the result.  In other words, we apply this reward as the direction of the gradient to each turn. <br><br>  (for the curious, there is a little more subtle and the gradient is multiplied by the difference between the result and the position estimate via the value network) <br><br>  And here we repeat and repeat this process - after that the RL-policy is much stronger than the SL-policy from the first step. <br>  The prediction of this trained RL-policy is already tearing up most of the past programs that play Go, without any trees and overrules. <br><br>  <em>Including <a href="http://www.theguardian.com/technology/2016/jan/28/go-playing-facebook-spoil-googles-ai-deepmind">DarkForest</a> Facebook?</em> <br>  It was not compared with it, it is not clear. <br><br>  An interesting detail!  In the original article it is written that this process lasted only 1 day (the rest of the workouts - weeks). <br><br><h3>  Step 3: we will train the network, which tells us at a glance to the alignment, what are our chances to win!  - Value network </h3><br>  Those.  predicts just one value from -1 to 1. <br>  It has exactly the same architecture as the policy network (there is one extra convolution layer, it seems) + a naturally fully connected layer at the end. <br><br>  <em>That is, she has the same features?</em> <br>  value network give another feature - whether the player plays with black or not (the policy network transmits a ‚Äúfriend or foe‚Äù stone, not a color).  As I understand it, this is so that she can take into account the Komi - extra points for white, because they go second <br><br>  It turns out that it cannot be trained in all positions from the games of people ‚Äî since many positions belong to the game with the same result, such a network starts to overfit ‚Äî that is,  remember which party it is, instead of evaluating the position. <br>  Therefore, she is trained on synthetic data ‚Äî N moves are made through the SL network, then a random legal move is made, then she is played through the RL-network to find out the result, and she is trained on N + 2 (!) During only one position per generated game. <br><br><h1>  So, here we have these trained bricks.  How do we play with them? </h1><br>  <strong>TL; DR:</strong> Policy network predicts probable moves to reduce the search width (less than possible moves in the node), value network predicts how advantageous the position is to reduce the required search depth <br><br>  Attention, picture! <br><img src="https://habrastorage.org/files/c60/579/50e/c6057950ec794735a36c2bd86485b788.png" alt="image"><br><br>  So, we have a tree of positions, in the root - the current one.  For each position there is a certain value of Q, which means how much it leads to victory. <br>  We are simultaneously running a large number of simulations on this tree. <br><br>  Each simulation goes through the tree to where there is more Q + m (P).  m (P) is a special additive that stimulates exploration.  It is more if the policy network considers that this move has a high probability and less if it has already been followed a lot along this path. <br>  (this is a variation of the standard multi-armed bandit) <br><br>  When the simulation has reached the tree to the leaf, and wants to go on, where there is nothing yet ... <br>  That newly created tree node is evaluated in two ways. <br><br><ul><li>  First, through the value network described above </li><li>  secondly, it is played to the end with the help of the super-fast model from Step 1 (this is called rollout) </li></ul><br>  The results of these two estimates are mixed with a certain weight (in the release it is naturally 0.5), and the resulting score is recorded for all the tree nodes through which the simulation passed, and Q in each node is updated as the average of all scores for passes through this node. <br>  (there is quite a bit more complicated, but it can be neglected) <br>  Those.  each simulation runs through the tree to the most promising area (taking exploration into account), finds a new position, evaluates it, writes the result upwards through all the moves that led to it.  And then Q in each node is calculated as averaging over all simulations that ran through it. <br><br>  Actually, everything.  The best move is declared the node through which they ran the most (it turns out that it is a bit more stable than this Q-score).  AlphaGo is dealt if all Q-scores are &lt;-0.8, i.e.  probability of winning is less than 10%. <br><br>  An interesting detail!  In the paper for the initial probabilities of moves P, the weaker SL-policy was used, not the RL-policy. <br>  It turned out empirically that it was a little better this way (perhaps, the match with Lee Sedol was no longer there, but they played it with Fan Hui), i.e.  reinforcement learning was needed <em>only</em> to train value network <br><br>  Finally, what can be said about what the version of AlphaGo, which played with Fan Hui (and was described in the article), differed from the version that plays with Lee Sedol: <br><br><ul><li>  Cluster could be bigger.  The maximum version of the cluster in the article is 280 GPUs, but Fan Hui played with the version with 176 GPUs. </li><li>  It seems that it has become more time to spend on the move (in the article all the estimates are given for 2 seconds per move) + added some ML on the topic of time management </li><li>  There was more time to train the nets before the match.  My personal suspicion is fundamentally that more time is spent on reinforcement learning.  1 day in the original article is somehow not even funny. </li></ul><br>  Perhaps all.  Looking forward to 5: 0! <br><br>  <em>Bonus</em> : <a href="https://github.com/Rochester-NRT/AlphaGo">Attempt to open-source implementation.</a>  There, of course, another saw and saw. <br></div><p>Source: <a href="https://habr.com/ru/post/279071/">https://habr.com/ru/post/279071/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../279059/index.html">Svezhak for iOS developers - Digest MBLTdev</a></li>
<li><a href="../279061/index.html">Icy Rocks Test for Android: it's time for a real test</a></li>
<li><a href="../279063/index.html">Automatically update Firefox extensions</a></li>
<li><a href="../279067/index.html">How to create animations and transitions using Motion UI</a></li>
<li><a href="../279069/index.html">1.2 SFML and Code :: Blocks (MinGW)</a></li>
<li><a href="../279073/index.html">Security Week 10: extortionist for OS X, iPhone passcode bypass, Facebook vulnerability and bug bounty benefits</a></li>
<li><a href="../279075/index.html">Android N Preview: API and Developer Tools</a></li>
<li><a href="../279081/index.html">4 ways to make game learning interesting</a></li>
<li><a href="../279083/index.html">Grid implementation for working with large tables. Part 2</a></li>
<li><a href="../279085/index.html">About the importance of catching fleas. What is Global OpenStack Bug Smash for?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>