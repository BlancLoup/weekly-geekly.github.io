<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Swift and TensorFlow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I do not like to read articles, I immediately go to GitHub  GitHub: TensorFlowKit 
 GitHub: Example 
 GitHub: Other 

 TensorFlowKit API 

 I apologiz...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Swift and TensorFlow</h1><div class="post__text post__text-html js-mediator-article"><div class="spoiler">  <b class="spoiler_title">I do not like to read articles, I immediately go to GitHub</b> <div class="spoiler_text">  <a href="https://github.com/Octadero/TensorFlow">GitHub: TensorFlowKit</a> <br>  <a href="http://github.com/Octadero/Example">GitHub: Example</a> <br>  <a href="https://github.com/Octadero/">GitHub: Other</a> <br><br>  <a href="http://api.octadero.com/TensorFlowKit/index.html">TensorFlowKit API</a> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">I apologize in advance for this inconvenience.</b> <div class="spoiler_text">  Everything that will be described in this article in one way or another will affect several areas of computer science, but it is not possible to plunge into each separate sphere.  I apologize in advance for this inconvenience. <br></div></div><br><br>  It is probably not necessary to talk about machine learning and artificial intelligence in 2017.  A large number of both journalistic articles and serious scientific works have already been written on this topic.  Therefore, it is assumed that the reader already knows what it is.  Speaking of machine learning, the data scientist community and software engineers usually imply deep neural networks that have become very popular because of their performance.  Today in the world there are a large number of different software solutions and complexes for solving the problem of artificial neural networks: Caffe, TensorFlow, Torch, Theano (rip), cuDNN etc. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Swift </h3><br>  Swift is an innovative, protocol-oriented, open source programming language grown within Apple by Chris Latner (who recently left Apple after SpaceX and settled in Google). <br>  Apple's OSs already had various libraries for working with matrices and vector algebra: BLAS, BNNS, DSP, subsequently merged under the roof of one Accelerate library. <br>  In 2015, there were small solutions for the implementation of mathematics based on the metal graphic technology. <br>  In 2016, CoreML appeared: <br><img src="https://habrastorage.org/getpro/habr/post_images/3d7/e71/d55/3d7e71d55595d936ca13b785dfb5356d.png" alt="image"><br>  CoreML is able to import a ready-made, trained model (CaffeV1, Keras, scikit-learn) and further provide the developer with the opportunity to export it to the application. <br>  That is, you need to: Build a model on another platform, in Python or C ++, using third-party frameworks.  Next, train it on a third-party hardware solution. <br>  And only after that you can import and work in the Swift language.  In my opinion it is very piled up and difficult. <br><a name="habracut"></a><br><br><h3>  Tensorflow </h3><br>  TensorFlow, like other software packages that implement artificial neural networks, has many ready-made abstractions and a mechanic for working with neurons, the connections between them, the calculation of errors and the inverse distribution of errors.  But unlike other packages, Jeff Dean (an employee of Google, the creator of the distributed file system, TensorFlow, and many other great solutions) decided to build on TensorFlow the idea of ‚Äã‚Äãseparating the data execution model and the data execution process.  This means that you first describe the so-called calculation graph, and after that you start its calculation.  This approach allows you to separate and work very flexibly with <b>the</b> data execution <b>model</b> and directly with <b>the data execution process</b> , distributing the execution across different nodes (processors, video cards, computers and clusters). <br><br><h3>  TensorFlowKit </h3><br>  To solve the whole cycle of tasks from developing a model to working with it in the final application, I wrote an access and work interface with TensorFlow in one language. <br>  The solution architecture looks like two levels: medium and high. <br><ul><li>  At the low <a href="">C</a> level, the <a href="">module</a> allows access to libtensorflow from the swift language. </li><li>  The middle level allows you to get away from C pointers and operate with ‚Äúbeautiful bugs‚Äù. </li><li>  The high level implements various abstractions for accessing the elements of the model and various utilities for exporting, importing and visualizing the graph. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/10e/a48/8cb/10ea488cb777ea99ebaf61f1656a0dfd.png" alt="image"><br>  Thus, you can create a model (Graph computing) in swift language, train it on a server running Ubuntu OS, using several video cards and then easily open it in your program on macOS or tv OS.  Development can be conducted in the usual Xcode with all its advantages and disadvantages. <br>  <a href="http://api.octadero.com/TensorFlowKit/index.html">Documentation and API is located at this link.</a> <br><br><div class="spoiler">  <b class="spoiler_title">Very briefly about the theory of neural networks.</b> <div class="spoiler_text">  Artificial neural networks implement some (very simplified) model of neuron connections in the tissues of the nervous system.  The input signal in the form of a vector of large dimension enters the input layer consisting of neurons.  Further, each input neuron transmits this signal to the next layer, transforming it, based on the properties of connections (weights) between neurons and the properties of neurons of the next layers.  In the process of learning, an output signal is formed on the output layer, which is compared with the expected one.  Based on the difference between the output signal and the sample signal, an error value is generated.  Further, this error is used to calculate the so-called gradient - a vector, in the direction of which it is necessary to make corrections of connections between neurons, so that in the future the neural network generates a signal more similar to the expected one.  The process itself is called inverse error distribution or backpropagation.  Thus, neurons and connections between them accumulate information necessary for generalizing the properties of the data model that this neural network is learning.  The technical implementation rests on various mathematical operations on matrices and vectors, which, in turn, have already been implemented to one degree or another by solutions such as BLAS, LAPACK, DSP, etc. <br><img src="https://habrastorage.org/getpro/habr/post_images/864/044/161/86404416178a12de3ad11f28e6cc242c.png" alt="image"><br></div></div><br><br><h3>  MNIST </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/df1/7ba/a08/df17baa08279edbe2ba4339917642fa2.png" alt="image"><br><br>  For example, I took ‚ÄúHello world!‚Äù In the world of neural networks: the task of classifying <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> images.  MNIST datasets are thousands of 28 x 28 pixel handwritten digit images.  Thus, we have ten classes that are neatly distributed in 60,000 images for training and 10,000 images for the test.  Our task is to create a neural network capable of classifying an image and determining belonging to one of ten classes. <br><br>  Before working with TensorFlowKit itself, you must install TensorFlow.  On macOS, you can use the brew package manager: <br><pre><code class="bash hljs">brew install libtensorflow</code> </pre> <br>  <a href="">The linux build is available here.</a> <br>  Create a swift project, connect the dependency to it. <br><pre> <code class="hljs cs">dependencies: [ .package(url: <span class="hljs-string"><span class="hljs-string">"https://github.com/Octadero/TensorFlow.git"</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">from</span></span>: <span class="hljs-string"><span class="hljs-string">"0.0.7"</span></span>) ]</code> </pre> <br><br><h4>  Prepare MNIST dataset. </h4><br>  <a href="http://github.com/Octadero/MNISTKit">The package for working with MNIST data is written and available at the link.</a>  The package will download the dataset itself into a temporary directory, unpack it and present it in the form of ready-made classes. <br><pre> <code class="hljs lisp">dataset = MNISTDataset(<span class="hljs-name"><span class="hljs-name">callback</span></span>: { (<span class="hljs-name"><span class="hljs-name">error</span></span>: Error?) in print(<span class="hljs-string"><span class="hljs-string">"Ready"</span></span>) })</code> </pre><br><br>  We collect the necessary graph of operations. <br>  The entire space and subspace of the calculation graph is called Scope and may have its own name. <br><br>  At the entrance of our network, we will submit two vectors.  The first is directly the images presented in the form of a vector of higher dimension 784 (28x28 px). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/429/641/72e/42964172e7f6fe3007f422cadb9e970f.png" alt="image"><br><br>  That is, in each component of the vector x there will be a Float value from 0-1, corresponding to the color of the pixel of the picture. <br>  The second vector will be its corresponding class, encrypted in the form (see below) where the corresponding 1 component corresponds to the class number.  In this example, the class is 2. <br><br><pre> <code class="hljs json">[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span> ]</code> </pre><br>  Since the input parameters will change in the learning process, we create a Placeholder to reference them. <br><pre> <code class="hljs pgsql">//<span class="hljs-keyword"><span class="hljs-keyword">Input</span></span> sub scope let inputScope = scope.subScope(namespace: "input") let x = try inputScope.placeholder(operationName: "x-input", dtype: <span class="hljs-type"><span class="hljs-type">Float</span></span>.self, shape: Shape.dimensions(<span class="hljs-keyword"><span class="hljs-keyword">value</span></span>: [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">784</span></span>])) let yLabels = try inputScope.placeholder(operationName: "y-input", dtype: <span class="hljs-type"><span class="hljs-type">Float</span></span>.self, shape: Shape.dimensions(<span class="hljs-keyword"><span class="hljs-keyword">value</span></span>: [<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>]))</code> </pre><br><br>  To visualize the graph, I used <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> .  I will discuss how to create graphs and visualize the learning process with TensorFlowKit in another article. <br>  On the input column is as follows: <br><img src="https://habrastorage.org/getpro/habr/post_images/7eb/05c/9b2/7eb05c9b2b814febbc5dd1e8c1543466.png" alt="image"><br>  This is our input layer. <br>  Next, create weights (links) between the input layer and the hidden layer. <br><pre> <code class="hljs lisp">let weights = try weightVariable(<span class="hljs-name"><span class="hljs-name">at</span></span>: scope, name: <span class="hljs-string"><span class="hljs-string">"weights"</span></span>, shape: Shape.dimensions(<span class="hljs-name"><span class="hljs-name">value</span></span>: [<span class="hljs-number"><span class="hljs-number">784</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>])) let bias = try biasVariable(<span class="hljs-name"><span class="hljs-name">at</span></span>: scope, name: <span class="hljs-string"><span class="hljs-string">"biases"</span></span>, shape: Shape.dimensions(<span class="hljs-name"><span class="hljs-name">value</span></span>: [<span class="hljs-number"><span class="hljs-number">10</span></span>]))</code> </pre><br>  Since weights and bases will be changed (adjusted) in the process of network training, we create an operation of variables (variable) in the graph. <br>  And we initialize them with a tensor filled with zeros. <br><img src="https://habrastorage.org/getpro/habr/post_images/ddd/7db/370/ddd7db3706b9dd6297731043c8cd25f9.png" alt="image"><br>  Now create a hidden layer that will perform the simplest operation (x * W) + b <br>  This is the operation of <a href="https://www.octadero.com/2017/02/09/matrix-multiplication-ways/">multiplying the vector</a> x (dimension 1x784) <a href="https://www.octadero.com/2017/02/09/matrix-multiplication-ways/">by the matrix</a> W (dimension 784x10) and adding the basis. <br><img src="https://habrastorage.org/getpro/habr/post_images/e14/e00/710/e14e00710f2ee7ceb9b8a6bbab912752.png" alt="image"><br>  In our case, the hidden layer is already a weekend (task level ‚ÄúHello World!‚Äù), So we need to analyze the output signal and choose the winner.  To do this, use softmax operation. <br>  For a better understanding of what I will describe below, I suggest that we consider our neural network as a complex function.  The input to our function is the vector x (representing the image).  At the output, we get a vector that says how much the function is sure that the input vector belongs to each of the classes. <br>  Next, take the natural logarithm of the magnitude of the predictions in each of the classes and multiply it by the value of the vector of the correct class, neatly transmitted at the very beginning (yLabel). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/229/b75/ffd/229b75ffd84d0eef19e75e9dc1ff1b8b.png" alt="image"><br>  In this way, we get the meaning of the error and can use it to ‚Äúconvict‚Äù the neural network.  Below is a picture of two examples.  On the first class 2: the error was 2.3, on the second class 1: the error is zero. <br><img src="https://habrastorage.org/getpro/habr/post_images/e75/774/947/e757749477d4197b311e4ce908857571.png" alt="image"><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">let</span></span> log = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.log(operationName: <span class="hljs-string"><span class="hljs-string">"Log"</span></span>, x: softmax) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> mul = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.mul(operationName: <span class="hljs-string"><span class="hljs-string">"Mul"</span></span>, x: yLabels, y: log) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> reductionIndices = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.addConst(tensor: <span class="hljs-type"><span class="hljs-type">Tensor</span></span>(dimensions: [<span class="hljs-number"><span class="hljs-number">1</span></span>], values: [<span class="hljs-type"><span class="hljs-type">Int</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>)]), <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>: <span class="hljs-string"><span class="hljs-string">"reduction_indices"</span></span>).defaultOutput <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> sum = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.sum(operationName: <span class="hljs-string"><span class="hljs-string">"Sum"</span></span>, input: mul, reductionIndices: reductionIndices, keepDims: <span class="hljs-literal"><span class="hljs-literal">false</span></span>, tidx: <span class="hljs-type"><span class="hljs-type">Int32</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> neg = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.neg(operationName: <span class="hljs-string"><span class="hljs-string">"Neg"</span></span>, x: sum) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> meanReductionIndices = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.addConst(tensor: <span class="hljs-type"><span class="hljs-type">Tensor</span></span>(dimensions: [<span class="hljs-number"><span class="hljs-number">1</span></span>], values: [<span class="hljs-type"><span class="hljs-type">Int</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>)]), <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>: <span class="hljs-string"><span class="hljs-string">"mean_reduction_indices"</span></span>).defaultOutput <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> cross_entropy = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.mean(operationName: <span class="hljs-string"><span class="hljs-string">"Mean"</span></span>, input: neg, reductionIndices: meanReductionIndices, keepDims: <span class="hljs-literal"><span class="hljs-literal">false</span></span>, tidx: <span class="hljs-type"><span class="hljs-type">Int32</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>)</code> </pre><br><br>  What to do next? <br>  Mathematically speaking, we must minimize the objective function.  One of the approaches is the gradient descent method.  I will try to tell about it in the next article, if this is necessary. <br>  Thus, we have to calculate how much each of the weights (components of the W matrix) and the basis vector b need to be corrected so that the neural network makes a smaller error in such input data. <br>  Mathematically, we must find the partial derivatives of the output node by the values ‚Äã‚Äãof all our intermediate nodes.  The resulting symbolic gradients will allow us to ‚Äúshift‚Äù the values ‚Äã‚Äãof our components of the variables W and b according to how each of them influenced the result of previous calculations. <br><br>  Magic TensorFlow. <br>  The fact is that all ( <a href="">in fact, not yet all</a> ) these complex calculations TensorFlow is able to do on its own, after analyzing the graph that we built. <br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">let</span></span> gradientsOutputs = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.addGradients(yOutputs: [cross_entropy], xOutputs: [weights.variable, bias.variable])</code> </pre><br><br>  After calling this operation, TensorFlow will independently build another half hundred operations. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0c3/dc1/753/0c3dc17530df23cbdd35a400ca7a1ef7.png" alt="image"><br><br>  Now it is enough to add the operation of updating our weights to the value previously calculated by the gradient descent method. <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">let</span></span> <span class="hljs-number"><span class="hljs-number">_</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> scope.applyGradientDescent(operationName: <span class="hljs-string"><span class="hljs-string">"applyGradientDescent_W"</span></span>, `<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>`: weights.variable, alpha: learningRate, delta: gradientsOutputs[<span class="hljs-number"><span class="hljs-number">0</span></span>], useLocking: <span class="hljs-literal"><span class="hljs-literal">false</span></span>)</code> </pre><br>  Everything, the graph is ready! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/052/3bf/f7a/0523bff7a7e5389929765199d9aec6b7.png" alt="image"><br><br>  As I said, TensorFlow separates the model and calculations.  For this reason, the graph we have constructed is only a model for performing calculations. <br>  We can start the calculations using Session. <br><br>  Having prepared the data from the dataset and placed it in the tensors, we start the session. <br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">guard</span></span> <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> dataset = dataset <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-type"><span class="hljs-type">MNISTTestsError</span></span>.datasetNotReady } <span class="hljs-keyword"><span class="hljs-keyword">guard</span></span> <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> images = dataset.files(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span>: .image(<span class="hljs-built_in"><span class="hljs-built_in">stride</span></span>: .train)).first <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>? <span class="hljs-type"><span class="hljs-type">MNISTImagesFile</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-type"><span class="hljs-type">MNISTTestsError</span></span>.datasetNotReady } <span class="hljs-keyword"><span class="hljs-keyword">guard</span></span> <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> labels = dataset.files(<span class="hljs-keyword"><span class="hljs-keyword">for</span></span>: .label(<span class="hljs-built_in"><span class="hljs-built_in">stride</span></span>: .train)).first <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>? <span class="hljs-type"><span class="hljs-type">MNISTLabelsFile</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-type"><span class="hljs-type">MNISTTestsError</span></span>.datasetNotReady } <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> xTensorInput = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> <span class="hljs-type"><span class="hljs-type">Tensor</span></span>(dimensions: [bach, <span class="hljs-number"><span class="hljs-number">784</span></span>], values: xs) <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> yTensorInput = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> <span class="hljs-type"><span class="hljs-type">Tensor</span></span>(dimensions: [bach, <span class="hljs-number"><span class="hljs-number">10</span></span>], values: ys)</code> </pre><br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>..&lt;<span class="hljs-number"><span class="hljs-number">1000</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> resultOutput = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> session.run(inputs: [x, y], values: [xTensorInput, yTensorInput], outputs: [loss, applyGradW, applyGradB], targetOperations: []) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> index % <span class="hljs-number"><span class="hljs-number">100</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> lossTensor = resultOutput[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> gradWTensor = resultOutput[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> gradBTensor = resultOutput[<span class="hljs-number"><span class="hljs-number">2</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> wValues: [<span class="hljs-type"><span class="hljs-type">Float</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> gradWTensor.pullCollection() <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> bValues: [<span class="hljs-type"><span class="hljs-type">Float</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> gradBTensor.pullCollection() <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> lossValues: [<span class="hljs-type"><span class="hljs-type">Float</span></span>] = <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> lossTensor.pullCollection() <span class="hljs-keyword"><span class="hljs-keyword">guard</span></span> <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> lossValue = lossValues.first <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span> } <span class="hljs-built_in"><span class="hljs-built_in">print</span></span>(<span class="hljs-string"><span class="hljs-string">"\(index) loss: "</span></span>, lossValue) lossValueResult = lossValue <span class="hljs-built_in"><span class="hljs-built_in">print</span></span>(<span class="hljs-string"><span class="hljs-string">"w max: \(wValues.max()!) min: \(wValues.min()!) b max: \(bValues.max()!) min: \(bValues.min()!)"</span></span>) } }</code> </pre><br>  This code can be found on <a href="http://github.com/Octadero/Example">GitHub</a> . <br>  Every 100 operations, we derive the size of the error. <br><br>  New article: <a href="https://habrahabr.ru/post/342934/">Visualization of the neural network learning process using TensorFlowKit</a> published. </div><p>Source: <a href="https://habr.com/ru/post/342132/">https://habr.com/ru/post/342132/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../342118/index.html">‚ÄúOur application as a TARDIS: more inside than it seems from the outside‚Äù - Avito about mobile development</a></li>
<li><a href="../342124/index.html">Java EE 8: A brief and very optimistic overview of new features.</a></li>
<li><a href="../342126/index.html">Centralized continuous deployment for the year</a></li>
<li><a href="../342128/index.html">New LISP features in nanoCAD 8.5</a></li>
<li><a href="../342130/index.html">Back to the future: how did Joker 2017 go</a></li>
<li><a href="../342134/index.html">Impressions of Angular Connect 2017</a></li>
<li><a href="../342136/index.html">Can VeraCrypt become the next TrueCrypt?</a></li>
<li><a href="../342138/index.html">Security Week 45: Ether has frozen in wallets, a million fake WhatsApps, useless protection of intellectual property</a></li>
<li><a href="../342140/index.html">Nostalgia post. Secrets of Internet prices: why megabits can cost from $ 0 to $ 200 or how to get 100 Gbit / s for a penny?</a></li>
<li><a href="../342146/index.html">Expansion of the menu functionality in nanoCAD 8.5: macros and LISP expressions</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>