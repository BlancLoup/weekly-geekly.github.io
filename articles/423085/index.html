<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fine-tuning load balancing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article will focus on load balancing in web projects. Many believe that the solution to this problem is to distribute the load between servers - ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fine-tuning load balancing</h1><div class="post__text post__text-html js-mediator-article">  This article will focus on load balancing in web projects.  Many believe that the solution to this problem is to distribute the load between servers - the more accurate, the better.  But we know that this is not entirely true.  <strong>The stability of the system is much more important from a business point of view</strong> . <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  A small minute peak at 84 RPS "five hundred" is five thousand errors that were received by real users.  This is a lot and it is very important.  It is necessary to look for the reasons, to work on the errors and try to avoid such situations in the future. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <strong>Nikolay Sivko</strong> ( <a href="https://habr.com/users/nikolaysivko/" class="user_link">NikolaySivko</a> ) in his report on RootConf 2018 spoke about the thin and not very popular aspects of load balancing: <br><br><ul><li>  when to repeat the request (retries); </li><li>  how to select values ‚Äã‚Äãfor timeouts; </li><li>  how not to kill the underlying servers at the time of the accident / overload; </li><li>  do you need health checks; </li><li>  how to handle flickering problems. </li></ul><br>  Under the cut of this report. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/2-j2ADWFkkE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><a name="habracut"></a><br>  <strong>About Speaker:</strong> Nikolay Sivko co-founder okmeter.io.  He worked as a system administrator and group administrator.  Supervised the operation in hh.ru.  Founded monitoring service okmeter.io.  In this report, experience in monitoring development is the main source of case studies. <br><br><h2>  What are we going to talk about? <br></h2><br>  This article will discuss web projects.  Below is an example of live production: the graph shows requests per second for a web service. <br><br><img src="https://habrastorage.org/webt/oy/5c/qt/oy5cqtlz-halhw7y5ayuz6xl9lm.png"><br><br>  When I talk about balancing, many people perceive it as ‚Äúwe need to distribute the load between servers ‚Äî the more precisely, the better.‚Äù <br><br><img src="https://habrastorage.org/webt/pm/g2/sp/pmg2spartsnxrxcyhzi_4-ui64g.png"><br><br>  In fact this is not true.  This problem is relevant for a very small number of companies.  More often business is concerned with errors and stability of the system. <br><br><img src="https://habrastorage.org/webt/6i/vb/-w/6ivb-w0bzdgl_oa-hkep6luitfi.png"><br><br>  The small peak on the chart is ‚Äúpyatisotki‚Äù, which the server returned within a minute, and then stopped.  From a business point of view, such as an online store, this small peak at 84 RPS "five hundred" is 5040 errors for real users.  Some did not find something in your catalog, others could not put the goods in the basket.  And this is very important.  Suppose this peak on the graph does not look very large, but <strong>in real users it is a lot</strong> . <br><br>  As a rule, everyone has such peaks, and administrators do not always react to them.  Very often, when a business asks what it was, he is answered: <br><br><ul><li>  "This is a short burst!" </li><li>  "This is just a release rolling." </li><li>  "The server is dead, but everything is in order." </li><li>  ‚ÄúVasya switched the network of one of the backends‚Äù. </li></ul><br>  Often, people <strong>do not even try to understand the reasons</strong> why this happened, and they do not do any post-work so that this does not happen again. <br><br><h2>  Thin tuning <br></h2><br>  I called the report ‚ÄúFine Tuning‚Äù (English Fine tuning), because I thought that not everyone would get to this task, but it would be worth it.  Why not get? <br><br><ul><li>  <strong>Not everyone gets to this task,</strong> because when everything works, it is not visible.  This is very important in case of problems.  Fakapy do not happen every day, and such a small problem requires very serious efforts to solve it. </li><li>  <strong>Need to think a lot.</strong>  Very often the admin - the person who sets up the balancing - is not able to solve this problem on his own.  Next we will see why. </li><li>  <strong>Cling lower levels.</strong>  This task is very closely connected with the development, with the adoption of decisions that affect your product and your users. </li></ul><br>  <strong>I argue that it‚Äôs time to do this task for several reasons:</strong> <br><br><ul><li>  The world is changing, becoming more dynamic, there are many releases.  It is said that it is now correctly released 100 times a day, and release is a future fix with a probability of 50 to 50 (just like the probability of meeting a dinosaur) </li><li>  In terms of technology, everything is also very dynamic.  Kubernetes and other orchestrators appeared.  There is no good old deployment, when one backend on some IP is turned off, the update rolls in, and the service rises.  Now in the process of rollout in k8s, the list of IP upstream is completely changing. </li><li>  Microservices: now everyone communicates over the network, which means you need to do it reliably.  Balancing plays an important role. </li></ul><br><h2>  Test stand <br></h2><br>  Let's start with simple obvious cases.  For clarity, I will use the test bench.  This is a Golang application that gives out http-200, or you can switch it to "give up http-503" mode. <br><br>  Run 3 instances: <br><br><ul><li>  127.0.0.1:20001 </li><li>  127.0.0.1:20002 </li><li>  127.0.0.1:20003 </li></ul><br>  We serve 100rps via yandex.tank via nginx. <br><br>  Nginx out of the box: <br><br><pre><code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; location / { proxy_pass http://backends; } }</code> </pre> <br><h3>  Primitive script </h3><br>  At some point, we turn on one of the backends in a 503 give mode, and we get exactly a third of errors. <br><br><img src="https://habrastorage.org/webt/qp/m1/ro/qpm1rolcydmcpwpvule4pw97b-o.png"><br><br>  It is clear that nothing works out of the box: nginx out of the box does not retry if received <strong>any response</strong> from the server. <br><br><pre> <code class="plaintext hljs">Nginx default: proxy_next_upstream error timeout;</code> </pre><br>  In fact, this is quite logical on the part of nginx developers: nginx is not entitled to decide for you what you want to retry and what not. <br><br>  Accordingly, we need retries - repeated attempts, and we start talking about them. <br><br><h2>  Retries <br></h2><br>  We need to find a compromise between: <br><br><ul><li>  User request - holy, hurt yourself, but answer.  We want to answer the user at all costs, the user is the most important. </li><li>  It is better to answer with an error than server overload. </li><li>  Data integrity (with nonidempotent queries), that is, you cannot repeat certain types of queries. </li></ul><br>  <strong>The truth, as usual, is somewhere between -</strong> we have to balance between these three points.  Let's try to understand what and how. <br><br>  I divided the failed attempts into 3 categories: <br><br>  1. <strong>Transport error</strong> <br>  For HTTP, the transport is TCP, and, as a rule, we are talking here about connection setup errors and connection setup timeouts.  In my report I will mention 3 common balancers (let's talk about Envoy a little further): <br><br><ul><li>  <strong>nginx</strong> : errors + timeout (proxy_connect_timeout); </li><li>  <strong>HAProxy</strong> : timeout connect; </li><li>  <strong>Envoy</strong> : connect-failure + refused-stream. </li></ul><br>  Nginx has the ability to say that a failed attempt is a connection error and a connection timeout;  HAProxy has a connection timeout, Envoy also - everything is standard and normal. <br><br>  2. <strong>Request timeout:</strong> <br>  Suppose that we sent a request to the server, successfully connected to it, but the answer does not come to us, we waited for it and understand that there is no point in waiting any longer.  This is called request timeout: <br><br><ul><li>  <strong>Nginx</strong> has: timeout (prox_send_timeout * + proxy_read_timeout *); </li><li>  <strong>HAProxy</strong> - <strong>OOPS: (</strong> - in principle, it does not. Many do not know that HAProxy, if it has successfully established a connection, will never try to resend the request. </li><li>  <strong>Envoy</strong> can do everything: timeout ||  per_try_timeout. </li></ul><br>  3. <strong>HTTP status</strong> <br>  All balancers, except HAProxy, are able to process, if the backend responded to you, but with some erroneous code. <br><br><ul><li>  <strong>nginx</strong> : http_ * </li><li>  <strong>HAProxy</strong> : <strong>OOPS: (</strong> </li><li>  <strong>Envoy</strong> : 5xx, gateway-error (502, 503, 504), retriable-4xx (409) </li></ul><br><h3>  Timeouts <br></h3><br>  Let's talk now in detail about timeouts, it seems to me that it is worth paying attention to this.  There will be no further rocket science - this is simply structured information about what generally happens and how it applies to it. <br><br><h4>  Connect timeout <br></h4><br>  Connect timeout is the time to establish a connection.  This is a characteristic of your network and your specific server, and is independent of the request.  Usually, the default for connect timeout is set to small.  In all proxies, the default value is large enough, and this is wrong - there should be <strong>one, sometimes tens of milliseconds</strong> (if we are talking about a network within the same DC). <br><br>  If you want to identify problem servers a little faster than these units, tens of milliseconds, you can adjust the load on the backend by setting a small backlog to accept TCP connections.  In this case, you can, when the backlog of the application is full, tell Linux to reset the backlog overflow.  Then you will be able to shoot a ‚Äúbad‚Äù overloaded backend a bit earlier than the connect timeout: <br><br><pre> <code class="plaintext hljs">fail fast: listen backlog + net.ipv4.tcp_abort_on_overflow</code> </pre> <br><h4>  Request timeout <br></h4><br>  Request timeout is not a network characteristic, but a <strong>characteristic of a group of requests</strong> (handler).  There are different requests - they are different in severity, they have completely different logic inside, they need to apply to completely different repositories. <br><br>  Nginx, as such, <strong>does not have a timeout for the entire request.</strong>  He has: <br><br><ul><li>  proxy_send_timeout: time between two successful write operations write (); </li><li>  proxy_read_timeout: time between two successful read operations read (). </li></ul><br>  That is, if you have a backend slowly, one byte one time, it gives something to the timeout, then everything is fine.  As such, nginx has no request_timeout.  But we are talking about upstream.  In our data center, they are controlled by us, so with the assumption that there is no slow loris in the network, then, in principle, read_timeout can be used as request_timeout. <br><br>  Envoy has it all: timeout ||  per_try_timeout. <br><br><h4>  Choose request timeout <br></h4><br>  Now the most important thing, in my opinion, is which request_timeout.  We proceed from how long it is permissible for a user to wait ‚Äî this is a certain maximum.  It is clear that the user will not wait more than 10 seconds, so you need to respond to him faster. <br><br><ul><li>  If we want to handle the failure of a single server, then the timeout must be less than the maximum allowable timeout: <strong>request_timeout &lt;max.</strong> </li><li>  If you want to have <strong>2 guaranteed attempts to</strong> send a request for two different backends, then the timeout for one attempt is equal to half of this allowable interval: <strong>per_try_timeout = 0.5 * max.</strong> </li><li>  There is also an intermediate option - <strong>2 optimistic attempts</strong> in case the first backend blunted, but the second one <strong>responds</strong> quickly: <strong>per_try_timeout = k * max (where k&gt; 0.5).</strong> </li></ul><br>  There are different approaches, but in general <strong>it‚Äôs difficult to choose a timeout</strong> .  There will always be boundary cases, for example, the same handler is processed in 99% of cases in 10 ms, but there are 1% of cases when we wait for 500 ms, and this is normal.  It will have to settle. <br><br>  With this 1% you need to do something, because the whole group of requests must, for example, comply with the SLA and fit into 100 ms.  Very often in these moments the application is processed: <br><br><ul><li>  A paging appears in those places where it is impossible to give all the data entirely in timeout. </li><li>  Admin / reports are separated into a separate group of urls in order to raise timeout for them, and yes user requests, on the contrary, lower them. </li><li>  We fix / optimize those requests that do not fit into our timeout. </li></ul><br>  Immediately we need to take a not very simple psychological decision that if we don‚Äôt have time to answer the user within the allotted time, we give an error (it‚Äôs like in the ancient Chinese saying: ‚ÄúIf the mare died, let it fall!‚Äù) <strong>.</strong> <br><br>  After that, the process of monitoring your service from the user's point of view is simplified: <br><br><ul><li>  If there are errors, everything is bad, it needs to be repaired. </li><li>  If there are no errors, we are at the right time to answer, then everything is fine. </li></ul><br><h3>  Speculative retries <br></h3><br>  We made sure that choosing the timeout value is quite difficult.  As you know, to simplify something, you need to complicate something :) <br><br>  <strong>Speculative Retry</strong> - a repeated request to another server, which is started by some condition, but the first request is not interrupted.  The answer we take from the server that responded faster. <br><br>  I have not seen this feature in the well-known balancers, but there is a great example of Cassandra (rapid read protection): <br><br>  speculative_retry = N ms |  <strong>M <sup>th</sup> percentile</strong> <br><br>  Thus, you <strong>do not need to time out</strong> .  You can leave it at an acceptable level and in any case have a second attempt to get an answer to the request. <br><br>  In Cassandra there is an interesting opportunity to set a static speculative_retry or dynamic, then the second attempt will be made through the response time percentile.  Cassandra collects statistics on the response times of previous requests and adapts a specific timeout value.  It works quite well. <br><br>  In this approach, everything is on the balance between reliability and parasitic load. Not the servers You provide reliability, but sometimes you get extra requests to the server.  If you hurry somewhere and send a second request, and the first one answered, the server received a little more load.  In a single case, this is a minor issue. <br><br><img src="https://habrastorage.org/webt/uv/7c/bs/uv7cbswancegyh5vc8t7mwvr8uy.png"><br><br>  Timeout consistency is another important aspect.  We'll talk more about the request cancellation, but in general, if the timeout for the entire user request is 100 ms, then there is no point in setting the timeout for the request to the database for 1 s.  There are systems that allow you to do this dynamically: the service to the service transmits the rest of the time that you will wait for an answer to this request.  It's complicated, but if you suddenly need it, you can easily find how to do it in the same Envoy. <br><br>  What else do you need to know about retry? <br><br><h3>  Point of no return (V1) <br></h3><br>  Here V1 is not version 1. In aviation, there is such a thing - speed V1.  This is the speed, after which the acceleration along the runway cannot be slowed down.  It is necessary to take off, and then decide on what to do next. <br><br>  The same point of no return is in load balancers: <strong>when you sent 1 byte of response to your client, no errors can be corrected</strong> .  If the backend dies at this point, no retries will help.  You can only reduce the likelihood of such a scenario triggering, make a graceful shutdown, that is, say to your application: ‚ÄúYou are not accepting new requests now, but you‚Äôre working upon the old ones!‚Äù And only then extinguish it. <br><br>  If you control the client, this is some tricky Ajax or mobile application, it can try to repeat the request, and then you can get out of this situation. <br><br><h3>  Point of no return [Envoy] <br></h3><br>  Envoy was such a strange thing.  There is per_try_timeout - it limits how much each attempt to get an answer to a request can take.  If this timeout worked, but the backend had already begun to respond to the client, then everything was interrupted, the client received an error. <br><br>  My colleague Pavel Trukhanov ( <a href="https://habr.com/users/tru_pablo/" class="user_link">tru_pablo</a> ) made a <a href="https://github.com/envoyproxy/envoy/pull/3221">patch</a> , which will be in 1.7 in master Envoy.  Now it works as it should: if the response began to be transmitted, only global timeout will work. <br><br><h3>  Retries: need to limit <br></h3><br>  Retries are good, but there are so-called killer queries: heavy queries that perform very complex logic, access the database a lot and often do not fit per_try_timeout.  If we send retry again and again, then we kill our base.  Because <strong>in most (99.9%) database services there is no request cancellation</strong> . <br><br>  Request cancellation means that the client has unhooked, you need to stop all work right now.  Golang now actively promotes this approach, but, unfortunately, it ends with a backend, and many database repositories do not support this. <br><br>  Accordingly, retries need to be limited, which allows almost all balancers (we are no longer considering HAProxy from now on). <br><br>  <strong>Nginx:</strong> <br><br><ul><li>  proxy_next_upstream_timeout (global) </li><li>  proxt_read_timeout ** as per_try_timeout </li><li>  proxy_next_upstream_tries </li></ul><br>  <strong>Envoy:</strong> <br><br><ul><li>  timeout (global) </li><li>  per_try_timeout </li><li>  num_retries </li></ul><br>  In Nginx, we can say that we are trying to do retries for the duration of the X window, that is, at a given time interval, for example, 500 ms we do as many retries as will fit.  Or there is a setting that limits the number of repeated samples.  In <strong>Envoy</strong> as well - the number or timeout (global). <br><br><h4>  Retries: use [nginx] <br></h4><br>  Consider an example: we set up retry in nginx - accordingly, having received HTTP 503, we try to send a request to the server again.  Then turn off the <strong>two</strong> backends. <br><br><pre> <code class="plaintext hljs">upstream backends { server 127.0.0.1:20001; server 127.0.0.1:20002; server 127.0.0.1:20003; } server { listen 127.0.0.1:30000; proxy_next_upstream error timeout http_503; proxy_next_upstream_tries 2; location / { proxy_pass http://backends; } }</code> </pre><br>  Below are the graphics of our test bench.  There are no errors on the top chart, because there are very few of them.  If you leave only errors, it is clear that they are. <br><br><img src="https://habrastorage.org/webt/3h/sx/aq/3hsxaq8qyifcoyq3mvcyzmxm_cc.png"><br><br><img src="https://habrastorage.org/webt/sc/f_/2w/scf_2wz9tctmouvrs9hpqtpmau0.png"><br><br>  <strong>What happened?</strong> <br><br><ul><li>  proxy_next_upstream_tries = <strong>2.</strong> </li><li>  In the case when you make the first attempt to the "dead" server, and the second to the other "dead", you get HTTP-503 in the case of <strong>both</strong> attempts to the "bad" servers. </li><li>  There are few errors, since nginx "bans" a bad server.  That is, if some errors returned from the backend in nginx, it stops making the next attempts to send a request to it.  This is governed by the variable <strong>fail_timeout.</strong> </li></ul><br>  But there are mistakes, and it does not suit us. <br><br>  <strong>What to do with it?</strong> <br><br>  We can either increase the number of retries (but then we return to the problem of ‚Äúkiller requests‚Äù), or we can reduce the likelihood of a request for dead backends.  This can be done with <strong>health checks.</strong> <br><br><h2>  Health checks <br></h2><br>  I propose to consider health checks as optimizing the process of choosing a ‚Äúlive‚Äù server.  <strong>This in no way gives any guarantees.</strong>  Accordingly, during the execution of a user request, we are more likely to get only to the ‚Äúlive‚Äù servers.  The balancer regularly calls at a specific URL, the server responds to it: "I am alive and ready." <br><br><h4>  Health checks: in terms of backend <br></h4><br>  From the point of view of the backend you can do interesting things: <br><br><ul><li>  Check the availability of all underlying subsystems on which the work of the backend depends: the necessary number of connections to the database are established, there are free connections in the pool, etc., etc. </li><li>  On the Health checks URL, you can hang your logic if the balancer you are using is not particularly intelligent (let's say you take the Load Balancer from the hoster).  The server can remember that ‚Äúin the last minute I have given so many errors - probably, I‚Äôm some kind of‚Äú wrong ‚Äùserver, and for the next 2 minutes I‚Äôll respond with‚Äú five hundred ‚Äùon Health checks.  Thus, I will ban myself myself! ‚ÄùThis sometimes saves a lot when you have an uncontrolled Load Balancer. </li><li>  As a rule, the check interval is about a second, and it is necessary that the Health check handler does not kill your server.  It should be easy. </li></ul><br><h4>  Health checks: implementation <br></h4><br>  As a rule, everything here is all about the same: <br><br><ul><li>  Request; </li><li>  Timeout on it; </li><li>  Interval, through which we do checks.  The clever proxies have <strong>jitter</strong> , that is, some randomization so that all Health checks do not come to the backend all at once, and do not kill it. </li><li>  <strong>Unhealthy threshold</strong> - the threshold for how many <strong>unhealthy</strong> Health checks must pass in order to mark the service as Unhealthy. </li><li>  <strong>Healthy threshold</strong> - on the contrary, how many successful attempts must go through in order for the server to return to service. </li><li>  Additional logic.  You can disassemble Check status + body, etc. </li></ul><br>  Nginx implements the Health check functions only in the paid version of nginx +. <br><br>  I will note feature of <strong>Envoy</strong> , it has Health check <strong>panic mode.</strong>  When we are banned as ‚Äúunhealthy,‚Äù more than N% of hosts (say, 70%), he believes that all of our Health checks are lying, and all the hosts are actually alive.  In a very bad case, this will help you not to run into a situation where you yourself have shot your leg, and banned all the servers.  This is a way to hedge once again. <br><br><h2>  Putting it all together <br></h2><br>  Usually for Health checks put: <br><br><ul><li>  Or nginx +; </li><li>  Or nginx + something else :) </li></ul><br>  In our country, there is a tendency to put nginx + HAProxy, because in the free version of nginx there are no health checks, and until 1.11.5 there was no limit on the number of connections to the backend.  But this option is bad because HAProxy does not know how to retract after a connection is established.  Many people think that if HAProxy returns an error to nginx, and nginx retry, then everything will be fine.  Not really.  You can go to another HAProxy and the same backend, because the backend pools are the same.  So you enter for yourself another level of abstraction, which reduces the accuracy of your balancing and, accordingly, the availability of the service. <br><br>  We have nginx + Envoy, but, if confused, we can restrict ourselves to Envoy only. <br><br><h2>  What is such an Envoy? <br></h2><br>  Envoy is a trendy youth load balancer, originally developed in Lyft, written in C ++.  <strong>Out of the box can a bunch of buns on our current topic.</strong>  You probably saw it as a Service Mesh for Kubernetes.  As a rule, Envoy acts as a data plane, that is, directly balances traffic, and there is also a control plane, which provides information about what to distribute the load (service discovery, etc.). <br><br>  I'll tell you a few words about his buns. <br><br>  To increase the likelihood of a successful response when retry on the next attempt, you can sleep for a while and wait until the backend comes around.  In this way we will handle short problems on the database.  Envoy has a <strong>backoff for retries</strong> - pauses between retries.  Moreover, the delay interval between attempts increases exponentially.  The first retry occurs after 0-24 ms, the second after 0-74 ms, and then for each next attempt the interval increases, and the specific delay is chosen randomly from this interval. <br><br>  The second approach is not an Envoy-specific thing, but a pattern called <strong>Circuit breaking</strong> (lit. chain break or fuse).  When we have a backend blunts, in fact, we are trying to finish it every time.  This is because users in any incomprehensible situation press the refresh-at page, sending you more and more new requests.  Your balancers are nervous, send retries, the number of requests is increasing - the load is growing, and in this situation it would be good not to send requests. <br><br>  Circuit breaker just allows you to determine that we are in such a state, to quickly shoot off an error and let the backends ‚Äúcatch their breath‚Äù. <br><br><img src="https://habrastorage.org/webt/xb/mm/i8/xbmmi88cqacoqvkzdmujynq6da0.gif"><br>  <em>Circuit breaker (hystrix like libs),</em> <a href="https://www.ebayinc.com/stories/blogs/tech/application-resiliency-using-netflix-hystrix/"><em>original</em></a> <em>on ebay blog.</em> <br><br>  Above is the Circuit breaker circuit from Hystrix.  Hystrix is ‚Äã‚Äãa Java library from Netflix that is designed to implement fault tolerance patterns. <br><br><ul><li>  The ‚Äúfuse‚Äù can be in the ‚Äúclosed‚Äù state when all requests are sent to the backend and there are no errors. </li><li>  When a certain fail-threshold is triggered, that is, some errors have occurred, the circuit breaker goes into the ‚ÄúOpen‚Äù state.  It quickly returns an error to the client, and requests do not fall on the backend. </li><li>  Once in a while, still a small part of requests is sent to the backend.  If an error is triggered, the state remains ‚ÄúOpen‚Äù.  If everything starts to work well and respond, the "fuse" is closed, and work continues. </li></ul><br>  In Envoy, as such, this is not all.  There are high-level limits on the fact that there can not be more than N requests for a particular upstream group.  If more, something is wrong here - we return an error.  There can be no more N active retries (i.e. retries that are happening right now). <br><br>  You did not have retries, something exploded - send retries.  Envoy understands that more than N is not normal, and all requests should be shot off with an error. <br><br>  <strong>Circuit breaking [Envoy]</strong> <br><br><ul><li>  Cluster (upstream group) max connections </li><li>  Cluster max pending requests </li><li>  Cluster max requests </li><li>  Cluster max active retries </li></ul><br>  This simple thing works well, understandably configurable, no need to come up with special parameters, and the default settings are quite good. <br><br><h4>  Circuit breaker: our experience <br></h4><br>  Previously, we had an HTTP metrics collector, that is, agents installed on our clients' servers sent metrics to our cloud via HTTP.  If we have any problems in the infrastructure, the agent writes the metrics to his disk and then tries to send them to us. <br><br>  And the agents are constantly trying to send us data, they are not upset that we somehow do not respond, and do not leave. <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If, after recovery, we allow the full flow of requests (besides, the load will be even more normal, as we have accumulated metrics) on the servers, most likely everything will fall again, as some components will end up with a cold cache or something like that. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To cope with this problem, we clamped the incoming stream of the record through nginx limit req. That is, we say that we are processing, say, 200 RPS. When it all came to normal mode, we removed the restriction, because in a normal situation the collector is able to record much more than the crisis limit req.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then for some reason we switched to our protocol on top of TCP and lost HTTP proxying buns (the ability to use nginx limit req). And in any case, it was necessary to put this site in order. We no longer wanted to change limit req with our hands. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have enough opportunity, because we control both the client and the server. Therefore, we are </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in the agent of the</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Circuit breaker code, who understood that if he got N errors in a row, he needs to sleep, and after some time, and exponentially increasing, try again. When everything is normalized, it adds all the metrics, as it has a spool on the disk. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">server</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">we added Circuit breaker code of all calls to all subsystems + request cancellation (where possible). </font><font style="vertical-align: inherit;">Thus, if we received N errors from Cassandra, N errors from Elastic, from the base, from a neighboring service - from anything, we quickly give an error and do not perform further logic. </font><font style="vertical-align: inherit;">We just shoot a mistake and that's it - wait until it normalizes. </font></font><br><br><img src="https://habrastorage.org/webt/jo/l2/jk/jol2jk44vlcmz3twgvr0jgtpbii.png"><br><br><img src="https://habrastorage.org/webt/1a/7b/x4/1a7bx4yq20uehqoagd4tutqxgmw.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The graphs above show that we do not get a surge of errors in case of problems (conditionally: gray is ‚Äútwo hundred,‚Äù red is ‚Äúfive hundred‚Äù). </font><font style="vertical-align: inherit;">It can be seen that at the time of problems from 800 RPS to the backend flew 20-30. </font><font style="vertical-align: inherit;">This allowed our backend to "catch our breath", to rise, and continue to work well.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The most difficult mistakes </font></font><br></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The most difficult mistakes are those that are ambiguous. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If your server just turned off and does not turn on, or you realize that it is dead and you finished it yourself, this is actually a gift of fate. This option is well formalized. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Much worse when one server starts to behave unpredictably, for example, the server responds to all requests with errors, and to Health checks - HTTP 200. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I will give an example from life. </font></font><br><br><img src="https://habrastorage.org/webt/yu/cy/9s/yucy9sofdr-z7brvjedmnxt4_gc.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have a Load Balancer, 3 nodes, on each of which is an application and under it Cassandra. Applications access all instances of Cassandra, and all Cassandra interact with neighbors, because Cassandra has a two-level coordinator model and data noda. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schr√∂dinger's server is one of them:</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">kernel: NETDEV WATCHDOG: eth0 (ixgbe): transmit queue 3 timed out.</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The following happened there: in the network driver, the bug (in the Intel drivers they happen), and one of the 64 transmission queues just stopped sending. Accordingly, 1/64 of all traffic is lost. This can occur before the reboot, it can not be treated. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I, as the admin, are worried in this situation, not why there are such bugs in the driver. I‚Äôm worried why when you build a system without a single point of failure, problems on one server ultimately lead to the failure of the entire production. I was wondering how this can be resolved, and on the machine. I do not want to wake up at night to turn off the server. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cassandra: coordinator -&gt; nodes</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cassandra, there are those speculative retries, and this situation is handled very easily. There is a slight increase in latency on the 99 percentile, but this is not fatal, and in general everything works. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">App -&gt; cassandra coordinator</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nothing works out of the box. When an application knocks on Cassandra and hits the coordinator on a ‚Äúdead‚Äù server, it doesn‚Äôt work out in any way, errors are generated, latency increases, etc. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, we use gocql - a heaped enough cassandra client. We just did not use all its features. There is a HostSelectionPolicy in which we can slip the </font></font><a href="https://github.com/bitly/go-hostpool"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bitly / go-hostpool library</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . It uses Epsilon greedy algorithms to find and remove outliers from the list.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I will try to briefly explain how the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Epsilon-greedy</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> algorithm works </font><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is a multi-armed bandit problem: you are in a room with slot machines, you have several coins, and you have to try to win as much as possible for N attempts. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The algorithm includes two stages:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The phase of " </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">explore"</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - when you explore: 10 coins spend on it to determine which machine is better.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Phase " </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">exploit"</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - the remaining coins are lowered into the best machine.</font></font><br></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accordingly, a small number of requests (10 - 30%) are sent to the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">round</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">robin</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> simply to all hosts, we consider failures, the response time, we choose the best. </font><font style="vertical-align: inherit;">Then 70 - 90% of requests are sent to the best and update their statistics. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Host-pool each server evaluates by the number of successful responses and response time. </font><font style="vertical-align: inherit;">That is, he takes the fastest server in the end. </font><font style="vertical-align: inherit;">He calculates the response time as a weighted average (fresh measurements are more significant - what was right now is much more significant). </font><font style="vertical-align: inherit;">Therefore, we periodically reset the old measurements. </font><font style="vertical-align: inherit;">So with a good chance our server, which returns errors or tupit, goes, and it almost stops receiving requests.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Subtotal </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We added ‚Äúarmor-piercing‚Äù (fault tolerance) at the application level ‚Äî Cassandra and Cassandra coordinator-data. </font><font style="vertical-align: inherit;">But if our balancer (nginx, Envoy ‚Äî whatever) sends requests for a ‚Äúbad‚Äù Application, which will blunt the address to any Cassandra, because it itself has a non-working grid, in any case, we will get problems. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the Envoy out of box there is </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Outlier detection</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> by:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Consecutive http-5xx. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Consecutive gateway errors (502,503,504). </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Success rate. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">By successive "five hundred" it can be understood that something is wrong with the server and to ban it. </font><font style="vertical-align: inherit;">But not forever, but for a time interval. </font><font style="vertical-align: inherit;">Then a small number of requests begins to arrive there - if it is stupid again, we will ban it, but for a longer interval. </font><font style="vertical-align: inherit;">Thus, it all comes down to the fact that virtually no requests fall on this problematic server. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the other hand, to protect against the uncontrollable explosion of this ‚Äúcleverness‚Äù, we have max_ejection_percent. </font><font style="vertical-align: inherit;">This is the maximum number of hosts we can count as an outlier, as a percentage of all available. </font><font style="vertical-align: inherit;">That is, if we banned 70% of the hosts, it does not count, everyone is unbanned - in short, amnesty! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is a very cool thing that works great, while it is simple and clear - I advise you!</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> TOTAL </font></font><br></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I hope I convinced you that you need to fight with similar cases. </font><font style="vertical-align: inherit;">I believe that to deal with the emission of errors and latency is definitely worth it in order to:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> To do autodelivery, to be released the necessary number of times a day, etc. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Sleep well at night, do not jump up once again by SMS, but fix problem servers during working hours. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Obviously, </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">nothing works out of the box</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ; accept this fact. This is normal, because there are so many decisions that you have to make yourself, and the software cannot do it for you - it waits for you to configure it. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No need to chase the most sophisticated balancer</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . For 99% of the problems, the standard features of nginx / </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HAProxy</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> / Envoy are </font><font style="vertical-align: inherit;">enough </font><font style="vertical-align: inherit;">. A more sophisticated proxy will be needed if you want to make everything absolutely perfect and remove the five hundred micro-spikes. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The point is not about the specific proxy</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (if it is not HAProxy :)), </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">but how you set it up.</font></font></strong> <br><br><blockquote>  <a href="http://devopsconf.io/moscow/2018/">DevOpsConf Russia</a>      Kubernetes         .          <a href="http://devopsconf.io/moscow/2018/abstracts/"></a> . <br><br>    ,       ‚Äî <a href="http://eepurl.com/bN_0E1"></a>      DevOps. <br><br>    ,   ,   <a href="https://www.youtube.com/c/DevOpsChannel/">YouTube-</a> ‚Äî              . <br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/423085/">https://habr.com/ru/post/423085/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../423073/index.html">Interplanetary File System - Trivial Hash (Identity), DAG Block, and Protocol Buffers</a></li>
<li><a href="../423075/index.html">Why are CFOs so eager to translate IT capital expenditures into operational ones?</a></li>
<li><a href="../423077/index.html">X86 assembler guide for beginners</a></li>
<li><a href="../423079/index.html">The main theses of the interview with Ilona Mask Joe Rogan</a></li>
<li><a href="../423083/index.html">How I became a developer at ABBYY</a></li>
<li><a href="../423087/index.html">Don't push my eyes</a></li>
<li><a href="../423089/index.html">Programmers at MBLT DEV 2018</a></li>
<li><a href="../423091/index.html">Flutter for Android developers. How to create a UI for an Activity using Flutter</a></li>
<li><a href="../423093/index.html">Increase the randomness of the fact that it is [probably] [almost] random</a></li>
<li><a href="../423095/index.html">What's new showed on Apple presentation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>