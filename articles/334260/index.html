<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>VESNIN Server: First Disk Subsystem Tests</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Debugging an instance of the server of the first revision, we partially tested the speed of the I / O subsystem. In addition to the numbers with test ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>VESNIN Server: First Disk Subsystem Tests</h1><div class="post__text post__text-html js-mediator-article">  Debugging an instance of the server of the first revision, we partially tested the speed of the I / O subsystem.  In addition to the numbers with test results, in the article I tried to reflect the observations that can be useful to engineers when designing and configuring application I / O. <br><br><img src="https://habrastorage.org/web/155/19d/2e2/15519d2e294e475baf00b966ccadca09.jpg"><br><a name="habracut"></a><br><h2>  Test method and load </h2><br>  I'll start from afar.  Up to 4 processors (up to 48 POWER8 cores) and a lot of memory (up to 8 TB) can be put into our server.  This opens up many opportunities for applications, but a large amount of data in RAM entails the need to store them somewhere.  The data must be quickly retrieved from the disks and also quickly re-stuffed.  In the near future we are waiting for the beautiful world of disaggregated non-volatile and shared memory.  In this beautiful future, perhaps, there will be no need for a backing store at all.  The processor will copy the bytes directly from the internal registers to non-volatile memory with an access time like DRAM (tens NS) and the memory hierarchy will be reduced by one floor.  All this is then, now all the data are usually stored on a block disk subsystem. <br><br>  We define the initial conditions for testing: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The server has a relatively large number of cores.  This is useful for parallel processing of large amounts of data.  That is, one of the priorities is the high throughput capacity of the I / O subsystem with a large number of parallel processes.  As a result, it is logical to use microbenmark and set up quite a lot of parallel threads. <br><br>  In addition, the I / O subsystem is built on NVMe disks, which can process many requests in parallel.  Accordingly, we can expect a performance boost from asynchronous I / O.  In other words, interesting is the high throughput in parallel processing.  This is more appropriate for the purpose of the server.  Performance on single-threaded applications, and achieving a minimum response time, although it is one of the goals of future tuning, is not considered in this test. <br><br>  Benchmarks of individual NVMe disks are full on the network, it‚Äôs not worth creating extra ones.  In this article, I consider the disk subsystem as a whole, so the disks will be loaded mainly in groups.  As a load, we will use 100% random read and write with a block of different sizes. <br><br><h2>  What metrics look? </h2><br>  On a small block of 4K, we‚Äôll look at IOPS (number of operations per second) and secondarily latency (response time).  On the one hand, the focus on IOPS is a legacy from hard drives, where random access by a small unit brought the greatest delays.  In the modern world, all-flash systems are capable of issuing millions of IOPS, often more than software can use.  Now ‚ÄúIOPS-intensive loads‚Äù are valuable because they show the balance of the system in terms of computing resources and bottlenecks in the software stack. <br><br>  On the other hand, for some of the tasks, it is not the number of operations per second that is important, but the maximum throughput on a large block ‚â•64KB.  For example, when data is drained from memory into disks (database snapshot) or the database is loaded into memory for in-memory computing, warming up the cache.  For a server with 8 terabytes of memory, the throughput of the disk subsystem is of particular importance.  On a large block we will look at the bandwidth, that is, megabytes per second. <br><br><h2>  Built-in disk subsystem </h2><br>  The server disk subsystem can include up to 24 NVMe standard disks.  The drives are evenly distributed across four processors using two PMC 8535 PCI Express switches. Each switch is logically divided into three virtual switches: one x16 and two x8.  Thus, for each processor is available PCIe x16, or up to 16 GB / s.  6 NVMe disks are connected to each processor.  In total, we expect throughput up to 60 GB / s from all drives. <br><br><img src="https://habrastorage.org/web/95e/057/782/95e057782fba41748ca77581cc0c4c70.jpg"><br><br>  For tests, I have an instance of a server with 4 processors (8 cores per processor, the maximum is 12 cores).  The disks are connected to two sockets out of four.  That is, it is half of the maximum configuration of the disk subsystem.  On the backplane with PCI Express switches of the first revision, two Oculink connectors were faulty, and therefore only half of the drives are available.  In the second revision, this has already been fixed, but here I was able to put only half of the disks, namely, the following configuration was obtained: <br><br><ul><li>  4 √ó Toshiba PX04PMB160 </li><li>  4 √ó Micron MTFDHAL2T4MCF-1AN1ZABYY </li><li>  3 √ó INTEL SSDPE2MD800G4 </li><li>  1 √ó SAMSUNG MZQLW960HMJP-00003 </li></ul><br>  The variety of disks is caused by the fact that we simultaneously test them to form a nomenclature of standard components (disks, memory, etc.) from 2-3 manufacturers. <br><br><h2>  Load minimum configuration </h2><br>  To begin, let's perform a simple test in the minimum configuration - one disk (Micron MTFDHAL2T4MCF-1AN1ZABYY), one POWER8 processor and one fio stream with a queue = 16. <br><br><pre><code class="bash hljs">[global] ioengine=libaio direct=1 group_reporting=1 bs=4k iodepth=16 rw=randread [ /dev/nvme1n1 P60713012839 MTFDHAL2T4MCF-1AN1ZABYY] stonewall numjobs=1 filename=/dev/nvme1n1</code> </pre> <br>  It turned out like this: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># numactl --physcpubind=0 ../fio/fio workload.fio /dev/nvme1n1 P60713012839 MTFDHAL2T4MCF-1AN1ZABYY: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=16 fio-2.21-89-gb034 time 3233 cycles_start=1115105806326 Starting 1 process Jobs: 1 (f=1): [r(1)][13.7%][r=519MiB/s,w=0KiB/s][r=133k,w=0 IOPS][eta 08m:38s] fio: terminating on signal 2 Jobs: 1 (f=0): [f(1)][100.0%][r=513MiB/s,w=0KiB/s][r=131k,w=0 IOPS][eta 00m:00s] /dev/nvme1n1 P60713012839 MTFDHAL2T4MCF-1AN1ZABYY: (groupid=0, jobs=1): err= 0: pid=3235: Fri Jul 7 13:36:21 2017 read: IOPS=133k, BW=519MiB/s (544MB/s)(41.9GiB/82708msec) slat (nsec): min=2070, max=124385, avg=2801.77, stdev=916.90 clat (usec): min=9, max=921, avg=116.28, stdev=15.85 lat (usec): min=13, max=924, avg=119.38, stdev=15.85 ‚Ä¶‚Ä¶‚Ä¶... cpu : usr=20.92%, sys=52.63%, ctx=2979188, majf=0, minf=14</span></span></code> </pre><br>  What do we see here?  Received 133K IOPS with a response time of 119 ¬µs.  Note that the CPU usage is 73%.  It's a lot.  What is the processor busy with? <br><br>  We use asynchronous I / O, and this simplifies the analysis of the results.  fio separately counts slat (submission latency) and clat (completion latency).  The first includes the execution time of the read system call before returning to the user space.  That is, all the overhead of the kernel before the request leaves in iron is shown separately. <br><br>  In our case, the slat is only 2.8 Œºs per request, but given the repetition of this 133,000 times per second, we get a lot: 2.8 Œºs * 133,000 = 372 ms.  That is, the processor spends 37.2% of the time only on the IO submission.  And there is also the fio code itself, interrupts, the work of the asynchronous I / O driver. <br><br>  The total processor load is 73%.  It looks like another fio won't pull the kernel, but let's try: <br><br><pre> <code class="bash hljs">Starting 2 processes Jobs: 2 (f=2): [r(2)][100.0%][r=733MiB/s,w=0KiB/s][r=188k,w=0 IOPS][eta 00m:00s] /dev/nvme1n1 P60713012839 MTFDHAL2T4MCF-1AN1ZABYY: (g=0): rw=randread, bs=(R) pid=3391: Sun Jul 9 13:14:02 2017 <span class="hljs-built_in"><span class="hljs-built_in">read</span></span>: IOPS=188k, BW=733MiB/s (769MB/s)(430GiB/600001msec) slat (usec): min=2, max=963, avg= 3.23, stdev= 1.82 clat (nsec): min=543, max=4446.1k, avg=165831.65, stdev=24645.35 lat (usec): min=13, max=4465, avg=169.37, stdev=24.65 ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ cpu : usr=13.71%, sys=36.23%, ctx=7072266, majf=0, minf=72</code> </pre><br>  With two threads, the speed grew from 133k to 180k, but the core is overloaded.  According to the top processor utilization 100% and clat increased.  That is, 188k is the limit for one core on this load.  It is easy to see that the growth of clat is caused by the processor, and not by the disk.  Let's see the 'biotop' (): <br><br><pre> <code class="bash hljs">PID COMM D MAJ MIN DISK I/O Kbytes AVGus 3553 fio R 259 1 nvme0n1 633385 2533540 109.25 3554 fio R 259 1 nvme0n1 630130 2520520 109.25</code> </pre><br>  Because of the enabled tracing, the speed slipped somewhat, but the response time from the disks ~ 109 ¬µs is the same as in the previous test.  Measurements in another way (sar -d) show the same numbers. <br><br>  For the sake of curiosity, it is interesting to see what the processor is doing: <br><br><img src="https://habrastorage.org/web/d12/8ae/625/d128ae62545f4544b95b71b69c1b2fd0.png"><br>  <i>Single core (perf + flame graph) load profile with multithreading disabled and two fio processes running.</i>  <i>As you can see, it does something 100% of the time (idle = 0%).</i> <br><br>  Visually, processor time is more or less evenly distributed between a large number of user (fio code itself) and nuclear functions (asynchronous I / O, block level, driver, many small peaks are interrupts).  You cannot see any single function where an abnormally large amount of CPU time was spent.  It looks good and, when viewing stacks, it doesn‚Äôt come up with ideas that one could twist. <br><br><h2>  The effect of POWER8 multithreading on I / O speed </h2><br>  So, we found out that with active IO load, the processor is easy to overload.  The number of processor utilization indicates that it is busy for the operating system, but says nothing about loading the processor nodes.  In particular, the processor may appear to be loaded while waiting for external components, such as memory.  We don‚Äôt have a goal here to figure out processor utilization efficiency, but to understand the potential of tuning, it‚Äôs interesting to take a look at the ‚ÄúCPU counters‚Äù that are partially accessible through 'perf'. <br><br><pre> <code class="bash hljs">root@vesninl:~<span class="hljs-comment"><span class="hljs-comment"># perf stat -C 0 Performance counter stats for 'CPU(s) 0': 2393.117988 cpu-clock (msec) # 1.000 CPUs utilized 7,518 context-switches # 0.003 M/sec 0 cpu-migrations # 0.000 K/sec 0 page-faults # 0.000 K/sec 9,248,790,673 cycles # 3.865 GHz (66.57%) 401,873,580 stalled-cycles-frontend # 4.35% frontend cycles idle (49.90%) 4,639,391,312 stalled-cycles-backend # 50.16% backend cycles idle (50.07%) 6,741,772,234 instructions # 0.73 insn per cycle # 0.69 stalled cycles per insn (66.78%) 1,242,533,904 branches # 519.211 M/sec (50.10%) 19,620,628 branch-misses # 1.58% of all branches (49.93%) 2.393230155 seconds time elapsed</span></span></code> </pre><br>  The output above shows that IPC (insn per cycle) 0.73 is not so bad, but theoretically it can be up to 8 on Power8. In addition, 50% of the ‚Äúbackend cycles idle‚Äù (PM_CMPLU_STALL metric) can mean waiting for memory.  That is, the processor is busy for the Linux scheduler, but the resources of the processor itself are not particularly loaded.  It is quite possible to expect an increase in performance from the inclusion of multithreading (SMT), with an increase in the number of threads.  The result of what happened when you turned on SMT is shown in the graphs.  I received a significant increase in speed from additional fio processes running on other threads of the same processor.  For comparison, there is a case when all the threads work on different cores (diff cores). <br><br><img src="https://habrastorage.org/web/f0c/883/4ea/f0c8834ea6c34917a57bb81fb31af17a.png"><br><img src="https://habrastorage.org/web/79c/9e7/87c/79c9e787c70f4bf5ae4750435b89b454.png"><br><br>  The graphs show that the inclusion of SMT8 gives an almost two-fold increase in speed and a decrease in response time.  Quite well and we are shooting&gt; 400K IOPS from one core!  Along the way, we see that one core, even with SMT8 enabled, is not enough to fully load the NVMe disk.  Spreading fio streams across different cores, we get almost twice the best disk performance - this is what modern NVMe can do. <br><br>  Thus, if the application architecture allows you to adjust the number of writing / reading processes, then it is better to adjust their number to the number of physical / logical cores in order to avoid slowdowns from overloaded processors.  A single NVMe disk can easily overload the processor core.  The inclusion of SMT4 and SMT8 gives a multiple increase in performance and can be useful for loads with intensive input-output. <br><br><h2>  Impact of NUMA Architecture </h2><br>  For load balancing, 24 internal NVMe server disks are evenly connected to four processor sockets.  Accordingly, for each disk there is a ‚Äúnative‚Äù socket (NUMA locality) and a ‚Äúremote‚Äù one.  If an application accesses disks from a ‚Äúremote‚Äù socket, there may be overhead from the effect of the interprocessor bus.  We decided to see how access from a remote socket affects the final disk performance.  For the test, run fio again and use numactl to bind the fio processes to the same socket.  First, to the "native" socket, then to the "remote".  The purpose of the test is to understand whether it is worth wasting energy on setting up NUMA, and what effect can we expect?  On the graph, I compared only one remote socket out of three because there was no difference between them. <br><br>  Fio configuration: <br><br><ul><li>  60 processes (numjobs).  The number is taken from the hardware configuration.  We have 8 processors installed in the test sample and SMT8 is turned on.  From the point of view of the operating system, 64 processes can be run on each socket.  That is, I brazenly adjusted the load to the hardware capabilities. </li><li>  block size - 4kb </li><li>  load type - random read 100% </li><li>  load object - 6 disks connected to socket 0. </li></ul><br>  Changing the queue, I watched the bandwidth and response time, starting the load in the local socket, remote, and without binding to the socket in general. <br><br><img src="https://habrastorage.org/web/a68/653/0cf/a686530cff40480b9af5022cafd648de.png"><br><img src="https://habrastorage.org/web/175/ff5/eab/175ff5eab0cb4f718154490775489d81.png"><br><br>  As can be seen in the graphs, the difference between the local and remote socket is very noticeable on a large load.  Overheads occur when the queue is 16 (iodepth = 16)&gt; 2M IOPS with a 4K block (&gt; 8 GB / s, simply put).  One could conclude that paying attention to NUMA is only on tasks where you need a large bandwidth on input-output.  But not everything is so simple, in a real application, except for I / O, there will be traffic on the interprocessor bus when accessing memory in remote NUMA locality.  As a result, a slowdown can occur with less I / O traffic. <br><br><h2>  Performance under maximum load </h2><br>  And now, the most interesting.  We will load all available 12 disks at the same time.  Moreover, taking into account the previous experiments, we will do it in two ways: <br><br><ol><li>  the kernel chooses which socket to run fio on without considering the physical connection; </li><li>  fio only works on the socket to which the disks are connected. </li></ol><br>  Look what happened: <br><br><img src="https://habrastorage.org/web/7c2/6ae/a14/7c26aea14ba24902bc694067137ddd50.png"><br><img src="https://habrastorage.org/web/6ac/df4/590/6acdf45908c349a48a47a7f540f42545.png"><br><img src="https://habrastorage.org/web/2e5/86b/096/2e586b096f404d1da6a64ee61e7263b5.png"><br><img src="https://habrastorage.org/web/c90/3e6/4e1/c903e64e18b1447784ff68066374a28a.png"><br><br>  For random read operations with a 4KB block, we received ~ 6M IOPS with a response time &lt;330 ¬µs.  For a block of 64KB we got 26.2 GB / s.  Probably, we run into the x16 bus between the processor and the PCIe switch.  Let me remind you, this is half the hardware configuration!  Again, we see that at high loads, binding I / O to the ‚Äúhome‚Äù locality gives a good effect. <br><br><h2>  LVM Overhead </h2><br>  It is generally inconvenient to give disks to the application as a rule.  The application may be either too little one disk, or too much.  Often you want to isolate application components from each other through different file systems.  On the other hand, I want to balance the load between the I / O channels evenly.  LVM comes to the rescue.  Disks are combined into disk groups and distribute the space between applications through logical volumes.  In the case of conventional spindles, or even disk arrays, LVM overhead is relatively small compared to disk latency.  In the case of NVMe, the response time from the disks and the overhead of the software stack are the numbers of the same order.  interesting to see them separately. <br><br>  I created an LVM-group with the same disks as in the previous test, and loaded the LVM-volume with the same number of reading threads.  As a result, I received only 1M IOPS and 100% processor load.  Using perf, I did a processor profiling, and this is what happened: <br><br><img src="https://habrastorage.org/web/638/3ab/ea1/6383abea10c64c27bcd9ca993773a2bc.png"><br><br>  In Linux, LVM uses Device Mapper, and obviously the system spends a lot of time when calculating and updating disk statistics in the generic_start_io_acct () function.  I did not find how to disable statistics collection in Device Mapper (in dm_make_request ()).  There is probably some potential for optimization.  In general, at this point, the use of Device Mapper can have a bad effect on performance under heavy IOPS load. <br><br><h2>  Polling </h2><br>  Polling is a new mechanism for working Linux input / output drivers for very fast devices.  This is a new feature, and I will mention it only to say why it has not been tested in this review.  The novelty of the feature is that the process is not removed from execution while waiting for a response from the disk subsystem.  Context switching is usually performed while I / O is waiting and is costly in itself.  Overhead costs for context switching are evaluated in microsecond units (the Linux kernel needs to remove one process from execution, calculate the most worthy candidate for execution, etc., etc.) Interruptions during polling can be saved (only context switching is removed) or completely eliminated .  This method is justified for a number of conditions: <br><br><ol><li>  requires high performance for single-threaded tasks; </li><li>  the main priority is the minimum response time; </li><li>  Direct IO is used (no file system cache); </li><li>  for an application, the processor is not a bottleneck. </li></ol><br>  The reverse side of the polling is an increase in processor load. <br><br>  In the current Linux kernel (4.10 for me now), polling is enabled by default for all NVMe devices, but it works only for cases when the application specifically asks for it to be used for certain ‚Äúespecially important‚Äù I / O requests.  An application should set the RWF_HIPRI flag in the preadv2 () / pwritev2 () system calls. <br><br><pre> <code class="bash hljs">/* flags <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> preadv2/pwritev2: */ <span class="hljs-comment"><span class="hljs-comment">#define RWF_HIPRI 0x00000001 /* high priority request, poll if possible */</span></span></code> </pre> <br>  Since single-threaded applications do not belong to the main topic of the article, polling is postponed for the next time. <br><br><h2>  Conclusion </h2><br>  Although we have a test sample with half the disk subsystem configuration, the results are impressive: almost 6M IOPS with 4KB and&gt; 26 GB / s for 64KB.  For the built-in disk subsystem of the server, this looks more than convincing.  The system looks balanced in the number of cores, the number of NVMe disks per core and the width of the PCIe bus.  Even with terabytes of memory, the entire volume can be read from disks in a matter of minutes. <br><br>  The NVMe stack in Linux is lightweight, the processor is overloaded only with a large load of fio&gt; 400K IOPS, (4KB, read, SMT8). <br><br>  NVMe disk is fast in itself, and it is quite difficult to bring it to saturation with the application.  There is no longer a problem with slow disks, but there may be a problem with the limitations of the PCIe bus and software I / O stack, and sometimes kernel resources.  During the test, we almost did not rest on the disks.  Intensive I / O heavily loads all server subsystems (memory and processor).  Thus, for I / O-intensive applications, it is necessary to configure all server subsystems: processors (SMT), memory (interleaving, for example), applications (number of writing / reading processes, queue, binding to disks). <br><br>  If the planned load does not require large calculations, but it is intensive in input-output with a small block, it is still better to take POWER8 processors with the largest number of cores from the available line, that is 12. <br><br>  The inclusion of additional software layers (such as Device Mapper) on the NVMe I / O stack can significantly reduce peak performance. <br><br>  On a large block (&gt; = 64KB), binding IO loads to NUMA to the localities (processors) to which the disks are connected, reduces the response time from the disks and speeds up I / O.  The reason is that on such a load, the width of the bus from the processor to the disks is important. <br><br>  On a small block (~ 4KB) everything is less clear.  When tying the load to the locality, there is a risk of uneven CPU utilization.  That is, you can simply overload the socket to which the disks are connected and the load is tied. <br><br>  In any case, when organizing I / O, especially asynchronous, it is better to distribute the load across different cores using NUMA utilities in Linux. <br><br>  Using SMT8 greatly improves performance with a large number of writing / reading processes. <br><br><h2>  Final thoughts on the topic. </h2><br>  Historically, the I / O subsystem is slow.  With the advent of flush, it has become fast, and with the advent of NVMe, quite fast.  Traditional discs have mechanics.  It makes the spindle the slowest element of the computing complex.  What follows from this? <br><br><ol><li>  First, the speed of the disks is measured in milliseconds, and they are many times slower than all other server elements.  As a result, the chance to rest against the speed of the bus and disk controller is relatively low.  It is much more likely that a problem will arise with spindles.  It happens that one disk is loaded more than the others, the response time from it is slightly higher, and this slows down the entire system.  With NVMe, the bandwidth of the disks is huge, the bottleneck is shifting. <br><br></li><li>  Second, in order to minimize disk latency, the disk controller and the operating system use optimization algorithms, including caching, deferred writing, and read ahead.  It consumes computational resources and complicates setup.  When the discs are immediately fast, there is no need for a large amount of optimization.  Rather, its goals change.  Instead of reducing the expectations inside the disk, it becomes more important to reduce the delay to the disk and as soon as possible to bring a block of data from memory to disk.  The NVMe stack in Linux does not require configuration and runs quickly right away. <br><br></li><li>  And third, fun about performance consultants.  In the past, when the system slowed down, it was easy and pleasant to look for the cause.  Swear on the storage system and most likely you will not be mistaken.  Database consultants love and know how to do this.  In the system, with traditional disks, you can always find some problem with the storage performance and puzzle the vendor, even if the database slows down due to something else.  With fast drives, bottlenecks will increasingly shift to other resources, including the application.  Life will be more interesting. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/334260/">https://habr.com/ru/post/334260/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334250/index.html">Analysis of the tasks of the final Yandex. Algorithm 2017</a></li>
<li><a href="../334252/index.html">XBRL: Just About the Difficult - Chapter 5. Discovering New Dimensions</a></li>
<li><a href="../334254/index.html">#! ProgMsk Meetup: Virtualization and DevOps in the office Tutu.ru</a></li>
<li><a href="../334256/index.html">Planning Poker: how to make the task setting process as transparent and clear as possible</a></li>
<li><a href="../334258/index.html">Release of the openSUSE 42.3 Linux distribution</a></li>
<li><a href="../334262/index.html">Unexpected Kotlin poll results: a small investigation</a></li>
<li><a href="../334264/index.html">We read, listen, use. Guide on the sources for the development of Android-developer</a></li>
<li><a href="../334266/index.html">Applications for Tarantool. Part 1. Stored procedures</a></li>
<li><a href="../334268/index.html">Terrible recruiter, terrible candidate</a></li>
<li><a href="../334270/index.html">Automate CI / CD for Java applications using Microsoft Visual Studio Team Services</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>