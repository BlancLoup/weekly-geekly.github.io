<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fear and Loathing in Distributed Systems</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Roman Grebennikov explains the complexity of building distributed systems. This is a report Highload ++ 2016. 

 Hello everyone, my name is Roman Greb...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fear and Loathing in Distributed Systems</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/54d/429/2f5/54d4292f59864b5e9c85826d68d590ab.jpg"><br><br>  <i>Roman Grebennikov explains the complexity of building distributed systems.</i>  <i>This is a report <a href="http://www.highload.ru/">Highload ++</a> 2016.</i> <br><br>  Hello everyone, my name is Roman Grebennikov.  I work for Findify.  We do a search for online stores.  But the conversation is not about that.  In Findify, I deal in distributed systems. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      What is a distributed system? <br><a name="habracut"></a><br><img src="https://habrastorage.org/files/743/204/4d5/7432044d52b343d8bb921a1d2b7c720b.png"><br><br>  From the slides it is clear that ‚Äúfear and hatred‚Äù is a common thing in IT, and distributed systems are not quite ordinary.  We will try to understand first what is distributed systems, why there is so much pain, and why we need this report. <br><br>  Imagine that you have some kind of application that works somewhere there.  Suppose it is server-based, but it differs from ordinary applications in that it has some kind of internal state.  For example, you have a game, the internal state is a world where human beings enter and so on.  Sooner or later you grow, and your internal state all swells, it changes and ceases to fit on one server. <br><br><img src="https://habrastorage.org/files/040/b6c/ea4/040b6cea49e24d8990e59ba0ebfc813a.png"><br><br>  Here in the picture Winnie the Pooh is stuck in a hole. <br><br>  If you started to grow and no longer fit into one server, then you need to do something. <br><br>  <i>You have several options.</i> <br><br>  You can take a more powerful server, but this may be a dead end, because you may already be working on the fastest server you have. <br><br>  You can optimize, but that - it is not clear, on click everything is accelerated twice as hard. <br><br>  You can get on a very shaky path - the creation of distributed systems. <br><br>  <b>This path is shaky and scary.</b> <br><br><h3>  What am I going to talk about today? </h3><br>  First we will talk about distributed systems.  A little bit of materiel, why it is important, what integrity is, how to integrate this integrity, what are the approaches to the design of distributed systems, which the data does not lose or lose them quite a bit, what tools are there to check your distributed systems for what you have everything is good with data or almost everything is good. <br><br>  Since one theory is boring, we'll talk a little bit about the case and a little bit of practice.  We‚Äôll take this laptop and write our own little simple distributed database.  Not really a database, there will not be a key value store, but a value store.  Then we will try to prove that it does not lose the data, moreover, repeatedly in particularly terrible positions. <br><br>  <b>Next, a little philosophy of "how to live with it."</b> <br><br>  We imagine that distributed systems are such things that work somewhere far away, and we all sit here.  It can be servers, it can be mobile applications that communicate with each other, they have some kind of internal state.  If you and your friends are texting, then you are also part of a distributed system, you have a general condition, what are you trying to negotiate.  In general, a distributed system is a joke that consists of several parts, and these parts communicate with each other.  But everything is complicated by the fact that they communicate with delays and errors.  This is all very complicated. <br><br>  <b>A small example from life.</b> <br><br>  We once wrote a web spider that went on the Internet, downloading all sorts of different pages.  He had a big queue of tasks, we piled it all up.  We have a few basic operations at the queue.  This is something to take from the queue, something to put in the queue.  We also had a third operation for the queue: check if there is an object in the queue, so that you don‚Äôt put the same thing two times. <br><br><img src="https://habrastorage.org/files/a7f/8a6/250/a7f8a625010d474590b59778c8df4020.png"><br><br>  The problem was that the queue was quite large, and it did not fit into the memory.  We thought: what is so complicated here?  We are smart, we go to HighLoad.  So let's cut this queue into pieces, roll it out to different servers.  Each server will deal with their own piece of queue.  Yes, we will lose a little in integrity, in the sense that we will not be able to take the very first element from the queue, but we will be able to take almost the very first.  Just choosing the case shard, taking from it, and all is well.  That is, if you take from the lineup, almost everything is fine, the logic has become a bit more complicated.  Putting in a queue is also easy, we look at which shard to put and put.  Check if there is a queue, also no problem.  Yes, business logic has become a bit more complicated, but at least it has become uncritical, and there seems to be no blood here. <br><br><h3>  What could be the problem </h3><br>  We understand that if we have added some kind of network interaction, and we have more components, then the system has less reliability.  If less reliability, then surely something will go wrong.  If with software and hardware, everything is clear.  With iron, you can take the server more recently, with the software - do not deploy drunk on Friday.  And with the network, such a thing breaks down no matter what you do with it.  Microsoft has a great article with network hardware failure statistics, depending on the switch type in Windows Azure.  The likelihood that the port load balancer grunts during the year is about 17%.  That is, if you do not envisage what to do in case of refusal, then sooner or later you will have someone to eat good. <br><br>  The most popular problem that happens with a network is NETSPLIT.  When your network has collapsed in half, either it has constantly collapsed, or you have packet loss.  As a result, she then fell apart, then did not fall apart. <br><br>  <b>What will we have with the queue at the sharding, if we have problems with the network?</b> <br><br>  If we take something from the queue, if we are ready to take not the very first element from the queue, then we can simply take from the shards that are available to us, and somehow continue to live. <br><br>  If we need to check if there is a component in the queue, then everything is complicated by the fact that if we need to go to that shard that is not available to us, then we can do nothing. <br><br>  But everything gets complicated when we need to put something in the queue, and the shard is inaccessible.  We have nowhere to put it, because that shard is somewhere far away.  Nothing remains but to lose it, or to put it somewhere and then do something about it.  Because we have not laid the failures in the design of the system. <br><br><img src="https://habrastorage.org/files/fcb/6a7/d9d/fcb6a7d9d19d45768941b1bdba5ebfca.png"><br><br>  In the picture the uncle is sad.  Because he did not lay the failures in the design of the system.  We are smart, we know that for sure there are other uncles who are very smart and have come up with many different options for how to live with this problem. <br><br><img src="https://habrastorage.org/files/95d/da6/4a8/95dda64a80974b8f86c04072bfb2b424.png"><br><br>  <b>Here on the scene appears CAP-theorem.</b> <br><br>  CAP-theorem is the cornerstone of the design of distributed systems.  This is a theorem, which is formally not a theorem, but a rule of thumb, but everyone calls it a theorem. <br><br>  It sounds as follows.  We have three whales of building distributed systems.  It is integrity, availability and resistance to network problems.  We have three whales, we can choose any two.  And not exactly any two, almost any two.  We'll talk about this a little later. <br><br>  In order - what is integrity, accessibility and sustainability.  Same as a theorem, there should be a formal description. <br><br><h3>  Availability </h3><br>  This is availability, and it implies constant availability.  Each request to the system, any request to the system to any live node must be successfully processed.  That is, if we postpone a part of requests somewhere for later, or we postpone recordings to the side, because something went wrong with us - this is intermittent availability.  If not all nodes respond to requests, or not all requests all nodes respond - this is also inconsistent availability from the point of view of the CAP theorem. <br><br>  If it answers you like this: <br><br><img src="https://habrastorage.org/files/931/3e6/2ae/9313e62aecf04af1a8827d93dee9b8ab.png"><br><br>  This is also intermittent availability. <br><br><h3>  Integrity </h3><br>  The next item is integrity.  From the point of view of integrity, you can say that there are so many different types of integrity in distributed systems: <br><br><img src="https://habrastorage.org/files/56e/244/0b0/56e2440b03ad413f97e55751165ae6d7.png"><br><br>  There are about 50 of them.  What are some of them in the CAP theorem? <br><br>  <b>In the CAP-theorem the strictest type.</b>  <b>Called linearizability.</b> <br><br>  Integrity, linearizability.  It sounds very simple, but it has great consequences.  If operation B started after operation A, then B should see the system at the time of termination A or in a newer state.  That is, if A is completed, then the next operation cannot see what it was before A. It seems that everything is logical, there is nothing complicated.  To rephrase it in other words: "There is a consistent history of sequential operations." <br><br>  Now we will talk more about these stories. <br><br><img src="https://habrastorage.org/files/52b/a7f/a5b/52ba7fa5b5c74123a017ed73f25e7129.png"><br><br>  Imagine we have some sort of register.  This is what we can read, only what we wrote down before.  Just one hole for a variable.  We have one reader-writer.  We read and write everything, nothing complicated.  Even if we have several readers and writers, nothing too complicated. <br><br><img src="https://habrastorage.org/files/a4b/d7d/d40/a4bd7dd4067b44ae8ee699f634ce3cd9.png"><br><br>  But as soon as we move from the slide to the real world, this diagram looks a bit different, because we have network delays.  We do not know exactly when.  The record happened between w and w1, where exactly it happened, we do not know.  The same with the readings.  From the point of view of history, for example, we can record three such unpretentious stories. <br><br><img src="https://habrastorage.org/files/ab5/19a/4f2/ab519a4f2ef248c8a77e7367a6735909.png"><br><br>  First we read a, then we wrote down b, then we read b, clearly as in the picture.  In principle, another story is possible, when we read a, we read a again, and then we wrote down b, if everything is as in the picture. <br><br>  The third story.  If we read a, and then suddenly read b, and then write b, it contradicts itself, because we read b before it was written.  From the point of view of linearizability, such a story is not linearizable, but the CAP-theorem requires that there is at least one story that does not contradict itself.  Then your system is linearizable.  There may be several. <br><br><h3>  Resilience </h3><br>  The last item is the letter P. Partition tolerance, in Russian we can say that it is ‚Äúresistance to network failures‚Äù.  It looks like this: <br><br><img src="https://habrastorage.org/files/597/653/238/597653238cdc443291b8f56f123d1c0e.png"><br><br>  Imagine that you have several servers, and early in the morning I drove through an excavator and cut the wires between them.  You have two choices if your cluster is split in half.  The first exit: the big half lives, and the smaller has fallen off.  You have lost accessibility because the smaller one has fallen off.  But the big one lives.  But not lost integrity.  Either both halves work, we take notes here and there, we accept everything, everything is fine.  Only then, when the wire is soldered, we will understand that we had one system, and there were two, and they live their own lives. <br><br>  From the point of view of the CAP-theorem, we have three possible approaches to system design.  These are CP / AP / AC systems depending on two combinations of the three. <br><br>  There is a problem with AC systems.  On the one hand, they ensure that we have high availability and integrity.  Everything is cool, until we have broken the network.  And since this often happens, in the real world, AC systems can be used, but only if you understand the trade-offs you make when you use AC systems. <br><br>  In the real world there are two choices.  You can either shift in integrity and lose availability, or shift to availability, but lose integrity.  There is no third. <br><br>  In real life there are many algorithms that implement various CP / AP / AC systems.  Two-phase commit, Paxos, quorum and other rafts, Gossip and other heaps of algorithms. <br><br>  We will now try to implement some of them and see what happens.  You can say: ‚Äú10 minutes has passed, our head has already exploded, and we just arrived.‚Äù  Therefore, we will try to do something in practice. <br><br><h3>  What we will do in practice </h3><br>  We will write a simple master-slave distributed system.  To do this, we take Scala, Docker, pack everything.  We will have a master-slave distributed system that has asynchronous / synchronous replication.  Then we get Jepsen and show that in fact we wrote everything correctly or incorrectly.  We will try to explain the result after we run Jepsen.  What is Jepsen?  I will tell you a little later, surely many of you heard about it, but you did not see it with your eyes. <br><br>  So, master-slave.  In general, it looks like a basic thing.  The client sends a write request to the master.  Master writes to disk.  The master synchronously or asynchronously scatters it all into slaves.  Synchronously or asynchronously responds to the client, either before it scattered as records, or after. <br><br>  We will try to understand from the point of view of the CAP-theorem, how things are with integrity, availability and other things. <br><br>  <b>Let's try something.</b> <br><br><img src="https://habrastorage.org/files/fd4/77e/5d4/fd477e5d49f84a1eb9778e941277043a.png"><br><br>  There is a small preparation here, then I will write more for karaoke.  We have two functions that will help us work with other servers.  That is, we will use HTTP as the easiest way to communicate between nodes in a distributed system.  Why not? <br><br>  We have two functions.  One writes this data to this node: <br><br><img src="https://habrastorage.org/files/d2a/753/f44/d2a753f4460c4183be53d8368e489309.png"><br><br>  Another function that reads some data from this node, and does all this asynchronously: <br><br><img src="https://habrastorage.org/files/dcb/3c5/f84/dcb3c5f84aef4f68b54451c0f0dcd6a9.png"><br><br>  Also a useful feature that parses Response.  Takes Response, returns String: <br><br><img src="https://habrastorage.org/files/e68/af0/5b8/e68af05b83b04ab2b022510ba6e322e7.png"><br><br>  Nothing complicated. <br><br>  To begin, we will write a simple server of our distributed system. <br><br><img src="https://habrastorage.org/files/389/2c4/f58/3892c4f5881a4d719c1446843e319399.png"><br><br>  To begin with, we have homework here.  The data inside our system, we will store the entire stat in one variable, because we have a live demo here, and not a real database. <br><br><img src="https://habrastorage.org/files/cb8/dce/7c6/cb8dce7c6d6743bbb7005fea830065f1.png"><br><br>  Therefore, we just store the string, we will replicate it and so on.  There are all sorts of useful jokes, such as - read from the variable environment HOSTNAME, NODES neighboring and so on. <br><br><img src="https://habrastorage.org/files/dd0/a5d/5d3/dd0a5d5d3d224de8a27a1a67e4dd898c.png"><br><br>  We will write plugs for two functions here. <br><br><img src="https://habrastorage.org/files/24c/ea0/674/24cea0674cba464b80b8a9edea4a1e6a.png"><br><br>  Reading from our distributed system and writing from our distributed system.  Of course, we will not implement them now, but we will go further. <br><br>  Here we launch our balalaika: <br><br><img src="https://habrastorage.org/files/e73/e74/d2b/e73e74d2bd8e4ae2b0583c50d3b4edbd.png"><br><br>  Everything is very simple.  We have a route function, which is not yet implemented, but it does something.  It describes the rest route that we will use. <br><br><img src="https://habrastorage.org/files/c5f/8c4/319/c5f8c431962b4365ae783a3b14285fb9.png"><br><br>  We are stretching all this to the 8000th port: <br><br><img src="https://habrastorage.org/files/592/473/c44/592473c44b444c55937f144b68c568b6.png"><br><br>  What route will we have?  We will have two route.  The first is db. <br><br><img src="https://habrastorage.org/files/39e/240/3b9/39e2403b968e4eed9a233efea0eb668a.png"><br><br>  This route is for us, for database clients.  We work with him, and she is doing something under the hood. <br><br>  If we get there, we call our magic read function, which we have not yet implemented. <br><br><img src="https://habrastorage.org/files/0c6/2a3/3e4/0c62a33e4490475aa6632fc84fabde1e.png"><br><br>  If we post some data there, then we call our write function.  It seems nothing complicated. <br><br><img src="https://habrastorage.org/files/2ea/3bb/141/2ea3bb1412f3466eb7877dbf45d1a3fa.png"><br><br>  In addition, we will have another route, called local. <br><br><img src="https://habrastorage.org/files/6ae/ae6/0b2/6aeae60b2779430abf5b9aebd6792c0c.png"><br><br>  It is made not for us, but in order that the members of the distributed system, different nodes can communicate with each other.  One of the other could read what she had written there. <br><br>  If we get there, we read our magic variable value. <br><br><img src="https://habrastorage.org/files/944/aef/4e8/944aef4e85914dceb6c05fc77bced25f.png"><br><br>  If we do a post there, then we write what we posted there in this variable. <br><br><img src="https://habrastorage.org/files/5d2/d93/fd2/5d2d93fd2cc5460fbcbde35b52a56e7b.png"><br><br>  Nothing complicated.  A little brain explodes from Scala, probably, but that's okay. <br><br>  We kind of wrote our server.  It remains to make the logic for our MasterSlave.  We will now create a separate class that will be implemented by MasterSlave.  Reading from asynchronous MasterSlave, in fact, is just a local reading of what we have written to our variable there. <br><br><img src="https://habrastorage.org/files/38c/16e/61e/38c16e61ef264ba9ab07e140bc236275.png"><br><br>  Writing is a bit more complicated. <br><br><img src="https://habrastorage.org/files/5dc/45d/c4c/5dc45dc4c22a45db850070fa38d878f3.png"><br><br>  We first write to ourselves. <br><br><img src="https://habrastorage.org/files/730/762/873/73076287338e4f7394622e9a5423310a.png"><br><br>  Then we go over all the slaves that we have and write to all the slaves. <br><br><img src="https://habrastorage.org/files/df4/b57/1fd/df4b571fd22341edbea1681f64c3ecc2.png"><br><br>  But this function has a feature, it returns Future. <br><br><img src="https://habrastorage.org/files/16e/d28/886/16ed2888685a4f6aa0d1d5d01b048c75.png"><br><br>  Here we pop out asynchronously all requests to write to the slaves, and we say to the client: ‚ÄúEverything is OK, we have recorded, you can go on.‚Äù  Typical asynchronous replication, possibly with pitfalls, now we'll see it all. <br><br>  Now we try to compile it all.  This is Scala, it makes it long.  In general, it should compile, I rehearsed.  Compiled <br><br><h3>  What to do next </h3><br>  We wrote, but we need to start all this, and I have one laptop.  And we make a distributed system.  A distributed system with one node is not a completely distributed system.  Therefore, we will use Docker. <br><br><img src="https://habrastorage.org/files/a68/60a/3e9/a6860a3e939449aa96a3364b24874c56.png"><br><br>  Docker is an application containerization system, everyone probably heard about it.  Not everyone probably used in production.  We will try to use it.  This is a light virtualization system, if you simplify everything.  Docker has a rich ecosystem around, we will not use all of this ecosystem.  But since we need to launch not one container, but a group at once, we will use Docker Compose to roll them all together at once. <br><br>  We have a simple Dockerfile, but it is not quite simple. <br><br><img src="https://habrastorage.org/files/af5/ada/807/af5ada80769f4bc8bb228730074940bc.png"><br><br>  Here the Dockerfile, which installs Java, puts SSH in there.  Do not ask why it is needed.  Our application starts it all. <br><br>  And we have a Compose file that describes all 5 nodes. <br><br><img src="https://habrastorage.org/files/bac/3ba/4d3/bac3ba4d376845b98eeaaee127c81c2a.png"><br><br>  We have a description of several nodes here.  We will try to fix it now.  I have a script for this. <br><br><img src="https://habrastorage.org/files/73d/b9a/ddf/73db9addf1044149a75ca4128ce548c0.png"><br><br>  While it is deployed, I will change the colors. <br><br><img src="https://habrastorage.org/files/602/5a9/bee/6025a9bee59c47e58524fb52a1fd9f8f.png"><br><br>  Now it creates a Docker container.  Now it will launch it.  All of our 5 nodes are running. <br><br><img src="https://habrastorage.org/files/290/f7b/79c/290f7b79c2b14e6dadb8417349790d7d.png"><br><br>  Now we are waiting for our distributed system to knock that it is alive.  It takes some time, it's Java.  Our MasterSlave said it started. <br><br><img src="https://habrastorage.org/files/ffb/b95/0e7/ffbb950e72f94e26ad959e7bde65ba30.png"><br><br>  In addition, we have unpretentious scripts of everything, which will allow us to read something from this distributed system. <br><br>  Let's see what we have in node n1. <br><br><img src="https://habrastorage.org/files/4a7/921/cdc/4a7921cdcc3a4e02b999f43404718c59.png"><br><br>  It recorded 0. <br><br>  Since there was no protection against who is master, who is slave, we will take on faith that we always have n1 master.  We will only write to it only in order to simplify everything. <br><br>  Let's try to write something into this master. <br><br>  <i>put n1 1</i> <br><br>  There was a unit.  Let's see what we have here in the logs. <br><br><img src="https://habrastorage.org/files/ef8/3f1/92c/ef83f192cffc430ea86cb7b162e05ce1.png"><br><br>  Here we have our unit here in the record, we have this record rolled over all the slave, here she signed up. <br><br><img src="https://habrastorage.org/files/681/b18/347/681b1834780f4e9290e5f2af583b621d.png"><br><br>  We can even go to the node n3, let's see what is there. <br><br><img src="https://habrastorage.org/files/833/ae5/491/833ae5491d914a31a2d2a87eb25b618e.png"><br><br>  There is one. <br><br>  We can go for a bonus, we wrote our distributed system on the knee, which even works.  But it works, so far so good. <br><br>  Now we will try to make her feel bad.  In order to make it bad, we will take such a framework called Jepsen.  Jepsen - a framework for testing distributed systems, with one subtlety: it is written in Clojure.  Clojure is a lisp who does not know.  This is a set of ready-made tests for existing databases, queues and other things.  You can always write your own.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In addition, there are a lot of articles about the problems found, probably in all databases, except for exotics. </font><font style="vertical-align: inherit;">Perhaps not only got ZooKeeper and RethinkDB. </font><font style="vertical-align: inherit;">They got, but slightly compared to the others. </font><font style="vertical-align: inherit;">You can read about it.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How does Jepsen work </font></font></h3><br>  It simulates network errors, then generates random operations to your distributed system.  Then he looks at how these operations were applied to your distributed system and to the reference behavior, to the model of this distributed system, and whether there are any problems with this. <br><br>  If Jepsen you have found such a problem: <br><br><img src="https://habrastorage.org/files/4fd/9fe/77f/4fd9fe77f08d419fb7abbdf15cc8145d.png"><br><br>  This is where I wanted to joke. <br><br>  If Jepsen found some problem with you, he found counterexamples for your distributed system, and there really is some kind of a jamb.  But since the Jepsen tests are probabilistic in nature, if he doesn‚Äôt find anything, he may not have been looking well enough.  But if he looks good, he will find something.  In the case of, for example, RethinkDB, they drove tests about two weeks before the release to prove that it worked more or less somehow. <br><br>  We are not going to drive tests for two weeks, we will be five seconds.  Our task with the Jepsen test is to write to master, read from MasterSlave and understand how things are with integrity and whether we wrote our master / slave replication correctly or maybe not. <br><br>  The jepsen test consists of several important parts. <br><br><img src="https://habrastorage.org/files/98e/97a/44d/98e97a44d91e473fb9e67a7eb527f8fa.png"><br><br>  We have a generator that generates random operations that we apply to our distributed system.  The distributed system itself, which we launched somewhere.  It can be in Docker, maybe on real iron servers, why not.  And we have a reference model that describes the behavior of a distributed system.  In our case, this is a register, what we wrote down there, and we should read this.  There is nothing complicated.  Jepsen has a huge number of models for all occasions, but we will only use the register and Checker, which checks the compliance of the operation history applied to the distributed system for their compliance with the model. <br><br>  The problem is that Jepsen is written in Clojure, and tests need to be written too Clojure.  If there was an opportunity to write them on something else, it would be cool.  But trouble, trouble.  Clojure is a language where there is always a list.  If, for example, you want to add two numbers, then you make a list in which the first element is addition, and then two numbers, in the end you get three. <br><br><img src="https://habrastorage.org/files/e18/f2d/e50/e18f2de50753400a9ee4a6469bc25620.png"><br><br>  You can set your function to call another function called defn and say that the first argument is the name of the function, then the argument is the function, then the function body.  If you call her like this, she will say ‚Äúhello, highload!‚Äù.  This is a Clojure course for beginners. <br><br>  You can say that this Clojure course looks like this: <br><br><img src="https://habrastorage.org/files/f70/936/5d9/f709365d9ec048ae91e647e4a76531c6.png"><br><br>  And now for sure it will be, what is there on the right.  In fact, yes.  But I told you that it is enough for you to at least understand something that will now take place in frames, because Clojure is a bit specific. <br><br>  So, Jepsen.  To begin, we describe our test. <br><br><img src="https://habrastorage.org/files/2ab/9e9/618/2ab9e96188074f89977b74a7ad138db6.png"><br><br>  This footcloth should not be read correctly from left to right, from top to bottom, because it is a lisp, it is better to read from inside and outside.  We have some function that returns the description of our test, which we have not yet implemented.  We put this description into another function that runs this test, and it returns a reference book.  In this directory, we pick out something on the key results and see if there is a key in this directory called valid.  If it is there, then the test is passed.  Clojure reads like this.  We must first break the brain, but then everything becomes clear. <br><br>  Now we describe our test. <br><br><img src="https://habrastorage.org/files/7ed/8d7/63c/7ed8d763c52f48058c5109d221187b0b.png"><br><br>  Our test is also a function in which there are no arguments.  It extends another test that does nothing at all, and complements it with some things.  For example, the name, the client that he will use to communicate with our database, because Jepsen does not know about what, nor about what HTTP.  Checker, which we have not yet written, but we will write.  The model that we will use as a standard of our distributed system.  A generator that takes random read and write operations, inserts a delay of 10 milliseconds between them, starts them with the client and lets it all out for 5 seconds.  Then there to work with SSH. <br><br>  Now we describe our reading and writing.  This is also a function.  It's Clojure, it's all functions, there's nothing else. <br><br><img src="https://habrastorage.org/files/0ff/c95/0ea/0ffc950eaaf040f7961c4dd50565dea3.png"><br><br>  We also describe our client, but first an HTTP client. <br><br><img src="https://habrastorage.org/files/eaa/bd3/8cd/eaabd38cda924d14b858c7592dd4a2b4.png"><br><br>  We have here several functions for writing in HTTP, for reading in HTTP.  We will not go into how all this is done.  But in fact, if we returned 200, then everything is OK.  If 409 is HTTP 409 Conflict, a very useful code, then something is wrong.  We will use it a little further. <br><br>  Next we describe our client. <br><br><img src="https://habrastorage.org/files/d33/01c/4b3/d3301c4b32c44bd48ee2d76bc9bf7a0f.png"><br><br>  The same function that takes the host that we are going to write now and extends the interface called client.  He has three functions.  Setup which returns itself.  Teardown which does not need to remove the client, and there is nothing inside.  invoke which our operation, which we pass there, applies to this host.  We have a five-second timeout here, that is, if our distributed system did not work in 5 seconds, then we can say that it will not work either.  If we have this reading, we execute an HTTP-read, make a GET request.  If a record, then we do an HTTP-write, that is, a POST request.  Since we have MasterSlave and we have only one Master, we will change a little here, we write all the time in Master. <br><br><img src="https://habrastorage.org/files/290/021/941/2900219413e046c59856014efeeb9dd2.png"><br><br>  The last item that we have left is the Checker.  It could be used, which is inside the Jepsen called linearizability checker, but we will not use it because I stepped on it. <br><br><img src="https://habrastorage.org/files/631/e4a/f0d/631e4af0ddbf46a3bbff28f931a3a435.png"><br><br>  I do not know what to do with it.  Therefore, I wrote my own, which does not fall.  In fact, he uses the same thing, just does not try to generate a beautiful picture at the end, where he usually falls. <br><br><img src="https://habrastorage.org/files/555/913/e99/555913e99ea44f4792876786b763cbe2.png"><br><br>  We wrote our mega-test, let's try to run it. <br><br><img src="https://habrastorage.org/files/5c4/fd0/fd2/5c4fd0fd250a4af2956cebde886d28a3.png"><br><br>  We have our distributed system, the test, we are waiting for it to start. <br><br>  lein is such a build system for Clojure. <br><br>  It began to write different hosts, different read / write, in the logs, too, the magic is going on - read / write, replication.  In the end, it tells us ‚Äúfail‚Äù.  You forgot something here.  There is a list of stories that are not linearizable, and something here is clearly messed up. <br><br>  Let's look at this situation: <br><br><img src="https://habrastorage.org/files/e4b/8d2/3ec/e4b8d23ece054214a02f8f32abfd3c8d.png"><br><br>  Everyone knows that if we do asynchronous replication in the Master / slave, then the slave will always be late.  What does the slave delay mean in terms of linearizability?  This means that a recording happened here, after some time the recording was replicated, and reading happened here.  As a result, we read what we should not have read.  We had to read B, and read A. That is, we returned back in time, when the operation seemed to have already ended, and we went back and read what was from it.  Because the Master at ‚Äúasynchronous replication‚Äù says ‚Äúok‚Äù too early, because not all slaves caught up with the replication stream.  Therefore, the slave always lags behind the master. <br><br>  Of course you say that this is a kindergarten, let's do everything in an adult way.  Let's write synchronous replication, we are smart.  It will be almost the same as asynchronous, only synchronous. <br><br><img src="https://habrastorage.org/files/05d/016/b88/05d016b884ca4331b14ad0e6de2bcdfc.png"><br><br>  We will override one function that writes to our distributed system.  She is now doing it intelligently: she is waiting for the moment when all the slaves say ok.  Only then she says that well, everything is recorded.  If one of the slaves said that he wasn‚Äôt ‚Äúok,‚Äù it means we had trouble and we don‚Äôt go there. <br><br>  Now we will try to launch this whole balalaika.  Probably, you feel that now there will be some kind of setup, everything cannot be so good.  25 minutes has passed, and then the person is all live, and it all worked, so we‚Äôll wait until it compiles.  Now it collects everything, it‚Äôs not dynamic languages ‚Äã‚Äãfor you, when everything works right away, everything is long and painful here.  There is a time to think, whether you wrote, or wrote something else, rewrite everything until it is deployed. <br><br>  Master / slave sync started, great.  Let's now run the Jepsen test again in the hope that everything will be fine.  We did synchronous replication, which can go wrong.  It starts, starts writing, but not so fast anymore.  He writes again, reads again, something happens there.  As a result, something bad is happening again. <br><br><img src="https://habrastorage.org/files/bd1/483/fd1/bd1483fd12ad4395ae3fe44f2db58a5e.png"><br><br>  We again had a fail: it gave us a huge amount of stories that are not linearizable in terms of linearizability. <br><br>  <b>What is the problem?</b> <br><br>  We thought we did everything right.  Somehow everything turned out badly. <br><br><img src="https://habrastorage.org/files/ced/20a/a29/ced20aa298bb4b758ddc5e6a9164a7b0.png"><br><br>  The picture shows that everything has become even worse than it was. <br><br>  Imagine this situation. <br><br><img src="https://habrastorage.org/files/8bb/66e/c48/8bb66ec4866548e6a5555e83653f5504.png"><br><br>  We have our master node 1. We recorded operations B and C there. We have C here, we replicated it here.  We also do everything asynchronously over HTTP, which can go wrong.  In node 2, they recorded it in this sequence.  And at node 3, they messed up, we randomly sent them.  We forgot that there are magazines and other things.  They were not applied in the sequence in which they were to be applied.  As a result, instead of C, we got B, and here it is not clear what.  Whether B, or C, although they should read C. Therefore, such is the trouble. <br><br>  From the point of view of the master / slave CAP theorem and integrity, replication depends on whether synchronous replication or not.  If asynchronous, then it is clear that there is no linearizability, because slaves are late.  From the synchronous point of view, it depends on how curved you are and how you implement everything correctly.  If you're lucky, that's good.  If the same as me, then bad.  From the point of view of accessibility, the problem is that our slave cannot write, he can only read.  In terms of high availability, we cannot fulfill all the requests that come to us.  We can do only reading.  Therefore, we are not available. <br><br>  <b>Therefore, the CAP theorem is a very specific thing.</b>  It must be applied very carefully to databases, because its definitions are strict, they do not always describe what is in real databases.  Because it describes 1 register.  If you can reduce all your transactions and other transactions to one register, then this is good.  But usually it is difficult or even impossible.  Accessibility is such accessibility, but latency says nothing.  If your distributed system is fucking consistent, but responds to the request once a day, then it is also very difficult to use in production.  There are a lot of different practical aspects that are not considered in any way in the CAP-theorem.  The same loss of partitions packets, that is, if we have a network partition that is non-permanent, and variable, once every 5 seconds, several packets are lost. <br><br>  Therefore, from personal experience, when people start writing distributed systems with an immature mind, sooner or later their knowledge crystallizes, all crutches fill new bumps, and something turns out like a curved oblique consensus algorithm.  When all your nodes try to agree on a common state, but this algorithm is not very tested in extreme cases, they are rarely, why worry about it.  But the consensus in the general case is a general condition agreement with the possibility of even how to survive failures, if they happened. <br><br>  Such an example: <br><br><img src="https://habrastorage.org/files/b8a/886/617/b8a886617ae044e4b4e7ca0259761bf1.png"><br><br>  We have a client who writes to our distributed system, in which there are seven nodes.  We vote here.  If most of the nodes agreed that we have to write it all down, then everything seems to be fine.  The same with reading.  If we have a minority node fell off, 1-2-3, then do not worry.  We have not lost in availability or integrity, because everything is fine.  If the majority fell off, then we have lost accessibility, because we can no longer execute recordings, but have not yet lost integrity. <br><br>  Let's do a live demo on your knee. <br><br><img src="https://habrastorage.org/files/2be/c78/005/2bec78005e8841c19f62092a50de6928.png"><br><br>  We will try to make a quorum.  Let's try to get Jepsen again and try to explain what we can do. <br><br>  Everything will be a little more complicated here, because we seem to be already smart.  To begin, we will make a function that will describe the size of the quorum. <br><br><img src="https://habrastorage.org/files/64a/f41/7bd/64af417bdaf4420cb578d020f9ea39b9.png"><br><br>  For three nodes - these are two, for five nodes - three, and so on. Just a banal majority. <br><br>  Next we describe our magic functions read and write. <br><br><img src="https://habrastorage.org/files/dbd/719/22d/dbd71922dd354537a702a774085074a0.png"><br><br>  If we read from our distributed quorum system, we ask all HTTP nodes that you have it there: <br><br><img src="https://habrastorage.org/files/fac/925/093/fac925093b8a46f8ae358eb0459059af.png"><br><br>  Then we call our function, which will work with the quorum, which we now write: <br><br><img src="https://habrastorage.org/files/ed4/576/0cc/ed45760cc7324eac9a1b7e9b1da14287.png"><br><br>  In the end we say that we should succeed in the end: <br><br><img src="https://habrastorage.org/files/4ed/e78/0a2/4ede780a2c424011b913f9ce8c5f7b68.png"><br><br>  Approximately the same with the record.  We write to all nodes, see what is recorded: <br><br><img src="https://habrastorage.org/files/d06/9f6/512/d069f6512f9c42a59ab7ecbc968eb929.png"><br><br>  Have we got quorum or not: <br><br><img src="https://habrastorage.org/files/680/3a4/d08/6803a4d083274347be9de226916c8bc8.png"><br><br>  Further we form some answer to the user: <br><br><img src="https://habrastorage.org/files/83e/809/222/83e809222c4f4c7ba5b974f212e0ffc4.png"><br><br>  How will we live with a quorum? <br><br><img src="https://habrastorage.org/files/179/36d/db8/17936ddb8272495fb00a986152fdf821.png"><br><br>  This is a little Scala, more canonical.  We have here a sequence of answers that came to us from different servers, from different nodes of our distributed system.  This is practically just a sequence of lines that we read, for example, 0000. <br><br><img src="https://habrastorage.org/files/287/46f/01f/28746f01f2cd42dbb53bf3b4b8e2c76e.png"><br><br>  We group them by ourselves: <br><br><img src="https://habrastorage.org/files/ee7/c39/3eb/ee7c393eb801417dbe79b34d31d11f98.png"><br><br>  We look how often what groups meet: <br><br><img src="https://habrastorage.org/files/c80/792/e7f/c80792e7fd744affb3dcc5f353acaa16.png"><br><br>  Sort by popularity: <br><br><img src="https://habrastorage.org/files/7b6/736/26f/7b673626f4514952b20e27e705667e70.png"><br><br>  We take the most popular answer: <br><br><img src="https://habrastorage.org/files/dea/29b/251/dea29b251daf475dbdf6da6445e76ca9.png"><br><br>  It seems like everything is correct. <br><br>  Next is the function that forms the answer: <br><br><img src="https://habrastorage.org/files/d80/01f/43b/d8001f43b7d44237b7109c8a00a0493d.png"><br><br>  She is also very canonical on Scala, it is not entirely clear what is happening. <br><br>  We take a couple: the most popular answer and the number of votes: <br><br><img src="https://habrastorage.org/files/384/683/7b2/3846837b27d54b50b561bc4871fc7b44.png"><br><br>  And we look, if the number of votes is greater than our quorum or equal to this quorum, then everything is OK, we recorded it.  We say that it is normal, the recording was successful, the quorum was going. <br><br><img src="https://habrastorage.org/files/301/353/274/301353274d064a9a8ab290d5049b35dd.png"><br><br>  What can go wrong? <br><br>  If something is wrong, then we say our magic HTTP 409 Conflict: <br><br><img src="https://habrastorage.org/files/205/0d9/59a/2050d959a3e842b383f4b3a580f88db2.png"><br><br>  Now we will try to screw it all: <br><br><img src="https://habrastorage.org/files/9f0/926/acd/9f0926acd7244bc2b233511c209d57be.png"><br><br>  And redo it. <br><br>  We will now try again our magic scripts that make.  Let's see what we have in node n2. <br><br><img src="https://habrastorage.org/files/729/770/ebb/729770ebb313462997dcad17fb0fced8.png"><br><br>  Now it will be slow, such a meditative process.  Almost.  We are waiting for her to be knocked out.  Tapped off. <br><br>  Let's see what we have in node n2. <br><br><img src="https://habrastorage.org/files/19f/345/96f/19f34596fa7a425ca0b5e39c8b0ae46d.png"><br><br>  There lies 0. We see what happened here: <br><br><img src="https://habrastorage.org/files/bfb/31d/258/bfb31d2587484ea7aed4d1d8b4602483.png"><br><br>  Local read, we have 0 read with five votes, generally cool.  This distributed system. <br><br>  Let's write something there: <br><br><img src="https://habrastorage.org/files/43c/fd0/5f0/43cfd05f0e464bd19eef464e628f4ddf.png"><br><br>  We recorded there one.  She signed up: <br><br><img src="https://habrastorage.org/files/23d/419/9fe/23d4199fedba4942ace67d0753570fb0.png"><br><br>  We have rolled out this one here on other nodes.  We have a quorum here, one with five votes.  It seems like everything is fine. <br><br>  Let's return to our Jepsen.  Because we have a quorum, and a quorum can have many masters.  In general, there is no such thing as a quorum. <br><br>  Here we will remove our crutch in the client: <br><br><img src="https://habrastorage.org/files/7be/91c/b6c/7be91cb6ca03493f92b683dc0757ec2c.png"><br><br>  And we will try to restart it all and see what will happen to our quorum when we run our integrity test again.  If you abuse the code, then with all the force. <br><br>  It began to write, a lot of things happen here. <br><br>  In the end, it told us true: <br><br><img src="https://habrastorage.org/files/8ff/185/85d/8ff18585da784ee09b49a245163cd6bc.png"><br><br>  It seems like everything is fine.  But I told you about this that Jepsen is such a certain probabilistic nature of tests.  Therefore, instead of 5 seconds, we will include 15 here: <br><br><img src="https://habrastorage.org/files/ed1/40c/7d0/ed140c7d0d554757916aa7564165fd0c.png"><br><br>  And for reliability we will run this test again.  In the hope that he might bring something else to us, catching bugs is probabilistic.  Very interesting and relaxing process.  Let's hope that at the second attempt, it will find something for us.  Now we chase her a little longer, 15 seconds, in the hope that our quorum will grumble somewhere.  Almost.  Everyone is looking forward to what he will tell us.  It told us fail: <br><br><img src="https://habrastorage.org/files/864/2de/58c/8642de58cbca4d8e8b9eace023fbd457.png"><br><br>  What we waited for.  What happened to fail here? <br><br>  Let's look at these two functions abnormally here: <br><br><img src="https://habrastorage.org/files/df5/3ef/71d/df53ef71db5d4899a35efce9bfc6dcfa.png"><br><br>  There is something wrong here.  First, because it is Scala, there is always something wrong.  Secondly, there is some problem in logic. <br><br>  Let's look at the following sequence of records: <br><br><img src="https://habrastorage.org/files/35c/b64/93a/35cb6493a0a14f0892b52cabf470f405.png"><br><br>  We have two customers.  We recorded operation A here, then the nodes said ‚Äúok‚Äù, we recorded, we have a quorum here.  But until we told the client that we had recorded A, a faster client came running here, who managed to write B, the nodes told him that the quorum had converged, and we also managed to say that everything was fine.  We told this client that we wrote B, we said that we wrote A. Here we read something, what did we read here? <br><br><img src="https://habrastorage.org/files/eea/1ad/ad2/eea1adad273e4af697f98683e84d019f.png"><br><br>  Since time is short, I will tell you that we will read B here, and this is all non-linearizable.  Because we should have read A, because there should not be such a situation.  Because we have to resolve conflicts in our system. <br><br>  To avoid this, it is not necessary to reinvent the wheel and write quorum yourself, unless you are the author of the RAFT or PAXOS algorithm.  This algorithm helps us all to do normal distributed systems. <br><br>  The PAXOS &amp; RAFT quorum algorithms describe a finite state machine, and your finite state machine goes between states.  All these transitions are logged. <br><br><img src="https://habrastorage.org/files/d26/51e/ab7/d2651eab781545a28abf4b3d2cf587c8.png"><br><br>  These algorithms describe the agreement on the order of operations in this log.  They describe the choice of the wizard, the application of the operation, how to put this log on your nodes.  But if you have the same log on all nodes, then your finite state machine, executing this log from beginning to end, will come to the same state on all nodes.  It seems like a good thing. <br><br>  The problem is that PAXOS is quite complex.  It is written by a mathematician for mathematicians.  If you try to implement it, you will have another variation of the implementation of PAXOS, which is very much, even scary to think how much.  It is not for people, it is not divided into phases.  This is just a huge footcloth, describing mathematical constructions.  It is necessary to think out a lot.  All these variations of PAXOS and the implementation of PAXOS are about how to think out what is written in this paper, in the direction of real implementation, which can be used in production. <br><br>  To avoid this, there is such an algorithm RAFT.  It is newer, all problems are taken into account, all the steps that need to be taken to implement a good consensus algorithm are described there.  Everything is cool there.  There are a huge number of different implementations: <br><br><img src="https://habrastorage.org/files/351/4e6/a96/3514e6a964b349abae271f0f4da94d25.png"><br><br>  There is only for Java, but there are implementations for all languages, even in PHP, I think, there is, although it is not clear why.  You can take this library and try to test it for yourself.  Not all of them implement the correct RAFT.  I tried to use akka-raft, which seems to be supposed to work, but for some reason it did not pass the Jepsen tests, although it seems to be written that it should sometimes. <br><br>  Consensus algorithms are used in many places, even in the third version of MongoDB, RAFT appeared.  In Cassandra all my life has been PAXOS.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In many databases, queuing systems, when they grow to maturity, sooner or later an algorithm of consensus appears. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, when you write your distributed system, you need to understand that you have many paths you can follow. </font><font style="vertical-align: inherit;">You should know that there are these ways and not to make another one of your own. </font><font style="vertical-align: inherit;">When you choose a path, you need to understand that every path you have is a compromise between integrity, latency, throughput, and other different things. </font><font style="vertical-align: inherit;">If you understand this compromise and can apply it to your subject area, then all is well. </font></font><br><br><img src="https://habrastorage.org/files/8eb/104/48b/8eb10448b7444b4ebb28b36af05d2211.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We can not say which of these approaches is better. </font><font style="vertical-align: inherit;">It all depends on you. </font><font style="vertical-align: inherit;">It all depends on your business task. </font><font style="vertical-align: inherit;">These are just tools.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/nNzhUGx99JE" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Report: </font></font><a href="http://www.highload.ru/2016/abstracts/2244.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fear and Loathing in Distributed Systems.</font></font></a> </div><p>Source: <a href="https://habr.com/ru/post/322876/">https://habr.com/ru/post/322876/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../322858/index.html">Top key threats, strong trends and promising technologies. Gartner forecasts for the next few years</a></li>
<li><a href="../322860/index.html">Catch in 30 minutes and other records Social Media Support</a></li>
<li><a href="../322864/index.html">The time of cargo cults in the marketing of games has passed. Interview with Max Donskikh, Game Insight President</a></li>
<li><a href="../322870/index.html">Go to work in Germany and stay live forever</a></li>
<li><a href="../322872/index.html">The terms "frontend", "client side" and "interface" - how to use and not screw it up</a></li>
<li><a href="../322880/index.html">The Story of One Thick Binary</a></li>
<li><a href="../322884/index.html">We look inside the domestic 28nm MIPS processor - Baikal-T1</a></li>
<li><a href="../322886/index.html">Pricing when creating a site and problems on the route Client - Contractor</a></li>
<li><a href="../322888/index.html">What you need to know when developing your CMF and CMS. Experience 2 years long</a></li>
<li><a href="../322890/index.html">The subtleties of R. How minute saves the minute</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>