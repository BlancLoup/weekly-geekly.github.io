<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Cisco UCS Manager. Management Interface Overview</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the course of the post about unpacking Cisco UCS , which arrived at our Competence Center, we will talk about Cisco UCS Manager, which is used to c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Cisco UCS Manager. Management Interface Overview</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/fef/563/92c/fef56392ca0c3684295c3afc426a407c.jpg" align="left" alt="cisco UCS" title="general view of Cisco UCS"><br>  In the course of the post about <a href="http://habrahabr.ru/company/it-grad/blog/160733/">unpacking Cisco UCS</a> , which arrived at our Competence Center, we will talk about Cisco UCS Manager, which is used to configure the entire system.  We performed this task in preparation for testing <a href="http://www.it-grad.ru/resheniya/packaged-solutions-for-virtualization/">FlexPod</a> ( <a href="http://www.it-grad.ru/oborudovanie/cisco-ucs/">Cisco UCS</a> + <a href="http://www.it-grad.ru/oborudovanie/netapp/">NetApp</a> in MetroCluster mode) for one of our customers. <br><br>  We note the main advantages of Cisco UCS, thanks to which the choice fell on this particular system: <a name="habracut"></a><br><ul><li>  Cisco UCS is a unified system, not a scattered group of servers with an attempt to have a single management; </li><li>  the presence of a unified factory that contains embedded management, the use of universal transport, the absence of a switching level in the chassis and at the hypervisor level ‚Äî the Virtual Interface Card, simple cabling; </li><li>  a single convergent factory for the whole system (one-time management of up to 320 servers), unlike competitors, has its own factory for each chassis (up to 16 servers); </li><li>  One point of management, quick configuration when using policies and profiles, which minimizes risks when configuring, deploying, replicating, provides high scalability. </li></ul><br>  Traditional blade systems have the <b>following disadvantages</b> : <br><ul><li>  each server blade is ideologically the same rack or tower server from a management point of view, only in a different form factor; </li><li>  each server is a complex system with a large number of components and a large number of powerful management tools; </li><li>  each chassis, each server, and in most cases, each switch must be configured separately and most often "manually"; </li><li>  ‚ÄúUmbrella‚Äù ‚Äúunified‚Äù management systems exist, but in most cases this is just a set of links to individual utilities; </li><li>  ‚Äúupward‚Äù integration for these systems is possible, but limited to ‚Äústandard‚Äù simple interfaces. </li></ul><br>  Unlike competitors, Cisco UCS has a <b>thoughtful management system</b> - <b>UCS Manager</b> .  This product has the following features: <br><ul><li>  does not require a separate server, DBMS, OS and licenses for them - it is part of UCS; </li><li>  does not require installation and configuration - just turn on the system and set the IP address; </li><li>  has built-in fault tolerance; </li><li>  has efficient and simple configuration backup tools; </li><li>  updated with a few ‚Äúclicks‚Äù; </li><li>  delegates all its functionality "up" through the open XML API; </li><li>  worth $ 0.00. </li></ul><br>  Cisco UCS Manager itself is located on both Fabric Interconnect, and, accordingly, all settings, policies, etc.  also stored on them. <br><br>  Up to 20 blade chassis can be connected to one pair of Fabric Interconnect.  Each chassis is managed from a single point - UCS Manager.  This is possible due to the fact that Fabric Interconnect is built as a distributed switch with the participation of Fabric Extenders.  Each chassis has two Fabric Extender, with the ability to connect 4 or 8 10GB links each. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We start with the most basic: connect the console wire and set the initial IP address.  Then we launch the browser and connect.  (It will take java, since in fact UCS Manager's GUI is implemented on it).  The pictures below show the appearance of the program, which appears on the screen after entering the login and password and logging into the UCS Manager. <br><br><h3>  LAN tab </h3><br>  We start setting up work on <b>the ‚ÄúLAN‚Äù tab</b> .  It should be noted that setting up this tab, as well as the ‚ÄúSAN‚Äù tab, begins setting up the entire system.  This is especially true in the case of diskless servers - just our case. <br><br>  Everything that is related to the network is configured here: which VLANs are allowed on one or another Fabric Interconnect, which ports the Fabric Interconnect is connected to each other, which ports work in which mode and much more. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/169/187/c75/169187c75379523e101d4e3031cc055c.png" alt="     ¬´LAN¬ª" title="Work on setting the tab &quot;LAN&quot;"></a> <br><br>  Also on this tab are configured templates for virtual network interfaces: vNIC, which will later be used in the service policies described above.  For each vNIC (which can be up to 512 to a word on a host when installing 2 Cisco UCS VIC cards), a specific VLAN is tied, or the VLAN group and the ‚Äúphysical communication‚Äù of the physical host over the network go through these vNICs. <br><br>  The organization of such a number of vNICs is possible using a Virtual Interface Card (VIC).  There are 2 types of VIC in the Cisco UCS server blade family: 1240 and 1280. <br><br>  Each of them has the ability to support up to 256 vNIC or vHBA.  The main difference is that 1240 is a LOM module, and 1280 is a mezzanine card, also in bandwidth. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3a5/4d4/7bb/3a54d47bb64fc1456c7e7912e372cfc6.jpg" alt="    Cisco UCS  2  VIC: 1240  1280" title="There are 2 types of VIC in the Cisco UCS server blade family: 1240 and 1280"><br><br>  1240 independently has the ability to aggregate up to 40 GB, with the possibility of expanding up to 80GB when using the Expander Card. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/386/ad6/5f7/386ad65f76faf1d13d6f5ac079775f2e.jpg" alt="1240      40 GB,     80GB   Expander Card" title="1240 independently has the ability to aggregate up to 40 GB, expandable up to 80GB using Expander Card"><br><br>  1280 is initially able to pass through itself up to 80GB. <br><br>  VICs allow you to dynamically assign bandwidth to each vNIC or vHBA, and also support hardware failover. <br><br>  Below you can see several created pools of IP addresses, of which these same addresses will be assigned to network interfaces in certain VLANs (a sort of HP's EBIPA).  The situation is the same with MAC addresses that are taken from MAC pools. <br><br>  In general, the concept is simple: we create VLAN (s), configure ports on Fabric Interconnect, configure vNICs, issue vNIC MAC / IP addresses from pools and drive them into the service policy for future use. <br><br>  <i>Important note:</i> As you know, there are validated Cisco designs with major players in the storage market, such as FlexPod, for example.  The FlexPod design includes server and network equipment ‚Äî Cisco, and NetApp acts as storage.  Why do you ask there is also network equipment?  The thing is that in version 1.4 and 2.0 UCS Manager itself Fabric Interconnect did not implement FC zoning.  For this, an upstream switch (for example, Cisco Nexus) connected to Fabric Interconnect through Uplink ports was required.  In the version of UCS Manager 2.1.  Local area support has been added, making it possible to connect Storage directly to Fabric Interconnect.  Possible designs can be found <a href="http://www.cisco.com/en/US/prod/collateral/ps10265/ps10276/whitepaper_c11-702584.html">here</a> .  But still, you should not leave FlexPod without a switch, since  FlexPod is a standalone data center building block. <br><br><h4>  Go to the next tab: "SAN" </h4><br>  Here is everything that relates to a data network: iSCSI and FCoE.  The concept is the same as that of the vNIC network adapters, but with its differences: WWN, WWPN, IQN, etc. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/94f/56c/9ad/94f56c9ad3530078099eac85dabf9b45.png" alt=" ¬´SAN¬ª" title="SAN tab"></a> <br><br><h4>  Go to the tab "Equipment" </h4><br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/334/046/eec/334046eecbff032cad2003b29a34d3ab.png" alt=" ¬´Equipment¬ª" title="Equipment tab"></a> <br><br>  Through this interface, you can see information about our shared computing infrastructure, which has been deployed for testing.  So, one chassis (Chassis 1) and four blade servers (Server1, Server2, Server3, Server4) were involved.  This information can be obtained from the tree on the left side (Equipment) or in a graphical form (Physical Display). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a51/d8b/028/a51d8b028156221ff77de1932b297512.png" alt="  (Equipment)    (Physical Display)" title="Left side (Equipment) graphically (Physical Display)"><br><br>  In addition to the number of chassis / servers, you can view information about auxiliary components (fans, power supplies, etc.), as well as information about different types of alerts.  The total number of alerts is displayed in the ‚ÄúFault Summary‚Äù block. Information on each node can be viewed by clicking on it on the left.  The colored frames around the names of nodes on the left help us to find out on which node there are alerts.  Attentive readers will also notice small icons on the graphic view. <br><br>  Our Fabric Interconnect are configured to work in failover mode (if one of the units fails, another one will immediately take over its work).  Information about this is visible at the very bottom of the tree - designations primary and subordinate. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/883/17a/2cf/88317a2cff8e0f05d4c19c83287068dd.png" alt="     ‚Äì  primary  subordinate" title="Information at the very bottom of the tree - designation primary and subordinate"><br><br><h4>  Next, go to the tab "Servers" </h4><br>  Here are various policies, service profile templates and the service profiles themselves.  For example, we can create several organizations, each of which assigns its own boot device selection policy, access to certain VLAN / VSAN, etc. <br><br>  The following figure shows an example of setting up an iSCSI boot moon. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a8b/f9f/aba/a8bf9faba7d22356c6ea23d9e0a3b330.png" alt="  iSCSI  " title="An example of setting up an iSCSI boot moon"></a> <br><br>  The global meaning of all this is: having pre-configured all policies and templates, you can quickly configure new servers to work using one or another service profile (Service Profile). <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/7b1/7b8/b5c/7b17b8b5ce2bc290e6d586a8ec094648.png" alt="      ,       (Service Profile)" title="You can quickly configure new servers to work using one or another service profile (Service Profile)"></a> <br><br>  The service profile, which contains the above settings, can be applied to one specific server or group of servers.  If any errors or messages occur during the deployment, we can view them in the corresponding tab ‚ÄúFaults‚Äù: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/f7a/9ca/08a/f7a9ca08a8f42b16df80ec9d83b184d6.png" alt=" ¬´Faults¬ª" title="Tab &quot;Faults&quot;"></a> <br><br>  After that, users of these organizations can deploy and configure the server for themselves with the selected operating system and a set of pre-installed software.  To do this, you must take advantage of the capabilities of the UCS Manager interface, or by turning to automation: for example, using the VMware vCloud Automation Center or Cisco Cloupia product. <br><br><h4>  Go to the tab "VM" </h4><br>  You can install plugins for Cisco UCS Manager in VMware vCenter.  So, we connected a plugin through which we connected a virtual data center with the name "FlexPod_DC_1".  It runs Cisco Nexus 1000v (nexus-dvs), and we have the opportunity to watch its settings. <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/c5c/578/e7d/c5c578e7d816e014e609a3111de15dc4.png" alt=" ¬´VM¬ª" title="VM tab"></a> <br><br><h4>  Well, the last tab: "Admin" </h4><br>  Here we can see global events: errors, warnings.  Collect various data when contacting Cisco TAC support, etc.  things.  In particular, they can be obtained on the tabs "TechSupport Files", "Core Files". <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/79e/f8d/f94/79ef8df9416a987115fcb83630c66010.png" alt=" ¬´Admin¬ª" title="Admin tab"></a> <br><br>  The screenshot shows the errors associated with the outage.  As part of testing, we turned off the power supply units in turn (Power supply 1 in fabric interconnect A, B) to demonstrate to the customer the stability of the system in a simulated case of a power outage at one power input. <br><br><h4>  As a conclusion </h4><br>  We would like to note that the entire process of configuring and testing the FlexPod, which is part of the configuration and test of Cisco UCS, took a whole week.  The lack of documentation for the latest version of UCS Manager 2.1 made the task a bit more complicated.  Had to use the option for 2.0, which has a number of discrepancies.  Just in case we post a <a href="http://www.cisco.com/en/US/docs/unified_computing/ucs/UCS_CVDs/cisco_ucs_vmware_ethernet_flexpod.html">link to the documentation</a> .  Perhaps it will be useful to someone. <br><br>  In the following posts we will continue to cover the topic of <a href="http://www.it-grad.ru/resheniya/packaged-solutions-for-virtualization/">FlexPod</a> .  In particular, we plan to talk about how we, together with the customer, tested NetApp in MetroCluster mode as part of <a href="http://www.it-grad.ru/resheniya/packaged-solutions-for-virtualization/">FlexPod</a> .  Do not disconnect!  :) </div><p>Source: <a href="https://habr.com/ru/post/177283/">https://habr.com/ru/post/177283/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../177267/index.html">Cross-platform development on Adobe Air: a special case</a></li>
<li><a href="../177269/index.html">A new Adobe sharpen filter will be paid.</a></li>
<li><a href="../177273/index.html">Client with Linq support for Microsoft Dynamics CRM (alternative to client from sdk)</a></li>
<li><a href="../177277/index.html">App Annie Index: Mobile Application Market Report, Q1 '13</a></li>
<li><a href="../177279/index.html">Toshiba Satellite U920t Ultrabook Transformer Video Review</a></li>
<li><a href="../177285/index.html">My way to freelancing</a></li>
<li><a href="../177289/index.html">The head of the Russian Post was dismissed!</a></li>
<li><a href="../177291/index.html">Happy Farm Business Incubator presents an updated second cycle mentoring program</a></li>
<li><a href="../177293/index.html">Wikileaks published a five-hour conversation of Eric Schmidt and Julian Assange</a></li>
<li><a href="../177297/index.html">PVC windows calculator for JS site. Part 1. Analytics</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>