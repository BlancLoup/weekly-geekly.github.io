<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Reinforcement training or evolutionary strategies? - Both</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 

 We do not often decide to place here translations of texts two years ago, without a code and obviously an academic focus - but today we w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Reinforcement training or evolutionary strategies? - Both</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br>  We do not often decide to place here translations of texts two years ago, without a code and obviously an academic focus - but today we will make an exception.  We hope that the dilemma in the title of the article worries many of our readers, and you have already read the original work on evolutionary strategies with which this post is debated in the original or read it now.  Welcome under the cut! <br><br><img src="https://habrastorage.org/webt/-n/u-/i6/-nu-i6enynr12ma1d7utan_7ml8.jpeg"><br><a name="habracut"></a><br>  In March 2017, OpenAI created a stir in the deep learning community by publishing the article ‚Äú <a href="https://openai.com/blog/evolution-strategies/">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a> .‚Äù In this paper, impressive results were presented in favor of reinforcing learning (RL) did not converge, and when learning complex neural networks, it is advisable to try other methods.  Then a discussion flared up about the importance of learning with reinforcement and how it deserves the status of ‚Äúmandatory‚Äù technology in teaching problem solving.  Here I want to talk about, you should not consider these two technologies as competing, one of which is definitely better than the other;  on the contrary, they ultimately complement each other.  Indeed, if we think a little about what is required to create a <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">common AI</a> and such systems that throughout the whole existence would be capable of learning, judgment and planning, then almost certainly we will come to the conclusion that this will require some kind of a combined solution. .  By the way, it was precisely to the combined solution that nature came, which in the course of evolution endowed mammals and other higher animals with complex intelligence. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Evolutionary strategies </h4><br>  The main thesis of the OpenAI article was that instead of using reinforcement training in combination with traditional back propagation, they successfully trained the neural network to solve complex problems using the so-called ‚Äúevolutionary strategy‚Äù (ES).  Such an ES approach is to maintain the distribution of weight values ‚Äã‚Äãacross the network, with many agents operating in parallel and using the parameters selected from this distribution.  Each agent acts in its own environment and upon completion of a specified number of episodes or episode stages, the algorithm returns a cumulative reward, expressed as a fitness score.  Given this value, the distribution of parameters can be shifted towards more successful agents, depriving the less successful ones.  Millions of times by repeating such an operation involving hundreds of agents, you can move the distribution of weights into such a space that allows you to formulate a qualitative policy for the agents to solve their task.  Indeed, the results presented in the article are impressive: it is shown that if you run a thousand agents in parallel, the anthropomorphic movement on two legs can be studied in less than half an hour (whereas even the most advanced RL methods require spending more than one hour on this).  For more information, I recommend reading a great <a href="https://openai.com/blog/evolution-strategies/">post</a> from the authors of the experiment, as well as the <a href="https://arxiv.org/abs/1703.03864">scientific article</a> itself. <br><br><img src="https://habrastorage.org/webt/0j/lp/ms/0jlpmsa3-jz8ono405nv8c79ve8.gif"><br><br>  <i>Various strategies for teaching anthropomorphic upright walking, studied by the ES method from OpenAI.</i> <br><br><h4>  Black box </h4><br>  The great advantage of this method is that it is easily parallelized.  While RL methods, for example, A3C, require the exchange of information between workflows and a parameter server, the ES only needs validity estimates and summarized information on the distribution of parameters.  Thanks to this simplicity, this method is far from scaling up the modern RL methods.  However, all this comes not for nothing: you have to optimize the network on the principle of a black box.  In this case, the ‚Äúblack box‚Äù means that when training, the internal structure of the network is completely ignored, and only the overall result (reward per episode) is used, and it depends on it whether the weights of a particular network will be inherited by subsequent generations.  In situations where we do not receive pronounced feedback from the environment - and when solving many traditional problems associated with RL, the flow of rewards is very sparse - the problem turns from "a partly black box" into a "completely black box".  In this case, you can seriously improve performance, so that, of course, such a compromise is justified.  ‚ÄúWho needs gradients if they are still hopelessly noisy?‚Äù - this is the general opinion. <br><br>  However, in situations where the feedback is more active, the affairs of the ES begin to fail.  The OpenAI team describes how the simple MNIST classification network was trained using the ES, and this time the training went 1000 times slower.  The fact is that the gradient signal when classifying images is extremely informative on how to teach the network a higher quality classification.  Thus, the problem is connected not so much with the RL methodology, as with sparse rewards in environments that give noisy gradients. <br><br><h4>  The solution found by nature </h4><br>  If you try to learn from the example of nature, thinking through ways to develop AI, in some cases, AI can be represented as a <a href="https://twitter.com/fchollet/status/849076312853663744">problem-oriented approach</a> .  In the end, nature operates within the framework of such restrictions that information scientists simply do not have.  There is an opinion that a purely theoretical approach to solving a particular problem can provide more effective solutions than empirical alternatives.  Nevertheless, I still think that it would be advisable to check how the dynamic system operating under certain restrictions (Earth) formed agents (animals, in particular mammals) capable of flexible and complex behavior.  While some of these restrictions are not applicable in the modeled data science worlds, others are just fine. <br><br>  Having examined the intellectual behavior of mammals, we see that it is formed as a result of a complex interaction of two closely interrelated processes: <i>learning from other people's experience</i> and <i>learning from one‚Äôs own experience</i> .  The first is often identified with evolution due to natural selection, but here I use a broader term to take into account epigenetics, microbiomes and other mechanisms that ensure the exchange of experience between organisms that are not related to each other from a genetic point of view.  The second process, learning by doing, is all the information that an animal has learned during its life, and this information is directly determined by the interaction of this animal with the outside world.  This category covers everything from learning to recognize objects to mastering the communication inherent in the learning process. <br><br>  Roughly speaking, these two processes occurring in nature can be compared with two options for optimizing neural networks.  Evolutionary strategies, where information on gradients is used to update information about the body, converge with learning from other people's experiences.  Similarly, gradient methods, where obtaining one or another experience leads to one or another change in the behavior of an agent, are comparable to learning from one's own experience.  If you think about the varieties of intellectual behavior or about the abilities that each of these two approaches develops in animals, this comparison is more pronounced.  In both cases, "evolutionary methods" contribute to the study of reactive behaviors that allow you to develop a certain fitness (enough to stay alive).  Learning to walk or flee from captivity is in many cases equivalent to more ‚Äúinstinctive‚Äù behaviors that are ‚Äúhard-wired‚Äù in many animals at the genetic level.  In addition, this example confirms that evolutionary methods are applicable in cases where the signal-reward is extremely rare (such, for example, is the fact of successful rearing of the young).  In such a case, it is impossible to correlate remuneration with any particular set of actions that may have been performed many years before the occurrence of this fact.  On the other hand, if we consider the case in which the ES fails, namely the classification of images, the results will be remarkably comparable to the results of animal training achieved in the course of countless behavioral psychological experiments conducted over 100 years. <br><br><h4>  Animal learning </h4><br>  The methods used in training with reinforcement, in many cases, are taken directly from the psychological literature on <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B0%25D0%25BD%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B2%25D0%25BB%25D0%25B8%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">operant conditioning</a> , and operant conditioning has been studied on the material of animal psychology.  By the way, Richard Sutton, one of the two founders of reinforcement training, has a bachelor's degree in psychology.  In the context of operant conditioning, animals learn to associate reward or punishment with specific behavioral patterns.  Animal trainers and researchers can in one way or another manipulate such an association with reward, provoking animals to demonstrate ingenuity or certain behaviors.  However, operant conditioning used in the study of animals is nothing more than a more pronounced form of the conditioning itself, on the basis of which animals learn throughout their lives.  We constantly receive positive reinforcement signals from the environment and adjust our behavior accordingly.  In fact, many neurophysiologists and cognitives believe that in fact people and other animals act even to a higher level and constantly learn to predict the results of their behavior in future situations, counting on potential rewards. <br><br>  The central role of forecasting in learning by doing is changing the above dynamics in the most significant way.  The signal that was previously considered very sparse (episodic reward) turns out to be very dense.  Theoretically, the situation is approximately as follows: at each moment in time, the mammalian brain calculates the results based on a complex stream of sensory stimuli and actions, while the animal is simply immersed in this stream.  In this case, the final behavior of the animal gives a dense signal, which has to be guided by the adjustment of forecasts and the development of behavior.  The brain uses all these signals in order to optimize the forecasts (and, accordingly, the quality of the actions taken) in the future.  An overview of this approach is given in the excellent book ‚Äú <a href="https://www.amazon.com/Surfing-Uncertainty-Prediction-Action-Embodied/dp/0190217014/">Surfing Uncertainty</a> ‚Äù by the cognitive and philosopher Andy Clarke.  If we extrapolate such reasoning to the training of artificial agents, then a fundamental flaw is found in reinforcement training: the signal used in this paradigm is hopelessly weak compared to what it could be (or should be).  In cases where it is impossible to increase the signal saturation (perhaps because it is weak by definition or is associated with low-level reactivity), it is probably better to prefer a training method that is well parallelized, for example, ES. <br><br><h4>  More intensive neural network training </h4><br>  Based on the principles of higher nervous activity inherent in the mammalian brain, constantly engaged in forecasting, some progress has been made recently in reinforcement learning, which now takes into account the importance of such predictions.  On the move, I can recommend you two similar works: <br><br><ul><li>  <a href="https://arxiv.org/abs/1611.01779">Learning to Act By Predicting the Future</a> </li><li> <a href="https://arxiv.org/abs/1611.05397">Reinforcement Learning with Unsupervised Auxiliary Tasks</a> </li></ul><br>  In both of these articles, the authors supplement the typical default policy of their neural networks with the results of future environmental forecasts.  In the first article, prediction is applied to a set of measurement variables, and in the second, changes in the environment and the behavior of the agent itself.  In both cases, the sparse signal associated with positive reinforcement becomes much richer and more informative, providing both accelerated learning and the assimilation of more complex behavioral models.  Such improvements are only available when working with methods that use a gradient signal, but not with methods that operate according to the black box principle, such as, for example, an ES. <br><br>  In addition, learning by doing and gradient methods are much more effective.  Even in those cases when it was possible to study this or that problem using the ES method faster than with the help of reinforcement training, the gain was achieved due to the fact that the ES strategy involved many times more data than in RL.  Reflecting in this case on the principles of learning from animals, we note that the result of learning on another's example manifests itself after many generations, whereas sometimes a single event experienced by itself is enough for an animal to learn a lesson forever.  While such <a href="https://en.wikipedia.org/wiki/One-shot_learning">training without examples</a> does not quite fit into traditional gradient methods yet, it is much more user-friendly than ES.  There are, for example, such approaches as <a href="https://arxiv.org/abs/1703.01988">episodic neural control</a> , where Q-values ‚Äã‚Äãare preserved in the learning process, after which the program checks with them before performing actions.  It turns out a gradient method that allows you to learn how to solve problems much faster than before.  In the article on episodic neural control, the authors mention the human hippocampus, which is able to store information about an event even after having experienced it once and, therefore, plays a <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2650423/">crucial role</a> in the recall process.  Such mechanisms require access to the internal organization of the agent, which is also by definition impossible in the ES paradigm. <br><br><h4>  So why not combine them? </h4><br>  Probably, most of this article could have left such an impression, as if in it I was upholding RL methods.  However, in fact, I believe that in the long run, the best solution would be a combination of both methods, so that each is used in the situations in which it is best suited.  Obviously, in the case of many reactive policies or in situations with very sparse signals, positive ES reinforcement benefits, all the more so if you have computing power at your disposal on which you can run mass-parallel learning.  On the other hand, gradient methods using reinforcement training or teacher training will be useful when extensive feedback is available to us, and solving the problem requires learning quickly and with less data. <br><br>  Turning to nature, we find that the first method, in essence, lays the foundation for the second.  That is why in the course of evolution, mammals have developed a brain, which allows it to study extremely effectively on the material of complex signals from the environment.  So, the question remains open.  Perhaps evolutionary strategies will help us invent effective learning architectures that will also be useful for gradient learning methods.  After all, the solution found by nature is indeed very successful. </div><p>Source: <a href="https://habr.com/ru/post/456160/">https://habr.com/ru/post/456160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../456144/index.html">KubeCon EU 2019: 10 key findings</a></li>
<li><a href="../456146/index.html">DevOps in Development: Automating Web Application Code Writing</a></li>
<li><a href="../456148/index.html">News of the week: Facebook denies Huawei applications, OS Aurora instead of Android, penalties for mining</a></li>
<li><a href="../456154/index.html">Core UX Features & MVP when creating a product</a></li>
<li><a href="../456158/index.html">A little about the sources of nuclear fuel</a></li>
<li><a href="../456168/index.html">Where was your home millions of years ago?</a></li>
<li><a href="../45617/index.html">uTorrent for makovodov</a></li>
<li><a href="../456170/index.html">How to create an application for finance: 5 API to help the developer</a></li>
<li><a href="../456174/index.html">How to organize a hackathon as a student 101. Part Two</a></li>
<li><a href="../456178/index.html">Themes and styles in Android without magic. And how to cook them with SwitchCompat</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>