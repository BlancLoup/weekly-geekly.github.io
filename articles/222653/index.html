<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hadoop and Automation: Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 


 I continue my ‚Äúmerry‚Äù series of articles devoted to getting to know Hadoop and automating the deployment of a cluster. 

 In the first part...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hadoop and Automation: Part 2</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><img align="right" src="https://habrastorage.org/getpro/habr/post_images/a68/93c/224/a6893c224e8a271beb8ba230ae2e1c06.png"><br><br>  I continue my ‚Äúmerry‚Äù series of articles devoted to getting to know <b>Hadoop</b> and automating the deployment of a cluster. <br><br>  In the <a href="http://habrahabr.ru/company/epam_systems/blog/222485/">first part,</a> I briefly described what needed to be achieved, what cluster architecture to build and what the <i>Hadoop cluster is</i> from an architectural point of view.  Also, I considered, probably, the simplest part of the cluster - <i>Clients</i> , which is responsible for setting tasks, providing data for calculations and getting results. <br><a name="habracut"></a><br>  Now it's time to talk about the part of the cluster architecture, which is <i>Masters</i> - namely, <i>HDFS</i> and <i>YARN</i> . <br>  Let me give you another example of the <i>architecture</i> that was to be deployed in a private cloud.  It was intended solely for test needs, the actual data load was not provided. <br><img src="https://habrastorage.org/getpro/habr/post_images/7ff/4cc/6be/7ff4cc6be3a756614e65252fce5ed6db.png"><br>  So, it was assumed that we will have <i>2 nodes</i> responsible for the <b>NameNode-</b> role, between which <i>HA + Failover is</i> configured based on <b>Zookeeper</b> .  <i>NameNode,</i> in turn, is responsible for coordinating the data in our distributed <i>HDFS</i> file system.  <i>NameNode</i> own the directory tree and monitor the files distributed across our cluster.  By themselves, <i>NameNode</i> nodes do not store data, because  for this role we have <i>slaves</i> . <br>  <b>JournalNode</b> , in turn, is necessary for us if we implement <b>High Availability</b> based on <b>QJM</b> (Quorum Journal Manager), the essence of which is that dedicated virtual machines ( <b>JournalNode</b> ) are used to synchronize between <i>Active</i> and <i>Stanbdy NameNode</i> , which contain lists of changes in HDFS.  The logs of these changes are available to both NameNode, respectively, in any case of failover, we achieve synchronization of our <i>NameNode-</i> nodes. <br>  Another <b>high availability</b> option is using <b>NFS / NAS</b> .  The essence is approximately the same - there <i>is a</i> storage " <i>mounted</i> " on the network, in which all so-called ones are written.  <i>shared edits</i> , mentioned changes logs in <i>HDFS</i> . <br>  In our case, <i>YARN</i> is responsible for <i>1 node</i> - the <b>ResourceManager</b> , which manages tasks, calculations, distributes computing resources between tasks, and is also responsible for accepting and setting tasks for execution. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Deploy HDFS </h2><br>  <i>&lt;Lyrical digression&gt;</i> <br>  It should be noted that initially the idea of ‚Äã‚Äãautomating the deployment process of the <i>Hadoop</i> cluster looked quite exotic, in view of the rather ‚Äúgentle‚Äù nature of it (although it may be my personal experience ...).  What I mean?  During automation, it turned out that the slightest deviation from the documentation led to a large number of questions or to the identification of problems previously documented in <i>JIRA by the</i> developers of <i>Hadoop</i> distributions. <br>  In a word, cookbook turned out to be not the closest to the " <b>best practices</b> " in my understanding, since  it contains a fair amount of Bash code, execute resources, and only occasionally (where possible) uses <i>Ruby DSL</i> and <i>Chef resources</i> . <br>  Also, the idea that there is <b>Apache Ambari</b> , the native <i>Hadoop</i> cluster management system, was <b>haunted</b> .  But again - this is not our method, and I wanted to understand the process without the help of third-party software. <br>  <i>&lt;/ Lyrical digression&gt;</i> <br><br>  The time has come to dig deeper into the deployment process and the <i>cookbook</i> code, which will be responsible for automating the cluster deployment process. <br>  I mentioned in the first article that the <i>cookbook</i> I wrote was based on <a href="http://community.opscode.com/cookbooks/hadoop">Community practices</a> , around which I created a wrapper.  In fact, not everything went smoothly, because  in the <i>community cookbook</i> version available at the time of my work start - there were several problems, sometimes very annoying - a typo, the wrong team and other trifles (we must pay tribute to the developer, who very quickly responded to my modest contribution and standing questions). <br>  In fact, the achievements of the community were an excellent foundation for starting work, adding some solutions if possible. <br>  Well, I started with the <b>NameNode</b> deployment and the <i>HA + Failover</i> mechanism.  This process for <i>Linux</i> nodes can be described as follows: <br><ul><li>  Installing prerequisites in the form of <i>Java</i> ; </li><li>  Adding repositories with packages of <i>Hadoop</i> distribution; </li><li>  Creating the backbone of the directories needed to install <i>NameNode</i> ; </li><li>  Generating configuration files based on the <i>template</i> and <i>cookbook</i> <i>attributes</i> </li><li>  Installing distribution <i>packages</i> (hdfs-namenode, zookeeper-server, etc) </li><li>  Exchange <i>ssh-keys</i> between <i>NameNode</i> (required for controlled <i>failover</i> ); </li><li>  Depending on the role of the node ( <i>Active / Standby</i> ) - starting <i>processes</i> for cluster operation (hereinafter - more); </li><li>  Register the status of the deployment process. </li></ul><br>  Starting the processes necessary for <b>NameNode to</b> function consisted of the following steps: <br><ul><li>  <b>chown</b> directories related to <i>Hadoop</i> components (for example, the <i>NameNode HDFS</i> installation directories) is a very <b>important</b> step, because  in most cases leads to problems; </li><li> <code>hdfs namenode -format</code> , which formats and initializes the directory specified as <i>dfs.namenode.name.dir</i> ; </li><li>  <code>service zookeeper-server start</code> - start the <b>zookeeper</b> server used during <i>failover</i> </li><li>  <code>hdfs zkfc -formatZK</code> - creates a <b>znode</b> (Zookeeper Data Node, in other words - a participant in the failover process); </li><li>  <code>$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config /etc/hadoop/conf.chef/ --script hdfs start namenode"</code> - to start the <b>Active NameNode</b> process <br>  <b>or</b> <br>  <code>hdfs namenode -bootstrapStandby</code> - to run the <b>Standby NameNode</b> process on the node; </li><li>  <code>$HADOOP_PREFIX/sbin/hadoop-daemon.sh --config /etc/hadoop/conf.chef/ start zkfc</code> - to start the <b>ZooKeeper Failover Controller</b> process; </li><li>  <code>node.set['hadoop_services']['already_namenode'] = true</code> ‚Äî sets the status of the process, <i>Ruby is the</i> code that sets the attribute value of the node attribute. </li></ul><br>  After <b>Chef has</b> successfully worked on <i>2 nodes</i> , following these steps, you can begin the installation verification process.  There are several verification options that are best used: <br><ol><li>  Open the <i>NameNode DFS Health</i> web page, accessible by default at the following address - <b>FQDN: 50070</b> - which provides information about the role of the node ( <i>Active</i> or <i>Standby</i> ), available <i>Slaves</i> , as well as various system information and logs; </li><li>  Using the <b>jps</b> <i>utility</i> (provided in the <i>Hadoop</i> distribution and analogous to <i>ps</i> ), the output of which must include the <i>NameNode</i> , <i>DFSZKFailoverController</i> and <i>QuorumPeerMain processes</i> ; </li><li>  Launching local <code>hdfs haadmin -healthCheck</code> and <code>hdfs haadmin -getServiceState</code> , the result of which, according to the stunt, is shown on the web page in a more detailed format; </li><li>  To verify the <i>failover</i> process, you can call a controlled <i>failover</i> mechanism as follows: <code>hdfs haadmin -failover</code> , as a result of which NameNode nodes should switch to <i>Active / Standby</i> roles. </li></ol><br>  A successful result is a bunch of <i>2 nodes</i> , one of which takes on the role of <i>Active NameNode</i> , the other - <i>Standby NameNode</i> ;  between the nodes ( <i>znode</i> ) a <i>Zookeeper</i> control is installed, which is able to conduct a <i>failover</i> process;  access to the <i>Slave</i> nodes and the file system (the deployment of the <i>Slave</i> will be discussed in the next article). <br><br>  As already mentioned, the <b>high availability of</b> our cluster can be achieved in 2 ways - using <b>NFS / NAS</b> or a dedicated <b>JournalNode</b> node. <br>  In the case of <b>NFS / NAS</b> , all we need is to be able to ‚Äúmount‚Äù the storage over the network to our NameNode.  The storage should be available <i>24/7</i> (highly desirable), accessible with <i>low</i> latency, and also respond quickly to <i>read / write</i> operations over the network. <br>  In the case of using <b>JournalNode</b> , it is necessary to select the node on which the JournalNode package is installed from the Hadoop distribution.  For the configuration in the basic version, there are 2 parameters: <i>dfs.journalnode.http-address</i> ‚Äî the parameter that indicates the FQDN and the port on which the <i>JournalNode</i> service is <i>running</i> );  <i>dfs.journalnode.edits.dir</i> - directory in which <i>logs of</i> events occurring in <i>HDFS are</i> added. <br><br><h2>  Deploying YARN </h2><br>  <b>The YARN</b> cluster part is represented in our <b>ResourceManager</b> architecture as a node that works with tasks and resources for their execution.  <b>The process of</b> deploying this node looks simpler than <i>NameNode</i> : <br><ul><li>  Installing prerequisites in the form of <i>Java</i> ; </li><li>  Adding repositories with packages of <i>Hadoop</i> distribution; </li><li>  Creating the backbone of the directories needed to install <i>NameNode</i> ; </li><li>  Generating configuration files based on the <i>template</i> and <i>cookbook</i> <i>attributes</i> </li><li>  Installing distribution <i>packages</i> ( <i>hadoop-yarn-resourcemanager</i> ) </li><li>  Starting the <i>ResourceManager</i> process by <code>service hadoop-yarn-resourcemanager start</code> ; </li><li>  Register the status of the <i>deployment</i> process. </li></ul><br>  To verify successful deployment, you can do the following: <br><ol><li>  Open the <i>ResourceManager</i> web page, accessible, by default, at the following address - <b>FQDN: 8088</b> - which provides data on available <i>Slaves</i> , as well as various information about the tasks and resources allocated for their execution; </li><li>  Using the <b>jps</b> <i>utility</i> (provided in the <i>Hadoop</i> distribution and analogous to <i>ps</i> ), in the output of which the <i>ResourceManager</i> process must be present; </li></ol><br>  That's about my part <b>Masters</b> .  At the <i>end of the next article,</i> I plan to highlight a section in which I will describe the <i>minimum</i> cluster settings that are <i>REQUIRED</i> to run.  Also, in the next article, I will publish links to my modest project and useful documentation, which I used in the process of creating it. <br><br>  Thank you all for your attention!  Comments and especially <i>amendments</i> are <b>very</b> welcome! </div><p>Source: <a href="https://habr.com/ru/post/222653/">https://habr.com/ru/post/222653/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../222643/index.html">Spreading the iPhone and diffusing innovation</a></li>
<li><a href="../222645/index.html">Smart Transformer Wheelchair</a></li>
<li><a href="../222647/index.html">Application interface: our mistakes and 16 tips on how not to repeat them</a></li>
<li><a href="../222649/index.html">Kohana-form: beta release. Changes and innovations</a></li>
<li><a href="../222651/index.html">Zingaya app update for iOS</a></li>
<li><a href="../222655/index.html">Home robot: from idea to product</a></li>
<li><a href="../222657/index.html">Testing home robot version 0.3.1</a></li>
<li><a href="../222659/index.html">RedHat blocks Russian accounts</a></li>
<li><a href="../222661/index.html">Cross-compile POCO from Windows for Linux</a></li>
<li><a href="../222663/index.html">Ethernet gateway nooLite PR1132 - light control from a smartphone and tablet</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>