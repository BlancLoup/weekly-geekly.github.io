<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Understanding the neural network war (GAN)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Generative adversarial networks (GAN) are becoming increasingly popular. Many people talk about them, some even use them ... but, as it turns out, ver...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Understanding the neural network war (GAN)</h1><div class="post__text post__text-html js-mediator-article">  Generative adversarial networks (GAN) are becoming increasingly popular.  Many people talk about them, some even use them ... but, as it turns out, very few people (even those who use them) understand and can explain.  ;-) <br>  Let's take a look at the simplest example, how they work, what they learn and what they actually produce. <a name="habracut"></a><br><br>  To begin with, I recommend everyone to read the excellent article " <a href="https://habrahabr.ru/post/275429/">Counterfeiters vs. Bankers: Bleed adversarial networks in Theano</a> ".  Moreover, we will take an example from it. <br><br>  To some, this article may seem verbose, and the explanations are redundant and even repetitive.  However, the way it is.  However, do not forget that repetition is the mother of learning.  In addition, different people perceive the text very differently, so that sometimes even small changes in the wording play a big role.  In general, everything that is already clear, just scroll through. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Rival networks </h4><br>  So, we have two neural networks.  We denote the first network generating it as a function <i>y <sub>G</sub> = G (z)</i> , it takes some value <i>z</i> at the input, and gives the value y <sub>G</sub> at the output.  The second network is distinguishing, and we denote it as a function <i>y <sub>D</sub> = D (x)</i> , that is, at the input - <i>x</i> , at the output - <i>y <sub>D.</sub></i> <br><br>  The generating network must learn to generate such samples <i>y <sub>G</sub></i> that the distinguishing network <i>D</i> cannot distinguish them from some real, reference samples.  And the distinguishing network, in turn, must, on the contrary, learn to distinguish these generated patterns from the real ones. <br><br>  Initially, the networks do not know anything, so they need to be trained.  But how?  After all, for the training of ordinary networks need a special set of training data - <i>(X <sub>i</sub> , Y <sub>i</sub> )</i> .  X <sub>i</sub> are sequentially input to the network and <i>y y = N (X <sub>i</sub> ) are</i> calculated.  Then, according to the difference between <i>Y <sub>i</sub></i> (real value) and <i>y <sub>i</sub></i> (network performance), the network coefficients are recalculated and so on in a circle. <br><br>  So, not so.  In fact, at each learning step, the value of the loss function is calculated, the gradient of which then recalculates the network coefficients.  But the loss function in one way or another takes into account the difference between <i>Y <sub>i</sub></i> and <i>y <sub>i</sub></i> .  But strictly speaking, it is not necessary.  First, the loss function can be arranged and in a completely different way, for example, to take into account the values ‚Äã‚Äãof network coefficients or the value of some other function.  Second, instead of minimizing the loss function during network training, you can solve another optimization problem, for example, maximizing the success function.  And this is just used in the GAN. <br><br>  Learning the first, generating network is to maximize the functional <i>D (G (z))</i> .  That is, this network seeks to maximize not its result, but the result of the work of the second, distinguishing, network.  Simply put, the generating network must learn for any value supplied to its input, generate at the output such a value, which is supplied to the input of the discriminating network, we get the maximum value at its output <i>(perhaps it is worth reading it twice)</i> . <br>  If the output of the discriminating network is sigmoid, then we can say that it returns the probability that the network has the ‚Äúcorrect‚Äù value.  Thus, the generating network seeks to maximize the probability that the distinguishing network does not distinguish the result of the generating network from the ‚Äúreference‚Äù samples (which means that the generating network generates the correct samples). <br>  Here it is important to remember that in order to make one step in training the generating network, it is necessary to calculate not only the result of the generating network, but also the result of the work of the distinguishing network.  I will formulate, though clumsily, but intuitively clear: the generating network will learn from the gradient of the result of the work of the distinguishing network. <br><br>  Learning the second, distinguishing, network is to maximize the functional <i>D (x) (1 - D (G (z)))</i> .  Roughly, it (the network, that is, the <i>D ()</i> function) should produce "ones" for the reference samples and "zeroes" for the samples generated by the generating network. <br>  And the network does not know anything about whether it is submitted to the input: a standard or a fake.  Only the functional "knows" this.  However, the network studies in the direction of the gradient of this functional, which ‚Äúas if‚Äù transparently hints to it how good it is. <br>  Note that at each step of learning of the distinguishing network, the result of the generating network‚Äôs work is calculated once and twice the result of the discriminating network: for the first time, the reference sample is fed to the input, and the second is the result of the generating network. <br><br>  In general, both networks are connected in an indissoluble circle of mutual learning.  And in order for this whole construction to work, we need reference samples ‚Äî a set of training data <i>(X <sub>i</sub> )</i> .  Note that <i>Y <sub>i</sub></i> is not needed here.  Although, it is clear that in fact the default implies that each <i>X <sub>i</sub></i> corresponds to <i>Y <sub>i</sub> = 1</i> . <br><br>  In the process of learning, at each step, some values ‚Äã‚Äãare given to the input of the generating network, for example, completely random numbers (or even non-random numbers ‚Äî recipes for controlled generation).  At each step, the next reference sample and the next result of operation of the generating network are fed to the input of the distinguishing network. <br>  As a result, the originating network must learn to generate samples as close as possible to the reference ones.  And the distinguishing network must learn to distinguish between reference and generated samples. <br><br>  Although it is worth remembering that the generated samples can turn out to be unsuccessful, because the purpose of training the generating network is to maximize the similarity functional.  And this maximum reached can be only a little more than zero, that is, the <i>G</i> network has not really learned anything.  So be sure to check and make sure everything is fine. <br><br><h4>  Parse the code </h4><br>  Now let's take a look at the code, because there is also not so simple. <br><br>  First, we import all the necessary modules: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> lasagne <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano.tensor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> T <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> lasagne.nonlinearities <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> rectify, sigmoid, linear, tanh</code> </pre> <br>  We define a function that returns a uniform noise on the interval [-5,5], which we will further apply to the input of the generating network. <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample_noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(M)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.float32(np.linspace(<span class="hljs-number"><span class="hljs-number">-5.0</span></span>, <span class="hljs-number"><span class="hljs-number">5.0</span></span>, M) + np.random.random(M) * <span class="hljs-number"><span class="hljs-number">0.01</span></span>).reshape(M,<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Create a character variable that will be the input to the generating network: <br><pre> <code class="python hljs">G_input = T.matrix(<span class="hljs-string"><span class="hljs-string">'Gx'</span></span>)</code> </pre><br>  And we describe the network itself: <br><pre> <code class="python hljs">G_l1 = lasagne.layers.InputLayer((<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), G_input) G_l2 = lasagne.layers.DenseLayer(G_l1, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=rectify) G_l3 = lasagne.layers.DenseLayer(G_l2, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=rectify) G_l4 = lasagne.layers.DenseLayer(G_l3, <span class="hljs-number"><span class="hljs-number">1</span></span>, nonlinearity=linear) G = G_l4 G_out = lasagne.layers.get_output(G)</code> </pre><br>  In terms of the lasagne library in this network, there are 4 layers.  But, from an academic point of view, the input and output layers are not considered, so we get a two-layer network. <br>  The result of the network operation will be recorded in the <i>G_out</i> variable after some value has been supplied to its input (in <i>G_input</i> ).  Subsequently, <i>G_out</i> will be transmitted to the input of the distinguishing network, therefore, in their format, <i>G_out</i> and <i>D_input</i> must match. <br><br>  Now we will create a character variable that will be the input of the distinguishing network and to which we will submit ‚Äúreference‚Äù samples. <br><pre> <code class="python hljs">D1_input = T.matrix(<span class="hljs-string"><span class="hljs-string">'D1x'</span></span>)</code> </pre><br>  And we describe the distinguishing network.  In this case, it is almost no different from the generator, only at the output of her sigmoid. <br><pre> <code class="python hljs">D1_target = T.matrix(<span class="hljs-string"><span class="hljs-string">'D1y'</span></span>) D1_l1 = lasagne.layers.InputLayer((<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), D1_input) D1_l2 = lasagne.layers.DenseLayer(D1_l1, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=tanh) D1_l3 = lasagne.layers.DenseLayer(D1_l2, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=tanh) D1_l4 = lasagne.layers.DenseLayer(D1_l3, <span class="hljs-number"><span class="hljs-number">1</span></span>, nonlinearity=sigmoid) D1 = D1_l4</code> </pre><br><br>  And now let's make a tricky trick.  As you remember, the reference samples must be submitted to the input of the discriminating network, then the result of the generating network.  But in a computational graph (in other words, in Theano, TensorFlow and similar libraries) this cannot be done.  Therefore, we will create a third network that will become a complete copy of the previously described distinguishing network. <br><pre> <code class="python hljs">D2_l1 = lasagne.layers.InputLayer((<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), G_out) D2_l2 = lasagne.layers.DenseLayer(D2_l1, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=tanh, W=D1_l2.W, b=D1_l2.b) D2_l3 = lasagne.layers.DenseLayer(D2_l2, <span class="hljs-number"><span class="hljs-number">10</span></span>, nonlinearity=tanh, W=D1_l3.W, b=D1_l3.b) D2_l4 = lasagne.layers.DenseLayer(D2_l3, <span class="hljs-number"><span class="hljs-number">1</span></span>, nonlinearity=sigmoid, W=D1_l4.W, b=D1_l4.b) D2 = D2_l4</code> </pre><br>  And here, the input of the network is the value <i>G_out</i> , which is the result of the operation of the generating network.  Moreover, the coefficients of all layers of the third network are equal to the coefficients of the second network.  Therefore, the third and second network are complete copies of each other. <br><br>  However, the results of the work of these two identical networks will be displayed in different variables. <br><pre> <code class="python hljs">D1_out = lasagne.layers.get_output(D1) D2_out = lasagne.layers.get_output(D2)</code> </pre><br>  So we got to the task of optimization functionals: <br><pre> <code class="python hljs">G_obj = (T.log(D2_out)).mean() D_obj = (T.log(D1_out) + T.log(<span class="hljs-number"><span class="hljs-number">1</span></span> - D2_out)).mean()</code> </pre><br>  Now you see why we needed two output variables of the distinguishing network. <br><br>  Next, create a learning function for the generating network: <br><pre> <code class="python hljs">G_params = lasagne.layers.get_all_params(G, trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) G_lr = theano.shared(np.array(<span class="hljs-number"><span class="hljs-number">0.01</span></span>, dtype=theano.config.floatX)) G_updates = lasagne.updates.nesterov_momentum(<span class="hljs-number"><span class="hljs-number">1</span></span> - G_obj, G_params, learning_rate=G_lr, momentum=<span class="hljs-number"><span class="hljs-number">0.6</span></span>) G_train = theano.function([G_input], G_obj, updates=G_updates)</code> </pre><br>  In <i>G_params there</i> will be a list of all coefficients of all layers of the generating network. <br>  <i>Learning</i> speed will be stored in <i>G_lr</i> . <br>  <i>G_updates</i> - actually, the function of updating coefficients by the method of gradient descent.  Notice that the first parameter takes the loss function, that is, it does not maximize <i>G_obj</i> , but minimizes <i>(1-G_obj)</i> (but this is just a feature of Theano).  By the second parameter, all network coefficients are transferred to it, and then the learning rate and the constant with the impulse value (which is needed only because the Nesterov impulse method is chosen as the gradient descent method). <br>  As a result, in <i>G_train</i> we get the network learning function, the input of which is <i>G_input</i> , and the result of the calculation is <i>G_obj</i> , that is, the optimization functional for the generating network. <br><br>  Now everything is the same for the distinguishing network: <br><pre> <code class="python hljs">D_params = lasagne.layers.get_all_params(D1, trainable=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) D_lr = theano.shared(np.array(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, dtype=theano.config.floatX)) D_updates = lasagne.updates.nesterov_momentum(<span class="hljs-number"><span class="hljs-number">1</span></span> - D_obj, D_params, learning_rate=D_lr, momentum=<span class="hljs-number"><span class="hljs-number">0.6</span></span>) D_train = theano.function([G_input, D1_input], D_obj, updates=D_updates)</code> </pre><br>  Notice that <i>D_train is</i> already a function of the two variables <i>G_input</i> (input of the generating network) and <i>D1_input</i> (reference samples). <br><br>  Finally we start the training.  In a cycle by epoch: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(epochs):</code> </pre><br>  First, we teach the distinguishing network, and not just once, but K times. <br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(K):</code> </pre><br>  x - reference samples (in this case, the numbers from the normal distribution with the parameters <i>mu</i> and <i>sigma</i> ): <br>  z - random noise <br><pre> <code class="python hljs"> x = np.float32(np.random.normal(mu, sigma, (M,<span class="hljs-number"><span class="hljs-number">1</span></span>))) z = sample_noise(M)</code> </pre><br>  Next comes the magic of Theano: <br>  - reference samples are fed to the input of the distinguishing network <br>  - random noise <i>z</i> is fed to the input of the generating network, and its result to the input of the distinguishing network <br>  - after which the optimization functional for the distinguishing network is calculated. <br><pre> <code class="python hljs"> D_train(z, x)</code> </pre><br>  The result of the D_train function, and this, as you remember, the optimization functionality of D_obj, is not needed by itself, however, it is explicitly used to train this network, albeit in a somewhat imperceptible way. <br>  Then we train the generating network: again we form a vector of random values ‚Äã‚Äãand generate a result, which, however, is also used at the training stage only in the process of calculating the optimization functional. <br><pre> <code class="python hljs"> z = sample_noise(M) G_train(z)</code> </pre><br>  In theory, based on the original description of the task, both networks should take real value as input 1, but to speed up the learning we immediately feed a vector from <i>M</i> values, that is, as if, <i>M</i> iterations of the network are performed. <br><br>  Every 10 epochs, we slightly decrease the learning speed of both networks. <br><pre> <code class="python hljs"> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: G_lr *= <span class="hljs-number"><span class="hljs-number">0.999</span></span> D_lr *= <span class="hljs-number"><span class="hljs-number">0.999</span></span></code> </pre><br>  In the end, the training is completed, and you can use the <i>G</i> network to generate samples, giving random inputs to it, or even non-random data (recipes) that will generate samples with certain properties. </div><p>Source: <a href="https://habr.com/ru/post/278425/">https://habr.com/ru/post/278425/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../278409/index.html">Introduction to practical analytics, or what is common in neural networks with diet pills</a></li>
<li><a href="../278411/index.html">Authentication using Spring Security and JWT tokens</a></li>
<li><a href="../278413/index.html">Overview of synchronization primitives - mutex and cond</a></li>
<li><a href="../278421/index.html">Emotional landing page? Wow wow, easy</a></li>
<li><a href="../278423/index.html">Migration of UI patterns and gestures. Who has anyone that podtyril</a></li>
<li><a href="../278427/index.html">Connect to the Device Guard Webinar on Windows 10. Start March 3 at 11:00 (GMT)</a></li>
<li><a href="../278429/index.html">Fresh exhibition ISE - new LEDs, screens in the windows and how to break the screen that does not break</a></li>
<li><a href="../278431/index.html">First hakaton 2GIS</a></li>
<li><a href="../278435/index.html">Image binarization: Bradley algorithm</a></li>
<li><a href="../278437/index.html">If programmers were making pancakes (according to kosher methodologies)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>