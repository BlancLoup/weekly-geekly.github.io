<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Heat equation in tensorflow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Some time ago I became interested in deep learning and began to slowly study tensorflow. While digging into tensorflow, I remembered my cour...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Heat equation in tensorflow</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  Some time ago I became interested in deep learning and began to slowly study tensorflow.  While digging into tensorflow, I remembered my coursework on parallel programming, which I did that year in 4th year of university.  The task there was formulated as follows: <br><br>  Linear initial-boundary value problem for a two-dimensional heat equation: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20t%7D%20%3D%20%5Csum%20%5Climits_%7B%5Calpha%3D1%7D%5E%7B2%7D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cleft%20(k_%5Calpha%20%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cright%20)%20-u%2C%20%5Cquad%20x_%5Calpha%20%5Cin%20%5B0%2C1%5D%20%5Cquad%20(%5Calpha%3D1%2C2)%2C%20%5C%20t%3E0%3B%0A" alt="\ frac {\ partial u} {\ partial t} = \ sum \ limits _ {\ alpha = 1} ^ {2} \ frac {\ partial} {\ partial x_ \ alpha} \ left (k_ \ alpha \ frac {\ partial u} {\ partial x_ \ alpha} \ right) -u, \ quad x_ \ alpha \ in [0,1] \ quad (\ alpha = 1,2), \ t &amp; gt; 0;"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ak_%5Calpha%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%2050%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%201%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="k_ \ alpha = \ begin {cases} 50, (x_1, x_2) \ in \ Delta ABC \\ 1, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A(%5Calpha%20%3D%201%2C2)%2C%20%5C%20A(0.2%2C0.5)%2C%20%5C%20B(0.7%2C0.2)%2C%20%5C%20C(0.5%2C0.8)%3B%0A" alt="(\ alpha = 1,2), \ A (0.2,0.5), \ B (0.7,0.2), \ C (0.5,0.8);"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C%20x_2%2C%200)%20%3D%200%2C%5C%20u(0%2Cx_2%2Ct)%20%3D%201%20-%20e%5E%7B-%5Comega%20t%7D%2C%5C%20%20u(1%2C%20x_2%2C%20t)%20%3D%200%2C%0A" alt="u (x_1, x_2, 0) = 0, \ u (0, x_2, t) = 1 - e ^ {- \ omega t}, \ u (1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C0%2Ct)%20%3D%201%20-%20e%5E%7B-%5Comega%20t%7D%2C%5C%20u(0%2C%20x_2%2C%20t)%20%3D%200%2C%5C%20%20%5Comega%20%3D%2020.%0A" alt="u (x_1,0, t) = 1 - e ^ {- \ omega t}, \ u (0, x_2, t) = 0, \ \ omega = 20."></div><br>  Although it would be better to call it the diffusion equation. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The task then needed to be solved by the finite difference method in an implicit scheme, using MPI for parallelization and the conjugate gradient method. <br><br>  I am not an expert in numerical methods until I am an expert in tensorflow, but I already have experience.  And I was eager to try to calculate urmata on a deep learning framework.  The method of conjugate gradients to implement the second time is no longer interesting, but it is interesting to see how tensorflow will cope with the calculation and what difficulties will arise.  This post is about what came of it. <br><br><h2>  Numerical algorithm </h2><br><a name="habracut"></a><br>  Define the grid: <br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5COmega%20%3D%20%5Comega_%7Bx_1%7D%20%5Ctimes%20%5Comega_%7Bx_2%7D%20%5Ctimes%20%5Comega_t%2C%0A" alt="\ Omega = \ omega_ {x_1} \ times \ omega_ {x_2} \ times \ omega_t,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5C%5B%20%5Comega_%7Bx_%5Calpha%7D%20%3D%20%5Cleft%20%5C%7B%20x_%7B%5Calpha%2C%20i_%5Calpha%7D%20%3D%20i_%5Calpha%20h%2C%20i_%5Calpha%20%3D%200%2C...%2CN%2C%20h%20%3D%20%5Cfrac%7B1%7D%7BN%7D%2C%20%5Cright%20%5C%7D%5C%20%5Calpha%20%3D%201%2C2%2C%20%5C%5D%0A" alt="\ [\ omega_ {x_ \ alpha} = \ left \ {x _ {\ alpha, i_ \ alpha} = i_ \ alpha h, i_ \ alpha = 0, ..., N, h = \ frac {1} {N }, \ right \} \ \ alpha = 1,2, \]"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5C%5B%20%5Comega_t%20%3D%20%5Cleft%20%5C%7Bt_j%20%3D%20j%20%5Ctau%2C%20j%3D0%2C...%2CN_t%2C%20%5Ctau%20%3D%20%5Cfrac%7Bt_%7Bmax%7D%7D%7BN_t%7D%5Cright%20%5C%7D.%0A" alt="\ [\ omega_t = \ left \ {t_j = j \ tau, j = 0, ..., N_t, \ tau = \ frac {t_ {max}} {N_t} \ right \}."></div><br>  <strong>Difference scheme:</strong> <br><br>  To make it easier to paint, we introduce the operators: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5CDelta_%7B1%7Df_%7Bi%2Cj%7D%20%3D%20%5Cfrac%7Bf_%7Bi%2B1%2F2%2Cj%7D%20-%20f_%7Bi-1%2F2%2Cj%7D%7D%7Bh%7D%2C%0A" alt="\ Delta_ {1} f_ {i, j} = \ frac {f_ {i + 1/2, j} - f_ {i-1/2, j}} {h},"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5CDelta_%7B2%7Df_%7Bi%2Cj%7D%20%3D%20%5Cfrac%7Bf_%7Bi%2Cj%2B1%2F2%7D%20-%20f_%7Bi%2Cj-1%2F2%7D%7D%7Bh%7D.%0A" alt="\ Delta_ {2} f_ {i, j} = \ frac {f_ {i, j + 1/2} - f_ {i, j-1/2}} {h}."></div><br>  Explicit Difference Scheme: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5E%7Bt-1%7D%7D%7B%5Ctau%7D%20%3D%20%5CDelta_%7B1%7D(k_%7Bi%2Cj%7D%5CDelta_%7B1%7Du_%7Bi%2Cj%7D%5E%7Bt-1%7D)%20%2B%20%5CDelta_%7B2%7D(k_%7Bi%2Cj%7D%5CDelta_%7B2%7Du_%7Bi%2Cj%7D%5E%7Bt-1%7D)%20-%20u_%7Bi%2Cj%7D%5Et.%0A" alt="\ frac {u_ {i, j} ^ t - u_ {i, j} ^ {t-1}} {\ tau} = \ Delta_ {1} (k_ {i, j} \ Delta_ {1} u_ {i , j} ^ {t-1}) + \ Delta_ {2} (k_ {i, j} \ Delta_ {2} u_ {i, j} ^ {t-1}) - u_ {i, j} ^ t ."></div><br>  In the case of an explicit difference scheme, the values ‚Äã‚Äãof the function at the previous time point are used for the calculation and there is no need to solve the equation for values <img src="https://tex.s2cms.ru/svg/u%5Et_%7Bi%2Cj%7D" alt="u ^ t_ {i, j}">  .  However, this scheme is less accurate and requires a significantly smaller time step. <br><br>  Implicit difference scheme: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5E%7Bt-1%7D%7D%7B%5Ctau%7D%20%3D%20%5CDelta_%7B1%7D(k_%7Bi%2Cj%7D%5CDelta_%7B1%7Du_%7Bi%2Cj%7D%5Et)%20%2B%20%5CDelta_%7B2%7D(k_%7Bi%2Cj%7D%5CDelta_%7B2%7Du_%7Bi%2Cj%7D%5Et)%20-%20u_%7Bi%2Cj%7D%5Et%2C%0A" alt="\ frac {u_ {i, j} ^ t - u_ {i, j} ^ {t-1}} {\ tau} = \ Delta_ {1} (k_ {i, j} \ Delta_ {1} u_ {i , j} ^ t) + \ Delta_ {2} (k_ {i, j} \ Delta_ {2} u_ {i, j} ^ t) - u_ {i, j} ^ t,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5E%7Bt-1%7D%7D%7B%5Ctau%7D%20%3D%20%5CDelta_%7B1%7D(k_%7Bi%2Cj%7D%5Cfrac%7Bu_%7Bi%2B1%2F2%2Cj%7D%5Et%20-%20u_%7Bi-1%2F2%2Cj%7D%5Et%7D%7Bh%7D)%20%2B%20%0A%5CDelta_%7B2%7D(k_%7Bi%2Cj%7D%5Cfrac%7Bu_%7Bi%2Cj%2B1%2F2%7D%5Et%20-%20u_%7Bi%2Cj-1%2F2%7D%5Et%7D%7Bh%7D)%20-%20u_%7Bi%2Cj%7D%5Et%2C%0A" alt="\ frac {u_ {i, j} ^ t - u_ {i, j} ^ {t-1}} {\ tau} = \ Delta_ {1} (k_ {i, j} \ frac {u_ {i + 1 / 2, j} ^ t - u_ {i-1/2, j} ^ t} {h}) + \ Delta_ {2} (k_ {i, j} \ frac {u_ {i, j + 1/2 } ^ t - u_ {i, j-1/2} ^ t} {h}) - u_ {i, j} ^ t,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5E%7Bt-1%7D%7D%7B%5Ctau%7D%20%3D%20%5Cfrac%7Bk_%7Bi%2B1%2F2%2Cj%7D%5Cfrac%7Bu_%7Bi%2B1%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5Et%7D%7Bh%7D%20-%20k_%7Bi-1%2F2%2Cj%7D%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi-1%2F2%2Cj%7D%5Et%7D%7Bh%7D%7D%7Bh%7D%20%2B%0A%5Cfrac%7Bk_%7Bi%2Cj%2B1%2F2%7D%5Cfrac%7Bu_%7Bi%2Cj%2B1%7D%5Et%20-%20u_%7Bi%2Cj%7D%5Et%7D%7Bh%7D%20-%20k_%7Bi%2Cj-1%2F2%7D%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj-1%2F2%7D%5Et%7D%7Bh%7D%7D%7Bh%7D%20-%20u_%7Bi%2Cj%7D%5Et%2C%0A" alt="\ frac {u_ {i, j} ^ t - u_ {i, j} ^ {t-1}} {\ tau} = \ frac {k_ {i + 1/2, j} \ frac {u_ {i + 1, j} ^ t - u_ {i, j} ^ t} {h} - k_ {i-1/2, j} \ frac {u_ {i, j} ^ t - u_ {i-1/2, j} ^ t} {h}} {h} + \ frac {k_ {i, j + 1/2} \ frac {u_ {i, j + 1} ^ t - u_ {i, j} ^ t} { h} - k_ {i, j-1/2} \ frac {u_ {i, j} ^ t - u_ {i, j-1/2} ^ t} {h}} {h} - u_ {i, j} ^ t,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bu_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5E%7Bt-1%7D%7D%7B%5Ctau%7D%20%3D%20%5Cfrac%7Bk_%7Bi%2B1%2F2%2Cj%7Du_%7Bi%2B1%2Cj%7D%5Et%20-%20u_%7Bi%2Cj%7D%5Et%20-%20k_%7Bi-1%2F2%2Cj%7Du_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi-1%2F2%2Cj%7D%5Et%20%2B%20k_%7Bi%2Cj%2B1%2F2%7Du_%7Bi%2Cj%2B1%7D%5Et%20-%20u_%7Bi%2Cj%7D%5Et%20-%20k_%7Bi%2Cj-1%2F2%7Du_%7Bi%2Cj%7D%5Et%20-%20u_%7Bi%2Cj-1%2F2%7D%5Et%7D%7Bh%5E2%7D%20-%20u_%7Bi%2Cj%7D%5Et.%0A" alt="\ frac {u_ {i, j} ^ t - u_ {i, j} ^ {t-1}} {\ tau} = \ frac {k_ {i + 1/2, j} u_ {i + 1, j } ^ t - u_ {i, j} ^ t - k_ {i-1/2, j} u_ {i, j} ^ t - u_ {i-1/2, j} ^ t + k_ {i, j +1/2} u_ {i, j + 1} ^ t - u_ {i, j} ^ t - k_ {i, j-1/2} u_ {i, j} ^ t - u_ {i, j- 1/2} ^ t} {h ^ 2} - u_ {i, j} ^ t."></div><br>  Transfer to the left side everything related to <img src="https://tex.s2cms.ru/svg/u%5Et" alt="u ^ t">  , and to the right <img src="https://tex.s2cms.ru/svg/u%5E%7Bt-1%7D" alt="u ^ {t-1}">  and multiply by <img src="https://tex.s2cms.ru/svg/%5Ctau" alt="\ tau">  : <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A(1%20%2B%20%5Cfrac%7B%5Ctau%7D%7Bh%5E2%7D(k_%7Bi%2B1%2F2%2Cj%7D%20%2B%20k_%7Bi-1%2F2%2Cj%7D%20%2B%20k_%7Bi%2Cj%2B1%2F2%7D%20%2B%20k_%7Bi%2Cj-1%2F2%7D)%20%2B%20%5Ctau)u_%7Bi%2Cj%7D%5Et%20-%20%5C%5C%20-%20%5Cfrac%7B%5Ctau%7D%7Bh%5E2%7D(k_%7Bi%2B1%2F2%2Cj%7Du_%7Bi%2B1%2Cj%7D%5Et%20%2B%20k_%7Bi-1%2F2%2Cj%7Du%5Et_%7Bi-1%2Cj%7D%20%2B%20k_%7Bi%2Cj%2B1%2F2%7Du%5Et_%7Bi%2Cj%2B1%7D%20%2B%20k_%7Bi%2Cj-1%2F2%7Du%5Et_%7Bi%2Cj-1%7D)%20%3D%20u%5E%7Bt-1%7D_%7Bi%2Cj%7D.%0A" alt="(1 + \ frac {\ tau} {h ^ 2} (k_ {i + 1/2, j} + k_ {i-1/2, j} + k_ {i, j + 1/2} + k_ { i, j-1/2}) + \ tau) u_ {i, j} ^ t - \\ - \ frac {\ tau} {h ^ 2} (k_ {i + 1/2, j} u_ {i + 1, j} ^ t + k_ {i-1/2, j} u ^ t_ {i-1, j} + k_ {i, j + 1/2} u ^ t_ {i, j + 1} + k_ {i, j-1/2} u ^ t_ {i, j-1}) = u ^ {t-1} _ {i, j}."></div><br>  In fact, we obtained an operator equation over the grid: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AAu%5Et%20%3D%20u%5E%7Bt-1%7D%2C%0A" alt="Au ^ t = u ^ {t-1},"></div><br>  what if to write down values <img src="https://tex.s2cms.ru/svg/u%5Et" alt="u ^ t">  in the nodes of the grid as an ordinary vector, is an ordinary system of linear equations ( <img src="https://tex.s2cms.ru/svg/Ax%20%3D%20b" alt="Ax = b">  ).  The values ‚Äã‚Äãat the previous time constant, as already calculated. <br>  For convenience, we present the operator <img src="https://tex.s2cms.ru/svg/A" alt="A">  as the difference of two operators: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AA%20%3D%20D_A%20-%20(A%5E%2B%20%2B%20A%5E%7B-%7D)%2C%0A" alt="A = D_A - (A ^ + + A ^ {-}),"></div><br>  Where: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AD_A%20u%5Et%20%3D%20(1%20%2B%20%5Cfrac%7B%5Ctau%7D%7Bh%5E2%7D(k_%7Bi%2B1%2F2%2Cj%7D%20%2B%20k_%7Bi-1%2F2%2Cj%7D%20%2B%20k_%7Bi%2Cj%2B1%2F2%7D%20%2B%20k_%7Bi%2Cj-1%2F2%7D)%20%2B%20%5Ctau)%20u%5Et_%7Bi%2Cj%7D%2C%0A" alt="D_A u ^ t = (1 + \ frac {\ tau} {h ^ 2} (k_ {i + 1/2, j} + k_ {i-1/2, j} + k_ {i, j + 1 / 2} + k_ {i, j-1/2}) + \ tau) u ^ t_ {i, j},"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A(A%5E%2B%20%2B%20A%5E%7B-%7D)u%5Et%20%3D%20%5Cfrac%7B%5Ctau%7D%7Bh%5E2%7D(k_%7Bi%2B1%2F2%2Cj%7Du%5Et_%7Bi%2B1%2Cj%7D%20%2B%20k_%7Bi-1%2F2%2Cj%7Du%5Et_%7Bi-1%2Cj%7D%20%2B%0Ak_%7Bi%2Cj%2B1%2F2%7Du%5Et_%7Bi%2Cj%2B1%7D%20%2B%20k_%7Bi%2Cj-1%2F2%7Du%5Et_%7Bi%2Cj-1%7D).%0A" alt="(A ^ + + A ^ {-}) u ^ t = \ frac {\ tau} {h ^ 2} (k_ {i + 1/2, j} u ^ t_ {i + 1, j} + k_ { i-1/2, j} u ^ t_ {i-1, j} + k_ {i, j + 1/2} u ^ t_ {i, j + 1} + k_ {i, j-1/2} u ^ t_ {i, j-1})."></div><br>  Replacing <img src="https://tex.s2cms.ru/svg/u%5Et" alt="u ^ t">  to our assessment <img src="https://tex.s2cms.ru/svg/%5Chat%7Bu%7D%5Et" alt="\ hat {u} ^ t">  , we write the error functional: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ar%20%3D%20A%5Chat%7Bu%7D%5Et%20-%20u%5E%7Bt-1%7D%20%3D%20(D_A%20-%20A%5E%2B%20-%20A%5E%7B-%7D)%5Chat%7Bu%7D%5Et%20-%20u%5E%7Bt-1%7D%2C%0A" alt="r = A \ hat {u} ^ t - u ^ {t-1} = (D_A - A ^ + - A ^ {-}) \ hat {u} ^ t - u ^ {t-1},"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AL%20%3D%20%5Csum%20r_%7Bi%2Cj%7D%5E2.%0A" alt="L = \ sum r_ {i, j} ^ 2."></div><br>  Where <img src="https://tex.s2cms.ru/svg/r_%7Bi%2Cj%7D" alt="r_ {i, j}">  - error in the grid nodes. <br><br>  We will iteratively minimize the error functional using a gradient. <br><br>  As a result, the task has been reduced to the multiplication of tensors and gradient descent, and this is exactly what <strong>tensorflow</strong> was intended for. <br><br><h2>  Implementation at tensorflow </h2><br><h4>  Briefly about <strong>tensorflow</strong> </h4><br>  In tensorflow, the computation graph is first constructed.  Resources under the graph are allocated inside <strong>tf.Session</strong> .  Graph nodes are operations on data.  The cells for the input data in the graph are <strong>tf.placeholder</strong> .  To execute a graph, you need to run the run method on the session object, passing in it the operation of interest and input data for placeholders.  The <strong>run</strong> method returns the result of the operation and can also change the values ‚Äã‚Äãinside <strong>tf.Variable</strong> within the session. <br><br>  tensorflow itself is able to build graphs of operations that implement the <em>backpropagation of a</em> gradient, provided that the original graph contains only operations for which the gradient is implemented (not everyone yet). <br><br><h4>  Code: </h4><br>  First is the initialization code.  Here we make all the preliminary operations and consider everything that can be calculated in advance. <br><br><div class="spoiler">  <b class="spoiler_title">Initialization</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  import numpy as np import pandas as pd import tensorflow as tf import matplotlib.pyplot as plt import matplotlib from matplotlib.animation import FuncAnimation from matplotlib import cm import seaborn #    ,   #     class HeatEquation(): def __init__(self, nxy, tmax, nt, k, f, u0, u0yt, u1yt, ux0t, ux1t): self._nxy = nxy #    x, y self._tmax = tmax #   self._nt = nt #    self._k = k #  k self._f = f #  f self._u0 = u0 #   #   self._u0yt = u0yt self._u1yt = u1yt self._ux0t = ux0t self._ux1t = ux1t #       self._h = h = np.array(1./nxy) self._ht = ht = np.array(tmax/nt) print("ht/h/h:", ht/h/h) self._xs = xs = np.linspace(0., 1., nxy + 1) self._ys = ys = np.linspace(0., 1., nxy + 1) self._ts = ts = np.linspace(0., tmax, nt + 1) from itertools import product #       self._vs = vs = np.array(list(product(xs, ys)), dtype=np.float64) #   self._vcs = vsc = np.array(list(product(xs[1:-1], ys[1:-1])), dtype=np.float64) #      k vkxs = np.array(list(product((xs+h/2)[:-1], ys)), dtype=np.float64) # k_i+0.5,j vkys = np.array(list(product(xs, (ys+h/2)[:-1])), dtype=np.float64) # k_i ,j+0.5 #    k self._kxs = kxs = k(vkxs).reshape((nxy,nxy+1)) self._kys = kys = k(vkys).reshape((nxy+1,nxy)) #   D_A D_A = np.zeros((nxy+1, nxy+1)) D_A[0:nxy+1,0:nxy+0] += kys D_A[0:nxy+1,1:nxy+1] += kys D_A[0:nxy+0,0:nxy+1] += kxs D_A[1:nxy+1,0:nxy+1] += kxs self._D_A = D_A = 1 + ht/h/h*D_A[1:nxy,1:nxy] + ht # ,    self._U_shape = (nxy+1, nxy+1, nt+1) #        , #    ,     self._U = np.zeros(self._U_shape) #       self._U[:,:,0] = u0(vs).reshape(self._U_shape[:-1])</span></span></code> </pre> <br></div></div><br><img src="https://tex.s2cms.ru/svg/%5Ctau" alt="\ tau">  and <img src="https://tex.s2cms.ru/svg/h" alt="h">  should be taken such that <img src="https://tex.s2cms.ru/svg/%5Cfrac%7B%5Ctau%7D%7Bh%5E2%7D" alt="\ frac {\ tau} {h ^ 2}">  was small, preferably at least &lt;1, especially when using "nonsmooth" functions. <br><br>  The method that builds the equation graph: <br><br><div class="spoiler">  <b class="spoiler_title">Computation graph</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># ,   def build_graph(self, learning_rate): def reset_graph(): if 'sess' in globals() and sess: sess.close() tf.reset_default_graph() reset_graph() nxy = self._nxy #   kxs_ = tf.placeholder_with_default(self._kxs, (nxy,nxy+1)) kys_ = tf.placeholder_with_default(self._kys, (nxy+1,nxy)) D_A_ = tf.placeholder_with_default(self._D_A, self._D_A.shape) U_prev_ = tf.placeholder(tf.float64, (nxy+1, nxy+1), name="U_t-1") f_ = tf.placeholder(tf.float64, (nxy-1, nxy-1), name="f") #      ,     U_ = tf.Variable(U_prev_, trainable=True, name="U_t", dtype=tf.float64) #   def s(tensor, frm): return tf.slice(tensor, frm, (nxy-1, nxy-1)) #    A+_A-  u Ap_Am_U_ = s(U_, (0, 1))*s(self._kxs, (0, 1)) Ap_Am_U_ += s(U_, (2, 1))*s(self._kxs, (1, 1)) Ap_Am_U_ += s(U_, (1, 0))*s(self._kys, (1, 0)) Ap_Am_U_ += s(U_, (1, 2))*s(self._kys, (1, 1)) Ap_Am_U_ *= self._ht/self._h/self._h #  res = D_A_*s(U_,(1, 1)) - Ap_Am_U_ - s(U_prev_, (1, 1)) - self._ht*f_ #  ,    loss = tf.reduce_sum(tf.square(res), name="loss_res") #         u0yt_ = None u1yt_ = None ux0t_ = None ux1t_ = None if self._u0yt: u0yt_ = tf.placeholder(tf.float64, (nxy+1,), name="u0yt") loss += tf.reduce_sum(tf.square(tf.slice(U_, (0, 0), (1, nxy+1)) - tf.reshape(u0yt_, (1, nxy+1))), name="loss_u0yt") if self._u1yt: u1yt_ = tf.placeholder(tf.float64, (nxy+1,), name="u1yt") loss += tf.reduce_sum(tf.square(tf.slice(U_, (nxy, 0), (1, nxy+1)) - tf.reshape(u1yt_, (1, nxy+1))), name="loss_u1yt") if self._ux0t: ux0t_ = tf.placeholder(tf.float64, (nxy+1,), name="ux0t") loss += tf.reduce_sum(tf.square(tf.slice(U_, (0, 0), (nxy+1, 1)) - tf.reshape(ux0t_, (nxy+1, 1))), name="loss_ux0t") if self._ux1t: ux1t_ = tf.placeholder(tf.float64, (nxy+1,), name="ux1t") loss += tf.reduce_sum(tf.square(tf.slice(U_, (0, nxy), (nxy+1, 1)) - tf.reshape(ux1t_, (nxy+1, 1))), name="loss_ux1t") #  ,        , #   ,    loss /= (nxy+1)*(nxy+1) #    train_step = tf.train.AdamOptimizer(learning_rate, 0.7, 0.97).minimize(loss) #     ,    self.g = dict( U_prev = U_prev_, f = f_, u0yt = u0yt_, u1yt = u1yt_, ux0t = ux0t_, ux1t = ux1t_, U = U_, res = res, loss = loss, train_step = train_step ) return self.g</span></span></code> </pre><br></div></div><br>  In an amicable way, it was necessary to consider the values ‚Äã‚Äãof the function at the edges as given and optimize the values ‚Äã‚Äãof the function only in the inner region, but with this there were problems.  There was no way to make only a part of the tensor optimized, and the gradient of the tensor does not have a gradient written at the moment of writing the post.  One could try slyly tinkering on the edges or write your own optimizer.  But simply adding the difference at the edges of the function values ‚Äã‚Äãand the boundary conditions in the error functional works well. <br><br>  It should be noted that the method with the adaptive moment proved to be the best, let the error functional and quadratic. <br><br>  <em>Calculation of the function</em> : at each moment of time we do several optimization iterations, until we exceed maxiter or the error is less than eps, save and proceed to the next moment. <br><br><div class="spoiler">  <b class="spoiler_title">Calculation</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_graph</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, eps, maxiter, miniter)</span></span></span><span class="hljs-function">:</span></span> g = self.g losses = [] <span class="hljs-comment"><span class="hljs-comment">#    with tf.Session() as sess: #       sess.run(tf.global_variables_initializer(), feed_dict=self._get_graph_feed(0)) for t_i, t in enumerate(self._ts[1:]): t_i += 1 losses_t = [] losses.append(losses_t) d = self._get_graph_feed(t_i) p_loss = float("inf") for i in range(maxiter): #     #    u,   _, self._U[:,:,t_i], loss = sess.run([g["train_step"], g["U"], g["loss"]], feed_dict=d) losses_t.append(loss) if i &gt; miniter and abs(p_loss - loss) &lt; eps: p_loss = loss break p_loss = loss print('#', end="") return self._U, losses</span></span></code> </pre><br></div></div><br>  <strong><em>Run:</em></strong> <br><br><div class="spoiler">  <b class="spoiler_title">Startup code</b> <div class="spoiler_text"><pre> <code class="python hljs">tmax = <span class="hljs-number"><span class="hljs-number">0.5</span></span> nxy = <span class="hljs-number"><span class="hljs-number">100</span></span> nt = <span class="hljs-number"><span class="hljs-number">10000</span></span> A = np.array([<span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>]) B = np.array([<span class="hljs-number"><span class="hljs-number">0.7</span></span>, <span class="hljs-number"><span class="hljs-number">0.2</span></span>]) C = np.array([<span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.8</span></span>]) k1 = <span class="hljs-number"><span class="hljs-number">1.0</span></span> k2 = <span class="hljs-number"><span class="hljs-number">50.0</span></span> omega = <span class="hljs-number"><span class="hljs-number">20</span></span> <span class="hljs-comment"><span class="hljs-comment">#     def triang(v, k1, k2, A, B, C): v_ = v.copy() k = k1*np.ones([v.shape[0]]) v_ = v - A B_ = B - A C_ = C - A m = (v_[:, 0]*B_[1] - v_[:, 1]*B_[0]) / (C_[0]*B_[1] - C_[1]*B_[0]) l = (v_[:, 0] - m*C_[0]) / B_[0] inside = (m &gt; 0.) * (l &gt; 0.) * (m + l &lt; 1.0) k[inside] = k2 return k # 0.0 def f(v, t): return 0*triang(v, h0, h1, A, B, C) # 0.0 def u0(v): return 0*triang(v, t1, t2, A, B, C) #   def u0ytb(t, ys): return 1 - np.exp(-omega*np.ones(ys.shape[0])*t) def ux0tb(t, xs): return 1 - np.exp(-omega*np.ones(xs.shape[0])*t) def u1ytb(t, ys): return 0.*np.ones(ys.shape[0]) def ux1tb(t, xs): return 0.*np.ones(xs.shape[0]) #     eq = HeatEquation(nxy, tmax, nt, lambda x: triang(x, k1, k2, A, B, C), f, u0, u0ytb, u1ytb, ux0tb, ux1tb) _ = eq.build_graph(0.001) U, losses = eq.train_graph(1e-6, 100, 1)</span></span></code> </pre><br></div></div><br><h2>  results </h2><br>  Original condition: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/e9a/a90/98d/e9aa9098dd4a606b9e6e439ebc8a5340.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/23b/82f/b9b/23b82fb9b42c01d063cd5e4dd4043e70.gif"><br></div></div><br>  Condition as original, but without <img src="https://tex.s2cms.ru/svg/-u" alt="-u">  in the equation: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20t%7D%20%3D%20%5Csum%20%5Climits_%7B%5Calpha%3D1%7D%5E%7B2%7D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cleft%20(k_%5Calpha%20%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cright%20)%2C%20%5Cquad%20x_%5Calpha%20%5Cin%20%5B0%2C1%5D%20%5Cquad%20(%5Calpha%3D1%2C2)%2C%20%5C%20t%3E0%3B%0A" alt="\ frac {\ partial u} {\ partial t} = \ sum \ limits _ {\ alpha = 1} ^ {2} \ frac {\ partial} {\ partial x_ \ alpha} \ left (k_ \ alpha \ frac {\ partial u} {\ partial x_ \ alpha} \ right), \ quad x_ \ alpha \ in [0,1] \ quad (\ alpha = 1,2), \ t &amp; gt; 0;"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ak_%5Calpha%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%2050%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%201%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="k_ \ alpha = \ begin {cases} 50, (x_1, x_2) \ in \ Delta ABC \\ 1, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C%20x_2%2C%200)%20%3D%200%2C%5C%20u(0%2Cx_2%2Ct)%20%3D%201%20-%20e%5E%7B-%5Comega%20t%7D%2C%5C%20%20u(1%2C%20x_2%2C%20t)%20%3D%200%2C%0A" alt="u (x_1, x_2, 0) = 0, \ u (0, x_2, t) = 1 - e ^ {- \ omega t}, \ u (1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C0%2Ct)%20%3D%201%20-%20e%5E%7B-%5Comega%20t%7D%2C%5C%20u(0%2C%20x_2%2C%20t)%20%3D%200%2C%5C%20%20%5Comega%20%3D%2020.%0A" alt="u (x_1,0, t) = 1 - e ^ {- \ omega t}, \ u (0, x_2, t) = 0, \ \ omega = 20."></div><br>  What is easily corrected in the code: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   __init__ self._D_A = D_A = 1 + ht/h/h*D_A[1:nxy,1:nxy]</span></span></code> </pre><br>  There is almost no difference, because derivatives have larger orders than the function itself. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f1f/6cf/ef3/f1f6cfef3292e253fa60b43d1bbff6ab.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/c72/46d/4aa/c7246d4aabef079c7d17394354f611b9.gif"><br></div></div><br>  Further everywhere: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20t%7D%20%3D%20%5Csum%20%5Climits_%7B%5Calpha%3D1%7D%5E%7B2%7D%20%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cleft%20(k_%5Calpha%20%5Cfrac%7B%5Cpartial%20u%7D%7B%5Cpartial%20x_%5Calpha%7D%20%5Cright%20)%20%2Bf%2C%20%5Cquad%20x_%5Calpha%20%5Cin%20%5B0%2C1%5D%20%5Cquad%20(%5Calpha%3D1%2C2)%2C%20%5C%20t%3E0%3B%0A" alt="\ frac {\ partial u} {\ partial t} = \ sum \ limits _ {\ alpha = 1} ^ {2} \ frac {\ partial} {\ partial x_ \ alpha} \ left (k_ \ alpha \ frac {\ partial u} {\ partial x_ \ alpha} \ right) + f, \ quad x_ \ alpha \ in [0,1] \ quad (\ alpha = 1,2), \ t &amp; gt; 0;"></div><br><br>  Condition with one heating edge: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ak_%5Calpha%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%2010%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%201%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="k_ \ alpha = \ begin {cases} 10, (x_1, x_2) \ in \ Delta ABC \\ 1, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x_1%2Cx_2%2Ct)%20%3D%200%2C%0A" alt="f (x_1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C%20x_2%2C%200)%20%3D%200%2C%5C%20u(0%2Cx_2%2Ct)%20%3D%201%20-%20e%5E%7B-%5Comega%20t%7D%2C%5C%20%20u(1%2C%20x_2%2C%20t)%20%3D%200%2C%0A" alt="u (x_1, x_2, 0) = 0, \ u (0, x_2, t) = 1 - e ^ {- \ omega t}, \ u (1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C0%2Ct)%20%3D%200%2C%5C%20u(0%2C%20x_2%2C%20t)%20%3D%200%2C%5C%20%20%5Comega%20%3D%2020.%0A" alt="u (x_1,0, t) = 0, \ u (0, x_2, t) = 0, \ \ omega = 20."></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5fe/2f9/4f7/5fe2f94f71a6a52dfcd355f76d252c25.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/45e/0b5/2a0/45e0b52a01121f3ce2bb33aae34a4019.gif"><br></div></div><br>  The condition with the cooling of the initially heated area: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ak_%5Calpha%20%3D%201%2C%0A" alt="k_ \ alpha = 1,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x_1%2Cx_2%2Ct)%20%3D%200%2C%0A" alt="f (x_1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C%20x_2%2C%200)%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%200.1%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%200%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="u (x_1, x_2, 0) = \ begin {cases} 0.1, (x_1, x_2) \ in \ Delta ABC \\ 0, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(0%2Cx_2%2Ct)%20%3D%200%2C%5C%20%20u(1%2C%20x_2%2C%20t)%20%3D%200%2C%0A" alt="u (0, x_2, t) = 0, \ u (1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C0%2Ct)%20%3D%200%2C%5C%20u(0%2C%20x_2%2C%20t)%20%3D%200.%0A" alt="u (x_1,0, t) = 0, \ u (0, x_2, t) = 0."></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/11d/942/d81/11d942d811263c24e7c9f880ac5b24af.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/7c5/c0c/c3d/7c5c0cc3d529ca14a413dab08995942c.gif"><br></div></div><br>  The condition with the inclusion of heating in the area: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ak_%5Calpha%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%202%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%2010%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="k_ \ alpha = \ begin {cases} 2, (x_1, x_2) \ in \ Delta ABC \\ 10, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Af(x_1%2Cx_2%2Ct)%20%3D%0A%5Cbegin%7Bcases%7D%0A%20%20%20%2010%2C%20(x_1%2C%20x_2)%20%5Cin%20%5CDelta%20ABC%5C%5C%0A%20%20%20%200%2C%20(x_1%2C%20x_2)%20%5Cnotin%20%5CDelta%20ABC%0A%5Cend%7Bcases%7D%0A" alt="f (x_1, x_2, t) = \ begin {cases} 10, (x_1, x_2) \ in \ Delta ABC \\ 0, (x_1, x_2) \ notin \ Delta ABC \ end {cases}"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C%20x_2%2C%200)%20%3D%200%2C%5C%20u(0%2Cx_2%2Ct)%20%3D%200%2C%5C%20%20u(1%2C%20x_2%2C%20t)%20%3D%200%2C%0A" alt="u (x_1, x_2, 0) = 0, \ u (0, x_2, t) = 0, \ u (1, x_2, t) = 0,"></div><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Au(x_1%2C0%2Ct)%20%3D%200%2C%5C%20u(0%2C%20x_2%2C%20t)%20%3D%200.%0A" alt="u (x_1,0, t) = 0, \ u (0, x_2, t) = 0."></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/046/b89/ab4/046b89ab415ced48c3faf30f94ac7bc4.gif"><br><img src="https://habrastorage.org/getpro/habr/post_images/047/7aa/a4d/0477aaa4d76a092298ee5ac6328fc666.gif"><br></div></div><br><h2>  Drawing gifs </h2><br>  3D gif drawing function: <br><br><div class="spoiler">  <b class="spoiler_title">3d gif</b> <div class="spoiler_text">  In the main class, add a method that returns U in the form of pandas.DataFrame <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_U_as_df</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, step=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> nxyp = self._nxy + <span class="hljs-number"><span class="hljs-number">1</span></span> nxyp2 = nxyp**<span class="hljs-number"><span class="hljs-number">2</span></span> Uf = self._U.reshape((nxy+<span class="hljs-number"><span class="hljs-number">1</span></span>)**<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">-1</span></span>)[:, ::step] data = np.hstack((self._vs, Uf)) df = pd.DataFrame(data, columns=[<span class="hljs-string"><span class="hljs-string">"x"</span></span>,<span class="hljs-string"><span class="hljs-string">"y"</span></span>] + list(range(len(self._ts))[<span class="hljs-number"><span class="hljs-number">0</span></span>::step])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> df</code> </pre><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Udf, fname)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> mpl_toolkits.mplot3d <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Axes3D <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.ticker <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LinearLocator, FormatStrFormatter <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.interpolate <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> griddata fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>)) ts = list(Udf.columns[<span class="hljs-number"><span class="hljs-number">2</span></span>:]) data = Udf <span class="hljs-comment"><span class="hljs-comment">#    ,    matplotlib x1 = np.linspace(data['x'].min(), data['x'].max(), len(data['x'].unique())) y1 = np.linspace(data['y'].min(), data['y'].max(), len(data['y'].unique())) x2, y2 = np.meshgrid(x1, y1) z2s = list(map(lambda x: griddata((data['x'], data['y']), data[x], (x2, y2), method='cubic'), ts)) zmax = np.max(np.max(data.iloc[:, 2:])) + 0.01 zmin = np.min(np.min(data.iloc[:, 2:])) - 0.01 plt.grid(True) ax = fig.gca(projection='3d') ax.view_init(35, 15) ax.set_zlim(zmin, zmax) ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f')) norm = matplotlib.colors.Normalize(vmin=zmin, vmax=zmax, clip=False) surf = ax.plot_surface(x2, y2, z2s[0], rstride=1, cstride=1, norm=norm, cmap=cm.coolwarm, linewidth=0., antialiased=True) fig.colorbar(surf, shrink=0.5, aspect=5) #       def update(t_i): label = 'timestep {0}'.format(t_i) ax.clear() print(label) surf = ax.plot_surface(x2, y2, z2s[t_i], rstride=1, cstride=1, norm=norm, cmap=cm.coolwarm, linewidth=0., antialiased=True) ax.view_init(35, 15+0.5*t_i) ax.set_zlim(zmin, zmax) return surf, #     anim = FuncAnimation(fig, update, frames=range(len(z2s)), interval=50) anim.save(fname, dpi=80, writer='imagemagick')</span></span></code> </pre><br></div></div><br>  2D gif drawing function: <br><br><div class="spoiler">  <b class="spoiler_title">2d gif</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_2d_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(U, fname, step=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> fig = plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">7</span></span>)) zmax = np.max(np.max(U)) + <span class="hljs-number"><span class="hljs-number">0.01</span></span> zmin = np.min(np.min(U)) - <span class="hljs-number"><span class="hljs-number">0.01</span></span> norm = matplotlib.colors.Normalize(vmin=zmin, vmax=zmax, clip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im=plt.imshow(U[:,:,<span class="hljs-number"><span class="hljs-number">0</span></span>], interpolation=<span class="hljs-string"><span class="hljs-string">'bilinear'</span></span>, cmap=cm.coolwarm, norm=norm) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) nst = U.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>] // step <span class="hljs-comment"><span class="hljs-comment">#       def update(i): im.set_array(U[:,:,i*step]) return im #     anim = FuncAnimation(fig, update, frames=range(nst), interval=50) anim.save(fname, dpi=80, writer='imagemagick')</span></span></code> </pre><br></div></div><br><h2>  Total </h2><br>  It should be noted that the original condition without using a GPU was considered 4m 26s, and using a GPU 2m 11s.  For large points, the gap grows.  However, not all operations in the resulting graph are GPU-compatible. <br><br>  Machine Specifications: <br><br><ul><li>  Intel Core i7 6700HQ 2600 MHz, <br></li><li>  NVIDIA GeForce GTX 960M. <br></li></ul><br>  You can see what operations are performed on what with the following code: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    def check_metadata_partitions_graph(self): g = self.g d = self._get_graph_feed(1) with tf.Session() as sess: sess.run(tf.global_variables_initializer(), feed_dict=d) options = tf.RunOptions(output_partition_graphs=True) metadata = tf.RunMetadata() c_val = sess.run(g["train_step"], feed_dict=d, options=options, run_metadata=metadata) print(metadata.partition_graphs)</span></span></code> </pre><br></div></div><br>  It was an interesting experience.  Tensorflow showed itself well for this task.  Maybe even this approach will get some kind of application - it is more pleasant to write code on python than on C / C ++, and with the development of tensorflow it will become even easier. <br><br>  Thanks for attention! <br><br><h2>  References </h2><br>  - Bakhvalov N. S., Zhidkov N. P., G. M. Kobelkov <em>Numerical Methods</em> , 2011 </div><p>Source: <a href="https://habr.com/ru/post/321734/">https://habr.com/ru/post/321734/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321720/index.html">How to prokrastirovat effectively: 6 useful online services</a></li>
<li><a href="../321722/index.html">How to stop guessing and start counting</a></li>
<li><a href="../321724/index.html">You are the proton of my electron: love and marriage in IT</a></li>
<li><a href="../321726/index.html">Corporate wifi on UBNT with portal and domain authentication</a></li>
<li><a href="../321728/index.html">Crash course on UI design</a></li>
<li><a href="../321736/index.html">How we sequenced the hackathons or ‚Äúthe harder the work, the easier it is to settle on it‚Äù</a></li>
<li><a href="../321738/index.html">OONP problem: there is no clear and obligatory core object-oriented modeling</a></li>
<li><a href="../321744/index.html">As I made the fastest resize of images. Part 0</a></li>
<li><a href="../321746/index.html">jQuery UI FadeSlide 4.0</a></li>
<li><a href="../321748/index.html">JavaScript start performance</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>