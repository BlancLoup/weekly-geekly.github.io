<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>NewSQL = NoSQL + ACID</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Until recently, about 50 TB of data processed in real time were stored in SQL Server in Odnoklassniki. For such a volume, it is almost impossible to p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>NewSQL = NoSQL + ACID</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/t5/hc/h2/t5hch2mr-lw9ffeahr_9gaxizlg.jpeg" width="600"></div><br>  Until recently, about 50 TB of data processed in real time were stored in SQL Server in Odnoklassniki.  For such a volume, it is almost impossible to provide fast and reliable, and even fault tolerant to the data center access using SQL DBMS.  Usually in such cases one of the NoSQL-storages is used, but not everything can be transferred to NoSQL: some entities require ACID transaction guarantees. <br><br>  This brought us to the use of NewSQL-storage, that is, a DBMS that provides fault tolerance, scalability and performance of NoSQL-systems, but at the same time retains the traditional ACID-guarantee.  There are few operating industrial systems of this class, therefore we implemented such a system ourselves and launched it into commercial operation. <br><br>  How it works and what happened - read under the cut. <br><a name="habracut"></a><br>  Today, Odnoklassniki‚Äôs monthly audience amounts to more than 70 million unique visitors.  We are <a href="https://www.similarweb.com/top-websites/category/internet-and-telecom/social-network">among the top five</a> largest social networks in the world, and in the top twenty sites where users spend the most time.  The ‚ÄúOK‚Äù infrastructure handles very high loads: over a million HTTP requests / sec to the fronts.  Parts of the fleet of servers in the amount of more than 8,000 units are located close to each other - in four Moscow data centers, which allows for a network delay of less than 1 ms between them. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We have been using Cassandra since 2010, starting with version 0.6.  Today there are several dozen clusters in operation.  The fastest cluster processes more than 4 million operations per second, and the largest stores 260 TB. <br><br>  However, all this is the usual NoSQL-clusters used to store <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BE%25D0%25B3%25D0%25BB%25D0%25B0%25D1%2581%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C_%25D0%25B2_%25D0%25BA%25D0%25BE%25D0%25BD%25D0%25B5%25D1%2587%25D0%25BD%25D0%25BE%25D0%25BC_%25D1%2581%25D1%2587%25D1%2591%25D1%2582%25D0%25B5">poorly consistent</a> data.  We also wanted to replace the main consistent storage, Microsoft SQL Server, which has been used since the founding of Odnoklassniki.  The repository consisted of more than 300 SQL Server Standard Edition machines that contained 50 TB of data ‚Äî business entities.  These data are modified as part of ACID transactions and require <a href="https://ru.wikipedia.org/wiki/ACID">high consistency</a> . <br><br>  To distribute data among SQL Server nodes, we used both vertical and horizontal <a href="https://en.wikipedia.org/wiki/Partition_(database)">partitioning</a> (sharding).  Historically, we used a simple data sharding scheme: each entity was associated with a token - a function of the entity ID.  Entities with the same token were placed on a single SQL server.  The master-detail type relationship was implemented so that the tokens of the main and child records always coincide and reside on the same server.  In a social network, almost all entries are generated on behalf of the user, which means that all user data within the same functional subsystem is stored on the same server.  That is, tables of a single SQL server were almost always involved in a business transaction, which made it possible to ensure data consistency using local ACID transactions, without the need for <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">slow and unreliable</a> distributed ACID transactions. <br><br>  Thanks to sharding and to speed up SQL: <br><br><ul><li>  We do not use Foreign key constraints, as when sharding an entity ID it may be located on another server. </li><li>  We do not use stored procedures and triggers due to the additional load on the DBMS CPU. </li><li>  We do not use JOINs because of all the above and many random reads from the disk. </li><li>  Outside the transaction, we use the Read Uncommitted isolation level to reduce deadlocks. </li><li>  We perform only short transactions (on average shorter than 100 ms). </li><li>  We do not use multi-row UPDATE and DELETE due to the large number of deadlocks - we update only one record. </li><li>  Inquiries are always performed only by indexes - a query with a full table view plan for us means overloading the database and its failure. </li></ul><br>  These steps made it possible to squeeze almost the maximum performance out of SQL servers.  However, the problems became more and more.  Let's consider them. <br><br><h2>  SQL issues </h2><br><ul><li>  Since we used samopisny sharding, adding new shards was done by administrators manually.  All this time, scalable data replicas have not served requests. </li><li>  As the number of records in the table grows, the insertion and modification speed decreases, when adding indexes to an existing table, the speed drops multiple, the creation and re-creation of indexes comes with downtime. </li><li>  Having a small number of Windows for SQL Server production complicates infrastructure management </li></ul><br>  But the main problem is <br><br><h2>  fault tolerance </h2><br>  The classic SQL server has poor resiliency.  Suppose you have only one database server, and it fails every three years.  At this time, the site does not work for 20 minutes, this is acceptable.  If you have 64 servers, the site does not work once every three weeks.  And if you have 200 servers, the site does not work every week.  This is problem. <br><br>  What can be done to improve SQL server resiliency?  Wikipedia offers us to build a <a href="https://en.wikipedia.org/wiki/High-availability_cluster">highly accessible cluster</a> : where in case of failure of any of the components there is a duplicate. <br><br>  This requires a fleet of expensive equipment: numerous redundancy, optical fiber, shared storage, and the inclusion of the reserve works unreliably: about 10% of the inclusions result in the failure of the backup node by the engine behind the main node. <br><br>  But the main drawback of such a highly accessible cluster is zero availability in case of failure of the data center in which it is located.  Odnoklassniki has four data centers, and we need to ensure work in case of a complete accident in one of them. <br><br>  To this end, <a href="https://technet.microsoft.com/en-us/library/ms151196.aspx">Multi-Master</a> replication built into SQL Server could be applied.  This solution is much more expensive due to the cost of software and suffers from well-known replication problems ‚Äî unpredictable transaction delays with synchronous replication and delays in the use of replications (and, as a result, lost modifications) with asynchronous.  The implied <a href="https://docs.microsoft.com/en-us/sql/relational-databases/replication/transactional/peer-to-peer-conflict-detection-in-peer-to-peer-replication">manual resolution of conflicts</a> makes this option completely inapplicable to us. <br><br>  All these problems required a cardinal solution and we began to analyze them in detail.  Here we need to get acquainted with what SQL Server basically does - transactions. <br><br><h2>  Simple transaction </h2><br>  Consider the simplest transaction from the point of view of an applied SQL programmer: adding a photo to an album.  Albums and photos are stored in different tablets.  The album has a count of public photos.  Then such a transaction is divided into the following steps: <br><br><ol><li>  Lock the album by key. </li><li>  Create an entry in the photo table. </li><li>  If a photo has a public status, then we wind up a counter of public photos in the album, update the record and commit the transaction. </li></ol><br>  Or in the form of pseudocode: <br><br><pre><code class="hljs pgsql">TX.<span class="hljs-keyword"><span class="hljs-keyword">start</span></span>("Albums", id); Album album = albums.<span class="hljs-keyword"><span class="hljs-keyword">lock</span></span>(id); Photo photo = photos.<span class="hljs-keyword"><span class="hljs-keyword">create</span></span>(‚Ä¶); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (photo.status == <span class="hljs-built_in"><span class="hljs-built_in">PUBLIC</span></span> ) { album.incPublicPhotosCount(); } album.<span class="hljs-keyword"><span class="hljs-keyword">update</span></span>(); TX.<span class="hljs-keyword"><span class="hljs-keyword">commit</span></span>();</code> </pre> <br>  We see that the most common business transaction scenario is to read data from the database into the memory of the application server, change something, and save the new values ‚Äã‚Äãback to the database.  Usually in such a transaction we update several entities, several tables. <br><br>  When performing a transaction, competitive modification of the same data from another system may occur.  For example, Antispam may decide that a user is suspicious and therefore all user photos should no longer be public, they need to be sent for moderation, which means changing photo.status to some other value and unscrewing the corresponding counters.  Obviously, if this operation takes place without guarantees of atomicity of application and isolation of competing modifications, as in <a href="https://ru.wikipedia.org/wiki/ACID">ACID</a> , the result will not be what is necessary - or the photo counter will show the wrong value, or not all photos will be sent for moderation. <br><br>  There is a lot written about such code, which manipulates various business entities in the framework of a single transaction, during the entire existence of Odnoklassniki.  From the experience of migrations to NoSQL with <a href="https://en.wikipedia.org/wiki/Eventual_consistency">Eventual Consistency,</a> we know that the greatest difficulties (and time costs) make it necessary to develop code aimed at maintaining consistency of data.  Therefore, we considered the provision of real ACID transactions for the application logic to be the main requirement for the new storage. <br><br>  Other equally important requirements were: <br><br><ul><li>  If the data center fails, both reading and writing to the new storage should be available. </li><li>  Maintain current development speed.  That is, when working with a new repository, the amount of code should be approximately the same, there should be no need to add something to the repository, develop conflict resolution algorithms, maintain secondary indexes, etc. </li><li>  The speed of the new storage should be sufficiently high, both when reading data and processing transactions, which effectively meant the inapplicability of academically rigorous, universal, but slow solutions, such as <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-phase commits</a> . </li><li>  Automatic scaling on the fly. </li><li>  Use of ordinary cheap servers, without the need to buy exotic pieces of iron. </li><li>  The possibility of developing storage by the developers of the company.  In other words, priority was given to its or open source-based solutions, preferably in Java. </li></ul><br><h2>  Decisions </h2><br>  Analyzing possible solutions, we came to two possible choices of architecture: <br><br>  The first is to take any SQL server and implement the required fault tolerance, scaling mechanism, failover cluster, conflict resolution, and distributed, reliable, and fast ACID transactions.  We appreciated this option as very non-trivial and time consuming. <br><br>  The second option is to take ready NoSQL storage with implemented scaling, failover cluster, conflict resolution and implement transactions and SQL by yourself.  At first glance, even the task of implementing SQL, not to mention the ACID transaction, looks like a year task.  But then we realized that the set of SQL capabilities that we use in practice is far from ANSI SQL as far as <a href="https://en.wikipedia.org/wiki/Apache_Cassandra">Cassandra CQL is</a> far from ANSI SQL.  Looking more closely at CQL, we realized that it was close enough to what we needed. <br><br><h2>  Cassandra and CQL </h2><br>  So, what makes Cassandra interesting, what capabilities does it have? <br><br>  First, here you can create tables with support for various data types, you can make a SELECT or UPDATE by the primary key. <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> photos (<span class="hljs-keyword"><span class="hljs-keyword">id</span></span> <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span> <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>, owner <span class="hljs-built_in"><span class="hljs-built_in">bigint</span></span>,‚Ä¶); <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> photos <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">id</span></span>=?; <span class="hljs-keyword"><span class="hljs-keyword">UPDATE</span></span> photos <span class="hljs-keyword"><span class="hljs-keyword">SET</span></span> ‚Ä¶ <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">id</span></span>=?;</code> </pre> <br>  To ensure consistency of replica data, Cassandra uses a <a href="https://en.wikipedia.org/wiki/Quorum_(distributed_computing)">quorum approach</a> .  In the simplest case, this means that when placing three replicas of the same row on different nodes of the cluster, the recording is considered successful if most of the nodes (ie, two of the three) have confirmed the success of this write operation.  The data of the series is considered consistent if, while reading, the majority of the nodes were interrogated and confirmed.  Thus, if there are three replicas, complete and instant data consistency is guaranteed in case of failure of one node.  This approach allowed us to implement an even more reliable scheme: always send requests for all three replicas, waiting for a response from the two fastest.  The late response of the third replica in this case is discarded.  A node that was late with the response can have serious problems - brakes, garbage collection in JVM, direct memory reclaim in linux kernel, hardware failure, disconnection from the network.  However, this does not affect the client‚Äôs operations and data. <br><br>  The approach, when we turn to three nodes, and get a response from two, is called <a href="https://en.wikipedia.org/wiki/Speculative_execution">speculation</a> : a request for extra replicas is sent before it ‚Äúfalls off‚Äù. <br><br>  Another advantage of Cassandra is Batchlog - a mechanism that guarantees either full use or complete non-use of the package of changes you make.  This allows us to solve A in ACID - atomicity out of the box. <br><br>  Closest to transactions in Cassandra are the so-called " <a href="https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html">lightweight transactions</a> ".  But they are far from ‚Äúreal‚Äù ACID transactions: in fact, it is possible to make <a href="https://en.wikipedia.org/wiki/Compare-and-swap">CAS</a> on the data of only one record, using the consensus on the Paxos heavy protocol.  Therefore, the speed of such transactions is low. <br><br><h2>  What we lacked in Cassandra </h2><br>  So, we had to implement real ACID transactions in Cassandra.  Using which we could easily implement two other convenient features of classic DBMS: consistent fast indexes, which would allow us to perform data sampling not only on the primary key and the usual monotone auto-increment ID generator. <br><br><h4>  C * One </h4><br>  Thus was born the new <b>C * One</b> DBMS, consisting of three types of server nodes: <br><br><ul><li>  Storage - (almost) standard servers Cassandra, responsible for storing data on local drives.  As the load and data volume grows, their number can be easily scaled to tens and hundreds. </li><li>  Transaction Coordinators - ensure the execution of transactions. </li><li>  Clients are application servers that implement business operations and initiate transactions.  There may be thousands of such customers. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/eb6/f4f/498/eb6f4f4983f44dcbe63cd545e87f8d1a.png"><br><br>  Servers of all types are in a common cluster, use the internal Cassandra message protocol to communicate with each other and <a href="https://en.wikipedia.org/wiki/Gossip_protocol">gossip</a> to exchange cluster information.  With the help of Heartbeat, servers learn about mutual failures, support a single data scheme ‚Äî tables, their structure and replication;  partition scheme, cluster topology, etc. <br><br><h4>  Customers </h4><br><img src="https://habrastorage.org/getpro/habr/post_images/b83/9ff/971/b839ff971b0049d56d55bd309f44ae4e.png"><br><br>  Fat lient mode is used instead of standard drivers.  Such a node does not store data, but can act as a coordinator for the execution of requests, that is, the Client himself acts as a coordinator for his requests: he polls the replicas of the repository and resolves conflicts.  This is not only safer and faster than the standard driver, which requires communication with the remote coordinator, but also allows you to control the transmission of requests.  Outside the transaction opened on the client, requests are sent to the repositories.  If the client has opened a transaction, then all requests within the transaction are sent to the transaction coordinator. <br><img src="https://habrastorage.org/getpro/habr/post_images/d39/d43/483/d39d43483590319f4d49e41a25316058.png"><br><br><h2>  C * One Transaction Coordinator </h2><br>  The coordinator is what we implemented for C * One from scratch.  He is responsible for managing transactions, locks, and the order in which transactions are applied. <br><br>  For each accepted transaction, the coordinator generates a timestamp: each subsequent one is larger than the previous transaction.  Since in Cassandra the conflict resolution system is based on timestamps (out of two conflicting entries, the current one is considered to be with the latest timestamp), the conflict will always be resolved in favor of the subsequent transaction.  Thus, we implemented <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B0%25D1%2581%25D1%258B_%25D0%259B%25D1%258D%25D0%25BC%25D0%25BF%25D0%25BE%25D1%2580%25D1%2582%25D0%25B0">a Lamport watch</a> - a cheap way to resolve conflicts in a distributed system. <br><br><h2>  Locks </h2><br>  To ensure isolation, we decided to use the easiest way - pessimistic locks on the primary key of the record.  In other words, in a transaction, you must first block the record, only then read, modify and save.  Only after a successful commit can a record be unlocked so that competing transactions can use it. <br><br>  The implementation of such a lock is simple in an unallocated environment.  In a distributed system, there are two main ways: either to implement distributed locking on a cluster, or to distribute transactions so that transactions with the same record are always serviced by the same coordinator. <br><br>  Since, in our case, the data is already distributed into local transaction groups in SQL, it was decided to assign the local transaction groups to the coordinators: one coordinator performs all transactions with a token from 0 to 9, the second with a token from 10 to 19, and so on.  As a result, each of the instances of the coordinator becomes a transaction group master. <br><br>  Then locks can be implemented as a banal HashMap in the memory of the coordinator. <br><br><h2>  Failures of coordinators </h2><br>  Since one coordinator exclusively serves a group of transactions, it is very important to quickly determine that he was rejected so that a retry of execution of the transaction will be timed out.  To make it fast and reliable, we used a fully connected quorum hearbeat protocol: <br><br>  Each data center hosts at least two nodes of the coordinator.  Periodically, each coordinator sends a heartbeat message to the other coordinators and informs them about his functioning, as well as about the heartbeat messages from which coordinators in the cluster he received last time. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cde/1e4/ccd/cde1e4ccd6d23620eda08adc231aeec7.jpg"><br><br>  Receiving similar information from the rest of their heartbeat messages, each coordinator decides for himself which cluster nodes are functioning and which are not, guided by the quorum principle: if node X received information from the majority of nodes in the cluster about normal receiving messages from node Y, then , Y is working.  And vice versa, as soon as the majority reports about the loss of messages from node Y, it means that Y has failed.  It is curious that if a quorum informs node X that it does not receive more messages from it, then node X itself will consider itself refused. <br><br>  Heartbeat messages are sent with a high frequency, about 20 times per second, with a period of 50 ms.  In Java, it is difficult to guarantee an application response for 50 ms due to the comparable length of pauses caused by the garbage collector.  We managed to achieve such a response time using the G1 garbage collector, which allows you to specify a target for the duration of GC pauses.  However, sometimes, quite rarely, the pause of the collector goes beyond 50 ms, which can lead to a false failure detection.  To prevent this from happening, the coordinator does not report the failure of the remote node when the first heartbeat message disappears from it only if it disappears several consecutively. So we managed to detect the failure of the coordinator node within 200 ms. <br><br>  But it is not enough to quickly understand which node has ceased to function.  Need something to do with it. <br><br><h2>  Reservation </h2><br>  The classical scheme assumes, in the event of a master's failure, to launch a new election using one of the <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_Raft">trendy</a> <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%259F%25D0%25B0%25D0%25BA%25D1%2581%25D0%25BE%25D1%2581">universal</a> algorithms.  However, such algorithms have well-known problems with the convergence in time and duration of the election process itself.  We managed to avoid such additional delays using the equivalent circuit of coordinators in a fully meshed network: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e90/45e/007/e9045e00777362a5846eb78ef15bedea.png"><br><br>  Suppose we want to perform a transaction in group 50. We will define the replacement scheme in advance, that is, which nodes will execute group 50 transactions in the event of a failure of the main coordinator.  Our goal is to keep the system working in case of a data center failure.  We define that the first reserve will be a node from another data center, and the second reserve will be a node from the third.  This scheme is selected once and does not change until the cluster topology changes, that is, until new nodes enter it (which happens very rarely).  The procedure for selecting a new active master if the old one fails will always be like this: the first reserve will become the active master, and if it also ceases to function, the second reserve will become. <br><br>  Such a scheme is more reliable than the universal algorithm, since to activate the new wizard, it is enough to determine if the old one has failed. <br><br>  But how will clients understand which of the masters is working now?  For 50 ms, it is impossible to send information to thousands of clients.  It is possible that the client sends a request to open a transaction, not knowing that this wizard is no longer functioning, and the request will hang on a timeout.  To prevent this from happening, clients speculatively send a request to open a transaction immediately to the group master and both of his reserves, but only the one who is currently the active master is responding to this request.  All subsequent communication within the framework of the transaction the client will produce only with the active master. <br><br>  Backup masters receive requests for non-own transactions in a queue of unborn transactions, where they are stored for some time.  If the active master dies, the new master processes requests to open transactions from its turn and responds to the client.  If the client has already managed to open a transaction with the old master, then the second answer is ignored (and, obviously, such a transaction will not end and will be repeated by the client). <br><br><h2>  How a transaction works </h2><br>  Suppose the client sent a request to the coordinator to open a transaction for such and such an entity with such and such a primary key.  The coordinator blocks this entity and places it in the table of locks in memory.  If necessary, the coordinator reads this entity from the repository and stores the received data in the transaction state in the coordinator's memory. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4d1/dfb/dcd/4d1dfbdcd92e4b05ca49ef5168177eb8.png"><br><br>  When a client wants to change data in a transaction, he sends a request to modify the entity to the coordinator, who then places the new data in the transaction status table in memory.  At this point, the recording is completed - no entry is made to the storage. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/00e/803/570/00e803570a0fbe19cb76786cbc6f0142.png"><br><br>  When a client requests its own changed data as part of an active transaction, the coordinator acts like this: <br><br><ul><li>  if the ID is already in the transaction, then the data is taken from memory; </li><li>  if the ID is not in memory, then the missing data is read from the node-storages, combined with those already in memory, and the result is given to the client. </li></ul><br>  Thus, the client can read his own changes, and other clients do not see these changes, because they are stored only in the coordinator's memory, they are not yet in the Cassandra nodes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8e/140/5f7/d8e1405f7995228ec3d061af59f1f685.png"><br><br>  When the client sends a commit, the state stored in the memory of the service is saved by the coordinator at the logged batch, and already in the form of a logged batch is sent to the Cassandra repositories.  The repositories do everything necessary to ensure that this package is atomically (fully) applied, and returns a response to the coordinator, which releases the locks and confirms the success of the transaction to the client. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/090/739/9fc/0907399fc025879f16174ff26163a937.png"><br><br>  And for a rollback, the coordinator need only free up the memory occupied by the state of the transaction. <br><br>  As a result of the above improvements, we implemented the principles of ACID: <br><br><ul><li>  <b>Atomicity</b>  This is a guarantee that no transaction will be fixed in the system partially, either all its suboperations will be executed, or none will be executed.  We have this principle respected by logged batch in Cassandra. </li><li>  <b>Consistency</b>  Each successful transaction by definition captures only valid results.  If, after opening a transaction and performing part of the operation, it is found that the result is invalid, it is rolled back. </li><li>  <b>Isolation</b>  When performing a transaction, parallel transactions should not affect its result.  Competing transactions are isolated using pessimistic locks on the coordinator.  For readings out of transaction, the principle of isolation at the Read Committed level is observed. </li><li>  <b>Sustainability</b> .  Regardless of the problems at the lower levels - system de-energization, equipment failure, - changes made by a successfully completed transaction should remain saved after the resumption of operation. </li></ul><br><h2>  Reading by Index </h2><br>  Take a simple table: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> photos ( id <span class="hljs-type"><span class="hljs-type">bigint</span></span> <span class="hljs-keyword"><span class="hljs-keyword">primary key</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">owner</span></span> <span class="hljs-type"><span class="hljs-type">bigint</span></span>, modified <span class="hljs-type"><span class="hljs-type">timestamp</span></span>, ‚Ä¶)</code> </pre> <br>  She has the ID (primary key), the owner and the date of the change.  It is necessary to make a very simple request - select data by owner with the date of change ‚Äúfor the last 24 hours‚Äù. <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> owner=? <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> modified&gt;?</code> </pre> <br>  In order to process such a query quickly, it is necessary to build an index in columns (owner, modified) in a classical SQL DBMS.  We can do this quite simply, as we now have ACID guarantees! <br><br><h2>  Indices in C * One </h2><br>  There is a source table with photos in which the record ID is the primary key. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/039/5b3/a78/0395b3a784329992b88ba0618a94a7f1.jpg"><br><br>  For the index, C * One creates a new table that is a copy of the original one.  The key coincides with the index expression, while it also includes the primary key of the record from the source table: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/92a/8c2/93e/92a8c293e8c145dc37f49a5126e68095.jpg"><br><br>  Now the query by ‚Äúowner in the last 24 hours‚Äù can be rewritten as a select from another table: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> i1_test <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> owner=? <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> modified&gt;?</code> </pre> <br>  The consistency of the data in the source table photos and the index i1 is maintained by the coordinator automatically.  Based on the data scheme alone, upon receipt of a change, the coordinator generates and remembers the change not only of the main table, but also changes of copies.  No additional actions are performed on the index table, logs are not read, and locks are not used.  That is, adding indexes almost does not consume resources and practically does not affect the speed of application of modifications. <br><br>  With the help of ACID, we were able to implement indices ‚Äúas in SQL‚Äù.  They have consistency, can scale, work quickly, can be composite, and are built into the CQL query language.  To support indexes, you do not need to make changes to the application code.  Everything is simple, as in SQL.    ,          . <br><br><h2>  What happened </h2><br>   C*One        . <br><br>      ?          ,        .      ,    .   ¬´¬ª  20   ,   80 .     ,  8 . ACID-  ,    . <br><br>    SQL  replication factor = 1 (  RAID 10),        32   Microsoft SQL Server ( 11 ).    10    .  50  .       ,  . <br><br>        replication factor = 3 ‚Äî     -.    63   Cassandra  6  ,  69 .     ,      30 %    SQL.       30 %. <br><br>   C*One   :  SQL     4,5 .  C*One ‚Äî  1,6 .      40 ,    2 ,     ‚Äî   2 . 99-  ‚Äî  3-3,1 ,     100  ‚Äî      . <br><br>          SQL Server,     c  C*One.   C*One      <a href="https://habr.com/company/odnoklassniki/blog/346868/">one-cloud</a> ,      ,     .           . <br><br>           ‚Äî      . </div><p>Source: <a href="https://habr.com/ru/post/417593/">https://habr.com/ru/post/417593/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417583/index.html">Tomorrow ICFP Contest 2018, hooray! (+ useful for participating for the first time)</a></li>
<li><a href="../417585/index.html">How to get into the Program Committee of the classroom conference, and why it is needed</a></li>
<li><a href="../417587/index.html">Mass media: large-scale cyber attacks accelerated the growth of capitalization of companies from the information security industry</a></li>
<li><a href="../417589/index.html">Seven simple rules to make the Internet accessible to all.</a></li>
<li><a href="../417591/index.html">How to "learn" English in one year alone or an article for those who did not work out with English</a></li>
<li><a href="../417595/index.html">Antiquities: Palm OS, efficient code and ugly photos</a></li>
<li><a href="../417597/index.html">Secure storage with DRBD9 and Proxmox (Part 2: iSCSI + LVM)</a></li>
<li><a href="../417599/index.html">Fintech Digest: financial regulators need AI in order to work in modern conditions</a></li>
<li><a href="../417601/index.html">Choose a server. What to look for? Check list</a></li>
<li><a href="../417603/index.html">Announcement of mobile mitap: What to do when the application has become large?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>