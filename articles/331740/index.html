<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How HBO did Not Hotdog for the Silicon Valley TV Show</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The HBO series "Silicon Valley" released this AI application that recognizes hotdogs and non-hotdogs as an app in the fourth episode of the fourth sea...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How HBO did Not Hotdog for the Silicon Valley TV Show</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/600/999/5e2/6009995e23b940fd82c8d4265d5a7ca0.jpg"><br><br>  <a href="http://www.hbo.com/silicon-valley">The HBO series "Silicon Valley"</a> released this AI application that recognizes hotdogs and non-hotdogs as an app in the fourth episode of the fourth season (the app is <a href="https://www.seefoodtechnologies.com/nothotdog/">now available for Android, as well as for iOS</a> !) <br><br>  To achieve this, we developed a special neural architecture that works directly on your phone, and trained it using TensorFlow, Keras and Nvidia GPU. <br><a name="habracut"></a><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="874453786390966272"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Although the practical benefits of it are ridiculous, the application is an accessible example of both deep learning and boundary computing.  All AI work is 100% provided by the user device, and images are processed without leaving the phone.  It gives instant response (no need to share data with the cloud), offline accessibility and better privacy.  It also allows us to keep the application running at a price of $ 0, even with millions of users, which represents a significant savings compared to traditional cloud-based approaches to AI. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/ebf/afd/fd2/ebfafdfd2b711962b50e7db2444af233.jpg"><br>  <font color="gray">Developer's computer with eGPU connected for teaching AI Not Hotdog applications</font> <br><br>  The application was developed by the studio specifically for the series by one developer, on a single laptop with a connected GPU, using manually selected data.  In this regard, it can serve as a demonstration of what can be achieved today with limited time and resources, a non-technical company, individual developers and similar amateurs.  In this spirit, this article attempts to provide a detailed overview of the steps that others can repeat to create their own applications. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="872756283417513985"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  application </h1><br><iframe width="560" height="315" src="https://www.youtube.com/embed/ACmydtFDTGs" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  If you have not watched the show and have not experienced the <a href="https://www.seefoodtechnologies.com/nothotdog/">application</a> (and it should have been!), It takes a photo, and then gives you its opinion, is the hot dog shown in the photo or not.  This is a straightforward use that pays tribute to recent research in the field of AI and applications, in particular, ImageNet. <br><br>  Although we probably allocated more programmer resources to recognize hot dogs than anyone in the world, the application still sometimes makes mistakes in a terrible and / or subtle way. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="865093285886304256"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  And vice versa, at times it is able to recognize hotdogs in difficult situations ... <a href="https://www.engadget.com/2017/05/15/not-hotdog-app-hbo-silicon-valley/">As Engadget writes</a> , <i>‚ÄúThis is incredible.</i>  <i>In 20 minutes, I successfully recognized more food with this app than I had tagged and recognized songs with Shazam over the past two years. ‚Äù</i> <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="865662674864005120"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><h1>  From prototype to production </h1><br>  Ever reading Hacker News, did you catch yourself thinking: <i>‚ÄúThey raised $ 10 million in the first round for it?</i>  <i>I can develop this over the weekend! ‚Äù</i> This app is likely to make you feel the same way.  In the end, the original prototype was really created over the weekend, using the Google Cloud Platform Vision API and React Native.  But to create the final version of the application, which eventually entered the application catalog, it took months of additional work (several hours a day) to make meaningful improvements that are difficult to assess from the outside.  We spent weeks optimizing overall accuracy, learning time, analysis time, we tried different installations and tools to speed development, and spent the whole weekend optimizing the user interface with iOS and Android permissions (don't even start talking about this topic). <br><br>  All too often, technical blogs and scientific articles skip this part, preferring to immediately show the final version.  But to help others learn from our mistakes and actions, we will present a short list of approaches that did not work for us before we describe the final architecture, which we finally arrived at. <br><br><h3>  V0: Prototype </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/fea/1df/e68/fea1dfe681ac2ec83b14587458f3a50e.png"><br>  <font color="gray">An example of the image and the corresponding API issuance from the Google Cloud Vision documentation</font> <br><br>  To create a prototype, we chose React Native, because it provides a simple sandbox for experiments and helps to quickly provide support for many devices.  The experience was successful, and we left React Native to the end of the project: it did not always simplify the process, and the design of the application had to be purposefully limited, but in the end React Native did its job. <br><br>  We quickly abandoned another main component used for the prototype - <a href="https://cloud.google.com/vision/">Google Cloud Vision API</a> .  There are three main reasons: <br><br><ol><li>  The first, and most important, is that the accuracy of recognition of hot dogs was not very good.  Although he copes with the recognition of a variety of objects, but does not very well recognize one particular thing, and there were various examples of a rather general nature, where the service performed poorly during our experiments in 2016. </li><li>  By nature, the cloud service will always be slower than the native execution on the device (network lag hurts!), And does not work offline.  The idea of ‚Äã‚Äãtransferring images outside the device potentially has legal and privacy implications. </li><li>  In the end, if the application becomes popular, then work in the Google Cloud can fly into a pretty penny. </li></ol><br>  For this reason, we started experimenting with what is fashionable to call edge computing (‚Äúedge computing‚Äù).  In our case, this means that after learning our neural network on the laptop, we will transfer it directly to the mobile application, so that the phase of the neural network execution (or output of the conclusion) will be performed directly on the user's phone. <br><br>
<h3>  V1: TensorFlow, Inception and Retraining </h3><br><iframe width="560" height="315" src="https://www.youtube.com/embed/_bkZPpniYdo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  Thanks to the happy meeting with <a href="https://petewarden.com/">Pete Varden</a> from the TensorFlow development team, we learned that you can run TensorFlow directly by embedding it in an iOS device, and started experimenting in this direction.  After React Native, TensorFlow became the second integral part of our stack. <br><br>  It took just one day to integrate the Objective-C ++ TensorFlow's sample camera into our React Native shell.  It took much longer to master a learning script that helps to retrain the Inception architecture to work with more specific tasks of machine vision.  Inception is the name of a family of neural architectures created by Google for image recognition.  Inception is available as a pre-trained system, that is, the learning phase is complete and the weights are set.  Most often, image recognition neural networks are trained on ImageNet, an annual competition to find the best neural architecture, which recognizes more than 20,000 different types of objects (and hot dogs among them).  However, just like the Google Cloud Vision API, the competition encourages the largest possible number of recognizable objects, and the initial accuracy for one particular object out of more than 20,000 is not very high.  For this reason, retraining (also called ‚Äútransfer learning‚Äù) aims to take a fully trained neural network and retrain it to better accomplish a specific task that you are working with.  This usually involves some degree of ‚Äúforgetting‚Äù either through excision of entire layers from the stack, or slowly erasing the ability of the neural network to distinguish between certain types of objects (for example, chairs) for the sake of greater accuracy in recognizing the object you need (for example, hotdogs). <br><br>  Although the neural network (in this case, Inception) could be trained on 14 million ImageNet images, we were able to retrain it on just a few thousand hotdog photos to drastically improve the recognition of hotdogs. <br><br>  The great advantage of transferring training is that you get better results much faster and with less data than you would if you trained a neural network from scratch.  Full training could take several months on numerous GPUs and would require millions of images, while retraining can presumably be spent in a few hours on a laptop with a couple of thousand photos. <br><br>  One of the most difficult tasks we faced was the exact definition of what is considered a hot dog and what is not.  The definition of a ‚Äúhot dog‚Äù turned out to be surprisingly difficult (are cut sausages considered, and if so, which species?) And subject to cultural interpretation. <br><br>  Similarly, the nature of the ‚Äúopen world‚Äù of our problem meant that we would have to deal with a virtually infinite amount of input data.  Some computer vision tasks deal with a relatively limited set of input data (for example, x-rays of bolts with mechanical defects or without defects), but we had to prepare an application for processing selfies, images of nature and a wide variety of dishes. <br><br>  Suffice it to say that this approach was promising and led to some improvement in the results, but it had to be abandoned for a number of reasons. <br><br>  First, the nature of our task meant a strong imbalance in the data for training: there are many more examples of what is not hotdogs than the hotdogs themselves.  In practice, this means that if you train your algorithm on three images of hotdogs and 97 images that are not hotdogs, and it recognizes 0% of the first and 100% of the second, then you get a nominal accuracy of 97%!  This problem is not solved directly by the TensorFlow retraining tool and essentially forces you to establish a model of depth learning from scratch, import weights and conduct training in a more controlled way. <br><br>  At this stage, we decided to bite the bullet and start working with Keras, a deep learning library that provides better, easier-to-use abstractions over TensorFlow, including quite cool learning tools, as well as the class_weights option, which is ideal for solving this type of problem. with unbalanced dataset like ours. <br><br>  We used this opportunity to test other neural architectures, such as VGG, but one problem remained.  None of them provided a comfortable work on the iPhone.  They consumed too much memory, which led to application crashes, and sometimes it took up to 10 seconds to produce a result, which is not ideal in terms of UX.  We tried a lot to solve the problem, but in the end we recognized that these architectures are too cumbersome to work on a mobile device. <br><br><h3>  V2: Keras and SqueezeNet </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/827/b62/768/827b627684cc36051d860313839501dc.png"><br>  <font color="gray">SqueezeNet vs. AlexNet, the grandfather of computer vision architectures.</font>  <font color="gray">Source: <a href="https://arxiv.org/abs/1602.07360">SqueezeNet science article</a></font> <br><br>  <i>To give you the context of where we are, this is about halfway through the project's development history.</i>  <i>By this time, the UI was ready for more than 90%, leaving very little to change.</i>  <i>But now it is clear that the neural network was ready at best by 20%.</i>  <i>We had a good understanding of the problems and a good data set, but 0 lines of the ready neural architecture code were written, no our code could work reliably on a mobile phone, and even the accuracy would subsequently be drastically improved.</i> <br><br>  The problem immediately confronting us was simple: if Inception and VGG are too bulky, is there a simpler, pre-trained neural network that we can retrain?  On a tip from the always great Jeremy Howard (where has this guy been all our life?) We tried Xception, Enet and SqueezeNet.  Very quickly, we chose <a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> due to the explicit positioning of this system as a solution for embedded depth learning systems and due to the availability of the pre-trained Keras model on GitHub (hooray, open-source). <br><br>  So how big is the difference?  An architecture like VGG uses about 138 million parameters (essentially, this is the number of numbers needed to simulate neurons and the values ‚Äã‚Äãbetween them).  Inception represents significant progress, requiring only 23 million parameters.  For comparison, SqueezeNet works with 1.25 million parameters. <br><br>  This gives two advantages: <br><br><ol><li>  At the learning stage, the smaller neural network learns much faster.  There are fewer parameters for placement in memory, so you can slightly parallelize learning (larger packet sizes), and the neural network will converge faster (that is, approach an idealized mathematical function). <br><br></li><li>  In production, the model is much smaller and much faster.  SqueezeNet consumes less than 10 MB of RAM, while architectures like Inception require 100 MB or more.  The difference is gigantic, and it is especially important when working on mobile devices, which may have less than 100 MB of available memory for your application.  Smaller neural networks also calculate the final result much faster than larger ones. </li></ol><br>  Of course, something had to be sacrificed: <br><br><ol><li>  The smaller neural network has less available ‚Äúmemory‚Äù: it will not be as effective in difficult situations (such as recognizing 20,000 different objects) or even handling complex situations in a narrow class of tasks (for example, understanding the difference between hot dogs in New York style and chicago style).  As a result, smaller neural networks typically exhibit lower accuracy than larger networks.  When trying to recognize 20,000 ImageNet objects, SqueezeNet shows only 58% recognition accuracy, while VGG shows 72%. <br><br></li><li>  A small neural network is harder to retrain.  Technically, nothing prevents us from using the same approach as in the case of Inception and VGG, forcing SqueezeNet to ‚Äúforget‚Äù something - and retrain it specifically to distinguish between hot dogs and non-hot dogs.  In practice, we had difficulty adjusting the pace of learning, and the results were always less satisfactory than learning from SqueezeNet from scratch.  This may also be partly due to the nature of the ‚Äúopen world‚Äù of our task. <br><br></li><li>  In theory, smaller neural networks should rarely be retrained, but we came across this on several ‚Äúsmall‚Äù architectures.  Overfitting means that your network specializes too much, and instead of learning to recognize hotdogs in general, it learns to recognize exactly and only specific photos of the hotdogs on which you trained it.  A human analogy would be to memorize specific photos with hot dogs, which are shown to you, instead of abstraction, that a hot dog usually consists of sausages in a bun, possibly with spices, etc. If you are shown a completely new hot dog image, not something you remember, you will be inclined to say that this is not a hot dog.  Due to the fact that in small networks usually less ‚Äúmemory‚Äù, it is easy to understand why it is more difficult for them to specialize.  But in some cases, the accuracy of our small networks jumped up to 99% and suddenly she stopped recognizing images that she did not see at the training stage.  The effect usually disappeared when we added augmented data: half-random stretched / distorted images at the input, and instead of 1.00 times, each of the 1000 images is trained on a certain way of changing thousands of images to reduce the likelihood of the neural network to remember this 1000 images.  Instead, it should recognize hotdog ‚Äúsigns‚Äù (bun, sausage, seasoning, etc.), while remaining flexible and general enough not to be too attached to specific pixel values ‚Äã‚Äãof particular images in the training set. </li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/382/d81/6ad/382d816adb07b10e5f481f5a0a042f68.png"><br>  <font color="gray">Example of augmented data from <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">the Keras blog</a></font> <br><br>  At this stage, we began experimenting with customizing the neural network architecture.  In particular, we began to use Batch Normalization and try different activation functions. <br><br><ul><li>  Batch Normalization helps neural networks learn faster by smoothing the values ‚Äã‚Äãat various stages on the stack.  Exactly why this works is not fully understood, but the effect is known: the neural network converges much faster, which means it achieves greater accuracy with less training or more accuracy with the same amount of training, often drastically greater accuracy. <br><br></li><li>  Activation functions are internal mathematical functions that determine whether or not to activate your ‚Äúneurons‚Äù.  In many scientific articles, the ReLU (Rectified Linear Unit) is still mentioned, but we got better results at the ELU. </li></ul><br>  After adding Batch Normalization and ELU to SqueezeNet, we were able to train a neural network that reached accuracy above 90% when learning from scratch, but it was quite fragile, that is, the same neural network could in some cases be retrained or under-trained in other situations faced with testing in real conditions.  Even adding additional examples to the data set and experimenting with data augmentation did not help set up a network that showed normal results. <br><br>  So although this stage was promising and for the first time gave us a functioning application that worked entirely on the iPhone and calculated the result in less than a second, but in the end we switched to our fourth and final architecture. <br><br><h1>  3. DeepDog Architecture </h1><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.imagenet_utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> _obtain_input_shape <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Convolution2D, SeparableConvolution2D, \ GlobalAveragePooling2d \ Dense, Activation, BatchNormalization <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.engine.topology <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> get_source_inputs <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> get_file <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.utils <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> layer_utils <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">DeepDog</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input_tensor=None, input_shape=None, alpha=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, classes=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1000</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> input_shape = _obtain_input_shape(input_shape, default_size=<span class="hljs-number"><span class="hljs-number">224</span></span>, min_size=<span class="hljs-number"><span class="hljs-number">48</span></span>, data_format=K.image_data_format(), include_top=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> input_tensor <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: img_input = Input(shape=input_shape) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> K.is_keras_tensor(input_tensor): img_input = Input(tensor=input_tensor, shape=input_shape) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: img_input = input_tensor x = Convolution2D(int(<span class="hljs-number"><span class="hljs-number">32</span></span>*alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(img_input) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">32</span></span>*alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">64</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">128</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">128</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">256</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">256</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">5</span></span>): x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">512</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">512</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = SeparableConvolution2D(int(<span class="hljs-number"><span class="hljs-number">1024</span></span> * alpha), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'elu'</span></span>)(x) x = GlobalAveragePooling2D()(x) out = Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)(x) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> input_tensor <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>: inputs = get_source_inputs(input_tensor) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: inputs = img_input model = Model(inputs, out, name=<span class="hljs-string"><span class="hljs-string">'deepdog'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><h3>  Design </h3><br>  Our final architecture was largely influenced by a Google paper published on April 17, 2017 <a href="https://arxiv.org/abs/1704.04861">on MobileNets</a> , which described the new neural network architecture with accuracy like Inception on simple tasks like ours and using only 4 million parameters or so.  This means that it occupies an advantageous position between SqueezeNet, which may have been too simple for our task, and too overloaded with the Inception and VGG architectures, too heavy for mobile use.  This article describes some of the possibilities for adjusting the size and complexity of a neural network, specifically for choosing a balance between memory consumption / CPU and accuracy, which is exactly what we thought at the time. <br><br>  Less than a month before the deadline, we tried to reproduce the results from a scientific article.  It was an absolute disappointment when, within a day after the publication of the article, <a href="https://github.com/rcmalli/keras-mobilenet">the Keras implementation was</a> already publicly available on GitHub thanks to Refiq Kan Mulley, a student at Istanbul Technical University, whose results we already used when we took Keras SqueezeNet.  The size, qualification and openness of the depth learning community, as well as the presence of such talents as Refik, is what makes depth learning suitable for use in modern applications, but it also makes working in this industry more exciting than in any other technical industry. in which I was involved in my life. <br><br>  In our final architecture, we have significantly moved away from the original architecture of MobileNets and from generally accepted rules, in particular: <br><br><ul><li>  We don‚Äôt use Batch Normalization &amp; Activation between point-and-point coagulation, because the XCeption science article (which discusses depth coagulation in detail) suggests that this actually leads to a decrease in accuracy on this type of architecture (as <a href="https://www.reddit.com/r/MachineLearning/comments/663m43/r_170404861_mobilenets_efficient_convolutional/dgfaoz1/">helpfully remarked</a> on Reddit author of a quicknet research paper).  Our method also helps reduce the size of the neural network. </li><li>  We used ELU instead of ReLU.  Just as in the experiments with SqueezeNet, here the ELU provides better convergence speed and final accuracy compared to the ReLU. </li><li>  We did not use PELU.  Although promising, this activation function tends to fall into a binary state, no matter how we use it.  Instead of gradually improving the accuracy of our neural network, it alternated between approximately 0% and approximately 100% from one packet to another.  It is not clear why this is happening, and this may be due to some kind of implementation error or user error.  Merging the axes of the width and height of our images did not change the situation. </li><li>  We did not use SELU.  A small investigation between the release of versions for iOS and Android, showed results very similar to PELU.  We suspect that SELU cannot be used in isolation by itself as some kind of silver bullet of activation functions, and it should be used - as indicated by the name of the scientific article - as part of a narrowly defined SNN architecture. </li><li>  We continued to use Batch Normalization with ELU.  There are many indicators that this is not necessary, but in each experiment that we performed without Batch Normalization, the neural network completely refused to converge.  The reason may be the small size of our architecture. </li><li>  We applied Batch Normalization <i>before</i> activation.  Although this is the <a href="https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/">subject of some controversy</a> these days, there was no convergence in the experiments on the placement of BN after activation. </li><li>  To optimize the neural network, we used <a href="https://arxiv.org/abs/1506.01186">Cycal Learning Rates</a> and a great <a href="https://github.com/bckenstler/CLR">implementation of Keras</a> from (fellow student) Brad Kenstler.  The CLR plays the game, trying to guess the optimal learning rate for the neural network.  More importantly, by increasing and decreasing the learning rate, the CLR helps to achieve recognition accuracy, which in our experience is higher than that of the traditional optimizer.  For both of these reasons, we cannot understand why using anything other than the CLR to train neural networks in the future. </li><li>  For our task, we did not see the need to adjust the values. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B1;</mo></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.488ex" height="1.455ex" viewBox="0 -520.7 640.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMATHI-3B1" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>Œ±</mo></mrow></math></span></span><script type="math/tex" id="MathJax-Element-1"> Œ± </script>  or <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3C1;</mo></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.202ex" height="1.937ex" viewBox="0 -520.7 517.5 834" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMATHI-3C1" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>œÅ</mo></mrow></math></span></span><script type="math/tex" id="MathJax-Element-2"> œÅ </script>  from the MobileNets architecture.  Our model is small enough for our tasks when <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3B1;</mo></mrow><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.749ex" height="1.937ex" viewBox="0 -728.2 2475.1 834" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMATHI-3B1" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-3D" x="918" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-31" x="1974" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>Œ±</mo></mrow><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-3"> Œ± = 1 </script>  , and the calculations are quite fast when <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3C1;</mo></mrow><mo>=</mo><mn>1</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="5.463ex" height="2.419ex" viewBox="0 -728.2 2352.1 1041.5" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMATHI-3C1" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-3D" x="795" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-31" x="1851" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>œÅ</mo></mrow><mo>=</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-4"> œÅ = 1 </script>  , so we chose to concentrate on achieving maximum accuracy.  However, these changes may be useful when trying to run an application on older mobile phones or embedded platforms. </li></ul><br>  So how exactly does this stack work?  In-depth training often has a bad reputation as a kind of ‚Äúblack box‚Äù, and although many of the components can actually be mysterious, our neural networks often display information about how some of their magic tricks work.  We can take individual layers from this stack and see how they are activated on specific input images, which gives us an understanding of what abilities each layer has to recognize sausages, buns, or other most noticeable signs of a hotdog. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3fa/edd/edd/3faeddedd6ebb70638929a04e771af19.png"><br><br><h3>  Training </h3><br>  The quality of the source data was the most important.  The neural network can only be as good as the source data, and the improvement in the quality of the training set probably became one of the three things that we spent the most time on working on this project.  To improve it, we have taken the following key steps: <br><br><ul><li>  Search for more images and more diverse images (height / width, background, lighting conditions, cultural features, perspective, composition, etc.). </li><li>  Matching image types with expected photos in production.  We assumed that people would take photographs, mostly real hotdogs, another food, or try to trick the system with random objects in every possible way, so our data set reflected this assumption. </li><li>  Give as many examples of similar objects as possible.  Some dishes look more like hotdogs than others (for example, hamburgers and sandwiches or, in the case of naked hotdogs, these are young carrots or even cooked cherry tomatoes).  Our dataset reflected this. </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The expected distortion: most photos from a mobile phone will be worse than the ‚Äúaverage‚Äù photo from a digital SLR or in ideal lighting conditions. </font><font style="vertical-align: inherit;">Mobile photos are dim, noisy, taken at an angle. </font><font style="vertical-align: inherit;">Aggressive data augmentation has been a key tool for solving this problem.</font></font></li><li>   ,          ,          Google,       ( ,     ,    ,    -,   ).          ,      <a href="http://www.popsci.com/byzantine-science-deceiving-artificial-intelligence">     ()      </a> .      Keras    . </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/8b0/354/c9f/8b0354c9f0eed9d7dd002dadfcdd8870.jpg"><br> <font color="gray">  -   .  : <a href="">Wikimedia Commons</a></font> <br><br><ul><li>      . ,      ( )            .    ,   )          (       )  )           ,             .       ,   , . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the final form, our data set consisted of 150 thousand images, of which only 3,000 were hot dogs. </font><font style="vertical-align: inherit;">The unbalanced 49: 1 ratio was indicated in the Keras 49: 1 class weight settings in favor of hot dogs. </font><font style="vertical-align: inherit;">Most of the remaining 147 thousand photographs were different dishes, and only 3000 were not food to help the neural network to make slightly better generalizations and not take the image of a man in red clothes as a hot dog. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Our data augmentation rules are as follows:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We applied a rotation of ¬± 135 ¬∞ - much stronger than the average, because we programmed the application to not pay attention to the orientation of the phone. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The height and width were distorted by 20%. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Circumcision in the range of 30%. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Zooming in the range of 10%. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Channel shifts by 20%. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Random horizontal coups to help neural networks generalize. </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These parameters are derived intuitively, based on experiments and our understanding of how the application will be used in real conditions, as opposed to accurate experiments. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the last stage of our data processing pipeline, we used a </font></font><a href="https://github.com/stratospark/keras-multiprocess-image-data-generator"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">multi-</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> process </font><a href="https://github.com/stratospark/keras-multiprocess-image-data-generator"><font style="vertical-align: inherit;">image generator for Patras</font></a><font style="vertical-align: inherit;"> Rodriguez's </font><a href="https://github.com/stratospark/keras-multiprocess-image-data-generator"><font style="vertical-align: inherit;">Keras</font></a><font style="vertical-align: inherit;"> . Although Keras has an embedded implementation of multithreading and multiprocessing, the Patrick library worked consistently faster in our experiments for reasons that we did not have time to figure out. This library has shortened the training time by a third.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The network was trained on a 2015 MacBook Pro laptop with an external GPU (eGPU) connected, namely the Nvidia GTX 980 Ti (we probably would have bought 1080 Ti if we started today). </font><font style="vertical-align: inherit;">We were able to train the neural network on packets of 128 images. </font><font style="vertical-align: inherit;">The network has been trained for a total of 240 epochs. </font><font style="vertical-align: inherit;">This means that we passed through it all 150 thousand images 240 times. </font><font style="vertical-align: inherit;">It took about 80 hours. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We trained the neural network in three stages:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The first stage lasted 112 epochs (7 complete CLR cycles with a step size of 8 epochs), with a learning rate of between 0.005 and 0.03, with the rule of triangle 2 (this means that the maximum learning rate was halved every 16 epochs). </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The second stage continued another 64 epochs (4 CLR cycles with a step size of 8 epochs), with a learning rate of between 0.0004 and 0.0045, with the rule of triangle 2. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The third stage lasted another 64 epochs (4 CLR cycles with a step size of 8 epochs), with a learning rate between 0.000015 and 0.0002, with the triangle rule 2. </font></font></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/30a/a35/2f5/30aa352f56b14427907f5a893fc9221e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Although the pace of learning was determined by conducting a linear experiment recommended by the authors of a scientific article on CLR, they seem to be intuitive, here the maximum rate at each stage is approximately two times less than the previous minimum, which corresponds to the industry standard of halving the rate of learning if the recognition accuracy is the learning process has ceased to grow. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To save time, we spent part of the training on the Ubuntu Paperspace P5000 instance. </font><font style="vertical-align: inherit;">In some cases, it was possible to double the size of the packages, and the optimal pace of training at each stage also doubled.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Run neural networks on mobile phones </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Even having designed a relatively compact neural architecture and trained it to cope with specific situations in a mobile context, there was still a lot of work to be done for the application to work properly. If you run the first-class neural network architecture without changes, it will quickly consume hundreds of megabytes of RAM, which few of modern mobile devices will withstand. In addition to optimizing the network itself, it turned out that the image processing method and even the TensorFlow method itself have a huge effect on the speed of the neural network, the amount of RAM consumed and the number of failures.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is probably the most mysterious part of the project. It is rather difficult to find information on this topic, possibly due to the small number of in-depth learning applications that today run on mobile devices. However, we are grateful to the TensorFlow development team, and especially Pete Varden, Andrew Harp and Chad Wipki for the existing documentation and their goodwill in answering our questions.</font></font><br><br><ul><li>           25%  .  ,     ,    ,    N           ,       zip-.             .       ,           ,       ,        . </li><li>   TensorFlow        <code>-Os</code> . </li><li>     TensorFlow:     ,       TensorFlow,    , ,   .   <a href="https://github.com/tensorflow/tensorflow/issues/9073">    (  )</a> ,      TensorFlow    iOS. </li><li>    . ,          Android  1  <a href="https://github.com/tensorflow/tensorflow/pull/7832">  </a> ,       ,   TensorFlow  iOS      . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instead of using TensorFlow on iOS, we studied Apple‚Äôs embedded depth learning libraries (BNNS, MPSCNN, and later CoreML). </font><font style="vertical-align: inherit;">We would design the neural network on Keras, train it using TensorFlow, export all the values ‚Äã‚Äãof the weights, re-implement the neural network on BNNS or MPSCNN (or import via CoreML) and load the parameters into the new implementation. </font><font style="vertical-align: inherit;">However, the biggest obstacle was that the new Apple libraries are only available for iOS 10+, and we wanted to support older versions of iOS. </font><font style="vertical-align: inherit;">As iOS 10+ grows and these frameworks improve, you may not need to run TensorFlow on a mobile device in the future.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Change the behavior of the application by introducing a neural network on the fly </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you think that injecting JavaScript into your application on the fly is cool, then try introducing a neural network on the fly! </font><font style="vertical-align: inherit;">The last trick we used in production was the use of </font></font><a href="https://microsoft.github.io/code-push/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CodePush</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and relatively liberal Apple terms of use to implement new versions of our neural networks live after they were published in the application catalog. </font><font style="vertical-align: inherit;">This is done mainly to improve recognition accuracy after release, but this method, theoretically, can be used to drastically improve the functionality of your application without having to re-review in the AppStore.</font></font><br><br><pre> <code class="hljs smalltalk"><span class="hljs-symbol"><span class="hljs-symbol">#import</span></span> &lt;<span class="hljs-type"><span class="hljs-type">CodePush</span></span>/<span class="hljs-type"><span class="hljs-type">CodePush</span></span>.h&gt; ‚Ä¶ <span class="hljs-type"><span class="hljs-type">NSString</span></span>* <span class="hljs-type"><span class="hljs-type">FilePathForResourceName</span></span>(<span class="hljs-type"><span class="hljs-type">NSString</span></span>* name, <span class="hljs-type"><span class="hljs-type">NSString</span></span>* extension) { // <span class="hljs-type"><span class="hljs-type">NSString</span></span>* file_path = [[<span class="hljs-type"><span class="hljs-type">NSBundle</span></span> mainBundle] pathForResource:name ofType:extension]; <span class="hljs-type"><span class="hljs-type">NSString</span></span>* file_path = [[[[<span class="hljs-type"><span class="hljs-type">CodePush</span></span>.bundleURL.<span class="hljs-type"><span class="hljs-type">URLByDeletingLastPathComponent</span></span> <span class="hljs-type"><span class="hljs-type">URLByAppendingPathComponent</span></span>:@<span class="hljs-comment"><span class="hljs-comment">"assets"</span></span>] <span class="hljs-type"><span class="hljs-type">URLByAppendingPathComponent</span></span>:name] <span class="hljs-type"><span class="hljs-type">URLByAppendingPathExtension</span></span>:extension] path]; if (file_path == <span class="hljs-type"><span class="hljs-type">NULL</span></span>) { <span class="hljs-type"><span class="hljs-type">LOG</span></span>(<span class="hljs-type"><span class="hljs-type">FATAL</span></span>) &lt;&lt; <span class="hljs-comment"><span class="hljs-comment">"Couldn't find '"</span></span> &lt;&lt; [name <span class="hljs-type"><span class="hljs-type">UTF8String</span></span>] &lt;&lt; <span class="hljs-comment"><span class="hljs-comment">"."</span></span> &lt;&lt; [extension <span class="hljs-type"><span class="hljs-type">UTF8String</span></span>] &lt;&lt; <span class="hljs-comment"><span class="hljs-comment">"' in bundle."</span></span>; } return file_path; } ‚Ä¶</code> </pre> <br><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> React, { Component } <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'react'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> { AppRegistry } <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'react-native'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> CodePush <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">"react-native-code-push"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> App <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'./App'</span></span>; <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">nothotdog</span></span></span><span class="hljs-class"> </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Component</span></span></span><span class="hljs-class"> </span></span>{ render() { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"> &lt;App </span></span><span class="hljs-regexp"><span class="hljs-function"><span class="hljs-params"><span class="hljs-regexp">/&gt; ) } } require('./</span></span></span></span><span class="hljs-function"><span class="hljs-params">deepdog.pdf</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">') const codePushOptions = { checkFrequency: CodePush.CheckFrequency.ON_APP_RESUME }; AppRegistry.registerComponent('</span></span></span></span><span class="hljs-function"><span class="hljs-params">nothotdog</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">', () =&gt; CodePush(codePushOptions)(nothotdog));</span></span></span></span></span></span></code> </pre> <br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> What would we do differently </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are many things that did not work or did not have the time to experience them, and here are some of the ideas we would explore in the future: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> More careful setting of our augmentation parameters. </font></font></li><li>      ,         ,         ,      (   ,    ¬´¬ª,     0,90,       0,5),     . . </li><li>       ‚Äî          ,     . </li><li>      ,   224√ó224  ‚Äî ,    <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3C1;</mo></mrow><mo>&amp;gt;</mo><mn>1</mn><mo>,</mo><mn>0</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.659ex" height="2.419ex" viewBox="0 -728.2 3297.7 1041.5" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMATHI-3C1" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-3E" x="795" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-31" x="1851" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-2C" x="2352" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/331740/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi1brf7GC2kCbdDGqJT100m1t8C7Q#MJMAIN-30" x="2797" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow class="MJX-TeXAtom-ORD"><mo>œÅ</mo></mrow><mo>&gt;</mo><mn>1</mn><mo>,</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-5">œÅ > 1,0</script></li></ul><br><h1> UX/DX,    ¬´ ¬ª  </h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the end, it would be unforgivable not to mention the obvious and important influence of user interaction (UX), developer (DX) and embedded bias in the development of an AI application. Perhaps each of these topics deserves a separate article (or a separate book), but that was the very specific impact of these three factors on our work. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UX (user interaction)</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">it may be more important at each stage of developing an AI application than a regular application. Right now there are no depth learning algorithms that will give you perfect results, but there are many situations where the right combination of depth learning and UX will produce results indistinguishable from the ideal. Correct expectations regarding UX are invaluable when it comes to developing the correct direction for the design of a neural network and the elegant handling of cases of unavoidable AI failures. Creating AI applications without thinking about user interaction is like learning a neural network without a stochastic gradient descent: you are stuck in a local minimum of an </font></font><a href="https://en.wikipedia.org/wiki/Uncanny_valley"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ominous valley</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> on your way to creating a perfectly working AI. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e18/f2b/3ea/e18f2b3ea946375230e4c4ef6653bef9.jpg"><br> <font color="gray"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: </font></font><a href="https://www.newscientist.com/article/dn28432-into-the-uncanny-valley-80-robot-faces-ranked-by-creepiness/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">New Scientist</font></font></a></font> <br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DX (developer engagement) is</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> also extremely important, because the learning time of a neural network is a new hemorrhoid, along with the expectation of compiling the program. </font><font style="vertical-align: inherit;">We believe that you will definitely put DX in the first place in the list of priorities (therefore, choose Keras), because there is always an opportunity to optimize the environment for subsequent execution (manual parallelization of the GPU, augmentation of data for multiprocessing, the TensorFlow pipeline, even the repeated implementation for caffe2 / pyTorch).</font></font><br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-4" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="867244660569374721"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Even projects with relatively stupid documentation like TensorFlow make it much easier to interact with the developer by providing well-tested, widely used and superbly supported environments for learning and running neural networks. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the same reason, it is difficult to find something cheaper and more convenient than your own GPU for development. The ability to locally view and edit images, edit the code in your favorite editor without delay - this greatly improves the quality and speed of developing AI projects. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Most AI applications will encounter more </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cultural bias.</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">than our application. </font><font style="vertical-align: inherit;">But for example, even in our simplest case, initially thinking about cultural features, we taught the neural network to recognize hotdogs in French, Asian hotdogs and even more oddities that we had no idea about before. </font><font style="vertical-align: inherit;">It is important to remember that AI does not make ‚Äúbetter‚Äù decisions than a person ‚Äî they are affected by the same bias as humans, and infection occurs during human learning.</font></font><br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-5" style="position: static; visibility: visible; display: block; transform: rotate(0deg); width: 500px; margin: 10px auto; max-width: 100%; min-width: 220px;" data-tweet-id="866342755106148352"></twitter-widget><script async="" src="//platform.twitter.com/widgets.js" charset="utf-8"></script></div></div><p>Source: <a href="https://habr.com/ru/post/331740/">https://habr.com/ru/post/331740/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../331726/index.html">Security issues and major achievements of AI</a></li>
<li><a href="../331728/index.html">Apache Spark: from open source to industry</a></li>
<li><a href="../331730/index.html">Intel Vpro or IP-KVM for desktops</a></li>
<li><a href="../331732/index.html">Signs of distressed design</a></li>
<li><a href="../331734/index.html">The book "Command shell scripts. Linux, OS X and Unix. 2nd edition ¬ª</a></li>
<li><a href="../331744/index.html">Postquantum reincarnation of the Diffie-Hellman algorithm: past and present</a></li>
<li><a href="../331746/index.html">Using Python and Excel to process and analyze data. Part 1: Import Data and Set Up Environment</a></li>
<li><a href="../331748/index.html">How to comply with FZ-152 ‚ÄúOn personal data‚Äù with ‚ÄúBitrix24‚Äù and ‚Äú1C-Bitrix‚Äù</a></li>
<li><a href="../331750/index.html">Speed ‚Äã‚Äãreading: does it work or not? Part 3: simple tips</a></li>
<li><a href="../331752/index.html">Testing or parsing sites with a dynamic home and more. Nightmare.js - he doesn't care</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>