<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Intuitive use of MCMC</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Is it easy? I tried 
 Alexey Kuzmin, director of development and work with DomKlik data, teacher of Data Science in Netology, translated the article b...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Intuitive use of MCMC</h1><div class="post__text post__text-html js-mediator-article"><h4>  Is it easy?  I tried </h4><br>  <i>Alexey Kuzmin, director of development and work with DomKlik data, teacher of <a href="https://netology.ru/data-science/programs%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dbds_ou_cat_habr_18072019msms">Data Science</a> in Netology, translated the <a href="https://towardsdatascience.com/mcmc-intuition-for-everyone-5ae79fff22b1">article by</a> Rahul Agarwal about how Monte Carlo methods work with Markov chains to solve problems with a large state space.</i> <br><a name="habracut"></a><br>  Anyone who is connected to Data Science has ever heard of the Monte Carlo methods with Markov chains (MCMC).  Sometimes the topic is covered while studying Bayesian statistics, sometimes when working with tools like Prophet. <br><br>  But MCMC is hard to understand.  Whenever I read about these methods, I noticed that the essence of MCMC is hidden in the deep layers of mathematical noise, and behind this noise it is difficult to make out.  I had to spend many hours to understand this concept. <br><br>  In this article, an attempt to explain Monte Carlo methods with Markov chains is available, so that it becomes clear what they are used for.  I will discuss a few more ways to use these methods in my next post. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, let's begin.  MCMC consists of two terms: Monte Carlo and Markov chains.  Let's talk about each of them. <br><br><h2>  Monte Carlo </h2><br><img src="https://habrastorage.org/webt/i8/wx/2n/i8wx2nylvemkcfvwp7rxvznjfa0.jpeg"><br><br>  In the simplest expressions <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">, Monte Carlo methods</a> can be defined as a simple simulation. <br><br>  Monte Carlo methods get their name from the Monte Carlo casino in Monaco.  In many card games you need to know the probability of winning from the dealer.  Sometimes calculating this probability can be mathematically complex or difficult to solve.  But we can always run a computer simulation to reproduce the whole game many times and consider probability as the number of victories divided by the number of games played. <br>  That's all you need to know about Monte Carlo methods.  Yes, it's just a simple simulation technique with a fancy name. <br><br><h2>  Markov chains </h2><br><img src="https://habrastorage.org/webt/7r/my/np/7rmynpvle1yn4xk5tuwqdvkd7mo.jpeg"><br><br>  Since the term MCMC consists of two parts, one must also understand what <a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BF%25D1%258C_%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B0">Markov chains are</a> .  But before moving on to Markov chains, let's talk a little about Markov properties. <br><br>  Suppose there is a system from M-possible states, and you move from one state to another.  Let nothing be confusing you yet.  A specific example of such a system is weather, which varies from hot to cold and moderate.  Another example is the stock market, which jumps from bearish to bullish and stagnant. <br><br>  <i>The Markov property</i> says that for a given process, which is in a state of X <sub>n</sub> at a particular point in time, the probability X <sub>n + 1</sub> = k (where k is any of the M-states to which the process can go) depends only on what is the state at the moment.  And not about how it has reached the current state. <br>  Mathematically speaking, we can write it in the form of the following formula: <br><img src="https://habrastorage.org/webt/tw/kq/se/twkqseq2tra2alhiqdyfuyswnla.png"><br>  For clarity: you do not care about the sequence of conditions that the market took to become ‚Äúbullish‚Äù.  The likelihood that the next state will be ‚Äúbearish‚Äù is determined only by the fact that the market is currently in a ‚Äúbullish‚Äù state.  This also makes sense in practice. <br><br>  A process with a Markov property is called a Markov process.  Why is Markov chain important?  Because of its stationary distribution. <br><br><h3>  What is stationary distribution </h3><br>  I will try to explain the stationary distribution by calculating it for the example below.  Suppose you have a Markov process for the stock market, as shown below. <br><img src="https://habrastorage.org/webt/1k/o-/bb/1ko-bbb9hzmv8j9o_an1ocvyurs.png"><br>  You have a transition probability matrix, which determines the probability of transition from state X <sub>i</sub> to X <sub>j</sub> . <br><img src="https://habrastorage.org/webt/or/u7/jp/oru7jpnpioj12jbrcw7t4o6sk94.png"><br>  Transition probability matrix, Q <br><br>  In the above matrix of transition probabilities Q, the probability that the next state will be ‚Äúbull‚Äù, considering the current state ‚Äúbull‚Äù = 0.9;  the probability that the next state will be ‚Äúbearish‚Äù if the current state is ‚Äúbull‚Äù = 0.075.  And so on. <br><br>  Well, let's start with some particular state.  Our state will be set by vector [bull, bear, stagnation].  If we start with the ‚Äúbearish‚Äù state, the vector will be: [0,1,0].  We can calculate the probability distribution for the next state by multiplying the current state vector by the transition probability matrix. <br><img src="https://habrastorage.org/webt/or/jy/85/orjy85onzacwcu2wlgftmugzygk.png"><br>  <b>Notice that probabilities give a total of 1.</b> <br><br>  The following state distribution can be found by the formula: <br><img src="https://habrastorage.org/webt/ge/wk/fu/gewkfuf7xziuajc2ox7tn9blfcm.png"><br><br>  And so on.  In the end, you will reach a steady state in which the state stabilizes: <br><img src="https://habrastorage.org/webt/iy/5y/65/iy5y65fxgxo4foqpe3fpq2hs-dq.png"><br><br>  For the matrix of transition probabilities Q described above, the stationary distribution s is: <br><img src="https://habrastorage.org/webt/ae/ut/_8/aeut_8m8wsypenpgkig3onnzwdc.png"><br>  You can get the stationary distribution with the following code: <br><br><pre><code class="python hljs">Q = np.matrix([[<span class="hljs-number"><span class="hljs-number">0.9</span></span>,<span class="hljs-number"><span class="hljs-number">0.075</span></span>,<span class="hljs-number"><span class="hljs-number">0.025</span></span>],[<span class="hljs-number"><span class="hljs-number">0.15</span></span>,<span class="hljs-number"><span class="hljs-number">0.8</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>],[<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.25</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) init_s = np.matrix([[<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-number"><span class="hljs-number">0</span></span>]]) epsilon =<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> epsilon&gt;<span class="hljs-number"><span class="hljs-number">10e-9</span></span>:    next_s = np.dot(init_s,Q)    epsilon = np.sqrt(np.sum(np.square(next_s - init_s)))    init_s = next_s print(init_s) ------------------------------------------------------------------ matrix([[<span class="hljs-number"><span class="hljs-number">0.62499998</span></span>, <span class="hljs-number"><span class="hljs-number">0.31250002</span></span>, <span class="hljs-number"><span class="hljs-number">0.0625</span></span>  ]])</code> </pre> <br>  You can also start from any other state - reach the same stationary distribution.  Change the initial state in the code if you want to verify this. <br><br>  Now we can answer the question why stationary distribution is so important. <br><br>  The stationary distribution is important because it can be used to determine the probability of a system being in a certain state at a random time. <br><br>  For our example, we can say that in 62.5% of cases the market will be in a ‚Äúbullish‚Äù state, 31.25% - in a ‚Äúbearish‚Äù state and 6.25% - in stagnation. <br><br>  Intuitively, you can think of it as a random walk along a chain. <br><br><img src="https://habrastorage.org/webt/i3/e6/xt/i3e6xtqko2iip-janj6dvfbnv4q.png"><br>  Random walk <br><br>  You are at a certain point and select the next state, observing the probability distribution of the next state, taking into account the current state.  We can visit some sites more often than others, based on the probabilities of those sites. <br><br>  In this way, at the dawn of the Internet, Google solved the search problem.  The problem was sorting the pages, depending on their importance.  Google solved the problem using the Pagerank algorithm.  The Google Pagerank algorithm should consider the state as a page, and the probability of a page in a stationary distribution as its relative importance. <br><br>  We now turn directly to the consideration of the methods of MCMC. <br><br><h2>  What are the methods of Monte Carlo with Markov chains (MCMC) </h2><br>  Before answering what MCMC is, let me ask one question.  We know about beta distribution.  We know its probability density function.  But can we take a sample from this distribution?  Can you think of a way to do this? <br><br><img src="https://habrastorage.org/webt/ks/ai/wd/ksaiwdv7lomes7g55ihqbhxushs.png"><br>  Think ... <br><br>  MCMC allows you to choose from any probability distribution.  This is especially important when you need to make a selection of the posterior distribution. <br><img src="https://habrastorage.org/webt/12/kn/-5/12kn-58z9ub6t2ft1lctptwix28.png"><br>  The figure shows the Bayes theorem <br><br>  For example, you need to make a sample of the a posteriori distribution.  But is it easy to calculate the a posteriori component along with the normalizing constant (evidence)?  In most cases, you can find them in the form of likelihood and a priori probability.  But to calculate the normalizing constant (p (D)) does not work.  Why?  We will understand in more detail. <br><br>  Suppose H accepts only 3 values: <br><br>  p (D) = p (H = H1) .p (D | H = H1) + p (H = H2) .p (D | H = H2) + p (H = H3) .p (D | H = H3) <br><br>  In this case, p (D) is easy to calculate.  What if the value of H is continuous?  Would it work out just as easily, especially if H took infinite values?  For this, one would have to solve a complex integral. <br><br>  We want to make a random selection from the a posteriori distribution, but we also want to consider p (D) to be a constant. <br><br>  Wikipedia <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">writes</a> : <br><br>  Monte Carlo methods with Markov chains is a class of algorithms for sampling from a probability distribution based on the construction of a Markov chain, which has the desired form as a stationary distribution.  The state of the chain after a series of steps is then used as a sample of the desired distribution.  Sample quality improves with increasing number of steps. <br><br>  We will understand by example.  Suppose you need a sample from the <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B5%25D1%2582%25D0%25B0-%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">beta distribution</a> .  Its density is: <br><img src="https://habrastorage.org/webt/ag/xh/wj/agxhwjqglfhcz2bl88eu3ov84ja.png"><br><br>  where C is the normalizing constant.  In fact, this is some function of Œ± and Œ≤, but I want to show that it is not needed for a sample from the beta distribution, so we take it as a constant. <br><br>  The beta distribution task is really difficult, if not to say, practically insoluble.  In fact, you may have to work with more complex distribution functions, and sometimes you will not know the normalizing constants. <br><br>  The MCMC methods make life easier by providing algorithms that could create a Markov chain, which has a beta distribution as a stationary distribution, given that we can choose from a uniform distribution (which is relatively simple). <br><br>  If we start from a random state and move to the next state based on a certain algorithm several times, we will eventually create a Markov chain, which has a beta distribution as a stationary distribution.  And the states in which we will find ourselves after a long time can be used as a sample from the beta distribution. <br><br>  One of these MCMC algorithms is the <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%259C%25D0%25B5%25D1%2582%25D1%2580%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25B8%25D1%2581%25D0%25B0_%25E2%2580%2594_%25D0%2593%25D0%25B0%25D1%2581%25D1%2582%25D0%25B8%25D0%25BD%25D0%25B3%25D1%2581%25D0%25B0">Metropolis-Hastings algorithm.</a> <br><br><h2>  Metropolis-Hastings Algorithm </h2><br><img src="https://habrastorage.org/webt/qq/oj/89/qqoj89il8-rsyd3xo-sqawnrug0.jpeg"><br><br><h3>  Intuition: </h3><br>  So what is the purpose? <br><br>  <i>It is intuitively clear that we want to walk on some piecewise surface (our Markov chain) in such a way that the amount of time we spend at each location is proportional to the height of the surface at that location (the desired probability density from which we want to take a sample).</i> <i><br><br></i>  <i>For example, we would like to spend twice as much time on the top of a hill 100 meters high than on a nearby 50 meter hill.</i>  <i>It‚Äôs good that we can do this even if we don‚Äôt know the absolute heights of points on the surface: all you need to know is relative heights.</i>  <i>For example, if the top of hill A is twice as high as the top of hill B, then we would like to spend twice as much time as A than B.</i> <i><br><br></i>  <i>There are more complex schemes for proposing new places and the rules for their adoption, but the basic idea is this:</i> <i><br><br></i> <ol><li>  <i>Select a new "proposed" location.</i> </li><li>  <i>Find out how much higher or lower this location is compared to the current one.</i> </li><li>  <i>Stay in place or move to a new place with a probability proportional to the heights of places.</i> </li></ol> <i><br></i>  <i>The purpose of an MCMC is to select from a certain probability distribution without having to know its exact height at any point (no need to know C).</i> <i><br></i>  <i>If the ‚Äúwandering‚Äù process is configured correctly, you can make sure that this proportionality (between the time spent and the height of the distribution) is reached</i> . <br><br><h3>  Algorithm: </h3><br>  Now let's define and describe the task in a more formal way.  Let s = (s1, s2, ..., sM) be the desired stationary distribution.  We want to create a Markov chain with such a stationary distribution.  We start with an arbitrary Markov chain with M-states with the transition matrix P, so that pij represents the probability of transition from state i to j. <br><br>  Intuitively, we know how to roam the Markov chain, but the Markov chain does not have the required stationary distribution.  This circuit has some stationary distribution (which we do not need).  Our goal is to change the way we wander the Markov chain so that the chain has the desired stationary distribution. <br><br>  To do this: <br><br><ol><li>  Start with a random initial state i. </li><li>  Randomly select a new estimated state, looking at the transition probabilities in the i-th row of the transition matrix P. </li><li>  Calculate a measure called decision probability, which is defined as: aij = min (sj.pji / si.pij, 1). </li><li>  Now flip a coin that lands on the surface with an eagle with probability aij.  If an eagle falls, accept the offer, that is, move to the next state, otherwise, reject the offer, that is, remain in the current state. <br></li><li>  Repeat many times. <br></li></ol><br>  After a large number of tests, this circuit will converge and have a stationary distribution of s.  We can then use the states of the circuit as a sample of any distribution. <br><br>  Doing this for a beta distribution, the only time that you have to use probability density is to search for a probability of decision making.  To do this, divide sj by si (that is, the normalizing constant C is reduced). <br><br><h3>  Sample from beta distribution </h3><br><img src="https://habrastorage.org/webt/h9/ac/zw/h9aczwarm4hfwm0pya3hai-rg2e.jpeg"><br><br>  We now turn to the problem of sampling from the beta distribution. <br><br>  The beta distribution is a continuous distribution on [0,1] and can have infinite values ‚Äã‚Äãon [0,1].  Suppose that an arbitrary Markov chain P with infinite states on [0,1] has a transition matrix P such that pij = pji = all elements in the matrix. <br><br>  We do not need the P Matrix, as we will see later, but I want the description of the problem to be as close as possible to the algorithm we proposed. <br><br><ul><li>  Start with a random initial state i, obtained from a uniform distribution on (0,1). </li><li>  Randomly select a new estimated state, looking at the transition probabilities in the i-th row of the transition matrix P. Suppose we selected another state Unif (0,1) as the estimated state j. </li><li>  Calculate the measure, which is called the probability of a decision: </li></ul><br><img src="https://habrastorage.org/webt/lp/jo/-0/lpjo-0phzn3o8zl83oniptticmu.png"><br>  What is simplified to: <br><img src="https://habrastorage.org/webt/4c/he/pi/4chepi6_1om84fk52t8jquzpmku.png"><br>  Since pji = pij, and where <br><img src="https://habrastorage.org/webt/pm/qc/y2/pmqcy2hanok1y-mnhbxk49dqbve.png"><br><ul><li>  Now throw a coin.  With probability aij, an eagle will fall out.  If an eagle falls, then it is necessary to accept the offer, that is, go to the next state.  Otherwise, it is worth rejecting the proposal, that is, to remain in the same condition </li><li>  Repeat the test many times. </li></ul><br><h3>  Code: </h3><br>  It's time to move from theory to practice.  Let's write our beta sample in Python. <br><br><pre> <code class="python hljs">impo rt rand om <span class="hljs-comment"><span class="hljs-comment"># Lets define our Beta Function to generate s for any particular state. We don't care for the normalizing constant here. def beta_s(w,a,b): return w**(a-1)*(1-w)**(b-1) # This Function returns True if the coin with probability P of heads comes heads when flipped. def random_coin(p): unif = random.uniform(0,1) if unif&gt;=p: return False else: return True # This Function runs the MCMC chain for Beta Distribution. def beta_mcmc(N_hops,a,b): states = [] cur = random.uniform(0,1) for i in range(0,N_hops): states.append(cur) next = random.uniform(0,1) ap = min(beta_s(next,a,b)/beta_s(cur,a,b),1) # Calculate the acceptance probability if random_coin(ap): cur = next return states[-1000:] # Returns the last 100 states of the chain</span></span></code> </pre><br>  Check the results with the actual beta distribution. <br><br><pre> <code class="python hljs">impo rt num py <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pl <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.special <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ss %matplotlib inline pl.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">17.0</span></span>, <span class="hljs-number"><span class="hljs-number">4.0</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Actual Beta PDF. def beta(a, b, i): e1 = ss.gamma(a + b) e2 = ss.gamma(a) e3 = ss.gamma(b) e4 = i ** (a - 1) e5 = (1 - i) ** (b - 1) return (e1/(e2*e3)) * e4 * e5 # Create a function to plot Actual Beta PDF with the Beta Sampled from MCMC Chain. def plot_beta(a, b): Ly = [] Lx = [] i_list = np.mgrid[0:1:100j] for i in i_list: Lx.append(i) Ly.append(beta(a, b, i)) pl.plot(Lx, Ly, label="Real Distribution: a="+str(a)+", b="+str(b)) pl.hist(beta_mcmc(100000,a,b),normed=True,bins =25, histtype='step',label="Simulated_MCMC: a="+str(a)+", b="+str(b)) pl.legend() pl.show() plot_beta(0.1, 0.1) plot_beta(1, 1) plot_beta(2, 3)</span></span></code> </pre><br><br><img src="https://habrastorage.org/webt/6z/b_/zb/6zb_zbywfexkiagddl4lpusmcko.png"><br><br>  As you can see, the values ‚Äã‚Äãare very similar to the beta distribution.  Thus, the MCMC network has reached a steady state <br><br>  In the above code, we created a beta sampler, but the same concept applies to any other distribution from which we want to make a sample. <br><br><h2>  findings </h2><br><img src="https://habrastorage.org/webt/5f/oh/h5/5fohh5w_hsavzw3yvbryxewnnkw.png"><br><br>  It was a great post.  Congratulations if you have read it to the end. <br><br>  In essence, MCMC methods can be complex, but they provide us with more flexibility.  You can select from any distribution function using MCMC selection.  Typically, these methods are used to sample from the a posteriori distributions. <br><br>  You can also use MCMC to solve problems with a large state space.  For example, in a backpack problem or for decoding.  I will try to provide you with more interesting examples in the <a href="https://towardsdatascience.com/applications-of-mcmc-for-cryptography-and-optimization-1f99222b7132">next</a> post.  Keep for updates. <br><br><h2>  From the Editor </h2><br><ul><li>  Online course " <a href="https://netology.ru/programs/python-for-analytics%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dbds_pydp_ou_habr_18072019msms">Python to work with data</a> " </li><li>  Online course " <a href="https://netology.ru/programs/machine-learn%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dbds_ml_ou_habr_18072019msms">Machine learning</a> " </li><li>  Online course " <a href="https://netology.ru/programs/big-data%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dbds_abd_ou_habr_18072019msms">BIG DATA from scratch</a> " </li></ul></div><p>Source: <a href="https://habr.com/ru/post/460497/">https://habr.com/ru/post/460497/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../460489/index.html">Top 11 errors in the development of BCP</a></li>
<li><a href="../46049/index.html">The most advanced camera on Earth will monitor asteroids</a></li>
<li><a href="../460491/index.html">Arduino temperature and humidity sensor with sending and drawing graphics (Part 1)</a></li>
<li><a href="../460493/index.html">Killer apps for PCs from the 80s: VisiCalc and WordStar</a></li>
<li><a href="../460495/index.html">Container - on the conveyor: CRI-O is now the default in OpenShift Container Platform 4</a></li>
<li><a href="../460499/index.html">Three winners of the Dijkstra Prize: how did Hydra 2019 and SPTDC 2019 go</a></li>
<li><a href="../4605/index.html">IPTV subscribers by the end of the year will be double</a></li>
<li><a href="../46050/index.html">One gas emotion of Japanese Emoji</a></li>
<li><a href="../460501/index.html">An example implementation of Continuous Integration using BuildBot</a></li>
<li><a href="../460503/index.html">Wireless configuration Raspberry PI 3 B +</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>