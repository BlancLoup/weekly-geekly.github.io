<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Performance: what's in my name? - Alexey Shipilev about optimization in large projects</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Performance optimization has long haunted developers, seeming as a kind of ‚Äúgolden key‚Äù to interesting solutions and a good track record. A big tour o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Performance: what's in my name? - Alexey Shipilev about optimization in large projects</h1><div class="post__text post__text-html js-mediator-article">  Performance optimization has long haunted developers, seeming as a kind of ‚Äúgolden key‚Äù to interesting solutions and a good track record.  A big tour of the key milestones of optimizing large projects - from general principles to traps and contradictions - at the last JPoint 2017 was held by Alexey Shipilyov, a performance expert. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/p2b4JHESEOc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  Under the cut - decoding his report. <br><a name="habracut"></a><br>  And here you can find the presentation itself: <a href="https://shipilev.net/talks/jpoint-April2017-perf-keynote.pdf">jpoint-April2017-perf-keynote.pdf</a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>About speaker</i> <i><br></i>  <i>Alexey Shipilev - has been in the Java performance issue for more than 10 years.</i>  <i>Now he works at Red Hat, where he develops OpenJDK and is engaged in its performance.</i>  <i>Develops and maintains several subprojects in OpenJDK, including JMH, JOL, and JCStress.</i>  <i>Prior to Red Hat, he worked on Apache Harmony at Intel, and then moved to Sun Microsystems, which was absorbed by Oracle.</i>  <i>He actively participates in expert groups and communities working on issues of productivity and multithreading.</i> <br><br>  I work for Red Hat.  Earlier, when I was working at Oracle, we inserted Safe Harbor slides that said that everything that we will tell you might not be true, so you need to think with your head.  If you are trying to incorporate any solutions into your products, it would be nice to hire professionals who will tell you what‚Äôs true and what‚Äôs not. <br><br><h2>  Large: what are the criteria for success in developing </h2><br>  Suppose you are developing a product.  What is a successful product for you?  We are all commercial programmers.  We can fool ourselves that we are paid a salary for what we code.  But in fact, we are paid a salary in order to sell either services that serve the products, or products entirely. <br><br><img src="https://habrastorage.org/web/04f/3bf/4a3/04f3bf4a377a47bb885634ba46771c53.png"><br><br>  But what are the purely development criteria for the success of a product (without taking into account the business goal)? <br><br><ul><li>  When you communicate with programmers, they usually say that a good (successful) product is one that has a <i>correct implementation</i> . <br></li><li>  Then the security guards come and say: ‚ÄúYou, of course, got it there, but it would be nice to make sure <i>that there were no holes</i> .  Because otherwise we will sell, but then we will be dragged to court. ‚Äù  However, this is also not the main thing. <br></li><li>  The main criterion for the success of a project is the <i>compliance of what happened with the user's wishes</i> .  Of course, if we have a good marketing department, he can explain to the client that the result is exactly what he wants.  But in most cases, I want the client to understand this.  A lot of programmers, as it were, on a subcortex, have this in mind, but very few people directly verbalize it. <br></li><li>  Somewhere in fourth place - the <i>speed and ease of development</i> .  This convenience and not the madness of programmers.  When you talk to HR when you are hiring, they will promise all sorts of buns, massages and the like, but in fact the business doesn‚Äôt care how you live there, provided that you are still working and are not going to leave.  And that the code is written conditionally well, and not so that you want to throw yourself out of the window or go to work for another company. <br></li><li>  <i>Productivity is</i> usually even lower in the list of priorities.  Often it is not even in the criteria for success.  The product somehow moves, and thank God. <br></li></ul><br>  Therefore, I am surprised when I read posts on Java performance on Habr√© and see similar comments there: <br><br><img src="https://habrastorage.org/web/fab/042/f60/fab042f60a134f13aee7aee8e75aa1ed.png"><br><br>  Experts say: ‚ÄúWell, what about Java performance?  It works fine.  We are satisfied, everything is fine. "  But commentators come and answer: ‚ÄúIt‚Äôs very significant that none of the four experts rated Java as fast.  Rather, as sufficient and satisfying. " <br><br>  You speak as if it is bad.  If from a business point of view, technology meets business criteria, then thank God!  I have no idealistic idea that everything should be licked, smooth, perfect.  It does not happen - usually products contain errors. <br><br><h2>  Correct vs fast program </h2><br>  People have long learned some double thinking about the correctness of programs.  <i>(asks to the audience)</i> Who can honestly say that there are bugs in his programs?  <i>(in the hall a lot of hands)</i> The vast majority.  But they still believe that their programs are more or less correct.  The concept of a correct program has symmetry with the concept of a fast program. <br><br><img src="https://habrastorage.org/web/005/86c/f49/00586cf4918b4d1ba8db00a264cc00a3.png"><br><br>  "Correct program" is a program in which no user errors are visible.  That is, there are errors there, but they do not force the user to abandon the purchase of a product.  There is a symmetrical situation with the ‚Äúquick program‚Äù - it‚Äôs not that there are no performance problems, these problems just don‚Äôt beat the user every time he tries to do something. <br><br>  "We invested in success criteria."  The success criteria are both functional and performance: it‚Äôs good if the program is responsible for 100 milliseconds.  Answer?  Great, we go on. <br><br>  "The number of bugs in the correct program is usually known."  This is just one of the indicative metrics of the maturity of the project, since zero bugs in the program means that no one really cares about registering them in the bug tracker (because you don‚Äôt have users, ha ha!).  Performance problems have the same story.  Performance problems are known, and then we say that this is a ‚Äúfast‚Äù program.  <i>(makes air quotes)</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/88f/76c/9fc/88f76c9fc0c5490fa57edebcecae41f2.png"></div><br>  Both in the correct program and in the fast program, the ways to bypass these performance and functional bugs are known.  You have a FAQ that says, ‚ÄúIf you do this, it will be painful;  so <i>don't do that</i> . ‚Äù <br><br><h2>  Stages of development projects - curve them.  Sh </h2><br>  Practically all projects where I participated in performance work are undergoing some standard phases of development.  I once formulated that these phases look like this: <br><br><img src="https://habrastorage.org/web/102/f32/5f7/102f325f7dc84c3391330c60fb562164.png"><br><br>  This is a parametric graph: time flows here from point ‚ÄúA‚Äù to point ‚ÄúB‚Äù, ‚ÄúC‚Äù, ‚ÄúD‚Äù, ‚ÄúE‚Äù.  On the y-axis we have performance, on the x-axis is some abstract complexity of the code. <br><br>  Usually, it all starts with people cycling a prototype that works slowly but slowly.  It is rather complicated because we just bike or not so that it does not fall apart under its own weight. <br><br>  After the optimization begins - little by little the rewriting of different parts begins.  Being in this green zone, developers usually take profilers and rewrite pieces of code that are obviously badly written.  This at the same time reduces the complexity of the code (because you cut out bad pieces) and improves performance. <br><br>  At point ‚ÄúB‚Äù, the project reaches a certain subjective peak of ‚Äúbeauty‚Äù, when we have a kind of performance and a good one, and the product is not bad. <br><br>  Further, if developers want more performance, they go into the yellow zone when they take a more accurate profiler, write good workloads and carefully tighten the nuts.  In this process, they do things there that they would not do if it were not for productivity. <br><br>  If you want even further, then the project comes to a certain red zone, when developers begin to make their product go wild in order to get the last percentages of performance.  What to do in this zone is not very clear.  There is a recipe, at least for this conference - go to JPoint / JokerConf / JBreak and try product developers, how to write code that repeats the curvature of the lower layers.  Because, as a rule, in the red zone there are things that repeat the problems that arise in the lower layers. <br><br>  The rest of the report tells in detail about what usually happens in these zones. <br><br><h2>  Green Zone </h2><br>  The motivational card of the green zone is a fight against burrs in the code by brute force: <br><br><img src="https://habrastorage.org/web/b4d/ef0/751/b4def0751ea44f09a2273f60b3ddd07f.png"><br><br>  Brute force means that you do not need precise tools - just take and do.  There are some mental traps in the green zone. <br><br>  My favorite is: ‚ÄúProfile normally or not‚Äù: <br><br><img src="https://habrastorage.org/web/509/4c6/c20/5094c6c20aa54985aac4f0cd6f46de1b.png"><br><br>  I constantly hear: "Listen to the reports of Shipilev, he will tell you that you need to profile normally or not at all."  I never said that.  When you are in the green zone, the accuracy of diagnosis affects very little.  And, generally speaking, you need profiling in order to understand which part of the ‚Äúmicroservice monolith‚Äù that you managed to write, you need to rewrite first of all. <br><br><h2>  Profiling and diagnostics </h2><br>  If you look at the blogs of various cool performance dudes, for example, Brendan Gregg, it will show such scary diagrams, saying that you can look back and forth with such tools: <br><br><img src="https://habrastorage.org/web/9a8/946/3fc/9a89463fce074596b41fa41e1991cc1a.png"><br><br>  Many people say when they look at these diagrams: ‚ÄúPerformance is very difficult, and we will not do it.‚Äù  Although in fact the idea of ‚Äã‚ÄãBrendan is not about that.  It is that there are fairly simple ways to quickly assess what is happening in the application.  When we have at least a macrocellular understanding, we are much better. <br><br>  Therefore, diagnostics in the green zone is based on a very simple idea: we should not be afraid to look at our application.  Even a large-cell understanding of what resources we lack in our stack already gives us some idea of ‚Äã‚Äãwhat to pay attention to and what is not.  Even if you do some full-system profiling, it will already be approximately clear where your bottleneck is. <br><br><img src="https://habrastorage.org/web/fa5/796/cb4/fa5796cb416b4ca299e3971c5094ddae.png"><br><br><h2>  Profiling </h2><br>  Our goal in the green zone is to understand <i>approximately</i> where we spend time. <br><br><img src="https://habrastorage.org/web/c05/9aa/2d8/c059aa2d813743e691778065263b300e.png"><br><br>  Even a naive profiler, which you can collect from sticks and acorns, will give you enough information to perform conscious actions in the green zone. <br><br>  If you have a production, on which evil admins do not allow to install a profiler, you can simply take ssh through josh and make ‚Äúwhile true;  do jstack;  sleep 1;  done. "  Collect a thousand of these jstack, aggregate them.  This will be a ‚Äúknee-length‚Äù profiler, which will already give you enough understanding that your product is bad. <br><br>  Even if you place your hands on stopwatch in the product and estimate that in this part of the product you spend 80% of the time, and in this part - 20%, it will be better than just reading the tea leaves about what will happen if we in a class accidentally written by Vasya in 2005 will fix something. <br><br><h2>  performance measurement </h2><br>  The next mental trap associated with profiling says that you need to measure performance either normally or not at all. <br><br><img src="https://habrastorage.org/web/2de/85c/e10/2de85ce10e234b82b834695037511d6f.png"><br><br>  I constantly hear about it.  But in the green zone of improvement "plus or minus kilometer" you can usually see with your eyes.  Usually it is said in the context of ‚Äúyou have a bad benchmark, it doesn‚Äôt show anything, because it is written poorly, but you need to measure performance correctly,‚Äù and at the same time you have a rake there, which can be seen on almost any workload. <br><br><h2>  Morality </h2><br>  The moral is very simple: in the green zone, even trivial stress tests will show you major flaws. <br><br><img src="https://habrastorage.org/web/993/ad2/641/993ad26416524b909d10b6f1e537ab5f.png"><br><br>  I‚Äôve seen cases when people spend weeks trying to write load tests on JMeter, instead of putting a public link on some Twitter and get a bunch of people coming to beta testing and crashing your application (and you‚Äôll only have to sit with the profiler and watch where it fell).  Even the usual Apache Bench shows big flaws quite well. <br><br>  The sooner you are in the development of this performance data on major flaws, the sooner you can fix them.  This will all the more help you plan work on performance. <br><br><h2>  Example surprise </h2><br>  I once took JDK 9 Early Access and thought: I should try to build my projects with him, suddenly something has changed! <br><br><img src="https://habrastorage.org/web/890/7d1/58f/8907d158f76b44e3b0796442a4c0ce89.png"><br><br>  I'm building, and my compilation time is growing from 2 minutes to 8. Suddenly.  Do I need in such a situation to write some neat benchmark for this, to prove that this is really a regression? <br><br><img src="https://habrastorage.org/web/125/f0d/af9/125f0daf95e940e38ddf36b8abf134d1.png"><br><br>  Of course not.  I have a specific bug in the build, it is reproducible.  In the profiler you can find that there is a call stack that leads to a known location in javac.  In this javac you find the source code, and a quadratic loop is found there. <br><br>  Do I need to prove that this is a problem when you have: a) a workload that is bad;  b) when you have a profile that shows there, and c) theoretical reflections that a quadratic cycle is bad?  No, that's enough. <br><br><h2>  Optimization </h2><br>  Another mental trap: "Premature optimization is the root of all evil." <br><br><img src="https://habrastorage.org/web/120/6b2/9af/1206b29afc644571ad96843697950e56.png"><br><br>  The whip, of course, is still alive and well.  But I do not know how much he hiccups every time someone remembers this phrase, because it is usually remembered incorrectly.  Knut said that premature optimization is the root of all evil in 99.7% of cases, because people do not understand where they need to be optimized.  When you're in the green zone, you don't care.  You still rewrite your beautiful code.  You need profiling in order to determine what to rewrite first. <br><br>  What are the visits there? <br><br>  As a rule, the performance improvement there is mainly from rewriting the ‚Äúbad‚Äù code to ‚Äúgood‚Äù.  But ‚Äúbad‚Äù and ‚Äúgood‚Äù are to some extent subjective tastes.  Ask several programmers: one will say that it is necessary like this, so beautifully;  and the other will say: ‚ÄúWhat have you written here!‚Äù.  All this, of course, can be a taste, but it can also be suffered by techniques, including those suffered by you or Joshua Bloch, who wrote the book ‚ÄúEffective Java‚Äù. <br><br><img src="https://habrastorage.org/web/716/5fc/2f1/7165fc2f1b04474888b08187169ca820.png"><br><br>  For example, effective data structures.  I know projects in which the global s / LinkedList / ArrayList / g has improved performance without hesitation.  There are cases where LinkedList is faster, but these cases are very special, and they are usually visible to the naked eye. <br><br>  You may suddenly find that you have a linear search on an ArrayList in a place where you can use a HashMap.  Or you have an iteration over a pair of keySet and get, which can be changed to an entrySet, or bicycled your bubbleSort and suddenly it turned out that collections of a million items were coming there, and you spend a lot of time there, and so on. <br><br><h2>  Footer green zone </h2><br><img src="https://habrastorage.org/web/6f2/ab7/69f/6f2ab769f85549a3aec8b0d80d1268c5.png"><br><br>  Profiling is a necessary part of daily development. <br><br>  According to my observations, 95% of performance problems are solved at the very first profiling visits.  Regardless of the complexity of the project, no matter how experienced people develop it, when you show people the first profiling of their project, they have a gamut of emotions, in particular, "what kind of idiots are we."  Because more than 90% of these problems are trivially solvable and, in theory, should have been caught before the commit. <br><br>  If you are a technical leader, architect, technical director, or you hold some other technical leadership position and want productivity, then please make sure that the project has clear instructions on how to launch the profilers.  If you have a one-liner, or one-directory, or you have a web application that has an APM, then very well!  A developer should always have a way to do it quickly. <br><br>  In my experience, if you take a developer by the hand, no matter how ‚Äúsenior‚Äù he is, sit with them and once your product is reprofiled, this will surely stop his fear of performance work.  Many people have such a block in their heads that performance is difficult, that there are all sorts of interrelations between components and so on.  And they do not profile at all, because it is difficult - it means, for now it is not necessary.  But when once you sit down and do it with them, this block is removed from them, they begin to profile on their own.  And those 90% of the errors that they can resolve before someone comes to them, will show the profile and shame them, they will fix it in advance. <br><br>  And ‚Äútake the developer by the hand‚Äù - this does not mean that you drive them to the conference, put them in the hall for 1000 people, the speaker comes out and starts to carry something intelligently in the profiler.  It does not work.  It works differently: you sit down with a specific developer on your project and do it for a couple. <br><br><h2>  Yellow zone </h2><br>  The yellow zone is a code complication in exchange for performance when you do things that you would not do if you did not want more performance. <br><br><img src="https://habrastorage.org/web/d27/e52/297/d27e52297b1c436bad8cbd26cbc774a7.png"><br><br>  Mental traps are there too. <br><br><h2>  Profiling and diagnostics </h2><br>  The first mental trap: "Now we take a profiler, see what is where, and how we will begin to optimize." <br><br><img src="https://habrastorage.org/web/e47/c6a/41d/e47c6a41d5b84777998875364d2eca46.png"><br><br>  It turns out that in the yellow zone the price of an error has increased: now you get performance, and you lose something in return - support, developers' sleep, and so on.  Therefore, you need to make the right changes that require advanced diagnostics, and profiling is only one part of the diagnosis.  There is also benchmarking and stuff. <br><br>  Usually, when people get into the yellow zone and start thinking about what to optimize, they open the profiler and see it: <br><br><img src="https://habrastorage.org/web/e7d/4cf/df2/e7d4cfdf22d0486291373e0f10df19c2.png"><br><br>  And what are we going to optimize here?  Rewrite to java.nio or say that the hottest method is java.lang.Object.wait, which means you need to overclock it.  Or there Unsafe.park, it means that you need to overclock it ... or SocketInputStream.socketRead0, or socketAccept - it means that you need to urgently rewrite everything on Netty, because the network is visible.  True, all this garbage from JMX, but we will learn about it later, after 3 months of development.  Or there Object.hashCode - let's say that bad HotSpot did not optimize it, and ‚Äúyou promised us that everything would be quick and good, and our product is not to blame‚Äù. <br><br>  The modus operandi in the yellow zone is simple: when optimizing, you will now have to explain why you are doing it.  Maybe for yourself, and maybe for your project manager. <br><br>  It is desirable to have on hand: <br><br><ul><li>  <i>numerical estimates of growth</i> <br></li><li>  and it is advisable to have them <i>before you spend all the resources</i> .  And not when the three months of development have passed, and you said: ‚ÄúOh, you know, the task took three months, it's cool.‚Äù <br></li><li>  You must have an understanding <i>that this is the cheapest way</i> , and that this method is the one that will give you an improvement in overall performance. <br></li></ul><br><h2>  Amdal's Law </h2><br>  When people assign tasks to assess performance, the inexperienced start to freak out terribly, because it is very difficult for people to put non-linear dependencies in their heads.  One of these non-linear dependencies is the Amdal law, and it is usually arrived at as follows. <br><br>  Suppose we have an application.  It has two independent parts: A and B. And we, for example, know that part A takes 70% of the time and accelerates 2 times, and part B takes 30% of the time and accelerates 6 times.  You can overclock only one of them - there is only enough resources for this.  Which of these systems will we overclock?  If we even reduce them graphically, we see: <br><br><img src="https://habrastorage.org/web/1c0/858/6ba/1c08586ba91d4e6f88a48f38b3a38d17.png"><br><br>  Part A works at 70% of the total time.  It is better to optimize part A, despite the fact that we only overclock it by 2 times.  The impact on overall performance is greater. <br><br>  And if I were a freelance programmer, I would probably have overclocked Part B 6 times.  In my weekly report, this figure will look much better: ‚ÄúVasya dispersed twice, and I dispersed six times, so I need to increase my salary three times.‚Äù <br><br>  Amdal's law is derived as follows: <br><br><img src="https://habrastorage.org/web/72d/0d7/c3a/72d0d7c3aba14dc18c68b148fe8d92aa.png"><br><br>  If we have speedup S, then by definition it is total time A plus B divided by new time.  Part B there remains the same, so there is ‚Äúplus B‚Äù, and part A has decreased by S <sub>A</sub> times.  If we introduce two notation: Part <sub>A</sub> and Part <sub>B</sub> , which show the relative time of parts A and B in this appendix, we come to the following expression: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/67f/4b3/359/67f4b335969a47bca1ac29e91aeeb6da.png"></div><br>  This ratio has funny properties.  For example, if you go to infinity, then S is the limit: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/83e/203/4c5/83e2034c5df34e3287e51fa07400dd7a.png"></div><br>  This is not an obvious thing, you need to feel it several times on your own skin in order to understand.  You will be shown through such graphics: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/ba9/fbf/3be/ba9fbf3bed1d47dca3bd6bd3bb630443.png"></div><br>  ... and say: if you have an application in which 80% is occupied by the part that accelerates, then overclock it at least until you get stuck, but speedup is more than 5 times, you will not get. <br><br>  This means that if a database vendor comes to you and says: we know that the database occupies 50% in your workload.  We guarantee you, if you change your current solution to ours, without changing a single line of code, your performance will grow 10 times.  What do you have to tell him?  <i>(from the audience)</i> Bull shit! <br><br>  We go further: there is a generalization of the law of Amdal.  If we slightly reverse these terms and introduce two new notation: p is speedup A, this is how much we dispersed a specific part of A, and alpha is how much everything else is, then this law can be written in this form: <br><br><img src="https://habrastorage.org/web/09f/f75/5c5/09ff755c5022422187508ce8d0c05f8d.png"><br><br>  The thing is that these terms have some physical meaning.  The first member is usually called <i>concurrency</i> .  If we ignore the second term so far - the contention - expression will mean: how many times we accelerated part A, we got a common speedup at the same amount.  <i>Contention</i> describes the performance impact of everything else that provides this very asymptote in Amdal‚Äôs law.  By the way, if the graphs of this function are drawn, you get the same curves as in the Amdal law: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/3dc/5b8/257/3dc5b8257b684ca794603d86b4e80c06.png"></div><br><br>  However, as a rule, in most applications, the Amdala law in this form does not apply.  A more complex law applies when another term is added there, called <i>coherence</i> , which describes the interaction between components. <br><br><img src="https://habrastorage.org/web/6cf/169/10a/6cf16910a2e449d18d0898f209ca2efd.png"><br><br>  It turns out that if alpha and beta are non-negative, then you do not have saturation asymptotes.  You have some kind of peak efficiency, and after that, performance starts to fall.  Many people who are engaged in performance, on their own skin felt this law, until it was formulated as the Universal Scalability Law (USL): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7c0/6c0/0c2/7c06c00c2615e50c4dfed5892a6d5fc3.png"><br><br>  The concepts of "universal" and "law" are used here in a natural-scientific sense, that is, it has a theoretical justification, but it is not derived analytically.  It is derived as a law that stretches well on empirical data. <br><br>  Systems with alpha and beta, equal to zero, does not exist, because there are many cases when you add resources to the system or optimize something, and it gets worse.  For example, you removed one bottleneck, but rested against another, heavier one. <br><br><h2>  performance measurement </h2><br>  A mental trap with performance measurement says: ‚ÄúSince everything is complicated there, and we don‚Äôt know how everything will affect performance, we just find something and see what benchmarks tell us.‚Äù <br><br><img src="https://habrastorage.org/web/64d/6bb/15a/64d6bb15aa8d4f7b8ba6c931ad0dbfe0.png"><br><br>  In fact, performance testing is wildly expensive, and you‚Äôll not test it all.  Performance tests from functional tests differ in that functional (for example, unit tests) pass in 100 milliseconds or less, even if they are executed in a pack.  With performance tests not everything is so smooth.  There, tests pass from a minute and up, can take hours.  This means that one change can be tested for hundreds of engine hours.  If you make a bunch of commits a day, then you need a very large fleet of equipment in order to ensure at least some way to pass through the test system. <br><br>  Performance tests require isolation, otherwise they begin to speak every kind of heresy, because they begin to interact with each other. <br><br>  Performance tests, as a rule, give non-binary metrics.  Functional tests usually say "PASS" or "FAIL" - binary metrics, and performance tests say ... "67".  Worse: they say not ‚Äú67‚Äù, but ‚Äú67 plus or minus 5‚Äù.  This, among other things, means that testing errors are <i>only after parsing the data</i> , when you understand that everything is very beautiful everywhere, and here, in a dark corner, there are data that show that the experiment was fake.  This means that all other data also need to be thrown away and again spend hundreds of machine hours on a new cycle. <br><br>  And the worst thing is that the benchmarks give you data ‚Äî <i>numbers</i> , but you want <i>results</i> ‚Äî some knowledge you can get from these numbers.  And if data acquisition is well automated, then the part about extracting meaning from this data, as a rule, is poorly automated, since it requires a human understanding of what is there and how it can occur. <br><br>  Conclusion - in the active project it is almost impossible to test everything.  This means that you still need to have in your head some assessments and the ability to figure out where to dig, without chasing away all the benchmarks for every sneeze. <br><br><h2>  Benchmark classification </h2><br>  Many people divide benchmarks into two large classes: macrobench marks and microbench marks. <br><br><img src="https://habrastorage.org/web/58f/25e/fd1/58f25efd186d41a6983aeb556dff42f5.png"><br><br>  In macrobench marks, as follows from the ‚Äúmacro‚Äù prefix, we take our large website, application or library as a whole, write end-to-end tests and see how long it took one virtual user to buy our infinitely valuable products in the online store .  Normal test. <br><br>  Microbench marks are small workloads when we take a small part of our website or library, do a small isolated test and measure a specific part. <br><br>  The developer‚Äôs voices in his head usually whisper to him that the macrobenchmark reflects the real world, and the microbench marks reflect evil. <br><br>  Look: the real world is big, and the macrobenchmark is big.  <i>(conspiratorially)</i> Therefore, the macrobenchmark reflects the real world.  Therefore, it seems to some that any macrobenchmark is a good one.  Say, launched a macro benchmark - this is the real world.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This means that for any cool feature, the macrobenchmark will give a dramatic improvement, because it is also a ‚Äúreal world‚Äù. And if the macrobenchmark does not show improvements, then this feature is bad. And that for any cool bug, the macrobenmark will give a steep regression, and if there is no regression, then there is no bug. This is a ‚Äúreal-world‚Äù, which means that what happens in the real world should also be in the macro benchmark.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With microbench marks - the opposite situation. </font><font style="vertical-align: inherit;">Voices in the developer's head suggest that they are evil, so they can be ignored. </font><font style="vertical-align: inherit;">They tell you that the readings of the microbenchmark are not important, because you can write it in any way, which means that regression or improvement on it means nothing. </font><font style="vertical-align: inherit;">And when they encounter the real world, the thought comes that the microbenchmarks are written by enemies to defame their product. </font><font style="vertical-align: inherit;">Because it is easy to write them, you can write whatever you like, which means you can write a microbenchmark, thanks to which we later write in white paper that our product is better, because </font><font style="vertical-align: inherit;">at such a workload, it works better.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Benchmark life cycle </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If we discard this marketing bull shit about microbench marks, it turns out that the life cycle of all benchmarks are about the same. </font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/4e4/6eb/b76/4e46ebb76a7c4cdaa2df02f24c68e86e.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you have a big benchmark - gigabytes of code, in which dofig libraries, code, etc., it usually begins with the "innocent" stage, when it really tests many parts of your product, walks through many layers, and so on. </font><font style="vertical-align: inherit;">Suppose they take about the same time there. </font><font style="vertical-align: inherit;">But then evil developers come and start optimizing.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/e9e/64c/226/e9e64c22603045e3b33b9d9800590720.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We started with a macrobenchmark that covers many parts of the application, but at the end of its life cycle, its performance is stuck in one big bottleneck, which is very difficult for us to resolve. </font><font style="vertical-align: inherit;">And this is practically the case of a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">micro</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> benchmark, when we have an isolated test that tests one thing. </font><font style="vertical-align: inherit;">Such a degenerated macrobenchmark also tests only one thing. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But still this has some advantages. </font><font style="vertical-align: inherit;">Unobvious plus follows from the law of Amdal.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/4a5/011/683/4a501168302d48cba612db9dad680724.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Take an application with two parts - red and green, which is optimized. Even if we optimize the green part to 0, we still get the speedup only twice. But if the red part regresses (if we, say, 2 times regress it), it turns out that Amdal‚Äôs law, which has an asymptote, will become a linear relationship.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/fe5/da9/fe8/fe5da9fe842546198dd1b0bf0c093865.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In other words, if we regressed a small part a thousand times, the total metric will also decrease significantly. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can show it on the graph as follows: </font></font><br><br><img src="https://habrastorage.org/web/726/4f7/d3c/7264f7d3ca3c47e687d007ca416e6ac8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the abscissa axis we have the speedup of a specific part, and on the ordinate axis the speedup of the product as a whole. In the part where something is improving, we have asymptotes from Amdal‚Äôs law, and where everything is getting worse, we have a practically linear relationship. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The problem with testing is also that empirical performance testing is noisy. There are systematic and random errors in the experiments.</font></font><br><br><img src="https://habrastorage.org/web/48f/e2b/6b0/48fe2b6b05b04e5692adfb0aa58dec4b.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Often, the sensitivity of the test may simply not show you an improvement on a particular change. </font><font style="vertical-align: inherit;">If something has improved 50 times, then the improvement in the overall system will be below the sensitivity threshold of the test, and if it regressed 50 times, it will be much better seen. </font><font style="vertical-align: inherit;">This is, by the way, the answer to the question of why, when you are engaged in productivity, most of the things that you see in your automatic reports are regression. </font><font style="vertical-align: inherit;">Because, as a rule, testing is more sensitive to regression, and not to improvement. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">According to my observations: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Macrobenchmarks:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> They are few, they are written incomprehensibly by whom and it is not clear how; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> first show interesting results, and then degenerate into microbench marks; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> at the end of life is still applicable for regression testing. </font></font><br></li></ul><br>  Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Making a performance improvement on macro brands is a heartbreaking story. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Microbench marks:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There are too many of them, they are also written incomprehensibly as well as incomprehensible by whom, but they are so small that they can be embraced and corrected with one mind: understand what is wrong there and somehow modify them; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> at all stages of the life cycle, they show interesting results; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> respond well to regression and improvement. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, if we, for example, do performance optimization, we always have a body of micro-benchmarks (very sensitive workloads), on which we make changes and squeeze the maximum. </font><font style="vertical-align: inherit;">Only then we validate that they have not regressed and improved the macrobenchmark.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No matter how you spin, and you have to learn microbenchmark. The most interesting idea is that the ‚Äújust take some successful framework and rewrite all workloads on it‚Äù method does not work, because it is not enough just to write the benchmark code and get the numbers. To understand what is happening, you need to peer into these numbers, maybe build side experiments, correctly conduct them and analyze the data, and then draw conclusions and build a so-called performance model ‚Äî understand how your application or your stack reacts to changes in code, configuration, and so on. This will give you the opportunity to predict what will happen in the future.</font></font><br><br><h2>  Optimization </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Usually, however, is not the case. Usually, people fall into the next mental trap: </font></font><br><br><img src="https://habrastorage.org/web/b1f/2cd/95a/b1f2cd95a97a42abb6de5790b22596ac.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚ÄúWe tried, and it improved the metrics - it was not 100 operations per second, but 110. Probably because ...‚Äù and further the link to some report with JPoint follows. I, he said, was at a conference, and there a smart dude said that I could step on such and such a plunder. As a result, we rewrote the multiplication for non-multiplication, and we had a branch prediction or something in this spirit (here, more importantly, in a scientific manner, to be more accurate). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, it often happens that you are just lucky with the load, with the component, with the phase of the moon. A little luck will be gone, everything will be back to square one. As a rule, in relation to each improvement, you need to look at the following options (if I have an improvement, you need to understand what it refers to):</font></font><br><br><ol><li>     , ,   ‚Äî    N^3,      N log N.      ,     ,   , ,        ,      ; <br></li><li>         ,      ,  ,  ,  ,  ,     . ,         PR       ,    ; <br></li><li>         .          ,    ,    ,     -; <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Or it is an incorrigible library jamb or runtime, but to migrate from this library or runtime is very dear to us. </font><font style="vertical-align: inherit;">Then we make a permanent patch, we enter into our annals of technical logs, which is the only way to do this.</font></font><br></li></ol><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we do not classify our changes into these categories, then it may be that we make a bad temporary patch against a bug in the lower layer, that bug will sometime be fixed, and fixes in the regular library will be better than our patch. </font><font style="vertical-align: inherit;">But we will already pay for the maintenance of the patch, the failed performance in comparison with the competitors that did not twitch, etc.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> JVM options </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Another example: what will happen if we know what is special in our application, we will tell the JVM what mode to work in? </font><font style="vertical-align: inherit;">The joys are very simple there: mechanical sympathy, synergy and all that (we know how to tune the JVM). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In practice, of course, this happens differently. </font><font style="vertical-align: inherit;">You go to Google and search for: ‚ÄúJVM tuning options‚Äù. </font><font style="vertical-align: inherit;">You fall out this ski:</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/f22/db8/1e4/f22db81e4f634ed88499105d7d4cc267.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You put it in your config, it really improves something, and you draw yourself an asterisk ‚ÄúI know how to tying JVM‚Äù. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, there are problems. </font><font style="vertical-align: inherit;">It may be that, in general, this unit has improved your performance, but there are options, which, among other things, regressed. </font><font style="vertical-align: inherit;">If you understood what you are doing, you could take the whole improvement, not just a part of it.</font></font><br><br><h2>  Parallelism </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No matter what we have written where we are, we will take parallelStream (), Eexecutor.submit, new Thread, we will load a bunch of parallel tasks. Normal idea? The joy here is that special changes in the code are usually not necessary if it was originally written with the thought that someday it will be multi-threaded.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But there are the same problems: there is synchronization, and it is unclear how she will shoot on wider wheelbarrows. There are overheads - on small tasks you do not need to parallelize, because you all will eat on dispatching tasks. Or if you have a staging in which you have a lonely Vasya pokes in the mold, and there internal parallelism helps you. Well, for example, there is some kind of request, and you are there inside it submitting to some mega-Hadoop that MapReduce is doing to you in parallel; and it all works faster when everything is parallel. And then you deploy it to production, and suddenly it turns out that you have 10,000 users, i.e. internal parallelism is not needed at all - you have a lot of external parallelism that already exploits all your resources.</font></font><br><br><h2>  Data structures </h2><br>  Another example.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Some people think: "At conferences, they told me that, in general, Integer and generally wrappers over primitives in Java are an expensive thing, so we take and rewrite everything into int arrays and so on." </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, ‚Äúint-ovka is a holiday‚Äù in the sense that you take, rewrite and think that you are performing useful actions. </font><font style="vertical-align: inherit;">The problems, however, are very big: you do not know in advance how much you will eat on conversions to wrappers, how much time you will devour development time (working out all angular cases, round-trip conversion of all collections, writing inserts, deletions, brakes), and you will probably miss the optimization of the JDK itself in the collections and in some Valhalla.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Subtotal for the yellow zone </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Modus operandi: in the yellow zone, our main task is to maintain a plausible performance model of the project, because it tells us where to change, so that it is good and we do not regret about it later. This is a necessary condition for the development of the project. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">According to my observations, more than 50% of potential changes are not done where they are. And they usually miss for a very simple reason: commands are broken down into components, and they can quickly make changes to their component, even if you need to make a bugfix in the component of another command. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As a rule, more than 80% of changes are made in the right place after the study (and 83% of all statistical data are usually correct).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The ability to explore and update your knowledge of the project entirely helps to make the right decisions. </font><font style="vertical-align: inherit;">If your teams are divided by components, please have people who can look at this project from the outside, who will understand how the components are related to each other and what changes to make. </font><font style="vertical-align: inherit;">Usually this role is performed by the architect. </font><font style="vertical-align: inherit;">But architects are usually overwhelmed, so they hire performers who help feed them with data about what is happening in the project. </font><font style="vertical-align: inherit;">Thus, it is possible to help the architect in his head to update this very performance model.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Red zone </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Motivation in the red zone: when we chose all reasonable ways to improve performance, there were unreasonable ways. </font><font style="vertical-align: inherit;">Unreasonable ways - this is the exploitation of the curvature of the lower layers, dirty khaki, climbing into the intestine. </font><font style="vertical-align: inherit;">As a rule, in the absolute majority of projects there is no need to go there.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/08f/392/798/08f392798e1242449cb5c66afebeea8a.png"></div><br>  Competent technical management, sane tehlide, project manager or, in the end, caring mother should tell you: ‚ÄúYou don‚Äôt have to go there‚Äù! <br><br>  The basic idea in the red zone is simple - we want to improve performance by exploiting the implementation features in the lower layers. <br><br><img src="https://habrastorage.org/web/62e/e6e/aab/62ee6eaab8cf46348b362b65af86e761.png"><br><br>  For example: <br><br><ul><li>  we find that in a library that we really like, there is a hidden private method that does something faster.  Typical hack: we will take this method through reflection and pull; <br></li><li>  or we honor the library's course and find out that if we pull public methods in some special order (I call API API acupuncture), then we will translate this object into a state that is more acceptable to us - it will probably work faster; <br></li><li>  or we take and just zakhachim whole pieces of the private API; <br></li><li>  or we will begin to exploit the features of the specific glands on which we execute, the specific JDK that we target - i.e.  do low-level optimizations. <br></li></ul><br><br><h2>  Profiling and diagnostics </h2><br><br>  Usually everything happens in this zone by accident.  When all possible options are chosen, they say to themselves: ‚ÄúWell, let's think what to do next.‚Äù  And it seems to them that if you look at a profile for a long time, then you can see a solution there.  They understand that the decision will be difficult, so you need to look for a long time. <br><br><img src="https://habrastorage.org/web/c3a/f24/829/c3af24829cc6468bae8c6dca905ada4e.png"><br><br>  In fact, the ability to hack in the red zone grows out of understanding the interaction of moving parts.  And here in this place performances appear - these are people who are not particularly engaged in everything else, but they know how to dig in all layers at once.  They are a walking encyclopedia, they have in their head a set of hacks that work in such and such conditions.  They know the limits of their applicability.  The most important thing is that these are people who do not fall into a stupor when they see unfamiliar crap, but begin to study it. <br><br>  If they see a profile from which nothing is clear, they say: ‚ÄúWell, all right, we will reject this profile, let's do experiments that will be brought to us by some new data‚Äù.  The systematic inductive way of data collection works here.  Studying for them does not mean nafigachit a piece of code, posting it on StackOverflow and asking: ‚ÄúDo we have such a performance problem, what could it be?‚Äù And wait until some people come to them, such as John Skit, and will tell how and what to do there.  To study means to read documentation, look for references in articles, do experiments, extract knowledge from colleagues, systematize them in some way, and so on.  Many people come to the conference for this. <br><br><h2>  Conference Tricks </h2><br>  There is one more mental trap: ‚ÄúIf you travel a long time to conferences, someday some conference will be told a trick, which in our project will give an improvement five times, and then we will beat off the entire budget spent on tickets.‚Äù <br><br><img src="https://habrastorage.org/web/3cf/0a7/fc2/3cf0a7fc2a0b46efb492261b911c82ce.png"><br><br>  In fact, in a particular case, one out of a thousand low-level trick saves.  It is easier to find on your own than to wait for the weather by the sea.  The funny thing I see in the programmer get-together is that most of the performance discussions are about low-level hacks: <br><br><img src="https://habrastorage.org/web/155/4cc/4f5/1554cc4f5ed24028b93bc03b61b93ddc.png"><br><br>  As a rule, all these things happen in conditions when programmers in projects have simple <i>algorithmic</i> transformations that can repeatedly improve performance.  But since they were told at conferences that smart HotSpot somehow compiles, so you need to modify your project so that it compiles well with HotSpot, they miss the main thing - that in fact they have changes that they can find for themselves (and make it is fast) and it will improve performance. <br><br><h2>  Crutches </h2><br>  If we are in the red zone and find this kind of low-level difference, which in fact in most cases is the difference in the implementation of optimizers (many of the above-mentioned cases actually have to work the same way), we can zakhachit.  But hacks should be temporary patches, not the basis of our project. <br><br><img src="https://habrastorage.org/web/056/817/618/05681761873848298cb0145d77689fe3.png"><br><br>  As a rule, they indicate problems in the layer below, since the layer below should actually provide a more sane performance model in which any option will work better.  If you listen to the report, in which you will be told about different low-level game, please do not take it as a guide for immediate action.  This report helps you to build a corpus of possible hacks, which, maybe, someday will be needed if strongly pripret. <br><br><h2>  Example </h2><br>  There is a very simple example that was recently found in JDK 9. <br><br><img src="https://habrastorage.org/web/e99/7d4/2d4/e997d42d48894032b1ba374015a017c6.png"><br><br>  There is an ArrayList type in Java, it has an iterator that is implemented through the inner class Itr.  This class Itr, since it is private, has a synthetic so-called bridge method (public), so that the Itr constructor can be called.  The problem is that if we call Itr, the Hotspot inliner breaks down because it sees classes that are not yet loaded in the signature of this method. <br><br>  Where is this error (under-optimization)?  Is this a bug in the ArrayList code?  Probably not.  Is this a javac error?  Yes, rather.  Is this under-optimization in the hotspot code?  Yes, rather.  But it's easier to zachachit it directly in ArrayList, until compilers come to their senses and do not understand how to fix this in the compiler. <br><br><img src="https://habrastorage.org/webt/59/cb/80/59cb80b0c8395999064699.jpeg"><br><br>  Therefore, such a constructor without arguments is made there, a reference to the bug is written there, why this constructor is made, etc. <br><br><h2>  Correction Approaches </h2><br>  When you dig for a long time in the red zone, people usually come to you and say: "Let's quickly wag, and everything will be hurt."  The mental trap is very simple: they really think that if we quickly turn around with some kind of hack, it will be great. <br><br><img src="https://habrastorage.org/web/19e/550/77e/19e55077e6954bdd9ea60d3e545cdb2d.png"><br><br>  Practice shows that in the red zone <i>"zashib" will never happen</i> . <br>  As an example (I see it all the time): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/1e6/0d9/aa8/1e60d9aa84b94b6e8e80cdd0cfd63cd9.png"></div><br>  We found a performance problem in the JDK.  Instead of fixing it in the JDK, we said, ‚ÄúWell, no.  We are through Unsafe hacking.  And then it turns out that Unsafe is gone, and even setAccessible () does not work.  So do not be fooled.  Working in the red zone, you make a technical debt. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/d73/c7a/3ee/d73c7a3ee9144d77bd60c7dbef5f9487.png"></div><br>  Always document what the hack is done for, under what conditions we apply, how to check that it is no longer needed, what upstream bugs you are waiting for and so on. <br><br>  I was on the lessons OBZH at school was taught to impose a tourniquet.  I still remember this instruction, that you impose this tourniquet above or below the wound, depending on what color the blood gushes.  But I remember because of a separate paragraph of the instruction: after you put a tourniquet on the body of your comrade, you should put a note there, with a timestamp. <br><br>  Every time when I do such a hack in our code, this story is in my head: I put a harness on a sore spot in my code and have to write there that it was done then for such a reason, because there is - a bug, and this harness can be removed only if such conditions are met.  This makes it possible for me or my other colleagues, when they discovered this code, to understand whether it is worth cutting it off right away, or if you take off the burn, it will kill the rest of the project (or your comrade). <br><br><h2>  Subtotal in the red zone </h2><br>  I do not forbid you to do hacks.  It would be foolish to admonish about this, you will still do them, because sometimes it is different in any way.  But keeping the number and density of hacks in your project is a necessary condition for its survival. <br><br>  The ability to work with upstream (if you, for example, are engaged in open source, commit your changes to the repository above) greatly facilitates your long-term fate.  Because the more your inner patchwork covered product disperses from upstream, the more expensive you will support it. <br><br>  The ability to understand all these layers successfully trains ‚Äúon cats‚Äù in the sense that you must have a staging environment on which you can hack and make all sorts of non-trivial changes. <br><br><h2>  Parting words </h2><br>  But here we are at a hardcore conference.  What do we do with all this wealth? <br>  If you are at a hardcore conference and you are a user of products, then from my point of view, as a person who is engaged in performance, you should be here: <br><br><ul><li>  update the stock of information on the applicability of hacks from the red zone.  It‚Äôs not that you start recording right now what hacks from the red zone you should introduce into your production, but simply update this dictionary; <br></li><li>  find accomplices and discuss work approaches in yellow.  How to more accurately profile and monitor, and so on; <br></li><li>  or discuss what receptions from the yellow zone to bequeath to the green zone, i.e.  which things that you considered to be a performance-to-performance exchange are now simply standard code-writing practices. <br></li></ul><br>  If you are a product developer, then: <br><br><ul><li>  to help their users to discuss approaches in the yellow zone - how to profile, how to monitor, and so on; <br></li><li>  and understand which hacks from the red zone, which people constantly face, should be dragged either into the yellow or into the green zone. <br></li></ul><br><br>  Please remember that the main performance work takes place in the green zone.  Every time you are involved in project performance, first look at simple things.  It is they who will give you more performance exhaust.  Nobody will talk about them at the conference, because it‚Äôs boring.  You will usually be told about the yellow or red zone, because it is cool and raises the speaker in his eyes.  About how someone heroically changed LinkedList to ArrayList, usually no one will talk, because it's somehow embarrassing ... (end) <br><br><hr><br>  At Joker 2017, Aleksey Shipilev will give a new talk on ‚ÄúShenandoah: a garbage collector that could (part 2)‚Äù and talk about the problems that a low-level GC has to face like Shenandoah, and what can be done with them at the JVM level.  The conference will be held on November 3 and 4 in St. Petersburg.  The program and announcements of other reports are on the <a href="https://jokerconf.com/">official website of the event</a> . </div><p>Source: <a href="https://habr.com/ru/post/338732/">https://habr.com/ru/post/338732/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../338718/index.html">Understanding the new sync.Map in Go 1.9</a></li>
<li><a href="../338720/index.html">Use PubNub: Emotional Talking Chat Do It Yourself</a></li>
<li><a href="../338722/index.html">The second version of the Air Quality Monitor</a></li>
<li><a href="../338724/index.html">How to build a self-managed business: formulating the ‚Äúlaws of robotics‚Äù Hamster Marketplace</a></li>
<li><a href="../338730/index.html">IT events digest for October</a></li>
<li><a href="../338734/index.html">Dump memory and write maphack</a></li>
<li><a href="../338736/index.html">How to solve the perennial conflict between development and operation?</a></li>
<li><a href="../338740/index.html">The results of the summer internship 2017 in Digital Security. Department of research</a></li>
<li><a href="../338742/index.html">Hackathon from ABBYY</a></li>
<li><a href="../338744/index.html">Reamp library: pain killer for your Android applications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>