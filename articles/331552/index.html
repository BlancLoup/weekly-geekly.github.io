<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autoencoders in Keras, Part 3: Variational autoencoders (VAE)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 


- Part 1: Introduction 
- Part 2: Manifold learning and latent variables 
- Part 3: Variational autoencoders ( VAE ) 
- Part 4: Conditional...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autoencoders in Keras, Part 3: Variational autoencoders (VAE)</h1><div class="post__text post__text-html js-mediator-article"><h3>  Content </h3><br><ul><li>  Part 1: <a href="https://habrahabr.ru/post/331382/">Introduction</a> <br></li><li>  Part 2: <a href="https://habrahabr.ru/post/331500/"><em>Manifold learning</em> and <em>latent</em> variables</a> <br></li><li>  <strong>Part 3: Variational autoencoders ( <em>VAE</em> )</strong> <br></li><li>  Part 4: <a href="https://habrahabr.ru/post/331664/"><em>Conditional VAE</em></a> <br></li><li>  Part 5: <a href="https://habrahabr.ru/post/332000/"><em>GAN</em> (Generative Adversarial Networks) and tensorflow</a> <br></li><li>  Part 6: <a href="https://habrahabr.ru/post/332074/"><em>VAE</em> + <em>GAN</em></a> <br></li></ul><br>  In the <a href="https://habrahabr.ru/post/331500/">last part,</a> we have already discussed what hidden variables are, looked at their distribution, and also understood that it is difficult to generate new objects from the distribution of hidden variables in ordinary autoencoders.  In order to be able to generate new objects, the space of <em>hidden variables</em> ( <em>latent variables</em> ) must be predictable. <br><br>  <strong><em>Variational Autoencoders</em></strong> ( <em>Variational Autoencoders</em> ) are autoencoders that learn to map objects into a given hidden space and, therefore, sample them.  Therefore, <em>variational autoencoders are</em> also referred to the family of generative models. <br><br><img src="https://habrastorage.org/web/725/94b/5de/72594b5de85e4e58a0ae071bf2ab2ca7.png"><br><a name="habracut"></a><br>  Illustration of <strong><em>[2]</em></strong> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Having any one distribution <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  can get any other <img src="https://habrastorage.org/getpro/habr/post_images/769/287/e45/769287e45170be874760fef3fda50ab1.svg" alt="X = g (Z)">  for example, let <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  - normal normal distribution, <img src="https://habrastorage.org/getpro/habr/post_images/5ad/0fe/617/5ad0fe61711189fc76e7937ddb1cd01b.svg" alt="g (Z) = \ frac {Z} {| Z |} + \ frac {Z} {10}">  - also random distribution, but it looks completely different <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns Z = np.random.randn(<span class="hljs-number"><span class="hljs-number">150</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) X = Z/(np.sqrt(np.sum(Z*Z, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>))[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>]) + Z/<span class="hljs-number"><span class="hljs-number">10</span></span> fig, axs = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, sharex=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>)) ax = axs[<span class="hljs-number"><span class="hljs-number">0</span></span>] ax.scatter(Z[:,<span class="hljs-number"><span class="hljs-number">0</span></span>], Z[:,<span class="hljs-number"><span class="hljs-number">1</span></span>]) ax.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) ax.set_xlim(<span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>) ax.set_ylim(<span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>) ax = axs[<span class="hljs-number"><span class="hljs-number">1</span></span>] ax.scatter(X[:,<span class="hljs-number"><span class="hljs-number">0</span></span>], X[:,<span class="hljs-number"><span class="hljs-number">1</span></span>]) ax.grid(<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) ax.set_xlim(<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) ax.set_ylim(<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br></div></div><br><img src="https://habrastorage.org/web/fac/4db/fc6/fac4dbfc6dac4e14a89b4eea7607af99.png"><br><br>  Example above from <strong><em>[1]</em></strong> <br><br>  Thus, if you select the right functions, you can map the hidden variable spaces of regular autoencoders into some good spaces, for example, those where the distribution is normal.  And then back. <br><br>  On the other hand, it is not necessary to specifically study how to display some hidden spaces in others.  If there are any useful hidden spaces, then the correct autoencoder will learn them along the way itself, but ultimately it will display the space we need. <br><br><hr><br>  Below is a challenging, but necessary theory underlying the <em>VAE</em> .  I tried to squeeze out <strong><em>[1, Tutorial on Variational Autoencoders, Carl Doersch, 2016]</em></strong> all the most important <strong><em>things, dwelling</em></strong> in more detail on those places that seemed difficult for me. <br><br>  Let be <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  - hidden variables, and <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  - data.  Using the example of drawn numbers, consider the natural generative process that generated our sample: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b7/1b8/402/2b71b8402b0db65c99162f68c8b918d3.svg" alt="P (X) = \ int_ {z} P (X | Z) P (Z) dZ"></div><br><ul><li><img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  probability distribution of images of figures in the pictures, i.e.  the probability of a particular image of a digit is in principle to be drawn (if the picture is not like a digit, then this probability is extremely small, and vice versa), <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  - the probability distribution of hidden factors, for example, the distribution of the thickness of the stroke, <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/7d4/1ae/4ed/7d41ae4ed44b2e23789bd2a370daea77.svg" alt="P (X | Z)">  - the probability distribution of pictures for given hidden factors, the same factors can lead to different pictures (the same person in the same conditions does not draw absolutely identical numbers). <br></li></ul><br>  Imagine <img src="https://habrastorage.org/getpro/habr/post_images/7d4/1ae/4ed/7d41ae4ed44b2e23789bd2a370daea77.svg" alt="P (X | Z)">  as the sum of some generating function <img src="https://habrastorage.org/getpro/habr/post_images/bc1/12d/78f/bc112d78f43d281ebb9e846d5d4512f4.svg" alt="f (z)">  and some complicated noise <img src="https://habrastorage.org/getpro/habr/post_images/272/6bf/a03/2726bfa03e335f630171c1eb09b04019.svg" alt="\ epsilon"><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/330/941/00c/33094100c444c70dfa4a6e7a0ccf0cd6.svg" alt="P (X | Z) = f (Z) + \ epsilon"></div><br>  We want to build some artificial generative process that will create objects that are close in some metric to training <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/89a/576/e4d/89a576e4d822a13c63bb70e0477ea52b.svg" alt="P (X; \ theta) = \ int_ {z} P (X | Z; \ theta) P (Z) dZ \ \ \ (1)"></div><br>  and again <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3c4/3ac/62d/3c43ac62d0da8f0dd36d0b02633330de.svg" alt="P (X | Z; \ theta) = f (Z; \ theta) + \ epsilon"></div><br><img src="https://habrastorage.org/getpro/habr/post_images/8c0/781/62c/8c078162c4201db79ea0a1956baa832c.svg" alt="f (Z; \ theta)">  - some family of functions that our model represents, and <img src="https://habrastorage.org/getpro/habr/post_images/84a/640/df9/84a640df942df02ae3cff21ed52146ec.svg" alt="\ theta">  - its parameters.  Choosing a metric, we choose what kind of noise seems to us. <img src="https://habrastorage.org/getpro/habr/post_images/272/6bf/a03/2726bfa03e335f630171c1eb09b04019.svg" alt="\ epsilon">  .  If metric <img src="https://habrastorage.org/getpro/habr/post_images/446/5e5/b18/4465e5b1814e664ed806a987abd04819.svg" alt="L_2">  then we consider the noise as normal and then: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ea/1f7/757/5ea1f7757aebbb7e7ad8fa1956c502d4.svg" alt="P (X | Z; \ theta) = N (X | f (Z; \ theta), \ sigma ^ 2 I),"></div><br>  According to the maximum likelihood principle, it remains for us to optimize the parameters <img src="https://habrastorage.org/getpro/habr/post_images/84a/640/df9/84a640df942df02ae3cff21ed52146ec.svg" alt="\ theta">  in order to maximize <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  i.e.  the likelihood of objects from the sample. <br><br>  The problem is that we cannot directly optimize the integral (1) directly: the space can be high-dimensional, there are many objects, and the metric is bad.  On the other hand, if you think about it, then to each specific <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  can only result in a very small subset <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  for the rest <img src="https://habrastorage.org/getpro/habr/post_images/7d4/1ae/4ed/7d41ae4ed44b2e23789bd2a370daea77.svg" alt="P (X | Z)">  will be very close to zero. <br>  And with optimization it is enough to sample only good ones. <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  . <br><br>  In order to know which <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  we need to sample, we introduce a new distribution <img src="https://habrastorage.org/getpro/habr/post_images/ad3/da4/b09/ad3da4b09a7020394e1fbb061983b9a1.svg" alt="Q (Z | X)">  which depending on <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  will show the distribution <img src="https://habrastorage.org/getpro/habr/post_images/2d9/28e/e18/2d928ee1809930c655ae518cda1890e3.svg" alt="Z \ sim Q">  which could lead to this <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  . <br><br>  We first write the Kullback-Leibler distance (an asymmetric measure of the "similarity" of two distributions, for more details <strong><em>[3]</em></strong> ) between <br><img src="https://habrastorage.org/getpro/habr/post_images/ad3/da4/b09/ad3da4b09a7020394e1fbb061983b9a1.svg" alt="Q (Z | X)">  and real <img src="https://habrastorage.org/getpro/habr/post_images/f27/682/f2e/f27682f2e805f9e6d8f5d0f777275f96.svg" alt="P (Z | X)">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/43c/531/ddb/43c531ddb17a769031acfe1d57bdb491.svg" alt="KL [Q (Z | X) || P (Z | X)] = \ mathbb {E} _ {Z \ sim Q} [\ log Q (Z | X) - \ log P (Z | X)]"></div><br>  We apply the Bayes formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/930/3dd/de4/9303ddde467987ba7289e4695ba03b28.svg" alt="KL [Q (Z | X) || P (Z | X)] = \ mathbb {E} _ {Z \ sim Q} [\ log Q (Z | X) - \ log P (X | Z) - \ log P (Z)] + \ log P (X)"></div><br>  Select another Kullback-Leibler distance: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e40/62f/4ff/e4062f4ffaef2a6b6dafdcb7712ce0ad.svg" alt="KL [Q (Z | X) || P (Z | X)] = KL [Q (Z | X) || \ log P (Z)] - \ mathbb {E} _ {Z \ sim Q} [\ log P (X | Z)] + \ log P (X)"></div><br>  As a result, we obtain the identity: <br><br><hr><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6a9/4f3/a68/6a94f3a6831a88bf300ed100387ac0e6.svg" alt="\ log P (X) - KL [Q (Z | X) || P (Z | X)] = \ mathbb {E} _ {Z \ sim Q} [\ log P (X | Z)] - KL [ Q (Z | X) || P (Z)]"></div><br><hr><br>  This identity is the cornerstone of <em>variational autoencoders</em> , it is true for any <img src="https://habrastorage.org/getpro/habr/post_images/ad3/da4/b09/ad3da4b09a7020394e1fbb061983b9a1.svg" alt="Q (Z | X)">  and <img src="https://habrastorage.org/getpro/habr/post_images/6be/1da/243/6be1da243ff61a19717fc32a74cd4637.svg" alt="P (X, Z)">  . <br><br>  Let be <img src="https://habrastorage.org/getpro/habr/post_images/ad3/da4/b09/ad3da4b09a7020394e1fbb061983b9a1.svg" alt="Q (Z | X)">  and <img src="https://habrastorage.org/getpro/habr/post_images/7d4/1ae/4ed/7d41ae4ed44b2e23789bd2a370daea77.svg" alt="P (X | Z)">  depend on parameters: <img src="https://habrastorage.org/getpro/habr/post_images/407/7a2/584/4077a2584a945c686c59013deee10772.svg" alt="Q (Z | X; \ theta_1)">  and <img src="https://habrastorage.org/getpro/habr/post_images/7d5/7fb/6a2/7d57fb6a26c7f4401cd2d15445fd5792.svg" alt="P (X | Z; \ theta_2)">  , but <img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  - normal <img src="https://habrastorage.org/getpro/habr/post_images/981/05d/016/98105d016f10d5a71b901afe90ec8fac.svg" alt="N (0, I)">  then we get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bfc/222/82b/bfc22282b6adda3042f7b997aef20cc3.svg" alt="\ log P (X; \ theta_2) - KL [Q (Z | X; \ theta_1) || P (Z | X; \ theta_2)] = \ mathbb {E} _ {Z \ sim Q} [\ log P (X | Z; \ theta_2)] - KL [Q (Z | X; \ theta_1) || N (0, I)]"></div><br>  Let's take a closer look at what we did: <br><br><ul><li>  First of all, <img src="https://habrastorage.org/getpro/habr/post_images/407/7a2/584/4077a2584a945c686c59013deee10772.svg" alt="Q (Z | X; \ theta_1)">  , <img src="https://habrastorage.org/getpro/habr/post_images/7d5/7fb/6a2/7d57fb6a26c7f4401cd2d15445fd5792.svg" alt="P (X | Z; \ theta_2)">  Suspiciously similar to the encoder and decoder (more precisely, the decoder is <img src="https://habrastorage.org/getpro/habr/post_images/549/83a/c9e/54983ac9e8a725208d9d1eedda7caad0.svg" alt="f">  in terms of <img src="https://habrastorage.org/getpro/habr/post_images/147/ed7/0e5/147ed70e5a1f75baeb094836e61ac9cf.svg" alt="P (X | Z; \ theta_2) = f (Z; \ theta_2) + \ epsilon">  ), <br></li><li>  on the left in the identity - the value that we want to maximize for the elements of our training sample <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  + some mistake <img src="https://habrastorage.org/getpro/habr/post_images/6f2/bc8/837/6f2bc883712f9a3547d282b42910ed3f.svg" alt="KL \ (KL (x, y) \ ge 0 \ \ \ forall x, y)">  which hopefully with enough capacity <img src="https://habrastorage.org/getpro/habr/post_images/cb0/25a/395/cb025a395cc90c55c91d9d9d5535890c.svg" alt="Q">  will go to 0, <br></li><li>  to the right is a value that we can optimize by gradient descent, where the first term has a sense of prediction quality <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  decoder by values <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  and the second term is the distance K-L between the distribution <img src="https://habrastorage.org/getpro/habr/post_images/2d9/28e/e18/2d928ee1809930c655ae518cda1890e3.svg" alt="Z \ sim Q">  which the encoder predicts for a specific <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  and distribution <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  for all <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  right away <br></li></ul><br>  In order to be able to optimize the right side of the gradient descent, it remains to deal with two things: <br><br><h4>  1. More precisely, we define what <img src="https://habrastorage.org/getpro/habr/post_images/407/7a2/584/4077a2584a945c686c59013deee10772.svg" alt="Q (Z | X; \ theta_1)"></h4><br>  Usually <img src="https://habrastorage.org/getpro/habr/post_images/cb0/25a/395/cb025a395cc90c55c91d9d9d5535890c.svg" alt="Q">  is selected by the normal distribution: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4c4/a17/140/4c4a171406bd8cd5fc5743ce5bc2d65e.svg" alt="Q (Z | X; \ theta_1) = N (\ mu (X; \ theta_1), \ Sigma (X; \ theta_1))"></div><br>  That is, an encoder for each <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  predicts 2 values: average <img src="https://habrastorage.org/getpro/habr/post_images/4f1/75a/156/4f175a1563f9a13655c37ee6f97f48ea.svg" alt="\ mu">  and variation <img src="https://habrastorage.org/getpro/habr/post_images/ae3/89a/0f2/ae389a0f2891dc2e10daeef7756b3923.svg" alt="\ Sigma">  normal distribution from which values ‚Äã‚Äãare already sampled.  It all works like this: <br><br><img src="https://habrastorage.org/web/3df/aaf/bca/3dfaafbca9924d3187da7a7a9367fe93.png"><br><br>  Illustration of <strong><em>[2]</em></strong> <br><br>  Given that for each individual data point <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  the encoder predicts some normal distribution <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6b9/ce4/663/6b9ce4663a065cb1d1ab8e57bcfb2b81.svg" alt="P (Z | X) = N (\ mu (X), \ Sigma (X))"></div><br>  for marginal distribution <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  : <img src="https://habrastorage.org/getpro/habr/post_images/764/a10/f9c/764a10f9c0f82e9fb4531dab3304ba7c.svg" alt="P (Z) = N (0, I)">  that comes from the formula, and it's awesome. <br><br><img src="https://habrastorage.org/web/f31/674/a0b/f31674a0bb974f11983fac5a8ce1cedf.png"><br><br>  Illustration of <strong><em>[2]</em></strong> <br><br>  Wherein <img src="https://habrastorage.org/getpro/habr/post_images/403/70d/4e3/40370d4e34e3a0bf6f0335e84ae19e6f.svg" alt="KL [Q (Z | X; \ theta_1) || N (0, I)]">  takes the form: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b27/b66/67c/b27b6667cc1b2dd794a0d8cd220a540a.svg" alt="KL [Q (Z | X; \ theta_1) || N (0, I)] = \ frac {1} {2} \ left (tr (\ Sigma (X)) + \ mu (X) ^ T \ mu (X) - k - \ log \ det \ Sigma (X) \ right)"></div><br><h4>  2. We will understand how to distribute errors through <img src="https://habrastorage.org/getpro/habr/post_images/b30/2f5/5e4/b302f55e4a85ec37343537c4ed1119aa.svg" alt="\ mathbb {E} _ {Z \ sim Q} [\ log P (X | Z; \ theta_2)]"></h4><br>  The fact is that here we take random values <img src="https://habrastorage.org/getpro/habr/post_images/178/960/a9d/178960a9d537b379ec1beab0e2f324b4.svg" alt="Z \ sim Q (Z | X; \ theta_1)">  and pass them to the decoder. <br>  It is clear that it is impossible to directly propagate errors through random values, so the so-called <em>reparametrization</em> <em>trick is used</em> . <br><br>  The scheme is as follows: <br><img src="https://habrastorage.org/web/a4e/ec5/3a3/a4eec53a3cf24b289e494e4f03f71a39.png"><br><br>  Illustration of <strong><em>[1]</em></strong> <br><br>  Here on the left picture is a diagram without a trick, and on the right with a trick. <br>  Sampling is shown in red, and error calculation in blue. <br>  That is, in fact, just take the standard deviation predicted by the encoder. <img src="https://habrastorage.org/getpro/habr/post_images/ae3/89a/0f2/ae389a0f2891dc2e10daeef7756b3923.svg" alt="\ Sigma">  multiply by a random number of <img src="https://habrastorage.org/getpro/habr/post_images/981/05d/016/98105d016f10d5a71b901afe90ec8fac.svg" alt="N (0, I)">  and add the predicted average <img src="https://habrastorage.org/getpro/habr/post_images/4f1/75a/156/4f175a1563f9a13655c37ee6f97f48ea.svg" alt="\ mu">  . <br>  The direct propagation on both schemes is absolutely the same, but on the right scheme the reverse error propagation works. <br><br>  After we have trained such a variational autoencoder, the decoder becomes a full-fledged generative model.  In essence, an encoder is needed mainly in order to train the decoder to be separately a generative model. <br><img src="https://habrastorage.org/web/d1d/500/61b/d1d50061b0dd4836af3bd9d127b0a7e2.png"><br><br>  Illustration of <strong><em>[2]</em></strong> <br><div style="text-align:center;"><img src="https://habrastorage.org/web/211/52c/3b3/21152c3b383c45efb5819e5358da522b.png" width="250"></div><br>  Illustration of <strong><em>[1]</em></strong> <br><br>  But the fact that the encoder and decoder instead form a full-fledged autoencoder is a very nice plus. <br><br><h1>  VAE in Keras </h1><br>  Now, when we figured out what variational autoencoders are, let's write one on <em>Keras</em> . <br><br>  We import the necessary libraries and datasets: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) / <span class="hljs-number"><span class="hljs-number">255.</span></span> x_test = x_test .astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) / <span class="hljs-number"><span class="hljs-number">255.</span></span> x_train = np.reshape(x_train, (len(x_train), <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) x_test = np.reshape(x_test, (len(x_test), <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre><br>  Let's set the main parameters.  Hidden space will take dimension 2 to later generate from it and visualize the result. <br>  <strong><em>Note</em></strong> : dimension 2 is extremely small, so you should expect that the numbers will be very blurry. <br><br><pre> <code class="python hljs">batch_size = <span class="hljs-number"><span class="hljs-number">500</span></span> latent_dim = <span class="hljs-number"><span class="hljs-number">2</span></span> dropout_rate = <span class="hljs-number"><span class="hljs-number">0.3</span></span> start_lr = <span class="hljs-number"><span class="hljs-number">0.0001</span></span></code> </pre><br><br>  Let's write models of variational autoencoder. <br><br>  In order for learning to occur faster and better, add <em>dropout</em> layers and <em>batch normalization</em> . <br><br>  And in the decoder we use <em>leaky ReLU</em> as an activation, which we add as a separate layer after <em>dense</em> layers without activation. <br>  The <em>sampling</em> function implements the sampling of values. <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  of <img src="https://habrastorage.org/getpro/habr/post_images/ad3/da4/b09/ad3da4b09a7020394e1fbb061983b9a1.svg" alt="Q (Z | X)">  using the trick of reparameterization. <br><br>  <em>vae_loss</em> is the right side of the equation: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/674/f96/9d7/674f969d746b488c8d9488cf5e1720aa.svg" alt="\ log P (X; \ theta_2) - KL [Q (Z | X; \ theta_1) || P (Z | X; \ theta_2)] = \ mathbb {E} _ {Z \ sim Q} [\ log P (X | Z; \ theta_2)] - \ left (\ frac {1} {2} \ left (tr (\ Sigma (X)) + \ mu (X) ^ T \ mu (X) - k - \ log \ det \ Sigma (X) \ right) \ right)"></div>  will continue to be used as a loss. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Input, Dense <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization, Dropout, Flatten, Reshape, Lambda <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.objectives <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> binary_crossentropy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_vae</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> models = {} <span class="hljs-comment"><span class="hljs-comment">#  Dropout  BatchNormalization def apply_bn_and_dropout(x): return Dropout(dropout_rate)(BatchNormalization()(x)) #  input_img = Input(batch_shape=(batch_size, 28, 28, 1)) x = Flatten()(input_img) x = Dense(256, activation='relu')(x) x = apply_bn_and_dropout(x) x = Dense(128, activation='relu')(x) x = apply_bn_and_dropout(x) #    #  ,    ,    z_mean = Dense(latent_dim)(x) z_log_var = Dense(latent_dim)(x) #   Q    def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0) return z_mean + K.exp(z_log_var / 2) * epsilon l = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) models["encoder"] = Model(input_img, l, 'Encoder') models["z_meaner"] = Model(input_img, z_mean, 'Enc_z_mean') models["z_lvarer"] = Model(input_img, z_log_var, 'Enc_z_log_var') #  z = Input(shape=(latent_dim, )) x = Dense(128)(z) x = LeakyReLU()(x) x = apply_bn_and_dropout(x) x = Dense(256)(x) x = LeakyReLU()(x) x = apply_bn_and_dropout(x) x = Dense(28*28, activation='sigmoid')(x) decoded = Reshape((28, 28, 1))(x) models["decoder"] = Model(z, decoded, name='Decoder') models["vae"] = Model(input_img, models["decoder"](models["encoder"](input_img)), name="VAE") def vae_loss(x, decoded): x = K.reshape(x, shape=(batch_size, 28*28)) decoded = K.reshape(decoded, shape=(batch_size, 28*28)) xent_loss = 28*28*binary_crossentropy(x, decoded) kl_loss = -0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1) return (xent_loss + kl_loss)/2/28/28 return models, vae_loss models, vae_loss = create_vae() vae = models["vae"]</span></span></code> </pre><br>  <strong><em>Note</em></strong> : we used a <em>lambda</em> layer with a function that samples from <img src="https://habrastorage.org/getpro/habr/post_images/981/05d/016/98105d016f10d5a71b901afe90ec8fac.svg" alt="N (0, I)">  from the underlying framework, which clearly requires the size of the batch.  In all models in which this layer is present, we are now forced to transfer just such a size of the batch (that is, in the <em>encoder</em> and <em>vae</em> ). <br><br>  The optimization function will take <em>Adam</em> or <em>RMSprop</em> , both show good results. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.optimizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Adam, RMSprop vae.compile(optimizer=Adam(start_lr), loss=vae_loss)</code> </pre><br><br>  Code for drawing rows of numbers and digits from a manifold <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">digit_size = <span class="hljs-number"><span class="hljs-number">28</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_digits</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*args, invert_colors=False)</span></span></span><span class="hljs-function">:</span></span> args = [x.squeeze() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> args] n = min([x.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> args]) figure = np.zeros((digit_size * len(args), digit_size * n)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(args)): figure[j * digit_size: (j + <span class="hljs-number"><span class="hljs-number">1</span></span>) * digit_size, i * digit_size: (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) * digit_size] = args[j][i].squeeze() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> invert_colors: figure = <span class="hljs-number"><span class="hljs-number">1</span></span>-figure plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>*n, <span class="hljs-number"><span class="hljs-number">2</span></span>*len(args))) plt.imshow(figure, cmap=<span class="hljs-string"><span class="hljs-string">'Greys_r'</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) ax = plt.gca() ax.get_xaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) ax.get_yaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) plt.show() n = <span class="hljs-number"><span class="hljs-number">15</span></span> <span class="hljs-comment"><span class="hljs-comment">#   15x15  digit_size = 28 from scipy.stats import norm #     N(0, I),   ,          grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) def draw_manifold(generator, show=True): #     figure = np.zeros((digit_size * n, digit_size * n)) for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.zeros((1, latent_dim)) z_sample[:, :2] = np.array([[xi, yi]]) x_decoded = generator.predict(z_sample) digit = x_decoded[0].squeeze() figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit if show: #  plt.figure(figsize=(15, 15)) plt.imshow(figure, cmap='Greys_r') plt.grid(None) ax = plt.gca() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() return figure</span></span></code> </pre><br></div></div><br>  Often, in the process of learning the model, it is required to perform some actions: change the <em>learning_rate</em> , save intermediate results, save the model, draw pictures, etc. <br><br>  To do this, <em>keras</em> have callbacks that are passed to the <em>fit</em> method before starting the training.  For example, to influence the <em>learning rate</em> in the learning process, there are callbacks such as <em>LearningRateScheduler</em> , <em>ReduceLROnPlateau</em> , to save the model - <em>ModelCheckpoint</em> . <br><br>  A separate callback is needed in order to follow the learning process in <em>TensorBoard</em> .  It will automatically add to the log file all metrics and losses that are considered between eras. <br><br>  For the case when arbitrary functions are required to be performed in the learning process, there is a <em>LambdaCallback</em> .  It starts the execution of arbitrary functions at specified moments of training, for example, between eras or batch. <br>  We will follow the learning process by studying how numbers are generated from <img src="https://habrastorage.org/getpro/habr/post_images/981/05d/016/98105d016f10d5a71b901afe90ec8fac.svg" alt="N (0, I)">  . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LambdaCallback, ReduceLROnPlateau, TensorBoard <span class="hljs-comment"><span class="hljs-comment"># ,     ,    figs = [] latent_distrs = [] epochs = [] # ,     save_epochs = set(list((np.arange(0, 59)**1.701).astype(np.int)) + list(range(10))) #       imgs = x_test[:batch_size] n_compare = 10 #  generator = models["decoder"] encoder_mean = models["z_meaner"] # ,       def on_epoch_end(epoch, logs): if epoch in save_epochs: clear_output() #   output #      decoded = vae.predict(imgs, batch_size=batch_size) plot_digits(imgs[:n_compare], decoded[:n_compare]) #   figure = draw_manifold(generator, show=True) #     z     epochs.append(epoch) figs.append(figure) latent_distrs.append(encoder_mean.predict(x_test, batch_size)) #  pltfig = LambdaCallback(on_epoch_end=on_epoch_end) # lr_red = ReduceLROnPlateau(factor=0.1, patience=25) tb = TensorBoard(log_dir='./logs') #   vae.fit(x_train, x_train, shuffle=True, epochs=1000, batch_size=batch_size, validation_data=(x_test, x_test), callbacks=[pltfig, tb], verbose=1)</span></span></code> </pre><br>  Now, if <em>TensorBoard is</em> installed, you can follow the learning process. <br><br>  Here is how this encoder recovers images: <br><br><img src="https://habrastorage.org/web/670/bc6/c44/670bc6c44122449eb1d363144736f40f.png"><br><br>  And here is the result of sampling from <img src="https://habrastorage.org/getpro/habr/post_images/e3d/6b1/d29/e3d6b1d29a89a32d44eba93c5db11abf.svg" alt="N (0 | I)"><br><br><img src="https://habrastorage.org/web/92a/ac4/61b/92aac461bd794121874d9b307448ad2f.png" width="600"><br><br>  Here is the process of learning how to generate numbers: <br><br><div class="spoiler">  <b class="spoiler_title">Gif</b> <div class="spoiler_text"><img src="https://habrastorage.org/web/fe5/68a/a5e/fe568aa5ee8e4d939ec067584fc34874.gif" width="600"><br></div></div><br>  Distribution of codes in hidden space: <br><br><div class="spoiler">  <b class="spoiler_title">Gif</b> <div class="spoiler_text"><img src="https://habrastorage.org/web/c18/f23/cd8/c18f23cd8e044486a1336dd40551a5bc.gif" width="600"><br></div></div><br>  Not ideally normal, but rather close (especially, considering that the dimension of the hidden space is only 2). <br><br>  <em>TensorBoard</em> learning <em>curve</em> <br><img src="https://habrastorage.org/web/320/18b/d03/32018bd03465439e9eed0b418f1067bf.png" width="800"><br><br><div class="spoiler">  <b class="spoiler_title">GIF creation code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FuncAnimation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_2d_figs_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(figs, epochs, fname, fig)</span></span></span><span class="hljs-function">:</span></span> norm = matplotlib.colors.Normalize(vmin=<span class="hljs-number"><span class="hljs-number">0</span></span>, vmax=<span class="hljs-number"><span class="hljs-number">1</span></span>, clip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im = plt.imshow(np.zeros((<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>)), cmap=<span class="hljs-string"><span class="hljs-string">'Greys_r'</span></span>, norm=norm) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Epoch: "</span></span> + str(epochs[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i)</span></span></span><span class="hljs-function">:</span></span> im.set_array(figs[i]) im.axes.set_title(<span class="hljs-string"><span class="hljs-string">"Epoch: "</span></span> + str(epochs[i])) im.axes.get_xaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im.axes.get_yaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> im anim = FuncAnimation(fig, update, frames=range(len(figs)), interval=<span class="hljs-number"><span class="hljs-number">100</span></span>) anim.save(fname, dpi=<span class="hljs-number"><span class="hljs-number">80</span></span>, writer=<span class="hljs-string"><span class="hljs-string">'imagemagick'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_2d_scatter_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(zs, epochs, c, fname, fig)</span></span></span><span class="hljs-function">:</span></span> im = plt.scatter(zs[<span class="hljs-number"><span class="hljs-number">0</span></span>][:, <span class="hljs-number"><span class="hljs-number">0</span></span>], zs[<span class="hljs-number"><span class="hljs-number">0</span></span>][:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=c, cmap=cm.coolwarm) plt.colorbar() plt.title(<span class="hljs-string"><span class="hljs-string">"Epoch: "</span></span> + str(epochs[<span class="hljs-number"><span class="hljs-number">0</span></span>])) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i)</span></span></span><span class="hljs-function">:</span></span> fig.clear() im = plt.scatter(zs[i][:, <span class="hljs-number"><span class="hljs-number">0</span></span>], zs[i][:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=c, cmap=cm.coolwarm) im.axes.set_title(<span class="hljs-string"><span class="hljs-string">"Epoch: "</span></span> + str(epochs[i])) im.axes.set_xlim(<span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>) im.axes.set_ylim(<span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> im anim = FuncAnimation(fig, update, frames=range(len(zs)), interval=<span class="hljs-number"><span class="hljs-number">150</span></span>) anim.save(fname, dpi=<span class="hljs-number"><span class="hljs-number">80</span></span>, writer=<span class="hljs-string"><span class="hljs-string">'imagemagick'</span></span>) make_2d_figs_gif(figs, epochs, <span class="hljs-string"><span class="hljs-string">"./figs3/manifold.gif"</span></span>, plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))) make_2d_scatter_gif(latent_distrs, epochs, y_test, <span class="hljs-string"><span class="hljs-string">"./figs3/z_distr.gif"</span></span>, plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)))</code> </pre><br></div></div><br>  It can be seen that dimension 2 for such a task is very small, the numbers are very blurry, and also in the intervals between good many ragged numbers. <br>  In the <a href="https://habrahabr.ru/post/331664/">next part,</a> we will look at how to generate the numbers of the desired label, get rid of the ragged ones, and also how to transfer the style from one number to another. <br><br><h2>  Useful links and literature </h2><br>  The theoretical part is based on the article: <br>  [1] Tutorial on Variational Autoencoders, Carl Doersch, 2016, <a href="https://arxiv.org/abs/1606.05908">https://arxiv.org/abs/1606.05908</a> <br>  and in fact is her summary <br><br>  Many pictures are taken from Isaac Dykeman blog: <br>  [2] Isaac Dykeman, <a href="http://ijdykeman.github.io/ml/2016/12/21/cvae.html">http://ijdykeman.github.io/ml/2016/12/21/cvae.html</a> <br><br>  You can read more about Kullback-Leibler distance in Russian here: <br>  [3] <a href="http://www.machinelearning.ru/wiki/images/d/d0/BMMO11_6.pdf">http://www.machinelearning.ru/wiki/images/d/d0/BMMO11_6.pdf</a> <br><br>  The code is partly based on the <em>Francois Chollet</em> article: <br>  [4] <a href="https://blog.keras.io/building-autoencoders-in-keras.html">https://blog.keras.io/building-autoencoders-in-keras.html</a> <br><br>  Other interesting links: <br>  <a href="http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html">http://blog.fastforwardlabs.com/2016/08/12/introducing-variational-autoencoders-in-prose-and.html</a> <br>  <a href="http://kvfrans.com/variational-autoencoders-explained/">http://kvfrans.com/variational-autoencoders-explained/</a> </div><p>Source: <a href="https://habr.com/ru/post/331552/">https://habr.com/ru/post/331552/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../331540/index.html">Static call script</a></li>
<li><a href="../331542/index.html">Guide: how to use Python for algorithmic trading on the exchange. Part 1</a></li>
<li><a href="../331544/index.html">Mikrotik: Balancing in the CPSU and the observance of speed limits</a></li>
<li><a href="../331546/index.html">Installing ArchLinux ARM next to Android without chroot</a></li>
<li><a href="../331548/index.html">Connect: Modern Frontend Tips</a></li>
<li><a href="../331554/index.html">New features of C # that can be expected soon</a></li>
<li><a href="../331556/index.html">Automated programming - a new milestone or myth? Part 1. Introduction</a></li>
<li><a href="../331558/index.html">Whether to limit users on resources?</a></li>
<li><a href="../331560/index.html">Selection of the distribution law of a random variable according to statistical sampling using Python tools</a></li>
<li><a href="../331562/index.html">Digital counterparts. Design through reflection</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>