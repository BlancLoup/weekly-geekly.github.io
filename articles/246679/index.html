<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sphinx Text Processing Pipeline</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Text processing in the search engine looks quite simple from the outside, but in fact it is a complex process. When indexing, the text of the document...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sphinx Text Processing Pipeline</h1><div class="post__text post__text-html js-mediator-article">  Text processing in the search engine looks quite simple from the outside, but in fact it is a complex process.  When indexing, the text of the documents should be processed by the HTML stripper, tokenizer, stopword filter, word form filter and morphological processor.  And at the same time, you need to remember about exceptions, blended characters, N-grams and sentence boundaries.  When searching, everything becomes even more complicated, because in addition to all the above, you need to also process the syntax of the request, which adds all sorts of specials.  characters (operators and masks).  Now we will tell how it all works in Sphinx. <br><br><h1>  Picture as a whole </h1><br>  Simplified text processing pipeline (in version 2.x engine) looks like this: <br><br><img src="https://habrastorage.org/files/a7e/c78/da8/a7ec78da80c0478990cc94c385f1891a.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It looks quite simple, but the devil is in the details.  There are several very different filters (which are applied in a special order);  Tokenizer does something else besides breaking the text into words;  and finally, under ‚Äúetc.‚Äù in the morphology block, there are in fact at least three different variants. <br><br>  Therefore, the following picture will be more accurate: <br><br><img src="https://habrastorage.org/files/12d/9f1/74f/12d9f174fa5e40438f4d99489315d073.png"><br><br><a name="habracut"></a><br><br><h2>  Regular expression filters </h2><br><br>  This is an optional step.  In essence, it is a set of regular expressions that apply to documents and queries sent to the Sphinx, and nothing more!  Thus, this is just ‚Äúsyntactic sugar‚Äù, but quite convenient: with the help of regexps, Sphinx processes everything, and without them you would have to write a separate script to load data into the sphinx, then one more - to correct the requests, and both scripts needed would keep synchronous.  And from the inside of the Sphinx, we simply run all the filters above the fields and queries before any further processing.  Everything!  A more detailed description can be found in the <a href="http://sphinxsearch.com/docs/current.html">regexp_filter</a> section of the documentation. <br><br><h2>  HTML stripper </h2><br>  This is also an optional step.  This handler is connected only if the source directive contains the <a href="http://sphinxsearch.com/docs/current.html">html_strip</a> directive.  This filter works right after the regular expression filter.  The stripper removes all HTML tags from the incoming text.  In addition, he can <b>extract and index individual attributes of the specified tags</b> (see <a href="http://sphinxsearch.com/docs/current.html">html_index_attrs</a> ), as well as <b>delete text between tags</b> (see <a href="http://sphinxsearch.com/docs/current.html">html_remove_elements</a> ).  Finally, since the zones in the documents and paragraphs use the same SGML markup, the stripper performs the <b>definition of the zone boundaries and paragraphs</b> (see <a href="http://sphinxsearch.com/docs/current.html">index_sp</a> and <a href="http://sphinxsearch.com/docs/current.html">index_zones</a> ).  (Otherwise, you would have to make another <i>exactly the same</i> pass through the document in order to accomplish this task. Ineffective!) <br><br><h2>  Tokenization </h2><br>  This step is required.  Whatever happens, we need to break the phrase ‚ÄúMary had a little lamb‚Äù into separate keywords.  This is the essence of tokenization: transform a text field into a set of keywords.  It would seem, what could be easier? <br><br>  Everything is so, except that simple word splitting with spaces and punctuation marks does not always work, and therefore we have a whole set of parameters that control tokenization. <br><br>  First, there are tricky characters that are both ‚Äúcharacters‚Äù and ‚Äúnot characters‚Äù at the same time, and even more, they can simultaneously be a ‚Äúcharacter‚Äù, a ‚Äúspace‚Äù and a ‚Äúpunctuation mark‚Äù (which at first glance can also be interpreted as a space , but actually it is impossible).  To combat all of this economy, the <a href="http://sphinxsearch.com/docs/current.html">charset_table</a> , <a href="http://sphinxsearch.com/docs/current.html">blend_chars</a> , <a href="http://sphinxsearch.com/docs/current.html">ignore_chars</a> and <a href="http://sphinxsearch.com/docs/current.html">ngram_chars settings are used</a> . <br><br>  By default, the Sphinx tokenizer <b>treats all unknown characters as spaces</b> .  Therefore, no matter how crazy your unicode pseudo-graphics are, you can ‚Äúfill in‚Äù your document, it will be indexed simply as a space.  All characters mentioned in <b>charset_table are treated as ordinary characters</b> .  Also, the <b>charset_table allows you to map one character to another</b> : it is usually used to convert characters to a single case, to remove accents, or for everything together.  In most cases, this is already enough: we reduce the known characters to the contents of the charset_table;  we replace all unknowns (including punctuation) with spaces - and that's all, tokenization is ready. <br><br>  However, there are three significant exceptions. <br><br><ul><li>  Sometimes a text editor (for example, Word) inserts soft hyphen characters directly into the text!  And if you do not ignore them entirely (instead of simply replacing them with spaces), the text will be indexed as ‚Äúma ry had a lit t le lamb‚Äù.  To solve this problem, use ignore_chars. </li><li>  Eastern languages ‚Äã‚Äãwith hieroglyphs.  They are not significant gaps!  Therefore, for limited support for CJK texts (Chinese, Japanese, Korean) in the kernel, you can specify the ngram_chars directive, and then each such character will be considered as a separate keyword, as if it is surrounded by spaces (even if it is not so). </li><li>  For tricky characters like &amp; or.  we do not really know in the process of word splitting, whether we want to index them or delete them.  For example, in the phrase ‚ÄúJeeves &amp; Wooster‚Äù, the &amp; sign can be removed.  But in AT &amp; T - no way!  Also, you can not spoil the "Marwel's Agents of SHIELD".  For this Sphinx, you can specify a list of characters with the blend_chars directive.  The characters from this list will be processed in two ways at once: as ordinary characters, and as spaces.  Notice how a simple turnover of ordinary characters can lead to the generation of multiple tokens when the blend_chars list comes into play: say, the ‚ÄúBack to USSR‚Äù field will be, as usual, split into tokens, as usual, but also one more the ussr token will be indexed in the same position as the u in the base partition. </li></ul><br><br>  And all this happens with the most basic elements of the text - the characters!  Scared ?! <br><br>  In addition, the tokenizer (oddly enough) can work with exceptions (such as C ++ or C # - where special characters make sense only in these keywords and can be completely ignored in all other cases), and besides, it can determine bounds of sentences (if the <a href="http://sphinxsearch.com/docs/current.html">index_sp</a> directive is <a href="http://sphinxsearch.com/docs/current.html">given</a> ).  This task can not be solved later, since after tokenization we will no longer have any specials.  characters, no punctuation.  Also, it is not worthwhile to do this at earlier stages, since, again, 3 passes through the same text, in order to make 4 operations on it, this is worse than the one and only one, which will immediately put everything in its place. <br><br>  Inside, the tokenizer is designed so that <b>exceptions are triggered before anything else</b> .  In this sense, they are very similar to regular expression filters (and moreover, they can easily be emulated using regular expressions. We say "it is possible", but we never tried it: in fact, it is much easier and faster to work with exceptions. add one more regexp? Ok, this will lead to one more pass through the text of the field. But all exceptions apply at once on a single pass of the tokenizer and occupy 15-20% of the tokenization time (which in general will be 2-5% of the total indexing time) . <br><br>  Determination of the boundaries of sentences is defined in the tokenizer code and there is nothing to configure (and not necessary).  Just turn on and hope that everything will work (usually it happens; although who knows, there may be some strange regional cases). <br><br>  So, if you take a relatively innocuous point, and enter it first in one of the exceptions, as well as in blend_chars, and also put index_sp = 1 - you risk turning the whole axle nest (fortunately, not going beyond the tokenener boundaries).  Again, everything ‚Äújust works‚Äù outside (although if you turn on ALL of the above options, and then you also try to index some strange text that will trigger all the conditions at the same time and thereby awaken Cthulhu - well, you are to blame!) <br><br>  From now on, we have tokens!  And <i>all</i> subsequent processing phases deal specifically with individual tokens.  Forward! <br><br><h2>  Word Forms and Morphology </h2><br>  Both steps are optional;  both are disabled by default.  More interestingly, word forms and morphological processors (stemmers and lemmatizers) are interchangeable in some way, and therefore we consider them together. <br><br>  Each word created by a tokenizer is processed separately.  Several different handlers are available: from the stupid, but still in some places popular Soundex and Metaphone, to Porter's classical grammers, including the libstemmer library, as well as full-fledged dictionary lemmatizers.  All handlers generally take one word and replace it with the given normalized form.  So far so good. <br><br>  And now the details: <b>morphological handlers are applied in exactly the order as mentioned in the config file, until the word is processed</b> .  That is, as soon as the word is changed by one of the handlers, that's all, the processing chain ends and all subsequent handlers will not even be called.  For example, in the chain morphology = stem_en, stem_fr the English stemmer will have an advantage;  morphology = stem_fr in the chain, stem_en - French.  And in the chain morphology = soundex, stem_en, the mention of an English stemmer is essentially useless, since soundex converts all English words before the stemmer reaches them.  An important edge effect of this behavior is that if a word is already in normal form and one of the stemmers discovered it (but, of course, it didn‚Äôt change anything), then it will be processed by subsequent stemmers. <br><br>  Further.  <b>Regular word forms are an implicit morphological processor of the highest priority</b> .  If word forms are specified, then the words are first processed by them, and get into the handlers of morphology only if no transformations have happened.  Thus, any unpleasant error of a stemmer or lemmatizer can be corrected with the help of word forms.  For example, English Stemmer leads the words "business" and "busy" to the same basis "busi".  And this is easily corrected by adding one line of ‚Äúbusiness =&gt; business‚Äù to word forms.  (and yes, notice - the word forms are even more than morphology, since in this case the fact of replacing the word is sufficient, and it does not matter that it itself, in fact, has not changed). <br><br>  Above, "ordinary word forms" were mentioned.  And here's why: there <b>are three different types of word forms</b> . <br><ol><li>  <b>Common word forms</b> .  They display 1: 1 tokens and in some way <i>replace</i> morphology (we just mentioned this) </li><li>  <b>Morphological word forms</b> .  You can replace all those who run on walking with a single line "~ run =&gt; walk" instead of a set of rules about "running", "running", "running", "running away", etc.  And if in the English language there may not be so many such options, in some others, like our Russian, one base may have dozens or even hundreds of different inflections.  <b>Morphological word forms are applied after morphology handlers</b> .  And they still display the words 1: 1 </li><li>  <b>Multiforms</b> .  They display the words M: N.  In general, they work like normal substitution and are performed at an earlier stage as possible.  The easiest way to present multi-forms as some kind of early replacement.  In this sense, they are a kind of regexp or exception, but they are applied at a different stage and therefore ignore punctuation.  Note that <b>after applying the multi forms, the resulting tokens undergo all other morphological treatments, <i>including the</i> usual 1: 1 word forms</b> ! </li></ol><br><br>  Consider an example: <br><br><pre>  morphology = stem_en
 wordforms = myforms.txt
 
 myforms.txt:
 walking =&gt; crawling
 running shoes =&gt; walking pants
 ~ pant =&gt; shoes </pre><br><br>  Suppose we index the ‚Äúmy running shoes‚Äù document with these strange settings.  What will be the result in the index? <br><br><ul><li>  First we get three tokens - ‚Äúmy‚Äù ‚Äúrunning‚Äù ‚Äúshoes‚Äù. </li><li>  Then the multiform will apply and transform it into ‚Äúmy‚Äù ‚Äúwalking‚Äù ‚Äúpants‚Äù. </li><li>  The usual word form displays "walking" in "crawling" (you get "my" "crawling" "pants") </li><li>  The morphological processor (English stemmer) will process ‚Äúmy‚Äù and ‚Äúpants‚Äù (since ‚Äúwalking‚Äù is already processed with the usual word form) and will issue ‚Äúmy‚Äù ‚Äúcrawling‚Äù ‚Äúpant‚Äù </li><li>  Finally, the morphological word form will display all forms of the word pant in shoes.  The resulting ‚Äúmy‚Äù ‚Äúcrawling‚Äù ‚Äúshoes‚Äù tokens will be saved in the index. </li></ul><br><br>  It sounds solid.  However, how can a <a href="http://www.youtube.com/watch%3Fv%3D5PsnxDQvQpw">mere mortal</a> who does not develop Sphinx and is not at all used to debugging C ++ code, guess all <i>this</i> ?  Very simple: for this there is a special command: <br><br><pre> mysql&gt; call keywords ('my running shoes', 'test1');
 + ------ + --------------- + ------------ +
 |  qpos |  tokenized |  normalized |
 + ------ + --------------- + ------------ +
 |  1 |  my |  my |
 |  2 |  running shoes |  crawling |
 |  3 |  running shoes |  shoes |
 + ------ + --------------- + ------------ +
 3 rows in set (0.00 sec)
</pre><br><br>  and to conclude this section, we illustrate how morphology and three different types of word forms interact together: <br><br><img src="https://habrastorage.org/files/8b5/db5/28d/8b5db528d4b241e38cb9bc20d8d1d5f5.png"><br><br><h2>  Words and Positions </h2><br>  After all the treatments, the tokens have certain positions.  Usually they are simply numbered sequentially, starting from one.  However, <b>each position in the document can belong to several tokens simultaneously</b> !  This usually happens when a single ‚Äúraw‚Äù token generates several versions of the final word, either using merged characters, or lemmatization, or in several other ways. <br><br><h3>  Magic characters </h3><br>  For example, ‚ÄúAT &amp; T‚Äù in the case of a single ‚Äú&amp;‚Äù will be split into ‚Äúat‚Äù in position 1, ‚Äút‚Äù in position 2, and also ‚Äúat &amp; t‚Äù in position 1. <br><br><h3>  Lemmatization </h3><br>  This is more interesting.  For example, we have the document ‚ÄúWhite dove flew away.  I dove into the pool. ‚ÄùThe first entry of the word‚Äú dove ‚Äùis a noun.  The second is the verb "dive" in past tense.  But analyzing these words as separate tokens, we can‚Äôt say anything about it (and even if we look at several tokens at once, it can be quite difficult to make the right decision).  In this case, morphology = lemmatize_en_all will lead to the indexation of <i>all</i> possible options.  In this example, in positions 2 and 6, two different tokens will be indexed, so that both ‚Äúdove‚Äù and ‚Äúdive‚Äù will be saved. <br><br>  Positions affect the search using phrases (phrase) and inaccurate phrases (proximity);  they also affect ranking.  And as a result of any of the four requests - ‚Äúwhite dove‚Äù, ‚Äúwhite dive‚Äù, ‚Äúdove into‚Äù, ‚Äúdive into‚Äù will lead to finding the document in the phrase mode. <br><br><h2>  Stopslova </h2><br>  The step of removing stopwords is very simple: we just throw them out of the text.  However, a couple of things still need to be borne in mind: <br>  1. How can I completely ignore stopwords (instead of just wiping them with spaces).  Even though the stopwords are thrown away, the positions of the other words remain unchanged.  This means that ‚Äúmicrosoft office‚Äù and ‚Äúmicrosoft in the office‚Äù, in case of ignoring ‚Äúin‚Äù and ‚Äúthe‚Äù as stop words, will produce <i>different</i> indices.  In the first document, the word ‚Äúoffice‚Äù is in position 2. In the second, in position 4. If you want to completely remove stop words, you can use the <a href="http://sphinxsearch.com/docs/current.html">stopword_step</a> directive and set it to 0. This will affect the search for phrases and ranking. <br>  2. How to add in stop words separate forms or complete lemmas.  This setting is called <a href="http://sphinxsearch.com/docs/current.html">stopwords_unstemmed</a> and it is determined whether the removal of stop words is applied before or after the morphology. <br><br><h1>  What is left? </h1><br>  Well, we almost covered all the typical tasks of everyday text processing.  Now you should be clear what is happening inside, how it all works together, and how to set up the Sphinx to achieve the desired result.  Hooray! <br><br>  But there is something else.  In brief, we mention that there is also the <a href="http://sphinxsearch.com/docs/current.html">index_exact_words</a> option, which instructs to index the initial token (before the morphology is applied) in addition to the morphology.  There is also the <a href="http://sphinxsearch.com/docs/current.html">bigram_index</a> option, which will force the sphinx to index word pairs (‚Äúa brown fox‚Äù will become ‚Äúa brown‚Äù, ‚Äúbrown fox‚Äù tokens) and then use them for an ultra-fast search for phrases.  You can also use indexing and query plugins that will allow you to implement almost any desired token processing. <br><br>  And finally, in the upcoming release of Sphinx 3.0, there are plans to unify all these settings, so that instead of general directives that apply to the entire document, you can create separate filter chains for processing individual fields.  So that it was possible, for example, to first remove some stop words, then apply word forms, then morphology, then another word form filter, etc. </div><p>Source: <a href="https://habr.com/ru/post/246679/">https://habr.com/ru/post/246679/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../246665/index.html">Meet the .Net community at CLRium # 2. How does the CLR, Roslyn, RyuJIT, CoreFx work?</a></li>
<li><a href="../246667/index.html">Auto Layout with a variable set of elements</a></li>
<li><a href="../246671/index.html">Network UPS Tools (NUT) on CentOS and Windows with SMS sending via smstools + playsms</a></li>
<li><a href="../246673/index.html">Black Swift: Why We Made It</a></li>
<li><a href="../246675/index.html">JMSpy - spy on method calls</a></li>
<li><a href="../246683/index.html">Neuromarketing: A New Approach To Increasing Conversions (Part 2)</a></li>
<li><a href="../246685/index.html">How normal guys go to Megu: 7 stores per month</a></li>
<li><a href="../246689/index.html">The number of working hours per week for the highest productivity - 35</a></li>
<li><a href="../246691/index.html">The practice of automating the measurement of performance indicators SED</a></li>
<li><a href="../246695/index.html">Vulnerabilities PayPal accounts, fraud refund transactions, hacking account</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>