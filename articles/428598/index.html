<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep neural networks for automatic call evaluation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Call evaluation is a key part of quality control for call centers. It allows organizations to fine-tune workflow so that operators can do work faster ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep neural networks for automatic call evaluation</h1><div class="post__text post__text-html js-mediator-article">  Call evaluation is a key part of quality control for call centers.  It allows organizations to fine-tune workflow so that operators can do work faster and more efficiently, as well as avoid meaningless routines. <br><br>  Bearing in mind that the call center should be effective, we worked on automating the evaluation of calls.  As a result, we came up with an algorithm that processes calls and distributes them into two groups: suspicious and neutral.  All suspicious calls were immediately sent to the quality assessment team. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yj/0t/yq/yj0tyqtftcw-e0m_wcxkh_ls7l0.jpeg"></div><br><a name="habracut"></a><br><h2>  How we trained deep neural network </h2><br>  We took 1700 audio files for the samples and trained the network.  Since the neuron initially did not know what was considered suspicious and neutral, we manually marked all the files accordingly. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In neutral samples, the operators are: <br><br><ul><li>  did not raise his voice; </li><li>  give customers all the requested information; </li><li>  did not respond to provocations from the client. </li></ul><br>  In suspicious patterns, operators often did the following: <br><br><ul><li>  used obscene language; </li><li>  raised their voice or shouted at customers; </li><li>  passed on personalities; </li><li>  refused to advise on issues. </li></ul><br>  When the algorithm finished processing the files, it marked 200 files as invalid.  These files contained neither suspicious nor neutral signs.  We found out what was in these 200 files: <br><br><ul><li>  the customer hung up immediately after the operator answered; </li><li>  the client said nothing after being answered; </li><li>  there was too much noise on the client or operator side. </li></ul><br>  When we deleted these files, we divided the remaining 1500 into training and test examples.  Later we used these datasets to train and test the deep neural network. <br><br><h2>  Step 1: Extraction of the signs </h2><br>  High-level feature extraction plays an important role in machine learning, since  it directly affects the efficiency of the algorithm.  After analyzing all possible sources, we selected the following signs: <br><br><h3>  Time statistics </h3><br><ul><li>  <a href="https://en.wikipedia.org/wiki/Zero-crossing_rate">Zero-crossing rate</a> : the rate at which the signal changes from plus to negative and vice versa. </li><li>  <b>The median frame energy</b> is the sum of the signals squared and normalized by the corresponding frame length. </li><li>  <b>Subframe energy entropy</b> : entropy of the normalized energy of subframes.  It can be interpreted as a measure of drastic changes. </li><li>  <b>Mean / median / standard deviation of the frame</b> . </li></ul><br><h3>  Spectral statistics (with frequency intervals) </h3><br><ol><li>  Spectral centroid. </li><li>  Spectral distribution. </li><li>  Spectral entropy. </li><li>  Spectral radiation. </li><li>  Spectral attenuation. </li></ol><br>  Cepstral tone frequency coefficients and saturation vector are sensitive to the length of the input signal.  We could extract them from the whole file at a time, but by doing so, we would have missed the development of the feature over time.  Since this method did not suit us, we decided to divide the signal into ‚Äúwindows‚Äù (temporary blocks). <br><br>  In order to improve the quality of the trait, we broke the signal into chunks, which partially overlapped each other.  Next, we extracted the tag sequentially for each chunk;  therefore, the attribute matrix was calculated for each audio file. <br><br>  Window size - 0.2 s;  window pitch - 0.1 s. <br><br><h2>  Step 2: we determine the tone of the voice in separate phrases </h2><br>  Our first approach to solving the problem is to define and process each phrase in the stream separately. <br><br>  First of all, we did <a href="https://en.wikipedia.org/wiki/Speaker_diarisation">diarization</a> and isolated all the phrases using the <a href="http://lium3.univ-lemans.fr/diarization/doku.php/overview">LIUM</a> library.  The input files were of poor quality, so at the output we also applied anti-aliasing and adaptive thresholding for each file. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ri/8t/me/ri8tmehn045h1dbiztmtq4ixb-o.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/z3/c9/_t/z3c9_tkardcnk9irbxxokxxso44.png"></div><br><h3>  Handling of interruptions and long silence </h3><br>  When we determined the time limits for each phrase (both client and operator), we imposed them on each other and found cases when both people speak at the same time, as well as cases when both are silent.  It remained only to determine the magnitude of the threshold.  We agreed that if 3 or more seconds are spoken to by the participants at the same time, this is considered interrupting.  For silence, the threshold was set at 3 seconds smoothly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ke/md/dh/kemddhguibgoz6kug0imtvphl84.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dl/ti/_o/dlti_okejnlhqdwlnpihys0d0vi.png"></div><br>  The point is that each phrase has its own length.  Therefore, the number of features extracted from each phrase is different. <br><br>  The <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">LSTM</a> neural network could handle this problem.  Networks of this kind can not only process sequences of different lengths, but they can also contain feedback, which gives you the opportunity to save information.  These features are very important because the phrases spoken earlier contain information that affects the phrases said after. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s1/fa/sr/s1fasrlj1hby2a67opppjka_n5m.png"></div><br>  Then we trained our LSTM network to determine the intonation of each phrase. <br><br>  As a training set, we took 70 files with 30 phrases on average (15 phrases for each side). <br><br>  The main goal was to evaluate the phrases of the call center operator, so we did not use client speech for training.  We used 750 phrases as a training dataset, and 250 phrases as a test.  As a result, Neuronka learned to classify speech with an accuracy of 72%. <br><br>  But in the end, we were not satisfied with the performance of the LSTM network: working with it took too much time, and the results were far from perfect.  Therefore, it was decided to use a different approach. <br><br>  It is time to tell how we determined the tone of voice using <a href="https://en.wikipedia.org/wiki/Xgboost">XGBoost</a> plus a combination of LSTM and XGB. <br><br><h2>  Determine the tone of voice for the entire file. </h2><br>  We marked files as suspicious if they contained at least one phrase that violated the rules.  So we tagged 2500 files. <br><br>  To extract features, we used the same method and the same <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">ANN</a> architecture, but with one difference: we scaled the architecture to fit the new dimensions of the features. <br><br>  With optimal parameters, the neural network produced 85% accuracy. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pu/1f/q3/pu1fq3pnbx828w78lppbfkcdrla.png" width="650"></div><br><h3>  Xgboost </h3><br>  <a href="https://en.wikipedia.org/wiki/Gradient_boosting">The XGBoost model</a> requires a fixed number of attributes for each file.  To satisfy this requirement, we have created several signals and parameters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lh/n0/ec/lhn0ecwl46_pfqmzgq-nelhxagu.jpeg"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6v/kl/xh/6vklxhkhif4zlv_haaqhk71pmgy.jpeg"></div><br>  The following statistics were used: <br><br><ol><li>  The average value of the signal. </li><li>  The average value of the first 10 seconds of the signal. </li><li>  The average value of the last 3 seconds of the signal. </li><li>  The average value of local maxima in the signal. </li><li>  The average value of local maxima in the first 10 seconds of the signal. </li><li>  The average value of local maxima in the last 3 seconds of the signal. </li></ol><br>  All indicators were calculated separately for each signal.  The total number of signs is 36, with the exception of the record length.  As a result, we had 37 numeric characters for each record. <br><br>  The prediction accuracy of this algorithm is 0.869. <br><br><h3>  LSTM and XGB combination </h3><br>  To combine classifiers, we crossed these two models.  At the exit, this increased accuracy by 2%. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/e9/wi/21/e9wi21ggxfrlo6tj_shuo6b3pve.png"></div><br>  That is, we managed to improve the prediction accuracy to 0.9 <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC</a> - <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUC</a> (Area Under Curve). <br><br><h2>  Result </h2><br>  We tested our deep neural network on 205 files (177 neutral, 28 suspicious).  The network had to process each file and decide which group it belongs to.  Below are the results: <br><br><ul><li>  170 neutral files were defined correctly; </li><li>  7 neutral files were identified as suspicious; </li><li>  13 suspicious files were identified correctly; </li><li>  15 suspicious files were identified as neutral. </li></ul><br>  To estimate the percentage of correct / false results, we used the <a href="http://ru.learnmachinelearning.wikia.com/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BE%25D0%25BA_(Confusion_matrix)">Error Matrix</a> in the form of a 2x2 table. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mh/iu/4g/mhiu4g2975opf6esqhuiflvphsm.jpeg"></div><br><h2>  Find a specific phrase in the conversation </h2><br>  We could not wait to try this approach to recognize words and phrases in audio files.  The goal was to find files in which the call center operator was not presented to clients in the first 10 seconds of the conversation. <br><br>  We took 200 phrases with an average length of 1.5 seconds, in which operators give their name and company name. <br><br>  The search for such files manually took a lot of time, because  I had to listen to each file in order to check if the necessary phrases were in it.  To speed up the subsequent learning, we artificially increased the dataset: we changed each file randomly 6 times - added noise, changed the frequency and / or volume.  So we got in 1500 files. <br><br><h3>  Total </h3><br>  We used the first 10 seconds of the operator's response to train the classifier, because it was in this interval that the necessary phrase was uttered.  Each such fragment was divided into windows (window length - 1.5 s, window pitch - 1 s) and processed by the neural network as an input file.  As an output file, we obtained the probability of pronouncing each phrase in the selected window. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cr/md/4-/crmd4-kcd7c-_qfd5h_arxpa6ei.jpeg"></div><br>  We drove another 300 files through the network to find out if the necessary phrase was uttered in the first 10 seconds.  For these files, the accuracy was 87%. <br><br><h2>  Actually, what is all this for? </h2><br>  Automatic call evaluation helps define clear KPIs for call center operators, highlight and follow best practices, and increase call center productivity.  But It is worth noting that speech recognition software can also be used for a wider range of tasks. <br><br>  Below are a few examples of how speech recognition can help organizations: <br><br><ul><li>  collect and analyze data to improve voice UX; </li><li>  analyze call records to identify connections and trends; </li><li>  recognize people by voice; </li><li>  find and define customer emotions to improve user satisfaction; </li><li>  increase the average revenue per call; </li><li>  reduce the outflow; </li><li>  and much more! </li></ul></div><p>Source: <a href="https://habr.com/ru/post/428598/">https://habr.com/ru/post/428598/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428582/index.html">The forgotten story of the PLO</a></li>
<li><a href="../428588/index.html">IT digest of November events (part two)</a></li>
<li><a href="../428590/index.html">Micro-interactions and micro-prompts in the interface</a></li>
<li><a href="../428592/index.html">Stop hiring "effective managers." They are not only useless, but also harmful</a></li>
<li><a href="../428596/index.html">Ilon Mask fired Starlink satellite Internet project managers due to non-compliance with deadlines</a></li>
<li><a href="../428600/index.html">SSD with special effects or something that was missing for modding - HyperX FURY RGB drive overview</a></li>
<li><a href="../428602/index.html">We study Adversarial Tactics, Techniques & Common Knowledge (ATT @ CK). Enterprise Tactics. Part 4</a></li>
<li><a href="../428604/index.html">Online Data Science Championship</a></li>
<li><a href="../428606/index.html">‚ÄúUnderstanding how the system works allowed us to hack a lot‚Äù: Roy Beniosef about Android development</a></li>
<li><a href="../428608/index.html">Media holdings agreed with Yandex about the removal of pirated materials</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>