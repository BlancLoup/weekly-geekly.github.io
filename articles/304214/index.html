<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How the principal component method (PCA) works with a simple example</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I would like to talk about exactly how the principal component component analysis (PCA) works from the point of view of the intuition ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How the principal component method (PCA) works with a simple example</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/705/c34/273/705c34273b894d19ad8f6305f1c008c9.jpg"><br><br>  In this article I would like to talk about exactly how the principal component component analysis (PCA) works from the point of view of the intuition behind its mathematical apparatus.  As simple as possible, but in detail. <br><a name="habracut"></a><br>  Mathematics is generally very beautiful and elegant science, but sometimes its beauty is hidden behind a bunch of layers of abstraction.  It is best to show this beauty with simple examples, which, so to speak, can be twisted, played and felt, because in the end everything turns out to be much easier than it seems at first glance - the most important thing is to understand and imagine. <br><br>  In data analysis, as in any other analysis, it is sometimes superfluous to create a simplified model that most accurately describes the real state of affairs.  It often happens that the signs are quite dependent on each other and their simultaneous presence is redundant. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For example, our fuel consumption is measured in liters per 100 km, and in the United States in miles per gallon.  At first glance, the values ‚Äã‚Äãare different, but in fact they are strictly dependent on each other.  In a mile of 1600m, and in a gallon of 3.8l.  One sign strictly depends on another, knowing one, we know and another. <br><br>  But more often it happens that the signs depend on each other not so strictly and (importantly!) Not so clearly.  Engine size as a whole has a positive effect on acceleration to 100 km / h, but this is not always the case.  And it may also turn out that, taking into account factors that are not visible at first glance (such as improving fuel quality, using lighter materials and other modern advances), the year of the car is not much, but it also affects its acceleration. <br><br>  Knowing the dependencies and their strength, we can express several signs through one, merge, so to speak, and work with a simpler model.  Of course, most likely, it will not be possible to avoid information loss, but the PCA method will help us to minimize it. <br><br>  To put it more strictly, this method approximates an n-dimensional cloud of observations to an ellipsoid (also n-dimensional), the semi-axes of which will be the future main components.  And when projected on such axes (reduction of dimension), the greatest amount of information is saved. <br><br><h3>  Step 1. Data preparation </h3><br>  Here, for the sake of simplicity, I will not take real learning datasets for dozens of signs and hundreds of observations, but I will make my own, as simple as possible toy example.  2 signs and 10 observations will be quite enough to describe what, and most importantly - why, occurs in the depths of the algorithm. <br><br>  Generate a sample: <br><br><img src="https://habrastorage.org/files/f12/91d/85d/f1291d85dfaa45d9bab72fa2e3884f24.png" align="right"><pre><code class="python hljs">x = np.arange(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>) y = <span class="hljs-number"><span class="hljs-number">2</span></span> * x + np.random.randn(<span class="hljs-number"><span class="hljs-number">10</span></span>)*<span class="hljs-number"><span class="hljs-number">2</span></span> X = np.vstack((x,y)) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> X OUT: [[ <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">2.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">4.</span></span> <span class="hljs-number"><span class="hljs-number">5.</span></span> <span class="hljs-number"><span class="hljs-number">6.</span></span> <span class="hljs-number"><span class="hljs-number">7.</span></span> <span class="hljs-number"><span class="hljs-number">8.</span></span> <span class="hljs-number"><span class="hljs-number">9.</span></span> <span class="hljs-number"><span class="hljs-number">10.</span></span> ] [ <span class="hljs-number"><span class="hljs-number">2.73446908</span></span> <span class="hljs-number"><span class="hljs-number">4.35122722</span></span> <span class="hljs-number"><span class="hljs-number">7.21132988</span></span> <span class="hljs-number"><span class="hljs-number">11.24872601</span></span> <span class="hljs-number"><span class="hljs-number">9.58103444</span></span> <span class="hljs-number"><span class="hljs-number">12.09865079</span></span> <span class="hljs-number"><span class="hljs-number">13.78706794</span></span> <span class="hljs-number"><span class="hljs-number">13.85301221</span></span> <span class="hljs-number"><span class="hljs-number">15.29003911</span></span> <span class="hljs-number"><span class="hljs-number">18.0998018</span></span> ]]</code> </pre> <br clear="right"><br>  In this sample, we have two signs that strongly correlate with each other.  Using the PCA algorithm, we can easily find the sign-combination and, at the price of a piece of information, express both of these signs with one new one.  So, let's understand! <br><br>  First, some statistics.  Recall that moments are used to describe a random variable.  We need - mat.  waiting and dispersion.  You could say mate.  Expectation is the ‚Äúcenter of gravity‚Äù of a magnitude, and dispersion is its ‚Äúsize‚Äù.  Roughly speaking, mat.  the expectation specifies the position of the random variable, and the variance determines its size (more precisely, the spread). <br><br>  The projection onto the vector itself does not affect the mean values, since to minimize information loss, our vector must pass through the center of our sample.  Therefore, there is nothing terrible if we center our sample - we linearly shift it so that the average values ‚Äã‚Äãof the signs are equal to 0. This will greatly simplify our further calculations (although it is worth noting that you can do without centering). <br>  The operator inverse to the shift will be equal to the vector of initial averages - it will be needed to restore the sample to its original dimension. <br><br><img src="https://habrastorage.org/files/a8b/14b/a3d/a8b14ba3d26f4a4398b908591aa885db.png" align="right"><pre> <code class="python hljs">Xcentered = (X[<span class="hljs-number"><span class="hljs-number">0</span></span>] - x.mean(), X[<span class="hljs-number"><span class="hljs-number">1</span></span>] - y.mean()) m = (x.mean(), y.mean()) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> Xcentered <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Mean vector: "</span></span>, m OUT: (array([<span class="hljs-number"><span class="hljs-number">-4.5</span></span>, <span class="hljs-number"><span class="hljs-number">-3.5</span></span>, <span class="hljs-number"><span class="hljs-number">-2.5</span></span>, <span class="hljs-number"><span class="hljs-number">-1.5</span></span>, <span class="hljs-number"><span class="hljs-number">-0.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">1.5</span></span>, <span class="hljs-number"><span class="hljs-number">2.5</span></span>, <span class="hljs-number"><span class="hljs-number">3.5</span></span>, <span class="hljs-number"><span class="hljs-number">4.5</span></span>]), array([<span class="hljs-number"><span class="hljs-number">-8.44644233</span></span>, <span class="hljs-number"><span class="hljs-number">-8.32845585</span></span>, <span class="hljs-number"><span class="hljs-number">-4.93314426</span></span>, <span class="hljs-number"><span class="hljs-number">-2.56723136</span></span>, <span class="hljs-number"><span class="hljs-number">1.01013247</span></span>, <span class="hljs-number"><span class="hljs-number">0.58413394</span></span>, <span class="hljs-number"><span class="hljs-number">1.86599939</span></span>, <span class="hljs-number"><span class="hljs-number">7.00558491</span></span>, <span class="hljs-number"><span class="hljs-number">4.21440647</span></span>, <span class="hljs-number"><span class="hljs-number">9.59501658</span></span>])) Mean vector: (<span class="hljs-number"><span class="hljs-number">5.5</span></span>, <span class="hljs-number"><span class="hljs-number">10.314393916</span></span>)</code> </pre><br clear="right">  Dispersion strongly depends on the orders of the values ‚Äã‚Äãof the random variable, i.e.  sensitive to scaling.  Therefore, if the units of measurement of signs vary greatly in their orders, it is highly recommended to standardize them.  In our case, the values ‚Äã‚Äãdo not vary much in order, so for the sake of simplicity, we will not perform this operation. <br><br><h3>  Step 2. Covariance matrix </h3><br>  In the case of a multidimensional random variable (random vector), the position of the center will still be mat.  expectations of its projections on the axis.  But to describe its shape, its axial dispersions are no longer enough.  Look at these graphs, all three random variables have the same expectation and variance, and their projections on the axes as a whole will be the same! <br><br><img src="https://habrastorage.org/files/1ac/586/4ef/1ac5864effa34f0da4da9357c256251d.png"><br><br>  <b>To describe the shape of a random vector, a covariance matrix is ‚Äã‚Äãneeded.</b> <br><br>  This is a matrix for which <i>(i, j)</i> -element is a correlation of signs (X <sub>i</sub> , X <sub>j</sub> ).  Recall the covariance formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c28/191/5cf/c281915cfbe3478da9b2c96998fd8ef9.png"></div><br><br>  In our case, it is simplified, since <nobr>E (X <sub>i</sub> ) = E (X <sub>j</sub> ) = 0:</nobr> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8b5/ae8/550/8b5ae85500654985bab26246e391a23d.png"></div><br><br>  Note that when X <sub>i</sub> = X <sub>j</sub> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/fb5/8d9/51e/fb58d951efa6440dbafa0b6e849c9458.png"></div><br><br>  and this is true for any random variables. <br><br>  Thus, in our matrix there will be dispersion of signs along the diagonal (since i = j), and in the other cells - covariances of the corresponding pairs of features.  And due to the symmetry of the covariance, the matrix will also be symmetric. <br><br><blockquote>  <b>Note: The</b> covariance matrix is ‚Äã‚Äãa generalization of variance in the case of multidimensional random variables - it also describes the shape (variation) of a random variable, as does the variance. </blockquote><br>  Indeed, the dispersion of a one-dimensional random variable is a 1x1 covariance matrix, in which its single term is given by the formula Cov (X, X) = Var (X). <br><br>  So, let's form a covariance matrix <b>Œ£</b> for our sample.  To do this, we calculate the variances of X <sub>i</sub> and X <sub>j</sub> , as well as their covariance.  You can use the above formula, but since we are armed with Python, it's a sin not to use the <b>numpy.cov (X)</b> function.  It takes as input a list of all attributes of a random variable and returns its covariance matrix and where X is an n-dimensional random vector (n is the number of rows).  The function is excellent for calculating the unbiased dispersion, for the covariance of two quantities, and for compiling the covariance matrix. <br>  <i>(Let me remind you that in Python, the matrix is ‚Äã‚Äãrepresented by a column array of row arrays.)</i> <br><br><pre> <code class="python hljs">covmat = np.cov(Xcentered) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> covmat, <span class="hljs-string"><span class="hljs-string">"\n"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Variance of X: "</span></span>, np.cov(Xcentered)[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Variance of Y: "</span></span>, np.cov(Xcentered)[<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"Covariance X and Y: "</span></span>, np.cov(Xcentered)[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>] OUT: [[ <span class="hljs-number"><span class="hljs-number">9.16666667</span></span> <span class="hljs-number"><span class="hljs-number">17.93002811</span></span>] [ <span class="hljs-number"><span class="hljs-number">17.93002811</span></span> <span class="hljs-number"><span class="hljs-number">37.26438587</span></span>]] Variance of X: <span class="hljs-number"><span class="hljs-number">9.16666666667</span></span> Variance of Y: <span class="hljs-number"><span class="hljs-number">37.2643858743</span></span> Covariance X <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> Y: <span class="hljs-number"><span class="hljs-number">17.9300281124</span></span></code> </pre><br><h3>  Step 3. Eigenvectors and values ‚Äã‚Äã(ij pairs) </h3><br>  OK, we got a matrix that describes the shape of our random variable, from which we can get its dimensions in x and y (that is, X <sub>1</sub> and X <sub>2</sub> ), as well as an approximate shape on the plane.  Now we need to find such a vector (in our case only one), at which the size (variance) of the projection of our sample on it would be maximized. <br><br><blockquote>  <i><b>Note: The</b> generalization of variance to higher dimensions is the covariance matrix, and these two concepts are equivalent.</i>  <i>When projected onto a vector, the variance of the projection is maximized, and when projected onto spaces of higher orders, all its covariance matrix is ‚Äã‚Äãprojected.</i> </blockquote><br>  So, take the unit vector onto which we will project our random vector X. Then the projection onto it will be equal to v <sup>T</sup> X. The dispersion of the projection onto the vector will be equal to Var (v <sup>T</sup> X), respectively.  In general, in vector form (for centered quantities), the variance is expressed as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e43/b5d/feb/e43b5dfeb1ac4152a8a2c052cddff0a3.png"></div><br><br>  Accordingly, the projection variance: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f6e/141/ced/f6e141cedfb04726896d66d95d2b913b.png"></div><br><br>  It is easy to see that the variance is maximized at the maximum value of v <sup>T</sup> Œ£v.  Here Rayleigh's attitude will help us.  Without going too deep into mathematics, I‚Äôll just say that the Rayleigh relationship has a special case for covariance matrices: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ed6/995/7a4/ed69957a41a248488b29efac7f7ad096.png"></div><br>  and <br><div style="text-align:center;"><img src="https://habrastorage.org/files/a01/6c9/d14/a016c9d140a144d8b79bf7924de7e297.png"></div><br><br>  The latter formula should be familiar with the topic of the decomposition of the matrix into eigenvectors and values.  x is an eigenvector, and Œª is an eigenvalue.  The number of eigenvectors and values ‚Äã‚Äãis equal to the size of the matrix (and the values ‚Äã‚Äãcan be repeated). <br><br><blockquote>  By the way, in English, eigenvalues ‚Äã‚Äãand vectors are called <b>eigenvalues</b> and <b>eigenvectors,</b> respectively. <br>  It seems to me that this sounds much more beautiful (and briefly) than our terms. </blockquote><br>  <b>Thus, the direction of the maximum dispersion of the projection always coincides with the Igenvector, which has a maximum eigenvalue equal to the magnitude of this dispersion</b> . <br><br>  And this is also true for projections onto a larger number of dimensions ‚Äî the variance (covariance matrix) of the projection onto the m-dimensional space will be maximum in the direction of the m highgenvectors with maximal eigenvalues. <br><br>  The dimension of our sample is two and the number of aigenvectors for it, respectively, is 2. Find them. <br><br>  The numpy library implements the <b>numpy.linalg.eig (X)</b> function, where X is a square matrix.  It returns 2 arrays - an array of origin values ‚Äã‚Äãand an array of source vectors (column vectors).  And the vectors are normalized - their length is 1. Just what we need.  These 2 vectors define a new basis for the sample, such that its axes coincide with the semi-axes of the approximating ellipse of our sample. <br><br><img src="https://habrastorage.org/files/695/466/809/6954668091cf4bb1b64a3ee4629fd79a.png"><br>  On this graph, we approximated our sample with an ellipse with radii of 2 sigma (i.e., it should contain 95% of all observations - which, in principle, we are seeing here).  I have inverted a larger vector (the function eig (X) sent it in the opposite direction) - the direction, not the orientation of the vector, is important for us. <br><br><h3>  Step 4. Reduction dimension (projection) </h3><br>  The largest vector has a direction similar to the regression line and, having projected our sample onto it, we lose information comparable to the sum of the residual regression members (only the distance is now Euclidean, not the delta in Y).  In our case, the relationship between the signs is very strong, so that the loss of information will be minimal.  The ‚Äúprice‚Äù of the projection ‚Äî dispersion along the smaller igengenctor ‚Äî as seen from the previous graph, is very small. <br><br><blockquote>  <b>Note: the</b> diagonal elements of the covariance matrix show the variances according to the initial basis, and its eigenvalues ‚Äã‚Äã- according to the new (according to the main components). </blockquote><br>  It is often required to estimate the amount of information lost (and stored).  It is most convenient to present in percentage.  We take the variances along each axis and divide by the total sum of the variances along the axes (that is, the sum of all the eigenvalues ‚Äã‚Äãof the covariance matrix). <br>  Thus, our larger vector describes 45.994 / 46.431 * 100% = 99.06%, and the smaller one, respectively, approximately 0.94%.  Rejecting a smaller vector and projecting the data to a larger one, we lose less than 1% of the information!  Excellent result! <br><br><blockquote>  <b>Note:</b> In practice, in most cases, if the total loss of information is no more than 10-20%, then you can safely reduce the dimension. </blockquote><br>  To carry out the projection, as mentioned earlier in step 3, it is necessary to carry out the operation v <sup>T</sup> X (the vector must be of length 1).  Or, if we have not one vector, but a hyperplane, then instead of vector v <sup>T</sup> we take the matrix of basis vectors V <sup>T.</sup>  The resulting vector (or matrix) will be an array of projections of our observations. <br><br><pre> <code class="python hljs">_, vecs = np.linalg.eig(covmat) v = -vecs[:,<span class="hljs-number"><span class="hljs-number">1</span></span>]) Xnew = dot(v,Xcentered) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> Xnew OUT: [ <span class="hljs-number"><span class="hljs-number">-9.56404107</span></span> <span class="hljs-number"><span class="hljs-number">-9.02021624</span></span> <span class="hljs-number"><span class="hljs-number">-5.52974822</span></span> <span class="hljs-number"><span class="hljs-number">-2.96481262</span></span> <span class="hljs-number"><span class="hljs-number">0.68933859</span></span> <span class="hljs-number"><span class="hljs-number">0.74406645</span></span> <span class="hljs-number"><span class="hljs-number">2.33433492</span></span> <span class="hljs-number"><span class="hljs-number">7.39307974</span></span> <span class="hljs-number"><span class="hljs-number">5.3212742</span></span> <span class="hljs-number"><span class="hljs-number">10.59672425</span></span>]</code> </pre><br>  <i><b>dot (X, Y)</b> - term by product (as we multiply vectors and matrices in Python)</i> <br><br>  It is easy to see that the values ‚Äã‚Äãof the projections correspond to the picture in the previous graph. <br><br><h3>  Step 5. Data Recovery </h3><br>  It is convenient to work with projection, build hypotheses on its basis and develop models.  But not always obtained the main components will have a clear, understandable to a stranger, meaning.  Sometimes it is useful to decode, for example, the detected outliers, in order to see what they are observing. <br><br>  It is very simple.  We have all the necessary information, namely, the coordinates of the basis vectors in the original basis (the vectors on which we have projected) and the vector of means (to cancel the alignment).  Take, for example, the largest value: 10.596 ... and decode it.  To do this, multiply it on the right by the transposed vector and add the vector of averages, or in general, for the entire sample: X <sup>T</sup> v <sup>T</sup> + m <br><br><pre> <code class="python hljs">n = <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-comment"><span class="hljs-comment">#    Xrestored = dot(Xnew[n],v) + m print 'Restored: ', Xrestored print 'Original: ', X[:,n] OUT: Restored: [ 10.13864361 19.84190935] Original: [ 10. 19.9094105]</span></span></code> </pre><br>  The difference is small, but it is.  After all, lost information is not restored.  However, if simplicity is more important than accuracy, the reconstructed value perfectly approximates the original one. <br><br><h3>  Instead of a conclusion - check the algorithm </h3><br>  So, we have disassembled the algorithm, showed how it works on a toy example, now it remains only to compare it with the PCA implemented in sklearn - because we will use it. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.decomposition <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PCA pca = PCA(n_components = <span class="hljs-number"><span class="hljs-number">1</span></span>) XPCAreduced = pca.fit_transform(transpose(X))</code> </pre><br>  The <b>n_components</b> parameter indicates the number of dimensions for which the projection will be made, that is, up to how many dimensions we want to reduce our data.  In other words, these are n iigenvectors with the largest eigenvalues.  Check the result of reducing the dimension: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Our reduced X: \n'</span></span>, Xnew <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Sklearn reduced X: \n'</span></span>, XPCAreduced OUT: Our reduced X: [ <span class="hljs-number"><span class="hljs-number">-9.56404106</span></span> <span class="hljs-number"><span class="hljs-number">-9.02021625</span></span> <span class="hljs-number"><span class="hljs-number">-5.52974822</span></span> <span class="hljs-number"><span class="hljs-number">-2.96481262</span></span> <span class="hljs-number"><span class="hljs-number">0.68933859</span></span> <span class="hljs-number"><span class="hljs-number">0.74406645</span></span> <span class="hljs-number"><span class="hljs-number">2.33433492</span></span> <span class="hljs-number"><span class="hljs-number">7.39307974</span></span> <span class="hljs-number"><span class="hljs-number">5.3212742</span></span> <span class="hljs-number"><span class="hljs-number">10.59672425</span></span>] Sklearn reduced X: [[ <span class="hljs-number"><span class="hljs-number">-9.56404106</span></span>] [ <span class="hljs-number"><span class="hljs-number">-9.02021625</span></span>] [ <span class="hljs-number"><span class="hljs-number">-5.52974822</span></span>] [ <span class="hljs-number"><span class="hljs-number">-2.96481262</span></span>] [ <span class="hljs-number"><span class="hljs-number">0.68933859</span></span>] [ <span class="hljs-number"><span class="hljs-number">0.74406645</span></span>] [ <span class="hljs-number"><span class="hljs-number">2.33433492</span></span>] [ <span class="hljs-number"><span class="hljs-number">7.39307974</span></span>] [ <span class="hljs-number"><span class="hljs-number">5.3212742</span></span> ] [ <span class="hljs-number"><span class="hljs-number">10.59672425</span></span>]]</code> </pre><br>  We returned the result as a matrix of column vector observations (this is a more canonical form from the point of view of linear algebra), while PCA returns a vertical array to sklearn. <br><br>  In principle, this is not critical, it is simply worth noting that in linear algebra, matrix canonically be written through column vectors, and in data analysis (and other DB-related areas) observations (transactions, records) are usually written in rows. <br><br>  We also check other model parameters ‚Äî the function has a number of attributes that allow access to intermediate variables: <br><br>  - Medium vector: <b>mean_</b> <br>  - Projection vector (matrix): <b>components_</b> <br>  - Dispersion of the axes of the projection (selective): <b>explained_variance_</b> <br>  - Information share (share of total variance): <b>explained_variance_ratio_</b> <br><br><blockquote>  <b>Note:</b> explained_variance_ shows the <u>sample</u> variance, while the cov () function for building the covariance matrix calculates <u>unbiased</u> variances! </blockquote><br>  Let's compare our values ‚Äã‚Äãwith the values ‚Äã‚Äãof the library function. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Mean vector: '</span></span>, pca.mean_, m <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Projection: '</span></span>, pca.components_, v <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">'Explained variance ratio: '</span></span>, pca.explained_variance_ratio_, l[<span class="hljs-number"><span class="hljs-number">1</span></span>]/sum(l) OUT: Mean vector: [ <span class="hljs-number"><span class="hljs-number">5.5</span></span> <span class="hljs-number"><span class="hljs-number">10.31439392</span></span>] (<span class="hljs-number"><span class="hljs-number">5.5</span></span>, <span class="hljs-number"><span class="hljs-number">10.314393916</span></span>) Projection: [[ <span class="hljs-number"><span class="hljs-number">0.43774316</span></span> <span class="hljs-number"><span class="hljs-number">0.89910006</span></span>]] (<span class="hljs-number"><span class="hljs-number">0.43774316434772387</span></span>, <span class="hljs-number"><span class="hljs-number">0.89910006232167594</span></span>) Explained variance: [ <span class="hljs-number"><span class="hljs-number">41.39455058</span></span>] <span class="hljs-number"><span class="hljs-number">45.9939450918</span></span> Explained variance ratio: [ <span class="hljs-number"><span class="hljs-number">0.99058588</span></span>] <span class="hljs-number"><span class="hljs-number">0.990585881238</span></span></code> </pre><br>  The only difference is in the variances, but as already mentioned, we used the cov () function, which uses unbiased variance, whereas the explained_variance_ attribute returns selective.  They differ only in the fact that the first divides by (n-1) to get a mathematical expectation, and the second divides by n.  It is easy to verify that 45.99 ‚àô (10 - 1) / 10 = 41.39. <br><br>  All other values ‚Äã‚Äãare the same, which means that our algorithms are equivalent.  And finally, I note that the attributes of the library algorithm have less accuracy, since it is probably optimized for speed, or just rounds the values ‚Äã‚Äãfor convenience (or I have some glitches). <br><br><blockquote><img src="https://habrastorage.org/files/b46/6fe/138/b466fe138a54493895d45e9743df1f63.png" align="right"><br><br>  <b>Note: the</b> library method automatically projects on the axes that maximize dispersion.  It is not always rational.  For example, in this figure, an inaccurate reduction in dimensionality will lead to the fact that the classification will become impossible.  However, the projection onto a smaller vector will successfully reduce the dimension and preserve the classifier. <br><br></blockquote><br clear="right"><br>  So, we reviewed the principles of the PCA algorithm and its implementation in sklearn.  I hope this article was clear enough for those who are just starting to become familiar with data analysis, and also at least a little informative for those who know this algorithm well.  An intuitive presentation is extremely useful for understanding how a method works, and understanding is very important for correct tuning of the selected model.  Thanks for attention! <br><br>  <b>PS:</b> Please do not scold the author for possible inaccuracies.  The author himself is in the process of exploring the data analysis and wants to help the same as he is in the process of mastering this amazing field of knowledge!  But constructive criticism and diverse experience are strongly encouraged! </div><p>Source: <a href="https://habr.com/ru/post/304214/">https://habr.com/ru/post/304214/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../304204/index.html">Yandex "Mail for Domain" as a mail gateway for your servers</a></li>
<li><a href="../304206/index.html">Checking the source code of WPF Samples from Microsoft</a></li>
<li><a href="../304208/index.html">AndroidAudit. Your Android application as a crime scene</a></li>
<li><a href="../304210/index.html">On the relative brightness, or how tenacious is Legacy</a></li>
<li><a href="../304212/index.html">The book "Head First. Learning Ruby</a></li>
<li><a href="../304218/index.html">Harvard CS50 course in Russian: second lecture appeared</a></li>
<li><a href="../304220/index.html">Building a chain of trust in PKI, is it all so simple</a></li>
<li><a href="../304222/index.html">Organization of access to the Moscow metro WI-FI network from a security point of view</a></li>
<li><a href="../304226/index.html">How to become a specialist in the field of "big data"?</a></li>
<li><a href="../304228/index.html">MQTT and Modbus: a comparison of the protocols used in IoT gateways</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>