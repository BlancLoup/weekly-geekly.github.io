<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Collect data using Scrapy</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Here already casually mentions of this framework for data collection skipped. The tool is really powerful and deserves more attention. In this review ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Collect data using Scrapy</h1><div class="post__text post__text-html js-mediator-article">  Here already casually mentions of this <a href="http://scrapy.org/">framework</a> for data collection skipped.  The tool is really powerful and deserves more attention.  In this review I will tell you how <br><br><img src="https://habrastorage.org/storage/b479cd35/f38a186d/f58de30c/120ddbe6.jpg" alt="scrapy" align="right"><br><br><ul><li>  create a spider that performs GET requests, </li><li>  extract data from an HTML document </li><li>  process and export data. </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <br><br><br><a name="habracut"></a><br><h4>  Installation </h4><br><br>  Requirements: Python 2.5+ (3rd branch not supported), Twisted, lxml or libxml2, simplejson, pyopenssl (for HTTPS support) <br><br>  No problem installed from the <a href="http://doc.scrapy.org/topics/ubuntu.html">Ubuntu</a> repositories.  The <a href="http://doc.scrapy.org/intro/install.html">Installation guide</a> page describes installation in other Linux distributions, as well as in Mac OS X and Windows. <br><br><h4>  Task </h4><br><br>  Probably, someone wants to parse an online store and pull out the entire catalog with product descriptions and photos, but I will not intentionally do that.  Let's take some open data, for example, a <a href="http://abitur.nica.ru/">list of educational institutions</a> .  The site is quite typical and it can show several techniques. <br><br>  Before writing a spider, you need to examine the source site.  Note, the site is built on frames (?!), In the frameset we are looking for a frame with a <a href="http://abitur.nica.ru/new/www/index.php">start page</a> .  Here is a search form.  Suppose we only need universities in Moscow, so fill out the appropriate field, click "Find". <br><br>  We analyze.  We have a page with pagination links, 15 universities per page.  Filter parameters are transmitted via GET, only the page value is changed. <br><br>  So, we formulate the problem: <br><br><ol><li>  Go to <a href="http://abitur.nica.ru/new/www/search.php%3Fregion%3D77%26town%3D0%26opf%3D0%26type%3D0%26spec%3D0%26ed_level%3D0%26ed_form%3D0%26qualif%3D%26substr%3D%26page%3D1">abitur.nica.ru/new/www/search.php?region=77&amp;town=0&amp;opf=0&amp;type=0&amp;spec=0&amp;ed_level=0&amp;ed_form=0&amp;qualif=&amp;substr=&amp;page=1</a> </li><li>  Go through each page with the results, changing the value of the page </li><li>  Go to the university description <a href="http://abitur.nica.ru/new/www/vuz_detail.php%3Fcode%3D486%26region%3D77%26town%3D0%26opf%3D0%26type%3D0%26spec%3D0%26ed_level%3D0%26ed_form%3D0%26qualif%3D%26substr%3D%26page%3D1">abitur.nica.ru/new/www/vuz_detail.php?code=486¬Æion=77&amp;town=0&amp;opf=0&amp;type=0&amp;spec=0&amp;ed_level=0&amp;ed_form=0&amp;qualif=&amp;substr=&amp;page=1</a> </li><li>  Save a detailed description of the university in the CSV file </li></ol><br><br><h4>  Creating a project </h4><br><br>  Go to the folder where our project will be located, create it: <br><br><pre><code class="bash hljs">scrapy startproject abitur <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> abitur</code> </pre> <br><br>  In the <b>abitur</b> folder of our project there are files: <br><br><ul><li>  <b>items.py</b> contains classes that list the fields of the data to be collected, </li><li>  <b>pipelines.py</b> allows you to specify certain actions when opening / closing a spider, saving data, </li><li>  <b>settings.py</b> contains custom spider settings, </li><li>  <b>spiders</b> - the folder in which the files are stored with the classes of spiders.  Each spider is usually written in a separate file named name_spider.py. </li></ul><br><br><h4>  Spider </h4><br><br>  In the created file spiders / abitur_spider.py we describe our spider <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturSpider</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(CrawlSpider)</span></span></span><span class="hljs-class">:</span></span> name = <span class="hljs-string"><span class="hljs-string">"abitur"</span></span> allowed_domains = [<span class="hljs-string"><span class="hljs-string">"abitur.nica.ru"</span></span>] start_urls = [<span class="hljs-string"><span class="hljs-string">"http://abitur.nica.ru/new/www/search.php?region=77&amp;town=0&amp;opf=0&amp;type=0&amp;spec=0&amp;ed_level=0&amp;ed_form=0&amp;qualif=&amp;substr=&amp;page=1"</span></span>] rules = ( Rule(SgmlLinkExtractor(allow=(<span class="hljs-string"><span class="hljs-string">'search\.php\?.+'</span></span>)), follow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), Rule(SgmlLinkExtractor(allow=(<span class="hljs-string"><span class="hljs-string">'vuz_detail\.php\?.+'</span></span>)), callback=<span class="hljs-string"><span class="hljs-string">'parse_item'</span></span>), ) <span class="hljs-string"><span class="hljs-string">"..."</span></span></code> </pre> <br><br>  Our class is inherited from <b>CrawlSpider</b> , which will allow us to register link templates that the spider itself will extract and navigate through them. <br><br>  In order: <br><br><ul><li>  name - the name of the spider, is used to run, </li><li>  allowed_domains - the domains of the site, outside of which the spider should not look for anything, </li><li>  start_urls - a list of starting addresses </li><li>  rules - a list of rules for extracting links. </li></ul><br><br>  As you noticed, among the rules, the callback function is passed as a parameter.  We will be back soon. <br><br><h4>  Items </h4><br><br>  As I said before, <b>items.py</b> contains classes that list the fields of the data to be collected. <br>  This can be done like this: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturItem</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Item)</span></span></span><span class="hljs-class">:</span></span> name = Field() state = Field() <span class="hljs-string"><span class="hljs-string">"..."</span></span></code> </pre> <br><br>  Parsed data can be processed before export.  For example, an educational institution can be ‚Äústate‚Äù and ‚Äúnon-state‚Äù, and we want to store this value in boolean format or the date ‚ÄúJanuary 1, 2011‚Äù should be recorded as ‚Äú01/01/2011‚Äù. <br><br>  To do this, there are input and output handlers, so we write the state field differently: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturItem</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Item)</span></span></span><span class="hljs-class">:</span></span> name = Field() state = Field(input_processor=MapCompose(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> s: <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> re.match(<span class="hljs-string"><span class="hljs-string">u'\s*'</span></span>, s))) <span class="hljs-string"><span class="hljs-string">"...."</span></span></code> </pre> <br><br>  <a href="http://doc.scrapy.org/topics/loaders.html">MapCompose</a> is applied to each state list item. <br><br><h4>  Search for items on the page </h4><br><br>  We return to our parse_item method. <br><br>  For each <b>Item</b> element you can use your own loader.  Its purpose is also related to data processing. <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturLoader</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(XPathItemLoader)</span></span></span><span class="hljs-class">:</span></span> default_input_processor = MapCompose(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> s: re.sub(<span class="hljs-string"><span class="hljs-string">'\s+'</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>, s.strip())) default_output_processor = TakeFirst() <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturSpider</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(CrawlSpider)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"..."</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parse_item</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, response)</span></span></span><span class="hljs-function">:</span></span> hxs = HtmlXPathSelector(response) l = AbiturLoader(AbiturItem(), hxs) l.add_xpath(<span class="hljs-string"><span class="hljs-string">'name'</span></span>, <span class="hljs-string"><span class="hljs-string">'//td[@id="content"]/h1/text()'</span></span>) l.add_xpath(<span class="hljs-string"><span class="hljs-string">'state'</span></span>, <span class="hljs-string"><span class="hljs-string">'//td[@id="content"]/div/span[@class="gray"]/text()'</span></span>) <span class="hljs-string"><span class="hljs-string">"..."</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> l.load_item()</code> </pre> <br><br>  In our case, extreme and duplicate spaces are removed from each field.  You can also add individual rules to the bootloader, which we did in the AbiturItem class: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">AbiturLoader</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(XPathItemLoader)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-string"><span class="hljs-string">"..."</span></span> state_in = MapCompose(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> s: <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> re.match(<span class="hljs-string"><span class="hljs-string">u'\s*'</span></span>, s))</code> </pre> <br><br>  So, do as you like. <br><br>  The parse_item () function returns an Item object, which is passed to Pipeline (described in pipelines.py).  There you can write your own classes for saving data in formats that are not provided by the standard Scrapy functionality.  For example, export to mongodb. <br><br>  The fields of this element are set using <b>XPath</b> , which can be read <a href="http://habrahabr.ru/blogs/webdev/114772/">here</a> or <a href="http://msdn.microsoft.com/ru-ru/library/ms256471%2528v%3DVS.90%2529.aspx">here</a> .  If you use FirePath, note that it adds the tbody tag inside the table.  Use the built-in <a href="http://doc.scrapy.org/topics/shell.html">console</a> to verify <b>XPath</b> paths. <br><br>  And one more note.  When you use <b>XPath</b> , the results found are returned in the form of a list, so it is convenient to connect the <b>TakeFirst</b> output processor, which takes the first element of this list. <br><br><h4>  Launch </h4><br><br>  You can get the source code here, to launch it, go to the project folder and type in the console <br><br><pre> <code class="bash hljs">scrapy crawl abitur --<span class="hljs-built_in"><span class="hljs-built_in">set</span></span> FEED_URI=scraped_data.csv --<span class="hljs-built_in"><span class="hljs-built_in">set</span></span> FEED_FORMAT=csv</code> </pre> <br><br>  In short, I described everything, but this is only a small part of the <b>Scrapy features</b> : <br><ul><li>  find and extract their HTML and XML data </li><li>  data conversion before export </li><li>  export to JSON, CSV, XML </li><li>  file download </li><li>  framework extension with own middlewares, pipelines </li><li>  POST requests, support for cookies and sessions, authentication </li><li>  substitution user-agent </li><li>  shell debug console </li><li>  logging system </li><li>  monitoring via web-interface </li><li>  telnet console management </li></ul><br><br>  It‚Äôs impossible to describe all all in one article, so ask questions in the comments, read the <a href="http://doc.scrapy.org/">documentation</a> , suggest topics for future articles on Scrapy. <br><br>  The working example posted on <a href="https://github.com/bekbulatov/scrapy-abitur">GitHub</a> . </div><p>Source: <a href="https://habr.com/ru/post/115710/">https://habr.com/ru/post/115710/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../115704/index.html">20 most necessary tools to check the site display</a></li>
<li><a href="../115705/index.html">Highlight your website in Speed ‚Äã‚ÄãDial</a></li>
<li><a href="../115706/index.html">Purchase at the Apple Store with delivery to Russia</a></li>
<li><a href="../115708/index.html">Enliven STB Amino</a></li>
<li><a href="../115709/index.html">Android 2.x is already used on 92% of Android devices</a></li>
<li><a href="../115711/index.html">kedDroid - S02E06. Video review software for Android</a></li>
<li><a href="../115713/index.html">iOS 4.3 vs. Android 2.3: testing download speed sites</a></li>
<li><a href="../115714/index.html">PrestaShop 1.4 - a new version of the online store platform</a></li>
<li><a href="../115715/index.html">We train MacBook to work with an external monitor</a></li>
<li><a href="../115716/index.html">Open Day "Business Informatics" at St. Petersburg State University</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>