<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Some repositories to help learners and teachers of Python and machine learning.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to the community! 

 I am Yuri Kashnitsky, I used to do a review of some MOOCs on computer science here and searched for ‚Äúoutliers‚Äù among Playbo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Some repositories to help learners and teachers of Python and machine learning.</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/b63/833/7a8/b638337a8df741af84ea0cae4cf110f9.png"><br><br>  Hello to the community! <br><br>  I am Yuri Kashnitsky, I used to <a href="http://habrahabr.ru/post/248069/">do</a> a review of some MOOCs on computer science here and <a href="http://habrahabr.ru/post/251225/">searched for</a> ‚Äúoutliers‚Äù among Playboy models. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Now I teach Python and machine learning at the Faculty of Computer Science at HSE and in the online <a href="http://mlclass.ru/">MLClass</a> data analysis community course, as well as machine learning and big data analysis at a data school of one of the Russian telecom operators. <br><br>  Why not Sunday evening to share with the community materials on Python and a review of machine learning repositories ... In the first part there will be a description of the GitHub repository with IPython notebooks on Python programming.  The second is an example of course material ‚ÄúMachine Learning with Python‚Äù.  In the third part, I will show one of the tricks used by the participants of the Kaggle competition, specifically, Stanislav Semenov (4th place in the current world rating of Kaggle).  Finally, I'll review the cool GitHub repositories for programming, data analysis, and machine learning in Python. <br><br><a name="habracut"></a><br><br><h2>  Part 1. Python programming course in the form of IPython notebooks </h2><br><br>  The course consists of 5 lessons: a review of development tools, an introduction to the Python language, 2 lessons about data structures (Python and not only) and some algorithms and one lesson about functions and recursion.  Yes, the topic of OOP and heaps of everything useful is not touched upon, but work on the course is under way, the repository will be updated. <br><br>  The objectives of this course are: <br><ul><li>  Learn the basics of the Python language </li><li>  Introduce the main data structures. </li><li>  Give the skills needed to develop simple algorithms </li></ul><br><br>  The IPython notebooks are chosen as the main means of presenting the material due to the fact that they can combine text, images, formulas and code.  As a demonstration in the repository, a notebook about decision trees and their implementation in the <a href="http://scikit-learn.org/">Scikit-learn</a> machine learning library is <a href="http://scikit-learn.org/">provided</a> .  The materials of the first lesson describe how to use these notebooks properly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/4a2/b22/309/4a2b2230988b4e0b86317f92fddd8daa.jpg" width="60%" height="60%"></div><br><br>  Here is a <a href="https://github.com/Yorko/python_intro">link</a> to the GitHub repository with IPython notebooks.  Branch and multiply, you can use for any purpose other than commercial, with reference to the author.  If you want to connect to the project GitHub - write.  You can report bugs in the materials. <br><br><h2>  Part 2. Example material for the course "Machine Learning with Python". Decision Trees </h2><br><br>  The course ‚ÄúMachine learning using Python‚Äù consists of 6 lessons and about 35 notebooks IPython about the basics of machine learning with the Scikit-learn library, using the NumPy, SciPy, Pandas libraries, with examples of solving real Kaggle problems and parsing practices used by the winners of Kaggle competitions - stacking, blending, ensembles of algorithms. Detailed program <a href="http://dscourse.mlclass.ru/">here</a> . As an example of the presentation of the material consider one of the first topics - decision trees. <br><br><h3>  What is a decision tree? </h3><br><br>  We begin the review of classification methods, of course, with the most popular - decision tree. <br>  Decision trees are used in everyday life in the most diverse areas of human activity, sometimes very far from machine learning.  Decision tree can be called visual instruction what to do in what situation.  Let us give an example from the field of consulting research institutes.  The Higher School of Economics publishes info-schemes that make life easier for its employees.  Here is a fragment of the instruction for publishing a scientific article on the institute portal. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2e5/607/3a3/2e56073a327147c38b769ad911b564db.png" width="60%" height="60%"></div><br><br>  In terms of machine learning, we can say that this is an elementary classifier that determines the form of publication on the portal (book, article, chapter of the book, preprint, publication in ‚ÄúHSE and the media‚Äù) on several grounds - the type of publication (monograph, brochure, article and etc.), the type of publication in which the article was published (a scientific journal, a collection of papers, etc.) and the rest. <br>  Often, a decision tree serves as a synthesis of expert experience, a means of transferring knowledge to future employees or a model of a company's business process.  For example, prior to the introduction of scalable machine learning algorithms in the banking sector, the task of credit scoring was solved by experts.  The decision to issue a loan to the borrower was made on the basis of some intuitively (or by experience) derived rules that can be represented as a decision tree. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/194/9b6/ae9/1949b6ae97ab4fc9b1a37fbf182eda8f.gif"></div><br><br>  In this case, we can say that the binary classification problem is being solved (the target class has two meanings - ‚ÄúGive out credit‚Äù and ‚ÄúRefuse‚Äù) according to the signs ‚ÄúAge‚Äù, ‚ÄúHousehold‚Äù, ‚ÄúIncome‚Äù and ‚ÄúEducation‚Äù. <br>  The decision tree as an algorithm for machine learning is essentially the same, combining logical rules of the form ‚ÄúCharacteristic value a is less than x And Characteristic value b is less than y ... =&gt; Class 1‚Äù into the data structure ‚ÄúTree‚Äù.  The tremendous advantage of decision trees is that they are easily interpretable, understandable to humans.  For example, according to the scheme in Figure 2, the borrower can be explained why he was denied a loan.  For example, because he does not have a home and the income is less than 5000. As we will see later, many other, albeit more accurate, models do not have this property and can rather be viewed as a ‚Äúblack box‚Äù into which they downloaded data and received a response.  Due to this ‚Äúclarity‚Äù of decision trees and their similarity to the model of decision making by a person (you can easily explain your model to the boss), decision trees have gained immense popularity, and one of the representatives of this group of classification methods, C4.5, is considered first in the list 10 best data mining algorithms (Top 10 algorithms in data mining, 2007). <br><br><h3>  How to build a decision tree </h3><br><br>  In the example of credit scoring, we saw that the decision to issue a loan was made on the basis of age, availability of real estate, income, and others.  But which sign to choose first?  To do this, consider an example more simply, where all the signs are binary. <br>  Here you can recall the game ‚Äú20 questions‚Äù, which is often mentioned in the introduction to decision trees.  Surely everyone was playing it.  One person makes a celebrity, and the second tries to guess by asking only questions that can be answered with ‚ÄúYes‚Äù or ‚ÄúNo‚Äù (omitting the options ‚ÄúI don‚Äôt know‚Äù and ‚ÄúI can‚Äôt say‚Äù).  What question guesses ask the first thing?  Of course, one that will reduce the number of remaining options the most.  For example, the question ‚ÄúIs this Angelina Jolie?‚Äù In the case of a negative answer will leave more than 6 billion options for further searching (of course, less, not every person is a celebrity, but still a lot), but the question ‚ÄúIs this a woman?‚Äù half celebrities.  That is, the sign of ‚Äúsex‚Äù is much better shared by a sample of people than the sign of ‚Äúthis is Angelina Jolie‚Äù, ‚ÄúSpanish nationality‚Äù or ‚Äúloves football‚Äù.  This is intuitively consistent with the concept of information gain based on entropy. <br><br><h3>  Entropy </h3><br><br>  Shannon entropy is defined as <br><br><img src="https://habrastorage.org/files/abe/49b/cfe/abe49bcfefee4648bd8cdc6ee2ed22f4.PNG"><br><br>  where N is the number of possible implementations, p_i (sorry, I didn‚Äôt bother with Habr's integration with TeX, it's time to deploy MathJax) - the probabilities of these implementations.  This is a very important concept used in physics, information theory and other fields.  Omitting the premises of the introduction (combinatorial and information-theoretic) of this concept, we note that, intuitively, entropy corresponds to the degree of chaos in the system.  The higher the entropy, the less ordered the system and vice versa.  This will help there to formalize the ‚Äúeffective division of the sample‚Äù, which we talked about in the context of the game ‚Äú20 questions‚Äù. <br><br><h3>  Example </h3><br><br>  Consider a toy example in which we predict the color of the ball according to its coordinate.  Of course, it has nothing to do with life, but it does allow showing how entropy is used to build a decision tree.  Pictures are taken from <a href="http://habrahabr.ru/post/171759/">this</a> article. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a61/a96/d2f/a61a96d2f83c4e939c0828494c5efa5e.png"></div><br><br>  There are 9 blue balls and 11 yellow balls.  If we pulled a ball at random, it will be blue with probability p1 = 9/20 and yellow with probability p2 = 11/20.  This means that the state entropy is S0 = -9/20 log (9/20) -11/20 log (11/20) = ~ 1. <br>  This value itself does not tell us anything yet. <br>  Now let's see how the entropy changes, if the balls are divided into two groups - with a coordinate less than or equal to 12 and more than 12. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a8b/e4f/a40/a8be4fa40b6c4cbcb11cf15bb7d4f4bf.png" width="60%" height="60%"></div><br><br>  In the left group were 13 balls, of which 8 are blue and 5 are yellow.  The entropy of this group is S1 = -5/13 log (5/13) -8/13 log (8/13) = ~ 0.96.  In the right group were 7 balls, of which 1 is blue and 6 are yellow.  The entropy of the right group is S2 = - 1/7 log (1/7) - 6/7 log (6/7) = ~ 0.6.  As you can see, the entropy decreased in both groups as compared with the initial state, although not much in the left state.  Since entropy is essentially a degree of chaos (or uncertainty) in a system, a decrease in entropy is called an increase in information.  Formally, the information gain (information gain, IG) when dividing a sample on the basis of Q (in our example, it is ‚Äúx &lt;= 12‚Äù) is defined as <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a09/de0/238/a09de023878e4a59892dd74e6771898d.PNG"></div><br><br>  where q is the number of groups after splitting, N_i is the number of sample elements whose attribute Q has the i-th value.  In our case, after separation, we got two groups (q = 2) ‚Äîone of the 13 elements (N1 = 13), the second of 7 (N2 = 7).  The increase in information turned out to be IG (‚Äúx &lt;= 12‚Äù) = S0 - 13/20 S1 - 7/20 S2 = ~ 0.16. <br>  It turns out that by dividing the balls into two groups on the basis of a ‚Äúcoordinate less or equal to 12‚Äù, we have already received a more ordered system than at the beginning.  We continue the division of the balls into groups until the balls in each group are of the same color. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/216/f0f/75e/216f0f75e4424bc8adc81c98f07aae16.png" width="60%" height="60%"></div><br><br>  For the right group, it took only one additional splitting on the basis of ‚Äúthe coordinate is less or equal to 18‚Äù with the increase in information, for the left - three more.  Obviously, the entropy of a group with balls of the same color is 0 (log (1) = 0), which corresponds to the idea that the group of balls of the same color is ordered.  As a result, a decision tree was built, predicting the color of the ball according to its coordinate.  Note that such a decision tree can work poorly for new objects (determining the color of new balls), since it ideally adjusted to the training sample (the original 20 balls).  To classify new balls, a tree with a smaller number of ‚Äúquestions‚Äù, or divisions, is better suited, even if it does not ideally divide the training set by color.  This problem, retraining, we will look further. <br><br><h3>  Tree building algorithm </h3><br><br>  One can make sure that the tree constructed in the previous example is in a certain sense optimal - it took only 5 ‚Äúquestions‚Äù (conditions for the x attribute) to ‚Äúfit‚Äù the decision tree to the training set, that is, the tree correctly classified any learning object.  Under other conditions of sampling separation, the tree will turn out deeper. <br><br>  The basis of popular decision-making algorithms, such as ID3 and C4.5, is the principle of greedily maximizing the growth of information - at each step, the feature is chosen, when divided into which information growth is greatest.  Then the procedure is repeated recursively until the entropy is zero or some small value (unless the tree is ideally fitted to the training set in order to avoid retraining).  Different algorithms use different heuristics for ‚Äúearly stopping‚Äù or ‚Äúclipping‚Äù to avoid building a retrained tree. <br><br><h3>  Example </h3><br>  Consider an example of using the decision tree from the Scikit-learn library for synthetic data. <br><br><pre><code class="python hljs">%pylab inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pylab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt plt.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">10.0</span></span>, <span class="hljs-number"><span class="hljs-number">8.0</span></span>)</code> </pre> <br><br>  Generate data.  Two classes will be generated from two normal distributions with different means. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   train_data = np.random.normal(size=(100, 2)) train_labels = np.zeros(100) #    train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)] train_labels = np.r_[train_labels, np.ones(100)]</span></span></code> </pre><br><br>  We will write an auxiliary function that will return the grid for further beautiful visualization. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_grid</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> x_min, x_max = data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>].min() - <span class="hljs-number"><span class="hljs-number">1</span></span>, data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>].max() + <span class="hljs-number"><span class="hljs-number">1</span></span> y_min, y_max = data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>].min() - <span class="hljs-number"><span class="hljs-number">1</span></span>, data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>].max() + <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.meshgrid(np.arange(x_min, x_max, <span class="hljs-number"><span class="hljs-number">0.01</span></span>), np.arange(y_min, y_max, <span class="hljs-number"><span class="hljs-number">0.01</span></span>))</code> </pre><br><br>  Display the data <br><br><pre> <code class="python hljs">plt.scatter(train_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], train_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=train_labels, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>)</code> </pre><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/73f/507/bfb/73f507bfbd044ab48f3cf04b99058084.png" width="60%" height="60%"></div><br><br>  Let's try to separate these two classes, having trained a decision tree.  Visualize the resulting class separation boundary. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTreeClassifier <span class="hljs-comment"><span class="hljs-comment">#  min_samples_leaf ,     #        clf = DecisionTreeClassifier(min_samples_leaf=5) clf.fit(train_data, train_labels) xx, yy = get_grid(train_data) predicted = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) plt.pcolormesh(xx, yy, predicted, cmap='autumn') plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, cmap='autumn')</span></span></code> </pre><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f1c/02e/9b9/f1c02e9b94974779b9c0e94860a1682e.png" width="60%" height="60%"></div><br><br>  We carry out the same procedure, but now we predict the real probabilities of belonging to the first class. <br>  Strictly speaking, these are not probabilities, but normalized numbers of objects of different classes from one leaf of the tree. <br>  For example, if there are 5 positive objects (labeled 1) and 2 negative (labeled 0) in a sheet, <br>  then, instead of predicting label 1 for all objects that fall on this sheet, it will be predicted <br>  the value of 5/7. <br><br><pre> <code class="python hljs">predicted_proba = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, <span class="hljs-number"><span class="hljs-number">1</span></span>].reshape(xx.shape) plt.pcolormesh(xx, yy, predicted_proba, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>) plt.scatter(train_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], train_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=train_labels, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>)</code> </pre><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f5d/ad9/a32/f5dad9a3280b400e8b9ac9f0cb6e3b1f.png" width="60%" height="60%"></div><br><br>  Generate randomly distributed test data. <br><br><pre> <code class="python hljs">test_data = np.random.normal(size=(<span class="hljs-number"><span class="hljs-number">100</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), loc = <span class="hljs-number"><span class="hljs-number">1</span></span>) predicted = clf.predict(test_data)</code> </pre><br><br>  Display them.  Since the labels of test objects are unknown, we draw them in gray. <br><br><pre> <code class="python hljs">plt.scatter(test_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], test_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=<span class="hljs-string"><span class="hljs-string">"gray"</span></span>, s=<span class="hljs-number"><span class="hljs-number">100</span></span>)</code> </pre><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/31b/0ed/f5a/31b0edf5a29843e88796a950bc47aa94.png" width="60%" height="60%"></div><br><br>  Let's see how the decision tree classified test cases.  For contrast, use a different color range. <br><pre> <code class="python hljs">plt.pcolormesh(xx, yy, predicted_proba, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>) plt.scatter(train_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], train_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=train_labels, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>) plt.scatter(test_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], test_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=predicted, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'cool'</span></span>)</code> </pre><br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/686/bfe/165/686bfe1657674ac9b9fbbd87ad0e4968.png" width="60%" height="60%"></div><br><br><h3>  Pros and cons of approach </h3><br><br>  Here we got acquainted with the simplest and most intuitive classification method, the decision tree, and looked at how it is used in the Scikit-learn library in the classification task.  So far we have not discussed how to assess the quality of classification and how to deal with retraining decision trees.  Note the pros and cons of this approach. <br>  Pros: <br><ul><li>  Generation of clear classification rules that are understandable to a person, for example, ‚Äúif age is &lt;25 and interest in motorcycles, refuse credit‚Äù </li><li>  Decision trees can be easily visualized. </li><li>  Relatively fast learning and classification processes </li><li>  Small number of model parameters </li><li>  Support for both numeric and categorical features </li></ul><br>  Minuses: <br><ul><li>  The separating boundary built by the decision tree has its limitations (consists of hypercubes), and in practice the decision tree in terms of the quality of classification is inferior to some other methods </li><li>  The need to cut off the branches of the tree (pruning) or set the minimum number of elements in the leaves of the tree or the maximum depth of the tree to combat retraining.  However, retraining is the problem of all machine learning methods. </li><li>  Instability.  Small changes in data can significantly change the constructed decision tree.  They are struggling with this problem with the help of decision tree ensembles </li><li>  The problem of finding the optimal decision tree is NP-complete; therefore, in practice, heuristics are used such as a greedy search for a feature with maximum information gain, which do not guarantee finding a globally optimal tree </li><li>  Difficult supported gaps in the data.  Friedman estimated that it took 50% of the CART code to support data gaps. </li></ul><br><br><h2>  Part 3. An example of one of the tricks that we teach </h2><br><br>  Let us take a closer look at one of the tricks actually used by many participants in Kaggle competitions, in this case Stanislav Semenov (4th place in the current Kaggle world ranking at the time of writing) in solving the <a href="https://www.kaggle.com/c/wise-2014">problem</a> ‚ÄúGreek Media Monitoring Multilabel Classification (WISE 2014)‚Äù.  The task was to classify articles on topics.  The organizers of the competition shared a training sample of about 65 thousand articles submitted with the help of bag models of words and TF-IDF in the form of vectors of length ~ 300 thousand.  For each document, it was indicated which topics it can be attributed to (from one topic to thirteen different, on average, one or two).  It was necessary to predict which topics include ~ 35 thousand articles from the test sample. <br><br>  We divide the training sample into validation (validation) and left (holdout) parts in a ratio of 80:20.  The test sample will be split again into two parts.  On one we will train the linear machine of support vectors ( <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">LinearSVC</a> in Scikit-learn), with the help of the second we will select the parameter C, so as to maximize the measure of the quality of classification F1 - the target one in this competition.  The holdout sample remains at the very end, the model is checked on it before sending the solution.  This is a common practice that allows you to combat retraining in the process of cross-validation. <br><br>  When iterating through a parameter in the range from 10 ^ -5 to 10 ^ 5, we get the following picture.  The maximum value of the F1-measure is obtained at C = 100 (10 ^ 2) and is equal to ~ 0.6958. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/978/bcd/80a/978bcd80a1db4aaba414da0cbff0824d.png" width="60%" height="60%"></div><br><br>  Let's look through the parameters in a narrower range - from 10 ^ 0 to 10 ^ 4.  We see that F1 does not tend asymptotically to 0.7 with increasing parameter C, as it may seem, but has a local maximum at C = 10 ^ 1.75 <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a8a/935/49c/a8a93549c40649e58329cd6722394318.png" width="60%" height="60%"></div><br><br>  If you send a solution obtained on a test sample using LinearSVC with the parameter C = 10 ^ 1.75, the F1 metric is 0.61, which is quite far from the <a href="https://www.kaggle.com/c/wise-2014/leaderboard">best</a> assumptions. <br><br>  But let's think about how SVM classifies objects in the simplest case of binary classification.  In this case, whether the document is related to a topic or not.  For each object, the distance from it to the constructed separating hyperplane is considered. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a8c/4de/926/a8c4de92636b47d395055847a660d16d.png" width="60%" height="60%"></div><br><br>  If this distance is positive, the object belongs to one class, otherwise - to another.  It turns out that the boundary value of the distance to the separating hyperplane is equal to 0. But who said that this value is optimal for classification?  Indeed, in the more complicated case of linear inseparability, the picture may not be so simple, and the hyperplane that separates the classes will not be absolutely unmistakable.  In this case, the optimal (to maximize the F1 classification measure) threshold for the distance to the hyperplane may differ from 0. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/997/de7/567/997de7567bcd495ca7c6a586017494a1.png" width="60%" height="60%"></div><br><br>  In Scikit-learn, the distance from objects to the border constructed by the LinearSVC algorithm can be obtained using the decision_function method of the LinearSVC class. <br>  Let us see how the quality of classification (F1) will change if we change the threshold of the distance from the object to the separating hyperplane.  For example, in the case of a threshold equal to 0.1, objects that are one side more than 0.1 from the hyperplane will be classified positively, and the rest - negatively. <br>  When the threshold changes from -2 to 2, training LinearSVC on one part of the test sample (80%) and tracking the F1-measure on the other part (20%), we get the following picture. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e85/534/b2c/e85534b2c9b649a1bdc5f9db67ad57e7.png" width="60%" height="60%"></div><br><br>  It turns out that to maximize the F1-measure it is better to use the cut-off threshold for the distance to the hyperplane equal to -0.5. <br>  In this case, the F1-measure on the test sample reaches 0.763, and if you train the algorithm on the entire training sample and use the threshold of -0.5 to classify on the test sample, you can reach F1 = 0.73 for the public sample, which immediately ‚Äúraises‚Äù the top 5% the rating of this Kaggle competition (at the time of writing). <br><br><h2>  Part 4. Some useful GitHub repositories for programming and machine learning </h2><br>  This is far from an exhaustive list of cool GitHub repositories for programming, data analysis and machine learning in Python.  Almost all of them are IPython notebook sets.  Some of the materials in my course described above are a translation of these. <br><br><ul><li>  <a href="https://github.com/ehmatthes/intro_programming">The course of</a> programming in the Python language, the basis of the site <a href="http://introtopython.org/">introtopython.org</a> . </li><li>  "Data science IPython notebooks" - a <a href="https://github.com/donnemartin/data-science-ipython-notebooks">lot of</a> quality notebooks for the main Python libraries for data analysis - NumPy, SciPy, Pandas, Matplotlib, Scikit-learn.  Apache Spark and Kaggle Titanic: Machine Learning from Disaster. </li><li>  Harvard Data Analysis <a href="https://github.com/cs109/content">Course</a> </li><li>  "Interactive coding challenges" - a <a href="https://github.com/donnemartin/interactive-coding-challenges">selection of</a> basic tasks on data structures, graphs, sorting, recursion and more.  For many tasks, solutions and explanatory material with pictures are given. </li><li>  Olivier Griselle <a href="https://github.com/ogrisel/notebooks">repository</a> (one of the authors of the Scikit-learn library) with IPython training notebooks <a href="https://github.com/ogrisel/parallel_ml_tutorial">.</a>  <a href="https://github.com/ogrisel/parallel_ml_tutorial">One more</a> . </li><li>  <a href="https://github.com/jakevdp/sklearn_scipy2013">Scikit</a> -learn tutorial, also from authors </li><li>  <a href="https://github.com/subokita/mlclass">Analysis of the</a> tasks of the course Andrew Ng "Machine learning" in Python </li><li>  <a href="https://github.com/ptwobrussell/Mining-the-Social-Web-2nd-Edition">Materials</a> in addition to the book "Mining the Social Web (2nd Edition)" (Matthew A. Russell, O'Reilly Media) </li><li>  <a href="https://github.com/MLWave/Kaggle-Ensemble-Guide">Tutorial</a> on the use of ensembles to solve problems Kaggle. </li><li>  <a href="https://github.com/dmlc/xgboost">The</a> XGBoost <a href="https://github.com/dmlc/xgboost">library</a> , which is used by most Kaggle competition winners.  You can also get acquainted with their success stories.  XGBoost is good in prediction quality, effectively implemented, well paralleled. </li><li>  Data Collection FiveThirtyEight.  Just a bunch of interesting datasets. </li><li>  <a href="https://github.com/jseabold/538model">Forecasting</a> US election results.  A good example of analyzing data with Pandas </li></ul><br><br>  I hope these materials will help you in learning / teaching Python and analyzing data. <br>  The list of cool repositories with IPython notebooks (and simply with Python code), of course, can be continued.  For example, in the comments. </div><p>Source: <a href="https://habr.com/ru/post/270449/">https://habr.com/ru/post/270449/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270439/index.html">Linux containers: when containers gets bigger</a></li>
<li><a href="../270441/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ184 (November 2 - 8, 2015)</a></li>
<li><a href="../270443/index.html">Processing of private data on public computer networks</a></li>
<li><a href="../270445/index.html">Design and evolution of the C ++ language: excerpts</a></li>
<li><a href="../270447/index.html">Design development for MyOffice applications</a></li>
<li><a href="../270451/index.html">7 enhanced features of Visual Studio 2015 Enterprise</a></li>
<li><a href="../270453/index.html">Big data from A to Z. Part 3: Techniques and strategies for developing MapReduce-applications</a></li>
<li><a href="../270457/index.html">Apple TV</a></li>
<li><a href="../270459/index.html">Google Cloud Endpoints in Java: A Guide. Part 2 (Frontend)</a></li>
<li><a href="../270461/index.html">Web two-null shortcuts for Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>