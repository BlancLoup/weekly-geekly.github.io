<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A new round of CUDA architecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 
 In early April, I saw the announcement of a new video card from nVidia, with a new major index compute capability - 3.0. Having carefully stu...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A new round of CUDA architecture</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br>  In early April, I saw the <a href="http://www.geforce.com/Active/en_US/en_US/pdf/GeForce-GTX-680-Whitepaper-FINAL.pdf">announcement of a</a> new video card from nVidia, with a new major index compute capability - 3.0.  Having carefully studied the specs, I was surprised - it turned out all over that now the branching will lead to the worst consequences: big losses in productivity.  I liked that from version to version branching play an ever smaller role, and Kepler seemed to be a step backwards in this plan.  I understood with my brain that this is hardly possible and decided to wait a bit. <br>  And now this week I got a <a href="http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf">whitepaper</a> on a new number crusher on Kepler architecture and clarified a lot. <br><a name="habracut"></a><br><h2>  Beginnings </h2><br>  Initially, this note was conceived only for those who are in the subject, but just in case, I will explain what compute capability in CUDA is. <br>  So, nVidia has been developing video cards for general computing for the past 5 years.  Not to say that before this could not be considered on the GPU, but to do so was very uncomfortable for a normal person.  In 2007, the Universal Architecture for Computing Devices (CUDA) based on video cards was proposed.  This allowed extensively increasing the power of devices, while maintaining the main features of the architecture.  That is, the number of processors and the amount of memory is constantly growing, but the division of memory into shared / global / texture and registers have been preserved since those ancient times (with SS 2.0 there were also surface and a couple of caches, but this is another song). <br>  However, nothing stands still and the architecture and instruction set change over time - sometimes significantly, sometimes not very much.  For displaying families of GPUs with identical architecture, versions of Compute Capability (CC) were identified.  For example: devices with CC 1.0 were not able to do atomic operations in general, with CC 1.1 - they were able in global memory and with CC 1.2 - both in global and in shared memory.  A complete list of the capabilities of different CCs is traditionally given at the end of the CUDA C Programming Guide. <br><br><h2>  What will Kepler bring us? </h2><br>  Firstly, a new huge multiprocessor.  If earlier multiprocessors had 8 (CC 1.x), 32 (CC 2.0) or 48 (CC 2.1) stream processors, then Kepler uses a new chip for 192 processors.  The remaining characteristics are also impressive: <br><table><tbody><tr><th></th><th>  FERMI GF100 </th><th>  FERMI GF104 </th><th>  KEPLER GK104 </th><th>  KEPLER GK110 </th></tr><tr><th>  CC version </th><td>  2.0 </td><td>  2.1 </td><td>  3.0 </td><td>  3.5 </td></tr><tr><th>  Warp threads </th><td>  32 </td><td>  32 </td><td>  32 </td><td>  32 </td></tr><tr><th>  Number of warp per multiprocessor </th><td>  48 </td><td>  48 </td><td>  64 </td><td>  64 </td></tr><tr><th>  Threads on multiprocessor </th><td>  1536 </td><td>  1536 </td><td>  2048 </td><td>  2048 </td></tr><tr><th>  Blocks on multiprocessor </th><td>  eight </td><td>  eight </td><td>  sixteen </td><td>  sixteen </td></tr><tr><th>  32-bit registers per multiprocessor </th><td>  32768 </td><td>  32768 </td><td>  65536 </td><td>  65536 </td></tr><tr><th>  Maximum number of registers / stream </th><td>  63 </td><td>  63 </td><td>  63 </td><td>  255 </td></tr><tr><th>  Shared memory configurations </th><td>  16K <br>  48K </td><td>  16K <br>  48K </td><td>  16K <br>  32K <br>  48K </td><td>  16K <br>  32K <br>  48K </td></tr><tr><th>  X maximum grid size </th><td>  2 ^ 16-1 </td><td>  2 ^ 16-1 </td><td>  2 ^ 16-1 </td><td>  2 ^ 32-1 </td></tr><tr><th>  Hyper-Q </th><td>  Not </td><td>  Not </td><td>  Not </td><td>  there is </td></tr><tr><th>  Dynamic concurrency </th><td>  Not </td><td>  Not </td><td>  Not </td><td>  there is </td></tr></tbody></table><br>  What conclusions can be drawn from this plate?  The size of the warp remained the same, and this is good.  The main thing I was afraid of was an increase in the size of the warp, which would lead to unnecessary downtime during branching.  But not everything is so rosy.  Now you need to be even more careful with registers - in order to load SMX as much as possible (by the number of simultaneously loaded blocks / streams) you need to use even less ... On the other hand, the architecture allows you to store up to 255 registers per stream, and if the postulate used to work than to store, ‚Äùnow is not so simple.  Thus, we have another fork in the optimization path - you can store the results of calculations in the stream, but then SMX will fit less blocks and it is quite possible that there will be read / write downtime in RAM. <br>  You can still casually note that the calculations of double precision and specials.  the functions (sin, cos, etc.) will now be performed faster and this pleases. <br>  The second notable innovation is the new <b>Warp Scheduler</b> .  For each multiprocessor, there are 4 processors, and each of them is able to execute 2 warp instructions per clock.  In short, an interesting thing.  Make sure that branching inside maximum of 2 paths inside the warp) <br>  Further incomprehensible things: <br>  <b>Dynamic Parallelism</b> - I didn‚Äôt understand how it will work, but it looks like the streams will be able to launch new grids ... Let's see how it will be.  It seems that soon there will be only I / O on the host, and everything else can be run on the GPU, since there is an opportunity to manage the kernels. <br>  <b>GPUDirect</b> - now you can take and transfer data from one GPU to another directly, without a host.  And even over the network.  Seeing is believing.  It sounds too cool. <br>  All sorts of pleasant things: <br><ul><li>  <b>Shuffle Instruction</b> - a new way to exchange data between threads in a block.  If we are too lazy to allocate a separate shared mem and control access to it, then we take a local variable and juggle it from stream to stream. </li><li>  <b>Atomic operations with double precision</b> .  Well, added a couple.  The main thing - they promise acceleration up to 9 times! </li><li>  <b>New read only data cache.</b>  L1 and L2 data caches appeared in Fermi, and in Kepler you can mark data as read only ( <pre><code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> __restrict</code> </pre>  ) so that the compiler stuffs them into this new cache.  And before that I stuffed the data into the textures.  Eh. </li><li>  <b>Improved L2 cache.</b>  First, it became 2 times more, and secondly, access to it is 2 times faster. </li><li>  <b>Hyper-Q, Grid Management Unit</b> - Fermi has the opportunity to run multiple tasks simultaneously.  But these 2 technologies allow you to do it correctly, but they work at the hardware level and we will notice this only from a more complete load of the GPU. </li></ul><br>  And finally, very cool - nVidia sold the souls of a pair of engineers to the gods of energy consumption and now the GTX 680 promises to eat less than a similar Fermi based on W / FLOPS.  Well, because cool, eh?  Straight even throw out the old 280 Ovens.  And then they are heated to 130 degrees and in reserve they had to allocate 1.5 kW. <br><br><h2>  Conclusion </h2><br>  In short, I do not know about you, but I‚Äôm eagerly looking forward to waiting for the new Kepler!  But I don‚Äôt know if it will be possible to get to two computers with GPU CC 3.5 - so that GPUDirect can feel.  If anyone has it - let him touch it, okay? </div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/144202/">https://habr.com/ru/post/144202/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../144197/index.html">Indexing AJAX sites by the Yandex search robot</a></li>
<li><a href="../144198/index.html">How I walked behind the backtrace</a></li>
<li><a href="../144199/index.html">Today, YouTube is seven years old. And you set the record again</a></li>
<li><a href="../144200/index.html">Finger huffman algorithm</a></li>
<li><a href="../144201/index.html">System of automatic reports on the work of GPS equipment</a></li>
<li><a href="../144203/index.html">Total Commander for Android on the Play Store</a></li>
<li><a href="../144205/index.html">Catalog of Microsoft Training Seminars MUK Training Center</a></li>
<li><a href="../144207/index.html">The source code of free Mac OS X 10.7.4 modules has been published.</a></li>
<li><a href="../144208/index.html">Greensock - now also for javascript</a></li>
<li><a href="../144209/index.html">Web Design Manifesto 2012</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>