<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What video codecs (do not) use browsers for video calls</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A typical Voximplant technical support request: ‚ÄúWhy does a video call between two Chrome look better than a video call between MS Edge and a native i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What video codecs (do not) use browsers for video calls</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/bp/z1/1m/bpz11m_orhftn04lqz-4xbh6pzm.png"></div><br>  A typical Voximplant technical support request: ‚ÄúWhy does a video call between two Chrome look better than a video call between MS Edge and a native iOS application?‚Äù  Colleagues usually respond neutral - ‚Äúbecause codecs‚Äù.  But we, IT professionals, are curious.  Even if I do not develop a new Skype-for-web, reading ‚Äúwhat browser can do that‚Äù and how they break one video into several streams of different quality enriches the picture of the world and gives a fresh topic for discussion in the smoking room.  Successfully turned up article from widely known in narrow circles of <a href="http://webrtcbydralex.com/">Dr Alex</a> (with the best explanation of the term "media engine" from all that I saw), a little of our experience, a couple of evenings in "Dial" - and the translation adapted for Habr waits under the cut! <br><a name="habracut"></a><br><h2>  Codecs and channel width </h2><br>  When talking about video codecs, the balance of quality and width of the channel used is most often discussed.  And they like to ignore the issues of CPU load and how to technically transfer video.  It is quite reasonable if we discuss the encoding of an already recorded video. <br><br>  After all, if you have a finished video, then there is not much difference, it will be compressed for a couple of minutes, a couple of hours or even a couple of days.  Any costs of the processor and memory will be justified, because this is a one-time investment, and then the video can be distributed to millions of users.  The best video codecs compress video in several passes: <br><br><ol><li>  Passage # 1: The video is divided into parts with common features: the action takes place on the same background, a fast or slow scene, and the like. </li><li>  Pass # 2: Collecting statistics for coding and information on how frames change over time (you need several frames to get this information). </li><li>  Passage 3: Each part is encoded with its own codec settings and using the information obtained in the second step. </li></ol><br>  Streaming is a different matter.  No one will wait for the end of the podcast, stream or show before starting to encode the video.  Encode and send right away.  Live on the fact that it is direct, that the minimum delay becomes the most important thing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When using physical media, DVD or Blu-Ray discs, the video size is fixed and the codec has the task to ensure maximum quality for a given size.  If the video is distributed over the network, the task of the codec is to prepare such file (s) in order to get the maximum quality at a fixed channel width or the minimum channel width at a fixed quality, if you need to reduce the price.  Network latency can be ignored and buffered on the client side for as many seconds of video as needed.  But for streaming, neither size nor quality is fixed, there is no special need, the codec has another task: to reduce delays at any cost. <br><br>  Finally, the creators of codecs for a long time kept in mind only one usage scenario: one and only one video is played on the user's computer.  Which, moreover, is almost always possible to decode by video chip.  Then came the mobile platform.  And then WebRTC, to ensure minimal delays which developers really wanted to use the Selective Forwarding Unit servers. <br><br>  Using codecs for video calls is so different from traditional use when playing video that comparing codecs head-on becomes meaningless.  When, at the dawn of WebRTC, VP8 and H.264 were compared, one of the hottest debates was about codec settings: making them ‚Äúrealistic‚Äù considering unreliable networks or ‚Äúideal‚Äù for maximum video quality.  Fighters for the ‚Äúpurity of codec comparisons‚Äù quite seriously argued that codecs should be compared without taking into account packet loss, jitter and other network problems. <br><br><h2>  What is now with codecs? </h2><br><ul><li>  H.264 and VP8 are approximately the same in terms of video quality and channel width used; </li><li>  H.265 and VP9 also roughly correspond to each other, showing an average of 30% better results than the previous generation of codecs due to an increase of 20% in CPU usage; </li><li>  The new AV1 codec, the explosive mixture of VP10, daala and thor, is about the same as the codec of the previous generation, about as much as those of their predecessors. </li></ul><br>  And now the surprise: no one cares about these differences when it comes to video calls and video conferencing.  The most important thing is how the codec plays in a team with the rest of the infrastructure.  Developers are concerned with what is called the new <b>media engine</b> term: how the browser or mobile application captures video, encodes / decodes it, breaks it up into RTP packets and fights network problems (remember the video from our <a href="https://habr.com/company/Voximplant/blog/419423/">previous habrastaty</a> ? So, they compared media engine - translator's note).  If the encoder can not work with a sharp decrease in the channel width or stably support 20 frames per second, if the decoder can not work with the loss of a single network packet, then what difference does it make how well the codec compresses the video?  It‚Äôs easy to see why Google is sponsoring <a href="https://habr.com/company/Voximplant/blog/419423/">research from the Stanford team</a> for better interaction between the codec and the network.  This is the future of video communications. <br><br><h2>  Codecs and media engine: everything is difficult </h2><br>  Video calls and video conferencing have <b>almost</b> the same tasks as regular media.  But the priorities are <b>completely</b> different: <br><br><ol><li>  It takes 30 frames per second (codec speed). </li><li>  It takes 30 frames per second with interactivity (minimal delays). </li></ol><br>  We also have internet between the participants, the quality of which we can only guess.  It's usually worse.  Therefore: <br><br><ol><li>  It is good to experience small changes in the width of the channel when another visitor comes to coworking. </li><li>  It is necessary to at least somehow experience strong changes in the width of the channel when this visitor starts downloading torrents. </li><li>  It is necessary to experience jitter well (random delays between received packets, due to which they can not just linger, but come not in the order in which they were sent). </li><li>  Need to experience packet loss. </li></ol><br><h2>  3.1.  Main tasks of media engine </h2><br>  What does "need 30 frames per second" mean?  This means that the media engine has 33 milliseconds to capture video from the camera, sound from a microphone, compress it with a codec, split it into RTP packets, protect the transmitted data (SRTP = RTP + AES) and send over the network (UDP or TCP , in most cases, UDP).  All this on the sending side.  And on the receiving side - repeat in the reverse order.  Since encoding is usually more complicated than decoding, the sending side is harder. <br><br>  On the technical side, the goal ‚Äúyou need 30 frames per second‚Äù is achievable with delays.  And the longer the delay, the easier it is to achieve the goal: if the sending side encodes not one frame at a time, but several at once, then you can significantly save on the channel width (codecs better compress several frames by analyzing the changes between all of them, not just between current and previous).  At the same time, the delay between receiving a video stream from a camera and sending it over the network increases in proportion to the number of buffered frames, plus compression becomes slower due to additional calculations.  Many sites use this trick, declaring the response time between sending and receiving network packets between video call participants.  The delay in coding and decoding, they are silent. <br><br>  In order to make video calls look like personal communication, the creators of communication services discard all settings and codec profiles that may cause delays.  It turns out such a degradation of modern codecs to frame-by-frame compression.  At first, this situation caused rejection and criticism from codec developers.  But times have changed, and now modern codecs in addition to the traditional presets ‚Äúminimum size‚Äù and ‚Äúmaximum quality‚Äù have added a set of real-time settings.  But at the same time, there is also a ‚Äúscreen sharing‚Äù, also for video calls (there is a specificity there - a large resolution, a little changing picture, the need for lossless compression, otherwise the text will float). <br><br><h2>  3.2.  Media engine and public networks </h2><br><blockquote>  Small channel width changes </blockquote><br>  Previously, codecs could not change the bitrate: when starting compression, they took the target bitrate as a setting and then gave out a fixed number of megabytes of video per minute.  In those old times, video calls and video conferencing were the lot of local networks and reserved bandwidth.  And in case of problems, they called the administrator who repaired the channel width reservation on the tsiska. <br><br>  The first evolutionary change was the ‚Äúadaptive bit rate‚Äù technology.  The codec has many settings that affect the bitrate: video resolution, a slight decrease in fps from 30 to 25 frames per second, quantization of the video signal.  The last item on this list is the ‚Äúcoarsening‚Äù of the transition between colors, the minor changes of which are hardly noticeable to the human eye.  Most often the main ‚Äúsetting‚Äù for adaptive bitrate was precisely quantization.  And the media engine told the codec about the channel width. <br><br><blockquote>  Large channel width changes </blockquote><br>  The adaptive bitrate mechanism helps the media engine to continue streaming video with minor changes in channel width.  But if your colleague started downloading torrents and the available channel sank two or three times, then the adaptive bitrate will not help.  It will help reduce the resolution and frame rate.  The latter is preferable, since our eyes are less sensitive to the number of frames per second than to the video resolution.  Typically, the codec starts to skip one or two frames, reducing the frame rate from 30 to 15 or even to 10. <br><br>  An important detail: the media engine will skip frames on the sending side.  If we have video conferencing for several participants or broadcasting, and the network problem is not with the sender, then one ‚Äúweak link‚Äù will worsen the video quality for all participants.  In this situation, the simulcast bundle helps (the sending side sends several video streams of different quality at once) and the SFU (Selective Forwarding Unit, the server gives each participant of a video conference or broadcast a stream of the required quality).  Some codecs have the ability to create multiple simulcast streams, SVCs that complement each other: customers with the weakest channel receive a minimum quality stream, customers with a better channel receive the same stream plus the first ‚Äúupgrade‚Äù, customers with an even better channel are given already two streams of "upgrade" and so on.  This method allows you to not transmit the same data in multiple streams and saves about 20% of traffic compared to encoding several full-fledged video streams.  It also simplifies the work of the server - no need to switch threads, it‚Äôs enough not to transfer packets with an ‚Äúupgrade‚Äù to clients.  However, any codec can be used for simulcast, this is a feature of the media engine and the organization of RTP packets, and not a codec. <br><br><blockquote>  Jitter and packet loss </blockquote><br>  Loss is the hardest to fight.  Jitter is a bit simpler - it is enough to make a buffer on the host side in which to collect late and confused packets.  Not too big buffer, otherwise you can break realtime and become buffering YouTube video. <br><br>  Packet loss is usually fought by re-forwarding (RTX).  If the sender has a good connection with the SFU, then the server can request the lost packet, retrieve it again and still be within 33 milliseconds.  If the network connection is unreliable (more than 0.01% packet loss), then complex lossy algorithms, such as <a href="https://en.wikipedia.org/wiki/Forward_error_correction">FEC</a> , are needed. <br><br>  The best solution at the moment is to use SVC codecs.  In this case, to get at least some video, only ‚Äúsupport‚Äù packets with a stream of minimum quality are needed, these packets are smaller, hence it is easier to send them again, this is enough for ‚Äúsurvival‚Äù even with a very bad network (more than 1% packet loss).  If Simulcast + SFU allows you to deal with channel width subsidence, then Simulcast using the SVC codec + SFU solves both channel width issues and packet loss issues. <br><br><h2>  What browsers support now </h2><br>  Firefox and Safari use Google‚Äôs Media Engine and update libwebrtc from time to time.  They do it much less frequently than Chrome, a new version of which is released every 6 weeks.  From time to time they begin to lag far behind, but then synchronize again.  With the exception of support for the VP8 codec in Safari.  Don't even ask. <br><br>  Kata table with a full comparison of who supports what, but in general, everything is quite simple.  Edge all usually ignore.  The choice is between the support of the mobile version of Safari and good video quality.  iOS Safari only supports H.264 video codec, while libwebrtc allows you to use simulcast only with VP8 codecs (different streams with different frame rates) and VP9 (SVC support).  But you can read and use libwebrtc on iOS by creating a native application.  Then, with simulcast, everything will be fine and users will get the highest possible video quality with an unstable Internet connection.  A few examples: <br><br><ul><li>  <b>Highfive</b> is a desktop application on Electron (Chromium) with H.264 simulcast (libwebrtc) and Dolby audio codecs; </li><li>  <b>Attlasian</b> - An interesting client solution for React Native and libwebrtc for simulcast; </li><li>  <b>Symphony</b> - Electron for the desktop, React Native for the mobile device, and there and there supported simulcast + additional security features that are compatible with what banks want; </li><li>  <b>Tokbox</b> - VP8 with simulcast in the mobile SDK, using the patched version of libvpx in libwebrtc. </li></ul><br><h2>  Future </h2><br>  It is already clear that VP8 and VP9 will not be in Safari (unlike Edge, which VP8 supports). <br><br>  Although Apple supported the inclusion of H.265 in WebRTC, the latest news and a number of indirect signs point to AV1 as the ‚Äúnext big thing‚Äù.  Unlike the rest of the article, this is my personal opinion.  The AV1 data transfer is ready, but work is underway on the codec.  Now the reference implementation of the encoder shows a sad 0.3 frames per second.  This is not a problem when playing pre-compressed content, but so far not applicable to Realtime Communications.  In the meantime, you can <a href="https://hacks.mozilla.org/2017/11/dash-playback-of-av1-video/">try</a> playing AV1 video in Firefox, although this has nothing to do with RTC.  The implementation from the bitmovin team, which developed MPEG-DASH and received 30 million investments to create the next-generation video infrastructure. </div><p>Source: <a href="https://habr.com/ru/post/419949/">https://habr.com/ru/post/419949/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../419939/index.html">How I did navigation in React Native is not so terrible</a></li>
<li><a href="../419941/index.html">Photo tour of the office "Audiomania": Part One</a></li>
<li><a href="../419943/index.html">What we read in July: how to find time to read, five books for a timlid and some fresh articles</a></li>
<li><a href="../419945/index.html">How to prepare for an interview in Google and not pass it. Twice</a></li>
<li><a href="../419947/index.html">Connection to PiZeroW with Raspbian Stretch Lite, without additional adapters and monitor</a></li>
<li><a href="../419951/index.html">Experience using WebRTC. Yandex lecture</a></li>
<li><a href="../419953/index.html">I am writing a book about the first ‚Äúour‚Äù startup that has conquered the world: help</a></li>
<li><a href="../419955/index.html">UART FIFO Buffer Features in ESP32</a></li>
<li><a href="../419961/index.html">The digest of interesting materials for the mobile developer # 265 (August 6 ‚Äî August 12)</a></li>
<li><a href="../419963/index.html">Making a smart controller for an air conditioner on an ESP8266</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>