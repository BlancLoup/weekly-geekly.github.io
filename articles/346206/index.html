<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Clustering and visualization of textual information</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the Russian-speaking sector of the Internet there are very few educational practical examples (and with an example of a code even less) analysis of...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Clustering and visualization of textual information</h1><div class="post__text post__text-html js-mediator-article">  In the Russian-speaking sector of the Internet there are very few educational practical examples (and with an example of a code even less) analysis of text messages in Russian.  Therefore, I decided to collect the data together and consider an example of clustering, since data preparation for training is not required. <br><a name="habracut"></a><br>  Most of the libraries used are already in the <a href="https://www.anaconda.com/download/">Anaconda 3</a> distribution, so I advise you to use it.  Missing modules / libraries can be installed as standard via pip install ‚Äúpackage name‚Äù. <br>  We connect the following libraries: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nltk <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> codecs <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> feature_extraction <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mpld3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mpl</code> </pre> <br>  For analysis, you can take any data.  This task came to my eyes then: <a href="https://hubofdata.ru/es_AR/dataset/clearspending-visitors-2014-2015">Statistics of search requests of the project State expenses</a> .  They needed to break the data into three groups: private, public, and commercial organizations.  I did not want to invent an extraordinary thing, so I decided to check how clustering would lead in this case (looking ahead - not very much).  But you can download the data from VK of any public: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> vk <span class="hljs-comment"><span class="hljs-comment"># id  session = vk.Session(access_token='') # URL   access_token,  tvoi_id  id   : # https://oauth.vk.com/authorize?client_id=tvoi_id&amp;scope=friends,pages,groups,offline&amp;redirect_uri=https://oauth.vk.com/blank.html&amp;display=page&amp;v=5.21&amp;response_type=token api = vk.API(session) poss=[] id_pab=-59229916 #id    , id     info=api.wall.get(owner_id=id_pab, offset=0, count=1) kolvo = (info[0]//100)+1 shag=100 sdvig=0 h=0 import time while h&lt;kolvo: if(h&gt;70): print(h) #  ,       pubpost=api.wall.get(owner_id=id_pab, offset=sdvig, count=100) i=1 while i &lt; len(pubpost): b=pubpost[i]['text'] poss.append(b) i=i+1 h=h+1 sdvig=sdvig+shag time.sleep(1) len(poss) import io with io.open("public.txt", 'w', encoding='utf-8', errors='ignore') as file: for line in poss: file.write("%s\n" % line) file.close() titles = open('public.txt', encoding='utf-8', errors='ignore').read().split('\n') print(str(len(titles)) + '  ') import re posti=[] #      for line in titles: chis = re.sub(r'(\&lt;(/?[^&gt;]+)&gt;)', ' ', line) #chis = re.sub() chis = re.sub('[^-- ]', '', chis) posti.append(chis)</span></span></code> </pre><br>  I will use search query data to show how short text data clusters poorly.  I cleared the special characters and punctuation marks from the text plus the replacement of abbreviations (for example, the individual entrepreneur is an individual entrepreneur).  It turned out the text, where in each line there was one search query. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We read the data into an array and proceed to normalization - bringing the word to the initial form.  This can be done in several ways using Porter Stemmer, MyStem Stemmer and PyMorphy2.  I want to warn - MyStem works through the wrapper, so the speed of operations is very slow.  Let us dwell on Porter's template, although no one bothers to use others and combine them with each other (for example, go through PyMorphy2, and after Porter's template). <br><br><pre> <code class="python hljs">titles = open(<span class="hljs-string"><span class="hljs-string">'material4.csv'</span></span>, <span class="hljs-string"><span class="hljs-string">'r'</span></span>, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>, errors=<span class="hljs-string"><span class="hljs-string">'ignore'</span></span>).read().split(<span class="hljs-string"><span class="hljs-string">'\n'</span></span>) print(str(len(titles)) + <span class="hljs-string"><span class="hljs-string">'  '</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> nltk.stem.snowball <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SnowballStemmer stemmer = SnowballStemmer(<span class="hljs-string"><span class="hljs-string">"russian"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">token_and_stem</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> tokens = [word <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sent <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(text) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.word_tokenize(sent)] filtered_tokens = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokens: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'[--]'</span></span>, token): filtered_tokens.append(token) stems = [stemmer.stem(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> filtered_tokens] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> stems <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">token_only</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function">:</span></span> tokens = [word.lower() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> sent <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.sent_tokenize(text) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.word_tokenize(sent)] filtered_tokens = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> token <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tokens: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> re.search(<span class="hljs-string"><span class="hljs-string">'[--]'</span></span>, token): filtered_tokens.append(token) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> filtered_tokens <span class="hljs-comment"><span class="hljs-comment">#  ()    totalvocab_stem = [] totalvocab_token = [] for i in titles: allwords_stemmed = token_and_stem(i) #print(allwords_stemmed) totalvocab_stem.extend(allwords_stemmed) allwords_tokenized = token_only(i) totalvocab_token.extend(allwords_tokenized)</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Pymorphy2</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pymorphy2 morph = pymorphy2.MorphAnalyzer() G=[] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> titles: h=i.split(<span class="hljs-string"><span class="hljs-string">' '</span></span>) <span class="hljs-comment"><span class="hljs-comment">#print(h) s='' for k in h: #print(k) p = morph.parse(k)[0].normal_form #print(p) s+=' ' s += p #print(s) #G.append(p) #print(s) G.append(s) pymof = open('pymof_pod.txt', 'w', encoding='utf-8', errors='ignore') pymofcsv = open('pymofcsv_pod.csv', 'w', encoding='utf-8', errors='ignore') for item in G: pymof.write("%s\n" % item) pymofcsv.write("%s\n" % item) pymof.close() pymofcsv.close()</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">pymystem3</b> <div class="spoiler_text">  Executable analyzer files for the current operating system will be automatically downloaded and installed when you first use the library. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pymystem3 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Mystem m = Mystem() A = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> titles: <span class="hljs-comment"><span class="hljs-comment">#print(i) lemmas = m.lemmatize(i) A.append(lemmas) #       "" import pickle with open("mystem.pkl", 'wb') as handle: pickle.dump(A, handle)</span></span></code> </pre><br></div></div><br>  Create a weight matrix TF-IDF.  We will take each search query for a document (as they do when analyzing posts on Twitter, where each tweet is a document).  We will take tfidf_vectorizer from the sklearn package, and we will take the stop words from the ntlk package (we‚Äôll initially need to download via nltk.download ()).  Parameters can be adjusted as you see fit - from the upper and lower bounds to the number of n-gram (in this case, take 3). <br><br><pre> <code class="python hljs">stopwords = nltk.corpus.stopwords.words(<span class="hljs-string"><span class="hljs-string">'russian'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   - stopwords.extend(['', '', '', '', '', '', '', '', '']) from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer n_featur=200000 tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=10000, min_df=0.01, stop_words=stopwords, use_idf=True, tokenizer=token_and_stem, ngram_range=(1,3)) get_ipython().magic('time tfidf_matrix = tfidf_vectorizer.fit_transform(titles)') print(tfidf_matrix.shape)</span></span></code> </pre><br>  Over the resulting matrix, we begin to apply various clustering methods: <br><br><pre> <code class="python hljs">num_clusters = <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-comment"><span class="hljs-comment">#  - - KMeans from sklearn.cluster import KMeans km = KMeans(n_clusters=num_clusters) get_ipython().magic('time km.fit(tfidf_matrix)') idx = km.fit(tfidf_matrix) clusters = km.labels_.tolist() print(clusters) print (km.labels_) # MiniBatchKMeans from sklearn.cluster import MiniBatchKMeans mbk = MiniBatchKMeans(init='random', n_clusters=num_clusters) #(init='k-means++', 'random' or an ndarray) mbk.fit_transform(tfidf_matrix) %time mbk.fit(tfidf_matrix) miniclusters = mbk.labels_.tolist() print (mbk.labels_) # DBSCAN from sklearn.cluster import DBSCAN get_ipython().magic('time db = DBSCAN(eps=0.3, min_samples=10).fit(tfidf_matrix)') labels = db.labels_ labels.shape print(labels) #   from sklearn.cluster import AgglomerativeClustering agglo1 = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean') #affinity        : cosine, l1, l2, manhattan get_ipython().magic('time answer = agglo1.fit_predict(tfidf_matrix.toarray())') answer.shape</span></span></code> </pre><br>  The data obtained can be grouped in the dataframe and count the number of queries that fell into each cluster. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#k-means clusterkm = km.labels_.tolist() #minikmeans clustermbk = mbk.labels_.tolist() #dbscan clusters3 = labels #agglo #clusters4 = answer.tolist() frame = pd.DataFrame(titles, index = [clusterkm]) #k-means out = { 'title': titles, 'cluster': clusterkm } frame1 = pd.DataFrame(out, index = [clusterkm], columns = ['title', 'cluster']) #mini out = { 'title': titles, 'cluster': clustermbk } frame_minik = pd.DataFrame(out, index = [clustermbk], columns = ['title', 'cluster']) frame1['cluster'].value_counts() frame_minik['cluster'].value_counts()</span></span></code> </pre><br>  Due to the large number of queries, it is not quite convenient to look at the tables and I would like more interactivity to understand.  Therefore, we will make graphs of the relative positions of requests relative to each other <br><br>  First you need to calculate the distance between the vectors.  The cosine distance will be used for this.  The articles propose to use subtraction from one so that there are no negative values ‚Äã‚Äãand are in the range from 0 to 1, so we will do the same: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics.pairwise <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cosine_similarity dist = <span class="hljs-number"><span class="hljs-number">1</span></span> - cosine_similarity(tfidf_matrix) dist.shape</code> </pre><br>  Since the graphics will be two-, three-dimensional, and the original distance matrix is ‚Äã‚Äãn-dimensional, you will have to apply algorithms for reducing the dimension.  There are many algorithms to choose from (MDS, PCA, t-SNE), but we‚Äôll stop the choice on Incremental PCA.  This choice was made as a result of practical application - I tried MDS and PCA, but I did not have enough RAM (8 gigabytes) and when I began to use the swap file, I could immediately take my computer to restart. <br><br>  The Incremental PCA algorithm is used as a replacement for the principal component method (PCA) when the data set to be decomposed is too large to fit in RAM.  IPCA creates a low-level approximation for the input data using memory that does not depend on the number of input data samples. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#    - PCA from sklearn.decomposition import IncrementalPCA icpa = IncrementalPCA(n_components=2, batch_size=16) get_ipython().magic('time icpa.fit(dist) #demo =') get_ipython().magic('time demo2 = icpa.transform(dist)') xs, ys = demo2[:, 0], demo2[:, 1] # PCA 3D from sklearn.decomposition import IncrementalPCA icpa = IncrementalPCA(n_components=3, batch_size=16) get_ipython().magic('time icpa.fit(dist) #demo =') get_ipython().magic('time ddd = icpa.transform(dist)') xs, ys, zs = ddd[:, 0], ddd[:, 1], ddd[:, 2] #   ,     #from mpl_toolkits.mplot3d import Axes3D #fig = plt.figure() #ax = fig.add_subplot(111, projection='3d') #ax.scatter(xs, ys, zs) #ax.set_xlabel('X') #ax.set_ylabel('Y') #ax.set_zlabel('Z') #plt.show()</span></span></code> </pre><br>  Let's proceed directly to the visualization itself: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> rc <span class="hljs-comment"><span class="hljs-comment">#     font = {'family' : 'Verdana'}#, 'weigth': 'normal'} rc('font', **font) #     import random def generate_colors(n): color_list = [] for c in range(0,n): r = lambda: random.randint(0,255) color_list.append( '#%02X%02X%02X' % (r(),r(),r()) ) return color_list #  cluster_colors = {0: '#ff0000', 1: '#ff0066', 2: '#ff0099', 3: '#ff00cc', 4: '#ff00ff',} #  ,  -     01234 cluster_names = {0: '0', 1: '1', 2: '2', 3: '3', 4: '4',} #matplotlib inline # data frame,    ( PCA) +      df = pd.DataFrame(dict(x=xs, y=ys, label=clusterkm, title=titles)) #   groups = df.groupby('label') fig, ax = plt.subplots(figsize=(72, 36)) #figsize     for name, group in groups: ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, label=cluster_names[name], color=cluster_colors[name], mec='none') ax.set_aspect('auto') ax.tick_params( axis= 'x', which='both', bottom='off', top='off', labelbottom='off') ax.tick_params( axis= 'y', which='both', left='off', top='off', labelleft='off') ax.legend(numpoints=1) #   1  # /  ,     #for i in range(len(df)): # ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=6) #  plt.show() plt.close()</span></span></code> </pre><br>  If you uncomment the line with the addition of titles, it will look something like this: <br><br><div class="spoiler">  <b class="spoiler_title">10 Cluster Example</b> <div class="spoiler_text"><img src="https://habrastorage.org/webt/kb/hp/zg/kbhpzgvaybz9ygaih6atqkajwdc.png" alt="image"><br></div></div><br>  Not exactly what one would expect.  We use mpld3 to convert the image to an interactive graph. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Plot fig, ax = plt.subplots(figsize=(25,27)) ax.margins(0.03) for name, group in groups_mbk: points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, #ms=18 label=cluster_names[name], mec='none', color=cluster_colors[name]) ax.set_aspect('auto') labels = [i for i in group.title] tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels, voffset=10, hoffset=10, #css=css) mpld3.plugins.connect(fig, tooltip) # , TopToolbar() ax.axes.get_xaxis().set_ticks([]) ax.axes.get_yaxis().set_ticks([]) #ax.axes.get_xaxis().set_visible(False) #ax.axes.get_yaxis().set_visible(False) ax.set_title("Mini K-Means", size=20) #groups_mbk ax.legend(numpoints=1) mpld3.disable_notebook() #mpld3.display() mpld3.save_html(fig, "mbk.html") mpld3.show() #mpld3.save_json(fig, "vivod.json") #mpld3.fig_to_html(fig) fig, ax = plt.subplots(figsize=(51,25)) scatter = ax.scatter(np.random.normal(size=N), np.random.normal(size=N), c=np.random.random(size=N), s=1000 * np.random.random(size=N), alpha=0.3, cmap=plt.cm.jet) ax.grid(color='white', linestyle='solid') ax.set_title("", size=20) fig, ax = plt.subplots(figsize=(51,25)) labels = ['point {0}'.format(i + 1) for i in range(N)] tooltip = mpld3.plugins.PointLabelTooltip(scatter, labels=labels) mpld3.plugins.connect(fig, tooltip) mpld3.show()fig, ax = plt.subplots(figsize=(72,36)) for name, group in groups: points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, label=cluster_names[name], mec='none', color=cluster_colors[name]) ax.set_aspect('auto') labels = [i for i in group.title] tooltip = mpld3.plugins.PointLabelTooltip(points, labels=labels) mpld3.plugins.connect(fig, tooltip) ax.set_title("K-means", size=20) mpld3.display()</span></span></code> </pre><br>  Now, when you hover on any point in the chart, the text pops up with the corresponding search query.  An example of the finished html file can be found here: <a href="http://olegbezverhii.github.io/clusters/mbk.html">Mini K-Means</a> <br><br>  If you want in 3D and with a variable scale, then there is a service <a href="https://plot.ly/">Plotly</a> , which has a plugin for Python. <br><br><div class="spoiler">  <b class="spoiler_title">Plotly 3d</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   3D     import plotly plotly.__version__ import plotly.plotly as py import plotly.graph_objs as go trace1 = go.Scatter3d( x=xs, y=ys, z=zs, mode='markers', marker=dict( size=12, line=dict( color='rgba(217, 217, 217, 0.14)', width=0.5 ), opacity=0.8 ) ) data = [trace1] layout = go.Layout( margin=dict( l=0, r=0, b=0, t=0 ) ) fig = go.Figure(data=data, layout=layout) py.iplot(fig, filename='cluster-3d-plot')</span></span></code> </pre><br></div></div><br>  The results can be seen here: <a href="https://plot.ly/~OlegBezverhii/0.embed">Example</a> <br><br>  And the final point is to perform hierarchical (agglomerative) clustering by the Ward method for creating a dendogram. <br><br><pre> <code class="python hljs">In [<span class="hljs-number"><span class="hljs-number">44</span></span>]: <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> scipy.cluster.hierarchy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ward, dendrogram linkage_matrix = ward(dist) fig, ax = plt.subplots(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>)) ax = dendrogram(linkage_matrix, orientation=<span class="hljs-string"><span class="hljs-string">"right"</span></span>, labels=titles); plt.tick_params(\ axis= <span class="hljs-string"><span class="hljs-string">'x'</span></span>, which=<span class="hljs-string"><span class="hljs-string">'both'</span></span>, bottom=<span class="hljs-string"><span class="hljs-string">'off'</span></span>, top=<span class="hljs-string"><span class="hljs-string">'off'</span></span>, labelbottom=<span class="hljs-string"><span class="hljs-string">'off'</span></span>) plt.tight_layout() <span class="hljs-comment"><span class="hljs-comment">#  plt.savefig('ward_clusters2.png', dpi=200)</span></span></code> </pre><br>  <b>findings</b> <br><br>  Unfortunately, in the field of natural language research there are a lot of unresolved issues and not all data can be easily and easily grouped into specific groups.  But I hope that this guide will increase interest in this topic and provide a basis for further experiments. </div><p>Source: <a href="https://habr.com/ru/post/346206/">https://habr.com/ru/post/346206/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346196/index.html">Vim: Search Javascript documentation</a></li>
<li><a href="../346198/index.html">How I Parsil Habr, Part 1: Trends</a></li>
<li><a href="../346200/index.html">What's wrong with Telegram or 5 controversial UX / UI solutions that can be fixed in it</a></li>
<li><a href="../346202/index.html">Tuning toolchain for Arduino for continuing</a></li>
<li><a href="../346204/index.html">Oracle upgrade experience 11.2.0.4 to 12c</a></li>
<li><a href="../346210/index.html">On-Premise vs. Cloud IaaS - advantages and disadvantages</a></li>
<li><a href="../346214/index.html">First contact with ‚Äúvar‚Äù in Java 10</a></li>
<li><a href="../346216/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ296 (January 1 - 7, 2018)</a></li>
<li><a href="../346218/index.html">11 libraries (sets of components) for Angular, which is worth knowing in 2018</a></li>
<li><a href="../346220/index.html">11 libraries (sets of components) for Vue, which is worth knowing in 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>