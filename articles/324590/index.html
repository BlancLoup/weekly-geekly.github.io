<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The history of the 3rd place on ML Boot Camp III</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The ML Machine Campaign for Mail.Ru ML has recently ended. 

 Being new to machine learning, I managed to take 3rd place. And in this article I will t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The history of the 3rd place on ML Boot Camp III</h1><div class="post__text post__text-html js-mediator-article">  The ML Machine Campaign for Mail.Ru ML has recently ended. <br><br>  Being new to machine learning, I managed to take 3rd place.  And in this article I will try to share my experience of participation. <br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/files/3fd/d40/0a3/3fdd400a34e94d29be920c8896f5ee6f.png"></div><a name="habracut"></a><br>  I have been participating in various sports programming contests for a long time, including other championships from Mail.Ru, from which I actually learned about it. <br><br>  I was familiar with machine learning only at the level of laboratory work.  I heard about such a resource as kaggle, but I did not participate in anything like this before.  Therefore, it became for me a kind of challenge - remember what was taught at the university, and try to collect something from this knowledge. <br>  I didn‚Äôt really hope for high places, but the prizes added some good motivation. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Shortly before the start, the task was already known.  This is a binary classification task.  The choice of programming language was not very great, I knew that it was customary to use Python or R. With both of them I am minimally familiar, but I chose R. In it, everything you need is out of the box, you don‚Äôt need to bother with building or installing packages .  Although it is strange, it has a GC curve, it periodically falls, I did not regret my choice. <br><br><h3>  Task </h3><br>  There are data on the participation of players in the online game for the last 2 weeks in the form of 12 numerical signs.  It is necessary to determine whether the player will leave her at least for a week, or will remain.  Namely, say the probability of this event. <br><br>  ‚Üí <a href="http://mlbootcamp.ru/championship/10/">Link</a> to the full condition of the problem. <br><br>  At first glance, the task is absolutely classic.  There, probably, the sea on kaggle, take the ready code and use.  Even in training they offer a similar task of <a href="http://mlbootcamp.ru/sandbox/5/">credit scoring</a> .  As it turned out, not so simple. <br><br><h4>  I found it useful: </h4><br><ul><li>  2 computers i5-3210M 2.5GHz √ó 4, 12GB RAM and i3-4170 3.7GHz √ó 4, 16GB RAM (there were 8, but I had to buy more) </li><li>  Installed RStudio on each </li><li>  A bit of luck </li></ul><br><h3>  Data overview </h3><br>  Training and test sample without gaps.  The size of 25,000 each - for so much.  At this research data is almost over.  I almost did not do any graphs and other visualizations. <br><br><h3>  First attempts </h3><br>  It was already determined in advance that I would start with logical classification algorithms - decisive trees, a decisive forest, as well as random forest, bagging, boosting over them. <br><br>  Decisive trees and random forest is not difficult to program yourself.  This is done according to <a href="http://www.ccas.ru/voron/download/LogicAlgs.pdf">Vorontsov‚Äôs lectures</a> , where the ID3 algorithm is described. <br><br>  It became clear that you would not go far with samopisnyh algorithms, although this procedure helped greatly in their understanding.  Need to use something ready. <br><br><h3>  Xgboost </h3><br>  <a href="https://github.com/dmlc/xgboost">Xgboost</a> is a library that implements gradient boosting.  Used by many winners of the kaggle competition - it means you need to try. <br><br>  One of the learning parameters was the number of trees ( <i>nrounds</i> ), but it is not immediately clear how many should be indicated.  There is an alternative - to split the sample into 2 parts - training and control.  If, when adding regular trees, the control error begins to deteriorate, then stop learning.  I used the Bootstrap aggregating technique. <br><br>  We divide the sample 200 times randomly into training and control (according to my experiments, the optimal ratio was 0.95 / 0.05), and run xgboost.  The final classifier is the voting (average) of all 200 basic classifiers. <br><br>  It gave me a much better result than the Random Random Forest or AdaBoost. <br><br><h3>  Feature engineering </h3><br>  The next step was the generation of new signs. <br><br>  The simplest thing you could think of was to generate a lot of non-linear combinations of existing features, then remove the unnecessary ones, leaving only the optimal set. <br><br>  New features are just pairwise multiplication and pairwise division of each with each.  Together with the baseline, 144 signs appeared. <br><br>  In the same lectures, Vorontsov proposes using the greedy Add-Del algorithm, alternately removing and adding a new feature.  But due to the instability of the model (at different random seed with the same data, the quality assessment varies greatly), this approach did not work. <br><br>  I used a genetic algorithm.  We will generate the initial population - binary vectors, meaning which signs to take and which ones to not.  New individuals appear by crossing and mutation of the previous ones.  Here it was necessary to work out the selection of various probabilities, fines for the number of signs, etc. For 4-6 generations and for 12 hours of work, everything usually came down to a local minimum.  However, this local minimum has already given good ratings.  Xgboost is not very sensitive to non-informative features (as opposed to the neural network, which will be discussed later) - one of the reasons why crossing two good sets of features also gives a good set. <br><br>  As a result, 63 were selected from 144 signs. <br><br><h3>  LightGBM </h3><br>  Later, I switched to using the Microsoft <a href="https://github.com/Microsoft/LightGBM">LightGBM</a> library.  It gave almost the same results as Xgboost, but it worked many times faster.  And also had additional training options.  For example, the ability to limit not only the maximum depth of a tree ( <i>max_depth</i> ), but also the number of leaves ( <i>num_leaves</i> ) was <i>useful</i> .  For me, <i>num_leaves</i> = 9 and <i>max_depth</i> = 4 were optimal. <br><br><h3>  Neural network </h3><br>  After unsuccessful attempts to use SVM, KNN, Random Forest, I stopped at the neural network.  Or rather, on a perceptron with one hidden layer, using the <a href="https://cran.r-project.org/web/packages/nnet/index.html">nnet</a> package. <br><br>  One of the first things I did was run something like this: <br><br><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">set</span></span>.seed(<span class="hljs-number"><span class="hljs-number">2707</span></span>) trControl = trainControl(<span class="hljs-keyword"><span class="hljs-keyword">method</span></span>=<span class="hljs-string"><span class="hljs-string">'cv'</span></span>, number=<span class="hljs-number"><span class="hljs-number">5</span></span>, classProbs=T, summaryFunction=mnLogLoss) model = train(X, Y, <span class="hljs-keyword"><span class="hljs-keyword">method</span></span>=<span class="hljs-string"><span class="hljs-string">'nnet'</span></span>, metric=<span class="hljs-string"><span class="hljs-string">'logLoss'</span></span>, maxit=<span class="hljs-number"><span class="hljs-number">1000</span></span>, maximize=F, trControl=trControl)</code> </pre> <br>  This is practically an example from the manual.  Next, I took the arithmetic mean with what I received from LightGBM, and made a parcel to the server.  I was very surprised how it threw me into the first place, where I lasted about a week. <br><br><h3>  Handling of special cases </h3><br>  As in the training and test samples, the same vectors were present, but with different answers.  For example, there were those who met 1423, 278, 357, 110 times.  Probably there is nothing better than counting probabilities for them separately, which I did.  Processed only those that met more than 15 times. <br><br>  The question was only to exclude these repetitions from training, or not.  I tried both options, and concluded that the exception makes a little bit worse. <br><br>  In fact, now you can stop.  This would give the final first place with a small margin.  But in hindsight, everyone is smart. <br><br><h3>  Ensemble of two models </h3><br>  It was worthwhile to come up with a more successful aggregation function than just the arithmetic mean or the geometric mean.  The idea is as follows: <br><br><ul><li>  Create a new sample based on the old one.  The only difference is in the answer column.  Column of answers - 0 or 1, depending on which of the two models gave the best result compared to the correct answer. </li><li>  Run this sample logistic regression, SVM, boosting, or whatever.  As it turned out, it was worth taking SVM. </li><li>  From these results of this aggregating model, we obtain probabilities with which we need to trust each of the two initial ones (boosting or neural network).  In fact, if we use a linear model, we get the optimal coefficients, instead of the usual average. <br></li></ul><br><h3>  Neural network + bootstrap </h3><br>  Leave what happened with the successful choice of the seed was impossible.  Just lucky, and it seemed like an obvious overfit.  Then I spent all the time trying to get closer to the result. <br><br>  So, lucky with a good choice of seed, i.e.  successfully weighed weight for neurons.  Then it was necessary to run training many times, and choose the best.  But how to determine whether it turned out better?  And I came up with the following: <br><br><ul><li>  We split the sample 200 times in a ratio of 0.9 / 0.1 (training / control). </li><li>  For each partition, we start training on a training subsample 20 times.  Choose the model that gave the best result on the control. </li><li>  The final model is a vote (average) of 200 models (it is important that not all 200 √ó 20). </li></ul><br>  Thus, I almost came close to the desired result.  But unfortunately, a little bit was not enough to win. <br><br><h3>  Unrealized ideas </h3><br><ul><li>  Use a graphics card to speed up learning.  The official version of LightGBM does not support, but there are forks. </li><li>  Use a computing cluster of two computers.  The <a href="https://cran.r-project.org/web/packages/doParallel/index.html">doParallel</a> package <a href="https://cran.r-project.org/web/packages/doParallel/index.html">supports</a> this.  Previously, I just went over RDP to a second computer and started it manually. </li><li>  Theoretically, having spent several premises, it was possible to calculate more accurate probabilities for repeating vectors with different answers (in other words, to extract some more data from a hidden test subsample). </li></ul><br>  <i>Thanks for attention.</i> <br><br>  Bibliography: <br><br><ul><li>  <a href="http://www.ccas.ru/voron/download/LogicAlgs.pdf">Lectures on logical classification algorithms, KV Vorontsov</a> </li><li>  <a href="http://www.machinelearning.ru/wiki/images/archive/9/97/20140227072517!Voron-ML-Logic-slides.pdf">Logic classification algorithms, KV Vorontsov</a> </li><li>  <a href="http://www.machinelearning.ru/wiki/images/4/4f/Voron-ML-Modeling-slides.pdf">Generalizing ability of the selection of traits, K. V. Vorontsov</a> </li></ul><br>  ‚Üí <a href="https://github.com/tyamgin/mlbootcamp/tree/master/championship10">Code on github</a> </div><p>Source: <a href="https://habr.com/ru/post/324590/">https://habr.com/ru/post/324590/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../324578/index.html">IntelliJ IDEA 2017.1 Review: Java 9, Kotlin 1.1, Spring, Gradle, JavaScript, Go, and more</a></li>
<li><a href="../324580/index.html">Asterisk queues, minor tricks</a></li>
<li><a href="../324582/index.html">Spring Games KIPS. Or master the budget for information security in $ 300.000</a></li>
<li><a href="../324586/index.html">Employees and social networks</a></li>
<li><a href="../324588/index.html">Continuous integration with Drone CI, Docker and Ansible</a></li>
<li><a href="../324592/index.html">Mahou updated to version 2.0</a></li>
<li><a href="../324594/index.html">JNI Receive and Connect to JVM in Delphi</a></li>
<li><a href="../324596/index.html">Statistics on property values ‚Äã‚Äã- visualization on the map</a></li>
<li><a href="../324598/index.html">Quadstor - performance and reliability</a></li>
<li><a href="../324600/index.html">GitHub introduces SHA-1 collision detection system</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>