<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine orientation over long distances with automated reinforcement learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Only in the United States alone there are 3 million people with limited mobility who cannot leave their homes. Auxiliary robots that can automatically...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine orientation over long distances with automated reinforcement learning</h1><div class="post__text post__text-html js-mediator-article">  Only in the United States alone there are 3 million people with limited mobility who cannot leave their homes.  Auxiliary robots that can automatically navigate long distances can make such people more independent by bringing them food, medicine, and packages.  Studies show that deep reinforcement learning (OP) is well suited for comparing raw input data and actions, for example, for learning to <a href="https://ai.googleblog.com/2018/06/scalable-deep-reinforcement-learning.html">capture objects</a> or <a href="https://ai.google/research/pubs/pub47151">moving robots</a> , but usually OP <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BD%25D1%2582%25D0%25B5%25D0%25BB%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25B3%25D0%25B5%25D0%25BD%25D1%2582">agents have</a> no understanding of large physical spaces necessary for safe orientation to distant distances without human help and adaptation to the new environment. <br><a name="habracut"></a><br>  In three recent works, " <a href="https://ieeexplore.ieee.org/document/8643443">Learning to Oriente from the Ground with AOP</a> ", " <a href="https://ai.google/research/pubs/pub46570">PRM-RL: Implementing Robotic Orientation over Long Distances with a Combination of Learning with Reinforcement and Sample-Based Planning</a> " and " <a href="https://arxiv.org/abs/1902.09458">Long-Range Orientation with PRM-RL</a> We study autonomous robots that easily adapt to the new environment, combining deep EP with long-term planning.  We train local scheduling agents to perform basic actions necessary for orientation, and to move over short distances without colliding with moving objects.  Local planners make noisy observations of the environment using sensors such as one-dimensional lidars, giving the distance to the obstacle, and give linear and angular velocities to control the robot.  We train the local scheduler in simulations using automated reinforcement learning (AOP), a method that automates the search for rewards for the OP and neural network architecture.  Despite the limited radius of action, 10‚Äì15 m, local planners adapt well both to use in real robots and to new, previously unknown environments.  This allows them to be used as building blocks for orientation in large spaces.  Then we build a roadmap, a graph where the nodes are separate sections, and the edges connect the nodes only if the local planners, who imitate real robots well with the help of noisy sensors and controls, can move between them. <br><br><h2>  Automatic reinforcement learning (AOP) </h2><br>  In <a href="https://ieeexplore.ieee.org/document/8643443">our first job,</a> we train the local scheduler in a static environment of small size.  However, when learning with a standard deep OT algorithm, for example, a deep deterministic gradient ( <a href="https://arxiv.org/abs/1509.02971">DDPG</a> ), you can encounter several obstacles.  For example, the real goal of local planners is to achieve a given goal, with the result that they receive rare rewards.  In practice, this requires researchers to spend considerable time on the step-by-step implementation of the algorithm and the manual adjustment of rewards.  Also, researchers have to make decisions about the architecture of neural networks, not having clear successful recipes.  And finally, algorithms such as DDPG are unstable and often demonstrate <a href="https://en.wikipedia.org/wiki/Catastrophic_interference">catastrophic forgetfulness</a> . <br><br>  To overcome these obstacles, we have automated deep learning with reinforcements.  AOP is an evolutionary automatic wrapper around a deep OP, looking for rewards and neural network architecture with the help of <a href="https://ai.google/research/pubs/pub46180">large-scale optimization of hyper-parameters</a> .  It works in two stages, the search for awards and the search for architecture.  During the search for rewards, AOP simultaneously trains a population of DDPG agents for several generations, and each one has its own slightly modified reward function optimized for the true task of the local scheduler: reaching the end point of the path.  At the end of the reward search phase, we choose the one that most often leads agents to the goal.  In the search phase of the neural network architecture, we repeat this process for this race using the chosen reward and adjusting the network layers, optimizing the cumulative reward. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/c96/017/74a/c9601774ae263fe9a2f333d2066e923d.png"><br>  <i>AOP with search awards and neural network architecture</i> <br><br>  However, this step-by-step process makes AOP ineffective in terms of the number of samples.  Training AOP with 10 generations of 100 agents requires 5 billion samples, equivalent to 32 years of training!  The advantage is that after AOP, the manual learning process is automated, and DDPG does not have a catastrophic forgetting.  Most importantly, the quality of the resulting policies is higher - they are resistant to noise from the sensor, drive and localization, and are well generalized to new environments.  Our best policy is 26% more successful than other orienteering methods at our test sites. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/049/823/97c/04982397cb2b6b7a7d20bc9e49ee1a75.png"><br>  <i>Red - AOP successes at short distances (up to 10 m) in several unknown buildings.</i>  <i>Comparison with manually trained DDPG (dark red), artificial potential fields (blue), dynamic window (blue) and behavior cloning (green).</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Kq1nQAF4xeM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>The local AOP scheduler policy works well with robots in real unstructured environments.</i> <br><br>  And although these policies are only capable of local orientation, they are resistant to moving obstacles and are well tolerated on real robots in unstructured environments.  And although they were trained in simulations with static objects, they effectively cope with moving objects.  The next step is to combine AOP policies with sample-based layout to expand their field of work and teach them how to navigate long distances. <br><br><h2>  Long-range orientation with PRM-RL </h2><br>  <a href="https://en.wikipedia.org/wiki/Motion_planning">Sample-based planners</a> work with orientation over long distances, approximating the movements of the robot.  For example, the robot builds <a href="https://ieeexplore.ieee.org/document/508439">probabilistic roadmaps</a> (PRM), passing realizable transition paths between sections.  In our <a href="https://ai.google/research/pubs/pub46570">second job</a> , which won an award at the <a href="https://icra2018.org/">ICRA 2018</a> conference, we combine PRM with manually adjusted local OP planners (without AOP) to train robots locally, and then adapt them to other environments. <br><br>  First, for each robot, we teach the local scheduler policy in a generalized simulation.  Then we create PRM taking into account this policy, the so-called PRM-RL, based on the map of the environment where it will be used.  The same card can be used for any robot we wish to use in the building. <br><br>  To create a PRM-RL, we merge nodes from samples only if the local OP-scheduler can reliably and repeatedly move between them.  This is done in a Monte Carlo simulation.  The resulting map adapts to the capabilities and geometry of a particular robot.  Cards for robots with the same geometry, but different sensors and actuators, will have different connectivity.  Since the agent can turn the corner, you can include nodes that are not in direct line of sight.  However, nodes located near walls and obstacles will be included in the map with a lower probability due to noise from sensors.  During execution, the OP agent moves on the map from one site to another. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/227/75f/f50/22775ff503bbaeb6113227523d06aa8a.gif"><br>  <i>A map is created with three Monte-Carlo simulations for each randomly selected pair of nodes.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/546/268/9f1/5462689f131cbd48339eec89f36add51.png"><br>  <i>The largest map was 288x163 m in size and contained almost 700,000 ribs.</i>  <i>300 workers collected it for 4 days, having spent 1.1 billion collision checks.</i> <br><br>  <a href="https://arxiv.org/abs/1902.09458">The third work</a> provides several improvements to the original PRM-RL.  First, we replace the manually adjusted DDPG with local schedulers with AOP, which gives an improvement in orientation over long distances.  Secondly, <a href="https://ru.wikipedia.org/wiki/SLAM_(%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4)">simultaneous localization and markup maps</a> ( <a href="https://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping">SLAM</a> ) are added, which robots use at runtime as a source for building roadmaps.  SLAM cards are subject to noise, and this closes the ‚Äúgap between the simulator and reality‚Äù, a problem known in robotics, due to which agents trained in simulations behave much worse in the real world.  Our level of success in the simulation coincides with the level of success of real robots.  And finally, we added distributed building maps, so we can create very large maps containing up to 700,000 nodes. <br><br>  We evaluated this method with the help of our AOP agent, who created maps based on blueprints of buildings that were 200 times more than the training environment, including only edges, which were successfully completed in 90% of cases in 20 attempts.  We compared the PRM-RL with various methods at distances of up to 100 m, seriously exceeding the range of the local scheduler.  PRM-RL achieved success 2-3 times more often than usual methods due to the correct connection of nodes, suitable for the capabilities of the robot. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/659/37a/fa465937a0e3a90383480a831ef4cec7.png"><br>  <i>The percentage of success in moving 100 meters in different buildings.</i>  <i>Blue - local AOP scheduler, first job;</i>  <i>red - original PRM;</i>  <i>yellow - artificial potential fields;</i>  <i>green is the second job;</i>  <i>red is the third job, PRM with AOP.</i> <br><br>  We tested PRM-RL on a variety of real robots in a variety of buildings.  Below is one of the test suites;  the robot moves reliably almost everywhere, except for the most erratic places and areas beyond the SLAM card. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e05/773/fbd/e05773fbd5e6cd4cb41adfebf9a8d083.png"><br><br><h2>  Conclusion </h2><br>  Machine orientation can seriously increase the independence of people with reduced mobility.  This can be achieved by developing autonomous robots that can easily adapt to the environment, and the methods available for implementation in the new environment based on the information already available.  This can be done by automating basic orientation training for small distances with AOP, and then use the acquired skills together with the SLAM maps to create roadmaps.  Road maps consist of nodes connected by edges, along which robots can safely move.  As a result, a policy of robot behavior is developed, which, after one training, can be used in different environments and issue roadmaps specially adapted for a specific robot. </div><p>Source: <a href="https://habr.com/ru/post/444372/">https://habr.com/ru/post/444372/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444362/index.html">Unity is working on a new physics engine with Havok</a></li>
<li><a href="../444364/index.html">Playing for Rust in 24 hours: personal development experience</a></li>
<li><a href="../444366/index.html">Seminar "Requirements of information security: how business live with them"</a></li>
<li><a href="../444368/index.html">We just printed out the microphone on a 3D printer in the laboratory - and then there will be complete science fiction</a></li>
<li><a href="../444370/index.html">What is Mini PCI-e format capable of?</a></li>
<li><a href="../444374/index.html">Hipster effect: why nonconformists often look the same</a></li>
<li><a href="../444376/index.html">Attention economy is almost dead</a></li>
<li><a href="../444378/index.html">USPACE - a single space for manned and unmanned aircraft</a></li>
<li><a href="../444380/index.html">Yandex and Hyundai will make a drone of the 5th level of autonomy</a></li>
<li><a href="../444382/index.html">How to visit Korean University using the Network File System</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>