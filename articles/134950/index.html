<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What Professor Ng hasn't taught us</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As can be seen from discussions in Habr√©, several dozen habrovchan attended a course of Stanford University ml-class.org, which was conducted by the c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What Professor Ng hasn't taught us</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/816/24d/65d/81624d65d62b48b6291ac321f2cb46eb.jpg">  As can be seen from discussions in Habr√©, several dozen habrovchan attended a course of Stanford University ml-class.org, which was conducted by the charming professor Andrew Ng.  I also enjoyed this course with pleasure.  Unfortunately, a very interesting topic, which was stated in the plan, fell out of the lectures: combining learning with a teacher and learning without a teacher.  As it turned out, Professor Ng published an excellent course on this topic - Unsupervised Feature Learning and Deep Learning (spontaneous feature extraction and deep learning).  I offer a brief summary of this course, without a strict presentation and an abundance of formulas.  In the <a href="http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial">original,</a> all this is. <br><a name="habracut"></a><br>  <i>My sidebars which are not part of the original text are in italics, but I could not resist and included my own comments and considerations.</i>  <i>I apologize to the author for the shameless use of illustrations from the original.</i>  <i>I also apologize for the direct translation of some terms from English (for example, spatial autoencoder -&gt; sparse autoencoder).</i>  <i>We are Stanford poorly know Russian terminology :)</i> <br><br><h4>  Sparse autoencoder </h4><br>  The most commonly used neural networks of direct distribution are designed for training with a teacher and are used, for example, for classification.  An example of the topology of such a neural network is shown in the figure: <br><br><img src="https://habrastorage.org/storage2/808/254/232/808254232cd4983cac374c5cc2a1fc87.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Training of such a neural network is usually done by back-propagating the error in such a way as to minimize the root-mean-square error of the network response on the training set.  Thus, the training sample contains pairs of feature vectors (input data) and reference vectors (tagged data) <b>{(x, y)}</b> . <br><br>  Now imagine that we have no tagged data ‚Äî only a set of feature vectors <b>{x}</b> .  The autoencoder is an unsupervised learning algorithm that uses a neural network and an error back-propagation method to ensure that the input feature vector triggers a network response equal to the input vector, i.e.  <b>y = x</b> .  Example of autoencoder: <br><br><img src="https://habrastorage.org/storage2/36f/c19/372/36fc1937240eb6a502670c5ee120d321.png"><br><br>  The autoencoder is trying to build a function <b>h (x) = x</b> .  In other words, it tries to find an approximation of such a function so that the response of the neural network approximately equals the value of the input features.  In order for the solution of this problem to be non-trivial, special conditions are imposed on the network topology: <br><br>  ‚Ä¢ The number of neurons in the hidden layer must be less than the dimension of the input data (as in the figure), or <br>  ‚Ä¢ Activation of neurons in the hidden layer must be sparse. <br><br>  The first limitation allows you to get data compression when transmitting the input signal to the network output.  For example, if the input vector is a set of image brightness levels of 10x10 pixels (100 signs total), and the number of neurons in the hidden layer 50, the network is forced to learn how to compress the image.  Indeed, the requirement <b>h (x) = x</b> means that, based on the activation levels of fifty neurons of the hidden layer, the output layer must recover 100 pixels of the original image.  Such compression is possible if there are hidden relationships in the data, a correlation of features, and in general some kind of structure.  In this form, the operation of the auto-encoder is very similar to the method of principal component analysis (PCA) in the sense that the dimension of the input data decreases. <br><br>  The second limitation - the requirement of sparse activation of neurons of the hidden layer - allows you to get non-trivial results even when the number of neurons of the hidden layer exceeds the dimension of the input data.  If informality is described informally, then we will consider a neuron as active when the value of its transfer function is close to 1. If a sigmoid transfer function is used, but for an inactive neuron its value should be close to 0 (for the hyperbolic tangent function - to -1).  Sparse activation is when the number of inactive neurons in the hidden layer significantly exceeds the number of active ones. <br><br>  If we calculate the value of p as the average (according to the training sample) value of the activation of the neurons of the hidden layer, we can add an additional penalty term to the objective function used in the gradient training of the neural network by the method of backward error propagation.  The formulas are in the original lectures, and the meaning of the penalty coefficient is similar to the regularization method for calculating the regression coefficients: the error function increases significantly if the value of p differs from the predetermined sparseness parameter.  For example, we may require that the average activation value for a training sample be 0.05. <br><br>  <i>The requirement of sparse activation of neurons of the hidden layer has a bright biological analogy.</i>  <i>Jeff Hawkins, author of the original theory of brain structure, notes the fundamental importance of inhibitory connections between neurons (see the Russian text <a href="http://www.numenta.com/htm-overview/education/HTM_CorticalLearningAlgorithms_ru.pdf">Hierarchical Temporal Memory (HTM) and its cortical learning algorithms</a> ).</i>  <i>In the brain between neurons located in one layer there is a large number of "horizontal connections".</i>  <i>Although neurons in the cerebral cortex are very tightly interconnected, numerous inhibitory (inhibitory) neurons ensure that only a small percentage of all neurons are active at a time.</i>  <i>That is, it turns out that information is always presented in the brain only by a small number of active neurons from all those present there.</i>  <i>This, apparently, allows the brain to make generalizations, for example, to perceive the image of a car from any angle just like a car.</i> <br><br><h4>  Visualization of hidden layer functions </h4><br>  By training an autoencoder on an unmarked data set, you can try to visualize the functions approximated by this algorithm.  Very visual is the visualization of the above example of teaching an encoder on images of 10x10 pixels.  Let us ask ourselves the question: ‚ÄúWhat combination of input x will cause the maximum activation of the hidden neuron number i?‚Äù That is, what set of input data does each of the hidden neurons search for? <br><br>  A nontrivial answer to this question is contained in the <a href="http://ufldl.stanford.edu/wiki/index.php/Visualizing_a_Trained_Autoencoder">lecture</a> , and we will limit ourselves to an illustration, which was obtained by visualizing the function of a network with a hidden layer of 100 neurons. <br><br><img src="https://habrastorage.org/storage2/bba/65d/c64/bba65dc649b64734bd89f1b1b39abd7d.png"><br><br>  Each square fragment represents the input image x, which maximally activates one of the hidden neurons.  Since the corresponding neural network was trained on examples of natural images (for example, fragments of photographs of nature), the neurons of the hidden layer independently studied the functions of detecting contours from different angles! <br><br>  <i>In my opinion, this is a very impressive result.</i>  <i>The neural network itself, by observing a large number of various images, built a structure similar to the biological structures in the brain of humans and animals.</i>  <i>As can be seen from the following illustration from the great book ‚ÄúFrom the Neuron to the Brain‚Äù by J. Nichols and others, this is how the lower visual divisions of the brain are arranged:</i> <br><br><img src="https://habrastorage.org/storage2/fbc/21f/32f/fbc21f32f6cf9f150af67d228383b4bc.jpg"><br><br>  <i>The figure shows the responses of complex cells in the cat's striped bark to visual stimuli.</i>  <i>The cell responds best (most output pulses) to the vertical border (see the first fragment).</i>  <i>The reaction to the horizontal border is practically absent (see the third fragment).</i>  <i>A complex cell in a striped cortex approximately corresponds to our trained neuron in a hidden layer of an artificial neural network.</i> <br><br>  <i>The whole set of neurons in the hidden layer has learned to detect contours (brightness difference boundaries) from different angles - just like in the biological brain.</i>  <i>The following illustration from the book ‚ÄúFrom Neuron to Brain‚Äù schematically shows the axes of orientation of the receptive fields of neurons as they sink deeper into the cat's brain cortex.</i>  <i>Similar experiments helped to establish that cells with similar properties in cats and monkeys are arranged in the form of columns running at certain angles to the surface of the cortex.</i>  <i>Individual neurons in the column are activated by visual stimulation of the corresponding part of the field of view of the animal with a black stripe against a white background, rotated at a certain angle specific for each neuron in the column.</i> <br><br><img src="https://habrastorage.org/storage2/720/629/439/720629439c920098c700ce273b872a39.jpg"><br><br><h4>  Self study </h4><br>  The most effective way to get a reliable machine learning system is to provide the learning algorithm with as much data as possible.  According to the experience of solving large-scale tasks, there is a qualitative transition when the size of the training sample exceeds 1-10 million samples.  You can try to get more tagged data for training with the teacher, but this is not always possible (and cost-effective).  Therefore, the use of unmarked data for self-learning of neural networks seems promising. <br><br>  Unmarked data contains less training information than tagged data.  But the amount of data available for teaching without a teacher is much more.  For example, an unlimited number of digital photographs are available on the Internet in image recognition tasks, and only a tiny percentage of them are marked. <br><br>  In self-learning algorithms, we will give our neural network a large amount of unmarked data, from which the network will learn to extract useful signs.  Further, these features can be used to train specific classifiers using relatively small labeled training samples. <br><br>  We draw the first stage of self-learning in the form of a neural network with 6 inputs and three neurons of the hidden layer.  The outputs of these neurons will be generalized signs that are extracted from the unmarked data by the sparse autoencoder algorithm. <br><br>  Now you can train the output layer of the neural network, or use logistic regression, the support vector machine or the softmax algorithm for classifying training based on the identified features a.  The input to these traditional algorithms will be the labeled training sample <b>xm</b> .  There are two options for the self-learning network topology: <br><br>  ‚Ä¢ To the input of a traditional classifier (for example, the output layer of a neural network) only signs a are fed; <br>  ‚Ä¢ Signs a and input signs <b>xm</b> are fed to the input of the traditional classifier (for example, the output layer of the neural network). <br><br>  Further in the Ng lectures, multilayer networks and the use of autoencoders for their training are considered. </div><p>Source: <a href="https://habr.com/ru/post/134950/">https://habr.com/ru/post/134950/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../134942/index.html">Intelligent multichannel fiber optic connections</a></li>
<li><a href="../134944/index.html">Monetization of applications - where is the money, or why is it all advertising?</a></li>
<li><a href="../134946/index.html">HDD manufacturers reduce warranty periods</a></li>
<li><a href="../134947/index.html">The benefits of patent trolling</a></li>
<li><a href="../134948/index.html">Seagate bought Samsung HDD</a></li>
<li><a href="../134951/index.html">Build GNU Emacs for Ubuntu</a></li>
<li><a href="../134952/index.html">140 character idea</a></li>
<li><a href="../134954/index.html">Hidden Markov models in speech recognition</a></li>
<li><a href="../134956/index.html">On January 21, PHP conference of phpDev Minsk developers</a></li>
<li><a href="../134957/index.html">Competition "Holidays of the Future"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>