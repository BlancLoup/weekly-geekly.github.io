<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The science of emotion: how smart technologies learn to understand people</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Valentina Evtyukhina, the author of the Digital Eva channel, and the specialists of the project company and the R & D laboratory of the Neurodata Lab,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The science of emotion: how smart technologies learn to understand people</h1><div class="post__text post__text-html js-mediator-article">  <i>Valentina Evtyukhina, the author of the <a href="https://t.me/evaevt">Digital Eva</a> channel, and the specialists of the project company and the R &amp; D laboratory of the <a href="http://www.neurodatalab.com/about/">Neurodata Lab,</a> specifically for the <a href="https://netology.ru/%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dhabr">Netology</a> blog, prepared an article on how technologies are developing in the field of emotion recognition.</i> <br><br>  The science of emotion became popular not so long ago, and mainly thanks to Paul Ekman, an American psychologist, author of the book Psychology of Lies and consultant to the popular TV show Lie to Me, which is based on the materials of the book. <br><br> <a href="https://habr.com/company/netologyru/blog/354828/"><img src="https://habrastorage.org/webt/fg/1l/hi/fg1lhihxa1helhapgryq4hvi9ci.png"></a> <br>  <i>Paul Ekman and Tim Roth - starring in the TV series "Lie to Me", whose character is written off from the very Ekman</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The series started in 2009, and at the same time public interest in the topic of emotion recognition has grown significantly.  The boom in the startup environment occurred in 2015‚Äì2016, when two technology giants ‚Äî Microsoft and Google ‚Äî were immediately available to ordinary users for their pilot projects for working with the science of emotions. <a name="habracut"></a><br><br><img src="https://habrastorage.org/webt/8q/vm/qh/8qvmqhabznkf2sbsgifdjd1jpui.png" alt="image"><br>  <i>Service for the recognition of emotions Emotion Recognition, <a href="https://tjournal.ru/56669-microsoft-emotion-api">launched by</a> Microsoft</i> <i><br></i>  <i>in 2015</i> <br><br>  This was the impetus for the creation of a variety of applications and algorithms based on the technology of recognition of emotions.  For example, Text Analytics API is one of the Microsoft Cognitive Services package services that allow developers to embed ready-made smart algorithms into their products.  Among other services of the package: image recognition, face recognition, speech, and many others.  Now, emotions can be identified by text, voice, photo and even video. <br><blockquote>  Gartner agency <a href="https://www.gartner.com/smarterwithgartner/gartner-top-strategic-predictions-for-2018-and-beyond/">claims</a> that our smartphone in 2021-2022 will know us better than our friends and relatives, and interact with us on a subtle emotional level. </blockquote><br><h2>  The market for emotion recognition technology - what's wrong with it? </h2><br>  He is, but he is young, he still has everything ahead. <br><br><blockquote>  Now the emotion detection market is experiencing a boom, and according to Western experts, by 2021 it will grow, according to various estimates, from $ 19 billion to $ 37 billion. </blockquote><br>  Thus, according to the influential agency MarketsandMarkets, the global volume of the emotions market in 2016 <a href="https://www.marketsandmarkets.com/PressReleases/emotion-detection-recognition.asp">amounted to</a> $ 6.72 billion, and it is assumed that by the mid-2020s.  it will increase to $ 36.07 billion. The market of emotional technologies is not monopolized.  There is a place for corporations, for laboratories, and for startups.  Moreover, normal market practices: corporations integrate smaller companies into their solutions. <br><br>  Emotional and behavioral technologies are in demand in various fields, including medical. <br><br>  Turning to foreign experience, we recall how Empatica, under the leadership of Rosalind Picard, was the first in the world to <a href="https://www.prnewswire.com/news-releases/embrace-by-empatica-is-the-worlds-first-smart-watch-to-be-cleared-by-fda-for-use-in-neurology-300593398.html">obtain</a> permission from US supervisors responsible for clinical trials (FDA clearing) a few weeks ago to use their Embrace wristband, which not only captures physiological data about the state of the owner, but also evaluates his emotional background and predicts the likelihood of the occurrence of difficult situations for the organism.  It can help people with autism spectrum disorders, depression and in difficult cases in neurology and medicine. <br><br>  The Israeli company Beyond Verbal together with the Mayo Clinic is <a href="http://www.beyondverbal.com/mayo-clinic-study-shows-voice-analyzing-app-may-be-useful-in-heart-disease-diagnosis/">looking for</a> vocal biomarkers in a human voice, which determine not only emotions, but also lays the possibility of predicting coronary coronary diseases, Parkinson's and Alzheimer's diseases, which already bring emotional issues to the topic of gerontology and the search for ways to slow aging. <br><br>  If we talk about the applicability of technology, then B2B sphere is mainly involved in sectors like intelligent transport, retail, advertising, HR, IoT, gaming. <br><br>  But there is also a demand in B2C: EaaS (Emotion as a Service) or a cloud-based analytical solution (Human data analytics) will allow anyone to upload a video file and get all the emotional and behavioral statistics for each record fragment. <br><br>  If we are talking about the election debate for the presidency (whether it be Russia or the United States), then hardly anything is going to disappear from the algorithm.  Moreover, in a couple of years, the technology for recognizing emotions will be in every smartphone. <br><br><h2>  Technology and science stack </h2><br>  AI boom <a href="http://www.cityam.com/270451/gartner-hype-cycle-2017-artificial-intelligence-peak-hype">was predicted</a> for 2025-2027. <br><br>  The trend will be the creation of smart interfaces for recognizing human emotions - the software will allow you to determine the user's state at an arbitrary point in time using a regular webcam. <br><br>  This is a promising niche, since the definition of human emotions can be used for commercial purposes: from analyzing the perception of video and audio content to investigating criminal cases. <br><br>  On the other hand, these are endless possibilities of the entertainment industry.  For example, the new iPhone X has Face ID, a face recognition technology that not only unlocks the phone, but also can create emoji with your facial expressions: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/7iDB-AzS-9A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  The bulk of new products in the field of emotional science is based on seven basic emotions and micro-expression of the face, which reflects our emotions at a level beyond the control of the brain.  Consciously, we can hold back a smile, but the slight twitching of the corners of the lips will remain, and this will be a signal for emotion recognition technology. <br><br>  There is also a block of technologies specializing in the analysis of speech, voice and gaze.  The use of these methods in psychiatry or criminal proceedings will allow you to learn the maximum about the emotional state of a person and his true mood thanks to information about the smallest changes in facial expressions and body movements. <br><br>  Now companies and individual teams can use open scientific data on the recognition of emotions and use them in a stack with technologies, forming the field of emotional computing (affective computing). <br><br>  The FAANG top five ( <b>Facebook</b> , <b>Apple</b> , <b>Amazon</b> , <b>Netflix</b> , <b>Google</b> ) and technology giants like <b>IBM</b> made a huge contribution to the development of the market of emotional technologies. <br><br><h2>  Emotion Recognition and Law </h2><br>  There are no direct legal barriers to emotional technologies, and the industry itself is regulated rather weakly and precisely.  There are expected barriers and concerns: first of all, it is a problem of privacy and personal data protection. <br><br><blockquote>  Emotions are private, fairly personal data about a person, his states, sensations, responses to stimuli, people and the environment, thoughts and intentions, sometimes not fully realized rationally. </blockquote><br>  At the same time, universal digitalization, distribution of gadgets and devices of any kind, universal access to images and videos (several billion videos hit the network daily), publicity on social networks allows you to efficiently extract emotional data from the general stream and use them for analyzing a person - as a consumer of goods and services, and as a user.  And all this should take place in the legal field, correctly and ethically. <br><br>  New European <a href="https://www.eugdpr.org/">regulations</a> on personal data protection (GDPR) imply a number of limitations: the data for learning and training machine learning algorithms can be used freely if they are: <br><br><ul><li>  they remain depersonalized, that is, biosensor data are separated from biometrics (identification of people); </li><li>  if the group format is respected (analysis of the crowd, of many people, and not of individual subjects); </li><li>  if a photograph is being taken, the person must be aware of this and be in agreement with it, otherwise it will be a violation of the regulations and entail responsibility. </li></ul><br>  How the story will unfold with the regulatory framework in Russia, time will tell. <br><br><h2>  Where in the coming years will need recognition of emotions </h2><br><h3>  Health and Health tech </h3><br>  The health industry is actively introducing the most modern methods of collecting and analyzing data about patients or users, as computer algorithms <a href="http://uofuhealth.utah.edu/innovation/blog/2015/10/10AlgorithmsChangingHealthCare.php">determine</a> symptoms using hundreds and thousands of similar cases. <br><br>  There are already mobile applications that analyze the psycho-emotional state of the <a href="https://azure.microsoft.com/ru-ru/services/cognitive-services/emotion/">photo</a> and <a href="https://woebot.io/">text</a> , and the more a person communicates with the program, the better she learns, ‚Äúunderstands‚Äù it and gives accurate predictions of treatment. <br><br>  It‚Äôs one thing when a device simply picks up, ‚Äúunderstands‚Äù at its level your mood and according to it switches on music, adjusts light or makes coffee.  The other is when it evaluates the degree of fatigue or determines any deviations from the norm according to your appearance.  Or disease.  For example, Alzheimer's or Parkinson‚Äôs. <br><br>  Long before its manifestation, the disease begins to affect the muscles of the face, the speed of eye movement, the seemingly insensible changes in the voice and micromovements. <br><br><h3>  Forensics </h3><br>  The series "Lie to Me" was released in 2009 and immediately gained worldwide popularity.  The protagonist Dr. Lightman can read the truth on face micromimics.  This is his ‚Äúsuperpowers‚Äù, which helps to find the killer and uncover a network of convoluted crimes. <br><br>  Neural interfaces can all be the same, only better, better and faster.  You can rent a person in the interrogation room and then put a special program on the video that predicts the percentage of emotions on his face - anger, fear, bitterness, resentment, and so on.  These data will help the investigation to understand at what point a person could cheat or under-commit something. <br><br><h3>  Monitoring social activities </h3><br>  It is believed that the Internet does not convey emotions, but it is not.  From a series of tweets or posts on Facebook, you can determine with high accuracy what kind of mood and state the user was at the time he wrote it. <br><br>  The simplest example of determining the psycho-emotional state of the text's style is a well-known situation where a person begins to put an end to the end of a message, and his interlocutor perceives this as a signal that something went wrong in the conversation. <br><br>  Globally, with machine learning, you can create a system that will track outbursts of anger, requests for help, or fear in messages and respond to them ‚Äî for example, send a signal to rescue services. <br><br><h3>  Advertising </h3><br>  Already, global retail networks integrate online into offline as much as possible, trying to find out what the buyer wants and what he is most likely to buy.  When neural interfaces reach the level of accurate, highly sensitive emotion recognition, advertising in the storefront of a shopping center will adjust to the mood of people passing by in a fraction of seconds.  Such technology is shown in science fiction films, for example, ‚ÄúMinority Report‚Äù and ‚ÄúBlade Runner 2049‚Äù. <br><br><img src="https://habrastorage.org/webt/9h/ca/ew/9hcaewtp7t6luuwo0pwh0dnayog.png" alt="image"><br>  <i>Shot from the movie ‚ÄúBlade Runner 2049‚Äù, where holographic advertising</i> <i><br></i>  <i>ginoid reacts to the emotions on the face of the protagonist.</i> <br><br>  About a year ago, in April 2017, a San Francisco research team taught the LSTM neural network to more accurately recognize the emotional component of the text.  Now the machine almost correctly identifies the mood in the customer reviews on Amazon and movie reviews of Rotten Tomatoes, which helps to improve the service and predict the popularity of the product among users. <br><br><h3>  Game industry </h3><br>  When the first model of Google Glass glasses came out, it was assumed that gesture control would reach a new level - in order to read the text on the inside of the lens, it was enough to hold your eyes from top to bottom so that the system would understand that you had already read this paragraph and you can show the following .  Despite the fact that the gadget itself did not go beyond the scope of the prototype, the story of the study of eye movements has moved into a new field - the game. <br><br>  It is very important for game developers to understand how and at what point the player feels, how special effects and game obstacles affect him.  The company-developer of emotion recognition technology Affectiva <a href="https://www.affectiva.com/success-story/flying-mollusk/">helped</a> create the game Nevermind, in which the complexity depends on the level of stress playing, and the plot adjusts to the state of stress or the calmness of the player. <br><br><h2>  And what about Russia?  Neurodata Lab Experience </h2><br>  After the beginning of 2016, the <a href="http://www.envirtuecapital.com/">Envirtue Capital</a> fund team came to the conclusion that in many aspects the existing venture capital market of Russia in terms of emotion recognition technology does not meet investors' expectations, it was decided to develop projects within its R &amp; D laboratory, fully autonomous and funded from its own sources.  Thus was born the company Neurodata Lab LLC. <br><br>  <i>‚ÄúSince September 2016, our team began to form, including today both research workers - specialists in natural and cognitive sciences, and technical experts with competences and background in the field of computer vision, machine learning, data science.</i>  <i>The interdisciplinary nature of the research of emotions predetermined our choice in favor of a mixed team, which allows us to think about solving problems from different points of view, to combine in one contour both a purely technical part, and the views and ideas of biology, psychophysiology and neurolinguistics. ‚Äù</i> <br>  <b>George Pliev</b> <br>  Managing Partner of Neurodata Lab <br><br>  Neurodata Lab develops solutions that cover a wide range of areas in the field of emotion research and their recognition by audio and video, including technologies for voice separation, layer-by-layer analysis and voice identification of the speaker in the audio stream, integrated tracking of body and arm movements, as well as detection and recognition of key points and movements of the muscles of the face in the video stream in real time. <br><br>  One of these projects is the <a href="http://www.neurodatalab.com/en/projects/EyeCatcher/">development of a</a> prototype of the EyeCatcher software that allows you to extract eye and head movement data from video files recorded on a regular camera.  This technology opens up new horizons in the study of human eye movements in natural, not laboratory, conditions and significantly expands research capabilities - you can now find out how a person views pictures, reacts to sound, color, taste, what eye movement is when he is happy or surprised .  This data will be used as a basis for creating a more advanced technology for recognizing human emotions. <br><br>  <i>‚ÄúOur goal is to design a flexible platform and develop technologies that will be in demand by private and corporate clients from various industries, including niche ones.</i>  <i>When detecting and recognizing emotions, it is important to take into account that human emotions are a very variable, ‚Äúelusive‚Äù entity, which often changes from person to person, from society to society;</i>  <i>There are ethnic, age, gender, socio-cultural differences.</i>  <i>To identify patterns, you need to train the algorithms on very large samples of qualitative data.</i>  <i>This is the phase on which the team of our laboratory is now focused. ‚Äù</i> <br>  <b>George Pliev</b> <br>  Managing Partner of Neurodata Lab <br><br>  One of the main difficulties that research groups encounter when studying emotions is the limited and ‚Äúnoisy‚Äù data to work with emotions in a natural setting or the need to use uncomfortable wearable instruments to track the emotional state of a participant in an experiment that distorts perception.  Therefore, as one of its first projects, the Neurodala Lab team collected the Russian-language multimodal dataset RAMAS (The Russian Acted Multimodal Affective Set) - a comprehensive set of data about the emotions experienced, including parallel recording of 12 channels: audio, video, eytreker, wearable motion sensors, etc. about each of the situations of interpersonal interaction.  Actors from VGIK who recreate various situations of everyday communication took part in the creation of the dataset.  Today, access to the multi-modal RAMAS database is <a href="http://www.neurodatalab.com/projects/RAMAS/">provided</a> free of charge to academic institutions, universities and laboratories. <br><br><img src="https://habrastorage.org/webt/3c/8s/5n/3c8s5nj9t16rgu16xqius53lfgy.png" alt="image"><br><br>  The presence of a broad database is one of the key factors of high-quality research work with emotions, but it is impossible to accumulate such a database in laboratory conditions and game simulations.  To solve this well-known problem, Neurodata Lab specialists developed and launched their own <a href="https://www.emotionminer.com/">Emotion Miner</a> platform for collecting, marking, analyzing and processing emotional data, which collected more than 20,000 annotator markers, marking data from more than 30 countries.  To date, <a href="http://www.neurodatalab.com/en/projects/emotion-miner-corpus/">Emotion Miner Data Corpus</a> is one of the world's largest tagged multimodal emotional video data. <br><br>  Since the establishment of the Neurodata Lab, laboratory staff have collaborated with academic institutions, universities, laboratories and specialized centers of competence in the United States, Europe and Russia, and actively participate in major international conferences, including Interspeech and ECEM, publish academic articles.  The company participated in a <a href="http://go.affectiva.com/emotion-ai-summit">summit</a> on emotional artificial intelligence promoted jointly by MIT and Affectiva, and in March 2018 <a href="https://ifmo.timepad.ru/event/675637/">organized the</a> first Russian conference ‚ÄúEmotion AI: New Challenges for Science and Education, New Business Opportunities‚Äù with NRU ITMO.  The plans are to create the Russian Association for Emotion AI, to consolidate the community of scientific experts, laboratories and startups. <br><br>  <i>‚ÄúWhen emotion recognition technology reaches maturity, it will have a significant impact on the entire ecosystem, on the whole technosphere, allowing people to better, deeper and more fully communicate with each other using gadgets and with the world of rapidly‚Äú smarter machines ‚Äùwith a human-computer interface.</i>  <i>The technology carries the potential for the development of mutual understanding and empathy, will allow to solve the problems of people with disabilities (for example, with autism) and will find the keys to alleviate socially critical diseases.</i>  <i>However, not only technology is important, but also how people use it.</i>  <i>We fully share the ethical imperative and proceed from the fact that the system of checks and balances, including legislative ones, will not transform emotion recognition technology into total control technology.</i>  <i>Her mission is to help a person without restricting his freedom, his rights, his personal space.</i>  <i>Of course, separate aberrations are inevitable, but are eliminated. ‚Äù</i> <br>  <b>George Pliev</b> <br>  Managing Partner of Neurodata Lab <br><br><h2>  From the Editor </h2><br>  Courses "Netology" on the topic: <br><br><ul><li>  full-time course " <a href="https://netology.ru/programs/data-scientist">Data Scientist</a> ", Moscow; <br></li><li>  Full-time course " <a href="https://netology.ru/programs/analytics-for-executives">Analytics for Managers</a> ", Moscow; <br></li><li>  online program " <a href="https://netology.ru/programs/big-data%3Futm_source%3Dblog%26utm_medium%3D747%26utm_campaign%3Dhabr">Big Data: Basics of working with large data arrays</a> "; <br></li><li>  online profession " <a href="https://netology.ru/programs/python">Python-developer</a> ". <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/354828/">https://habr.com/ru/post/354828/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../354818/index.html">Synchronous system interface interaction with peripheral blocks in the volume of the crystal VLSI or FPGA. STI 1.0</a></li>
<li><a href="../354820/index.html">Program Overview Heisenbug 2018 Piter</a></li>
<li><a href="../354822/index.html">Studies show: people who have ‚Äútoo many interests‚Äù are more likely to succeed</a></li>
<li><a href="../354824/index.html">Payment system architecture. Experience proven banality</a></li>
<li><a href="../354826/index.html">Procedural dungeon generation in roguelike</a></li>
<li><a href="../354830/index.html">How to detect FinFisher. ESET Manual</a></li>
<li><a href="../354832/index.html">Biomechanics. Start</a></li>
<li><a href="../354834/index.html">DevConf: a little about blockchain</a></li>
<li><a href="../354836/index.html">Secure Development Section on the PHDays 8 Forum</a></li>
<li><a href="../354838/index.html">PHDays CTF 2018. Writeup maker-up</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>