<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Home BigData. Part 1. Practice Spark Streaming on an AWS Cluster</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. 

 In this article, we will install Apache Kafka, Apache Spark, Zookeeper, Spark-shell on EC2 AWS (Amazon Web Services) platform at home and le...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Home BigData. Part 1. Practice Spark Streaming on an AWS Cluster</h1><div class="post__text post__text-html js-mediator-article">  Hello. <br><br>  In this article, we will install Apache Kafka, Apache Spark, Zookeeper, Spark-shell on EC2 AWS (Amazon Web Services) platform at home and learn how to use it all. <br><a name="habracut"></a><br><h3>  Introduction to the Amazon Web Services Platform </h3><br>  1.1.  Under the link <a href="https://aws.amazon.com/console">aws.amazon.com/console</a> you have to register.  Enter the name and remember the password. <br><br>  1.2.  Configure node instances for Zookeeper and Kafka services. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Select "Services-&gt; EC2" in the menu.  Next, you need to select the operating system version of the image of the virtual machine, select Ubuntu Server 16.04 LTS (HVM), SSD volume type, click "Select". Go to setting up the server instance: type "t3.medium" with the parameters 2vCPU, 4 GB of memory, General Purpose Click "Next: Configuring Instance Details". </li><li>  Add the number of copies 1, click "Next: Add Storage" </li><li>  We accept the default value for the disk size of 8 GB and change the type to Magnetic (in the Production settings based on the data volume and High Performance SSD) </li><li>  In the section ‚ÄúTag Instances‚Äù for ‚ÄúName‚Äù, enter the name of the instance of the node ‚ÄúHome1‚Äù (where 1 is just a sequence number) and click on ‚ÄúNext: ...‚Äù </li><li>  In the "Configure Security Groups" section, select the "Use existing security group" option, select the name of the security group ("Spark_Kafka_Zoo_Project") and set the incoming traffic rules.  Click on "Next: ..." </li><li>  Review the Review screen to verify the entered values ‚Äã‚Äãand, and launch Launch. </li><li>  To connect to the cluster nodes, you must create (in our case, use an existing) public key pair for identification and authorization.  To do this, select the type of operation ‚ÄúUse existing pair‚Äù from the list. </li></ul><br><h3>  Creating keys </h3><br><ul><li>  <a href="https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html">Download Putty</a> for the client or use SSH connection from the terminal. </li><li>  The key file .pem uses the old format for convenience we convert it to the ppk format used by Putty.  To do this, run the PuTTYgen utility, load the key in the old .pem format into the utility.  We convert the key and save (Save Private Key) for later use in the home folder with the .ppk extension. </li></ul><br><h3>  Cluster startup </h3><br>  1.3.  For convenience, rename the cluster nodes to Node01-04 notation.  To connect to the cluster nodes from the local computer via SSH, you must determine the IP address of the node and its public / private DNS name, select each cluster node in turn, and for the selected instance write down its public / private DNS name for connection via SSH and for installation Software to text file HadoopAdm01.txt. <br><br>  Example: ec2-35-162-169-76.us-west-2.compute.amazonaws.com <br><br><h3>  Installing Apache Kafka in SingleNode Mode on an AWS Cluster Node </h3><br>  2.1.  To install the software, select our node (copy its Public DNS) to connect via SSH.  We configure connection through SSH.  Use the saved name of the first node to configure the connection via SSH using the Private / Public key pair ‚ÄúHadoopUser01.ppk‚Äù created in clause 1.3.  Go to the Connection / Auth section using the Browse button and look for the folder where we previously saved the ‚ÄúHadoopUser.ppk‚Äù file. <br><br>  Save the connection configuration in the settings. <br><br>  2.2.  Connect to the site and use login: ubuntu. <br><br>  ‚Ä¢ Using root privileges, update packages and install additional packages required for further installation and configuration of the cluster. <br><br><pre><code class="plaintext hljs">sudo apt-get update sudo apt-get -y install wget net-tools netcat tar</code> </pre> <br>  ‚Ä¢ Install java 8 jdk and check java version. <br><br><pre> <code class="plaintext hljs">sudo apt-get -y install openjdk-8-jdk</code> </pre><br>  ‚Ä¢ For normal cluster node performance, you need to adjust the memory swapping settings.  VM swappines is set to 60% by default, which means when 60% of the memory is utilized, the system will actively swap data from RAM to disk.  Depending on the Linux version, the VM swappines parameter can be set to 0 or 1: <br><br><pre> <code class="plaintext hljs">sudo sysctl vm.swappiness=1</code> </pre><br>  ‚Ä¢ To save the settings upon reboot, add a line to the configuration file. <br><br><pre> <code class="plaintext hljs">echo 'vm.swappiness=1' | sudo tee --append /etc/sysctl.conf</code> </pre><br>  ‚Ä¢ Edit entries in the / etc / hosts file to conveniently resolve the cluster node names kafka and zookeeper to private IP addresses assigned to the cluster nodes. <br><br><pre> <code class="plaintext hljs">echo "172.31.26.162 host01" | sudo tee --append /etc/hosts</code> </pre><br>  We check the correctness of name recognition by ping any of the records. <br><br>  ‚Ä¢ Download the latest current versions (http://kafka.apache.org/downloads) of the kafka and scala distribution kits and prepare the directory with the installation files. <br><br><pre> <code class="plaintext hljs">wget http://mirror.linux-ia64.org/apache/kafka/2.1.0/kafka_2.12-2.1.0.tgz tar -xvzf kafka_2.12-2.1.0.tgz ln -s kafka_2.12-2.1.0 kafka</code> </pre><br>  ‚Ä¢ Delete the tgz archive file, we will no longer need it. <br><br>  ‚Ä¢ Let's try to start the Zookeeper service, for this: <br><br><pre> <code class="plaintext hljs">~/kafka/bin/zookeeper-server-start.sh -daemon ~/kafka/config/zookeeper.properties</code> </pre><br>  Zookeeper starts with default startup options.  You can check the log: <br><br><pre> <code class="plaintext hljs"> tail -n 5 ~/kafka/logs/zookeeper.out</code> </pre><br>  To ensure the launch of the Zookeeper daemon, after a reboot, we need to start Zookeper as a background service: <br><br><pre> <code class="plaintext hljs">bin/zookeeper-server-start.sh -daemon config/zookeeper.properties</code> </pre><br>  To verify the launch of Zookepper we check <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181</code> </pre><br>  2.3.  We configure the Zookeeper and Kafka service for work.  Initially edit / create the file /etc/systemd/system/zookeeper.service (file contents below). <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Zookeeper server Documentation=http://zookeeper.apache.org Requires=network.target remote-fs.target After=network.target remote-fs.target [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/zookeeper-server-start.sh /home/ubuntu/kafka/config/zookeeper.properties ExecStop=/home/ubuntu/kafka/bin/zookeeper-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre><br>  Next, for Kafka, we will edit / create the file /etc/systemd/system/kafka.service (the contents of the file below). <br><br><pre> <code class="plaintext hljs">[Unit] Description=Apache Kafka server (broker) Documentation=http://kafka.apache.org/documentation.html Requires=zookeeper.service [Service] Type=simple ExecStart=/home/ubuntu/kafka/bin/kafka-server-start.sh /home/ubuntu/kafka/config/server.properties ExecStop=/home/ubuntu/kafka/bin/kafka-server-stop.sh [Install] WantedBy=multi-user.target</code> </pre><br>  ‚Ä¢ Activate systemd scripts for Kafka and Zookeeper services. <br><br><pre> <code class="plaintext hljs">sudo systemctl enable zookeeper sudo systemctl enable kafka</code> </pre><br>  ‚Ä¢ Check systemd scripts. <br><br><pre> <code class="plaintext hljs">sudo systemctl start zookeeper sudo systemctl start kafka sudo systemctl status zookeeper sudo systemctl status kafka sudo systemctl stop zookeeper sudo systemctl stop kafka</code> </pre><br>  ‚Ä¢ Check the operation of the Kafka and Zookeeper services. <br><br><pre> <code class="plaintext hljs">netcat -vz localhost 2181 netcat -vz localhost 9092</code> </pre><br>  ‚Ä¢ Check the zookeeper log file. <br><br><pre> <code class="plaintext hljs">cat logs/zookeeper.out</code> </pre><br><h3>  First joy </h3><br>  2.4.  Create your first topic on the assembled server kafka. <br><ul><li>  It is important to use the connection to "host01: 2181" as you specified in the server.properties configuration file. </li><li>  Let's write some data in the topic. </li></ul><br><pre> <code class="plaintext hljs">kafka-console-producer.sh --broker-list host01:9092 --topic first_topic    </code> </pre><br>  Ctrl-C - exit from the console of the topic. <br><br>  ‚Ä¢ Now try to read the data from the topic. <br><br><pre> <code class="plaintext hljs">kafka-console-consumer.sh --bootstrap-server host01:9092 --topic last_topic --from-beginning</code> </pre><br>  ‚Ä¢ View the list of topics kafka. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --list</code> </pre><br>  ‚Ä¢ We edit kafka server parameters for adjustment under single cluster setup <br>  # it is necessary to change the ISR parameter to 1. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper spark01:2181 --config min.insync.replicas=1 --topic __consumer_offsets --alter</code> </pre><br>  ‚Ä¢ Restart the Kafka server and try to reconnect <br><br>  ‚Ä¢ See the list of topics. <br><br><pre> <code class="plaintext hljs">bin/kafka-topics.sh --zookeeper host01:2181 --list</code> </pre><br><h3>  Configuring Apache Spark on a single-node cluster </h3><br>  We prepared an instance of the node with the Zookeeper and Kafka service installed on AWS, now you need to install Apache Spark, for this: <br><br>  3.1.  Download the latest version of the Apache Spark distribution. <br><br><pre> <code class="plaintext hljs">wget https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.6.tgz</code> </pre><br>  ‚Ä¢ Unzip the distribution and create a symbolic link for spark and delete unnecessary archive files. <br><br><pre> <code class="plaintext hljs">tar -xvf spark-2.4.0-bin-hadoop2.6.tgz ln -s spark-2.4.0-bin-hadoop2.6 spark rm spark*.tgz</code> </pre><br>  ‚Ä¢ Go to the sbin directory and run the spark wizard. <br><br><pre> <code class="plaintext hljs">./start-master.sh</code> </pre><br>  ‚Ä¢ Connect via a web browser to Spark server on port 8080. <br><br>  ‚Ä¢ Run spark-slaves on the same node <br><br><pre> <code class="plaintext hljs">./start-slave.sh spark://host01:7077</code> </pre><br>  ‚Ä¢ We start spark shell with the master on host01 node. <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre><br>  ‚Ä¢ If the launch does not work, add the path to Spark in bash. <br><br><pre> <code class="plaintext hljs">vi ~/.bashrc #      SPARK_HOME=/home/ubuntu/spark export PATH=$SPARK_HOME/bin:$PATH</code> </pre><br><pre> <code class="plaintext hljs">source ~/.bashrc</code> </pre><br>  ‚Ä¢ Run the spark shell again with a wizard on host01. <br><br><pre> <code class="plaintext hljs">./spark-shell --master spark://host01:7077</code> </pre><br>  3.2.  Single-node cluster with Kafka, Zookeeper and Spark running.  Hooray! <br><br><h3>  A bit of creativity </h3><br>  4.1.  Download the editor Scala-IDE (on the link <a href="http://scala-ide.org/">scala-ide.org</a> ).  We start and start writing code.  Here I will not repeat, since there is a <a href="https://habr.com/ru/company/piter/blog/417123/">good article on Habr√©</a> . <br><br>  4.2.  Help useful literature and courses: <br><br>  <a href="https://courses.hadoopinrealworld.com/courses/enrolled/319237">courses.hadoopinrealworld.com/courses/enrolled/319237</a> <br>  <a href="https://data-flair.training/blogs/kafka-consumer/">data-flair.training/blogs/kafka-consumer</a> <br>  <a href="https://www.udemy.com/apache-spark-with-scala-hands-on-with-big-data/">www.udemy.com/apache-spark-with-scala-hands-on-with-big-data</a> </div><p>Source: <a href="https://habr.com/ru/post/443912/">https://habr.com/ru/post/443912/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443894/index.html">The fourth level of the Max Tegmark multi-universe</a></li>
<li><a href="../443896/index.html">Technical duty like tetris</a></li>
<li><a href="../443898/index.html">Nails in the lid of the coffin runet</a></li>
<li><a href="../443906/index.html">Video analytics combines: what the brain and machines do with our faces</a></li>
<li><a href="../443910/index.html">Scratch programming concepts</a></li>
<li><a href="../443916/index.html">Ministry of Culture and filmmakers propose to create a state system of accounting for viewers on the Internet</a></li>
<li><a href="../443918/index.html">Two new minimalist pocket games</a></li>
<li><a href="../443920/index.html">Unpublished game for NES discovered 30 years later</a></li>
<li><a href="../443924/index.html">GDPR protects your personal data very well, but only if you are in Europe</a></li>
<li><a href="../443926/index.html">Cars "catamarans"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>