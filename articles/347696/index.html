<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apply CatBoost models inside ClickHouse. Yandex lecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In what situations is it convenient to apply pre-trained machine learning models inside ClickHouse? Why is CatBoost best suited for this task? Not so ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apply CatBoost models inside ClickHouse. Yandex lecture</h1><div class="post__text post__text-html js-mediator-article">  In what situations is it convenient to apply pre-trained machine learning models inside ClickHouse?  Why is <a href="https://habrahabr.ru/company/yandex/blog/333522/">CatBoost</a> best suited for this task?  Not so long ago, we had a meeting dedicated to the simultaneous use of these two open-source technologies.  Developer Nikolay Kochetov spoke at the meeting - we decided to share his lecture with you.  Nikolay analyzes the described problem on the example of the algorithm for predicting the probability of purchase. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/2VhgvdQ4BdM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - First about how ClickHouse works.  ClickHouse is an analytic distributed DBMS.  It is columnar and open source.  The most interesting word here is ‚Äúcolumnar‚Äù.  What does it mean? <a name="habracut"></a><br><img src="https://habrastorage.org/webt/j3/kl/xm/j3klxmujbr6kmjqzx8l32kpcfze.jpeg"><br>  This is how data is stored in regular databases.  Are stored line by line, files are recorded in a row.  In other words, if we need to read the data for just one value, we are forced to read the whole line.  This is due to the characteristics of the hard disk.  It is impossible to read one byte, and random reading is much more expensive than sequential.  As a result, we read a lot of unnecessary data. <br><img src="https://habrastorage.org/webt/n2/_g/fn/n2_gfne4vkexs4empicg-afoyhe.jpeg"><br>  In ClickHouse, everything is completely different.  ClickHouse stores data by columns, which means that when we have to read only one sign, one column, we read exactly one file.  This is done much faster, much like on the slide. <br><br>  Additionally, ClickHouse compresses the data, and the fact that we store data in columns gives us an advantage.  Data is more distant and better compressed. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Compression also saves us disk space and allows us to perform queries faster in many scenarios. <br><br>  How fast is ClickHouse?  We conducted speed tests in the company.  I will show better testing people from the outside. <br><br>  The graph compares ClickHouse, Spark and the Amazon Redshift database.  ClickHouse works much faster on all requests, although the databases are similar in structure, they are also columnar. <br><img src="https://habrastorage.org/webt/qc/qb/8u/qcqb8ut7a8bmfwot2pduaode6bm.jpeg"><br>  ClickHouse supports SQL syntax.  This is not fair SQL, this is a dialect.  Also, ClickHouse has the ability to use special functions: work with arrays, built-in data structures or functions specific to working with URLs.  Insertion in ClickHouse also happens quickly.  We inserted the data and immediately can work with them.  It turns out work online not only for requests, but also for inserts. <br><br>  Speaking of distributed data - ClickHouse can work on laptops, but it feels great on hundreds of servers.  For example, in Yandex. Metric. <br><br>  We turn to the second part.  We will use some auxiliary task - the prediction of the probability of purchase.  The task is quite simple.  Suppose there is data from the store.  We want to benefit from them, find out more information about users.  It would be possible to consider the probability of a purchase, something to do with it.  About what exactly to do with it, I will not tell in detail.  I'll tell you by example.  For example, you can divide users into two categories: good and not.  A good show more ads, not very good less.  And - save on advertising or get more conversion. <br><img src="https://habrastorage.org/webt/oe/8z/ll/oe8zll0cx8w3fyq64nxzqecsgmy.jpeg"><br>  Let's look at the workflow.  It is very simple and is familiar to all of you, in three stages: working with signs, collecting data and getting a sample. <br><br>  The second stage is training the model and assessing its quality. <br><br>  The last is the use of the model in production. <br><br>  Where to get the data from?  You can take from Yandex. Metrics, where data is stored inside ClickHouse.  We will also use ClickHouse.  But the data that we took, we took using the Logs API metrics.  We presented ourselves to external users and loaded data using what was available to external people. <br><br>  What data could we get?  First, the information about the pages viewed, what products saw what he visited.  This is also the state of the user's basket and what purchases he made for this. <br><br>  The latter is the device from which the user logged in, the type of browser, desktop or mobile client. <br><img src="https://habrastorage.org/webt/to/dj/6w/todj6wnvq5szusmtqqsxrvqsbyc.jpeg"><br>  The fact that we store data in ClickHouse gives us advantages.  Specifically, in that we can count almost any signs that we can think of, since ClickHouse stores data in a non-aggregated form, there is an example - calculating the average session duration. <br><img src="https://habrastorage.org/webt/xr/wk/tw/xrwktwxq510rbkc1ryz258itjwk.jpeg"><br>  The example has several features.  The first one is clipping by dates.  We used start and end_date, and ClickHouse works effectively with this, it will not read what we have not requested.  We also used sampling: we said that we would read 1/10 of the data.  ClickHouse does it efficiently, it discards unnecessary data immediately after reading the data from the disk. <br><img src="https://habrastorage.org/webt/tz/qp/64/tzqp64kihwbz6rmgouhv9fkaq4g.jpeg"><br>  Mention training models.  What classifiers did we use? <br><br>  We have a trained model, it gives out good quality.  How to implement it now? <br><br>  It would seem a simple question.  Let's do it simply: every Monday, unload the data for the previous month from ClickHouse, run Python scripts, get the probability of purchase and upload back to, say, the same ClickHouse table.  This all works well, it is simply and efficiently written, but some drawbacks arise. <br><br>  The first disadvantage is that the data must be unloaded.  It can brake, and not inside ClickHouse, but most likely inside Python scripts. <br><br>  The second drawback is that we prepare answers in advance, and if we considered the probability of a purchase from Monday to Sunday, we cannot simply find out from Wednesday to Thursday, for example, or over the past year. <br><br>  One way to solve this is to carefully look at the model.  The model is simple: addition operations, multiplications, conditional transitions.  Let's write the model in the form of ClickHouse queries, and immediately get rid of our shortcomings.  Firstly, we do not use data upload, and secondly, we can substitute any dates and everything will work. <br><br>  But the question arises: what algorithms can we transfer to the DBMS?  The first thing that comes to mind is the use of linear classifiers. <br><img src="https://habrastorage.org/webt/bc/ua/qn/bcuaqnx442w5uy5o5jkh3mrvl3w.jpeg"><br>  To use it, you need to multiply the weights vectors by the value of the signs and compare them with some threshold.  For example, I trained our logistic regression samples and obtained log loss quality of 0.041.  Then I checked how fast it works in ClickHouse, wrote a request, and it worked in less than half a second.  Not a bad result. <br><img src="https://habrastorage.org/webt/dp/nn/z3/dpnnz3xsve3ebm28bfnbi4zi5fa.jpeg"><br>  If we have something more complicated than a linear model, for example, trees, they can also be written as a ClickHouse query.  What I've done?  Took a selection of <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2580%25D0%25B8%25D1%2581%25D1%258B_%25D0%25A4%25D0%25B8%25D1%2588%25D0%25B5%25D1%2580%25D0%25B0">irises</a> .  This sample is good because you can train a small tree for it with a depth of two.  She gets an error of less than 5%.  If you are so lucky that you have a good sample, you can use a small tree. <br><img src="https://habrastorage.org/webt/aw/ua/es/awuaesrtbaze9mf-87g2i_hxb2w.jpeg"><br>  You can write a query, conditionally, in the form of two choice functions, and everything will also work well.  Ingredient boosting or wood, for example.  It is necessary to calculate the amount or average.  I also tested it for a small forest, 100 trees of depth 3, in the end I got the quality even worse, and it works in ClickHouse for 2.5 seconds. <br><br>  Not everything is so bad, if I trained more trees, the quality would be better.  But why did I not do that?  That's why. <br><img src="https://habrastorage.org/webt/p-/pw/in/p-pwinyh6ve3lxdcmt7fkafeq-0.jpeg"><br>  This is what the third part of the forest looks like on 100 trees.  I can see that this is very similar to the forest.  And if you make the request even more, then ClickHouse just starts to slow down on parsing. <br><img src="https://habrastorage.org/webt/8n/cn/6-/8ncn6-ipe6dqncxx4ct5vabyx3a.jpeg"><br>  And got additional drawbacks.  In addition to the fact that the query is very long, we have to invent it, and this can slowly work in the database itself.  For example, for trees, in order to apply them, ClickHouse is forced to read the value at each node of the tree.  And this is inefficient, because we could only calculate the values ‚Äã‚Äãon the way from the root to the leaf. <br><br>  There is a problem with performance, something can slow down and work inefficiently.  This is not what we want. <br><img src="https://habrastorage.org/webt/0h/xk/f7/0hxkf7tpgudqxn84tm2r-9mcgue.jpeg"><br>  We would like simpler things: to tell the database that you yourself will figure out how to apply the model, we just tell you that the machine learning library is here, and you just give me the modelEvaluate function, in which we put our list of signs .  Such an approach, firstly, has all the advantages of the previous one, and secondly, it shifts all the work to the database.  And all there is one drawback - your base must support the application of these models. <br><br>  In ClickHouse, we implemented it for CatBoost, of course, we tested it.  Received the best quality, despite the fact that I taught ... The training time is not very long, just 4 seconds.  This is comparatively good compared to a forest 100 of depth 3. <br><img src="https://habrastorage.org/webt/cu/4_/sn/cu4_snigz8v3w8kgwd2qqebvyq8.jpeg"><br>  What can be seen in the end? <br><img src="https://habrastorage.org/webt/db/ut/np/dbutnp0pnxmmuf9dfsat6wvs6vm.jpeg"><br>  If it is important for you to see the speed of work, the speed of burdening the algorithm, the quality is not so important, then you can use logistic regression, this will all work quickly. <br><br>  If quality is important to you, train CatBoost and use it.  How to use CatBoost in ClickHouse? <br><img src="https://habrastorage.org/webt/zr/oj/hu/zrojhucqashxzpvuddvbhy8ztk8.jpeg"><br>  We need to do some pretty simple steps.  The first is to describe the configuration of the model.  Only 4 significant lines.  Model type - now it's CatBoost, maybe sometime we'll add a new one. <br><br>  The name of the model that will be used as a parameter to the modelEvaluate () function. <br><br>  The location of the trained model on the file system. <br><br>  Update time.  Here, 0 means that we will not reread the disc material.  If it were 5, they would reread every 5 seconds. <br><img src="https://habrastorage.org/webt/fj/o4/eg/fjo4egp89w3fgt3aabw5qgbgbas.jpeg"><br>  Next you need to tell ClickHouse where the configuration file is located, this is the bottom parameter on the slide.  It uses an asterisk, which means that all files that fit this mask will be loaded. <br><br>  You also need to tell ClickHouse where the machine learning library itself is located, namely the wrapper module for CatBoost. <br><img src="https://habrastorage.org/webt/e2/zb/mg/e2zbmgnsqh_jj8vyt0cltry19kw.jpeg"><br>  Everything, when we indicated this, we get the possibility of the modelEvaluate () function, which takes the model name from the configuration file as the first argument, and our attributes for the rest.  And you must first specify the numerical characteristics, then categorical.  But do not worry if you confuse something.  First, ClickHouse will tell you that you have a mistake.  Secondly, you can test the quality and understand that everything is not as you thought. <br><img src="https://habrastorage.org/webt/qp/19/bm/qp19bmluonnynvlp43kgnphh3x4.jpeg"><br>  To make it easier to test, we also added support for reading from the CatBoost data pool.  Probably, someone worked with CatBoost from the console instead of working with it from Python or from R. Then you would have to use a special format for describing your sample.  The format is quite simple - just two files.  The first one is TSV with three columns, the sign number and its type: categorical, numerical or target.  Plus an optional optional name parameter.  The second file is the data itself, also TSV. <br><img src="https://habrastorage.org/webt/lt/06/mv/lt06mvapblr3c7k4qinnviem7ds.jpeg"><br>  Just in ClickHouse, we added a table function atBoostPool, which takes these two files as arguments, and returns a special table of the file type, temporary, from which data can be read. <br><img src="https://habrastorage.org/webt/ml/r6/4t/mlr64tvht-uiiaoacs9hy5pmfss.png"><br>  I created a small CatBoost Pool of three values: two signs and a target.  It contains five lines.  Two are signs, one is a target, two more are additionally ALIAS. <br><br>  Also, the numeric and categorical features have been swapped.  This is done for convenience so that you can submit the parameters to the playModule function in the correct order. <br><img src="https://habrastorage.org/webt/jm/4t/94/jm4t947dovwtuu2fzkgmfuxbrf0.jpeg" width="700"><br>  If we make describe what gives the SELECT * query, then the values ‚Äã‚Äãbecome less, only two signs remain in the right order.  This is done again so that you can conveniently use the CatBoost Pool, namely, passing an asterisk as the remaining arguments to the moduleEvaluate function. <br><img src="https://habrastorage.org/webt/oh/u5/vk/ohu5vkjxnfymnm9oz3yq8tricwy.png" width="700"><br>  Let's try to apply something.  To begin with, we calculate the probability of a purchase.  What will we use?  First of all, we will consider from the CatBoost Pool, use the moduleEvaluate function and pass the asterisk to the second parameter. <br><br>  The last line, the third, the application of sigmoid. <br><img src="https://habrastorage.org/webt/gr/bb/49/grbb49t-ifcj8dial2t-oce5prw.png" width="700"><br>  Let's complicate the query, consider the quality.  Let's write the previous query as a ClickHouse subquery.  In total, we added two significant lines, counting the logloss metrics.  Calculated the average, got the value from the table. <br><br>  I was interested in how quickly such a query works.  I spent three tests and got this schedule. <br><img src="https://habrastorage.org/webt/y1/74/cg/y174cg7rwrkfaboq8eehxetwfii.jpeg"><br>  On the left - reading from the table, on the right - from the pool.  It can be seen that the columns are almost identical, there is almost no difference, but reading from the ClickHouse table works a little faster - apparently because ClickHouse compresses the data, and we don‚Äôt spend too much time reading from the file.  And CatBoost Pool does not compress, so we spend time reading and transposing signs.  It turns out a little faster, but still it is convenient to use CatBoost Pool for tests. <br><br>  Let's sum up.  The integration is that we can apply trained models, use the CatBoost Pool and read data from it, which is quite convenient.  The plans - add the use of other models.  Also, the task of training within the database is in question, but so far we have not decided whether to do it.  If there are a lot of requests, we will seriously consider this. <br><br>  We have <a href="https://telegram.me/clickhouse_ru">Russian</a> and <a href="https://telegram.me/clickhouse_en">English</a> groups in Telegram, where we actively respond, there is a <a href="https://github.com/yandex/ClickHouse/">GitHub</a> <a href="">mailing list</a> .  If you have questions, you can send questions to <a href="">me in the mail</a> , <a href="https://t.me/kochetovnicolai">in the Telegram</a> or in the <a href="https://groups.google.com/group/clickhouse">Google group</a> .  Thank! </div><p>Source: <a href="https://habr.com/ru/post/347696/">https://habr.com/ru/post/347696/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347684/index.html">Telephony for hotels: review of the hotel module PBX 3CX</a></li>
<li><a href="../347686/index.html">The theory of "broken" warnings</a></li>
<li><a href="../347688/index.html">Performance comparison of C and C ++ using the example of Huffman compression</a></li>
<li><a href="../347692/index.html">The digest of interesting materials for the mobile developer # 238 (January 22 - January 28)</a></li>
<li><a href="../347694/index.html">What indie developers can learn from indie writers</a></li>
<li><a href="../347698/index.html">Programmer Unknown's BattleGround: an open area for programmer battles</a></li>
<li><a href="../347700/index.html">Taming SphinxSearch with an Elephant</a></li>
<li><a href="../347702/index.html">Penetration Test with Metasploit Framework: A Basic Guide for the System Administrator</a></li>
<li><a href="../347704/index.html">Is it possible to prepare for the CCIE for the year. Project results</a></li>
<li><a href="../347708/index.html">Facebook or Telegram? History of the Ukrainian .NET Core Community</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>