<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Virtual Private Cloud: Work with CoreOS and RancherOS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We recently added a new image with the RancherOS operating system to the Virtual Private Cloud service and updated the CoreOS image. 


 These operati...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Virtual Private Cloud: Work with CoreOS and RancherOS</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/58a/61c/c94/58a61cc94dba17ed0c95c76590202ce2.png" alt="CoreOS" width="100%" height="100%"><br><br><p>  We recently added a new image with the <a href="http://rancher.com/rancher-os/" rel="nofollow">RancherOS</a> operating system to the <a href="https://selectel.ru/services/vpc/" rel="nofollow">Virtual Private Cloud</a> <a href="http://rancher.com/rancher-os/" rel="nofollow">service</a> and updated the <a href="https://coreos.com/" rel="nofollow">CoreOS</a> image. </p><br><p>  These operating systems will be of interest to users who need a tool for simple management of a large number of applications in containers and the use of various container clustering systems - <a href="http://kubernetes.io/" rel="nofollow">Kubernetes</a> , <a href="https://docker.github.io/swarm/overview/" rel="nofollow">Docker Swarm</a> , <a href="http://mesos.apache.org/" rel="nofollow">Apache Mesos,</a> and others. </p><br><a name="habracut"></a><p>  CoreOS and RancherOS differ from most popular Linux distributions: they contain only a minimal set of software for running applications inside containers.  These systems lack package managers and interpreters of various programming languages ‚Äã‚Äã(Python, Ruby, and others.). </p><br><p>  Below we look at the specifics of CoreOS and RancherOS, analyze the configuration features of the etcd and fleet services, and also give an example of setting up a Docker Swarm cluster based on RancherOS. </p><br><h3>  CoreOS </h3><br><p>  The <a href="https://coreos.com/" rel="nofollow">CoreOS</a> distribution <a href="https://coreos.com/" rel="nofollow">is</a> designed specifically for the rapid and mass deployment of a large number of user applications in isolated containers. </p><br><p>  CoreOS contains pre-installed tools for conveniently managing multiple nodes and containers in a cluster: </p><br><ul><li>  <a href="https://coreos.com/etcd/" rel="nofollow">etcd</a> - distributed data storage, which is used by nodes to get information about available services in a cluster, configuration and temporary files; </li><li>  <a href="https://www.docker.com/" rel="nofollow">Docker</a> - the main platform for delivering and running applications inside containers; </li><li>  <a href="https://coreos.com/rkt/" rel="nofollow">rkt</a> - alternative Docker, developed by the creators of CoreOS; </li><li>  <a href="https://coreos.com/fleet/" rel="nofollow">fleet</a> is a distributed initialization system that allows you to manage multiple <a href="https://www.freedesktop.org/wiki/Software/systemd/" rel="nofollow">systemd</a> instances on individual nodes of the cluster and launch the necessary services on less loaded nodes. </li></ul><br><h4>  Configuration </h4><br><p>  The main method of changing system parameters is a description of the required settings using the declarative <a href="http://yaml.org/" rel="nofollow">YAML</a> language in the cloud-config.yml configuration file.  This file is used by the <a href="https://github.com/coreos/coreos-cloudinit" rel="nofollow">Cloud-init</a> utility to configure the system. </p><br><p>  Please note that CoreOS uses its own implementation of Cloud-init, whose configuration files are only partially compatible with the settings for the more common version of <a href="https://launchpad.net/cloud-init" rel="nofollow">Launchpad-cloud-init</a> used in most common distributions for system configuration in various cloud services, including the service. "Virtual Private Cloud". </p><br><p>  Here is an example of the cloud-config.yml file: </p><br><pre><code class="hljs vbscript">#cloud-config # <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span> hostname hostname: <span class="hljs-string"><span class="hljs-string">"node01"</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span> ntp servers write_files: - path: /etc/systemd/timesyncd.conf content: | [<span class="hljs-built_in"><span class="hljs-built_in">Time</span></span>] NTP=<span class="hljs-number"><span class="hljs-number">0.</span></span>ru.pool.ntp.org <span class="hljs-number"><span class="hljs-number">1.</span></span>ru.pool.ntp.org coreos: units: # Configure static network <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> eth0 interface - name: iface-eth0.network runtime: <span class="hljs-literal"><span class="hljs-literal">true</span></span> content: | [Match] Name=eth0 [Network] DNS=<span class="hljs-number"><span class="hljs-number">188.93</span></span><span class="hljs-number"><span class="hljs-number">.16</span></span><span class="hljs-number"><span class="hljs-number">.19</span></span> DNS=<span class="hljs-number"><span class="hljs-number">188.93</span></span><span class="hljs-number"><span class="hljs-number">.17</span></span><span class="hljs-number"><span class="hljs-number">.19</span></span> DNS=<span class="hljs-number"><span class="hljs-number">109.234</span></span><span class="hljs-number"><span class="hljs-number">.159</span></span><span class="hljs-number"><span class="hljs-number">.91</span></span> Address=<span class="hljs-number"><span class="hljs-number">10.11</span></span><span class="hljs-number"><span class="hljs-number">.12</span></span><span class="hljs-number"><span class="hljs-number">.13</span></span>/<span class="hljs-number"><span class="hljs-number">24</span></span> Gateway=<span class="hljs-number"><span class="hljs-number">10.11</span></span><span class="hljs-number"><span class="hljs-number">.12</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> # Change standard SSH port - name: sshd.socket command: restart runtime: <span class="hljs-literal"><span class="hljs-literal">true</span></span> content: | [Socket] ListenStream=<span class="hljs-number"><span class="hljs-number">2345</span></span> FreeBind=<span class="hljs-literal"><span class="hljs-literal">true</span></span> Accept=yes</code> </pre> <br><p>  Using the specified parameters, the Cloud-init utility will change the hostname and standard port 22 for the SSH service, configure the static network on the eth0 interface, and add ntp servers for time synchronization. </p><br><p>  More information about the structure of the configuration file cloud-config.yml can be found in the <a href="https://coreos.com/os/docs/latest/" rel="nofollow">official documentation</a> . </p><br><p>  When creating a server from a ready CoreOS image in the Virtual Private Cloud service, you will not need to perform basic system configuration using cloud-config.yml.  We have already made the necessary changes to the image, with the help of which the standard server configuration is automatically performed, including the configuration of network interfaces. </p><br><p>  Access via SSH to the server running CoreOS will initially be possible only by the key specified when creating the server.  The specified key will be added for the <strong>core</strong> user. </p><br><p>  In addition to the native version of cloud-init, the creators of CoreOS have developed the <a href="https://coreos.com/blog/introducing-ignition.html" rel="nofollow">Ignition</a> utility.  This tool duplicates the capabilities of cloud-init and also allows for low-level system settings, for example, changing disk partition tables and formatting file systems.  This is possible due to the fact that Ignition starts working during the early stages of system boot, during the <a href="https://en.wikipedia.org/wiki/Initramfs" rel="nofollow">initram</a> process. </p><br><p>  Ignition uses <a href="http://www.json.org/" rel="nofollow">JSON</a> format for configuration files. </p><br><p>  Below is an example of a file with which Ignition will format the root partition in <a href="https://btrfs.wiki.kernel.org/index.php/Main_Page" rel="nofollow">Btrfs</a> , and configure the system with parameters similar to the example above for Cloud-init: </p><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"ignition"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"2.0.0"</span></span> }, <span class="hljs-attr"><span class="hljs-attr">"storage"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"filesystems"</span></span>: [{ <span class="hljs-attr"><span class="hljs-attr">"mount"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"device"</span></span>: <span class="hljs-string"><span class="hljs-string">"/dev/disk/by-label/ROOT"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"format"</span></span>: <span class="hljs-string"><span class="hljs-string">"btrfs"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"create"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"force"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"options"</span></span>: [ <span class="hljs-string"><span class="hljs-string">"--label=ROOT"</span></span> ] } } }], <span class="hljs-attr"><span class="hljs-attr">"files"</span></span>: [{ <span class="hljs-attr"><span class="hljs-attr">"filesystem"</span></span>: <span class="hljs-string"><span class="hljs-string">"root"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"path"</span></span>: <span class="hljs-string"><span class="hljs-string">"/etc/systemd/timesyncd.conf"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"mode"</span></span>: <span class="hljs-number"><span class="hljs-number">420</span></span>, <span class="hljs-attr"><span class="hljs-attr">"contents"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"source"</span></span>: <span class="hljs-string"><span class="hljs-string">"data:,%5BTime%5D%0ANTP=0.ru.pool.ntp.org%201.ru.pool.ntp.org%0A"</span></span> } }, { <span class="hljs-attr"><span class="hljs-attr">"filesystem"</span></span>: <span class="hljs-string"><span class="hljs-string">"root"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"path"</span></span>: <span class="hljs-string"><span class="hljs-string">"/etc/hostname"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"mode"</span></span>: <span class="hljs-number"><span class="hljs-number">420</span></span>, <span class="hljs-attr"><span class="hljs-attr">"contents"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"source"</span></span>: <span class="hljs-string"><span class="hljs-string">"data:,node01"</span></span> } } ] }, <span class="hljs-attr"><span class="hljs-attr">"networkd"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"units"</span></span>: [{ <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"iface-eth0.network"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"contents"</span></span>: <span class="hljs-string"><span class="hljs-string">"[Match]\nName=eth0\n\n[Network]\nDNS=188.93.16.19\nDNS=188.93.17.19\nDNS=109.234.159.91\nAddress=10.11.12.13/24\nGateway=10.11.12.1"</span></span> }] }, <span class="hljs-attr"><span class="hljs-attr">"systemd"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"units"</span></span>: [{ <span class="hljs-attr"><span class="hljs-attr">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"sshd.socket"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"command"</span></span>: <span class="hljs-string"><span class="hljs-string">"restart"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"runtime"</span></span>: <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-attr"><span class="hljs-attr">"contents"</span></span>: <span class="hljs-string"><span class="hljs-string">"[Socket]\nListenStream=2345\nFreeBind=true\nAccept=yes"</span></span> }] } }</code> </pre><br><p>  More information about all the features of Ignition can be found in the <a href="https://coreos.com/ignition/docs/latest/" rel="nofollow">official documentation</a> . </p><br><h4>  CoreOS Cluster Configuration Example </h4><br><p>  For this example, we will need to pre-create three servers running CoreOS in <a href="https://support.selectel.ru/vpc/projects/" rel="nofollow">the VPC control panel</a> with the following parameters: </p><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c23/cf8/065/c23cf8065bc036ae7cb7753ca9812c5e.png" alt="CoreOS" width="100%" height="100%">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  After creating the servers, you need a <a href="https://coreos.com/etcd/docs/latest/discovery_protocol.html" rel="nofollow">discovery url</a> for the etcd service.  To do this, we can use the public free service <a href="https://discovery.etcd.io/" rel="nofollow">discovery.etcd.io</a> .  Run the following command on the <strong>node00</strong> host: </p><br><pre> <code class="bash hljs">core@node00 ~ $ curl -w <span class="hljs-string"><span class="hljs-string">"\n"</span></span> <span class="hljs-string"><span class="hljs-string">'https://discovery.etcd.io/new?size=3'</span></span> https://discovery.etcd.io/ec42cfef0450bd8a99090ee3d3294493</code> </pre><br><p>  Add the resulting URL and additional parameters etcd to the file <strong>/usr/share/oem/cloud-config.yml</strong> on each server: </p><br><pre> <code class="hljs pgsql">coreos: etcd2: discovery: https://discovery.etcd.io/ec42cfef0450bd8a99090ee3d3294493 advertise-client-urls: http://<span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.10</span></span>:<span class="hljs-number"><span class="hljs-number">2379</span></span>,http://<span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.10</span></span>:<span class="hljs-number"><span class="hljs-number">4001</span></span> initial-advertise-peer-urls: http://<span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.10</span></span>:<span class="hljs-number"><span class="hljs-number">2380</span></span> <span class="hljs-keyword"><span class="hljs-keyword">listen</span></span>-client-urls: http://<span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>:<span class="hljs-number"><span class="hljs-number">2379</span></span>,http://<span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>:<span class="hljs-number"><span class="hljs-number">4001</span></span> <span class="hljs-keyword"><span class="hljs-keyword">listen</span></span>-peer-urls: http://<span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.10</span></span>:<span class="hljs-number"><span class="hljs-number">2380</span></span> units: - <span class="hljs-type"><span class="hljs-type">name</span></span>: etcd2.service command: <span class="hljs-keyword"><span class="hljs-keyword">start</span></span> - <span class="hljs-type"><span class="hljs-type">name</span></span>: fleet.service command: <span class="hljs-keyword"><span class="hljs-keyword">start</span></span></code> </pre><br><p>  The parameters in the etcd2 section indicate the URLs in the <strong>protocol: // address: port</strong> format, which are required to announce the node and exchange data with other cluster members. </p><br><p>  The units section allows you to manage systemd units in the system;  in our case, it will launch the <strong>etc2d.service</strong> and <strong>fleet.service units</strong> . </p><p></p><p>  For <strong>node01</strong> and <strong>node02 servers,</strong> you will need to add similar changes to <strong>cloud-config.yml</strong> , changing the ip address from <strong>192.168.0.10</strong> to <strong>192.168.0.11</strong> for the <strong>node01</strong> server, and to the address <strong>192.168.0.12</strong> for the <strong>node02</strong> server. <br><br></p><p>  After adding new settings, run the Cloud-init utility on all three servers (or you can simply restart these servers): </p><br><pre> <code class="bash hljs">core@node00 ~ $ sudo coreos-cloudinit --from-file /usr/share/oem/cloud-config.yml</code> </pre><br><p>  Also check the status of the etcd service: </p><br><pre> <code class="bash hljs">core@node00 ~ $ etcdctl cluster-health member 3aaf91fd1172594e is healthy: got healthy result from http://192.168.0.11:2379 member 8cf40790248e1dcd is healthy: got healthy result from http://192.168.0.12:2379 member 96b61f40c082cd0b is healthy: got healthy result from http://192.168.0.10:2379 cluster is healthy</code> </pre><br><p>  Check the availability of the distributed storage, add a new directory (in the etcd terminology, the key) with the value on the <strong>node00</strong> server: </p><br><pre> <code class="bash hljs">core@node00 ~ $ etcdctl <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> /common/cluster-name selectel-test-cluster selectel-test-cluster</code> </pre><br><p>  Then make sure that our new key with value is available from any other node in the cluster: </p><br><pre> <code class="bash hljs">core@node01 ~ $ etcdctl get common/cluster-name selectel-test-cluster</code> </pre><br><p>  Great, etcd is functioning and we can start using the fleet service. </p><br><p>  In our example, we will use fleet to run Nginx in an isolated Docker container.  First, create a systemd-unit <strong>/etc/systemd/system/nginx.service</strong> : </p><br><pre> <code class="bash hljs">[Unit] Description=Nginx Test Service [Service] EnvironmentFile=/etc/environment ExecStartPre=/usr/bin/docker pull nginx ExecStart=/usr/bin/docker run --rm --name nginx -p 80:80 nginx ExecStop=/usr/bin/docker <span class="hljs-built_in"><span class="hljs-built_in">kill</span></span> nginx [X-Fleet] Conflicts=nginx.service</code> </pre><br><p>  The <strong>Conflicts</strong> parameter in the <strong>X-Fleet</strong> section prohibits running two or more Nginx services on the same node, which will ‚Äúspread out‚Äù the load on the cluster and increase the availability of the service.  Additional X-Fleet parameters that can be used inside systemd units are described in the <a href="https://coreos.com/fleet/docs/latest/unit-files-and-scheduling.html" rel="nofollow">fleet documentation</a> . </p><br><p>  The remaining sections in the file are standard for systemd, you can read them for example in a small introductory <a href="https://coreos.com/docs/launching-containers/launching/getting-started-with-systemd/" rel="nofollow">instructions</a> from CoreOS. </p><br><p>  Run the new <strong>nginx.service</strong> unit in our cluster using <strong>fleetctl</strong> : </p><br><pre> <code class="bash hljs">core@node00 ~ $ fleetctl start /etc/systemd/system/nginx.service Unit nginx.service inactive Unit nginx.service launched on 1ad018e0.../192.168.0.11</code> </pre><br><p>  After that we can see the presence of Nginx in the list of units on any node of the cluster: </p><br><pre> <code class="bash hljs">core@node02 ~ $ fleetctl list-units UNIT MACHINE ACTIVE SUB nginx.service 1ad018e0.../192.168.0.11 active running</code> </pre><br><pre> <code class="bash hljs">core@node02 ~ $ fleetctl list-unit-files UNIT HASH DSTATE STATE TARGET nginx.service 0c112c1 launched launched 1ad018e0.../192.168.0.11</code> </pre><br><p>  As we see, the container with Nginx was launched on the server with the IP address <strong>192.168.0.11</strong> , which corresponds to the <strong>node01</strong> server. <br><br></p><p>  We can disable or completely remove the <strong>node01</strong> server, after which fleet will automatically transfer Nginx to another available node. </p><br><p>  Etcd will report member unavailability when checking cluster status: </p><br><pre> <code class="bash hljs">core@node00 ~ $ etcdctl cluster-health failed to check the health of member 3aaf91fd1172594e on http://192.168.0.11:2379: Get http://192.168.0.11:2379/health: dial tcp 192.168.0.11:2379: i/o timeout failed to check the health of member 3aaf91fd1172594e on http://192.168.0.11:4001: Get http://192.168.0.11:4001/health: dial tcp 192.168.0.11:4001: i/o timeout member 3aaf91fd1172594e is unreachable: [http://192.168.0.11:2379 http://192.168.0.11:4001] are all unreachable member 8cf40790248e1dcd is healthy: got healthy result from http://192.168.0.12:2379 member 96b61f40c082cd0b is healthy: got healthy result from http://192.168.0.10:2379 cluster is healthy</code> </pre><br><p>  But as we can see, the status of the Nginx service is normal, it started on a different working node: </p><br><pre> <code class="bash hljs">core@node00 ~ $ fleetctl list-units UNIT MACHINE ACTIVE SUB nginx.service 4a1ff11c.../192.168.0.10 active running</code> </pre><br><p>  We have analyzed an elementary example of a cluster configuration of etcd + Docker + fleet based on CoreOS.  In the next section, we will look at another distro, RancherOS, which is based on CoreOS and is also intended for active use of containers. </p><br><h3>  RancherOS </h3><br><p>  The RancherOS distribution is a CoreOS fork.  A distinctive feature of this system is that not only user applications, but also all system services are launched in containers.  Moreover, in RancherOS Docker has a <strong>PID = 1</strong> , so it starts immediately after the system kernel. </p><br><p>  The operating system contains two Docker instances;  one of them is a system one, it runs udev, acpid, syslog, ntp and other basic services necessary for the system to work.  The system Docker-instance replaces the traditional system initialization (systemd, Upstart, SysV), present in the usual distributions of Linux. <br><br></p><p>  The second Docker instance is used to launch user applications and is a special container running inside the system Docker. <br><br></p><p>  This separation protects the system containers from incorrect user actions. </p><br><br><img src="https://habrastorage.org/getpro/habr/post_images/167/c9b/ef8/167c9bef899af364bb17ff86516badda.png" alt="RancherOS" width="100%" height="100%"><br><br><p>  Since the Docker-instance in RancherOS is a regular Docker environment, we can use the <a href="https://docs.docker.com/engine/reference/commandline/" rel="nofollow">standard</a> Docker <a href="https://docs.docker.com/engine/reference/commandline/" rel="nofollow">command set</a> to manage both user and system containers. </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   Docker [rancher@rancher-os ~]$ docker --version Docker version 1.12.1, build 23cf638</span></span></code> </pre><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#     Docker Hub [rancher@rancher-os ~]$ docker run -d nginx Unable to find image 'nginx:latest' locally latest: Pulling from library/nginx ‚Ä¶ Status: Downloaded newer image for nginx:latest</span></span></code> </pre><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#     [rancher@rancher-os ~]$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 2ec5126e8dd5 nginx "nginx -g 'daemon off" About a minute ago Up About a minute 80/tcp, 443/tcp sad_fermat</span></span></code> </pre><br><p>  To access system containers, you must use the <strong>system-docker command</strong> along with the <a href="https://www.sudo.ws/" rel="nofollow">sudo</a> utility. </p><br><p>  You can view the list of running basic services of the operating system using the command: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo system-docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a566d0c7cc4c rancher/os-docker:1.12.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/user-docker"</span></span> 8 minutes ago Up 8 minutes docker 8697a04e90a4 rancher/os-console:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes console c8c337282aaa rancher/os-base:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes network 1e55244fc99c rancher/os-base:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes ntp fd6a03fdb28f rancher/os-udev:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes udev 7e9e50c28651 rancher/os-acpid:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes acpid 5fec48b060f2 rancher/os-syslog:v0.6.1 <span class="hljs-string"><span class="hljs-string">"/usr/bin/ros entrypo"</span></span> 8 minutes ago Up 8 minutes syslog</code> </pre><br><p>  Restarting the system container is as follows: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo system-docker restart ntp</code> </pre><br><h4>  Configuration </h4><br><p>  The main tool for automatic configuration of the system is its own implementation of cloud-init, which was divided into two separate utilities: <a href="https://github.com/rancher/os/tree/master/cmd/cloudinitexecute" rel="nofollow">cloud-init-execute</a> and <a href="https://github.com/rancher/os/tree/master/cmd/cloudinitsave" rel="nofollow">cloud-init-save</a> .  By analogy with other implementations of cloud-init, the version of RancherOS uses declarative YAML, but cloud-config.yml for another distribution will not be compatible with RancherOS. </p><br><p>  All possible configuration file directives are described in the official documentation <a href="http://docs.rancher.com/os" rel="nofollow">docs.rancher.com/os</a> . </p><br><p>  System services use the parameters obtained from the files in the sequence described below (each subsequent file from the specified list overwrites the previous parameters if they match): </p><br><ul><li>  <strong>/usr/share/ros/os-config.yml</strong> ‚Äî default system settings; </li><li>  <strong>/usr/share/ros/oem/oem-config.yml</strong> - in our case, this file allows automatic configuration of static network settings, in accordance with the parameters from the VPC control panel; </li><li>  YAML files in the <strong>/var/lib/rancher/conf/cloud-config.d/</strong> directory; </li><li>  <strong>/var/lib/rancher/conf/cloud-config.yml</strong> is a file that stores the values ‚Äã‚Äãset by the ros utility, which we will discuss next; </li><li>  kernel parameters starting with the keyword "rancher"; </li><li>  <strong>/ var / lib / rancher / conf / metadata</strong> - metadata from the cloud service used, added by the cloud-init-save utility. </li></ul><br><p>  You can change the system configuration using the ros utility, after which the new parameters will be displayed in the <strong>/var/lib/rancher/conf/cloud-config.yml</strong> file: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo ros config <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> rancher.network.dns.nameservers <span class="hljs-string"><span class="hljs-string">"['188.93.16.19','188.93.17.19', '109.234.159.91']"</span></span></code> </pre><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo cat /var/lib/rancher/conf/cloud-config.yml rancher: network: dns: nameservers: - 188.93.16.19 - 188.93.17.19 - 109.234.159.91</code> </pre><br><p>  In addition to changing the configuration of system services, the ros utility is also used to manage operating system versions, Docker versions, TLS and SELinux settings. </p><br><p>  When using a ready-made image of RancherOS in the Virtual Private Cloud service, you do not need to configure the basic system parameters (network interface configuration, etc.) </p><br><p>  After installation, the server will be available at your chosen IP address in the VPC control panel.  You can connect to the server using the SSH protocol, however, authentication will initially only be possible using the key specified when creating the server.  This key will be added to the <strong>rancher</strong> user. </p><br><h4>  Version control </h4><br><p>  To find out the current version of the operating system, use the command: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo ros os version v0.6.1</code> </pre><br><p>  And to check all available releases: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo ros os list rancher/os:v0.4.0 remote rancher/os:v0.4.1 remote rancher/os:v0.4.2 remote rancher/os:v0.4.3 remote rancher/os:v0.4.4 remote rancher/os:v0.4.5 remote rancher/os:v0.5.0 remote rancher/os:v0.6.0 remote rancher/os:v0.6.1 remote</code> </pre><br><p>  You can install the latest stable version of the system using the sudo ros os upgrade command, or select the required version by specifying the <strong>-i</strong> parameter: </p><br><pre> <code class="bash hljs">[rancher@rancher-os ~]$ sudo ros os upgrade -i rancher/os:v0.5.0</code> </pre><br><p>  Since the version 0.5.0 indicated by us is older than the installed 0.6.1, instead of updating the system, a downgrade will occur </p><br><p>  Managing the Docker version is just as easy.  Here are some examples: </p><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   [rancher@rancher-os ~]$ sudo ros engine list disabled docker-1.10.3 disabled docker-1.11.2 current docker-1.12.1</span></span></code> </pre><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  1.11.2 [rancher@rancher-os ~]$ sudo ros engine switch docker-1.11.2 INFO[0000] Project [os]: Starting project INFO[0000] [0/19] [docker]: Starting ‚Ä¶ INFO[0010] Recreating docker INFO[0010] [1/19] [docker]: Started</span></span></code> </pre><br><h4>  Cluster Docker Swarm on RancherOS </h4><br><p>  In Docker 1.12, integrated tools for managing a cluster of Docker Swarm containers appeared, making it much easier to set up a ready-made cluster. </p><br><p>  In our example, we will use a ready-made image of RancherOS in the Virtual Private Cloud service.  It uses Docker version 1.12.1. <br></p><p>  We will need to create three servers with the following parameters: </p><br><br><img src="https://habrastorage.org/getpro/habr/post_images/8c0/64d/dd8/8c064ddd8f88d914728beda1ed862554.png" alt="pr-3225-02" width="100%" height="100%"><br><br><p>  First we need to initialize the creation of the Docker Swarm cluster, for this we <strong>will</strong> execute the following command on the manager0 <strong>node</strong> : </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker swarm init --advertise-addr 192.168.0.100 Swarm initialized: current node (8gf95qb6l61v5s6561qx5vfy6) is now a manager. To add a worker to this swarm, run the following <span class="hljs-built_in"><span class="hljs-built_in">command</span></span>: docker swarm join \ --token SWMTKN-1-0hlhela57gsvxzoaqol70n2b9wos6qlu3ukriry3pcxyb9j2k6-0n1if4hkvdvmrpbb7f3clx1yg \ 192.168.0.100:2377 To add a manager to this swarm, run <span class="hljs-string"><span class="hljs-string">'docker swarm join-token manager'</span></span> and follow the instructions.</code> </pre> <br><p>  As you can see from the hint that appears, on the <strong>worker0</strong> and <strong>worker1 nodes</strong> you need to execute the command: </p><br><pre> <code class="bash hljs">docker swarm join \ --token SWMTKN-1-0hlhela57gsvxzoaqol70n2b9wos6qlu3ukriry3pcxyb9j2k6-0n1if4hkvdvmrpbb7f3clx1yg \ 192.168.0.100:2377</code> </pre><br><p>  Check the status of the cluster nodes. To do this, run the command on <strong>manager0</strong> : </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS 366euocei01dw3ay66uwzb3pv worker0 Ready Active 67pt4fo5gi13cxkphtfzfbfbo worker1 Ready Active 8gf95qb6l61v5s6561qx5vfy6 * manager0 Ready Active Leader</code> </pre><br><p>  Docker Swarm is ready to use! </p><br><p>  By default, new services will be launched on all nodes of the cluster.  We will adjust these settings so that all services are started on the <strong>worker0</strong> and <strong>worker1 nodes</strong> , and the <strong>manager0</strong> node <strong>fulfills</strong> only the management role. </p><br><p>  To make these settings, change the ‚Äúavailability‚Äù of <strong>manager0</strong> node: </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker node update --availability drain manager0</code> </pre><br><p>  Let's start the service with the Nginx container: </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service create --name webserver --replicas 1 --publish 80:80 nginx</code> </pre><br><p>  In response, we get the service ID: </p><br><pre> <code class="bash hljs">av1qvj32mz8vwkwihf0lauiz8</code> </pre><br><p>  Initially, the number of replicas will be 0, and the only task related to this service will be in ‚ÄúPreparing‚Äù status: </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ls ID NAME REPLICAS IMAGE COMMAND av1qvj32mz8v webserver 0/1 nginx</code> </pre><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ps webserver ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR cjxkza3qzx5m73bgbls0r26m6 webserver.1 nginx worker1 Running Preparing about a minute ago</code> </pre><br><p>  After a few seconds, the required Nginx image will be automatically downloaded from the Docker Hub, after which the service status will change to ‚ÄúRunning‚Äù: </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ls ID NAME REPLICAS IMAGE COMMAND av1qvj32mz8v webserver 1/1 nginx</code> </pre><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ps webserver ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR cjxkza3qzx5m73bgbls0r26m6 webserver.1 nginx worker1 Running Running 8 minutes ago</code> </pre><br><p>  As we can see, the service started on the worker1 <strong>node</strong> (the value in the ‚ÄúNODE‚Äù column).  Disable this node through the VPC control panel and check the status of the <strong>webserver</strong> service again: </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ps webserver ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 5lwf4z54p74qtoopuilshw204 webserver.1 nginx worker0 Running Preparing 32 seconds ago cjxkza3qzx5m73bgbls0r26m6 \_ webserver.1 nginx worker1 Shutdown Running 16 minutes ago</code> </pre><br><p>  The status of the service has changed to ‚ÄúPreparing‚Äù, Docker Swarm is trying to transfer the service to another <strong>worker0</strong> worker node, the delay is due to the fact that before running on the <strong>worker0</strong> node an Nginx image from the Docker Hub must be added. </p><br><p>  After the image is automatically added, the webserver service will start successfully on <strong>worker0</strong> : </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ps webserver ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 5lwf4z54p74qtoopuilshw204 webserver.1 nginx worker0 Running Running about a minute ago cjxkza3qzx5m73bgbls0r26m6 \_ webserver.1 nginx worker1 Shutdown Running 19 minutes ago</code> </pre><br><p>  To avoid downtime when transferring a service from a fallen node, we can increase the number of replicas of this service. </p><br><p>  Run the command (don't forget to turn <strong>worker1</strong> back): </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service update --replicas 2 webserver</code> </pre><br><p>  The service will start replicating to the second node <strong>worker1</strong> : </p><br><pre> <code class="bash hljs">[rancher@manager0 ~]$ docker service ps webserver ID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR 5lwf4z54p74qtoopuilshw204 webserver.1 nginx worker0 Running Running 11 minutes ago cjxkza3qzx5m73bgbls0r26m6 \_ webserver.1 nginx worker1 Shutdown Complete 6 minutes ago f3o7icke69zw83ag92ywol0dr webserver.2 nginx worker1 Running Preparing 2 seconds ago</code> </pre><br><h3>  Conclusion </h3><br><p>  In this article we have done a review of Linux distributions designed for quick deployment and convenient management of a cluster of isolated containers. </p><br><p>  We will be glad if you share your own experience of using CoreOS, RancherOS and similar operating systems. </p><p></p><p></p><p></p><p></p><p></p><p></p><p></p></div><p>Source: <a href="https://habr.com/ru/post/315930/">https://habr.com/ru/post/315930/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../315916/index.html">You, Inc. How to develop personal and professional skills, sell them and stand out from the crowd</a></li>
<li><a href="../315918/index.html">How an entrepreneur destroyed a mediocre business to create a great company</a></li>
<li><a href="../315924/index.html">Biological background of the degradation of companies</a></li>
<li><a href="../315926/index.html">Fedora 25. New Hope: Wayland, Storaged, Raspberry Pi Support ...</a></li>
<li><a href="../315928/index.html">Welcome to Unity Moscow Meetup December 2</a></li>
<li><a href="../315934/index.html">How IT professionals work. Alexey Vladyshev, author of Zabbix</a></li>
<li><a href="../315936/index.html">UI and UX dichotomy: UI designer –¥–∏–∑–∞–π–Ω–µ—Ä UX designer</a></li>
<li><a href="../315938/index.html">Features of multi-window mode on Android tablets</a></li>
<li><a href="../315940/index.html">A convenient browser for accessing the I2P network is available.</a></li>
<li><a href="../315942/index.html">Javascript animation of elements as in jQuery, only with your own hands</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>