<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine Learning Algorithm Brief Review of the Support Vectors Vectors (SVM)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Foreword 


 In this article we will explore several aspects of SVM: 



- theoretical component of SVM; 
- how the algorithm works on samples that ca...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine Learning Algorithm Brief Review of the Support Vectors Vectors (SVM)</h1><div class="post__text post__text-html js-mediator-article"><h2>  Foreword </h2><br><img src="https://habrastorage.org/webt/eg/fk/rq/egfkrqshjcqqzkc7ktnnyyx9iuy.jpeg"><br><br>  In this article we will explore several aspects of SVM: <br><br><ul><li>  theoretical component of SVM; </li><li>  how the algorithm works on samples that cannot be broken down into class or linear; </li><li>  An example of using in Python and the implementation of the algorithm in the SciKit Learn library. </li></ul><a name="habracut"></a><br>  In the following articles, I will try to talk about the mathematical component of this algorithm. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As you know, machine learning tasks are divided into two main categories - classification and regression.  Depending on which of these tasks we face, and which we have datas for this task, we choose which algorithm to use. <br><br>  Support Vector Vectors or SVM (from Support Vector Machines) is a linear algorithm used in classification and regression tasks.  This algorithm has wide application in practice and can solve both linear and nonlinear problems.  The essence of the work of the ‚ÄúMachines‚Äù Support Vectors is simple: the algorithm creates a line or hyperplane that divides the data into classes. <br><br><h4>  Theory </h4><br>  The main task of the algorithm is to find the most correct line, or hyperplane, which divides the data into two classes.  An SVM is an algorithm that receives input data and returns such a dividing line. <br><br>  Consider the following example.  Suppose we have a data set, and we want to classify and divide the red squares from the blue circles (for example, positive and negative).  The main goal in this task will be to find the ‚Äúideal‚Äù line that will separate these two classes. <br><br><img src="https://habrastorage.org/webt/lj/e4/oy/lje4oybbp_pbe_slxkvhm6yqaoy.png"><br><br>  Find the perfect line, or hyperplane, that divides the data set into blue and red classes. <br><br>  At first glance, it's not so difficult, right? <br><br>  But, as you can see, there is no one, unique line that would solve such a problem.  We can pick up an infinite number of such lines that can divide these two classes.  How exactly does SVM find the ‚Äúideal‚Äù line, and what does ‚Äúideal‚Äù mean in its understanding? <br><br>  Take a look at the example below, and think which of the two lines (yellow or green) best separates the two classes, and fits the ‚Äúideal‚Äù description? <br><br><img src="https://habrastorage.org/webt/w4/_f/kz/w4_fkz5krspejxz1o73l1yjnidy.png"><br><br>  Which line best separates the data set in your opinion? <br><br>  If you chose the yellow straight, I congratulate you: this is the same line that the algorithm would choose.  In this example, we can intuitively understand that the yellow line separates and accordingly classifies the two classes better than the green. <br><br>  In the case of the green line - it is too close to the red class.  Despite the fact that it correctly classified all the objects of the current data set, such a line will not be generalized - it will not behave as well with an unfamiliar data set.  The task of finding a generalized separating two classes is one of the main tasks in machine learning. <br><br><h4>  How SVM finds the best line </h4><br>  The SVM algorithm is designed in such a way that it looks for points on the graph that are located directly to the dividing line closest.  These points are called support vectors.  Then, the algorithm calculates the distance between the reference vectors and the dividing plane.  This distance is called the gap.  The main goal of the algorithm is to maximize the clearance distance.  The best hyperplane is such a hyperplane for which this gap is as large as possible. <br><br><img src="https://habrastorage.org/webt/ps/iy/he/psiyhexemtrhnqukbvmvaqzafvi.png"><br><br>  Pretty simple, isn't it?  Consider the following example, with a more complex dataset that cannot be divided linearly. <br><br><img src="https://habrastorage.org/webt/jh/5v/bx/jh5vbxwn7vfzyzeuxibxpleejyk.png"><br><br>  Obviously, this data set cannot be divided linearly.  We cannot draw a straight line that would classify this data.  But, this dataset can be divided linearly by adding an additional dimension, which we will call the Z axis. Imagine that the coordinates on the Z axis are governed by the following constraint: <br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> z = x¬≤ + y¬≤ </script></p><br>  Thus, the ordinate Z is represented from the square of the distance of the point to the beginning of the axis. <br>  Below is a visualization of the same dataset, on the Z axis. <br><br><img src="https://habrastorage.org/webt/vd/nj/ce/vdnjce7p5csbhfp12tkaj6t-4-s.png"><br><br>  Now the data can be divided linearly.  Suppose the magenta line separates the data z = k, where k is a constant.  If a <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.717ex" height="2.419ex" viewBox="0 -780.1 4614.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-7A" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-3D" x="746" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-78" x="1802" y="0"></use><g transform="translate(2375,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-2B" x="2856" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-79" x="3857" y="0"></use><g transform="translate(4354,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-2"> z = x¬≤ + y¬≤ </script></p>  then <p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.84ex" height="2.419ex" viewBox="0 -780.1 4667.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-6B" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-3D" x="799" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-78" x="1855" y="0"></use><g transform="translate(2428,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMAIN-2B" x="2909" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/428503/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhw84GqPW2z8QJPKNElsp3Vmi3iUg#MJMATHI-79" x="3910" y="0"></use><g transform="translate(4407,0)"><text font-family="STIXGeneral,'Arial Unicode MS',serif" stroke="none" transform="scale(51.874) matrix(1 0 0 -1 0 0)">¬≤</text></g></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-3"> k = x¬≤ + y¬≤ </script></p>  - formula of a circle.  In this way, we can design our linear separator, back to the original number of sample dimensions, using this transformation. <br><br><img src="https://habrastorage.org/webt/we/nu/zn/wenuznqe4e7n4isscmunomrwzfw.png"><br><br>  As a result, we can classify a non-linear data set by adding an additional dimension to it, and then, bring it back to its original form using a mathematical transformation.  However, not with all data sets it is possible to rotate such a transformation with the same ease.  Fortunately, the implementation of this algorithm in the sklearn library solves this problem for us. <br><br><h4>  Hyperplane </h4><br>  Now that we are familiar with the logic of the algorithm, let's move on to the formal definition of a hyperplane <br><br>  A hyperplane is an n-1 dimensional subplane in n-dimensional Euclidean space that divides space into two separate parts. <br><br>  For example, imagine that our line is represented as a one-dimensional Euclidean space (i.e., our data set lies on a straight line).  Select a point on this line.  This point will divide the data set, in our case the line, into two parts.  The line has one measure, and the point has 0 measures.  Therefore, the point is the hyperplane of the line. <br><br>  For the two-dimensional dataset we met earlier, the dividing line was the same hyperplane.  Simply put, for an n-dimensional space, there is an n-1 dimensional hyperplane that divides this space into two parts. <br><br>  CODE <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np X = np.array([[<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]) y = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])</code> </pre> <br>  The points are represented as an array of X, and the classes to which they belong as an array of y. <br>  Now we will train our model with this sample.  For this example, I set the linear parameter of the ‚Äúkernel‚Äù of the classifier (kernel). <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SVC clf = SVC(kernel=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>) clf = SVC.fit(X, y)</code> </pre><br>  Predicting the class of a new object <br><br><pre> <code class="python hljs">prediction = clf.predict([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>]])</code> </pre> <br><h4>  Settings </h4><br>  Parameters are the arguments you pass when creating the classifier.  Below, I have listed some of the most important customizable SVM options: <br><br>  <b>‚ÄúC‚Äù</b> <br><br>  This parameter helps to adjust the thin line between ‚Äúsmoothness‚Äù and the accuracy of classification of objects of the training set.  The higher the value of ‚ÄúC‚Äù, the more objects of the training set will be correctly classified. <br><br><img src="https://habrastorage.org/webt/rq/01/6s/rq016soikp4qfockp86li66s5mq.png"><br><br>  In this example, there are several decision thresholds that we can determine for this particular sample.  Pay attention to the straight (represented on the graph as a green line) decision threshold.  It is pretty simple, and for this reason, several objects were classified incorrectly.  These points that were classified incorrectly are called outliers in the data. <br><br>  We can also adjust the parameters in such a way that eventually we will get a more curved line (light blue decision threshold) that will classify all the data of the training sample correctly.  Of course, in this case, the chances that our model will be able to generalize and show equally good results on new data are catastrophically small.  Therefore, if you are trying to achieve accuracy when training a model, you should aim at something more even, direct.  The higher the number ‚ÄúC‚Äù, the more entangled the hyperplane will be in your model, but the higher the number of correctly classified objects of the training set.  Therefore, it is important to ‚Äútweak‚Äù the model parameters for a specific data set in order to avoid retraining but at the same time achieve high accuracy. <br><br>  <b>Gamma</b> <br><br>  In the official documentation, the SciKit Learn library says that gamma determines how far each of the elements in a data set has an impact in defining the ‚Äúperfect line‚Äù.  The lower the gamma, the more elements, even those that are quite far from the dividing line, take part in the process of choosing this very line.  If, however, the gamma is high, then the algorithm will ‚Äúrely‚Äù only on those elements that are closest to the line itself. <br>  If you set the gamma level to be too high, then only the elements closest to the line will participate in the process of deciding on the location of the line.  This will help to ignore outliers in the data.  The SVM algorithm is designed in such a way that the points located most closely relative to each other have more weight when making decisions.  However, with the correct setting of ‚ÄúC‚Äù and ‚Äúgamma‚Äù it is possible to achieve an optimal result, which will build a more linear hyperplane ignoring outliers, and therefore more generalizable. <br><br><h4>  Conclusion </h4><br>  I sincerely hope that this article has helped you to understand the essence of the work of SVM or the Method of Supporting Vectors.  I expect any comments and advice from you.  In subsequent publications, I will talk about the mathematical component of SVM and optimization problems. <br><br>  Sources: <br>  <a href="http://scikit-learn.org/stable/modules/svm.html">Official SVM Documentation in SciKit Learn</a> <br>  <a href="https-medium-com-pupalerushikesh-svm-f4b42800e989">Blog TowardsDataScience</a> <br>  <a href="https://www.youtube.com/watch%3Fv%3Dg8D5YL6cOSE">Siraj Raval: Support Vector Machines</a> <br>  <a href="https://www.youtube.com/watch%3Fv%3Dm2a2K4lprQw">Video course Intro to Machine Learning Udacity about SVM: Gamma</a> <br>  <a href="https://en.wikipedia.org/wiki/Support_vector_machine">Wikipedia: SVM</a> </div><p>Source: <a href="https://habr.com/ru/post/428503/">https://habr.com/ru/post/428503/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428493/index.html">Git subrepo</a></li>
<li><a href="../428495/index.html">How I did a football simulator for 13 years</a></li>
<li><a href="../428497/index.html">Customizable Noolite SUF-1-300 wireless dimmer</a></li>
<li><a href="../428499/index.html">Creepy blue giants can reveal the secrets of the evolution of stars</a></li>
<li><a href="../428501/index.html">DartUP: the first Russian-language conference on Dart and Flutter on December 1 in St. Petersburg</a></li>
<li><a href="../428505/index.html">Getting links to audio without VKApi</a></li>
<li><a href="../428507/index.html">We write chat bot for VKontakte on python using longpoll</a></li>
<li><a href="../428509/index.html">How H & M tries to save itself with AI and Big Data</a></li>
<li><a href="../428511/index.html">Hydrogen energy: the beginning of a long journey</a></li>
<li><a href="../428513/index.html">500 laser pointers in one place</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>