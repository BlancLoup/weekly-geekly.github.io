<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Badoo time-series storage: so she was called Cassandra</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! My name is Evgeny Guguchkin, I am a Badoo developer on the Platform team. 


 Our team is working on interesting and necessary tasks. One of...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Badoo time-series storage: so she was called Cassandra</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/927/0e1/3df/9270e13df75f46dcbc31c2c252bbcdcd.jpeg" alt="enter image description here"></p><br><p>  Hi, Habr!  My name is Evgeny Guguchkin, I am a Badoo developer on the Platform team. </p><br><p>  Our team is working on interesting and necessary tasks.  One of them is the development of a distributed time series repository, in the decision of which I was directly involved. </p><br><p>  We recently completed a large and complex phase, and we wanted to share with you our successes, to tell why we were engaged in this task and what results we achieved. </p><a name="habracut"></a><br><h3 id="zachem-nam-voobsche-hranilische-vremennyh-ryadov">  Why do we need a time series repository? </h3><br><p>  Imagine a system with complex behavior, consisting of many components and connections between them.  To assess the state of this system, deviation from the norm, the reason for this deviation is a non-trivial task.  Badoo is one example of such a system.  We collect and store the values ‚Äã‚Äãof a huge number of metrics, the bill goes to hundreds of millions.  And it is vital for the company, because it allows you to detect problems in the early stages and quickly find the sources of these problems. </p><br><p>  In order to have the fullest possible picture, we collect quite a few different metrics, ranging from purely technical, related to the profiling of the code and components of our architecture, and ending with a lot of ‚Äúproduct‚Äù metrics. </p><br><p>  We have to save this big data stream.  Where?  Right!  In the time series repository. </p><br><p>  We currently store over 300 million metrics, updating them at a speed of about 200,000 values ‚Äã‚Äãper second.  This data occupies 16 TB on 24 servers. </p><br><h3 id="kakoe-hranilische-my-ispolzovali-do-sih-por">  What kind of storage we have used so far </h3><br><p>  For many years now, we have been using <a href="http://oss.oetiker.ch/rrdtool/">RRDtool</a> , a set of utilities for working with time series.  Despite the fact that it was created 18 years ago and has not been developed for a long time, it has its advantages: </p><br><ul><li>  "Out of the box" can draw graphics; </li><li>  can make calculations on the fly with time series; </li><li>  the volume occupied by one metric does not change no matter how much data is written there. </li></ul><br><p>  The last property is especially important for us, and I will tell you more about it.  Each metric is stored in a separate file in the RRD format.  The file contains special structures: circular archives. </p><br><p><img src="https://habrastorage.org/files/e6d/227/cc1/e6d227cc160a4a849b91bf2a1f4832a3.png" alt="enter image description here"></p><br><p>  We can imagine the ring archive as a fixed set of cells, each of which stores one value for a certain time interval. </p><br><p>  Suppose the time interval covered by a cell is five minutes, and our aggregation function is the calculation of the arithmetic average.  This means that the cell will contain the sum of all the measurements that fall into our five-minute divided by the number of these measurements. </p><br><p>  This process is shown schematically in the figure above.  We call it downsampling, because with its help we lower the level of data detail. </p><br><p>  Since we want to see recent measurements with good detail, we store several circular archives for each metric.  And even in this case, reducing the level of detail gives us a significant savings in disk space. </p><br><p>  In general, over the years of using RRDtool, we have evaluated: </p><br><ul><li>  high reading speed: RRDtool is written in C, reads data from a local disk, performs all calculations and immediately sends the finished image in PNG format;  it is unlikely that any other solution will surpass it in speed of the request; </li><li>  no need to think about all the nuances associated with looping and aggregation: RRDtool does everything by itself, we just send data there; </li><li>  Controlled dataset size: we only need to know the number of metrics and do not need to take into account the frequency of measurements. </li></ul><br><p>  Why do we think about the new repository?  It's time to talk about the shortcomings and problems that we faced. </p><br><h3 id="pochemu-nam-ponadobilos-novoe-hranilische">  Why we needed a new repository </h3><br><h4 id="neeffektivnoe-ispolzovanie-diskov">  Inefficient disk usage </h4><br><p>  Apparently, the developers of RRDtool did not expect to write large amounts of data, because already when updating 300 metrics per second we begin to rest on the disk. </p><br><p>  The reason is, firstly, that read before write is used, that is, with each update, it is necessary to read the metrics file from the disk and write a new version or part of it. </p><br><p>  Secondly, such an operation is performed with each metric.  This means that for three hundred metrics we have, respectively, several hundred disk operations with random access per second. </p><br><p>  To solve this problem, we use a special <a href="https://oss.oetiker.ch/rrdtool/doc/rrdcached.en.html">rrdcached</a> daemon, which buffers several incoming values ‚Äã‚Äãof one metric in memory, say, ten minutes and then writes them in one iteration.  So we were able to reduce the number of disk operations by an order of magnitude and, consequently, increase the throughput. </p><br><p>  The next step was the use of SSD, which also gave a tenfold increase in bandwidth to write. </p><br><p>  As a result, we achieved a record of 30 thousand records per second.  Nevertheless, due to the above features of RRDtool on our rrd servers, we have: </p><br><ul><li>  thousands of disk operations per second with random access and, as a result, high iowait: 15‚Äì20%; </li><li>  high write intensity, which ultimately leads to SSD wear; </li><li>  RAM consumption: in our configuration it is more than 30 GB for buffering, and more than 16 GB of OS allocates for inode cache (we have tens of millions of metric files in the file system). </li></ul><br><h4 id="problemy-s-rezervnym-kopirovaniem">  Backup Issues </h4><br><p>  The incremental backups approach does not work with RRDtool: by sending one value to the metric, we change the entire file with the metric, and in the next backup we need to save the whole new version.  And so for each of the tens of millions of metrics on the server.  It takes so much time that we had to give up regular backups. </p><br><h4 id="lokalnyy-dostup-k-faylam">  Local File Access </h4><br><p><img src="https://habrastorage.org/files/f83/db7/ef9/f83db7ef9da14225bf39581d896675c3.png" alt="Drawing" align="left">  This feature in itself is not a problem, as long as we are located on the same server and do not plan to scale horizontally. </p><br><p>  When we stepped beyond the limits of a single server, we solved this problem in a conservative way: each application working with time series is written on a specific server and runs only where its data is stored. </p><br><p><img src="https://habrastorage.org/files/6d4/a55/137/6d4a55137d6d47e68e908351658c4a9c.png" alt="Drawing" align="right">  It works, although it causes inconvenience: </p><br><ul><li>  each time you create a new application, you have to think about where it is better to ‚Äúshare it‚Äù; </li><li>  if suddenly your colleague at the same time chose the same server, a situation of "overpopulation" may arise; </li><li>  It is often necessary to transfer applications with all its data manually from server to server. </li></ul><br><p>  In the end, this approach has also exhausted itself: sooner or later, an application should appear, the data of which does not fit on one server.  And this moment has come - an application has appeared, for which we have allocated a separate server, but the free space is rapidly ending.  The score went on for days.  Then we quickly made a relatively simple implementation for the distribution of writing and reading through a single entry point in the form of a REST API. </p><br><p>  From the very beginning this decision was considered by us as temporary.  However, we solved the tactical task: we removed the obstacles to growth.  It worked, and in this form, the dataset of our application grew to six servers. </p><br><p>  But this added new difficulties: not only did the temporary solution be inconvenient in operation and required improvements, a critical problem for us was added - low fault tolerance: when one node was stopped, the entire recording stopped.  In addition, this repository inherited all the problems RRDtool, which I described above. </p><br><p>  In general, at this stage it became obvious that a more suitable solution should be sought, devoid of these shortcomings. </p><br><p>  And since we decided to look for a replacement, it would be nice to take into account a few more problematic points, a little less critical for us. </p><br><ul><li>  I would like to be able to rewrite or <strong>append data in hindsight</strong> .  RRDtool, for example, requires that each new metric value be over a time longer than the previous one.  This makes it necessary to collect all the data in one place, arrange it by time and send it to the storage one by one.  In addition, we are forced to reserve a certain time interval during which we are waiting for all the data to reach our central queue if possible. </li><li>  The problem with <strong>long names of metrics</strong> .  Since in RRDtool we display the name of the metric on the file name, we rest on the limitations of the file system, where the full path of the file should not exceed 255 characters. </li><li>  <strong>Compactness</strong> .  In the case of storage of sparse time series in the RRD format, the fixed file size turns to us with its ... dark side: even one recorded value reserves space on the disk as for a full metric. </li></ul><br><p>  In addition, we added such a requirement as <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2587%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">longevity</a> , since, as it turned out, some of the solutions in question under certain conditions lose the data stored in them. </p><br><h3 id="kakie-alternativy-my-rassmatrivali">  What alternatives have we considered </h3><br><p>  So, we have compiled a list of requirements and evaluated the most popular open-source solutions in this subject area.  Here's what we got: </p><br><p><img src="https://habrastorage.org/files/0c3/1ef/f46/0c31eff46edf44edbb8c8c2afb1d68b9.png" alt="image"></p><br><p>  In fact, it is not always possible to clearly determine compliance with a particular requirement (where the situation is ambiguous, I put a question mark). </p><br><p>  Personally, I really liked the solutions that we considered.  In all there are interesting ideas and just convenient features.  But now I will not dwell on them in detail, but I will try to identify only points that are important for us and critical shortcomings. </p><br><h4 id="graphite">  Graphite </h4><br><p>  It has most of the drawbacks of RRDtool, besides it consumes a lot of CPU.  But the main complaint - Graphite "silently" loses data when it lacks resources, if, for example, it does not have time to process or write. </p><br><h4 id="opentsdb">  OpenTSDB </h4><br><p>  With all the advantages of this solution, it also has a number of annoying restrictions: </p><br><ul><li>  no downsampling during recording and storage; </li><li>  the inability to understand which metrics (and how many) are written into it; </li><li>  restrictions on the names of metrics; </li><li>  the lack of a simple way to remove a metric (and we regularly delete obsolete ones); </li><li>  a request to get the last metric value leads to scanning data on the disk and is very inefficient; </li><li>  restrictions in the mass reading API and, as a result, difficulties in the implementation of downsampling. </li></ul><br><p>  And all these problems are not due to architectural constraints, but simply unfinished functionality. </p><br><h4 id="influxdb">  InfluxDB </h4><br><p>  This solution seemed perfect.  Only one, the most important point was missing - it did not scale out of the box.  However, its open-source version does not scale even now: the authors worked on clustering for more than a year and eventually decided to close this functionality.  A pity, we very much hoped ... </p><br><h4 id="druid">  Druid </h4><br><p>  When we got to know him, it was not so much a service as a framework.  Besides almost without documentation.  To get from him what we wanted, perhaps, would have to append or rewrite some of its parts. </p><br><h4 id="elasticsearch">  Elasticsearch </h4><br><p>  Elasticsearch is actually a slightly different product.  But lately, functions for analytical queries have appeared in it.  In general, he proved that he copes with the task of storing "raw" data, but at the same time he demanded four times more disk space and consumed five times more CPU than OpenTSDB.  And we also faced a strange feature: the more shards in the index, the slower it works on writing (although common sense dictates that it should be the other way around). </p><br><p>  As a result, none of these solutions out of the box satisfied our requirements.  It was necessary either to continue the search, or to ‚Äútake up the file‚Äù in order to achieve the desired. </p><br><h3 id="cassandra-pochemu-my-vybrali-eyo">  Cassandra.  Why did we choose her </h3><br><p>  It must be said that we were particularly interested in the two solutions from the table, so we integrated these storage facilities at the level of proof of concept into our framework for testing in real applications on real workload. </p><br><p>  Speech about InfluxDB and OpenTSDB.  Unfortunately, InfluxDB was dropped after the developers refused to support clustering in the open version, and OpenTSDB did not support downsampling and some other necessary functions that we already mentioned (deleting metrics, getting the last value, etc.). </p><br><p>  However, OpenTSDB showed very good performance, scalability, fault tolerance.  In general, the most important points in our table. </p><br><p>  If to understand, OpenTSDB is a ‚Äúwrapper‚Äù over the distributed Apache HBase DBMS, which adds REST API access and is able to pack data for the DBMS accordingly.  All the qualities we like are provided by the distributed DBMS, and all the missing functions are easily implemented on top of it. </p><br><p>  When we finally realized that a suitable repository does not exist, we decided to make it ourselves by analogy with OpenTSDB based on a distributed DBMS, adding circular archives there, as in RRDtool. </p><br><p>  But now we have a choice of distributed DBMS.  This time we did not conduct a large-scale study, but limited ourselves to comparing the already mentioned <a href="https://ru.wikipedia.org/wiki/HBase">Apache HBase</a> and another distributed DBMS - <a href="https://ru.wikipedia.org/wiki/Apache_Cassandra">Apache Cassandra</a> .  Both databases are based on the <a href="https://ru.wikipedia.org/wiki/BigTable">Google Bigtable</a> data model.  Apache Cassandra also borrowed some ideas from <a href="https://ru.wikipedia.org/wiki/Amazon_DynamoDB">Amazon DynamoDB</a> , which only benefited it. </p><br><p>  As usual, each option has both advantages and disadvantages.  Nevertheless, both of them are able to cope with our task.  How did we choose between these DBMS? </p><br><p>  In my opinion, every story of choice is always a little subjective story.  Nevertheless, I will try to justify our choice.  The table below lists the differences that influenced our decision.  It would be wrong to say that these are HBase drawbacks ‚Äî no, just features that need to be kept in mind when developing. </p><br><table><thead><tr><th>  <strong>Cassandra</strong> </th><th>  <strong>Hbase</strong> </th></tr></thead><tbody><tr><td>  No dependencies </td><td>  Wanted ZooKeeper and Hadoop </td></tr><tr><td>  Easy setup </td><td>  Need to configure all components </td></tr><tr><td>  CQL query language </td><td>  Cumbersome API </td></tr><tr><td>  Decentralized </td><td>  There is a master node - SPOF </td></tr><tr><td>  Uniform load distribution </td><td>  Hotspot problem </td></tr><tr><td>  Materialized views </td><td>  Application-side implementation </td></tr><tr><td>  Secondary indexes </td><td>  Application-side implementation </td></tr><tr><td>  Logged batches </td><td>  Application-side implementation </td></tr><tr><td>  Productive driver for PHP 7 </td><td>  PHP Thrift Client </td></tr></tbody></table><br><p>  As you can see from the table, working with Cassandra is simpler and more convenient on many points.  This is a simpler installation, configuration, and its own query language, and good documentation.  Unlike HBase, Cassandra does not have a master node and, accordingly, does not need to duplicate it to ensure high resiliency.  With Cassandra, it is not necessary to ensure that the data is distributed evenly among the nodes, while in HBase there are possible distortions in the load, and the application itself must monitor how the data is distributed.  Not the last role was played by the presence of a driver for PHP 7, which we then switched to. </p><br><h3 id="kak-my-hranim-vremennye-ryady-v-cassandra">  How we store time series in Cassandra </h3><br><p>  A detailed description of Apache Cassandra would not fit into the whole article.  In addition, the Internet has a sufficient amount of good materials on this topic, including on Habr√©.  However, some common points are worth mentioning. </p><br><ul><li>  The data in Cassandra is distributed in a cluster in such a way that each record has copies on its neighboring nodes.  The number of copies of the record (replication factor) is a configurable parameter. </li><li>  In the event that one of the nodes becomes unavailable, both read and write requests continue to be serviced by neighboring nodes containing replicas of the unavailable node. </li><li>  Adding new nodes occurs without downtime.  Data is automatically redistributed across the cluster. </li><li>  Productivity increases linearly with increasing cluster size. </li><li>  LSM-tree is used for data storage - a structure that provides very effective data update and insertion of new records. <br><img src="https://habrastorage.org/files/48b/a76/811/48ba768112514a9590080c10e51d11e3.png" alt="Drawing" align="right"></li><li>  For access to data, Cassandra offers a tabular data model and CQL query language, but this is all syntactic sugar, but in reality, the key-value model lies under the hood.  But unlike the simple key-value-model, where both the key and the values ‚Äã‚Äãare of type BLOB, in Cassandra value is a structure, namely, an associative array, ordered by key value.  <strong>Wherein:</strong> <br>  ‚Ä¢ you can change and add one or more elements of such an array at a time; <br>  ‚Ä¢ elements of each such associative array are stored on the disk as consistently as possible; <br>  ‚Ä¢ this allows very efficiently, in one access to the disk, to receive both one element of the array and the range of values, and, of course, the entire array as a whole. </li></ul><br><p>  It is precisely at the level of this key-value-model that I will try to explain on the fingers how we implemented the storage of time series. </p><br><p><img src="https://habrastorage.org/files/e76/aaa/542/e76aaa542ea6489383dd5fecf8dc69c2.png" alt="Drawing" align="right">  First, let's look at the storage of primary, non-aggregated data.  They consist of the name of the metric and its values.  Each value is a pair of timestamp: double.  Put this data in the Points table. </p><br><p>  As you can see, this structure fits very well on the internal presentation of data in Cassandra.  But there is one caveat: so we can easily exceed the constraint of Cassandra on the size of the associative array.  Formally, this is 2 billion, but in practice it is better not to exceed 100 thousand. </p><br><p><img src="https://habrastorage.org/files/ab4/5a5/a53/ab45a5a538154d3cae7efd9cb8ea1cda.png" alt="Drawing" align="left">  This restriction is easily circumvented by sharding the data of one metric by day.  In the name of the key, the date is added to the name of the metric.  Now the metric values ‚Äã‚Äãwill be stored in several associative arrays - one for each day.  Let's call these shards with primary data daily segments, or simply segments.  If you write values ‚Äã‚Äãto the metric every second, then the size of the segment will not exceed 86,400 values. </p><br><p><img src="https://habrastorage.org/files/988/0b2/c04/9880b2c04971421b83dbbb6d67229504.png" alt="Drawing" align="right">  With the primary data, it seems, everything is clear.  But remember, we wanted to store historical data in the form of aggregated circular archives.  This data will be stored in the new table Rollups. </p><br><p>  Here we will have a complex key.     ,   .  , ,   RRDtool:    ,     . </p><br><p>     : </p><br><ul><li><script type="text/javascript">function gtElInit() {var lib = new google.translate.TranslateService();lib.translatePage('ru', 'en', function () {});}</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=gtElInit&amp;client=wt"></script> the key is the K cell number calculated by the formula <code>K = floor(time % retention / granularity)</code> ; </li><li>  value - a tuple consisting of a pair: the time corresponding to the cell, and the actual value of the cell. </li></ul><br><p>  As a result, the data of one metric were scattered in different tables and keys.  The metric has primary data in several segments and several archives with different details. </p><br><p>  To read all the metrics, you need to know its keys - what are its segments and archives.  We will collect such information in a separate Meta table. </p><br><p><img src="https://habrastorage.org/files/0a7/34d/184/0a734d1844814ec5ac2b033102cb8e0d.png" alt="enter image description here"></p><br><p>  Here by the name of the metric we store: </p><br><ul><li>  a list of its segments; </li><li>  a list of its archives; </li><li>  the oldest cell among all the metrics archives (on the timeline, this can be represented as the right border of the archived data; with its help, we can always calculate the left border and choose the most suitable archive corresponding to the query). </li></ul><br><p>  You may notice that the name of the metric is included in all keys.  This complicates the operation of its renaming and increases the consumption of disk space, because the metrics we have are really long, with an average of at least 100 characters.  We decided to use an identifier instead of a name and store it in a separate Names table. </p><br><p>  Now we just need to create a new entry in the Names table - and we renamed the metric.  In addition, in future, it will be possible to assign attributes to metrics and make complex sampling of metrics by the values ‚Äã‚Äãof these attributes. </p><br><p><img src="https://habrastorage.org/files/56b/961/a78/56b961a781e04359821b70fd6e54c6dc.png" alt="enter image description here"></p><br><h3 id="perezapis-dannyh-i-daunsempling">  Data overwriting and downsampling </h3><br><p>  We decided to store both primary and aggregated data.  Primary, in the Points table, are stored for several days, after which whole segments are aggregated and sent to the archive.  This allows us on the one hand to overwrite data in hindsight (although only within the last seven days), and on the other - to store data in a compact archive. </p><br><p>  We have high demands for downsampling performance.  To keep up, you need to process 700 metrics per second, or more than 60 million per day.  That is how many segments we create per day at the moment.  How to ensure this and not load disks?  Cassandra allows you to sequentially scan the entire table or some part of it.  And due to this, it is possible to avoid ineffective random access to the disk.  With current volumes, we can subtract all the raw data in six to eight hours in several streams.  Due to this, the maximum performance of our downsampling is 170 million daily segments per day. </p><br><h3 id="chtenie-dannyh">  Reading data </h3><br><p>  The read request contains the following parameters: </p><br><ul><li>  metric_name is the name of the metric; </li><li>  start - the beginning of the requested interval; </li><li>  end - the end of the requested interval; </li><li>  func - aggregation function; </li><li>  step - the step of detailing. </li></ul><br><p>  This request is executed by the following algorithm: </p><br><ol><li>  By the name of the metric we find the identifier. </li><li>  By identifier we obtain meta information about the metric: a list of segments, a list of archives and the maximum time of archives. </li><li>  We select the appropriate archive in such a way that it corresponds to the aggregating function, covers the requested time interval and has the appropriate granularity. </li><li>  Select the segments included in the desired interval. </li><li>  Archives and raw data are combined and resampled with the detail step we need. </li><li>  At the output we get a set of metric values ‚Äã‚Äãfrom start to end with step step. </li></ol><br><p><img src="https://habrastorage.org/files/9c9/0cd/a88/9c90cda8871c475a92e40fde26de7f8e.png" alt="enter image description here"></p><br><h3 id="rest-api">  REST API </h3><br><p>  To be able to write to our repository not only from PHP, we have accessed it through the REST API.  And implemented it, of course, on our favorite nginx and PHP. </p><br><p>  The important point is the use of the cache, where we store the identifiers of the metrics by their names.  We need a fast cache, from where we plan to read more than 200 thousand times per second.  We use <a href="http://php.net/manual/ru/intro.apcu.php">APCu cache</a> . </p><br><p>  To prevent the REST API from becoming a bottleneck, we made it distributed.  And in order to increase the hit rate and make the use of the metrics cache efficient, we assign the metric to a specific server by its hash. </p><br><p>  For the client, all nodes are the same, and it is not necessary for him to know about the binding of the metric to the server.  How are requests processed in this case?  Let's look at the example of five nodes. </p><br><p>  The client selects one of the five nodes of the REST API cluster randomly.  Suppose a node request for writing 100 metrics comes to node 3.  On the server, these 100 metrics are grouped into five subqueries.  So in each group there are approximately 20 metrics assigned to one server.  Subsequently, these subqueries are sent to their neighboring nodes. </p><br><p><img src="https://habrastorage.org/files/f96/730/879/f9673087970143c4aa121e415fd401e8.png" alt="enter image description here"></p><br><p>  In such a scheme, when the metric should be processed only on a specific node, we get a problem in the event of a node falling.  That is, it is not enough for us to have fault tolerant storage ‚Äî we need a failover cluster of REST API nodes. </p><br><p><img src="https://habrastorage.org/files/309/8dd/b86/3098ddb86f5147b4b0e00bcae61a80a3.png" alt="enter image description here"></p><br><p>  In our case, this is not difficult.  We implemented a simple failover. </p><br><p>  Imagine that node 4 is inaccessible.  In this case, one of the subqueries remains unprocessed.  And we can not stop processing and wait until node number 4 rises. </p><br><p>  The solution is obvious: we repeat the procedure with the grouping of metrics imperceptibly for the client, but now only for one raw subquery, and distribute the subqueries only between the live nodes. </p><br><p><img src="https://habrastorage.org/files/907/809/5cf/9078095cf48c4cb697a1d6d617917115.png" alt="enter image description here"></p><br><p>  The data about native metrics is cached by nodes for a day, and the data about aliens for only one hour.  Thus, the fall of one node does not particularly affect the hit rate: most metrics are still serviced by their nodes, and foreign metrics do not occupy the cache for a long time. </p><br><h3 id="kakih-rezultatov-nam-udalos-dobitsya">  What results have we achieved </h3><br><p>  To date, we have transferred part of the data to the new storage.  These are metrics of that application which dataset was not located on one server and for which we made an improvised cluster of six servers. </p><br><p>  This is what we ended up with: </p><br><ul><li>  The new cluster consists of nine servers; </li><li>  180 million metrics are currently stored there; </li><li>  maximum recording capacity - 250 thousand per second; </li><li>  maximum downsampling speed ‚Äì170 million segments per day; </li><li>  for the time series itself, we use a replication factor of two; </li><li>  data volume - approximately 9 TB; </li><li>  REST API cluster is located on the same servers as the Cassandra cluster. </li></ul><br><p>  Comparing the new storage with our RRD-cluster, we should separately mention such an indicator as the average size of the metric.  Formally, all other things being equal, the average size of a metric in Cassandra is 30% less than in RRD storage.  But the LSM trees used for storage in Cassandra require the reservation of free disk space for temporary files (for a process called compaction). </p><br><p>  In our case, we have to reserve about 60% of the size of the dataset.  As a result, the average size of the metric with this in mind turned out to be 20% larger than in the old storage.  This can be seen as a price for performance and fault tolerance. </p><br><p>  Summing up, we can say that our new solution for storing time series provides us with fault tolerance and scalability, while exceeding the old storage in performance.  And most importantly - even with some reservations, but we have achieved almost all the goals we set for ourselves. </p><br><p>  Unfortunately, within the framework of one article it is impossible to consider all the issues of implementation.  So, we have not touched on issues such as consistency in reading and writing;  how we manage without transactions, full support of which is absent in Cassandra;  how we use materialized representations;  what optimizations we plan to do;  and many others.  Perhaps this will be devoted to one of our next posts. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/324510/">https://habr.com/ru/post/324510/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../324500/index.html">RCC 2017. Analysis of the tasks of the hottest warm-up round</a></li>
<li><a href="../324502/index.html">The way to deal in CRM: automate it</a></li>
<li><a href="../324504/index.html">Do you know the speed of loading your site from mobile devices? Time to figure it out</a></li>
<li><a href="../324506/index.html">A brief history of javascript. Part 2</a></li>
<li><a href="../324508/index.html">Machine learning in Avito. Video recordings from the Data Science Case Club meeting on March 14</a></li>
<li><a href="../324512/index.html">Fighting beaver with donkey, or Adapting MSVC code under gcc</a></li>
<li><a href="../324514/index.html">Parametric modeling in CAD SolveSpace: Sketch</a></li>
<li><a href="../324516/index.html">Event for Unity-developers in Kharkov</a></li>
<li><a href="../324518/index.html">Functional programming and c ++ in practice</a></li>
<li><a href="../324522/index.html">Guess the filter by impulse response</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>