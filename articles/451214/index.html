<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fundamentals of modern artificial intelligence: how it works, and whether our society will destroy this year?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today's AI is technically ‚Äúweak‚Äù - but it is complex and can significantly affect society. 

 You do not need to be Cyrus Dully to know how frightenin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fundamentals of modern artificial intelligence: how it works, and whether our society will destroy this year?</h1><div class="post__text post__text-html js-mediator-article"><h3>  Today's AI is technically ‚Äúweak‚Äù - but it is complex and can significantly affect society. </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/727/0c9/c49/7270c9c490a792d7c135f02952b6a453.jpg"><br>  <i>You do not need to be Cyrus Dully to know how frightening artificial intelligence can become frightening [American actor who played the role of astronaut Dave Bowman in the 2001 film Odyssey 2001 / approx.</i>  <i>transl.]</i> <br><br>  AI, or artificial intelligence, is now one of the most important areas of knowledge.  ‚ÄúUnsolvable‚Äù problems are solved, billions of dollars are invested, and Microsoft even <a href="https://www.youtube.com/watch%3Fv%3D9tucY7Jhhs4">hires</a> <a href="https://ru.wikipedia.org/wiki/Common">Kommon</a> to tell us with poetic calmness what a wonderful thing it is - AI.  That's it. <br><a name="habracut"></a><br>  And, as with any new technology, it is difficult to get through all the hype.  For years I have been doing research in the field of UAVs and AI, but even it can be difficult for me to keep up with all this.  In recent years, I spent a lot of time looking for answers even to simple questions like: <br><br><ul><li>  What do people mean when they say "AI"? </li><li>  What is the difference between AI, machine learning and deep learning? </li><li>  What is so great about deep learning? </li><li>  Which former complex tasks are now easy to solve, and what is still difficult? </li></ul><br>  I know that no one is interested in such things.  Therefore, if you are wondering what all these enthusiasm about AI are connected with at the simplest level, it's time to look behind the scenes.  If you are an AI expert, and read reports from the conference on neurological information processing (NIPS) for entertainment, the article will not contain anything new for you - however, we expect from you clarifications and corrections in comments. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  What is AI? </h2><br>  There is such an old joke in computer science: what is the difference between AI and automation?  Automation is what can be done with a computer, and AI is what we would like to be able to do.  As soon as we learn how to do something, it moves from the field of AI to the category of automation. <br><br>  This joke is valid today, since the AI ‚Äã‚Äãis not clearly defined.  ‚ÄúArtificial Intelligence‚Äù is simply not a technical term.  If you get into Wikipedia, it says that AI is ‚Äúintelligence demonstrated by machines, as opposed to natural intelligence demonstrated by humans and other animals.‚Äù  Less clear and can not be said. <br><br>  In general, there are two types of AI: strong and weak.  Strong AI imagines most people when they hear about AI - this is some kind of God-like all-knowing intelligence like Skynet or Hal 9000, capable of reasoning and comparable to human intelligence, and at the same time superior to its capabilities. <br><br>  Weak AIs are highly specialized algorithms designed to get answers to certain useful questions in narrowly defined areas.  For example, a very good chess program falls into this category.  The same can be said about software that very precisely adjusts insurance payments.  In their field, such AIs achieve impressive results, but in general they are very limited. <br><br>  With the exception of Hollywood opus, today we didn‚Äôt even come close to a strong AI.  So far, any AI is weak, and most researchers in this field agree that the techniques we have invented of creating beautiful weak AI probably will not bring us closer to creating a strong AI. <br><br>  So today's AI is a more marketing term than a technical one.  The reason why companies advertise their "AI" instead of "automation" is because they want to embed a Hollywood AI into the public mind.  However, this is not so bad.  If this is not taken too strictly, companies only want to say that, although we are still very far from strong AI, today's weak AI is much more capable than it did a few years ago. <br><br>  And if to distract from marketing, then so it is.  In certain areas, the capabilities of machines have increased dramatically, and mainly due to two more fashionable phrases now: machine learning and deep learning. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/db6/c44/5c0/db6c445c04f9b4589b024b3d0ff7d91d.png"><br>  <i>A snapshot of a short video from Facebook engineers demonstrating how AI recognizes cats in real time (task, also known as the holy grail of the Internet)</i> <br><br><h2>  Machine learning </h2><br>  MO is a special way to create machine intelligence.  Suppose you want to launch a rocket, and predict where it will go.  In general, this is not so difficult: gravity has been studied fairly well, you can write down the equations and calculate where it goes, based on several variables, such as speed and starting position. <br><br>  However, this approach becomes clumsy if we turn to an area whose rules are not so well known and clear.  Suppose you want a computer to tell you if there are any cats in any images from the sample.  How will you write down the rules that describe the view in all possible points of view on all possible combinations of whiskers and ears? <br><br>  Today, the MO-approach is well known: instead of trying to write down all the rules, you create a system that can derive a set of internal rules on its own after studying a huge number of examples.  Instead of describing cats, you simply show your AI a bunch of photos of cats, and let them know for themselves what is a cat and what is not. <br><br>  And today is the perfect approach.  A self-learning system based on data can be improved by simply adding data.  And if our view can do something very well, it is to generate, store and manage data.  Want to learn to recognize cats better?  The Internet generates millions of examples right this minute. <br><br>  Increasing data flow is one of the reasons for the recent explosive growth of MO algorithms.  Other reasons are related to the use of this data. <br><br>  In addition to the data, for MO there are two more related issues: <br><br><ul><li>  How can I remember what I learned?  How to store and present on a computer communications and the rules that I derived from the data? </li><li>  How do i study?  How to change the saved representation in response to the arrival of new examples, and improve? </li></ul><br>  In other words, what exactly is being trained on the basis of all this data? <br><br>  In MO, the computational representation of learning that we store is a model.  The type of model used is very important: it determines how your AI learns, what data it can learn, and what questions you can ask it. <br><br>  Let's look at a very simple example.  Suppose we buy figs at the grocery store, and we want to make an AI with an MO that tells us if it is ripe.  This should be easy to do, because in the case of figs, the softer it is, the sweeter it is. <br><br>  We can take several samples of ripe and unripe figs, see how sweet they are, and then place them on the chart and adjust the line for it.  This straight will be our model. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a0/d91/8c1/1a0d918c1ef06cb1edfec80edca05dde.png"><br>  <i>The germ of AI in the form of "the softer they are, the sweeter"</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/88e/410/a9e/88e410a9e90287f3dd6a5dc5c604322d.png"><br>  <i>With the addition of new data, the task is complicated</i> <br><br>  Take a look!  The direct implicitly follows the idea that ‚Äúthe softer they are, the sweeter‚Äù, and we didn't even have to write anything down.  Our AI germ does not know anything about the sugar content or the ripening of the fruit, but it can predict the sweetness of the fruit by squeezing it. <br><br>  How to train a model to make it better?  We can collect even more samples and draw another straight line to get more accurate predictions (as in the second picture above).  However, the problems immediately become apparent.  So far, we have been training our fig AI on high-quality berries - and what if we take data from an orchard?  Suddenly, we have not only ripe, but also rotten fruit.  They are very soft, but definitely not suitable for eating. <br><br>  What should we do?  Well, since this is a MO model, we can simply feed it more data, right? <br><br>  As the first picture below shows, in this case we will get completely meaningless results.  The straight is simply not suitable for describing what happens when the fruit becomes too ripe.  Our model no longer fits into the data structure. <br><br>  Instead, we will have to change it, and use a better and more complex model - perhaps a parabola, or something similar.  This change complicates learning, because drawing a curve requires more complex math than drawing a straight line. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8ed/3db/429/8ed3db429631b34de691c718785200cc.png"><br>  <i>Okay, probably the idea of ‚Äã‚Äãusing direct for complex AI was not very successful.</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/281/09a/e40/28109ae40cf3b51a9af2ab579f301812.png"><br>  <i>Math is more difficult</i> <br><br>  The example is rather silly, but it shows that the choice of model determines the possibilities of learning.  In the case of figs, the data is simple, and models can be simple.  But if you are trying to learn something more complex, you need more complex models.  Just as no amount of data will force the linear model to reflect the behavior of rotten berries, so it is impossible to find a simple curve corresponding to a bunch of pictures to create an algorithm for computer vision. <br><br>  Therefore, the difficulty for the MO is to create and select the right models for the respective tasks.  We need a model that is complex enough to describe really complex connections and structures, but simple enough so that you can work and train it with it.  So, although the Internet, smartphones, and so on, have created incredible mountains of data on which to learn, we still need the right models to use this data. <br><br>  This is where deep learning comes into play. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0ff/b2d/663/0ffb2d6630240722208088bd6be34644.jpg"><br><br><h2>  Deep learning </h2><br>  Deep learning is machine learning using a certain type of model: deep neural networks. <br><br>  Neural networks are a type of MO model that uses a structure resembling neurons in the brain for computation and prediction.  Neurons in neural networks are organized in layers: each layer performs a set of simple calculations and transmits the answer to the next. <br><br>  The layered model allows for more complex calculations.  A simple network with a small number of neuron layers is enough to reproduce the straight line or parabola we used above.  Deep neural networks are neural networks with a large number of layers, with dozens, or even hundreds;  hence their name.  With so many layers you can create incredibly powerful models. <br><br>  This feature is one of the main reasons for the huge popularity of deep neural networks in recent times.  They can learn various complex things without forcing the human researcher to define some rules, and this allowed us to create algorithms capable of solving various tasks to which computers could not approach before. <br><br>  However, in the success of the neural networks, another aspect has also contributed: training. <br><br>  The ‚Äúmemory‚Äù of a model is a set of numeric parameters that determines how it issues answers to the questions asked of it.  Teaching a model means adjusting these parameters so that the model produces the best possible answers. <br><br>  In our model with figs, we looked for the equation of a straight line.  This is a simple regression task, and there are formulas that will give you an answer in one step. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aab/02d/755/aab02d7555ee5ec9e043f750cb7cb85b.png"><br>  <i>Simple neural network and deep neural network</i> <br><br>  With more complex models, things are not so simple.  A straight line and a parabola can be easily represented by several numbers, but a deep neural network can have millions of parameters, and a data set for its learning can also consist of millions of examples.  There is no one-step analytical solution. <br><br>  Fortunately, there is one strange trick: you can start with a bad neural network, and then improve it with the help of gradual adjustments. <br><br>  Training a model of MO in this way is similar to checking a student with the help of tests.  Each time we get an estimate comparing what answers the model thinks should be with the ‚Äúcorrect‚Äù answers in the training data.  Then we make improvements and run the check again. <br><br>  How do we know what parameters need to be adjusted, and how much?  Neural networks have such a cool property, when for many types of training it is possible not only to get an assessment in a test, but also to calculate how much it will change in response to changes in each parameter.  Mathematically speaking, estimation is a function of value, and for most of such functions we can easily calculate the gradient of this function relative to the parameter space. <br><br>  Now we know for sure in which direction we need to adjust the parameters to increase the assessment, and you can adjust the network by successive steps in all the best and best "directions" until you reach the point where nothing can be improved.  This is often referred to as climbing a hill, because it really looks like moving up a hill: if you constantly move up, you end up at the top. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/99e/68e/2bd/99e68e2bdb6e363bee4a0e3780d44352.png"><br>  <i>Did you see?</i>  <i>Vertex!</i> <br><br>  Thanks to this neural network is easy to improve.  If your network has a good structure, getting new data, you do not need to start from scratch.  You can start with the available parameters, and re-learn from the new data.  Your network will gradually improve.  The most prominent of today's AIs - from cat recognition on Facebook to technologies that Amazon (probably) uses in non-vendor stores - are built on this simple fact. <br><br>  This is the key to another reason why GO has spread so quickly and so widely: climbing a hill allows you to take one neural network trained for a task and retrain it for another, but similar.  If you have trained the AI ‚Äã‚Äãto recognize cats well, you can use this network to train an AI that recognizes dogs, or giraffes, without having to start from scratch.  Start with AI for cats, evaluate it by the quality of dog recognition, and then climb the hill, improving the network! <br><br>  Therefore, in the last 5-6 years there has been a dramatic improvement in the capabilities of AI.  Several pieces of the puzzle have developed in a synergistic way: the Internet has generated a huge amount of data on which to learn.  Computations, especially parallel computations on GPUs, made it possible to process these huge sets.  Finally, deep neural networks allowed to take advantage of these sets and create incredibly powerful MO models. <br><br>  And all this means that some things that were previously extremely complex are now very easy to do. <br><br><h2>  And what can we do now?  Pattern recognition </h2><br>  Perhaps the deepest (sorry for the pun) and the fastest impact of deep learning had on the field of computer vision - in particular, on the recognition of objects in photographs.  A few years ago, this xkcd comic perfectly described the leading edge of computer science: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed8/605/a55/ed8605a558349387039ba9c1d71400fa.png" alt="image"><br><br>  Today, the recognition of birds and even certain types of birds is a trivial task that a properly motivated high school student can solve.  What has changed? <br><br>  The idea of ‚Äã‚Äãvisual recognition of objects is easy to describe, but difficult to implement: complex objects consist of sets of simpler ones, which in turn consist of simpler forms and lines.  Faces consist of eyes, noses and mouths, and those consist of circles and lines, and so on. <br><br>  Therefore, face recognition becomes a matter of recognizing the patterns in which the eyes and mouths are located, which may require recognition of the shapes of the eyes and mouth from lines and circles. <br><br>  These patterns are called features, and before the emergence of deep learning for recognition it was necessary to describe all the features manually and program the computer to search for them.  For example, there is the famous <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%2592%25D0%25B8%25D0%25BE%25D0%25BB%25D1%258B_%25E2%2580%2594_%25D0%2594%25D0%25B6%25D0%25BE%25D0%25BD%25D1%2581%25D0%25B0">Viola-Jones</a> face recognition algorithm, based on the fact that eyebrows and nose are usually lighter than eye sockets, so they form a bright T-shape with two dark points.  The algorithm, in fact, is looking for similar T-shaped forms. <br><br>  The Viola-Jones method works well and surprisingly quickly, and serves as the basis for recognizing faces in cheap cameras, etc.  But, obviously, not every object that you need to recognize is susceptible to such a simplification, and people invented more and more complex and low-level patterns.  For the algorithms to work correctly, the work of the team of doctors of sciences was required, they were very sensitive and susceptible to failures. <br><br>  The big breakthrough happened thanks to GO, and in particular, to a certain type of neural networks called ‚Äúconvolutional neural networks‚Äù.  Convolutional neural networks, SNS are deep networks with a certain structure, inspired by the structure of the visual cortex of mammalian brain.  This structure allows the SNA to independently study the hierarchy of lines and patterns for object recognition, rather than waiting for PhDs to spend years researching which features are better suited for this.  For example, the SNS, trained on the faces, will learn its own internal representation of lines and circles, folding in the eyes, ears and noses, and so on. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/365/fe2/8ea/365fe28ea776309b21485655c338ffe7.png"><br>  <i>Old visual algorithms (Viola-Jones method, left) rely on manually selected features, and deep neural networks (right) on their own hierarchy of more complex features made up of simpler</i> <br><br>  The SNS was amazingly good for computer vision, and soon the researchers were able to train them to perform all sorts of tasks on visual recognition, from finding cats in the photo to identifying pedestrians who got into the camera of the mobile. <br><br>  This is all great, but there is another reason for the rapid and widespread adoption of the SNA - this is how easily they adapt.  Remember climbing the hill?  If our high school student wants to recognize a particular bird, he can take any of the many visual networks with open source and train her on his own data set, without even understanding how the underlying mathematics works. <br><br>  Naturally, this can be expanded further. <br><br><h2>  Who's there?  (face recognition) </h2><br>  Suppose you want to train a network that recognizes not just faces, but one particular person.  You could train the network to recognize a certain person, then another person, and so on.  However, time is spent on training networks, and this would mean that each new person would need to retrain the network.  No, really. <br><br>  Instead, we can start with a network that is trained to recognize faces in general.  Its neurons are tuned to recognize all facial structures: eyes, ears, mouths, and so on.  Then you simply change the output: instead of forcing it to recognize certain faces, you command it to give the face description in the form of hundreds of numbers describing the curvature of the nose or the shape of the eyes, and so on.  The network can do this because it already ‚Äúknows‚Äù which components it consists of. <br><br>  You, of course, do not define it all directly.  Instead, you train the network by showing it a set of faces, and then comparing the output.  You also teach it in such a way that it gives descriptions of the same person that are similar to each other, and descriptions of different people that are very different from each other.  Mathematically speaking, you train the network to build a correspondence to the face of a point in the space of features, where the Cartesian distance between the points can be used to determine their similarity. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eab/c79/fbe/eabc79fbe9499775a6b09c156430d8ed.png"><br>  <i>Changing a neural network from face recognition (left) to a description of faces (right) only requires changing the format of the output, without changing its basis</i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eee/10c/6c5/eee10c6c5e26a529bc02380b0af80c57.png"><br>  <i>Now you can recognize faces by comparing the descriptions of each person created by the neural network.</i> <br><br>  Having trained a network, you can already easily recognize faces.  You take the original face and get its description.  Then take a new face and compare the description issued by the network with your original.  If they are close enough, you say that they are one and the same person.  And here you are from a network capable of recognizing one face, to what can be used to recognize any face! <br><br>  Such structural flexibility is another reason for the usefulness of deep neural networks.  A huge variety of MO-models have been developed for computer vision, and although they are developing in very different directions, the basic structure of many of them is based on early SNS such as Alexnet and Resnet. <br><br>  I even heard stories about people using visual neural networks to work with time series data or sensor measurements.  Instead of creating a special network for analyzing the flow of data, they trained an open-source computer network for the open source to literally look at the shapes of the graph lines. <br><br>  Such flexibility is a good thing, but not infinite.  To solve some other problems, you need to use other types of networks. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/06b/b0a/a24/06bb0aa2484e302f725f53865d740655.png"><br>  <i>And even to this point virtual assistants traveled for a very long time.</i> <br><br><h2>  What you said?  (Speech recognition) </h2><br>  Image cataloging and computer vision are not the only areas of AI revival.  Another area in which computers have progressed very far is speech recognition, especially in translating speech into writing. <br><br>  The basic idea of ‚Äã‚Äãspeech recognition is quite similar to the principle of computer vision: recognize complex things as sets of more simple ones.  In the case of speech, the recognition of sentences and phrases is based on the recognition of words, which is based on the recognition of syllables, or, to be more precise, phonemes.  So when someone says ‚ÄúBond, James Bond,‚Äù we actually hear BON + DUH + JAY + MMS + BON + DUH. <br><br>  In vision, the features are organized spatially, and this structure is processed by the SNA.  In the rumor, these features are organized in time.  People can speak quickly or slowly, without a clear beginning and end of speech.  We need a model that is able to perceive sounds as they become available, as a person, instead of waiting and looking for complete sentences in them.  We cannot, as in physics, say that space and time are one and the same. <br><br>  Recognizing individual syllables is fairly easy, but difficult to isolate.  For example, ‚ÄúHello there‚Äù may sound like ‚Äúhell no they're‚Äù ... So for any sequence of sounds, there are usually several combinations of syllables actually spoken. <br><br>  To understand all this, we need the ability to study the sequence in a specific context.  If I hear a sound, then what is more likely - that the person said ‚Äúhello there dear‚Äù or ‚Äúhell no they're deer?‚Äù Here again machine learning comes to the rescue.  With a fairly large set of patterns of spoken words, you can learn the most likely phrases.  And the more examples you have, the better it will be. <br><br>  For this, people use recurrent neural networks, RNS.  In most types of neural networks, such as, for example, in the SNS that deal with computer vision, the connections between neurons work in one direction, from input to output (mathematically, these are directed acyclic graphs).  In the RNS, the output of neurons can be redirected back to the neurons of the same level, to themselves or even further.  This allows the RNS to have its own memory (if you are familiar with binary logic, then this situation is similar to the operation of triggers). <br><br>  SNS works for one approach: we feed it an image, and it gives some description.  The RNS maintains an internal memory of what was given to it earlier, and gives answers based on what it has already seen, plus what it sees now. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/538/5ac/de9/5385acde9ab955d666e7cca4ea3f33a6.png"><br><br>  This property of memory in the RNS allows them not only to ‚Äúlisten‚Äù to syllables that come to it one by one.  This allows the network to learn what syllables go together, forming a word, and how likely certain sequences are. <br><br>  Using PHT, it is possible to get a very good transcription of human speech - to such an extent that in some measurements, the accuracy of transcription computers can now exceed people.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, sounds are not the only area where sequences appear. </font><font style="vertical-align: inherit;">Today, the RNS is also used to determine the sequence of movements for recognizing actions on the video.</font></font><br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/PCBTZh41Ris" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Show me how you can move (deep fakes and generative networks) </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So far we have been talking about MO models designed for recognition: tell me what is shown in the picture, tell me what the person said. But these models are capable of more - today's GO models can also be used to create content. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is meant when people talk about deepfake - incredibly realistic fake videos and images created using GO. Some time ago, an employee of German television caused an extensive political debate, </font></font><a href="https://www.theguardian.com/world/2015/mar/19/i-faked-the-yanis-varoufakis-middle-finger-video-says-german-tv-presenter"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">creating a fake video</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">on which the Minister of Finance of Greece showed Germany the middle finger. To create this video, it took a team of editors who worked to create a TV show, but in the modern world, this can be done by anyone in a few minutes with access to a medium-sized gaming computer. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All this is rather sad, but not so dark in this area - at the top shows my favorite video on this technology. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This team has created a model capable of processing a video with the dance movements of one person and creating a video with another person repeating these movements, magically performing them at the expert level. It is also interesting to read the accompanying </font></font><a href="https://arxiv.org/pdf/1808.07371.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">scientific work</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One can imagine that, using all the techniques we have examined, it is possible to train a network that receives an image of a dancer and tells you where his arms and legs are. </font><font style="vertical-align: inherit;">And in this case, obviously, at some level, the network learned how to connect the pixels in the image with the location of human limbs. </font><font style="vertical-align: inherit;">Considering that a neural network is simply data stored on a computer, rather than a biological brain, it should be possible to take this data and go in the opposite direction ‚Äî to get pixels corresponding to the location of the limbs.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Start with a network that extracts poses from images of people. </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MO models capable of doing this are called generative [Eng. generate - generate, produce, create / approx. trans.]. All previous models considered by us are called discriminatory [eng. discriminate - distinguish / approx. trans.]. The difference between them can be imagined as follows: a discriminatory model for cats looks at photos and distinguishes between photos containing cats and photos where there are none. The generative model creates images of cats based on, say, a description of what a cat should be. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d37/1c3/812/d371c38126f7a73f84782ea1775ed3d2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generative models that ‚Äúdraw‚Äù images of objects are created using the same SNS structures as the models used to recognize these objects. And these models can be trained basically the same way as other models of MO.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, the trick is to come up with an ‚Äúassessment‚Äù for their learning. When teaching a discriminatory model, there is an easy way to assess the correctness and incorrectness of the answer - such as whether the network distinguished the dog from the cat correctly. However, how to assess the quality of the resulting picture of a cat, or its accuracy? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And here for a person who loves conspiracy theories and believes that we are all doomed, the situation becomes a bit scary. You see, the best way we have invented to train generative networks is not to do it yourself. To do this, we simply use another neural network.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This technology is called the generative-contention network, or GSS. You force two neural networks to compete with each other: one network tries to create fakes, for example, by drawing a new dancer on the basis of the old poses. Another network is trained to find the difference between real and fake examples using heaps of examples of real dancers. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And these two networks are playing a competitive game. Hence the word "competitive" in the title. The generative network is trying to make convincing counterfeits, and the discriminatory network is trying to understand where the fake is and where the real thing is.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the case of a video with a dancer, a separate discriminatory network was created during the training process, which produced simple yes / no answers. She looked at the image of a person and the description of the position of his limbs, and decided whether the image was a real photograph or a picture drawn by the generative model. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bda/7d8/941/bda7d8941659939b72dee44d4ca3b394.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The GSS forces the two networks to compete with each other: one produces ‚Äúfakes‚Äù and the other tries to distinguish the fake from the original. </font></font></i> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/84d/77d/c61/84d77dc61b0d11d411225fc25cba54e3.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the final workflow, only the generative model is used that creates the necessary images.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During repeated rounds of training, the models got better and better. It‚Äôs like a jewelery expert‚Äôs competition with an appraisal expert ‚Äî competing with a strong contender, each one of them becomes stronger and smarter. Finally, when the work of the models is good enough, you can take a generative model and use it separately. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Generative models after training can be very useful for creating content. For example, they can generate images of faces (which can be used to train face recognition programs), or backgrounds for video games. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For all this to work properly, a lot of work on adjustments and corrections is required, but in fact the person here acts as an arbitrator. It is AI that work against each other, making major improvements.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> So, should we expect Skynet and Hal 9000 in the near future? </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In each documentary film about nature, there is an episode in the end, where the authors talk about how all this great beauty will soon disappear because of how terrible people are. </font><font style="vertical-align: inherit;">I think that in the same vein, every responsible discussion regarding AI should include a section on its limitations and social consequences. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, let us once again emphasize the current limitations of AI: the main idea, which I hope you have learned from reading this article, is that the success of MO or AI is extremely dependent on the learning models we have chosen. </font><font style="vertical-align: inherit;">If people poorly organize the network or use unsuitable materials for training, then these distortions can be very obvious to all.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Deep neural networks are incredibly flexible and powerful, but do not have magical properties. Despite the fact that you use deep neural networks for RNS and SNS, their structure is very different, and therefore people should still define it. So, even if you can take the SNS for cars, and retrain it for bird recognition, you cannot take this model and retrain it for speech recognition. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we describe it in human terms, then everything looks as if we understood how the visual cortex and the auditory cortex work, but we have no idea how the cerebral cortex works, and from where you can begin to approach it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This means that in the near future, we probably will not see a Hollywood god-like AI. But this does not mean that in its current form, AI cannot have a serious impact on society.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We often imagine how the AI ‚Äã‚Äã"replaces" us, that is, how the robots literally do our work, but in reality this will not happen. </font><font style="vertical-align: inherit;">Look, for example, at radiology: sometimes people, looking at the success of computer vision, say that AI will replace radiologists. </font><font style="vertical-align: inherit;">Perhaps we will not reach a point where we don‚Äôt have a single human radiologist at all. </font><font style="vertical-align: inherit;">But a future is quite possible in which, for a hundred of today's radiologists, AI will allow five to ten of them to do the work of everyone else. </font><font style="vertical-align: inherit;">If such a scenario is implemented, where will the remaining 90 doctors go?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Even if the current generation of AI does not justify the hopes of its most optimistic supporters, it will still lead to very extensive consequences. </font><font style="vertical-align: inherit;">And we will have to solve these problems, so a good start is likely to be the master of the basics of this area.</font></font></div><p>Source: <a href="https://habr.com/ru/post/451214/">https://habr.com/ru/post/451214/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../451204/index.html">Free text editors for collaboration</a></li>
<li><a href="../451206/index.html">What is happening with RDF repositories?</a></li>
<li><a href="../451208/index.html">"Topological" sorting graph with cycles</a></li>
<li><a href="../451210/index.html">Jira integration with GitLab</a></li>
<li><a href="../451212/index.html">Arc protection system with the ability to operate on a current signal</a></li>
<li><a href="../451216/index.html">How to completely disable Windows Defender on Windows 10</a></li>
<li><a href="../451220/index.html">Technical details of recent Firefox extensions crash</a></li>
<li><a href="../451222/index.html">I'm not an engineer at my mother</a></li>
<li><a href="../451224/index.html">News of the week: collectors want access to Russian phones, new Linux kernel Linux 5.1, Samsung data leak</a></li>
<li><a href="../451226/index.html">New life of old games: the most open source-ports directory (Java + PHP)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>