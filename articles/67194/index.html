<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hacking captcha file sharing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 

 This article briefly describes the process of hacking captcha with ifolder.ru . Application in the process of the Python language and ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hacking captcha file sharing</h1><div class="post__text post__text-html js-mediator-article"><h4>  <strong>Introduction</strong> </h4><br><br>  This article briefly describes the process of hacking captcha with <a href="http://ifolder.ru/">ifolder.ru</a> .  Application in the process of the Python language and third-party libraries.  Using the <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A5%25D0%25B0%25D1%2584%25D0%25B0">Hough transform</a> algorithm as part of the <a href="http://sourceforge.net/projects/opencv/">Open Computer Vision</a> ¬© Intel library will allow us to get rid of image noise, the easy-to-use and fast <a href="http://leenissen.dk/">FANN</a> (Fast Artificial Neural Network) library will make it possible to use an artificial neural network for the image recognition problem. <br><br>  My motivation was, above all, to try the Python language.  As you know, the best way to learn a language is to solve some applied problem on it.  Therefore, in parallel with the description of the image processing process, I will talk about which libraries and for which I used. <br><a name="habracut"></a><br><h4>  <strong>Problem overview</strong> </h4>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We have the following captcha: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/0.PNG"><br>  ifolder.ru is a file sharing service that, when downloading and uploading, wants to make sure that you are not a robot.  The resource was taken because I have long wanted to apply the following Hough transformation algorithm to this task. <br><br>  What is the difficulty of recognizing this captcha?  There are several of them, we describe them in order of impact on the complexity of solving the problem: <br><br>  <strong>1.</strong> <em>The presence of intersections of characters.</em>  A good example of such cases: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/1.PNG"><br>  The percentage of such cases is relatively small, so we write them into a marriage with the note "not recognizable." <br><br>  <strong>2.</strong> <em>The presence of lines.</em>  On each image there are 4 lines of different lengths (and the length may be equivalent to the linear elements of recognizable objects), thickness and angle of inclination.  We regard them as the main element of noise that we have to get rid of. <br><br>  <strong>3.</strong> <em>A large variation in the arrangement of characters in the image.</em>  Symbols are located at different levels, at different distances. <br><br>  <strong>4.</strong> <em>Rotate characters.</em>  Symbols have an inclination along one axis, but no more than ~ 30 degrees (the value is obtained empirically). <br><br>  <strong>5.</strong> <em>Floating size and thickness of characters.</em> <br><br>  It looks simple enough captcha for more detailed study is not so simple.  :) But it's not so bad.  Let's start. <br><br><h4>  <strong>Stage 1. Creating a training set and preprocessing</strong> </h4><br><br>  To begin with, we download several hundred captcha samples from the site, say 500. This is enough to work out the algorithms and create a primary training sample for our neural network. <br>  With the help of the urllib library and a plain script, we download the n-th number of required samples from the site.  After that we convert them from gif to 8-bit bitmap, with this format we will continue to work.  The important point is the inversion of the image.  Those.  white objects on a black background.  Later it will be clear why this is. <br><br>  The script that performs all of the above: <br><blockquote>  <font color="#ff7700">from</font> <font color="#dc143c">urllib2</font> <font color="#ff7700">import</font> urlopen <br>  <font color="#ff7700">from</font> <font color="#dc143c">urllib</font> <font color="#ff7700">import</font> urlretrieve <br>  <font color="#ff7700">from</font> PIL <font color="#ff7700">import</font> Image, ImageOps, ImageEnhance <br>  <font color="#ff7700">import</font> <font color="#dc143c">os</font> <br>  <font color="#ff7700">import</font> <font color="#dc143c">sys</font> <br>  <font color="#ff7700">import</font> <font color="#dc143c">re</font> <br>  <font color="#ff7700">import</font> <font color="#dc143c">time</font> <br><br>  <font color="#ff7700">def</font> main <font>(</font> url, n <font>)</font> : <br>  <font color="#808080"># get url session url</font> <br>  data = urlopen <font>(</font> url <font>)</font> .  <font>read</font> <font>(</font> <font>)</font> <br>  match = <font color="#dc143c">re</font> .  <font>search</font> <font>(</font> r <font color="#483d8b">"/ random / images / <font color="#000099">\?</font> session = [a-z0-9] + <font color="#000099">\ &amp;</font> quot;,</font> data <font>)</font> <br>  <font color="#ff7700">if</font> match: <br>  imgurl = <font color="#483d8b">" <a href="">ifolder.ru"</a></font> + match.  <font>group</font> <font>(</font> <font>)</font> <br>  <font color="#ff7700">else</font> : <br>  <font color="#ff7700">return</font> - <font color="#ff4500">1</font> <br><br>  <font color="#808080"># gen imgs</font> <br>  <font color="#ff7700">for</font> i <font color="#ff7700">in</font> <font color="#008000">range</font> <font>(</font> n <font>)</font> : <br>  urlretrieve <font>(</font> imgurl, <font color="#483d8b">'/ test /'</font> + <font color="#008000">str</font> <font>(</font> i <font>)</font> + <font color="#483d8b">'.gif'</font> <font>)</font> <br>  <font color="#dc143c">time</font> .  <font>sleep</font> <font>(</font> <font color="#ff4500">1</font> <font>)</font> <br>  <font color="#ff7700">print</font> <font color="#008000">str</font> <font>(</font> i <font>)</font> + <font color="#483d8b">'of'</font> + <font color="#008000">str</font> <font>(</font> n <font>)</font> + <font color="#483d8b">'downloaded'</font> <br><br>  <font color="#808080"># convert them</font> <br>  <font color="#ff7700">for</font> i <font color="#ff7700">in</font> <font color="#008000">range</font> <font>(</font> n <font>)</font> : <br>  img = Image.  <font color="#008000">open</font> <font>(</font> <font color="#483d8b">'/ test /'</font> + <font color="#008000">str</font> <font>(</font> i <font>)</font> + <font color="#483d8b">'.gif'</font> <font>)</font> .  <font>convert</font> <font>(</font> <font color="#483d8b">'L'</font> <font>)</font> <br>  img = ImageOps.  <font>invert</font> <font>(</font> img <font>)</font> <br>  img = ImageEnhance.  <font>Contrast</font> <font>(</font> img <font>)</font> .  <font>enhance</font> <font>(</font> <font color="#ff4500">1.9</font> <font>)</font> <br>  img.  <font>save</font> <font>(</font> <font color="#483d8b">'/ test /'</font> + <font color="#008000">str</font> <font>(</font> i <font>)</font> + <font color="#483d8b">'.bmp'</font> <font>)</font> <br>  <font color="#808080"># os.unlink ('/ test /' + str (i) + '.gif')</font> <br><br><br>  <font color="#ff7700">if</font> __name__ == <font color="#483d8b">"__main__"</font> : <br>  url = <font color="#dc143c">sys</font> .  <font>argv</font> <font>[</font> - <font color="#ff4500">1</font> <font>]</font> <br>  <font color="#ff7700">if</font> <font color="#ff7700">not</font> url.  <font>lower</font> <font>(</font> <font>)</font> .  <font>startswith</font> <font>(</font> <font color="#483d8b">"http"</font> <font>)</font> : <br>  <font color="#ff7700">print</font> <font color="#483d8b">"usage: python dumpimages.py http://ifolder.com/?num"</font> <br>  <font color="#dc143c">sys</font> .  <font>exit</font> <font>(</font> - <font color="#ff4500">1</font> <font>)</font> <br>  main <font>(</font> url, <font color="#ff4500">500</font> <font>)</font> </blockquote><br><br><h4>  <strong>Stage 2. Noise removal, localization and separation of objects.</strong> </h4><br><br>  The most interesting and time consuming stage.  Examples of captcha that I showed in the review, we will take a sample in this article and will work with them further.  So, after the first stage, we have the following: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/2.PNG"><br><br>  I used the <a href="http://www.pythonware.com/products/pil/">PIL</a> library to work with images.  Easy to use as a hoe, but quite functional and very convenient library. <br><br>  Let's return to our sheep.  In this case, by noise, I mean lines. <br>  As a solution to the problem, I see several options: <br>  1. Genetic algorithms. <br>  2. Conversion Hough.  Can be considered as a type of automatic vectorization. <br><br>  GA were covered on Habr√© several times, including in the process of solving a similar task of hacking <a href="http://habrahabr.ru/blogs/artificial_intelligence/64535/">captcha Yandex</a> .  It is not difficult to write a modification of the genetic algorithm for the detection of straight lines. <br><br>  Nevertheless, I made a choice in favor of the second algorithm.  Compared to GA, Hough's transformations are a mathematically more rigorous and deterministic algorithm, in which there is no influence of a random factor.  In this case, it is less resource-intensive, at the same time is simple enough for understanding and application. <br>  Briefly, the meaning of the algorithm is that any straight line on a plane can be defined by two variables - the angle of inclination and the distance from the origin (theta, r).  These variables can be considered as signs; they form their own two-dimensional space.  Since a straight line is a collection of points and each of them has its own pair of signs (theta, r), then in the space of these signs we will have clusters of points (maxima or peaks at the intersection) within the final neighborhoods of the signs corresponding to the points of the straight line on the original plane (image) .  But everything is simpler than it seems.  :) <br>  More information can be found in <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A5%25D0%25B0%25D1%2584%25D0%25B0">Wikipedia</a> and see the visualization of the algorithm <a href="http://www.rob.cs.tu-bs.de/content/04-teaching/06-interactive/HNF.html">here</a> .  Immediately it becomes clear what they mean. <br><br>  Writing the implementation yourself is naturally lazy.  In addition, it is in the <a href="http://sourceforge.net/projects/opencv/">OpenCV library</a> , with which I often work on C / C ++.  There are bindings for Python, which are easily assembled and installed. <br><br>  In general, OpenCV is quite a low-level library and working with it on python is not very convenient, so the authors have provided adapters for conversion to the format of PIL objects.  This is done very simply: <br><br><blockquote>  src = cvLoadImage <font>(</font> <font color="#483d8b">'image.bmp'</font> , <font color="#ff4500">1</font> <font>)</font> <font color="#808080"># OpenCV object</font> <br>  pil_image = adapters.  <font>Ipl2PIL</font> <font>(</font> src <font>)</font> <font color="#808080"># PIL object</font> </blockquote><br><br>  The procedure for deleting lines is as follows: <br><blockquote>  <font color="#ff7700">def</font> RemoveLines <font>(</font> img <font>)</font> : <br>  dst = cvCreateImage <font>(</font> cvGetSize <font>(</font> img <font>)</font> , IPL_DEPTH_8U, <font color="#ff4500">1</font> <font>)</font> <br>  cvCopy <font>(</font> img, dst <font>)</font> <br>  storage = cvCreateMemStorage <font>(</font> <font color="#ff4500">0</font> <font>)</font> <br>  lines = cvHoughLines2 <font>(</font> img, storage, CV_HOUGH_PROBABILISTIC, <font color="#ff4500">1</font> , CV_PI / <font color="#ff4500">180</font> , <font color="#ff4500">35</font> , <font color="#ff4500">35</font> , <font color="#ff4500">3</font> <font>)</font> <br>  <font color="#ff7700">for</font> line <font color="#ff7700">in</font> lines: <br>  cvLine <font>(</font> dst, line <font>[</font> <font color="#ff4500">0</font> <font>]</font> , line <font>[</font> <font color="#ff4500">1</font> <font>]</font> , bgcolor, <font color="#ff4500">2</font> , <font color="#ff4500">0</font> <font>)</font> <br>  <font color="#ff7700">return</font> dst </blockquote><br><br>  Images should be monochrome, meaningful white pixels.  That is why we inverted the image at the first stage and will invert it during recognition. <br>  The key point is the function call <em>cvHoughLines2</em> .  Attention should be paid to the <em>CV_HOUGH_PROBABILISTIC</em> parameter, which means the use of a more ‚Äúsmart‚Äù modification of the algorithm.  The last three parameters are also very important, they reflect: the number of points in the feature space cell;  minimum line length;  and the maximum space (gap), i.e.  The number of missing pixels on the line.  More information in the library <a href="http://opencv.willowgarage.com/documentation/image_processing.html">documentation</a> . <br>  It is very important to choose these parameters correctly, otherwise we will remove the straight lines that are part of the characters from the image or, on the contrary, leave a lot of noise.  I believe that the parameters I chose are optimal, but far from ideal.  For example, let's double the maximum gap.  This will lead to this effect: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/3.PNG"><br><br>  Together with the lines we removed a lot of useful information.  At the same time, properly selected parameters allow achieving an acceptable result: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/2.PNG"><br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/4.PNG"><br><br>  You have already noticed that, since the lines often intersect the characters, we inevitably remove the useful information too.  This is not fatal, in part we will be able to restore it later and ultimately everything will depend on how well our neural network is trained. <br><br>  The next task is localization and separation of characters.  Here the problems described in points 1 and 3 in the review arise.  The floating position of the symbols and rotation do not allow us to rely on the same coordinates and location.  The characters often ‚Äútouch‚Äù, which prevents us from applying any algorithm from the contours detection series. <br>  It is clear that it is necessary to divide vertically.  Without thinking twice, we calculate the number of white pixels in each column of the image and display them in the window: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/6.PNG"><img src="http://www.geliosmi.com/images/stories/articles/ifolder/7.PNG"><br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/8.PNG"><img src="http://www.geliosmi.com/images/stories/articles/ifolder/5.PNG"><br><br>  To build the graphics I used the <a href="http://matplotlib.sourceforge.net/">matplotlib</a> library.  The library is striking in its flexibility and inherent functionality; I have not seen anything like it in other languages.  <a href="http://www.riverbankcomputing.co.uk/software/pyqt/intro">PyQt4 was</a> used as the front-end GUI. <br><br>  If you compare the graphics with the image, then you can see the presence of 3 local minima.  According to them, we will ‚Äúcrop‚Äù the image.  The optimal algorithm for finding the minima in this case is difficult to come up with, if there is one at all.  Therefore, I implemented a simple local minimum search algorithm, the parameters were obtained empirically, and it is far from optimal.  This is an important point and a more thoughtful algorithm can significantly improve the quality of recognition. <br>  The procedure for dividing the image into symbols can be found in the source code (FindDividingCols and DivideDigits). <br><br>  Next, we cut the characters because  there is a lot of background area left.  After you can try to recover lost useful information.  I can advise you to apply <a href="http://en.wikipedia.org/wiki/Mathematical_morphology">morphological algorithms</a> , such as Erosion &amp; Dilation (Erosion and Dilatation) or Closing (Closure).  They can be found in the OpenCV library, an example of use on python is in the library repository - OpenCV \ samples \ python \ morphology.py.  All obtained images of individual characters are reduced to a single size of 18x24. <br><br>  The result of the division into characters: <br><img src="http://www.geliosmi.com/images/stories/articles/ifolder/9.PNG"><br><br><h4>  <strong>Stage 3. Recognition</strong> </h4><br><br>  The next stage is the creation of a neural network and its training.  Out of 500 images (4 characters each) I received less than 1000 samples of acceptable quality and content used for training.  If we train the network to the level of recognition of a single character with a probability of 0.5, then we obtain the total efficiency 0.5 ^ 4 = 0.0625 or 6%.  The goal is more than achievable.  The resulting sample was enough for her.  If you have a desire to work "Chinese" for several days, then the probability of achieving the best results is great, the main thing is patience, which I do not have.  :) <br>  To create and use neural networks, it is convenient to use the <a href="http://leenissen.dk/">FANN</a> library.  Wrapper for python without a file did not want to gather, I had to edit the code obtained by SWIG.  I decided to post a compiled library, an installer for python 2.6 and a few examples of usage.  Download <a href="http://depositfiles.com/files/t8wjgbkca">here</a> .  I wrote small installation instructions, see INSTALL. <br><br>  The input is an array of 18 * 24 = 432 pixels (more precisely, we transfer 1 if the pixel is significant and 0 if the background), at the output we get an array of 10 numbers, each of which reflects the probability that the input array belongs to a particular class (digit).  Thus, the input layer of our neural network consists of 432 neurons, the output of 10. There is another hidden layer with the number of neurons == 432/3. <br><br>  The code for creating and learning network: <br><blockquote>  <font color="#ff7700">from</font> pyfann <font color="#ff7700">import</font> libfann <br><br>  num_input = <font color="#ff4500">432</font> <br>  num_output = <font color="#ff4500">10</font> <br>  num_layers = <font color="#ff4500">3</font> <br>  num_neurons_hidden = <font color="#ff4500">144</font> <br>  desired_error = <font color="#ff4500">0.00006</font> <br>  max_epochs = <font color="#ff4500">50000</font> <br>  epochs_between_reports = <font color="#ff4500">1000</font> <br><br>  ann = libfann.  <font>neural_net</font> <font>(</font> <font>)</font> <br><br>  ann.  <font>create_standard</font> <font>(</font> num_layers, num_input, num_neurons_hidden, num_output <font>)</font> <br>  ann.  <font>set_activation_function_hidden</font> <font>(</font> libfann. <font>SIGMOID_SYMMETRIC_STEPWISE</font> <font>)</font> <br>  ann.  <font>set_activation_function_output</font> <font>(</font> libfann. <font>SIGMOID_SYMMETRIC_STEPWISE</font> <font>)</font> <br><br>  ann.  <font>train_on_file</font> <font>(</font> <font color="#483d8b">'samples.txt'</font> , max_epochs, epochs_between_reports, desired_error <font>)</font> <br><br>  ann.  <font>save</font> <font>(</font> <font color="#483d8b">'fann.data'</font> <font>)</font> <br>  ann.  <font>destroy</font> <font>(</font> <font>)</font> </blockquote><br><br>  Using: <br><blockquote>  <font color="#ff7700">def</font> MagicRegognition <font>(</font> img, ann <font>)</font> : <br>  ann = libfann.  <font>neural_net</font> <font>(</font> <font>)</font> <br>  ann.  <font>create_from_file</font> <font>(</font> <font color="#483d8b">'fann.data'</font> <font>)</font> <br><br>  sample = <font>[</font> <font>]</font> <br>  <font color="#ff7700">for</font> i <font color="#ff7700">in</font> img.  <font>size</font> <font>[</font> <font color="#ff4500">1</font> <font>]</font> : <br>  <font color="#ff7700">for</font> j <font color="#ff7700">in</font> img.  <font>size</font> <font>[</font> <font color="#ff4500">0</font> <font>]</font> : <br>  <font color="#ff7700">if</font> colordist <font>(</font> img. <font>getpixel</font> <font>(</font> <font>(</font> j, i <font>)</font> <font>)</font> , bgcolor <font>)</font> <font color="#66cc66">&lt;</font> <font color="#ff4500">10</font> : <br>  sample <font>[</font> j + i <font color="#66cc66">*</font> img.  <font>size</font> <font>[</font> <font color="#ff4500">0</font> <font>]</font> <font>]</font> = <font color="#ff4500">0</font> <br>  <font color="#ff7700">else</font> : <br>  sample <font>[</font> j + i <font color="#66cc66">*</font> img.  <font>size</font> <font>[</font> <font color="#ff4500">0</font> <font>]</font> <font>]</font> = <font color="#ff4500">1</font> <br><br>  res = ann.  <font>run</font> <font>(</font> sample <font>)</font> <br><br>  <font color="#ff7700">return</font> res.  <font>index</font> <font>(</font> <font color="#008000">max</font> <font>(</font> res <font>)</font> <font>)</font> </blockquote><br><br><h4>  <strong>Conclusion</strong> </h4><br><br>  Python is a great language, very concise and beautiful, with many third-party libraries that cover all my everyday needs.  Low threshold of entry, largely due to the solid community and the amount of documentation.  Pythoners, now I'm with you;) <br><br>  Additional libraries used: <br>  <a href="http://numpy.scipy.org/">Numpy</a> <br>  <a href="http://www.scipy.org/">Scipy</a> <br><br>  <a href="http://rapidshare.com/files/267991721/Captcha.zip.html">Sources</a> ( <a href="http://depositfiles.com/files/643hjf949">mirror 1</a> , <a href="http://ifolder.ru/13576091">mirror 2</a> ) <br><br>  For highlight syntax, the <a href="http://highlight.hohli.com/">highlight.hohli.com</a> resource was used. <br><br>  <b>UPD:</b> 1. Perezalil source for 3 resources.  2. Upon request, from the dumpimages.py file, removed the last three characters in the regular expression for the link to the captcha on the ifolder page.  And then the "children" indulge :) <br><br></div><p>Source: <a href="https://habr.com/ru/post/67194/">https://habr.com/ru/post/67194/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../67180/index.html">Working with http via non-blocking sockets</a></li>
<li><a href="../67185/index.html">An example of using custom events</a></li>
<li><a href="../67190/index.html">Debian Day 2009</a></li>
<li><a href="../67192/index.html">Installing Ubuntu Linux from the hard drive. Script</a></li>
<li><a href="../67193/index.html">What is good flexmojos? Flex Development in IntellliJ IDEA using maven</a></li>
<li><a href="../67195/index.html">Trillian Astra 4.0 final</a></li>
<li><a href="../67196/index.html">Maybe this is a photo of one of the prototypes of Apple's Tablet PC?</a></li>
<li><a href="../67197/index.html">radikal.ru has prohibited the use of the service on torrents.ru</a></li>
<li><a href="../67198/index.html">Clock with a broken arrow</a></li>
<li><a href="../67199/index.html">The Federal Antimonopoly Service continues to eradicate excessive provider sway</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>