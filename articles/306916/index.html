<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Stylization of images using neural networks: no mysticism, just matan</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Greetings to you, Habr! Surely, you have noticed that the theme of the stylization of photographs for various artistic styles is actively discussed on...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Stylization of images using neural networks: no mysticism, just matan</h1><div class="post__text post__text-html js-mediator-article"><p>  Greetings to you, Habr!  Surely, you have noticed that the theme of the stylization of photographs for various artistic styles is actively discussed on these web sites of yours.  Reading all these popular articles, you might think that magic is going on under the hood of these applications, and the neural network really fantasizes and redraws the image from scratch.  It just so happened that our team was faced with a similar task: as part of the intra-corporate hackathon, we made <a href="https://hi-tech.mail.ru/news/artisto-neural-network-app/">video styling</a> , because  application for photos already.  In this post, we'll figure out how this network "redraws" images, and analyze the articles that made this possible.  I recommend to get acquainted with the <a href="https://habrahabr.ru/company/mailru/blog/252965/">previous post</a> before reading this material and in general with the basics of <a href="https://habrahabr.ru/company/yandex/blog/307260/">convolutional neural networks</a> .  You will find a few formulas, a bit of code (I will give examples on <a href="https://github.com/Theano/Theano">Theano</a> and <a href="https://github.com/Lasagne/Lasagne">Lasagne</a> ), as well as a lot of pictures.  This post is built in the chronological order of the articles and, accordingly, the ideas themselves.  Sometimes I will dilute it with our recent experience.  Here you have a boy from hell to attract attention. </p><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/wOjZDo1NXaA%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgXgbuHc3qVYI8G-g_VGgkZR0uWbg" frameborder="0" allowfullscreen=""></iframe><a name="habracut"></a><br><h1>  <a href="https://arxiv.org/pdf/1311.2901v3.pdf">Visualizing and Understanding Convolutional Networks</a> (28 Nov 2013) </h1><br><p>  The first thing worth mentioning is an article in which the authors were able to show that the neural network is not a black box, but a completely interpretable thing (by the way, today it can be said <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">not only about convolutional networks</a> for computer vision).  The authors decided to learn to interpret the activation of neurons of hidden layers, for this they used the <a href="http://www.matthewzeiler.com/pubs/iccv2011/iccv2011.pdf">deconvolutionary neural network</a> (deconvnet) proposed several years earlier (by the way, by the same Zeiler and Fergus, who are the authors of this publication).  A deconvolutional network is actually the same network with convolutions and pooling, but applied in the reverse order.  In the original work on deconvnet, the network was used in unsupervised learning mode for generating images.  This time, the authors applied it simply for a backward pass from the signs obtained after a direct pass through the network to the original image.  The result is an image that can be interpreted as a signal that caused this activation on neurons.  Naturally, the question arises: how to make a reverse pass through convolution and nonlinearity?  And even more so through max-pooling, this is certainly not an invertible operation.  Consider all three components. </p><br><h2>  Reverse ReLu </h2><br><p>  In convolution networks, as the activation function, <em>ReLu (x) = max (0, x)</em> is often used, which makes all activations on the layer not negative.  Accordingly, when going back through nonlinearity, non-negative results must also be obtained.  For this, the authors propose to use the same ReLu.  From the point of view of the Theano architecture, it is necessary to redefine the gradient function of the operation (the <a href="https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%2520Maps%2520and%2520Guided%2520Backpropagation.ipynb">infinitely valuable laptop</a> is in the <a href="https://github.com/Lasagne/Recipes/">lasagna recipes</a> , from there you will get the details of what is behind the ModifiedBackprop class). </p><br><img src="https://habrastorage.org/files/833/dd2/eda/833dd2eda9334d039dd21a7aa2eb8294.gif"><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ZeilerBackprop</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ModifiedBackprop)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">grad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, inputs, out_grads)</span></span></span><span class="hljs-function">:</span></span> (inp,) = inputs (grd,) = out_grads <span class="hljs-comment"><span class="hljs-comment">#return (grd * (grd &gt; 0).astype(inp.dtype),) # explicitly rectify return (self.nonlinearity(grd),) # use the given nonlinearity</span></span></code> </pre> <br><h2>  Reverse convolution </h2><br><p>  It is a bit more complicated here, but everything is logical: it is enough to apply the transposed version of the same convolution kernel, but to the outputs from the reverse ReLu instead of the previous layer used in the forward pass.  But I'm afraid that in words it is not so obvious, let's look at the visualization of this procedure ( <a href="https://github.com/vdumoulin/conv_arithmetic">here</a> you will find even more visualizations of bundles). </p><br><div class="spoiler">  <b class="spoiler_title">Convolution with stride = 1</b> <div class="spoiler_text"><table><tbody><tr><th>  Convolution with stride = 1 </th><th>  Back version </th></tr><tr><td><img src="https://habrastorage.org/files/43f/c7e/405/43fc7e405f324ceca9a983b34a4873fb.gif"></td><td><img src="https://habrastorage.org/files/41d/9cd/ed5/41d9cded50b04361ba9e03bec1c582f6.gif"></td></tr></tbody></table></div></div><br><div class="spoiler">  <b class="spoiler_title">Convolution with stride = 2</b> <div class="spoiler_text"><table><tbody><tr><th>  Convolution with stride = 2 </th><th>  Back version </th></tr><tr><td><img src="https://habrastorage.org/files/b91/895/f21/b91895f2111c418e974d8735a3c6583f.gif"></td><td><img src="https://habrastorage.org/files/dd0/982/8d3/dd09828d3c33412fbb024e69000df491.gif"></td></tr></tbody></table></div></div><br><h2>  Reverse pooling </h2><br><p>  This operation (unlike the previous ones) is generally not invertible.  But we would still like to go through a maximum in some way back.  To do this, the authors propose to use the map of where was the maximum with a direct pass (max location switches).  During the backward pass, the input signal is converted into an ampling in order to approximately preserve the structure of the original signal, it‚Äôs really easier to see than to describe. </p><br><img width="400" src="https://habrastorage.org/files/a33/4df/81a/a334df81a74b4dd7bd21c6254f93884b.png"><br><h2>  Result </h2><br><p>  The visualization algorithm is extremely simple: </p><br><ol><li>  Make a straight pass. </li><li>  Select the layer of interest. </li><li>  To fix the activation of one or more neurons and reset the rest. </li><li>  Make a reverse conclusion. </li></ol><br><p>  Each gray square in the image below corresponds to the visualization of the filter (which is used for convolution) or the weights of one neuron, and each color image is the part of the original image that activates the corresponding neuron.  For clarity, neurons within a single layer are grouped into subject groups.  In general, it suddenly turned out that the neural network learns exactly what Hubel and Wazel wrote about <a href="http://cns-alumni.bu.edu/~slehar/webstuff/pcave/hubel.html">in the work on the structure of the visual system</a> , for which they were awarded the Nobel Prize in 1981.  Thanks to this article, we got a visual representation of what a convolutional neural network learns on each layer.  It is this knowledge that will allow later to manipulate the contents of the generated image, but this is still far away, the next few years were spent on improving the methods of "trepanning" neural networks.  In addition, the authors of the article proposed a method for analyzing how best to build the convolutional neural network architecture to achieve the best results (although they did not win ImageNet 2013, but they got to the top; <strong>UPD</strong> : it turns out they won, Clarifai is what they are). </p><br><div class="spoiler">  <b class="spoiler_title">Visualization of features</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/812/051/b71/812051b7114040dca73543223477b9bc.png"></div></div><br><p>  Here is an example of visualization of activations using deconvnet, today this result looks already so-so, but then it was a breakthrough. </p><br><div class="spoiler">  <b class="spoiler_title">Saliency Maps using deconvnet</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/06f/9ae/e59/06f9aee596b2461eac93bb9ca603cd04.jpeg"></div></div><br><h1>  <a href="https://arxiv.org/pdf/1312.6034.pdf">Deep Inside Convolutional Networks: Visualizing Image Classification Models and Saliency Maps</a> (19 Apr 2014) </h1><br><p>  This article is devoted to the study of the methods of knowledge visualization, enclosed in a convolutional neural network.  The authors propose two visualization methods based on gradient descent. </p><br><h2>  Class Model Visualization </h2><br><p>  So, imagine that we have a trained neural network to solve the problem of classification into a certain number of classes.  Denote by <img src="https://habrastorage.org/files/51f/987/7c3/51f9877c375d437891317f83884f16dc.gif">  activation value of the output neuron, which corresponds to class <em>c</em> .  Then the following optimization problem gives us exactly the image that maximizes the selected class: </p><br><img src="https://habrastorage.org/files/bd7/cc0/e03/bd7cc0e0328e47a4a5309a24930690a3.gif"><br><p>  This task is easy to solve using Theano.  Usually we ask the framework to take a derivative with respect to the model parameters, but this time we assume that the parameters are fixed and the derivative is taken from the input image.  The following function selects the maximum value of the output layer and returns a function that calculates the derivative of the input image. </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compile_saliency_function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(net)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Compiles a function to compute the saliency maps and predicted classes for a given minibatch of input images. """</span></span> inp = net[<span class="hljs-string"><span class="hljs-string">'input'</span></span>].input_var outp = lasagne.layers.get_output(net[<span class="hljs-string"><span class="hljs-string">'fc8'</span></span>], deterministic=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) max_outp = T.max(outp, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) saliency = theano.grad(max_outp.sum(), wrt=inp) max_class = T.argmax(outp, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> theano.function([inp], [saliency, max_class])</code> </pre> <br><p>  You must have seen strange images on the Internet with dog faces - DeepDream.  In the original article, the authors use the following process to generate images that maximize the selected class: </p><br><ol><li>  Initialize the initial image with zeros. </li><li>  Calculate the derivative value of this image. </li><li>  Change the image by adding to it the resulting image from the derivative. </li><li>  Return to step 2 or exit the loop. </li></ol><br><p>  These are the following images: </p><br><img src="https://habrastorage.org/files/981/5cb/bc3/9815cbbc331b40bda3f0233d060846c0.png"><br><p>  And if you initialize the first image of a real photo and run the same process?  But at each iteration we will choose a random class, reset the others and calculate the value of the derivative, then we will get such a deep dream. </p><br><div class="spoiler">  <b class="spoiler_title">Caution 60 mb</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/8e8/355/4e9/8e83554e9d9046f393e492450934ae52.gif"></div></div><br><p>  Why are there so many dog ‚Äã‚Äãfaces and eyes?  It's simple: there are almost 200 dogs in 1000 imad from 1000 classes, they have eyes.  And also many classes where people are just there. </p><br><h1>  Class Saliency Extraction </h1><br><p>  If we initialize this process with a real photo, stop after the first iteration and draw the value of the derivative, we will get such an image, adding it to the original one, we will increase the activation value of the selected class. </p><br><div class="spoiler">  <b class="spoiler_title">Saliency Maps using the derivative</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/1b8/344/8d4/1b83448d47eb448bb366325bbe348bc6.jpeg"></div></div><br><p>  Again the result is "so-so."  It is important to note that this is a new way of visualizing activations (nothing prevents us from fixing the values ‚Äã‚Äãof activations not on the last layer, but generally on any layer of the network and taking a derivative of the input image).  The following article will combine both of the previous approaches and give us a tool for how to set up a transfer style that will be described later. </p><br><h1>  <a href="https://arxiv.org/pdf/1412.6806v3.pdf">Striving for Simplicity: The All Convolutional Net</a> (13 Apr 2015) </h1><br><p>  This article is generally not about visualization, but about the fact that replacing a pooling with a convolution with a large straid does not lead to a loss of quality.  But as a by-product of their research, the authors proposed a new way of visualizing the features, which they used to more accurately analyze what the model was learning.  Their idea is as follows: if we simply take a derivative, then with deconvolution, those features that were less than zero in the input image do not go back (using ReLu for the input image).  And this leads to the fact that negative values ‚Äã‚Äãappear on the propped back image.  On the other hand, if you use deconvnet, then another ReLu is taken from the ReLu derivative - this allows you not to skip back negative values, but as you have seen the result is ‚Äúso-so‚Äù.  But what if you combine these two methods? </p><br><img src="https://habrastorage.org/files/093/4a6/5fc/0934a65fc4bb4720b653ba8c4d301ea7.png"><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">GuidedBackprop</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(ModifiedBackprop)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">grad</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, inputs, out_grads)</span></span></span><span class="hljs-function">:</span></span> (inp,) = inputs (grd,) = out_grads dtype = inp.dtype <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (grd * (inp &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>).astype(dtype) * (grd &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>).astype(dtype),)</code> </pre> <br><p>  Then get a completely clean and interpretable image. </p><br><div class="spoiler">  <b class="spoiler_title">Saliency Maps using Guided Backpropagation</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/76d/b81/858/76db81858e2947f295e0f0ee0afb7a65.jpeg"></div></div><br><h2>  Go deeper </h2><br><p>  Now let's think, what does this give us?  Let me remind you that each convolutional layer is a function that receives a three-dimensional tensor as an input and also produces a three-dimensional tensor as an output, perhaps another dimension <strong>d</strong> x <strong>w</strong> x <strong>h</strong> ;  <strong>d</strong> epth is the number of neurons in the layer, each of them generates a plate (feature map) of size <strong>w</strong> igth x <strong>h</strong> eight. </p><br><p>  Let's try the following experiment on the <a href="http://arxiv.org/pdf/1409.1556.pdf">VGG-19</a> network: </p><br><ul><li>  for each layer of the neural network, we will sort the plates by the value of the sum of activations inside the plates <img src="https://habrastorage.org/files/0c6/735/f1a/0c6735f1a05a40718df75e8421166c7c.gif">  - this will give us the most pronounced features on the image (in one plate there are activations of the same feature in different spatial coordinates); </li><li>  then on each plate we will choose the maximum element - this will give us the position where this feature is most clearly expressed; </li><li>  And now we take the derivative of the input image for a fixed value of one position on one plate and zeroed other values ‚Äã‚Äãin the layer - this will give us an idea of ‚Äã‚Äãthe feature, as well as the receptive area of ‚Äã‚Äãthis neuron (what area of ‚Äã‚Äãthe image this neuron is looking at). </li></ul><br><div class="spoiler">  <b class="spoiler_title">conv1_2</b> <div class="spoiler_text"><p>  Yes, you see almost nothing, because  the receptive region is very small, it is the second convolution of 3x3, respectively, the general area of ‚Äã‚Äã5x5.  But having increased, we will see that the feature is just a gradient detector. </p><br><img src="https://habrastorage.org/files/238/731/329/2387313294474789844cb17d7119f543.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">conv3_3</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/46d/a98/a38/46da98a38c694047a578801ec9e1d8ba.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">conv4_3</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/7d8/5e7/995/7d85e7995c314c868d5b3176df3cc4ba.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">conv5_3</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/d5e/a53/6da/d5ea536da32f4df38719138b63455371.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">pool5</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/424/229/deb/424229deb389451581579a7da42b1bdf.png"></div></div><br><p>  Now imagine that instead of a maximum on a plate, we will take the derived value of the sum of all the elements of the plate on the input image.  Then obviously the receptive region of the neuron group will cover the entire input image.  For the early layers, we will see bright maps, from which we conclude that these are color detectors, then gradients, then borders, and so on, in the direction of complicating patterns.  The deeper the layer, the duller the image is.  This is explained by the fact that the deeper layers have a more complex pattern, which they detect, and a complex pattern appears less frequently than a simple one, and therefore the activation map dims.  The first method is suitable for understanding layers with complex patterns, and the second one is for simple ones. </p><br><div class="spoiler">  <b class="spoiler_title">conv1_1</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/8f5/1c4/8ca/8f51c48cad214629a473f2c421030ba3.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">conv2_2</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/919/0c1/aee/9190c1aeed6d49eabf164d1ba47f42ac.png"></div></div><br><div class="spoiler">  <b class="spoiler_title">conv4_3</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/ffb/981/aa2/ffb981aa288b4246a045e7c263c50f2d.png"></div></div><br><p>  You can download a more complete database of activations for several images <a href="https://drive.google.com/file/d/0B4bl7YMqDnViMFRjZTZDZWdTRTA/view%3Fusp%3Dsharing">here</a> and <a href="https://drive.google.com/file/d/0B4bl7YMqDnViSmJkRkpCamVsSG8/view%3Fusp%3Dsharing">here</a> . </p><br><h1>  <a href="http://arxiv.org/pdf/1508.06576v2.pdf">A Neural Algorithm of Artistic Style</a> (2 Sep 2015) </h1><br><p>  So, a couple of years have passed since the first successful trepanning of the neural network.  We (in the sense of humanity) have a powerful tool in their hands, which allows us to understand what the neural network is learning, and also to remove what we don‚Äôt really want what it would learn.  The authors of this article are developing a method that allows one image to generate a similar card of activations for some target image, and perhaps even more than one - this is the basis of stylization.  At the entrance we give white noise, and with a similar iterative process as in deep dream we bring this image to one whose feature maps are similar to the target image. </p><br><h2>  Content loss </h2><br><p>  As already mentioned, each layer of the neural network produces a three-dimensional tensor of a certain dimension. </p><br><img src="https://habrastorage.org/files/311/d4b/ec4/311d4bec4021407b925f86982b584001.png"><br><p>  Denote the output of the <em>i-th</em> layer from the input as <img src="https://habrastorage.org/files/8e9/d24/cf8/8e9d24cf820a479bbcad6eb36de78d25.gif">  .  Then if we minimize the weighted sum of the residuals between the input image <img src="https://habrastorage.org/files/9d6/186/79c/9d618679cb5b4e6d8a1c02c52b475057.gif">  and by some image, to which we aspire <em>c</em> , we get exactly what we need.  Probably. </p><br><img src="https://habrastorage.org/files/a6a/dc6/55d/a6adc655d3a54dfa81b3cee074df1c2f.gif"><br><p>  For experiments with this article, you can use <a href="https://github.com/Lasagne/Recipes/blob/master/examples/styletransfer/Art%2520Style%2520Transfer.ipynb">this magic laptop</a> , there are calculations (both on the GPU and on the CPU).  The GPU is used to calculate the neural network features and the value of the cost function.  Theano returns a function that can calculate the gradient of the objective function <em>eval_grad</em> from the input image <em>x</em> .  This is then fed into lbfgs and an iterative process is started. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize with a noise image generated_image.set_value(floatX(np.random.uniform(-128, 128, (1, 3, IMAGE_W, IMAGE_W)))) x0 = generated_image.get_value().astype('float64') xs = [] xs.append(x0) # Optimize, saving the result periodically for i in range(8): print(i) scipy.optimize.fmin_l_bfgs_b(eval_loss, x0.flatten(), fprime=eval_grad, maxfun=40) x0 = generated_image.get_value().astype('float64') xs.append(x0)</span></span></code> </pre> <br><p>  If we launch the optimization of such a function, we will quickly get an image similar to the target one.  Now we are able to recreate from white noise images similar to some content-image. </p><br><div class="spoiler">  <b class="spoiler_title">Content Loss: conv4_2</b> <div class="spoiler_text"><p>  Content Image </p><br><img src="https://habrastorage.org/files/644/360/606/64436060627b455382e78b5c2b652f17.jpg"><br><p>  Optimization process </p><br><img src="https://habrastorage.org/files/485/f56/226/485f562268e74a45a7c7bbf28498e188.png"></div></div><br><p>  It is easy to notice two features of the resulting image: </p><br><ul><li>  lost colors - this is the result of the fact that in the concrete example only the conv4_2 layer was used (or, in other words, the weight w was non-zero for it, and zero for the other layers);  as you remember, it is the early layers that contain information about colors and gradient transitions, and the later ones contain information about larger details, which we observe ‚Äî colors are lost, but no content; </li><li>  some of the houses ‚Äúwent‚Äù, i.e.  straight lines are slightly curved - this is because the deeper layer, the less information about the spatial position of the feature it contains (the result of convolutions and pooling). </li></ul><br><p>  Adding early layers immediately corrects the situation with colors. </p><br><div class="spoiler">  <b class="spoiler_title">Content Loss: conv1_1, conv2_1, conv4_2</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/921/57d/bf8/92157dbf8caf4439ba4ccfe16779166e.png"></div></div><br><p>  I hope by this point you have felt that you can control what will be redrawn onto the image from the white noise. </p><br><h2>  Style loss </h2><br><p>  And so we got to the most interesting: how can we convey the style?  What is style?  Obviously, the style is not something that we optimized in Content Loss, because there is a lot of information about the spatial positions of features.  So the first thing to do is in some way remove this information from the views received at each layer. </p><br><p>  The author suggests the following method.  Take the tensor at the exit of a certain layer, turn on the spatial coordinates and calculate the covariance matrix between the dies.  We denote this transformation as <strong>G.</strong>  What did we actually do?  We can say that we calculated how often the signs inside the plate are found in pairs, or, in other words, we approximated the distribution of signs in the plates with a multidimensional normal distribution. </p><br><img src="https://habrastorage.org/files/e91/458/e96/e91458e9695f4ac290d5ce67ca0d6ac9.png"><br><p>  Then Style Loss is entered as follows, where <em>s</em> is an image with a style: </p><br><img src="https://habrastorage.org/files/e25/dfe/3f0/e25dfe3f0ad14c499568b77a236c31d9.gif"><br><p>  Let's try for Vincent?  In principle, we will get something expected - the noise in the Van Gogh style, the information about the spatial arrangement of features is completely lost. </p><br><div class="spoiler">  <b class="spoiler_title">Vincent</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/978/91b/855/97891b855a354603bbcb712393bb838a.jpg"><br><img src="https://habrastorage.org/files/7cb/7c8/6d2/7cb7c86d22e44a05a6b6684e811835d2.png"></div></div><br><p>  And what if instead of a style image to put a photo?  It will turn out already familiar features, familiar colors, but the spatial position is completely lost. </p><br><div class="spoiler">  <b class="spoiler_title">Photo at style loss</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/c17/4ae/f65/c174aef653914b51bd48eb2c78eec2a7.png"></div></div><br><p>  Surely you wondered about why we calculate the covariance matrix, and not something else?  After all, there are many ways to aggregate features so that spatial coordinates are lost.  This is really an open question, and if you take something very simple, the result will not change dramatically.  Let's check this, we will not calculate the covariance matrix, but simply the average value of each plate. </p><br><img src="https://habrastorage.org/files/603/cdc/e9a/603cdce9a64f45588f1ac8f57c4e2038.png"><br><div class="spoiler">  <b class="spoiler_title">simple style loss</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/cd1/287/b1b/cd1287b1be834152a5cdd8affd151dbd.png"></div></div><br><h2>  Combined Loss </h2><br><p>  Naturally, there is a desire to mix these two functions of value.  Then we will generate such an image from white noise that it will save the signs from the content-image (which have a binding to spatial coordinates), as well as the ‚Äústyle‚Äù signs that are not tied to spatial coordinates, i.e.  we will hope that the details of the image content will remain intact from their places, but will be redrawn with the right style. </p><br><img src="https://habrastorage.org/files/722/d7d/397/722d7d3970d7457bb8a8fbeab87df03f.gif"><br><p>  In fact, there is also a regularizer, but we omit it for simplicity.  It remains to answer the following question: which layers (weights) should be used for optimization?  And I am afraid that I have no answer to this question, and the authors of the article, too.  They have a suggestion to use the following, but this does not mean at all that another combination will work worse, too much search space.  The only rule that follows from the understanding of the model: it makes no sense to take the next layers, because  their signs will not differ much from each other, therefore a layer from each group of conv * _1 is added to the style. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Define loss function losses = [] # content loss losses.append(0.001 * content_loss(photo_features, gen_features, 'conv4_2')) # style loss losses.append(0.2e6 * style_loss(art_features, gen_features, 'conv1_1')) losses.append(0.2e6 * style_loss(art_features, gen_features, 'conv2_1')) losses.append(0.2e6 * style_loss(art_features, gen_features, 'conv3_1')) losses.append(0.2e6 * style_loss(art_features, gen_features, 'conv4_1')) losses.append(0.2e6 * style_loss(art_features, gen_features, 'conv5_1')) # total variation penalty losses.append(0.1e-7 * total_variation_loss(generated_image)) total_loss = sum(losses)</span></span></code> </pre> <br><p>  The final model can be represented as follows. </p><br><img src="https://habrastorage.org/files/ff1/4a2/cca/ff14a2ccaf7742b1ba73af1cad0e6279.png"><br><div class="spoiler">  <b class="spoiler_title">But the result of houses with Van Gogh.</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/fbf/31c/e4c/fbf31ce4c91d477c892edb4413127f0f.jpeg"><br><img src="https://habrastorage.org/files/65f/0ae/df5/65f0aedf54364332a752a9e35f799797.jpeg"></div></div><br><h2>  Attempt to control the process </h2><br><p>  Let's remember the previous parts, already two years before the current article, other scientists investigated what the neural network really learns.  Armed with all these articles, you can create visualizations of features of various styles, different images, different resolutions and sizes, and try to figure out which layers to take with what weight.  But even re-weighting the layers does not give full control over what is happening.  The problem here is more conceptual: <strong>we are optimizing the wrong function</strong> !  How so, you ask?  The answer is simple: this function minimizes the discrepancy ... well, you understand.  But what we really want is that we like the image.  The convex combination of content and style loss functions is not a measure of what our mind considers beautiful.  It was noted that if you continue styling for too long, the cost function naturally falls lower and lower, but the aesthetic beauty of the result drops sharply. </p><br><img src="https://habrastorage.org/files/dfe/fe2/81f/dfefe281fa5742cf9119913f6803a4ec.png"><br><p>  Well, okay, there is another problem.  Suppose we found a layer that retrieves the features we need.  Suppose some texture triangular.  But this layer still contains many other features, such as circles, which we really do not want to see in the resulting image.  Generally speaking, if one could hire a million Chinese, one could visualize all the features of the style image, and just look at all that we need, and only include them in the cost function.  But for obvious reasons, it is not so simple.  But what if we just remove all the circles that we don‚Äôt want to see on the result from the style image?  Then the activation of the corresponding neurons that react to the circles will simply not work.  And, of course, this will not appear in the resulting image.  The same with flowers.  Imagine a bright image with lots of color.  The distribution of colors will be very blurred over the entire space, the distribution of the resulting image will be the same, but in the process of optimization those peaks that were on the original will surely be lost.  It turned out that a simple decrease in the color depth of the color palette solves this problem.  The distribution density of most colors will be near zero, and there will be large peaks in several areas.  Thus, by manipulating the original in Photoshop, we manipulate the features that are extracted from the image.  It is easier for a person to express his desires visually, rather than trying to formulate them in the language of mathematics.  Until.  As a result, designers and managers, armed with photoshop and scripts for visualizing signs, achieved three times faster results better than the one that mathematicians did with programmers. </p><br><div class="spoiler">  <b class="spoiler_title">An example of the manipulation of color and size of features</b> <div class="spoiler_text"><table><tbody><tr><th>  Original </th><th>  Degraded version </th></tr><tr><td><img src="https://habrastorage.org/files/0cb/421/b50/0cb421b505b44875b26c4324647d94ba.JPG"></td><td><img src="https://habrastorage.org/files/e58/2db/758/e582db75898f4a16854beae0e3ba3ffc.png"></td></tr></tbody></table></div></div><br><div class="spoiler">  <b class="spoiler_title">And you can take a simple image as a style at once.</b> <div class="spoiler_text"><p>  Style </p><br><img src="https://habrastorage.org/files/090/94f/d6c/09094fd6cac9446293c8a00b8c825cff.jpg"><br><p>  results </p><br><img src="https://habrastorage.org/files/640/1ce/da5/6401ceda501147058a489da11a2536b1.jpg"><br><img src="https://habrastorage.org/files/c24/025/992/c240259922674893993b9c5a72c052cc.jpg"><br><img src="https://habrastorage.org/files/bbb/b62/e44/bbbb62e4405240b1b1c106af605450f6.jpg"></div></div><br><div class="spoiler">  <b class="spoiler_title">But vidosik, but only with the right texture</b> <div class="spoiler_text"><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/A0YKJ50gh2c%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhikSA9vvQEhhktCCVUOKUa339eWOg" frameborder="0" allowfullscreen=""></iframe></div></div><br><h1>  <a href="http://arxiv.org/pdf/1603.03417v1.pdf">Texture Networks: Feed-Forward Synthesis of Textures and Stylized Images</a> (Mar 10, 2016) </h1><br><p>       ,    .      .   ,  lbfgs   ,     .   ,      ,     10-15 .    .          .        17 ,      .   ,    ,    (    Style Loss    ).     ,    ,    ,    . </p><br><img src="https://habrastorage.org/files/020/e02/1eb/020e021eb3804518a7eafc0304305dde.png"><br><p>          ,     .         -.      ,    <em>z</em>           ,        .   -     , ..   Loss-  ,      . </p><br><img src="https://habrastorage.org/files/a18/cea/61c/a18cea61c54e4dc584a7a168ac2da442.png"><br><h1> <a href="https://arxiv.org/pdf/1603.08155.pdf">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a> (27 Mar 2016) </h1><br><p>    , ,     17     ,    .        <a href="https://arxiv.org/pdf/1512.03385v1.pdf">residual learning</a>   . </p><br><img src="https://habrastorage.org/files/93f/d75/ce8/93fd75ce88d54188b5c3e99da769a3ea.png"><br><p>  residual block  conv block. </p><br><img src="https://habrastorage.org/files/d3f/926/25f/d3f92625f80f475da50e771ee957af7a.png"><br><p>  ,               (          ). </p><br><h1>  Ending </h1><br><p>               <del>      </del>     : </p><br><ul><li> <a href="https://play.google.com/store/apps/details%3Fid%3Dcom.smaper.artisto">Artisto  Android</a> </li><li> <a href="https://itunes.apple.com/ru/app/artisto-free-video-editor/id1137893020%3Fl%3Den%26mt%3D8">Artisto  iOS</a> </li></ul><br><p>  - : </p><br><iframe id="embera-iframe-8c968" class="embera-facebook-iframe" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.facebook.com/plugins/post.php%3Fhref%3Dhttps%253A%252F%252Fwww.facebook.com%252Fartamonova%252Fvideos%252F10154391287764595%252F%26width%3D560%26height%3D315%26show_text%3Dtrue%26appId&amp;xid=17259,15700021,15700186,15700190,15700248,15700253&amp;usg=ALkJrhiIbe1lvu1hfdchKUfg4COHr7RbJA" width="560" height="315" "="" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowtransparency="true"></iframe><br><p>     : </p><br><ul><li> <a href="https://github.com/Theano/Theano">Theano</a> </li><li> <a href="https://github.com/Lasagne/Lasagne">Lasagne</a> </li><li> <a href="https://github.com/Lasagne/Recipes/">Lasagne/Recipes</a> ,  + ‚Äî  ; ,    TF,  -     8 ,       700  </li><li> <a href="https://github.com/Lasagne/Recipes/blob/master/examples/Saliency%2520Maps%2520and%2520Guided%2520Backpropagation.ipynb">Lasagne/Recipes/examples/Saliency Maps and Guided Backpropagation</a> </li><li> <a href="https://github.com/Lasagne/Recipes/blob/master/examples/styletransfer/Art%2520Style%2520Transfer.ipynb">Lasagne/Recipes/examples/styletransfer/Art Style Transfer</a> </li><li> <a href="https://github.com/DmitryUlyanov/texture_nets">     </a>       Torch </li><li> <a href="https://github.com/yusuketomoto/chainer-fast-neuralstyle"> resnet-like </a>   Chainer </li><li> <a href="https://github.com/jcjohnson/neural-style">  Gatys'</a>  Torch,    resnet-like ;        lbfgs   </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/306916/">https://habr.com/ru/post/306916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../306904/index.html">We work with a hybrid cloud: VMware vCloud Connector, part 1</a></li>
<li><a href="../306906/index.html">Shake dust from the globe: check the project NASA World Wind</a></li>
<li><a href="../306908/index.html">It doesn't matter if you are big or small. The main thing is not small. Stuffing Material Design</a></li>
<li><a href="../306912/index.html">Moscow Institute of Physics and Technology: the reception of applications to the online magistracy in modern combinatorics is ending</a></li>
<li><a href="../306914/index.html">Understanding Go: io Package</a></li>
<li><a href="../306918/index.html">11 Essential Hiring Tips for Starting Employers</a></li>
<li><a href="../306920/index.html">Is there life after the failure of popular browsers to support the architecture of NPAPI</a></li>
<li><a href="../306922/index.html">Creating a Doodle Jump game for Android in Intel XDK in 2 hours on JavaScript from scratch</a></li>
<li><a href="../306924/index.html">Mal, Yes Del: Review younger servers in the HPE ProLiant ML line</a></li>
<li><a href="../306928/index.html">Learning OpenGL ES2 for Android Lesson number 3. Lighting</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>