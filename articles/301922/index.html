<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Not we are - life is like this: Thematic analysis for the most impatient</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Why? 
 Now Relap.io generates 40 billion recommendations per month on 2000 media platforms of the Runet. Almost any recommender system, sooner or late...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Not we are - life is like this: Thematic analysis for the most impatient</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/6c3/fcf/ee5/6c3fcfee505af809ba86ce96330021e6.jpg" alt="bayesian" width="1000" height="480"><br><br><h2>  Why? </h2><br>  Now <a href="https://relap.io/">Relap.io</a> generates 40 billion recommendations per month on 2000 media platforms of the Runet.  Almost any recommender system, sooner or later, comes to the need to take into account the contents of recommended content, and rather quickly comes up against the need to somehow classify it: find some clusters or at least reduce the dimension to describe the interests of users, attract advertisers or else for some dark or not very goals. <br><br>  The task sounds quite obvious and there are many well-proven algorithms and their implementations: <a href="https://habrahabr.ru/company/surfingbird/blog/228249/">Dirichlet latent placement (LDA)</a> , <a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">Probabilistic latent semantic analysis (pLSA)</a> , <a href="https://en.wikipedia.org/wiki/Explicit_semantic_analysis">explicit semantic analysis (ESA)</a> , the list <a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">goes</a> on.  However, we decided to try to come up with something more simple, but at the same time, viable. <br><a name="habracut"></a><br>  There are several reasons for such a decision, but the main one was laziness and unwillingness to wait until the models were trained.  More seriously, we have a fairly large amount of data from numerous sources (hundreds of thousands of popular and millions of published articles) and the problem of guessing the number of topics for which we want to scatter everything was not at all obvious (and to such an extent that we didn‚Äôt even imagine order - 100 topics? 1000?).  With such introductory training, the LDA model or pLSA would be quite inefficient, especially considering the ever-growing hull.  I wanted something faster, and perhaps less accurate, but able to scatter at least 70% of the documents in handfuls and at the same time find out the number and size of these handfuls, and then build some ontology on their basis. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  How? </h2><br>  How to approach such a task: we obviously need some kind of generative model that links as many words as possible into certain topics (semantic fields). <br><br>  Despite the desire to replace the bicycle with a newly invented scooter, we do not refuse the basic principles of thematic analysis, that is, we still present documents in the form of unordered ‚Äúbags of words‚Äù (bag of words), and even consider the words themselves, but the lemmas that we got rid of all the texts through <a href="http://www.algorithmist.ru/2010/12/porter-stemmer-russian.html">Porter stemmer</a> .  We do not remove homonymy and do not store any grammatical information.  The sample consists of short news (publications - in fact, only the title and the first paragraph).  We also know how often each publication was read (such knowledge can be useful for ranking articles per se importance / relevance, etc.). <br><br>  In order to understand what we have simplified, let us first recall what the LDA is: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p%28d%2Cw%29%20%3D%20%5Csum_%7Bt%20%5Cin%20T%7D%20p%28d%29p%28w%7Ct%29p%28t%7Cd%29%2C"></div><br>  Where <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28d%2Cw%29" alt="inline_formula">  - this is the probability of the ‚Äúdocument-word‚Äù pair appearing, it consists of the sum on all topics ( <img src="http://tex.s2cms.ru/svg/%5Cinline%20T" alt="inline_formula">  - a lot of topics and works of the actual probability of the document (calculated as the ratio of the length of the document to the length of the body), the probability of the word appearing in the topic and the weight of the topic in the document (or rather the probability of the topic being present in the document).  All the components in the sum can be calculated using the Bayesian formula, the problem is that we do not know a priori distributions for either topics or words.  Moreover, our documents are approximately the same length, since we only store pieces of approximately the same length, sufficient for annotation and therefore <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28d%29" alt="inline_formula">  for us is irrelevant, it does not contain any information.  In other words, in the formulas: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p%28w%7Ct%29%20%3D%20%5Cfrac%7Bp%28w%2Ct%29%7D%7Bp%28t%29%7D%2C%20p%28t%7Cd%29%3D%5Cfrac%7Bp%28t%2C%20d%29%7D%7Bp%28d%29%7D"></div><br>  unknown to us <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28t%29" alt="inline_formula">  neither <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28t%2Cd%29" alt="inline_formula">  , but <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28d%29" alt="inline_formula">  the same for all documents, and directly we can not count anything.  LDA operates on the assumption that the vector of documents <img src="http://tex.s2cms.ru/svg/%5Cinline%20%5Ctheta_d%20%3D%20%28p%28t%7Cd%29%3A%20t%5Cin%20T%29" alt="inline_formula">  generated by a parametric distribution from the Dirichlet family of distributions <img src="http://tex.s2cms.ru/svg/%5Cinline%20Dir%28%5Ctheta%2C%20%5Calpha%29%2C%20%5Calpha%20%5Cin%20%5Cmathbb%7BR%7D%5E%5Calpha" alt="inline_formula">  .  This is where we came across the first restriction that we don‚Äôt like - we don‚Äôt have any thoughts on <img src="http://tex.s2cms.ru/svg/%5Cinline%20%5Calpha" alt="inline_formula">  , except for the fact that it is likely to be quite large, and therefore optimization for such a family of distributions will be rather computationally heavy. <br><br>  What can be simplified here?  Let's see what can be counted without any tweaks and what benefits can be gained from this? <br><br>  Let's try something quite primitive, what is <img src="http://tex.s2cms.ru/svg/%5Cinline%20p%28w%7Ct%29" alt="inline_formula">  , the probability of generating a word by topic?  If we know the theme of the text, then we can guess how likely the word will be there.  The Bayesian formula can always be ‚Äúturned upside down‚Äù and you can calculate the probability that a word belongs to a topic, or rather, the probability that a topic is present in a document containing a word. <br><br>  The problem is that we do not have a distribution of topics, but only some word statistics.  It may make sense to simplify our view of the corpus and not think about the generation of documents (and this is actually the basis of the "correct" thematic analysis) and focus exclusively on the "relationship" of words. <br><br>  In our case, a topic is simply a collection of words with some weights that are found together in documents describing interrelated things.  Is it possible to check whether two words belong to the same topic?  As it seems to us, it is possible, and with the help of rather simple calculations.  A similar approach works well for isolating collocations, but on <em>ordered sets</em> , we do not store information on word order, but we know the frequency with which words occur within the same document.  The joint distribution of pairs of words within one document is relatively many pairs and most of them will be completely meaningless. <br><br>  Intuitively, the assumption that two words relating to the same topic are most likely to be found together more often than two words from different topics does not cause any special doubts.  Immediately make a reservation that we argue about words with a pronounced affiliation to a more or less clearly defined topic: the words "kingpin" and "Lada" are probably found in texts on the automobile theme, and the words "carburetor" and "mayonnaise" are unlikely to meet ( or our imagination is not enough to come up with an example).  On the other hand, most of the verbs and adjectives fit harmoniously into the text on almost any subject: <br><blockquote>  Resident of the city N was killed by the explosion of a huge kingpin <br></blockquote><br>  (the author knows that the pins usually do not explode and that there is no N city in Russia) or <br><blockquote>  The guests were killed on the spot a huge mayonnaise cake <br></blockquote><br>  If you somehow find "semantically loaded" words, then it makes sense to look at what other words are found with them. <br><br>  Let's consider: <br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p%28w_i%7Cw_j%29%20%3D%20%5Cfrac%7Bp%28w_i%2Cw_j%29%7D%7Bp%28w_j%29%7D"></div><br>  this is the probability of the presence of the word <img src="http://tex.s2cms.ru/svg/%5Cinline%20w_i" alt="inline_formula">  in the document, if you know that there is a word <img src="http://tex.s2cms.ru/svg/%5Cinline%20w_j" alt="inline_formula">  , you can count such things directly from the case, but if you take into account all possible pairs, we again come to the need to calculate <img src="http://tex.s2cms.ru/svg/%5Cinline%20n%5E2" alt="inline_formula">  probabilities where <img src="http://tex.s2cms.ru/svg/%5Cinline%20n%20%3D%20%7CW%7C%2C%20w_i%5Cin%20W" alt="inline_formula">  that is, the size of our dictionary (the dictionary of the Pushkin language was about 40 thousand entries, but the news published by our publishers contains more than 200 thousand lemmas per hour, we will refrain from conclusions for the time being). <br><br>  Word <img src="http://tex.s2cms.ru/svg/%5Cinline%20w_j" alt="inline_formula">  is a kind of generator for the word <img src="http://tex.s2cms.ru/svg/%5Cinline%20w_i" alt="inline_formula">  Thus, if you intelligently select dependent word generators, you can get some meaningful sets of words.  Let's try? <br><br>  Let us return to the ‚Äúsemantically meaningful‚Äù words, the voices in the head begin to whisper softly but persistently: ‚Äútf-idf, tf-idf‚Äù. <br><br>  Let's not fight the demons of evidence and try to figure out how to use tf-idf to find out which of the words are more important than others.  Calculate tf-idf within each document, reduce the documents to a reasonable number of keywords (simply by sorting the words by their tf-idf values ‚Äã‚Äãand saving the desired number of words with the highest values).  There is hope that the dictionary will decrease and it will be easier to understand it. <br><br>  On the other hand, we reduced the documents, but increased the ‚Äúvalue‚Äù of words with a very narrow meaning: if one article met in detail describing the marriage habits of guinggnms, the word ‚Äúguinggnm‚Äù will receive high weight in this article and become a candidate for its own semantic field, which undoubtedly has the right to exist, but it is unlikely to help much in the subsequent classification of new articles.  You can avoid this by aggregating the tf-idf words across the whole body and once again ranking the words, this time not inside the documents, but across the whole body. <br><br><h2>  What happened? </h2><br>  In order not to be unfounded, let's see what happens if we take a selection of articles that have been read in the last half hour and sort the words, as it was suggested by the demons of evidence.  Let's first see what happens if we first just select words with significant tf-idf (without aggregation, the value of tf-idf is equal to the average for all articles where the word was encountered): <br><blockquote>  Latin, leagoo, melano, matron, material, polyp, tabl, ssd-accumulator, two-hour, tournament, fitch‚Ä¶ <br></blockquote><br>  The words are not bad, in the sense that they clearly indicate some clearly defined topics, but it is not clear whether the topics in our body are really important, which contains about 280 thousand short articles.  Let's try to aggregate and see what happened: <br><blockquote>  airborne, music, hare, portal, jumping, free, new, well, well, who, myself ... <br></blockquote><br>  It turns out some nonsense: frequently encountered words, even with the punishment in frequency, still ‚Äúpop up‚Äù to the top.  Let's try to fight it just by discarding the most popular words from consideration.  Without much thought, we threw out 300 words with the lowest idf, that's what happened (for clarity, instead of the top 10, we derive the top 30 words). <br><blockquote>  streets, successful, kreter, temple, thousand, lifan, fully, cr, bisquit, raised, brit, central, pres, sign, lead, included, magnetic, borodin, dol, machines, interaction, excluded, snow, pedicure, napkin, creator, latinin, michael, est, competence, raped ... <br></blockquote><br>  This list is already more meaningful: first, we encounter the mention of ‚ÄúLatin‚Äù (in the first place for the first time and on the 26th in the last list), this is clearly a surname, and apparently something happened to this character, something important (most likely, this is Yulia Latynina, but we cannot guarantee it yet).  Lifan is a brand of car whose presence is expected - a significant percentage of the traffic in this sample goes through automotive forums.  The remaining words from the list also look logical - in traffic there is always a discussion of recipes (‚Äúbiscuits‚Äù), economics (‚Äúcentral banks‚Äù), and the like.  Just looking at the list, you can hardly easily understand what the readers are concerned about at the moment, but you can already see that the words refer to different topics and events.  While this is enough - we are just looking for where to start, isn't it? <br><br>  Let's start the actual generation of topics - for now we don‚Äôt think about how many topics we can get, but simply take some (large) part of the resulting list and generate topics, and then think about what to do with them. <br><br>  Before throwing out topics for everyone to see, we will spend another couple of minutes thinking about the criterion of which words to keep in the subject, and which to throw out after the probabilities have been calculated.  To put some kind of hard cutoff on the absolute value is not worth it - depending on the size of the topic, the conditional probabilities can differ quite strongly.  Instead, try a relationship with the most likely word in the subject: <img src="http://tex.s2cms.ru/svg/%5Cinline%20T%28w_i%29%20%3D%20%5C%7Bw_j%5C%7D" alt="inline_formula">  Where <img src="http://tex.s2cms.ru/svg/%5Cinline%20p_%7B%5Cmax%7D%5Ei%20%3D%20%5Cmax_j%5C%7Bp%28w_j%7Cw_i%29%7C%20i%5Cne%20j%2C%20w_j%5Cin%20T_i%5C%7D" alt="inline_formula">  let's set some kind of cutoff <img src="http://tex.s2cms.ru/svg/%5Cinline%20thr" alt="inline_formula">  and we will reject all the words the ratio of the probability of which and the maximum probability of the word in the subject below: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/T%28w_i%29%20%3D%20%5Cleft%20%5C%7Bw_j%20%5Cin%20W%7C%5Cfrac%7Bp%28w_j%7Cw_i%29%7D%7Bp_%7B%5Cmax%7D%5Ei%7D%20%3E%20thr%5Cright%20%5C%7D."></div><br><br>  Let's start with a fairly relaxed requirements, put <img src="http://tex.s2cms.ru/svg/%5Cinline%20thr%20%3D%200.1" alt="inline_formula">  and see what happens, for readability, we took only the initial words of each topic (the topics are quite long, the numbers after the word are probabilities): <br><blockquote>  0 = "(flight log, List ((flight log, 1.0), (mazda, 0.02639662123248224), <br>  (mercedes-benz, 0.025020797337940742), (subaru, 0.02498880143341652), <br>  (Prior, 0.024668842388174312), (octavia, 0.024380879247456324), <br>  (front, 0.024316887438407882), (sedan, 0.02098931336788891), <br>  (Kalin, 0.01785371472451526) ... <br><br>  1 = "(music, List ((music, 1.0), (feat, 0.031221582297665956), <br>  (song, 0.016469637263817317), (dj, 0.013438415681519652), <br>  (Alexander, 0.012630089926240274), (Irin, 0.012326967768010509), <br>  (Vladimir, 0.011417601293321209), (circle, 0.008992624027483076), <br>  (mikha, 0.008487420430433464), (serg, 0.008386379711023543), <br>  (Grigor, 0.007982216833383854), (Caspian, 0.007780135394564009), <br>  (Viktor, 0.007780135394564009), (cargo, 0.007679094675154087), <br>  (leps, 0.0072749317975143975) ... <br><br>  2 = "(hare, List ((hare, 1.0), (feat, 0.03158217497955847), <br>  (song, 0.01655764513491415), (dj, 0.013593622240392478), <br>  (Alexander, 0.012775960752248568), (Irin, 0.012367130008176616), <br>  (Vladimir, 0.011549468520032708), (circle, 0.009096484055600982), <br>  (mikha, 0.00848323793949305), (serg, 0.008381030253475062), <br>  (Grigor, 0.008074407195421096), (Caspian, 0.007869991823385119), <br>  (Viktor, 0.007869991823385119), (cargo, 0.0077677841373671305), <br>  (leps, 0.0073589533932951765) ... <br><br>  3 = "(portal, List ((portal, 1.0), (feat, 0.031572494124859504), <br>  (song, 0.01655256973536324), (dj, 0.013589455400020435), <br>  (Alexander, 0.012772044548891385), (Irin, 0.012363339123326864), <br>  (Vladimir, 0.011443751915806682), (circle, 0.009093695718810668), <br>  (mih, 0.008480637580463881), (serg, 0.008378461224072748), <br>  (Grigor, 0.008071932154899356), (Caspian, 0.007867579442117094), <br>  (Viktor, 0.007867579442117094), (cargo, 0.007765403085725963), <br>  (leps, 0.007356697660161438) ... <br><br>  4 = "(download, List ((download, 1.0), (feat, 0.028736234923964345), <br>  (song, 0.01688515993707394), (torrent, 0.016255899318300997), <br>  (games, 0.014263240692186683), (alexander, 0.012690089145254328), <br>  (irin, 0.012480335605663346), (vladimir, 0.01101206082852648), <br>  (dj, 0.01101206082852648), (circle, 0.009229155742003147), <br>  (mikha, 0.008495018353434716), (serg, 0.008390141583639224) ... <br><br>  5 = "(free, List ((free, 1.0), (feat, 0.028751311647429174), <br>  (song, 0.016684155299055613), (irin, 0.01280167890870934), <br>  (Alexander, 0.012591815320041973), (Vladimir, 0.011017838405036727), <br>  (dj, 0.010912906610703044), (circle, 0.009233997901364114), <br>  (mikha, 0.008604407135362015), (serg, 0.008289611752360966), <br>  (Grigor, 0.0080797481636936), (Caspian, 0.007869884575026232), <br>  (Viktor, 0.007869884575026232), (cargo, 0.00776495278069255), <br>  (leps, 0.007345225603357817) ... <br><br>  6 = "(new, List ((new, 1.0), (select, 0.04891791522602125), <br>  (set, 0.04046612882571392), (event, 0.028812908182865922), <br>  (Telekana, 0.026892047637341526), ‚Äã‚Äã(review, 0.02650787552823665), <br>  (tut, 0.02612370341913177), (near Moscow, 0.025995646049430145), <br>  (address, 0.023306441285695992), (html, 0.021641695479574848), <br>  (news, 0.02087335126136509), (yesterday, 0.020745293891663463), <br>  (fresh, 0.02023306441285696), (oblast, 0.019592777564348827), <br>  (presented, 0.0145985401459854), (info, 0.014342425406582149), <br>  (Grodno, 0.013317966448969137) ... <br>  ... <br>  17 = (Ross, List ((Ross, 1.0), (combined, 0.0731070496083551), <br>  (one, 0.04960835509138382), (president, 0.03953748601268183), <br>  (Putin, 0.029093621782916825), (primaries, 0.023125699365908244), <br>  (Hoke, 0.01939574785527788), (Vladimir, 0.019022752704214847), <br>  (countries, 0.0175307720999627), (coach, 0.0175307720999627), <br>  (Chapters, 0.01566579634464752), (RF, 0.015292801193584483), <br>  (championship, 0.014546810891458413) ... <br></blockquote><br><br>  As it is easy to notice, topics 1 through 5 actually describe the same thing, but topic 17, on the contrary, mixes several topics together (in fact, this is the latest news about Russia).  This problem needs to be solved somehow. <br><br>  Let's start with repetitive topics.  The obvious solution would be to merge topics in which there are many common words, the only thing you need to decide on is understanding what means a lot and how to assess it a lot, you can simply use the Jaccard coefficient, instead of recalculating the elements, add their probabilities and again calculate the Jacquard coefficient, the main thing is that as a result, similar things can be combined. <br><br>  Recall that the Jacquard coefficient for finite sets <img src="http://tex.s2cms.ru/svg/%5Cinline%20A" alt="inline_formula">  and <img src="http://tex.s2cms.ru/svg/%5Cinline%20B" alt="inline_formula">  calculated by the formula: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20K_%7BA%2CB%7D%20%3D%20%5Cfrac%7B%7CA%5Ccap%20%20B%7C%7D%7B%7CA%20%5Ccup%20B%7C%7D%2C%20"></div><br>  It does not make sense to apply it directly to topics, because even for a subset, if its cardinality is much less than the cardinality of a larger set, the Jacquard coefficient will be much less than one, which is logical, but it does not suit us.  Instead, we can, for example, sort topics by length and start comparing the shortest topic with topics of greater length; if more than half of the words of the topic coincide with the words of some other topic, they should be merged.  Let's see what happened (again, 10 words at the beginning of each topic): <br><blockquote>  0 = "(Borzhurna, List ((Borzhurna, 1.0), (mazda, 0.02639662123248224), (mercedes-benz, 0.025020797337940742), (subaru, 0.02498880143341652)), (prior, 0.024668842388174312), (octavors), (a D), and,), (), (1). ), (sedan, 0.02098931336788891), (Kalin, 0.01785371472451526), ‚Äã‚Äã(astra, 0.017789722915466818) ... <br><br>  1 = "(music, List ((downloading, 1.0), (hare, 1.0), (portal, 1.0), (free, 1.0), (music, 1.0), (feat, 0.030372759594695493), (song, 0.016629833474044852), (torrent, 0.016255899318300997), (games, 0.014263240692186683), (alexander, 0.012691999938535306), (dj, 0.012509292152232418), (irin, 0.012467890282777335), <br><br>  2 = "(new, List ((new, 1.0), (review, 0.5132539377641183), (opt),), (26469259179654335), (complete set, 0.24270002476527985), (event, 0.028812908182865922), (telly, 0.02689204763734152888588565922), (tel (near Moscow, 0.025995646049430145), (address, 0.023306441285695992), (html, 0.021641695479574848), (news, 0.02087335126136509), (Tuesday, 0.020745293891663463), <br><br>  3 = "(s (mn, 0.01952130368358513), (count, 0.018842301816329992), (women, 0.01850280088270243), (cf. 0.01850280088270243), (term, 0.01782379901544729), <br><br>  4 = "(fret, List ((fret, 1.0), (prior, 0.5873114649209881), pick-up box, psikstek, 0.43989485258120614), (seda, 0.4370012256029478), (Kalin, 0.4247207293150209), (0) (grant, 0.0830172777075432), (3d, 0.0777496839443742), (sport, 0.07026217566672686), <br><br>  5 = "(Kot, List ((Kot, 1.0), (thing, 0.04718016238753566), (one hundred, 0.028966425279789335), (books, 0.0230414746543777778), (help, 0.021724818959842), (your, 0.017774851876234364), (reference, 0.01576495959842), (your, 0.017774851876234364)) (useful, 0.014702655255650647), (stars, 0.013385999561114768), (change, 0.013385999561114768), (wants, 0.01294711432960281), (girls, 0.01163045863506693), (words, 0.01141101601931095, 0.01463045863506693), (words, 0.01141101601931095, 0.01463045863506693), (words, 0.01141101601931095), (words, 0.011630863506693) , 0.010752688172043012), <br>  ... <br>  20 = (recipe, List ((recipe, 1.0), (prepared, 0.1684643040575244) (poshagov, 0.13405238828967642) (culinary, 0.11145351823317926) (lettuce, 0.1027221366204417) (dishes, 0.04982023626091423) (prigotov, 0.044170518746789934) ( cake, 0.0421160760143811), (pie, 0.04006163328197227), (soup, 0.03543913713405239), (chicken, 0.03338469440164355), (fast, 0.029275808936825888), <br>  ... <br></blockquote><br>  Finally!  Grigory Leps is only in one topic, the Russian car industry is separated from abstract car conversations, and the chicken got into one pile with soup, pie and adjective fast!  Against the background of general complacency stands out topic 2, which contains it is not clear that.  If you look at it in more detail, you can see that many words in this topic can belong to any other topic - you can get rid of such words simply by deleting words that are repeated more than in <img src="http://tex.s2cms.ru/svg/%5Cinline%20n" alt="inline_formula">  where <img src="http://tex.s2cms.ru/svg/%5Cinline%20n" alt="inline_formula">  somehow chosen parameter, it is also worth deleting words with low idf, we did not use them as generators, but nothing prevents us from getting them when calculating probabilities. <br><br>  Let's go back a little and look at the very first generated list - is it possible to use it somehow?  Naturally, each word will generate something, and there will be many such topics and they will have to be merged, which will take a long time: merging the topics together takes a quadratic time from the number of topics.  Is it possible to get a big topic from words like "ssd-drive"?  Demons of common sense insist that it is possible and periodically repeat words like "hierarchy" and "ontology", we only need to interpret these concepts as primitively as possible and slip them the Bayes formula again. <br><br>  Let's try the following: let's think of hierarchy as a tree, where the most common concepts are in the root, and the narrowest words mean in the leaves.  In this case, "ssd-drive" is a leaf of a tree, at the root of which sits a "computer", or "technology", or something like that, if we can recover at least part of this tree, we will have a good, albeit incomplete topic .  Let's try, pseudo-recursion on similar generators.  The term <i>pseudo-recursion</i> was coined just now and we mean the challenge of generating themes for each generated word in a freshly thought-out topic, such an operation (after normalization) can be called until we start receiving words about which are not suitable for classification ( we have already found similar words by checking their idf). <br><br>  Let's see what happened? <br><blockquote>  salmon = tomato, brightly, parmalat, salted, mousse, forgive, sauce, green, royal, creamy, garnish, pore, canap√©, quail, festive, snack, naive, salty, grat, hello, lover, roll, bake, fish, heads, salmon, breakfast, soup, wants, potato, raviol, salted, Borodinsk, oven, cream cheese, uh, taken, potatoes, salad, pink salmon, taste, salt, package, wonderful, steak, lightly, ready, medallion, roll, cooked ... <br><br>  nausea = morning, heartburn, dream, common, symptom, helping, down, strong, thirst, vomiting, rarely, roller, life, food, heaviness, feeling, urine, gestosis, izgag, forerunner, recently awakened, protein, morning, countries, dizzy, bottom, nausea <br></blockquote><br>  In principle, not bad: "salmon" has generated a topic about healthy eating, and the next topic is that something went wrong.  Actually the threads are longer, we just show small pieces.  Another interesting example: <br><blockquote>  Acceleration = Niv, gas, sable, Barguzin, Kalin, 4x4, hatchback, prior, v8, urban, sector, seda, universe, 5d, tested, acceleration, 4x4, gazelle, 3d <br></blockquote><br>  Here one of the characteristics of a car generates a field about cars in general, by increasing the number of calls, we will be able to generate more words, when there are many such fields you can merge together, as described above. <br><br>  Having looked a little at a couple of formulas and explanations for them, it is easy to see that as a result, as usual, a naive Bayes classifier turned out with all sorts of empirical tricks for selecting parameters and without the marked training corpus.  The latter is important for us: there is a lot of data, you don‚Äôt want to mark up something manually or even in semi-automatic mode, and therefore you need training without a teacher.  Strangely enough, such an approach, despite its simplicity, works well for large volumes of documents and allows you to at least sort them into small groups, and then sell diapers to young parents, and engine oils, to motorists! </div><p>Source: <a href="https://habr.com/ru/post/301922/">https://habr.com/ru/post/301922/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../301908/index.html">Who to live well in startups</a></li>
<li><a href="../301910/index.html">Perl plugin version 2.0 released for IntelliJ IDEA</a></li>
<li><a href="../301914/index.html">The best packages for working with data in R, part 1</a></li>
<li><a href="../301916/index.html">NeDB: SQLite equivalent for NodeJS</a></li>
<li><a href="../301918/index.html">Amazing Computer: ENIAC History</a></li>
<li><a href="../301924/index.html">"The candidate has the right to ask clarifying questions," or we are bringing the interviewer to a nervous breakdown</a></li>
<li><a href="../301930/index.html">Bidmanedzherami and killing rate - is the loss of almost half of the profits</a></li>
<li><a href="../301932/index.html">Security Week 21: rejection of passwords, life of ancient vulnerabilities, virus in network equipment</a></li>
<li><a href="../301936/index.html">Parsing resume</a></li>
<li><a href="../301940/index.html">Official Russian cryptocurrency. What kind of animal will it be?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>