<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of solving a multiple regression problem using Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Good afternoon, dear readers. 
 In past articles, with practical examples, I have shown how to solve classification problems ( credit s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An example of solving a multiple regression problem using Python</h1><div class="post__text post__text-html js-mediator-article"><h4>  Introduction </h4><br>  Good afternoon, dear readers. <br>  In past articles, with practical examples, I have shown how to solve classification problems ( <a href="http://habrahabr.ru/post/204500/">credit scoring task</a> ) and the basics of text information analysis ( <a href="http://habrahabr.ru/post/205360/">passport problem</a> ).  Today, I would like to touch on another class of problems, namely the <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">restoration of regression</a> .  Tasks of this class, as a rule, are used in <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259F%25D1%2580%25D0%25BE%25D0%25B3%25D0%25BD%25D0%25BE%25D0%25B7%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">forecasting</a> . <br>  For an example of solving a prediction problem, I took the <a href="http://archive.ics.uci.edu/ml/datasets/Energy%2Befficiency">Energy Efficiency</a> dataset from the largest repository <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3DUCI">UCI</a> .  By tradition, we will use Python with the analytical packages <a href="http://pandas.pydata.org/pandas-docs/stable/">pandas</a> and <a href="http://scikit-learn.org/stable/">scikit-learn</a> . <br><a name="habracut"></a><br><h4>  Description of the data set and problem statement </h4><br>  <a href="http://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx">A data set is given</a> that describes the following room attributes: <br><table><tbody><tr><th>  Field </th><th>  Description </th><th>  Type of </th></tr><tr><th>  X1 </th><td>  Relative compactness </td><td>  Float </td></tr><tr><th>  X2 </th><td>  Square </td><td>  Float </td></tr><tr><th>  X3 </th><td>  Wall area </td><td>  Float </td></tr><tr><th>  X4 </th><td>  Ceiling area </td><td>  Float </td></tr><tr><th>  X5 </th><td>  Overall height </td><td>  Float </td></tr><tr><th>  X6 </th><td>  Orientation </td><td>  Int </td></tr><tr><th>  X7 </th><td>  Glazing area </td><td>  Float </td></tr><tr><th>  X8 </th><td>  Distributed Glazing Area </td><td>  Int </td></tr><tr><th>  y1 </th><td>  Heating load </td><td>  Float </td></tr><tr><th>  y2 </th><td>  Cooling load </td><td>  Float </td></tr></tbody></table><br>  In him <img src="https://habrastorage.org/getpro/habr/post_images/9fc/b2d/c08/9fcb2dc08ba54b120d59e0393a6af2f1.png" title="LaTeX: X1 ... X8">  - characteristics of the premises on the basis of which the analysis will be conducted, and <img src="https://habrastorage.org/getpro/habr/post_images/61e/ffd/b17/61effdb1703ab84e602bf8766c0106e3.png" title="LaTeX: y1, y2">  - load values ‚Äã‚Äãthat need to be predicted. <br><br><h4>  Preliminary data analysis </h4><br>  First, let's load our data and look at it: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> read_csv, DataFrame <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KNeighborsRegressor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LinearRegression, LogisticRegression <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SVR <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestRegressor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> r2_score <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cross_validation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split dataset = read_csv(<span class="hljs-string"><span class="hljs-string">'EnergyEfficiency/ENB2012_data.csv'</span></span>,<span class="hljs-string"><span class="hljs-string">';'</span></span>) dataset.head()</code> </pre> <br><table><tbody><tr><th></th><th>  X1 </th><th>  X2 </th><th>  X3 </th><th>  X4 </th><th>  X5 </th><th>  X6 </th><th>  X7 </th><th>  X8 </th><th>  Y1 </th><th>  Y2 </th></tr><tr><th>  0 </th><td>  0.98 </td><td>  514.5 </td><td>  294.0 </td><td>  110.25 </td><td>  7 </td><td>  2 </td><td>  0 </td><td>  0 </td><td>  15.55 </td><td>  21.33 </td></tr><tr><th>  one </th><td>  0.98 </td><td>  514.5 </td><td>  294.0 </td><td>  110.25 </td><td>  7 </td><td>  3 </td><td>  0 </td><td>  0 </td><td>  15.55 </td><td>  21.33 </td></tr><tr><th>  2 </th><td>  0.98 </td><td>  514.5 </td><td>  294.0 </td><td>  110.25 </td><td>  7 </td><td>  four </td><td>  0 </td><td>  0 </td><td>  15.55 </td><td>  21.33 </td></tr><tr><th>  3 </th><td>  0.98 </td><td>  514.5 </td><td>  294.0 </td><td>  110.25 </td><td>  7 </td><td>  five </td><td>  0 </td><td>  0 </td><td>  15.55 </td><td>  21.33 </td></tr><tr><th>  four </th><td>  0.90 </td><td>  563.5 </td><td>  318.5 </td><td>  122.50 </td><td>  7 </td><td>  2 </td><td>  0 </td><td>  0 </td><td>  20.84 </td><td>  28.28 </td></tr></tbody></table><br>  Now let's see if there are any attributes related to each other.  This can be done by calculating the correlation coefficients for all columns.  How to do this was described in a previous <a href="http://habrahabr.ru/post/204500/">article</a> : 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> <code class="python hljs">dataset.corr()</code> </pre><br><table><tbody><tr><th></th><th>  X1 </th><th>  X2 </th><th>  X3 </th><th>  X4 </th><th>  X5 </th><th>  X6 </th><th>  X7 </th><th>  X8 </th><th>  Y1 </th><th>  Y2 </th></tr><tr><th>  X1 </th><td>  1.000000e + 00 </td><td>  -9.919015e-01 </td><td>  -2.037817e-01 </td><td>  -8.688234e-01 </td><td>  8.277473e-01 </td><td>  0.000000 </td><td>  1.283986e-17 </td><td>  1.764620e-17 </td><td>  0.622272 </td><td>  0.634339 </td></tr><tr><th>  X2 </th><td>  -9.919015e-01 </td><td>  1.000000e + 00 </td><td>  1.955016e-01 </td><td>  8.807195e-01 </td><td>  -8.581477e-01 </td><td>  0.000000 </td><td>  1.318356e-16 </td><td>  -3.558613e-16 </td><td>  -0.658120 </td><td>  -0.672999 </td></tr><tr><th>  X3 </th><td>  -2.037817e-01 </td><td>  1.955016e-01 </td><td>  1.000000e + 00 </td><td>  -2.923165e-01 </td><td>  2.809757e-01 </td><td>  0.000000 </td><td>  -7.969726e-19 </td><td>  0.000000e + 00 </td><td>  0.455671 </td><td>  0.427117 </td></tr><tr><th>  X4 </th><td>  -8.688234e-01 </td><td>  8.807195e-01 </td><td>  -2.923165e-01 </td><td>  1.000000e + 00 </td><td>  -9.725122e-01 </td><td>  0.000000 </td><td>  -1.381805e-16 </td><td>  -1.079129e-16 </td><td>  -0.861828 </td><td>  -0.862547 </td></tr><tr><th>  X5 </th><td>  8.277473e-01 </td><td>  -8.581477e-01 </td><td>  2.809757e-01 </td><td>  -9.725122e-01 </td><td>  1.000000e + 00 </td><td>  0.000000 </td><td>  1.861418e-18 </td><td>  0.000000e + 00 </td><td>  0.889431 </td><td>  0.895785 </td></tr><tr><th>  X6 </th><td>  0.000000e + 00 </td><td>  0.000000e + 00 </td><td>  0.000000e + 00 </td><td>  0.000000e + 00 </td><td>  0.000000e + 00 </td><td>  1.000000 </td><td>  0.000000e + 00 </td><td>  0.000000e + 00 </td><td>  -0.002587 </td><td>  0.014290 </td></tr><tr><th>  X7 </th><td>  1.283986e-17 </td><td>  1.318356e-16 </td><td>  -7.969726e-19 </td><td>  -1.381805e-16 </td><td>  1.861418e-18 </td><td>  0.000000 </td><td>  1.000000e + 00 </td><td>  2.129642e-01 </td><td>  0.269841 </td><td>  0.207505 </td></tr><tr><th>  X8 </th><td>  1.764620e-17 </td><td>  -3.558613e-16 </td><td>  0.000000e + 00 </td><td>  -1.079129e-16 </td><td>  0.000000e + 00 </td><td>  0.000000 </td><td>  2.129642e-01 </td><td>  1.000000e + 00 </td><td>  0.087368 </td><td>  0.050525 </td></tr><tr><th>  Y1 </th><td>  6.222722e-01 </td><td>  -6.581202e-01 </td><td>  4.556712e-01 </td><td>  -8.618283e-01 </td><td>  8.894307e-01 </td><td>  -0.002587 </td><td>  2.698410e-01 </td><td>  8.736759e-02 </td><td>  1.000000 </td><td>  0.975862 </td></tr><tr><th>  Y2 </th><td>  6.343391e-01 </td><td>  -6.729989e-01 </td><td>  4.271170e-01 </td><td>  -8.625466e-01 </td><td>  8.957852e-01 </td><td>  0.014290 </td><td>  2.075050e-01 </td><td>  5.052512e-02 </td><td>  0.975862 </td><td>  1.000000 </td></tr></tbody></table><br>  As can be seen from our matrix, the following columns correlate with each other (The value of the correlation coefficient is greater than 95%): <br><ul><li>  y1 -&gt; y2 </li><li>  x1 -&gt; x2 </li><li>  x4 -&gt; x5 </li></ul><br>  Now let's choose which columns of our pairs we can remove from our sample.  To do this, in each pair, select the columns that have a greater impact on the predicted values ‚Äã‚Äãof <i>Y1</i> and <i>Y2</i> and leave them, and remove the rest. <br>  As you can see, the matrices with the correlation coefficients on <i><b>y1</b></i> , <i><b>y2 have</b></i> more values <b><i>X2</i></b> and <b><i>X5</i></b> than X1 and X4, so we can delete the last columns we can. <br><br><pre> <code class="python hljs">dataset = dataset.drop([<span class="hljs-string"><span class="hljs-string">'X1'</span></span>,<span class="hljs-string"><span class="hljs-string">'X4'</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) dataset.head()</code> </pre><br>  In addition, it can be noted that the fields <i><b>Y1</b></i> and <i><b>Y2 are</b></i> very closely correlated with each other.  But, since we need to predict both values, we leave them "as is". <br><br><h4>  Model selection </h4><br>  We separate the predicted values ‚Äã‚Äãfrom our sample: <br><br><pre> <code class="python hljs">trg = dataset[[<span class="hljs-string"><span class="hljs-string">'Y1'</span></span>,<span class="hljs-string"><span class="hljs-string">'Y2'</span></span>]] trn = dataset.drop([<span class="hljs-string"><span class="hljs-string">'Y1'</span></span>,<span class="hljs-string"><span class="hljs-string">'Y2'</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  After processing the data, you can proceed to the construction of the model.  To build the model we will use the following methods: <br><ul><li>  <a href="http://ru.wikipedia.org/wiki/%25CC%25E5%25F2%25EE%25E4_%25ED%25E0%25E8%25EC%25E5%25ED%25FC%25F8%25E8%25F5_%25EA%25E2%25E0%25E4%25F0%25E0%25F2%25EE%25E2">Least square method</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/Random_forest">Random forest</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">Logistic regression</a> </li><li>  <a href="http://ru.wikipedia.org/wiki/%25CC%25E5%25F2%25EE%25E4_%25EE%25EF%25EE%25F0%25ED%25FB%25F5_%25E2%25E5%25EA%25F2%25EE%25F0%25EE%25E2">Support Vector Machine</a> </li><li>  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3DKNN">Nearest Neighbor Method</a> </li></ul><br>  The theory of these methods can be read in the <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259C%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_(%25D0%25BA%25D1%2583%25D1%2580%25D1%2581_%25D0%25BB%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B9%252C_%25D0%259A.%25D0%2592.%25D0%2592%25D0%25BE%25D1%2580%25D0%25BE%25D0%25BD%25D1%2586%25D0%25BE%25D0%25B2)">course of KV Vorontsov‚Äôs machine learning lectures</a> . <br>  We will estimate using the <a href="http://ru.wikipedia.org/wiki/%25CA%25EE%25FD%25F4%25F4%25E8%25F6%25E8%25E5%25ED%25F2_%25E4%25E5%25F2%25E5%25F0%25EC%25E8%25ED%25E0%25F6%25E8%25E8">coefficient of determination</a> ( <i>R-square</i> ).  This ratio is defined as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18a/6ec/f5f/18a6ecf5f82456d28c0f84e6e89fb661.png" title="LaTeX: R ^ 2 = 1 - \ frac {V (y | x)} {V (y)} = 1 - \ frac {\ sigma ^ 2} {\ sigma_y ^ 2}"><br><br>  where <img src="https://habrastorage.org/getpro/habr/post_images/9b9/735/84d/9b973584d4d32f949149aad7ca99885a.png" alt="image">  - conditional variance of the dependent quantity <i>y</i> by the factor <i>x</i> . <br>  Coefficient takes on the interval <img src="https://habrastorage.org/getpro/habr/post_images/8b6/206/f36/8b6206f36960f49e50a8f3135524b7c1.png" title="LaTeX: [0,1]">  and the closer it is to 1, the stronger the dependence. <br>  Well, now you can go directly to building a model and choosing a model.  Let's put all our models in one list for the convenience of further analysis: <br><br><pre> <code class="python hljs"> models = [LinearRegression(), <span class="hljs-comment"><span class="hljs-comment">#    RandomForestRegressor(n_estimators=100, max_features ='sqrt'), #   KNeighborsRegressor(n_neighbors=6), #    SVR(kernel='linear'), #       LogisticRegression() #   ]</span></span></code> </pre><br>  So the models are ready, now we will break our initial data into 2 subsamples: <i>test</i> and <i>training</i> .  Who read my previous articles knows that this can be done with the help of the train_test_split () function from the scikit-learn package: <br><br><pre> <code class="python hljs">Xtrn, Xtest, Ytrn, Ytest = train_test_split(trn, trg, test_size=<span class="hljs-number"><span class="hljs-number">0.4</span></span>)</code> </pre><br>  Now, since we need to predict 2 parameters <img src="https://habrastorage.org/getpro/habr/post_images/61e/ffd/b17/61effdb1703ab84e602bf8766c0106e3.png" title="LaTeX: y1, y2">  , it is necessary to build a regression for each of them.  In addition, for further analysis, you can record the results in a temporary <i>DataFrame</i> .  You can do it like this: <br><br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#   TestModels = DataFrame() tmp = {} #     for model in models: #   m = str(model) tmp['Model'] = m[:m.index('(')] #     for i in xrange(Ytrn.shape[1]): #  model.fit(Xtrn, Ytrn[:,i]) #   tmp['R2_Y%s'%str(i+1)] = r2_score(Ytest[:,0], model.predict(Xtest)) #    DataFrame TestModels = TestModels.append([tmp]) #     TestModels.set_index('Model', inplace=True)</span></span></code> </pre><br>  As you can see from the code above, to calculate the coefficient <img src="https://habrastorage.org/getpro/habr/post_images/879/d5c/f0f/879d5cf0fa87faddaefd28fc0906ad27.png" title="LaTeX: R ^ 2">  r2_score () function is used. <br>  So, the data for the analysis are received.  Let's now build the graphics and see which model showed the best result: <br><br><pre> <code class="python hljs">fig, axes = plt.subplots(ncols=<span class="hljs-number"><span class="hljs-number">2</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>)) TestModels.R2_Y1.plot(ax=axes[<span class="hljs-number"><span class="hljs-number">0</span></span>], kind=<span class="hljs-string"><span class="hljs-string">'bar'</span></span>, title=<span class="hljs-string"><span class="hljs-string">'R2_Y1'</span></span>) TestModels.R2_Y2.plot(ax=axes[<span class="hljs-number"><span class="hljs-number">1</span></span>], kind=<span class="hljs-string"><span class="hljs-string">'bar'</span></span>, color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>, title=<span class="hljs-string"><span class="hljs-string">'R2_Y2'</span></span>)</code> </pre><br><img src="https://habrastorage.org/getpro/habr/post_images/7e7/a6e/0cc/7e7a6e0cc7c2b999338cb464ba68b073.png" alt="image"><br><br><h4>  Analysis of the results and conclusions </h4><br>  From the graphs above, we can conclude that the <i>RandomForest</i> method (random forest) did the job better than others.  Its coefficients of determination are higher than the others in both variables: <img src="https://habrastorage.org/getpro/habr/post_images/e4f/8b7/cab/e4f8b7cab696a182562ce84e204086c8.png" title="LaTeX: R_ {y1} ^ 2 \ approx 99 \%, \ R_ {y2} ^ 2 \ approx 90 \%"><br>  For further analysis, let's re-train our model: <br><br><pre> <code class="python hljs">model = models[<span class="hljs-number"><span class="hljs-number">1</span></span>] model.fit(Xtrn, Ytrn)</code> </pre><br>  Upon careful consideration, the question may arise as to why the dependent sample <i>Ytrn was divided</i> into variables (by columns) the previous time, and now we are not doing this. <br>  The fact is that some methods, such as <i>RandomForestRegressor</i> , can work with several predictable variables, while others (for example, <i>SVR</i> ) can work with only one variable.  Therefore, in the previous training, we used partitioning by columns to avoid errors in the process of constructing some models. <br>  To choose a model is, of course, good, but it would also be nice to have information on how each factor influences the predicted value.  For this, the model has a property <i>feature_importances_</i> . <br>  With it, you can see the weight of each factor in the final models: <br><br><pre> <code class="python hljs">model.feature_importances_</code> </pre><br>  <i>array ([0.40717901, 0.11394948, 0.34984766, 0.00751686, 0.09158358,</i> <i><br></i>  <i>0.02992342])</i> <br><br>  In our case, it can be seen that the total height and area affect the load most of all during heating and cooling.  Their total contribution to the predictive model is about 72%. <br>  It should also be noted that according to the above scheme, it is possible to look at the influence of each factor separately on heating and separately on cooling, but since these factors are very closely correlated with each other ( <img src="https://habrastorage.org/getpro/habr/post_images/7d7/523/3aa/7d75233aa72e5427657c48ced3588fc0.png" title="LaTeX: r \ = \ 97 \%">  ), we made a general conclusion on both of them which was written above. <br><br><h4>  Conclusion </h4><br>  In the article, I tried to show the main stages in regression analysis of data using Python and analytic packages <b>pandas</b> and <b>scikit-learn</b> . <br>  It should be noted that the data set was specially chosen so as to be as formalized as possible and the initial processing of the input data would be minimal.  In my opinion, the article will be useful for those who are just starting their way in data analysis, as well as for those who have a good theoretical base, but choose tools for work. </div><p>Source: <a href="https://habr.com/ru/post/206306/">https://habr.com/ru/post/206306/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../206290/index.html">Tesla Tower: electrical calculation</a></li>
<li><a href="../206294/index.html">How important it is to write good code</a></li>
<li><a href="../206296/index.html">View from the inside: the world around us - 4</a></li>
<li><a href="../206300/index.html">Actor Models 40 years</a></li>
<li><a href="../206304/index.html">Suddenly. SugarSync finishes free accounts</a></li>
<li><a href="../206308/index.html">Save our souls: how reality shows affect people</a></li>
<li><a href="../206310/index.html">McLaren replaces wiper blades with a force field of sound waves</a></li>
<li><a href="../206312/index.html">Top 10 books to understand the stock market device</a></li>
<li><a href="../206320/index.html">A simple interpreter from scratch in Python (translation) # 1</a></li>
<li><a href="../206322/index.html">We build an OpenVPN bridge under Mac OSX</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>