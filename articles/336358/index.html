<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to create a racist AI, without even trying. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The other day, based on another article devoted to the problem of racism in speech recognition, I participated in a big controversy about who is to bl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to create a racist AI, without even trying. Part 1</h1><div class="post__text post__text-html js-mediator-article">  The other day, based on <a href="https://www.technologyreview.com/s/608619/ai-programs-are-learning-to-exclude-some-african-american-voices/">another article</a> devoted to the problem of racism in speech recognition, I participated in a big controversy about who is to blame.  Some people were sure that this was a conspiracy of programmers.  In fact, the truth is in the data that the AI ‚Äã‚Äãuses for its training.  I decided to conduct an experiment to clearly prove it.  It turned out that Rob Speer had already done everything for me. <br><br>  I want to share with you the translation of his material, which clearly shows that even the most default version of AI will be thoroughly imbued with racism.  In the first article we will conduct an experiment, <a href="https://habrahabr.ru/company/microsoft/blog/337272/">in the second</a> we will try to figure out how to overcome the monster that we have created. <br><br><img src="https://habrastorage.org/web/76a/657/343/76a6573430514aaab341abcf8b0f1a8d.jpg"><br><a name="habracut"></a><br>  Maybe you heard about the experimental <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">Tay chat bot</a> , which Microsoft experts launched on Twitter.  In just one day, his notes became so provocative that Microsoft had to turn off the bot and never mention its name.  You probably think that this does not threaten you, because you are not doing any strange things (in particular, do not give any idlers the opportunity to train your AI on Twitter). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In this tutorial, I want to show the following: even if you use the most standard natural language processing algorithms, popular data sets and methods, the result can be a racist classifier, which should not exist in nature. <br><br>  Good news: this can be avoided.  To eliminate the appearance of racist ways in your classifier, you will need to make a little extra effort.  In this case, the revised version may be even more accurate.  But to fix the problem, you need to know what it is, and not grab the first working option. <br><br><h2>  Let's make a text tone classifier! </h2><br>  Tonality analysis is a very common task of <a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a> , which is not surprising.  A system that is able to understand whether a person has left a positive or negative comment has many uses in business.  Such solutions are used to monitor social media publications, track customer reviews, and even securities trading (for <a href="https://www.theatlantic.com/technology/archive/2011/03/does-anne-hathaway-news-drive-berkshire-hathaways-stock/72661/">example</a> , bots who bought Berkshire Hathaway shares after actress Anne Hathaway received good critics feedback). <br><br>  This is a simplified (sometimes too simplistic) approach, but it is one of the easiest ways to get quantitative estimates of human-generated texts.  In just a few steps, you can prepare a system that processes texts and provides positive and negative evaluations.  In this case, you do not have to deal with complex data presentation formats, such as parse trees or entity diagrams. <br><br>  We will now compile a classifier that is familiar to any NLP specialist.  Moreover, at each stage we will choose the easiest option to implement.  Such a model, for example, is described in the article <a href="http://cs.umd.edu/~miyyer/pubs/2015_acl_dan.pdf">Deep Averaging Networks</a> .  It is not the main subject of the article; therefore, this reference should not be considered a criticism of the results obtained.  There, the model is given simply as an example of a well-known way of using vector representations of words. <br><br>  <b>Here is our action plan:</b> <br><br><ul><li>  Get somewhere widely used vector representations of words. </li><li>  Take data for training and testing, containing the most standard words of positive and negative tonality. </li><li>  Gradient descent method to train the classifier to recognize other positive and negative words. </li><li>  Calculate estimates of the tonality of text sentences using this classifier. </li><li>  To be terrified by the monster we created. </li></ul><br>  After that, you will know how to unintentionally make a racist AI. <br><br>  <b>I would like to avoid such a final, so then we will do the following</b> : <br><br><ul><li>  Let's perform a statistical assessment of the problem in order to be able to recognize it in the future. </li><li>  Improve the data so as to get a more accurate and less racist semantic model. </li></ul><br><h2>  Required software </h2><br>  This manual is written in Python, all libraries are listed below. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> statsmodels.formula.api <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SGDClassifier <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score %matplotlib inline seaborn.set_context(<span class="hljs-string"><span class="hljs-string">'notebook'</span></span>, rc={<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>: (<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)}, font_scale=<span class="hljs-number"><span class="hljs-number">1.5</span></span>)</code> </pre> <br>  You can replace scikit-learn with TensorFlow, Keras, or any other component that contains a gradient descent algorithm. <br><br><h2>  Step 1. Vector word representations </h2><br>  Vector word representations are often used to convert words to a format that is conveniently processed by machine learning systems.  Words are represented as vectors in multidimensional space.  The smaller the distance between the vectors, the closer the meaning of the corresponding words.  Vector representations of words make it possible to compare words not by letter, but by (approximate) meaning. <br><br>  To get good vector word representations, you need to process hundreds of gigabytes of text.  Fortunately, many groups of machine learning experts have already done this work and shared the finished materials. <br><br>  There are two well-known sets of vector representations of words: word2vec (data from Google News were used as educational material for their creation) and GloVe (educational material: web pages processed by Common Crawl).  The end results will be similar for both sets.  GloVe is based on a more transparent data source, so we will use it. <br><br>  Three GloVe archives are available for download: 6, 42 and 840 billion records.  840 billion is a lot, but to extract more value from this archive than from the $ 42 billion set will require complex post-processing.  The 42 billion version is quite functional and contains a round number of words - 1 million.  We are on the path of least resistance, so we‚Äôll use the 42 billion version. <br><br>  So, we download the <code>glove.42B.300d.zip</code> archive from <a href="https://nlp.stanford.edu/projects/glove/">the GloVe website</a> and unpack the <code>data/glove.42B.300d.txt</code> .  Next, we need to create a function that will read vector representations of words in a simple format. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_embeddings</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Load a DataFrame from the generalized text format used by word2vec, GloVe, fastText, and ConceptNet Numberbatch. The main point where they differ is whether there is an initial line with the dimensions of the matrix. """</span></span> labels = [] rows = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(infile): items = line.rstrip().split(<span class="hljs-string"><span class="hljs-string">' '</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(items) == <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-comment"><span class="hljs-comment"># This is a header row giving the shape of the matrix continue labels.append(items[0]) values = np.array([float(x) for x in items[1:]], 'f') rows.append(values) arr = np.vstack(rows) return pd.DataFrame(arr, index=labels, dtype='f') embeddings = load_embeddings('data/glove.42B.300d.txt') embeddings.shape # (1917494, 300)</span></span></code> </pre><br><h2>  Step 2. Standard Lexical Lexicon </h2><br>  We need somewhere to take information about which words have a positive tone and which ones have a negative one.  There are many lexicons of tonality, but, as usual, we will choose one of the simplest.  Download the <a href="https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html">archive from the Bin Liu website</a> and extract the lexicon files, <code>data/positive-words.txt</code> and <code>data/negative-words.txt</code> . <br><br>  Next, we need to set a way to read these files and read their contents into the variables <code>pos_words</code> and <code>neg_words</code> . <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_lexicon</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(filename)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Load a file from Bing Liu's sentiment lexicon (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing English words in Latin-1 encoding. One file contains a list of positive words, and the other contains a list of negative words. The files contain comment lines starting with ';' and blank lines, which should be skipped. """</span></span> lexicon = [] <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(filename, encoding=<span class="hljs-string"><span class="hljs-string">'latin-1'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> infile: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> infile: line = line.rstrip() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> line.startswith(<span class="hljs-string"><span class="hljs-string">';'</span></span>): lexicon.append(line) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> lexicon pos_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/positive-words.txt'</span></span>) neg_words = load_lexicon(<span class="hljs-string"><span class="hljs-string">'data/negative-words.txt'</span></span>)</code> </pre><br><h2>  Step 3. Learning a model for predicting the tonality of words </h2><br>  Some words are missing in the GloVe dictionary.  If the vector value is absent, then as a result of reading we get a vector from the values ‚Äã‚Äãof NaN.  Remove such vectors. <br><br><pre> <code class="python hljs">pos_vectors = embeddings.loc[pos_words].dropna() neg_vectors = embeddings.loc[neg_words].dropna()</code> </pre><br>  Next, create arrays of the desired input and output data.  Input: vector word meanings;  output: 1 for positively colored words and -1 for negatively colored.  We also need to save the words themselves in order to be able to interpret the results. <br><br><pre> <code class="python hljs">vectors = pd.concat([pos_vectors, neg_vectors]) targets = np.array([<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> pos_vectors.index] + [<span class="hljs-number"><span class="hljs-number">-1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entry <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> neg_vectors.index]) labels = list(pos_vectors.index) + list(neg_vectors.index)</code> </pre><br><h4>  Give me a sec!  But after all, some words are neutral, they are deprived of any tonality.  Don't we need a third grade for neutral words? </h4><br>  I think examples of neutral words would be useful to us, especially since the problems that we face arise from attributing a tone to neutral words.  If we could reliably identify neutral words, then the complication of the classifier (the addition of the third class) would be justified.  To do this, we need a source of examples of neutral words, because in the set we have chosen there are only positive and negatively colored words. <br><br>  Therefore, I created a separate version of this notebook, added 800 neutral words as examples and set a large weighting factor for the words to be neutral.  But the results were almost identical to those presented below. <br><br><h4>  How did the creators of the list share the words of positive and negative tonality?  Doesn't tonality depend on the context? </h4><br>  Good question.  A general analysis of the tonality of the text is not as simple as it seems.  The border we are trying to find is not always straightforward.  In the list that we chose, the word ‚Äúimpudent‚Äù is marked as bad, and ‚Äúambitious‚Äù - as good.  ‚ÄúComical‚Äù is bad, ‚Äúfunny‚Äù is good.  ‚ÄúReimbursement‚Äù is good, although situations in which you or you demand reimbursement are rarely pleasant. <br><br>  I think everyone understands that the tonality of a word depends on the context, but if we implement a simple approach to the analysis of tonality, we assume that the averaged values ‚Äã‚Äãof the tonality of words will allow us to get a generally correct answer without considering the context. <br><br>  We will divide the input vectors, output values, and labels into sets of training and test data.  For testing we will use 10% of the data. <br><br><pre> <code class="python hljs">train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \ train_test_split(vectors, targets, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre><br>  Next, we compose a classifier and start training for it - 100 iterations of processing training vectors.  As a loss function, we use a logistic function.  So our classifier will be able to calculate the probability that a given word is positive or negative. <br><br><pre> <code class="python hljs">model = SGDClassifier(loss=<span class="hljs-string"><span class="hljs-string">'log'</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">0</span></span>, n_iter=<span class="hljs-number"><span class="hljs-number">100</span></span>) model.fit(train_vectors, train_targets)</code> </pre><br>  Now check the classifier on the test vectors.  It turns out that he correctly recognizes the tonality of words outside the training set in 95% of cases.  Not bad at all. <br><br><pre> <code class="python hljs">accuracy_score(model.predict(test_vectors), test_targets) <span class="hljs-comment"><span class="hljs-comment"># 0,95022624434389136</span></span></code> </pre><br>  We also define a function that will show the tonality of individual words predicted by the classifier.  Our classifier is able to assess the tonality of words that are not included in the training set. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">vecs_to_sentiment</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(vecs)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># predict_log_proba gives the log probability for each class predictions = model.predict_log_proba(vecs) # To see an overall positive vs. negative classification in one number, # we take the log probability of positive sentiment minus the log # probability of negative sentiment. return predictions[:, 1] - predictions[:, 0] def words_to_sentiment(words): vecs = embeddings.loc[words].dropna() log_odds = vecs_to_sentiment(vecs) return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)</span></span></code> </pre><br><h2>  Step 4. Get an assessment of the tonality of the text </h2><br>  There are many ways to evaluate text tonality based on tonality values ‚Äã‚Äãfor vector representations of individual words.  We will continue to follow the path of least resistance and just average them. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re TOKEN_RE = re.compile(<span class="hljs-string"><span class="hljs-string">r"\w.*?\b"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># The regex above finds tokens that start with a word-like character (\w), and continues # matching characters (.+?) until the next word break (\b). It's a relatively simple # expression that manages to extract something very much like words from text. def text_to_sentiment(text): tokens = [token.casefold() for token in TOKEN_RE.findall(text)] sentiments = words_to_sentiment(tokens) return sentiments['sentiment'].mean()</span></span></code> </pre><br>  <b>What can be improved here?</b> <br><br><ul><li>  Calculate weights for words that are inversely proportional to their frequency so that the most common words (for example, the or I) do not have a strong effect on the assessment of tonality. </li><li>  Modify the averaging formula so that it does not get the largest modulo tonality estimates for short sentences. </li><li>  Take into account the context, that is, the whole phrase. </li><li>  Use a more functional sentence-word splitting algorithm that correctly processes apostrophes. </li><li>  Consider negatives, i.e., correctly handle phrases such as not happy. </li></ul><br>  But for all this, you need to write additional code, and the results below will not change fundamentally.  At the very least, we can roughly compare the relative emotional color of various sentences. <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"this example is pretty cool"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># 3.889968926086298 text_to_sentiment("this example is okay") # 2.7997773492425186 text_to_sentiment("meh, this example sucks") # -1.1774475917460698</span></span></code> </pre><br><h2>  Step 5. Fear the monster we created </h2><br>  Some sentences will not contain words with a single key.  Let's see how our system handles several analogs of the same neutral offer. <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"Let's go get Italian food"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># 2.0429166109408983 text_to_sentiment("Let's go get Chinese food") # 1.4094033658140972 text_to_sentiment("Let's go get Mexican food") # 0.38801985560121732</span></span></code> </pre><br>  Approximately the same thing happened to me in other experiments that analyzed reviews of restaurants using vector word meanings.  Then it turned out that all Mexican restaurants get a lower tonality mark without any objective reason. <br><br>  If you process words with context, the vector meanings of words can reflect subtle nuances of meaning.  So, they allow to detect and more pronounced phenomena, such as social prejudices. <br><br>  Here are some more neutral suggestions. <br><br><pre> <code class="python hljs">text_to_sentiment(<span class="hljs-string"><span class="hljs-string">"My name is Emily"</span></span>) <span class="hljs-comment"><span class="hljs-comment"># 2.2286179364745311 text_to_sentiment("My name is Heather") # 1.3976291151079159 text_to_sentiment("My name is Yvette") # 0.98463802132985556 text_to_sentiment("My name is Shaniqua") # -0.47048131775890656</span></span></code> </pre><br>  Well well. <br><br>  The name change alone greatly changes the assessment of the tonality that the system gives out.  This and many other examples show that when using names that are associated with white people, the predicted tone is on average more positive than with stereotypical names for people with dark skin. <br><br>  So, making sure that even the most basic implementation of AI is terribly biased, I suggest taking a short pause to think about it.  <a href="https://habrahabr.ru/company/microsoft/blog/337272/">In the second article,</a> we will return to the topic and will correct the mistakes of the non-intelligent AI. </div><p>Source: <a href="https://habr.com/ru/post/336358/">https://habr.com/ru/post/336358/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../336346/index.html">How I did realtime roulette on NodeJS & VueJS</a></li>
<li><a href="../336348/index.html">Using the terminal when developing for Android</a></li>
<li><a href="../336350/index.html">Calculation of the weight spectrum of a linear subspace in Wolfram Mathematica</a></li>
<li><a href="../336352/index.html">Convenient use of Redux in vue components</a></li>
<li><a href="../336356/index.html">App Store on iOS 11: what it will be and what it means</a></li>
<li><a href="../336360/index.html">B2P: how in 5 years to collect 3,000 news for a corporate publication in an IT company numbering> 100 people</a></li>
<li><a href="../336362/index.html">Barclays Bank installed spyware sensors to track employees in the workplace</a></li>
<li><a href="../336364/index.html">Games for programmers, part two</a></li>
<li><a href="../336368/index.html">Go 1.9 release</a></li>
<li><a href="../336372/index.html">TM visiting Booking.com</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>