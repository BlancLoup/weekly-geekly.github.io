<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Acquaintance with shaders on the example of GPUImage</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I am going to describe the development of an application for an iphone that will process video from the device‚Äôs camera in real time. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Acquaintance with shaders on the example of GPUImage</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/467/333/586/467333586cfe4711a12a20a6c6d014c6.png"><br><br>  In this article I am going to describe the development of an application for an iphone that will process video from the device‚Äôs camera in real time.  To do this, we will use the GPUImage framework, we will write our own shader on OpenGL ES and try to understand what filters are for image processing. <br><a name="habracut"></a><br><br><h4>  GPUImage framework </h4><br>  GPUImage is an iOS library written by Brad Larson and distributed under a BSD-licensed license that allows you to apply filters and other effects using the GPU to movies, live video and images. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  GPU vs CPU </h4><br>  Each iphone is equipped with two processors: CPU and GPU, each of which has its own strengths and weaknesses. <br>  When you write in C or Objective-C in Xcode, you create instructions that will be executed exclusively on the CPU.  The GPU, on the contrary, is a specialized chip, particularly well suited for computing, which can be divided into many small, independent operations, such as, for example, graphics rendering.  The types of instructions for the GPU are radically different from the CPUs, so we write code in another language, in OpenGL (or more precisely in the shader language GLSL). <br>  Comparing the performance of video rendering on the CPU and GPU, it is noticeable that the differences are huge: <br><br>  Frame rate: CPU vs.  GPU (Larger FPS is better) <br><br><table><tbody><tr><th>  Calculations </th><th>  GPU FPS </th><th>  CPU FPS </th><th>  Œî </th></tr><tr><td>  Threshold ‚òì 1 </td><td>  60.00 </td><td>  4.21 </td><td>  14.3 </td></tr><tr><td>  Threshold ‚òì 2 </td><td>  33.63 </td><td>  2.36 </td><td>  14.3 </td></tr><tr><td>  Threshold ‚òì 100 </td><td>  1.45 </td><td>  0.05 </td><td>  28.7 </td></tr></tbody></table><br><br><h4>  GPUImage vs Core Image </h4><br>  Core Image - a standard framework for image and video processing in almost real time.  Appeared since ios 5, and for this version had not such a large set of filters (although for most tasks it is quite enough), with the release of ios 6, the number of filters has increased significantly.  Core Image also allows for processing on both the CPU and the GPU. <br><br>  The main advantages of GPUImage over Core Image: <br><br><ul><li>  GPUImage allows you to write (create) your own filters (Core Image allows you to do this for now only on OS X, not on iOS); </li><li>  GPUImage is faster than Core Image; </li><li>  GPUImage uses GLSL instead of its own language; </li><li>  GPUImage is Open Source; </li></ul><br><br>  GPUImage is also a good way to start learning about OpenGL, as there are a lot of examples, documentation and ready-made solutions.  You can immediately move on to more exciting things, for example, writing new filters, and soon see the results! <br><br><h4>  GPUImage structure </h4><br>  GPUImage is inherently an abstraction on Objective-C around the rendering pipeline.  Images from an external source, be it a camera, a network or a disk, are loaded and modified passing through a chain of filters and giving the result as an image (UIImage), directly rendering to the screen (via GPUImageVIew) or just a data stream. <br><br><img src="https://habrastorage.org/storage2/c22/dbd/c08/c22dbdc0866c7a7c1dc2c61889e5111c.png"><br><br>  Speaking in a different language, the GPUImage API contains thousands of camera applications that are just waiting for the right combination of filters and a little imagination. <br>  For example, you can apply a <i>Color Levels</i> filter to an image from a video camera to simulate various types of color blindness and display them in real time. <br><div class="spoiler">  <b class="spoiler_title">Color levels</b> <div class="spoiler_text"><pre><code class="objectivec hljs">GPUImageVideoCamera *videoCamera = [[GPUImageVideoCamera alloc] initWithSessionPreset:<span class="hljs-built_in"><span class="hljs-built_in">AVCaptureSessionPreset640x480</span></span> cameraPosition:<span class="hljs-built_in"><span class="hljs-built_in">AVCaptureDevicePositionBack</span></span>]; videoCamera.outputImageOrientation = <span class="hljs-built_in"><span class="hljs-built_in">UIInterfaceOrientationPortrait</span></span>; GPUImageFilter *filter = [[GPUImageLevelsFilter alloc] initWithFragmentShaderFromFile:<span class="hljs-string"><span class="hljs-string">@"CustomShader"</span></span>]; [filter setRedMin:<span class="hljs-number"><span class="hljs-number">0.299</span></span> gamma:<span class="hljs-number"><span class="hljs-number">1.0</span></span> max:<span class="hljs-number"><span class="hljs-number">1.0</span></span> minOut:<span class="hljs-number"><span class="hljs-number">0.0</span></span> maxOut:<span class="hljs-number"><span class="hljs-number">1.0</span></span>]; [filter setGreenMin:<span class="hljs-number"><span class="hljs-number">0.587</span></span> gamma:<span class="hljs-number"><span class="hljs-number">1.0</span></span> max:<span class="hljs-number"><span class="hljs-number">1.0</span></span> minOut:<span class="hljs-number"><span class="hljs-number">0.0</span></span> maxOut:<span class="hljs-number"><span class="hljs-number">1.0</span></span>]; [filter setBlueMin:<span class="hljs-number"><span class="hljs-number">0.114</span></span> gamma:<span class="hljs-number"><span class="hljs-number">1.0</span></span> max:<span class="hljs-number"><span class="hljs-number">1.0</span></span> minOut:<span class="hljs-number"><span class="hljs-number">0.0</span></span> maxOut:<span class="hljs-number"><span class="hljs-number">1.0</span></span>]; [videoCamera addTarget:filter]; GPUImageView *filteredVideoView = [[GPUImageView alloc] initWithFrame:<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.view.bounds)]; [filter addTarget:filteredVideoView]; [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.view addSubView:filteredVideoView]; [videoCamera startCameraCapture]</code> </pre> <br></div></div><br>  Seriously, the filter submission application that comes as an example for GPUImage can be hosted on the Apple Store for about $ 3.99 without any changes.  And by adding Twitter integration and a couple of sound effects, you can raise the price to some $ 6.99. <br><br><h4>  Vertex Shaders GPUImage </h4><br>  <i>A shader</i> is a program for a graphics processor that controls the behavior of a single stage of the graphics pipeline and processes the corresponding input data. <br>  For a better understanding of what is happening in the main part of the article, let's touch on top of the vertex shaders. <br>  When working with an image, most of the time we deal with two-dimensional objects.  The image is displayed on the plane, which is a rectangle.  This is necessary for OpenGL, because  everything exists in three-dimensional space.  If we want to draw something, we must first create a surface where we will draw.  OpenGL ES 2.0 can only draw triangles (as well as points and lines, but not rectangles), so the plane is built from two triangles. <br>  Vertex shaders are a small program for processing ONE vertex. <br>  This is what the standard vertex shader on GPUImage looks like: <br><div class="spoiler">  <b class="spoiler_title">Vertex shader</b> <div class="spoiler_text"><pre> <code class="cpp hljs">attribute vec4 position; attribute vec4 inputTextureCoordinate; varying vec2 textureCoordinate; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ gl_Position = position; textureCoordinate = inputTextureCoordinate.xy; }</code> </pre></div></div><br>  Three types of variables are passed to the shader: attributes, varyings, and uniforms.  For each vertex, its own attributes are transferred - position in space, texture coordinates (how the texture will be displayed on the shape), color, normal, etc. <br>  Varying variables are the connection between the vertex and fragment shaders.  Variables of this type are declared and initialized in the vertex shader and then passed to the fragment shader.  But, since  the fragment shader operates with points on the whole shape - these values ‚Äã‚Äãare interpolated linearly.  For example, if the vertices of the left half of the shape are white and the right half is black, then the color of the shape will represent the gradient from white to black. <br>  Uniforms - variables are necessary for the shader to communicate with the outside world (the program itself).  They are the same for all vertices and fragments. <br>  In GPUImage we transfer two sets of coordinates - these are the coordinates of the plane itself and the texture coordinates.  As a rule, we will not have to take care of this, so we will not go into details. <br>  We will not include the vertex shader in our developed filter, but we will use the standard shader from the <i>GPUImageFilter</i> class. <br><br><h4>  Create a new project GPUImage </h4><br>  Let's create a new project for iphone.  To do this, run: File-&gt; Project ‚Üí Single View Application.  You can leave or uncheck the storyboards and ARC. <br>  Next, connect the frameworks to our created project (right-click on the folder with the framework ‚Üí Add files).  Then we should add some frameworks and libraries shown in the screenshot: <br><br><img src="https://habrastorage.org/storage2/236/869/c55/236869c55e265ba843dc507d0495253b.png"><br>  Finally, in the project build settings, we need to add the -ObjC flag to the rest of the linker flags and specify the location of the folder with the GPUImage framework in the ‚Äúsearch search paths‚Äù header. <br>  Now everything is ready and we can start writing our own filter!  We are going to create a <b>polar pixellate</b> shader and expand it by adding curl and posterisation (reducing the number of displayed colors on the screen). <br><br><h4>  Polar Pixellate Posterize Filter </h4><br>  Our filter will use the polar coordinate system to pixelate the incoming image. <br>  The first thing to do is create a new class inherited from GPUImageFilter.  Let's call it GPUImagePolarPixellatePosterizeFilter. <br><div class="spoiler">  <b class="spoiler_title">GPUImageFilter</b> <div class="spoiler_text"><pre> <code class="objectivec hljs"><span class="hljs-meta"><span class="hljs-meta">#import </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"GPUImageFilter.h"</span></span></span><span class="hljs-meta"> @interface GPUImagePolarPixellateFilterPosertize : GPUImageFilter { GLint centerUniform, pixelSizeUniform; } // The center about which to apply the distortion, with a default of (0.5, 0.5) @property(readwrite, nonatomic) CGPoint center; // The amount of distortion to apply, from (-2.0, -2.0) to (2.0, 2.0), with a default of (0.05, 0.05) @property(readwrite, nonatomic) CGSize pixelSize; @end</span></span></code> </pre><br></div></div><br>  We are going to pass two uniform variables in this filter.  The centerUniform variable is the point from which the polar coordinate system originates - by default it is 0.5, 0.5, that is, the center of the screen.  The coordinate system in OpenGL ranges from 0.0, 0.0 to 1.0, 1.0, with the beginning in the lower left corner (note the text about texture coordinates).  The pixellate value determines how large the 'pixels' will be after the filter is applied.  Since we use the polar coordinate system, the value of "x" is the radius (distance from the "center"), and the other value is the angle in radians. <br>  And although GPUImageFilter takes responsibility for configuring OpenGL and creating the necessary framebuffers, we still need to write the shader itself and pass the necessary uniform variables to it. <br><br><h4>  Our first shader </h4><br>  Add the following code before the @ implementation line: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="cpp hljs">NSString *<span class="hljs-keyword"><span class="hljs-keyword">const</span></span> kGPUImagePolarPixellatePosterizeFragmentShaderString = SHADER_STRING ( varying highp vec2 textureCoordinate; uniform highp vec2 center; uniform highp vec2 pixelSize; uniform sampler2D inputImageTexture; <span class="hljs-keyword"><span class="hljs-keyword">void</span></span> main() { highp vec2 normCoord = <span class="hljs-number"><span class="hljs-number">2.0</span></span> * textureCoordinate - <span class="hljs-number"><span class="hljs-number">1.0</span></span>; highp vec2 normCenter = <span class="hljs-number"><span class="hljs-number">2.0</span></span> * center - <span class="hljs-number"><span class="hljs-number">1.0</span></span>; normCoord -= normCenter; highp <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> r = length(normCoord); <span class="hljs-comment"><span class="hljs-comment">// to polar coords highp float phi = atan(normCoord.y, normCoord.x); // to polar coords r = r - mod(r, pixelSize.x) + 0.03; phi = phi - mod(phi, pixelSize.y); normCoord.x = r * cos(phi); normCoord.y = r * sin(phi); normCoord += normCenter; mediump vec2 textureCoordinateToUse = normCoord / 2.0 + 0.5; mediump vec4 color = texture2D(inputImageTexture, textureCoordinateToUse ); color = color - mod(color, 0.1); gl_FragColor = color; } );</span></span></code> </pre><br></div></div><br><br>  The shader code is enclosed in the SHADER_STRING () macro to represent it as an NSString string. <br>  A few words about data types and operations in the GLSL.  The main data types used are int, float, vector (vec2, vec3, vec4) and matrices (mat2, mat3, mat4).  You can perform simple arithmetic operations on matrices and vectors, such as, for example, the addition of vec2 + vec2.  In addition, operations to multiply a vector by a number (int or float) are allowed, for example: float * vec2 = vec2.x * float, vec2.y * float.  You can also use the appeal: vec4.xyz, if you want to get vec3.  A full list of supported types can be found <a href="http://iphonedevelopment.blogspot.ru/2010/11/opengl-es-20-for-ios-chapter-4.html">here</a> . <br>  Let's see what's going on here.  Varying textureCoordinate of type vec2 comes to us from the vertex shader by default.  Uniform - the center and pixelSize variables are variables that we pass from our filter class.  And finally, we have a variable inputImageTexture of type sampler2D.  This uniform is set by the superclass, GPUImageFilter, and is a two-dimensional image texture that we want to process. <br>  You may have noticed that we use the highp classifier all the time.  This is done to tell GLSL about the level of accuracy of our data types.  As you can imagine, the higher the accuracy, the greater the accuracy of our data types.  But this is not always relevant - for a simple rendering on the screen, less accuracy will fit, which will allow to perform calculations a little faster.  Accuracy classifiers are lowp, mediump, highp.  You can learn more about accuracy and actual limitations <a href="http://db-in.com/blog/2011/02/all-about-opengl-es-2-x-part-23/">here</a> . <br>  The shader always has the main function main ().  The result of the fragment shader is the color that will be set for the fragment being processed.  This color in our case is taken from the original image at the coordinates normCoord.  We will use this value to produce pixelation depending on the position in the polar coordinate system. <br><br>  The first thing we do is turn our coordinate system into a system with polar coordinates.  The variable TextureCoordinate is defined on the interval from 0.0, 0.0 to 1.0, 1.0.  The uniform variable Center is defined in the same range.  In order to describe our screen in polar coordinates, we need coordinates from -1.0, -1.0 to 1.0, 1.0.  The first two lines do this conversion.  The third line subtracts from the center normCoord.  Those.  we simply shifted the coordinate system to a new point centered on normCoord.  Find the value of the radius and angle phi, after which we will again return to the Cartesian coordinate system, shifting its center to its original place.  Thus, we obtain the range 0.0, 0.0 1.0, 1.0, which is needed for texture search.  To do this, call the texture2D () function, which takes two parameters as input: a two-dimensional texture object (in our case, inputImage) and coordinates (textureCoordinateToUse). <br>  Finally, we reduce the color gamut for red, green, blue (and alpha, but alpha is always 1.0 in our case, so ...) from 256 values ‚Äã‚Äãfor each component (16.8 million colors) to 10 (1000 colors) . <br>  This is our fragment shader and it will work very quickly.  If we had to do the same operation on a processor (CPU) to achieve the same goal, it would take much longer.  In many cases, real-time video filtering can be performed on the GPU, which would be impossible using the CPU. <br><br><h4>  Finishing the development of the filter </h4><br>  After we wrote the shader, the only thing left for us is setters for uniform variables.  During initialization, we pass the shader text to the superclass, define pointers to uniform ‚Äî variables, and set some default values. <br>  Add the following code after @implementation: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="objectivec hljs"><span class="hljs-keyword"><span class="hljs-keyword">@synthesize</span></span> center = _center; <span class="hljs-keyword"><span class="hljs-keyword">@synthesize</span></span> pixelSize = _pixelSize; <span class="hljs-meta"><span class="hljs-meta">#pragma mark - #pragma mark Initialization and teardown - (id)init { if (!(self = [super initWithFragmentShaderFromString:kGPUImagePolarPixellatePosterizeFragmentShaderString])) { return nil; } pixelSizeUniform = [filterProgram uniformIndex:@</span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"pixelSize"</span></span></span><span class="hljs-meta">]; centerUniform = [filterProgram uniformIndex:@</span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"center"</span></span></span><span class="hljs-meta">]; self.pixelSize = CGSizeMake(0.05, 0.05); self.center = CGPointMake(0.5, 0.5); return self; }</span></span></code> </pre><br></div></div><br>  When initWithFragmentShaderFromString is called, our shader passes through the appropriate methods for checking and compiling so that it is ready to run on the GPU.  If we wanted to submit a vertex shader in the same way, then there is the same challenge for it to go through all the same operations. <br>  We must call [filterProgram uniformIndex:] for each uniform variable in our shader.  This method returns a Glint pointer to a Uniform variable, with which we can set the value of the variable. <br>  Finally, we set some default values ‚Äã‚Äãduring the initialization phase so that our filter works without user interaction. <br>  The last thing we need to do is set the setters and getters for our uniform variables: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="objectivec hljs">- (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)setPixelSize:(<span class="hljs-built_in"><span class="hljs-built_in">CGSize</span></span>)pixelSize { _pixelSize = pixelSize; [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> setSize:pixelSize forUniform: pixelSizeUniform program:filterProgram]; } - (<span class="hljs-keyword"><span class="hljs-keyword">void</span></span>)setCenter:(<span class="hljs-built_in"><span class="hljs-built_in">CGPoint</span></span>)newValue; { _center = newValue; [<span class="hljs-keyword"><span class="hljs-keyword">self</span></span> setPoint:newValue forUniform: centerUniform program:filterProgram]; }</code> </pre><br></div></div><br><br><h4>  Create an application </h4><br>  Now we will create a simple video application.  Let's go to the View Controller class that we took as a template and set up earlier.  Modify the following lines in this file: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="objectivec hljs"><span class="hljs-meta"><span class="hljs-meta">#import </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"JGViewController.h"</span></span></span><span class="hljs-meta"> #import </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"GPUImage.h"</span></span></span><span class="hljs-meta"> #import </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"GPUImagePolarPixellatePosterizeFilter.h"</span></span></span><span class="hljs-meta"> @interface JGViewController () { GPUImageVideoCamera *vc; GPUImagePolarPixellatePosterizeFilter *ppf; } @end @implementation JGViewController - (void)viewDidLoad { [super viewDidLoad]; vc = [[GPUImageVideoCamera alloc] initWithSessionPreset:AVCaptureSessionPreset640x480 cameraPosition:AVCaptureDevicePositionBack ]; vc.outputImageOrientation = UIInterfaceOrientationPortrait; ppf = [[GPUImagePolarPixellatePosterizeFilter alloc] init]; [vc addTarget:ppf]; GPUImageView *v = [[GPUImageView alloc] init]; [ppf addTarget:v]; self.view = v; [vc startCameraCapture]; } -(void)touchesBegan:(NSSet *)touches withEvent:(UIEvent *)event { CGPoint location = [[touches anyObject] locationInView:self.view]; CGSize pixelS = CGSizeMake(location.x / self.view.bounds.size.width * 0.5, location.y / self.view.bounds.size.height * 0.5); [ppf setPixelSize:pixelS]; } -(void)touchesMoved:(NSSet *)touches withEvent:(UIEvent *)event { CGPoint location = [[touches anyObject] locationInView:self.view]; CGSize pixelS = CGSizeMake(location.x / self.view.bounds.size.width * 0.5, location.y / self.view.bounds.size.height * 0.5); [ppf setPixelSize:pixelS]; }</span></span></code> </pre><br></div></div><br><br>  Create a video camera with a specific resolution and location GPUImageVideoCamera and our filter GPUImagePolarPixellatePosterizeFilter. <br>  And set the GPUImageView as the main view of our view controller. <br>  Thus, our pipeline looks like this: camera video - pixel filtering and pasteurization (polarpixellateposterizefilter) - GPUImageView, which we will use to display the video on the phone screen. <br><br>  At this stage, we can already start the application and get a working filter.  But it would be nice to add some more interactivity!  To do this, we use the <i>touchesmoved</i> and <i>touchesbegan methods</i> , which capture the pressure, affecting the pixelSize uniform variable of our filter. <br>  To get the smallest 'pixels', just touch in the upper left corner of the image, and for the largest - in the lower right.  Now you can experiment with the filter yourself to get completely different results. <br>  Congratulations!  You wrote your first shader! <br><br><h4>  Other examples of image processing using shaders </h4><br>  Reducing the levels of red and green in the image, increasing the blue: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="cpp hljs">lowp vec4 color = sampler2D(inputImageTexture, textureCoordinate); lowp vec4 alter = vec4(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">1.5</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>); gl_FragColor = color * alter;</code> </pre><br></div></div><br>  Brightness reduction: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="cpp hljs">lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate); gl_FragColor = vec4((textureColor.rgb + vec3(<span class="hljs-number"><span class="hljs-number">-0.5</span></span>)), textureColor.w);</code> </pre><br></div></div><br>  Popular image blur: <br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="cpp hljs">mediump <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> texelWidthOffset = <span class="hljs-number"><span class="hljs-number">0.01</span></span>; mediump <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> texelHeightOffset = <span class="hljs-number"><span class="hljs-number">0.01</span></span>; vec2 firstOffset = vec2(<span class="hljs-number"><span class="hljs-number">1.5</span></span> * texelWidthOffset, <span class="hljs-number"><span class="hljs-number">1.5</span></span> * texelHeightOffset); vec2 secondOffset = vec2(<span class="hljs-number"><span class="hljs-number">3.5</span></span> * texelWidthOffset, <span class="hljs-number"><span class="hljs-number">3.5</span></span> * texelHeightOffset); mediump oneStepLeftTextureCoordinate = inputTextureCoordinate - firstOffset; mediump twoStepsLeftTextureCoordinate = inputTextureCoordinate - secondOffset; mediump oneStepRightTextureCoordinate = inputTextureCoordinate + firstOffset; mediump twoStepsRightTextureCoordinate = inputTextureCoordinate + secondOffset; mediump vec4 fragmentColor = texture2D(inputImageTexture, inputTextureCoordinate) * <span class="hljs-number"><span class="hljs-number">0.2</span></span>; fragmentColor += texture2D(inputImageTexture, oneStepLeftTextureCoordinate) * <span class="hljs-number"><span class="hljs-number">0.2</span></span>; fragmentColor += texture2D(inputImageTexture, oneStepRightTextureCoordinate) * <span class="hljs-number"><span class="hljs-number">0.2</span></span>; fragmentColor += texture2D(inputImageTexture, twoStepsLeftTextureCoordinate) * <span class="hljs-number"><span class="hljs-number">0.2</span></span>; fragmentColor += texture2D(inputImageTexture, twoStepsRightTextureCoordinate) * <span class="hljs-number"><span class="hljs-number">0.2</span></span>; gl_FragColor = fragmentColor;</code> </pre><br></div></div><br><br>  Getting started with GPUImage is easy enough and it is quite powerful to embody everything you dreamed of.  And even more, GPUImage is a dizzying number of filters, color settings, blending modes, and visual effects that you could only dream of (or did not even know about their existence).  You can find a bunch of examples using modern filters that include contour detection, fisheye, and a ton of other cool stuff. <br>  <a href="https://bitbucket.org/brovik/gpuimage-sample-project/src/031f2d9f0693edd12a3f1fbcfc6b9314223146ed%3Fat%3Dmaster">Sources</a> <br>  An introduction to OpenGL ES 2.0 (GLSL) and a graphics pipeline device: <a href="http://www.raywenderlich.com/3664/opengl-es-2-0-for-iphone-tutorial">one</a> , <a href="http://db-in.com/blog/2011/02/all-about-opengl-es-2-x-part-23/">two</a> , <a href="http://iphonedevelopment.blogspot.ru/2010/11/opengl-es-20-for-ios-chapter-4.html">three</a> . <br>  The article is a creative adaptation with the processing of knowledge and translation: <a href="http://indieambitions.com/idevblogaday/learning-opengl-gpuimage/">source 1</a> , <a href="http://nshipster.com/gpuimage/">source 2</a> </div><p>Source: <a href="https://habr.com/ru/post/181794/">https://habr.com/ru/post/181794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../181780/index.html">Two networks with one cable between Mikrotik Router OS and Shibby Tomato USB on Netgear WNR3500L</a></li>
<li><a href="../181782/index.html">jVForms.js - cross-browser form checking polyfill</a></li>
<li><a href="../181786/index.html">The digest of interesting materials from the world of web development and IT for the last week ‚Ññ59 (May 25 - 31, 2013)</a></li>
<li><a href="../181790/index.html">How to make friends gma3600 (intel cedar trail) and linux in a week</a></li>
<li><a href="../181792/index.html">Space miners went to Kickstarter</a></li>
<li><a href="../181796/index.html">The f.lux 3 program has been released with a movie mode and a temperature of 2700 K</a></li>
<li><a href="../181798/index.html">How to learn to relax</a></li>
<li><a href="../181804/index.html">ASP.NET Dynamic Data based data management system</a></li>
<li><a href="../181806/index.html">Tizen launched on Nexus 7 3G (video)</a></li>
<li><a href="../181808/index.html">The famous "cemetery of video games" are going to dig</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>