<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic models: examples and pictures</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today is the second series of the cycle begun last time ; then we talked about directed graphical probabilistic models, drew the main pictures of this...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic models: examples and pictures</h1><div class="post__text post__text-html js-mediator-article">  Today is the second series of the cycle begun <a href="http://habrahabr.ru/company/surfingbird/blog/176461/">last time</a> ;  then we talked about directed graphical probabilistic models, drew the main pictures of this science and discussed what dependencies and independence they correspond to.  Today - a series of illustrations to the material of the past;  We will discuss several important and interesting models, draw pictures corresponding to them and see how they correspond to the factorizations of the joint distribution of all variables. <br><br><img src="https://habrastorage.org/storage2/362/813/64a/36281364aee8578bc622e67937f0f94e.jpeg"><br><a name="habracut"></a><br><br><h3>  Naive Bayes Classifier </h3><br>  To begin with, I briefly repeat the previous text: we <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">have already talked about the naive Bayes classifier</a> in this blog.  In naive Bayes, an additional assumption is made about the conditional independence of attributes (words) subject to the condition: <br><img src="https://habrastorage.org/getpro/habr/post_images/7c8/de2/d30/7c8de2d301c1aa296a1da6b53d192084.png" alt="image"><br>  The result is a complex posterior distribution. <img src="https://habrastorage.org/getpro/habr/post_images/183/4ca/260/1834ca260491d8d0f614bc5dca7d2fd1.png" alt="image">  managed to rewrite as <br><img src="https://habrastorage.org/getpro/habr/post_images/bd4/c38/94e/bd4c3894e1c9f351b451e5c286b398ec.png" alt="image"><br>  And what a picture of this model corresponds to: <br><img src="https://habrastorage.org/storage2/fd3/ba8/9d9/fd3ba89d91ec12ee0a320772d4df191f.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Everything is exactly as we said last time: individual words in a document are associated with a variable of a category by a diverging link;  this shows that they are conditionally independent under the condition of this category.  Learning naive bayes is to train the parameters of individual factors: a priori distribution on categories <i>p</i> ( <i>C</i> ) and conditional distributions of individual parameters <img src="https://habrastorage.org/getpro/habr/post_images/ab1/e4a/5e2/ab1e4a5e2010f465185dcc0f782709c4.png">  . <br><br>  One more note about the pictures before moving on.  Very often in models there are a large number of variables of the same type that are associated with other variables by the same distributions (possibly with different parameters).  To make the picture easier to read and understand, so that it doesn‚Äôt have innumerable dots and things like ‚Äúwell, here is a complete bipartite graph with dots, well, you understand,‚Äù it is convenient to combine the same type variables into so-called ‚Äúplates‚Äù.  To do this, draw a rectangle into which one typical representative of the variable being propagated is placed;  somewhere in the corner of the rectangle it is convenient to sign how many copies are meant: <br><img src="https://habrastorage.org/storage2/fad/0df/d9f/fad0dfd9fba1514115a92ca85f85ffa3.png"><br><br>  And the general model of all documents (without dies, we did not draw it) will consist of several copies of this graph and, accordingly, look like this: <br><img src="https://habrastorage.org/storage2/e48/311/890/e48311890c8dddecba930812494b43b5.png"><br><br>  Here I explicitly drew the distribution parameters on the categories Œ± and the parameters are the probabilities of words in each category Œ≤.  Since these parameters do not have a separate factor in the decomposition, the network node does not correspond to them, but it is often convenient to also depict them for clarity.  In this case, the picture means that different copies of the variable <i>C</i> were generated from the same distribution <i>p</i> ( <i>C</i> ), and different copies of the words were generated from the same distribution, parametrized by the category value (i.e., Œ≤ is the matrix probabilities of different words in different categories). <br><br><h3>  Linear regression </h3><br>  We will continue with another model that you may vaguely recall from a course of mathematical statistics - linear regression.  The essence of the model is simple: we assume that the variable <i>y</i> , which we want to predict, is obtained from the feature vector <b>x</b> as some linear function with weights <b>w</b> (bold font will denote vectors - this is generally accepted, and in html it will be more convenient for me than every time to draw an arrow) and normally distributed noise: <br><img src="https://habrastorage.org/getpro/habr/post_images/8da/ca6/063/8daca6063ac06db6d72cf12d356e2f43.png"><br>  The model assumes that some data set is available to us, dataset <i>D.</i>  It consists of separate implementations of this regression itself, and (important!) It is assumed that these implementations were generated independently.  In addition, a priori distribution of parameters is often introduced in linear regression ‚Äî for example, the normal distribution <br><img src="https://habrastorage.org/getpro/habr/post_images/380/51d/c25/38051dc25f98cbb27dfcdef8ff3ad342.png"><br>  Then we come to this picture: <br><img src="https://habrastorage.org/storage2/a5c/807/717/a5c807717164c22f42b2c40a61d25b9f.png"><br><br>  Here I explicitly drew the parameters of the prior distribution Œº <sub>0</sub> and Œ£ <sub>0</sub> .  Pay attention - linear regression is very similar in structure to naive bayes. <br><br>  With dies, the same thing will look even simpler: <br><img src="https://habrastorage.org/storage2/650/d5f/96d/650d5f96de485850eedd11198f387aab.png"><br><br>  What are the main tasks that are solved in linear regression?  The first task is to find the posterior distribution on <b>w</b> , i.e.  learn how to recalculate the distribution of <b>w</b> with the available data ( <b>x</b> , <i>y</i> ) of <i>D</i> ;  mathematically we have to calculate the distribution parameters <br><img src="https://habrastorage.org/getpro/habr/post_images/c66/15f/a9e/c6615fa9ee7da03c3d6e0b506ff03c48.png"><br>  In graphic models, the variables are usually hatched, the values ‚Äã‚Äãof which are known;  Thus, the task is to recalculate the distribution <b>w</b> for such a column with evidence: <br><img src="https://habrastorage.org/storage2/bc2/c2d/a5b/bc2c2da5b41436cdb38b37ea79d590b6.png"><br><br>  The second main task (something even more basic) is to calculate the predictive distribution, estimate the new value of <i>y</i> at some new point.  Mathematically, this task looks significantly more complicated than the previous one - now it is necessary to integrate by the posterior distribution <br><img src="https://habrastorage.org/getpro/habr/post_images/cfe/e51/0c3/cfee510c3808702ca758f91099d8fcb3.png"><br>  And graphically, not so much changes - we draw a new variable that we want to predict, but the task is still the same: with some evidence (from dataset) recalculate the distribution of some other variable in the model, only now it is not <b>w</b> , but <i>y</i> <sup>*</sup> : <br><img src="https://habrastorage.org/storage2/465/8f0/a8f/4658f0a8fb74cf0632fdd902ea15e026.png"><br><br><h3>  Hidden Markov Models </h3><br>  Another widely known and popular class of probabilistic models is hidden Markov models (HMM).  They are used in speech recognition, for fuzzy search for substrings and in other similar applications.  The hidden Markov model is a Markov chain (a sequence of random variables, where each value <i>x <sub>t + 1</sub></i> depends only on the previous <i>x <sub>t</sub></i> and under the condition <i>x <sub>t</sub></i> conditionally independent with the previous <i>x <sub>tk</sub></i> ), in which we cannot observe hidden states, but only see some observables <i>y <sub>t</sub></i> that depend on the current state.  For example, in speech recognition, hidden states are phonemes that you want to say (this is a simplification, in fact, each phoneme is an entire model, but it will come down for illustration), and the observed ones are sound waves that reach the discriminator.  The picture is this: <br><img src="https://habrastorage.org/storage2/1fb/511/a71/1fb511a7100a1968a9c8d9ef86eb308d.png"><br><br>  This picture is enough to solve the problem of applying the already hidden Markov model: according to the existing model (consisting of transition probabilities between hidden states <i>A</i> , initial distribution of chain œÄ and distribution parameters of observable <i>B</i> ) and given sequence of observables, find the most likely sequence of hidden states;  i.e., for example, in a ready-made speech recognition system, recognize a new sound file.  And if you need to train the model parameters, it is better to clearly draw them in the picture, so that it is clear that the same parameters are involved in all transitions: <br><img src="https://habrastorage.org/storage2/55e/a63/958/55ea63958511fd8b5a3cff0e45c98a76.png"><br><br><h3>  Lda </h3><br>  Another model that <a href="http://habrahabr.ru/company/surfingbird/blog/150607/">we already talked about</a> - LDA (latent Dirichlet allocation, latent Dirichlet placement).  This is a model for thematic modeling, in which each document is represented not by a single topic, as in naive Bayes, but by a discrete distribution on possible topics.  In the same text, we have already described the LDA generative model - how to generate a document in the finished LDA model: <br><ul><li>  choose the length of the document <i>N</i> (this is not drawn on the graph - it is not that part of the model); </li><li>  choose vector <img src="http://habr.habrastorage.org/post_images/fbf/5b0/617/fbf5b061708fff69d170c3e2e37645bc.png">  - the vector of "severity" of each topic in this document; </li><li>  for each of the <i>N</i> words <i>w</i> : <br><ul><li>  choose a topic <img src="http://habr.habrastorage.org/post_images/af2/163/d1e/af2163d1ecb7e905400d31b46e144a79.png">  by distribution <img src="http://habr.habrastorage.org/post_images/92f/665/7da/92f6657da9a33ba894fb8655730c8a24.png">  ; </li><li>  choose a word <img src="http://habr.habrastorage.org/post_images/3a3/1f3/4bf/3a31f34bf1e79e7297672d298bdb8200.png">  with probabilities given in Œ≤. </li></ul></li></ul><br>  Now we understand what the corresponding picture will look like (it was also in that blog post, and again I will copy it from <a href="http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation">Wikipedia</a> , but the essence of the picture here is exactly the same as ours above): <br><img src="http://habr.habrastorage.org/post_images/5eb/d92/a73/5ebd92a738eeb5f628c0061564559736.png"><br><br><h3>  SVD and PMF </h3><br>  In a series of posts ( <a href="http://habrahabr.ru/company/surfingbird/blog/139863/">1</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/140555/">2</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/141959/">3</a> , <a href="http://habrahabr.ru/company/surfingbird/blog/143455/">4</a> ) we talked about one of the main tools of collaborative filtering - the singular decomposition of matrices, SVD. <br><br>  We looked for the SVD-decomposition by the gradient descent method: we constructed the error function, counted the gradient from it, went down it.  However, it is possible to formulate a general probabilistic problem formulation, which is usually called PMF (probabilistic matrix factorization).  To do this, we need to introduce a priori distributions for the vectors of features of users and products: <br><img src="https://habrastorage.org/getpro/habr/post_images/102/b23/5f6/102b235f6098db93d673daa721494e63.png"><br>  (where <i>I</i> is the identity matrix), and then, as in a regular SVD, present ratings as noisy linear combinations of user and product attributes: <br><img src="https://habrastorage.org/getpro/habr/post_images/425/e0c/fe7/425e0cfe7b050e78eb7869a71dd50f4c.png"><br>  where the product is taken from the ratings present in the training set.  It turns out such a picture (the picture is taken from the <a href="http://www.cs.utoronto.ca/~amnih/papers/bpmf.pdf">article [Salakhutdinov, Mnih, 2009]</a> ): <br><img src="https://habrastorage.org/storage2/24b/819/bd3/24b819bd3fd3fbaa507a35c5b5fb52b6.png"><br><br>  You can add another level of Bayesian inference and train at the same time hyper-parameters of the distribution of signs for users and products;  I will not go into it now, but I will simply provide the corresponding picture (from <a href="http://www.cs.utoronto.ca/~amnih/papers/bpmf.pdf">the same article</a> ) - perhaps, it is still possible to talk about this in more detail. <br><img width="500" src="https://habrastorage.org/storage2/048/b9c/80a/048b9c80ab1934184f34688707ff3122.png"><br><br><h3>  Bayesian rating systems </h3><br>  Another example that is personally close to me is that once Alexander Syrotkin and I <a href="http://130.203.133.150/viewdoc/summary%3Fdoi%3D10.1.1.231.3702">improved</a> one of the Bayesian rating systems;  Perhaps later in the blog we will talk about rating systems in more detail.  But here I‚Äôll just give the simplest example - how does the Elo rating for chess players work?  If you do not go into approximations and magic constants, the essence is very simple: what is a rating in general?  We would like the rating to be the measure of the strength of the game;  however, it is quite obvious that the strength of the game from one game to another may change quite strongly under the influence of external and internal random factors.  Thus, in fact, the strength of a game of a participant in a particular game (comparison of these forces determines the outcome of the game) is a random variable, the ‚Äútrue power‚Äù of a game of a chess player is its expectation, and rating is our inaccurate assessment of this expectation. .  For the time being, we will consider the simplest case in which the player‚Äôs playing power in a particular game is normally distributed around his true power with some constant fixed dispersion in advance (Elo‚Äôs rating does just that - hence his magic constant ‚Äúa chess player with a strength of 200 points gains more an average of 0.75 points per game ‚Äù).  Before each game, we have some a priori estimates of the strength of the game of each chess player;  Suppose that the prior distribution is also normal, with the parameters Œº <sub>1</sub> , œÉ <sub>1</sub> and Œº <sub>2</sub> , œÉ <sub>2,</sub> respectively.  Our task - knowing the result of the party, recalculate these parameters.  The picture is this: <br><img src="https://habrastorage.org/storage2/c40/37c/0be/c4037c0be05dd8fc056863d3d9d0e259.png"><br><br>  Here <i>s <sub>i</sub></i> (skill) is the ‚Äútrue power of the game‚Äù of a chess player, <i>p <sub>i</sub></i> (performance) is his strength of the game shown in this particular game, and <i>r</i> is a rather interesting random variable showing the result of the game, which is obtained from comparing <i>p</i> <sub>1</sub> and <i>p</i> <sub>2</sub> .  More about this today will not. <br><br><h3>  Internet user behavior </h3><br>  And I will end with another close example to me - models of the behavior of Internet users in search engines.  Again, I won‚Äôt go into detail - maybe we‚Äôll come back to this, but for now you can read, for example, <a href="http://www.mathnet.ru/php/archive.phtml%3Fwshow%3Dpaper%26jrnid%3Dtrspy%26paperid%3D525%26option_lang%3Drus">our review article with Alexander Fishkov</a> - just consider one such model for example.  We are trying to simulate what the user does when receiving search results.  Viewing links and clicks are treated as random events;  For a specific request session, the variable <i>E <sub>i</sub></i> denotes viewing the description of the link to the document shown at position <i>i</i> , <i>C <sub>i</sub></i> - click on this position.  We introduce a simplifying assumption: suppose that the process of viewing descriptions always starts from the first position and is strictly linear.  Position is viewed only if all previous positions have been viewed.  As a result, our virtual user reads the links from top to bottom, if he liked (which depends on the relevance of the link), clicks, and if the document really turns out to be relevant, the user leaves and no longer returns;  curious, but true: a good event for a search engine is when a user leaves and does not return as quickly as possible, and if he returns to the list, he could not find what he was looking for. <br><br>  The result is the so-called cascading click model (cascading click model, CCM);  in it, the user follows this flowchart: <br><img src="https://habrastorage.org/storage2/3e4/bc3/092/3e4bc309249bc44e760f2c04eff07745.png"><br><br>  And in the form of a Bayesian network, you can draw it like this: <br><img src="https://habrastorage.org/storage2/2ed/1e3/c39/2ed1e3c39b86139e2d7c2d7d43206e81.png"><br>  Here the subsequent events of <i>E <sub>i</sub></i> (this event is ‚Äúuser read the next snippet‚Äù, from the word examine) depend on whether there was a click on the previous link, and if so, whether the link was really relevant.  The task is again described as above: we observe part of the variables (user clicks), and must, based on clicks, train the model and draw conclusions about the relevance of each link (in order to further reorder the links according to their true relevance), i.e. .  about the values ‚Äã‚Äãof some other variables in the model. <br><br><h3>  Conclusion and conclusions </h3><br>  In this article, we looked at a few examples of probabilistic models, the logic of which is easy to ‚Äúread‚Äù from their directed graphical models.  In addition, we were convinced that what we usually want from a probabilistic model can be represented as one fairly well-defined problem ‚Äî in a directed graphical model, recalculate the distribution of some variables with known values ‚Äã‚Äãof some other variables.  However, logic is logic, but how, in fact, to teach?  How to solve this problem?  About this - in the following series. </div><p>Source: <a href="https://habr.com/ru/post/177889/">https://habr.com/ru/post/177889/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../177879/index.html">We read Habr, xkcd and rss on kindle</a></li>
<li><a href="../177881/index.html">Unity3d stops supporting Flash</a></li>
<li><a href="../177883/index.html">Avast! removes sndvol32.exe (volume control) as a virus</a></li>
<li><a href="../177885/index.html">We configure Windows for programming of OpenGL</a></li>
<li><a href="../177887/index.html">The new BeagleBone Black is available for order, for only $ 45</a></li>
<li><a href="../177891/index.html">In the American special issue of Forbes magazine built an access point</a></li>
<li><a href="../177897/index.html">Ubuntu 13.04 released</a></li>
<li><a href="../177899/index.html">LinkMeUp. Release 2</a></li>
<li><a href="../177903/index.html">NASA launched three smartphones into orbit</a></li>
<li><a href="../177907/index.html">There is an idea, but there is no knowledge: we create an application from idea to release, with zero knowledge of the code</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>