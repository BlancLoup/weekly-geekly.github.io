<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Organization of safe testing in production. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article discusses the different types of testing in production and the conditions under which each of them is most useful, and also discusses how...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Organization of safe testing in production. Part 1</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/0q/az/yt/0qazyteu8a-cel_dnpcbmyckzsk.jpeg"><br><br>  This article discusses the different types of testing in production and the conditions under which each of them is most useful, and also discusses how to organize secure testing of various services in production. <a name="habracut"></a><br><br>  It is worth noting that the content of this article applies only to those <b>services</b> whose deployment is controlled by developers.  In addition, you should immediately warn that the use of any of the types of testing described here is a difficult task, which often requires major changes to the systems design, development and testing processes.  And, despite the title of the article, I do not consider that any of the types of testing in production is absolutely reliable.  There is only an opinion that such testing can significantly reduce the level of risks in the future, and the investment costs will be reasonable. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i><b>(Approx. Lane .: since the original article is a longrid, for the convenience of readers, it is divided into two parts).</b></i> <br><br><h2>  Why do you need testing in production if it can be performed on pricing? </h2><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-0" style="position: absolute; visibility: hidden; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="974530841190608897"></twitter-widget><blockquote class="twitter-tweet" data-lang="en_US" data-twitter-extracted-i155110693266837636="true"><p lang="en" dir="ltr">  It is a moose effect. <br><br>  It‚Äôs still better than ‚Äúworks on my machine‚Äù. </p>  - Cindy Sridharan (@copyconstruct) <a href="https://twitter.com/copyconstruct/status/974530841190608897%3Fref_src%3Dtwsrc%255Etfw">March 16, 2018</a> </blockquote><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  The value of a staging cluster (or staging environment) is perceived differently by different people.  For many companies, deploying and testing a product on rating is an essential step prior to its final release. <br><br>  Many well-known organizations perceive stying as a miniature copy of the work environment.  In such cases, it is necessary to ensure their maximum synchronization.  In this case, it is usually necessary to ensure the operation of differing instances of stateful systems, such as databases, and regularly synchronize data from the production environment with the staging.  The only exception is confidential information that allows you to identify the user (this is necessary to comply with the requirements of <a href="http://www.eugdpr.org/">GDPR</a> , <a href="http://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard">PCI</a> , <a href="http://www.hhs.gov/hipaa/for-professionals/security/laws-regulations/index.html">HIPAA</a> and other regulations). <br><br>  The problem with this approach (in my experience) is that the difference lies not only in the use of a separate database instance containing the actual production data of the environment.  Often the difference extends to the following aspects: <br><br><ul><li>  The size of staging cluster (if it can be called a ‚Äúcluster‚Äù - sometimes it‚Äôs just one server under the guise of a cluster); </li><li>  The fact that staging usually uses a cluster of a much smaller scale also means that the configuration parameters for <i>each</i> service will differ.  This applies to load balancers, databases, and queues, for example, the number of open file descriptors, the number of open connections to the database, the size of the thread pool, etc. If the configuration is stored in a database or key-value data repository (for example, Zookeeper or Consul), these support systems must also be present in the staging environment; </li><li>  The number of real-time connections handled by a stateless service, or the way the proxy server reuses TCP connections (if this procedure is performed at all); </li><li>  Lack of monitoring on pricing.  But even if it is monitored, some signals may be completely inaccurate, since an environment other than the working environment is monitored.  For example, even if you are monitoring the delay of a MySQL query or response time, it is difficult to determine whether a new code contains a query that can initiate a full table scan in MySQL, since it is <i>much</i> faster (and sometimes even preferable) to perform a full scan of the small table used in the test database, rather than a production database, where the query may have a completely different performance profile. </li></ul><br>  Although it is fair to assume that all the above differences are not serious arguments against the use of staging as such, unlike antipatterns, which should be avoided.  At the same time, the desire to do everything correctly often requires huge labor costs for engineers in an attempt to ensure compliance with the environments.  Production is constantly changing and influenced by various factors, so trying to achieve the specified match is like going nowhere. <br><br>  Moreover, even if the conditions on pricing are as close as possible to the working environment, there are other types of testing that are better applied based on real production information.  A good example would be soak testing, in which the reliability and stability of a service is checked over an extended period of time with actual levels of multitasking and workload.  It is used to detect memory leaks, determine the duration of pauses in the GC, the CPU load and other indicators for a certain period of time. <br><br>  None of the above does not imply that stending is <i>absolutely</i> useless (this will become apparent after reading the section on shadow data duplication when testing services).  This only indicates that quite often relying on rating rely to a greater extent than is necessary, and in many organizations it remains the <i>only</i> type of testing performed before the full product release. <br><br><h2>  The Art of Testing in Production </h2><br>  So historically, the concept of ‚Äútesting in production‚Äù is associated with certain stereotypes and negative connotations (‚Äúpartisan programming‚Äù, lack or absence of unit and integration testing, carelessness or inattention to the perception of the product by the end user). <br><br>  Testing in production will certainly deserve such a reputation if it is performed carelessly and inadequately.  It <i>does</i> not in any way <i>replace</i> testing at the pre-production stage, and under no circumstances is it a <i>simple task</i> .  Moreover, I argue that a <i>successful</i> and <i>secure</i> production testing requires a significant level of automation, a good understanding of established practices, and the design of systems with an initial focus on this type of testing. <br><br>  To organize a comprehensive and secure process for effectively testing services in production, it is important not to regard it as a generic term for a set of different tools and techniques.  This mistake, unfortunately, was made by me - <a href="http://medium.com/%40copyconstruct/testing-microservices-the-sane-way-9bb31d158c16">in my previous post I</a> presented not quite a scientific classification of testing methods, and in the section ‚ÄúTesting in Production‚Äù, various methodologies and tools were grouped together. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2h/bo/od/2hboodum_vrej99d2xm911ifecw.png"></div><br>  <i>From the note Testing Microservices, the sane way (‚ÄúA reasonable approach to testing microservices‚Äù)</i> <br><br>  Since the publication of the note at the end of December 2017, I discussed its content and, in general, the topic of testing in production with several people. <br><br>  During these discussions, as well as after a series of individual conversations, it became clear to me that the topic of testing in production cannot be reduced to several points listed above. <br><br>  The concept of "testing in production" includes a whole range of techniques used <i>in three different stages</i> .  What exactly - let's understand. <br><br><img src="https://habrastorage.org/webt/6a/wc/5-/6awc5-yiregc_-2pzrk4i7gjxx0.jpeg"><br><br><h2>  Three stages of production </h2><br>  Usually, discussions about production are conducted only in the context of deploying code to production, monitoring, or emergency situations, when something goes wrong. <br><br>  I myself have so far used as synonyms terms such as "deployment", "release", "delivery", etc., thinking little about their meaning.  A few months ago, all attempts to distinguish between these terms would be rejected by me as something unimportant. <br>  After thinking about it, I came to the idea that <i>there is a</i> real need to distinguish between the various stages of production. <br><br><h3>  Stage 1. Deployment </h3><br>  When testing (even in production) is a test for achieving the <i>best possible performance</i> , the accuracy of testing (and indeed any testing) is ensured only if the method of performing tests is as close as possible to the actual use of the service in production. <br><br>  In other words, tests must be performed in an environment that <i>best mimics the working environment</i> . <br><br>  And the <i>best imitation of the</i> work environment is ... the work environment itself.  To perform as many tests as possible in a production environment, it is necessary that the failure of any one of them does not affect the end user. <br><br>  This, in turn, is possible only if, <b>when a service is deployed in a work environment, users do not receive direct access to this service</b> . <br><br>  In this article, I decided to use the terminology from the article <a href="http://blog.turbinelabs.io/deploy-not-equal-release-part-one-4724bc1e726b">Deploy! = Release</a> (‚ÄúDeployment - not release‚Äù), written by <a href="http://www.turbinelabs.io/">Turbine Labs</a> .  In it, the term "deployment" gives the following definition: <br><br>  ‚ÄúDeployment is the installation by the working group of a new version of the service code in the production infrastructure.  When we say that a new version of software is <b>deployed</b> , we mean that it runs somewhere within the framework of the working infrastructure.  This may be a new EC2 instance in AWS or a Docker container running in a pod in the Kubernetes cluster.  The service started successfully, passed the performance check and is ready (you hope!) To process the production environment, but it may not receive any data in reality.  This is an important point, I will emphasize it again: <b>for deployment, it is not necessary that users get access to a new version of your service</b> .  Given this definition, deployment can be called a process with almost zero risk. ‚Äù <br><br>  The words ‚Äúzero-risk process‚Äù are simply a balm for the soul of many people who have suffered from unsuccessful deployments.  The ability to install software <i>in a real environment</i> without user access to it has several advantages when it comes to testing. <br>  First, the need to maintain separate environments for development, testing and staging, which inevitably has to be synchronized with production, is minimized (and may even disappear altogether). <br><br>  In addition, at the design stage of services, it becomes necessary to isolate them from each other in such a way that the unsuccessful testing of a specific service instance in production <i>does not</i> lead to cascading or affecting users of other services.  One of the solutions to ensure this can be the design of the data model and database schema, in which nonidempotent queries (mainly <i>write operations</i> ) can: <br><br><ul><li>  Run against the production environment database for any test service launch in production (I prefer this approach); </li><li>  Be safely rejected at the application level until they reach the write or save level; </li><li>  Be selected or isolated at the record or save level in some way (for example, by storing additional metadata). </li></ul><br><h3>  Stage 2. Release </h3><br>  <a href="http://blog.turbinelabs.io/deploy-not-equal-release-part-two-acbfe402a91c">Deploy! = Release</a> makes the term ‚Äúrelease‚Äù the following definition: <br><br>  ‚ÄúWhen we say that a <b>release of a</b> service version has taken place, we mean that it provides data processing in a production environment.  In other words, <b>release</b> is a process that directs production data to a new version of software.  Given this definition, all the risks that we associate with sending new data streams (interruptions, customer dissatisfaction, poisonous notes in <a href="http://www.theregister.co.uk/2017/02/28/aws_is_awol_as_s3_goes_haywire">The Register</a> ) relate to the <b>release of</b> new software, rather than its deployment (in some companies this stage is also called <b>release</b> . In this article we will use the term <b>release</b> ) ". <br><br>  In Google‚Äôs SRE book, the term ‚Äúrelease‚Äù is used in the <a href="http://landing.google.com/sre/book/chapters/release-engineering.html">chapter on the organization of software release to describe it</a> . <br><br>  ‚ÄúA <b>release is a logical piece of work consisting of one or several separate tasks.</b>  <b>Our goal is to align the deployment process with the risk profile of this service</b> . <br><br>  In development or pre-production environments, we can perform the build hourly and automatically send releases after passing all the tests.  For large user-oriented services, we can start the release from one cluster and then scale it up until we upgrade all the clusters.  <b>For important elements of the infrastructure, we can extend the implementation period by several days and perform it in turn in different geographic regions. ‚Äù</b> <br><br>  In this terminology, the words ‚Äúrelease‚Äù and ‚Äúrelease‚Äù refer to what is generally understood as ‚Äúdeployment‚Äù, and terms often used to describe various <i>deployment</i> strategies (for example, blue-green deployment or canary deployment) software. <br><br>  Moreover, an unsuccessful <i>release of</i> applications can cause partial or significant interruptions.  At this stage, a <i>rollback</i> or <i>hotfix</i> is also performed if it turns out that the <i>released</i> new version of the service is unstable. <br><br>  The <i>release</i> process works best when it is automated and <i>incremental</i> .  Similarly, a <i>rollback</i> or <i>hotfix</i> service <i>provides</i> more benefit when the frequency of occurrence of errors and the frequency of requests are automatically correlated with basic indicators. <br><br><h3>  Stage 3. After release </h3><br>  If the release <i>went smoothly</i> and the new version of the service processes the production environment data without obvious problems, we can consider <i>it</i> successful.  A successful release is followed by a stage that can be called ‚Äúpost-release‚Äù. <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-1" style="position: absolute; visibility: hidden; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="961811607759159296"></twitter-widget><blockquote class="twitter-tweet" data-lang="en_US" data-twitter-extracted-i155110693266837636="true"><p lang="en" dir="ltr">  i'm like thinking about this? <br><br>  it's traumatic.  til it wasn‚Äôt done.  <a href="https://t.co/FnUWou0qoN">https://t.co/FnUWou0qoN</a> </p>  - Charity Majors (@mipsytipsy) <a href="https://twitter.com/mipsytipsy/status/961811607759159296%3Fref_src%3Dtwsrc%255Etfw">February 9, 2018</a> </blockquote><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  Any rather complex system will <i>always</i> be in a state of gradual loss of productivity.  This does not mean that a <i>rollback</i> or <i>hotfix is</i> necessarily required.  Instead, it is necessary to monitor such deterioration (for various operating and work purposes) and, if necessary, perform debugging.  For this reason, testing after the release is more like not the usual procedures, but rather <i>debugging</i> or collecting analytical data. <br><br>  In general, I believe that every component of the system should be created taking into account the fact that not a single large system works 100% flawlessly and that malfunctions should be recognized and taken into account during the design, development, testing, deployment and monitoring stages of the software. security <br><br><hr><br>  Now that we have defined the three stages of production, let's look at the different testing mechanisms available in each of them.  Not everyone has the opportunity to work on new projects or rewrite code from scratch.  In this article, I tried to clearly identify the techniques that would best show themselves in the development of new projects, and also tell about what else we can do to take advantage of the proposed methods, without making significant changes to existing projects. <br><br><h2>  Testing in production at the deployment stage </h2><br>  We have separated the deployment and release stages from each other, and now we will consider some types of testing that can be applied after deploying the code in the production environment. <br><br><h3>  Integration testing </h3><br>  Typically, integration testing is performed by a continuous integration server in an isolated test environment for each branch of Git.  A copy of the <i>entire</i> service topology (including databases, queues, proxy servers, etc.) is deployed for test suites of <i>all</i> services that will work together. <br><br>  I believe that this is not particularly effective for several reasons.  First of all, the test environment, like staging, cannot be deployed so that it is <i>identical to the</i> real production environment, <i>even if the</i> tests are run in the same Docker container that will be used in production.  This is especially true when the <i>only thing</i> that runs in a test environment is the tests themselves. <br><br>  Regardless of whether the test runs as a Docker container or a POSIX process, it most likely makes <i>one</i> or more connections to an upstream service, database, or cache, which is rare if the service is in a production environment where it can process multiple concurrent connections, often reusing inactive TCP connections (this is called re-using HTTP connections). <br><br>  Also, the problems are caused by the fact that most of the tests each time they run creates a new database table or cache key space on <i>the same node</i> where this test is performed (thus, the tests are isolated from network failures).  At best, this type of testing can show that the system works correctly with a very specific request.  It is rarely effective in simulating serious, well-established types of failures, not to mention the various types of partial failures.  <a href="http//www.hpl.hp.com/techreports/2006/HPL-2006-2.pdf">Exhaustive</a> <a href="http://www.gribble.org/papers/robust.pdf">studies</a> exist that confirm that distributed systems often exhibit <i>unpredictable behavior</i> that cannot be foreseen using analysis performed differently than for the entire system. <br><br>  But this does not mean that integration testing <i>is basically</i> useless.  We can only say that performing integration tests in an <i>artificial, completely isolated environment</i> , as a rule, does not make sense.  Integration testing should still be performed to verify that the new version of the service: <br><br><ul><li>  Does not break interaction with higher or lower services; </li><li>  It does not adversely affect the goals and objectives of higher or lower services. </li></ul><br>  The first can be provided to some extent with the help of contract testing.  <i>Due</i> to only one ensuring the proper operation of <i>interfaces</i> between services, <a href="http://docs.pact.io/best-practices/consumer/contract-tests-vs-functional-tests">contract testing</a> is an effective method of developing and testing individual services at <i>the pre-production stage</i> , which does not require the deployment of the entire service topology. <br><br>  Client-oriented contract testing platforms, such as <a href="http://docs.pact.io/">Pact</a> , currently support interaction between services only through RESTful JSON RPC, although, most likely, <a href="http://twitter.com/matthewfellows/status/970215548293332992">work is also under way to support asynchronous interaction via web sockets, off-server applications and message queues</a> .  In the future, support for the gRPC and GraphQL protocols will probably be added, but now it is not yet available. <br><br>  However, before the <i>release of the</i> new version, it may be necessary to check not only the correct operation of the <i>interfaces</i> .  And, for example, make sure that the duration of an RPC call between two services is within the allowable limit when the interface between them changes.  It is also necessary to check that the cache hit ratio remains constant, for example, when adding an additional parameter to the incoming request. <br><br>  As it turned out, integration testing is not <i>optional</i> , its goal is to ensure that the change being tested does not lead to <i>serious, widespread</i> types of system failure (usually those for which alerts are assigned). <br>  In this regard, the question arises: how to <i>safely</i> conduct integration testing in production? <br><br>  To do this, consider the following example.  The figure below shows the architecture that I worked with a couple of years ago: our mobile and web clients connected to a web server (service C) based on MySQL (service D) with a client part in the form of a memcache cluster (service B). <br><br>  Despite the fact that this is a rather traditional architecture (and you will not call it microservice), the combination of stateful and stateless services makes this system a good example for my article. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xl/nc/4e/xlnc4eni3elyx3huaw3trxs5uyc.jpeg"></div><br>  Separating the <i>release</i> from <i>deployment</i> means that we can safely <i>deploy a</i> new instance of the service in a production environment. <br><br>  Modern service discovery utilities allow services with the same name to receive <i>tags</i> (or tags) with which you can distinguish the <i>released</i> and <i>deployed</i> version of the service with the same name.  Thanks to this feature, customers can only connect to the <i>released</i> version of the desired service. <br><br>  Suppose we are <i>deploying a</i> new version of service C in production. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ze/8y/yp/ze8yypuxevqszd_krd3cls4hrqc.jpeg"></div><br>  To verify that the <i>deployed</i> version is working correctly, we must be able to run it and make sure that none of the contracts is violated.  The main advantage of loosely coupled services is that they allow working groups to develop, deploy and scale independently.  In addition, it is possible to independently perform <i>testing</i> , which paradoxically applies to integration testing. <br><br>  Google‚Äôs blog has an article called ‚Äú <a href="http://testing.googleblog.com/2015/04/just-say-no-to-more-end-to-end-tests.html">Just Say No to More End-to-End Tests</a> ‚Äù, where integration tests are described as follows: <br><br>  ‚Äú <i>During the integration test, a small set of modules (usually two) is tested for consistency in their work.</i>  <i>If the two modules do not integrate properly, why write a pass-through test?</i>  <i>You can write a much smaller in volume and more narrowly integrated integration test, which can reveal the same errors</i> .  <b>Although in general it is necessary to think broader, there is no need to pursue the scale when it comes only to checking the joint work of the modules. ‚Äù</b> <br><br>  Further, it is said that integration testing in production should follow the same philosophy: it should be sufficient and obviously <i>useful</i> only for comprehensive testing of small groups of modules.  With proper design, all upstream dependencies should be sufficiently isolated from the service being tested so that a poorly formed request from service A would not lead to a cascade failure in the architecture. <br><br>  For our example, this means that testing of the <i>deployed</i> version of the C service and its interaction with MySQL should be performed, as shown in the figure below. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/c4/bu/qc/c4buqc8zz2dt4e3pyr6svjnyguc.jpeg"></div><br>  Testing <i>read operations</i> in most cases should be straightforward (unless the flow of data readable by the service being tested does not fill the cache with its subsequent ‚Äúpoisoning‚Äù with data used by the <i>released</i> services).  At the same time, testing of the interaction of <i>deployed</i> code with MySQL becomes more complex if nonidempotent queries are used, which can lead to changes in data. <br><br>  My choice is to perform integration testing using a production environment database.  Previously, I kept a white list of clients who were allowed to send requests to the service being tested.  Some workgroups support a special set of user accounts to perform tests in the production system, so that any accompanying data change is limited to a small, experienced series. <br><br>  But if it is absolutely necessary that the data of the production environment <i>does</i> not under any circumstances change during the execution of the test, then the write / change operations: <br><br><ul><li>  You must reject requests at the C application level or write to another table / collection in the database; </li><li>  It is necessary to register in the database as a new record marked as ‚Äúcreated‚Äù during the test. </li></ul><br>  If in the second case it is necessary to select <i>test write operations</i> at the database level, then to support this type of testing, the database schema should be designed in advance (for example, by adding an additional field). <br><br>  In the first case, the rejection of write operations at the application level can occur if the application is able to determine that the request should <i>not</i> be processed.  This is possible either by checking the IP address of the client sending the test request, or by the user ID contained in the incoming request, or by checking the request for a header that is expected to be specified by the client working in test mode. <br><br>  What I propose is similar to mock or stub, but at the level of service, and this is not too far from the truth.  This approach is accompanied by a fair number of problems.  <a href="http://research.fb.com/wp-content/uploads/2016/11/kraken_leveraging_live_traf_c_tests_to_identify_and_resolve_resource_utilization_bottlenecks_in_large_scale_web_services.pdf%3F">Kraken‚Äôs</a> Facebook brochure states the following: <br><br>  ‚Äú <i>An alternative design solution is to use the shadow data stream when an incoming request is recorded and replayed in a test environment.</i>  <i>In the case of a web server, most operations have side effects that extend deep into the system.</i>  <i>Shadow tests should not activate these side effects, as this may lead to changes for the user.</i>  <b>The use of stubs for side effects in shadow testing is not only impractical due to frequent changes in the logic of the server, but also reduces the accuracy of the test, since dependencies that would otherwise be affected are not loaded. ‚Äù</b> <br><br>  Although new projects can be designed so that side effects are minimized, prevented, or even completely eliminated, the use of stubs in a ready-made infrastructure can bring more problems than benefits. <br><br>  The mesh architecture of the service can to some extent help with this.  When using the service mesh architecture, services know nothing about the network topology and wait for connections on the local node.        -.        -,  ,  ,    : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/kv/cj/ukkvcjxlzn3hdcpt-mxsep3pojm.jpeg"></div><br>  If we test service B, its outgoing proxy server can be configured to add a special <code>X-ServiceB-Test</code> header to each test request.  In this case, the incoming proxy server of the upstream service C will be able to: <br><br><ul><li>  Detect this header and send a standard response to service B; </li><li>  Report service C that the request is a <i>test</i> . </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3s/mr/-n/3smr-n7k-_87j3jjpirb02hkaja.jpeg"></div><br>  <i>Integration testing of the interaction of the deployed version of service B with the released version of service C, where write operations never reach the database</i> <br><br>  Performing integration testing in this way also provides testing of the interaction of service B with higher-level services <i>when they process normal production-environment data</i> ‚Äî this is probably a closer simulation of how service B will behave when it is <i>released</i> in production. <br><br>  It would also be nice if each service in this architecture supported real API calls in test or layout mode, allowing you to test the execution of service contracts with downstream services without changing the actual data.  This would be equivalent to contract testing, but at the network level. <br><br><h3>  Shadow data duplication (dark data flow testing or mirroring) </h3><br>  Shadow duplication (in an article from a Google blog, it is called a <a href="http://cloudplatform.googleblog.com/2017/08/CRE-life-lessons-what-is-a-dark-launch-and-what-does-it-do-for-me.html">dark launch</a> , and <a href="http://istio.io/">Istio</a> uses the term <a href="http://istio.io/docs/tasks/traffic-management/mirroring/">mirroring</a> ) in many cases has more advantages than integration testing. <br><br>  The Principles of Chaotic Design ( <a href="http://">Principles of Chaos Engineering</a> ) states the following: <br><br>  ‚Äú <i>Systems behave differently depending on the environment and the data transfer scheme.</i>  <i>Since the usage mode can change at any time</i> , <b>sampling real data is the only reliable way to fix the query path. ‚Äù</b> <br><br>  Shadow data duplication is a method by which the data stream of the production environment that enters this service is captured and reproduced in the new <i>deployed</i> version of the service.  This process can be performed either in real time, when the incoming data stream is divided and sent to both the <i>released</i> and the <i>deployed</i> versions of the service, or asynchronously, when a copy of the previously captured data is reproduced in the <i>deployed</i> service. <br><br>  When I was working at <a href="http://www.imgix.com/">imgix</a> (a startup with a staff of 7 engineers, of which only four were system engineers), dark data streams were actively used to test changes in our image visualization infrastructure.  We recorded a certain percentage of all incoming requests and sent them to the Kafka cluster ‚Äî we transferred the HAProxy access logs to the <a href="http://hekad.readthedocs.io/en/v0.10.0/">heka</a> pipeline, which in turn passed the analyzed request flow to the Kafka cluster.  Before the <i>release</i> stage <i>, a</i> new version of our image processing application was tested on a captured dark data stream - this made sure that requests are being processed correctly.  However, our imaging system was, by and large, a stateless service that was particularly well suited for this type of testing. <br><br>  Some companies prefer not to capture part of the data stream, but to transfer a new version of the application to a <i>full copy of</i> this stream.  <a href="http://code.fb.com/core-data/introducing-mcrouter-a-memcached-protocol-router-for-scaling-memcached-deployments/">Facebook's McRouter router</a> (memcached proxy server) supports this kind of shadow duplication of the memcache data stream. <br><br>  ‚Äú <i>During testing of the new installation for the cache, we found it very convenient to be able to redirect a complete copy of the data stream from clients.</i>  <i>McRouter supports flexible shadow shadowing.</i>  <i>You can perform shadow duplication of a pool of various sizes (by re-caching the key space), copy only a part of the key space, or dynamically change the parameters in the process</i> . ‚Äù <br><br>  The negative aspect of the shadow duplication of the entire data stream for the <i>deployed</i> service in the production environment is that if it is running at the time of maximum data transfer intensity, then it may need twice as much power. <br>  Proxy servers such as Envoy support shadow duplication of data flow to another cluster in fire-and-forget mode.  His <a href="http://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/route/route.proto.html%3Fhighlight%3Dshadowing">documentation</a> says: <br><br>  ‚ÄúThe <i>router can perform shadow data duplication from one cluster to another.</i>  <i>Fire-and-forget mode is currently implemented, in which the Envoy proxy server does not wait for a response from the shadow cluster before returning a response from the main cluster.</i>  <i>For the shadow cluster, all the usual statistics are collected, which is useful for testing purposes.</i>  <i>With shadow duplication, the <code>-shadow</code> parameter is added to the host / authority <code>-shadow</code> .</i>  <i>This is useful for logging.</i>  <i>For example, <code>cluster1</code> turns into <code>cluster1-shadow</code></i> ". <br><br>  However, it is often impractical or impossible to create a cluster replica synchronized with production for testing (for the same reason that it is problematic to organize synchronized aging cluster).  If shadow duplication is used to test a new <i>deployed</i> service that has many dependencies, it can trigger unintended changes in the state of the upstream services in relation to the test.  Shadow duplication of the daily volume of user registrations in the <i>deployed</i> version of the service with a record in the production database can lead to an increase in the error rate of up to 100% due to the fact that the shadow data stream will be perceived as repeated registration attempts and be rejected. <br><br>  My personal experience suggests that shadow duplication is best suited for testing nonidempotent queries or stateless services with server-side stubs.  In this case, shadow data duplication is more commonly used to test load, resilience, and configurations.  In this case, using integration testing or styling, you can test how a service interacts with a stateful server when working with non-idempotent queries. <br><br><h3>  Tap comparison </h3><br>  The only mention of this term is in an <a href="http://blog.twitter.com/engineering/en_us/a/2016/superroot-launching-a-high-sla-production-service-at-twitter.html">article</a> from Twitter blog dedicated to the launch of services with a high level of service quality. <br><br>  <i>‚ÄúTo verify the correctness of the new implementation of the existing system, we used a method called <b>tap-comparison</b> .</i>  <i>Our tap-comparison tool reproduces a sample of production data in a new system and compares the received answers with the results of the old one.</i>  <i>The results obtained helped us find and correct errors in the system before the end users encountered them. ‚Äù</i> <br><br>  <a href="http://blog.twitter.com/engineering/en_us/a/2014/push-our-limits-reliability-testing-at-twitter.html">Another article</a> from the Twitter blog gives the definition of a tap comparison: <br><br>  <i>"Sending requests to service instances in both production and staging environments with <b>validation of results</b> and evaluation of performance characteristics."</i> <br><br>  The difference between tap-comparison and shadow duplication is that in the first case, the answer returned by the <i>released</i> version is compared with the answer returned by the <i>deployed</i> version, and in the second, the request is duplicated into the <i>deployed</i> version in the autonomous mode, like fire-and-forget. <br><br>  Another tool for working in this area is the <a href="http://github.com/github/scientist">scientist</a> library, available on GitHub.  This tool was developed to test Ruby code, but was then ported to <a href="http://github.com/github/scientist">several other languages</a> .  It is useful for some types of testing, but has a number of unsolved problems.  Here is what the developer wrote with GitHub in one professional Slack community: <br><br>  <i>‚ÄúThis tool simply performs two branches of code and compares the results.</i>  <i>Be careful with the code for these branches.</i>  <i>Care should be taken not to duplicate database queries if this leads to problems.</i>  <i>I think that this applies not only to the scientist, but also to any situation in which you do something twice, and then compare the results.</i>  <i>The scientist tool was created to verify that the new permission system works the same way as the old one, and at certain times was used to compare data that is characteristic of virtually every Rails request.</i>  <i>I think that the process will take more time, since the processing is performed sequentially, but this is a Ruby problem that does not use threads.</i> <i><br><br></i>  <i>In most cases known to me, the scientist tool was used to work with read operations rather than write, for example, to find out whether new improved requests and permission schemes receive the same answer as the old ones.</i>  <i>Both options are performed in a production environment (on replicas).</i>  <i>If the tested resources have side effects, I suppose the testing will have to be done at the application level. ‚Äù</i> <br><br>  <a href="http://github.com/twitter/diffy">Diffy</a> is an open source tool written in Scala that Twitter introduced in 2015.  <a href="http://blog.twitter.com/engineering/en_us/a/2015/diffy-testing-services-without-writing-tests.html">An article</a> from a Twitter blog called <b>Testing without Writing Tests</b> is probably the best resource for understanding how tap comparisons work in practice. <br><br>  <i>‚ÄúDiffy detects potential errors in the service, simultaneously launching a new and old version of the code.</i>  <i>This tool works as a proxy server and sends all received requests to each of the running instances.</i>  <i>It then compares the responses of the instances and reports all deviations detected during the comparison.</i>  <i>Diffy is based on the following idea:</i> <b>if two service implementations return the same answers with a sufficiently large and diverse set of requests, then these two implementations can be considered equivalent, and the newer one - without any impairments in performance.</b>  <i>Diffy‚Äôs innovative interference mitigation technique sets it apart from other comparative regression analysis tools. ‚Äù</i> <br><br>  Tap comparisons are great when you need to check if the two versions give the same results.  According to Mark McBride ( <a href="http://twitter.com/mccv">Mark McBride</a> ), <br><br>  <i>‚ÄúDiffy tool was often used when redesigning systems.</i>  <i>In our case, we divided the Rails source code base into several services created using Scala, and a large number of API clients did not use the functions as we expected.</i>  <i>Functions like date formatting were especially dangerous. ‚Äù</i> <br><br>  Tap-comparison is not the best option for testing user activity or identity of the behavior of two versions of the service at maximum load.  As with shadow duplication, side effects remain an unsolved problem, especially when both the deployed version and the production version write data to the same database.  As in the case of integration testing, one of the ways to get around this problem is to use tap comparisons with only a limited set of accounts. <br><br><h3>  Stress Testing </h3><br>  For those who are not familiar with load testing, <a href="http://www.digitalocean.com/community/tutorials/an-introduction-to-load-testing">this article</a> can serve as a good starting point.  There is no shortage of open source load testing tools and platforms.  The most popular of them are <a href="httpd.apache.org/docs/2.4/programs/ab.html">Apache Bench</a> , <a href="http://gatling.io/">Gatling</a> , <a href="http://github.com/giltene/wrk2">wrk2</a> , <a href="http://">Tsung</a> , written in Erlang, <a href="http://www.joedog.org/siege-home/">Siege</a> , <a href="http://github.com/twitter/iago">Iago</a> from Twitter, written in Scala (which reproduces the HTTP server, proxy server or network packet sniffer logs in a test instance).  Some experts believe that the best tool for generating load is <a href="http://github.com/satori-com/mzbench">mzbench</a> , which supports a variety of protocols, including MySQL, Postgres, Cassandra, MongoDB, TCP, etc. Netflix <a href="http://github.com/Netflix/ndbench">NDBench</a> is another open source tool for load testing data warehouses. which supports most of the known protocols. <br><br>  <a href="http://blog.twitter.com/engineering/en_us/a/2012/building-and-profiling-high-performance-systems-with-iago.html">Iago‚Äôs</a> official Twitter blog describes in more detail what characteristics a good load generator should have: <br><br>  <i>‚ÄúNon-blocking requests are generated with a specified frequency based on the internal custom statistical distribution ( <a href="http://en.wikipedia.org/wiki/Poisson_point_process">the Poisson process is</a> modeled by default).</i>  <i>The request rate can be changed as needed, for example, to prepare the cache before working at full load.</i> <i><br><br></i>  <i>In general, the focus is on the frequency of requests in accordance with <a href="http://en.wikipedia.org/wiki/Little%2527s_law">Little's law</a> , rather than the number of concurrent users, which can vary depending on the amount of delay inherent in this service.</i>  <i>Due to this, new opportunities appear to compare the results of several tests and prevent deterioration in the service, slowing down the load generator.</i> <i><br><br></i>  <i>In other words, the Iago tool seeks to simulate a system in which requests are received regardless of the ability of your service to process them.</i>  <i>This is different from load generators that simulate closed systems in which users will patiently work with the existing delay.</i>  <i>This difference allows us to quite accurately simulate the failure modes that can be encountered in production. ‚Äù</i> <br><br>  Another type of load testing is stress testing by redistributing the data stream.  Its essence is as follows: the entire data stream of the production environment is sent to a smaller cluster than the one prepared for the service;  if this causes problems, the data stream is transferred back to the larger cluster.  This technique is used by Facebook, as described in one of the <a href="http://code.fb.com/production-engineering/how-production-engineers-support-global-events-on-facebook/">articles of its official blog</a> : <br><br>  <i>‚ÄúWe specifically redirect a larger data flow to individual clusters or nodes, measure the resource consumption at these nodes and determine the limits of service sustainability.</i>  <i>This type of testing is particularly useful for determining the CPU resources needed to support the maximum number of simultaneous Facebook Live broadcasts. ‚Äù</i> <br><br>  Here is what the former LinkedIn engineer writes in the professional Slack community: <br><br>  <i>‚ÄúLinkedIn also used redline tests in production ‚Äî servers were removed from the load balancer until the load reached thresholds or errors began to occur.‚Äù</i> <br><br>  Indeed, Google search provides a link to a <a href="http://ieeexplore.ieee.org/document/8029816/">full technical document</a> and a LinkedIn blog <a href="http://engineering.linkedin.com/blog/2017/02/redliner--how-linkedin-determines-the-capacity-limits-of-its-ser">article</a> on this topic: <br><br>  <i>‚ÄúThe Redliner solution for measurements uses real data flow from the production environment, thus avoiding errors that prevent accurate measurement of performance under laboratory conditions.</i> <i><br><br></i>  <i>Redliner redirects part of the data stream to the service being tested and analyzes its performance in real time.</i>  <i>This solution was implemented in hundreds of LinkedIn internal services and is used daily for various types of performance analysis.</i> <i><br><br></i>  <i>Redliner supports parallel test execution for canary and working instances.</i>  <b>This allows engineers to transfer the same amount of data to two different instances of the service: 1) a service instance that contains innovations, such as new configurations, properties, or new code;</b>  <b>2) an instance of the service of the current working version.</b> <br><br>  <i>The results of load testing are taken into account when making decisions and prevent the code from being deployed, which can lead to poor performance. ‚Äù</i> <br><br>  Facebook has brought load testing using real-world data streams to a whole new level thanks to the Kraken system, and its <a href="http://research.fb.com/wp-content/uploads/2016/11/kraken_leveraging_live_traf_c_tests_to_identify_and_resolve_resource_utilization_bottlenecks_in_large_scale_web_services.pdf%3F">description is</a> also worth reading. <br>  Testing is implemented by redistributing the data flow when the weights change (read from the distributed configuration storage) for edge devices and clusters in the <a href="http://github.com/facebook/proxygen">Proxygen</a> configuration (Facebook load balancer).  These values ‚Äã‚Äãdetermine the volumes of real data sent respectively to each cluster and region at a given point of presence. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wk/0e/by/wk0ebyvdpznpgdgqcc_6zbu1g6g.png"></div><br>  <i>Data from the Kraken technical paper</i> <br><br>  The monitoring system ( <a href="http://vldb.org/pvldb/vol8/p1816-teller.pdf">Gorilla</a> ) displays the performance of various services (as shown in the table above).  Based on the monitoring data and threshold values, it is decided whether to further send data in accordance with the weights, or whether it is necessary to reduce or even completely stop the transfer of data to a specific cluster. <br><br><h2>  Configuration Tests </h2><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-2" style="position: absolute; visibility: hidden; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="916383043933192192"></twitter-widget><blockquote class="twitter-tweet" data-lang="en_US" data-twitter-extracted-i155110693266837636="true"><p lang="fr" dir="ltr">  Hypothesis: config changes are more dangerous than code changes. </p>  - Lorin Hochstein (@lhochstein) <a href="https://twitter.com/lhochstein/status/916383043933192192%3Fref_src%3Dtwsrc%255Etfw">October 6, 2017</a> </blockquote><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  The new wave of open source infrastructure tools has made fixing all changes to the infrastructure in the form of code not only possible, but relatively <i>easy</i> .  It has also become possible to <i>test</i> these changes to varying degrees, although most infrastructure-as-code tests at the pre-production stage can only confirm the correctness of the specifications and syntax. <br><br>  At the same time, the refusal to test the new configuration before the <i>release of the</i> code caused a <a href="http://github.com/danluu/post-mortems">significant number of interruptions in operation</a> . <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-3" style="position: absolute; visibility: hidden; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="963093541575581696"></twitter-widget><blockquote class="twitter-tweet" data-lang="en_US" data-twitter-extracted-i155110693266837636="true"><p lang="en" dir="ltr">  It is important to infrastructural testing changes ‚Äî especially infrastructural changes ‚Äî in production more. <br><br>  From Epic games postmortem on MongoDB connection pooling issues ... <a href="https://t.co/qRcBmbO421">pic.twitter.com/qRcBmbO421</a> </p>  - Cindy Sridharan (@copyconstruct) <a href="https://twitter.com/copyconstruct/status/963093541575581696%3Fref_src%3Dtwsrc%255Etfw">February 12, 2018</a> </blockquote><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>  For complete testing of configuration changes, it is important to distinguish between different types of configurations.  Fred Hebert once suggested using the following quadrant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/fi/4i/yvfi4ioub6pkvt1mtzxvz2xuvnk.png"></div><br>  This option, of course, is not universal, but this distinction makes it possible to decide how best to test each of the configurations and at what stage to do it.  The build time configuration makes sense if you can ensure real repeatability of the builds.  Not all configurations are static, and on modern platforms a dynamic configuration change is inevitable (even if we are dealing with a ‚Äúpermanent infrastructure‚Äù). <br><br><div class="oembed"><twitter-widget class="twitter-tweet twitter-tweet-rendered" id="twitter-widget-4" style="position: absolute; visibility: hidden; display: block; transform: rotate(0deg); max-width: 100%; width: 500px; min-width: 220px; margin-top: 10px; margin-bottom: 10px;" data-tweet-id="1005924617981005824"></twitter-widget><blockquote class="twitter-tweet" data-lang="en_US" data-twitter-extracted-i155110693266837636="true"><p lang="en" dir="ltr">  Quite a lot to unpack about this Google BigQuery outage from about a month ago.  It‚Äôs a good job to get it.  Not only is it a tooling lacking, we rarely even talk about this. <a href="https://t.co/2NgJOpI9gS">pic.twitter.com/2NgJOpI9gS</a> </p> ‚Äî Cindy Sridharan (@copyconstruct) <a href="https://twitter.com/copyconstruct/status/1005924617981005824%3Fref_src%3Dtwsrc%255Etfw">June 10, 2018</a> </blockquote><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></div><br>       ,   ,       blue-green ,        .   ( <a href="http://twitter.com/jaqx0r">Jamie Wilkinson</a> ),  Google  , <a href="http://twitter.com/jaqx0r/status/1006170578183585792"></a> : <br><br> <i>¬´        ,   ,   ,  -     .    .</i> <b>    -  ,         ‚Äî  ,       ,   .        .</b> <br><br> <i>            ,  .     ,    , ‚Äî    ¬ª.</i> <br><br>  <a href="http://queue.acm.org/detail.cfm%3Fid%3D2839461">  Facebook</a>             : <br><br> <i>¬´           .    ‚Äî  ,              .             .        ,      .</i> <br><br><ul><li> <b>   </b> <br><br>             .  Facebook ,          .             ,     . </li><li> <b>   </b> <br><br>         (,  JSON).           ,           .          . <br><br>   (,  Facebook  Thrift)      .   ,           . </li><li> <b> </b> <br><br>         ,     ,    - .       .   ‚Äî A/B-,        1 % .     A/B-,        .      A/B-    . ,  ,                 ,       ,     .  , A/B-    .      ,    A/B-.     Facebook        . <br><br> ,     A/B-  1% ,   1%     ,          (   ¬´  ¬ª).          ,         .               ,      . </li><li> <b>  </b> <br><br>   Facebook            .       ,        .    ,              ,     .   ,  ,           . </li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Simple and convenient cancellation of changes</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In some cases, despite all the preventive measures, an unworkable configuration is being deployed. </font><font style="vertical-align: inherit;">Quickly finding and reversing changes is critical to solving a similar problem. </font><font style="vertical-align: inherit;">In our configuration system, version control tools are available that make it much easier to undo changes. ‚Äù</font></font></li></ul><br>  To be continued! <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UPD: continued </font></font><a href="http://habr.com/company/funcorp/blog/418329/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div><p>Source: <a href="https://habr.com/ru/post/418081/">https://habr.com/ru/post/418081/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../418069/index.html">Solving color Japanese crosswords at the speed of light</a></li>
<li><a href="../418071/index.html">IT industry for the people: TechTrain Festival in St. Petersburg</a></li>
<li><a href="../418075/index.html">TOP 5 things that can be printed on a 3D printer [video]</a></li>
<li><a href="../418077/index.html">Accidents "do not watch the clock": a statistical justification for the mode of technical support 24/7</a></li>
<li><a href="../418079/index.html">The most popular programming languages ‚Äã‚Äã- 2018</a></li>
<li><a href="../418083/index.html">Simple server with GraphQL instead of REST, java implementation</a></li>
<li><a href="../418085/index.html">Use of promises in javascript</a></li>
<li><a href="../418087/index.html">80% of self-checkout counters are at risk</a></li>
<li><a href="../418089/index.html">Overview of SolidCraft CNC Milling Machines</a></li>
<li><a href="../418091/index.html">List of articles and literature about NAS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>