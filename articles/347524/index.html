<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Major advances in natural language processing in 2017</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. Immediately divide the audience into two parts - those who like to watch the video, and those who, like me, better perceive the texts. In order...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Major advances in natural language processing in 2017</h1><div class="post__text post__text-html js-mediator-article"><p>  Hello.  Immediately divide the audience into two parts - those who like to watch the video, and those who, like me, better perceive the texts.  In order not to torment the first ones, the recording of my speech on Date-Elk: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/1Chk1Mi-yZ0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  There are all the main points, but the presentation format does not imply a detailed review of the articles.  Fans of links and detailed analysis, welcome under cat. </p><a name="habracut"></a><br><p>  Those who have read this far can finally find out that everything written below <s>can be used against them in court</s> is solely my point of view, and other people's points of view can differ from mine. </p><br><h2 id="trendy">  Trends </h2><br><p>  In 2017, the year in the development of our field (natural language processing, NLP), I highlight two main trends: </p><br><ul><li>  <em>acceleration and parallelization</em> - all models strive to accelerate, including at the expense of greater parallelism; </li><li>  <em>learning without a teacher</em> - approaches with learning without a teacher have long been popular in computer vision, but are relatively rare in NLP (perhaps word22c is a rare but vivid example of the use of these ideas);  This year, the use of such approaches has become very popular. </li></ul><br><p>  Now we will examine in more detail the main ideas of this year. </p><br><h3 id="attention-is-all-you-need">  Attention Is All You Need </h3><br><p><img src="https://habrastorage.org/getpro/habr/post_images/307/faf/c7a/307fafc7a79358ef93bddbc110702f8f.png" alt="image"><br>  This already <a href="https://arxiv.org/abs/1706.03762">well-known work</a> marks the second coming of fully connected networks in the NLP domain.  Its authors are Google employees (by the way, one of the authors, Ilya Polosukhin, will speak at our hackathon <a href="http://babel.deephack.me/">DeepHack.Babel</a> ).  The idea behind the Transformer architecture (exactly it is shown in the picture) is as simple as all ingenious: let's forget about recurrence and all this and just use attention to achieve a result. </p><br><p>  But first, let's remember that all current advanced machine translation systems work on recurrent networks.  Intuitively recurrent neural networks should be well suited for natural language processing tasks, including machine translation, because they have an explicit memory used in the process of working.  This feature of the architecture has obvious advantages, but also its inseparable disadvantages: since we use memory to work with data, we can process them only in a specific sequence.  As a result, full data processing can take a lot of time (compared to, for example, CNN).  And this is exactly what the authors wanted to compete with. </p><br><p>  And now Transformer is an architecture for machine translation that has no recurrence.  And only the attention that does all the work. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/e90/16d/111/e9016d111c43e9ee18da8d07fd12de1e.png" alt="image"></p><br><p>  Let's first recall what the standard approach to attention offered by Dmitry Bogdanov (Dzmitry Bahdanau) looks like. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/3b5/2f9/daa/3b52f9daa03219def103d54357f9ed54.png" alt="image"></p><br><p>  The idea of ‚Äã‚Äãthe attentional mechanism is that we need to focus on some relevant input to the encoder in order to produce better decoding.  In the simplest case, relevance is defined as the similarity of each input to the current output.  This similarity is determined in turn as the sum of the inputs with weights, where the weights are summed to 1, and the largest weight corresponds to the most relevant input. </p><br><p>  The picture above shows the classical approach of authorship of Dmitry Bogdanov: we have one set of inputs - the hidden state of the encoder (h), as well as the set of coefficients for these inputs (a).  These coefficients are calculated each time, based on some other input that is different from the hidden states. </p><br><p>  In contrast to the classical approach, the authors of this work proposed the so-called <em>self-attention</em> on the input data.  The word "self" in this case means that attention is applied to the same data on which it is calculated.  At the same time, in the classical approach, attention is computed from some additional input relative to the data to which it is applied. </p><br><p>  Moreover, this self-attention is called <em>Multi-Head</em> , because  performs one operation several times in parallel.  This feature may remind convolutional filters, since  each of the "heads" looks at different places in the input sequence.  Another important feature is that in this version attention takes three entities as input, rather than two, as in the standard approach.  As you can see in the picture above, the "sub-attention" is first calculated on the Q (request) and K (key) inputs, and then the sub-attention output is combined with the V (value) from the input.  This feature refers us to the concept of memory, a variation of which is the mechanism of attention. </p><br><p>  In addition to the most important, there are two more significant features: </p><br><ul><li>  <em>positional encoding</em> , </li><li>  masked attention for the decoder ( <em>masked attention</em> ). </li></ul><br><p>  <em>Positional encoding</em> - as we remember, the entire architecture of the model is a fully connected network, so the very concept of a sequence inside the network is not embedded.  To add knowledge about the existence of sequences, positional encoding has been proposed.  As for me, the use of trigonometric functions (sin and cos), which creates positional encoding, seems to be a completely non-obvious choice, but it works: the position encoding vector combined with the word vector (for example, with the word2vec mentioned above) delivers knowledge of the meaning of the word and its relative position in the sequence in the network. </p><br><p>  <em>Masked attention</em> is a simple but important feature: again, because  there is no concept of sequences in the network, we need to somehow filter the network's representations of the following words that are not available during decoding.  So, as you can see in the picture, we insert a mask that "closes" words that the network should not yet see. </p><br><p>  All these features allow the network to not only work, but even improve the current results of machine translation. </p><br><h3 id="parallel-decoder-for-neural-machine-translation">  Parallel Decoder for Neural Machine Translation </h3><br><p>  The last of the described features of the architecture did not suit the authors of the <a href="https://einstein.ai/static/images/pages/research/non-autoregressive-neural-machine-translation/non-autoregressive-neural-mt.pdf">following work</a> , the staff of Richard Soher's group (Richard Socher) from Salesforce Research (by the way, one of the employees, Romain Paulus, the author of another well-known <a href="https://arxiv.org/abs/1705.04304">work on summatrization</a> , will also perform on our Hackathon <a href="http://babel.deephack.me/">DeepHack .Babel</a> ).  The masked attention for the decoder was not fast enough for them compared to a fast parallel encoder, so they decided to take the next step: "Why not make a parallel decoder if we already have a parallel encoder?"  This is my guess, but I am ready to guarantee that the authors of this work had some similar thoughts in their heads.  And they found a way to carry out their plans. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/9d8/637/9c2/9d86379c2f13fd87f316644ce76536ee.png" alt="image"></p><br><p> They called it Non-Autoregressive Decoding, the entire Non-Autoregressive Transformer architecture, which means that now no word depended on another when decoding.  This is some exaggeration, but not such a big one.  The idea is that the encoder here will additionally produce a so-called <em>fertility rate</em> for each input word.  This level of fertility is used to generate the translation itself for each word, based only on the word itself.  You can look at this as an analogue of the standard correspondence matrix in machine translation (alignment matrix): </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c01/0a4/591/c010a4591e5d35c4e1c9477afb1e0100.png" alt="image"></p><br><p>  As you can see, some of the words correspond to several words, and some do not correspond to any specific word of another language.  Thus, the fertility rate simply cuts this matrix into pieces, where each piece refers to a specific word of the source language. </p><br><p>  So, we have a fertility level, but this is not enough for fully parallel decoding.  You can notice in the picture a few additional levels of attention - positional attention (which correlates with positional encoding) and inter-attention (which replaced the masked attention from the original work). </p><br><p>  Unfortunately, giving a serious gain in speed (8 times in some cases), the Non-Autoregressive Decoder gives quality by several BLEU units worse than the original.  But this is a reason to look for ways to improve! </p><br><h3 id="unsupervised-machine-translation">  Unsupervised Machine Translation </h3><br><p>  The next part of the article is devoted to a task that seemed impossible a few years ago: machine translation, trained without a teacher.  Works that we will discuss: </p><br><ul><li>  <a href="https://arxiv.org/abs/1710.11041">Unsupervised Neural Machine Translation</a> </li><li>  <a href="https://arxiv.org/abs/1711.00043">Unsupervised Machine Translation Using Monolingual Corpora Only</a> </li><li>  <a href="https://arxiv.org/abs/1705.09655">Style-Transfer from Non-Parallel Text by Cross-Alignment</a> </li></ul><br><p>  The last work, judging by the name, is superfluous in this series, but as they say, the first impression is deceptive.  All three works have a common idea at the base.  In a nutshell, it can be stated as follows: we have two auto-encoders for two different text sources (for example, different languages, or texts of different styles), and we simply swap the decoding parts of these auto-encoders.  How it works?  Let's try to figure it out. <br><img src="https://habrastorage.org/getpro/habr/post_images/280/938/af8/280938af8e2fefbb5ccfec9466c45210.png" alt="image"></p><br><p>  The autocoder (on the left in the picture above) is the encoder decoder (encoder-decoder), where the decoder decodes back to the original space.  This means that the input and output belong to the same language (or style).  Thus, we have some text and train the encoder to make a vector representation of this text so that the decoder can reconstruct the original sentence.  Ideally, the reconstructed sentence would be exactly the same.  But in most cases this is not the case, and we need to somehow measure the similarity of the original and reconstructed sentences.  And for machine translation, such a measure was invented.  This is the standard metric now called BLEU. </p><br><ul><li>  <strong>BLEU</strong> - this metric measures how many words and n-grams (n consecutive words) overlap between a given translation and some reference, previously known translation.  The most commonly used version is BLUE, which is called BLUE-4, which works with words and phrases of length from 2 to 4. Additionally, a penalty for a too short translation (relative to the reference one) is introduced. </li></ul><br><p>  As you might have guessed, this metric is not differentiable, so we need some other way to train our translator.  For an autocoder, this may be standard cross-entropy, but this is not enough for translation.  For now, let‚Äôs skip it and continue. </p><br><p>  OK, now we have a way to build our autoencoder.  The next thing we have to do is train them a couple: one for the source language (style) and the other for the target language.  And we also need to cross them so that the decoder of the target language can ‚Äúrecover‚Äù the encoded strings of the source language, and vice versa, which in this case is all the same. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/69f/120/752/69f1207522088bdab05a3f873ade4792.png" alt="image"></p><br><p>  Now it will be the most difficult to understand: in the auto-encoder (or any other encoder-decoder) in the middle there is the so-called hidden representation - a vector from a certain space of high dimensionality.  If we want two auto-encoders to be compatible (in the sense that we need), we must ensure that the hidden representation is from the same space.  How to achieve this?  By adding an extra penalty for these autocoders.  This penalty is imposed by the discriminator, which refers us to the GAN concept. </p><br><ul><li>  <strong>GAN</strong> - Generative Adversarial Network.  The idea of ‚Äã‚ÄãGAN can be expressed as "the network plays with itself and tries to deceive itself."  The GAN architecture has three main components: a <em>generator</em> ‚Äî it produces performances that should be as real as possible, the <em>Golden Source</em> gives real performances, and a <em>discriminator</em> ‚Äî it must distinguish the input from which it came from, the generator or the Golden Source;  the generator is punished if the discriminator can guess it.  But the opposite is also true - the discriminator is punished if he cannot guess.  In this way they train together in competition among themselves. </li></ul><br><p>  In our case, the discriminator (L_adv in the picture) should say where the input came from ‚Äî from the source language or the target language.  The picture above shows two auto-encoders in the form of separate blocks - encoders and decoders.  In the middle there is a connection between them, where the discriminator is located.  By training two auto-encoders with such an additional penalty, we force the model to make hidden views for both auto-encoders similar (upper part of the picture), and then everything is clear - just replace the original decoder with its counterpart from another auto-encoder (lower part of the picture) and voila - our model can transfer! </p><br><p>  All three works mentioned in this section have this idea basically, of course, with their own peculiarities.  The explanation above is mostly based on the work of <a href="https://arxiv.org/abs/1711.00043">Unsupervised Machine Translation Using Monolingual Corpora Only</a> , so I should mention the previous work of these authors, especially since its results are used in the discussed work above: </p><br><ul><li>  <a href="https://arxiv.org/abs/1710.04087">Word Translation Without Parallel Data</a> </li></ul><br><p>  The idea of ‚Äã‚Äãthis work is as simple as all ingenious: <br><img src="https://habrastorage.org/getpro/habr/post_images/6c3/00e/ce2/6c300ece24a5b8f0976373da581b8033.png" alt="image"><br>  Let's say we have vector representations for the words of two different languages.  (Suppose we work with texts from one domain, for example, news or fiction.) We can reasonably assume that the dictionaries for these languages ‚Äã‚Äãwill be very close: for most of the words from the source case, we can find matches to the words of the target case. for example, words denoting concepts, president, ecology, and taxes will most likely be in newsrooms in both languages.  So why not just link these words together and pull one vector space onto another?  Actually, they did.  Found a function that converts vector spaces and imposes points of one (word) on the points of another.  In this paper, the authors showed that this can be done without a teacher, which means that they do not need a dictionary as such. </p><br><p>  The work of <a href="https://arxiv.org/abs/1705.09655">Style-Transfer from Non-Parallel Text by Cross-Alignment</a> is placed in this section, because  Languages ‚Äã‚Äãcan be considered as different styles of text, and the authors themselves mention this in their work.  Also this work is interesting because  <a href="https://github.com/shentianxiao/language-style-transfer">implementation</a> is available to it. </p><br><h3 id="controllable-text-generation">  Controllable Text Generation </h3><br><p>  This section is close in spirit to the previous one, but still quite significantly different.  Works that will be reviewed here: </p><br><ul><li>  <a href="https://arxiv.org/abs/1711.06861">Style Transfer in Text: Exploration and Evaluation</a> </li><li>  <a href="https://arxiv.org/abs/1703.00955">Toward Controlled Generation of Text</a> </li></ul><br><p>  The first work presents a different approach to transferring style to texts, which is closer to controlled generation, so this work is placed here, unlike the previous one.  The idea of ‚Äã‚Äãcontrolled generation can be illustrated with the following picture: <br><img src="https://habrastorage.org/getpro/habr/post_images/915/208/d6f/915208d6ff3555b1d53025b57dfa732e.png" alt="image"></p><br><p>  Here we again see the auto-encoder in the text, but it has a feature: the hidden view (which is responsible for the meaning here) is additionally enriched with special features.  These signs encode specific properties of the text, such as tonality or grammatical time. </p><br><p>  In the picture you can also notice the discriminator in addition to the auto-encoder.  There may be more than one discriminator if we want to encode more specific properties.  As a result, we have a complex loss function - a reconstruction loss from an auto-encoder and an additional penalty for specific text properties.  Thus, the reconstruction loss here is responsible only and exclusively for the meaning of the proposal, without other properties. </p><br><h3 id="simple-recurrent-unit">  Simple Recurrent Unit </h3><br><p>  Last but not the least important section.  It also focuses on computation speed.  Despite the fact that at the beginning of the article we discussed the shock of the basics in the form of the return of fully connected networks, nevertheless all modern systems in NLP work on recurrent networks.  And everyone knows that RNN is much slower than CNN.  Or not?  To answer this question, let's consider the following article: </p><br><ul><li>  <a href="https://arxiv.org/abs/1709.02755">Training RNNs as Fast as CNNs</a> </li></ul><br><p>  I think the authors of this paper tried to answer the question: why are the RNNs so slow?  What makes them so?  And they found the key to the solution: RNN - consistent in nature.  But what if you can leave only a small piece of this consistent nature, and do everything else in parallel?  Let's assume that (almost) everything does not depend on its previous state.  Then we can process the entire sequence of inputs in parallel.  So the task is to throw out all unnecessary dependencies on previous states.  And this is what led to it: <br><img src="https://habrastorage.org/getpro/habr/post_images/1ee/494/dec/1ee494dec9d5475bc63a1c97fedc7be4.png" alt="image"><br>  As you can see, only the last two equations depend on the previous state.  And in these two equations, we work with vectors, not matrices.  And all the heavy calculations can be done independently and in parallel.  And then we just do some multiplications to process the data sequentially.  This formulation showed excellent results, see for yourself: <br><img src="https://habrastorage.org/getpro/habr/post_images/c66/d78/bd9/c66d78bd9906b47adea6f32ced3a5af0.png" alt="image"></p><br><p>  The Simple Recurrent Unit (SRU) speed is almost the same as that of CNN! </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  In 2017, new strong players, such as Transformer, appeared in our region, and breakthroughs were made like working machine translation without a teacher, but the old people do not give up - SRU will still stand up for the honor of RNN in this fight.  So I look to the 2018th with the hope of new breakthroughs, which I still can not imagine. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/347524/">https://habr.com/ru/post/347524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347514/index.html">GR8 CUL8R - WTF?!?! Understand popular abbreviations in English</a></li>
<li><a href="../347516/index.html">Dynamic code analysis with Iroh.js</a></li>
<li><a href="../347518/index.html">Death of microservice madness in 2018</a></li>
<li><a href="../347520/index.html">The book "Programming for children. Learning to create websites, applications and games. HTML, CSS and JavaScript ¬ª</a></li>
<li><a href="../347522/index.html">Dependencies between SQL objects: use regular expressions and small algorithmic focus</a></li>
<li><a href="../347526/index.html">Telegram-bot for Redmine. How to simplify the life of yourself and people</a></li>
<li><a href="../347528/index.html">NLog extension for error monitoring</a></li>
<li><a href="../347530/index.html">Can a JavaScript construct (a == 1 && a == 2 && a == 3) be true?</a></li>
<li><a href="../347534/index.html">All the pain of p2p development</a></li>
<li><a href="../347536/index.html">Chromium: the sixth project check and 250 bugs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>