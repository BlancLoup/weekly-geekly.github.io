<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural networks in pictures: from one neuron to deep architectures</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Many materials on neural networks immediately begin with a demonstration of quite complex architectures. At the same time, the most basic things conce...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural networks in pictures: from one neuron to deep architectures</h1><div class="post__text post__text-html js-mediator-article">  Many materials on neural networks immediately begin with a demonstration of quite complex architectures.  At the same time, the most basic things concerning the functions of activations, initialization of weights, selection of the number of layers in the network, etc.  if considered, then passing.  It turns out that a beginner of the practice of neural networks has to take typical configurations and work with them virtually blindly. <br><br>  In the article we will go the other way.  Let's start with the simplest configuration - one neuron with one input and one output, without activation.  Next, in small iterations we will complicate the network configuration and try to squeeze a reasonable maximum out of each of them.  This will allow you to pull the network for the strings and to gain practical intuition in building architectures of neural networks, which in practice turns out to be a very valuable asset. <a name="habracut"></a><br><br><h2>  Illustrative material </h2><br>  Popular applications of neural networks, such as classification or regression, are a superstructure over the network itself, which includes two additional stages - preparation of input data (feature extraction, data conversion into vector) and interpretation of results.  For our purposes, these additional stages are redundant, since  we are not looking at the work of the network in its pure form, but at a certain construction, where the neural network is only an integral part. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's remember that the neural network is nothing but an approach to the approximation of the multidimensional function Rn -&gt; Rn.  Considering the limitations of human perception, in our article we will approximate the function on a plane.  Several non-standard use of neural networks, but it is great for the purpose of illustration of their work. <br><br><h2>  Framework </h2><br>  To demonstrate the configurations and results, I suggest taking the popular framework Keras, written in Python.  Although you can use any other tool for working with neural networks - most often the differences will be only in the names. <br><br><h2>  The easiest neural network </h2><br>  The simplest possible configuration of neural networks is one neuron with one input and one output without activation (or you can say with linear activation f (x) = x): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6e8/181/7ce/6e81817ce6bc4452ab1dd3ff9b74ec77.png" alt="image"></div><br>  <b>NB</b> As you can see, two values ‚Äã‚Äãare fed to the input of the network - x and one.  The latter is necessary in order to introduce an offset b.  In all popular frameworks, the input unit is already implicitly present and is not specified by the user separately.  Therefore, hereinafter we will assume that one value is supplied to the input. <br><br>  Despite its simplicity, this architecture already allows linear regression, i.e.  approximate the function with a straight line (often with minimization of the standard deviation).  The example is very important, so I propose to disassemble it in as much detail as possible. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#     -3  3 x = np.linspace(-3, 3, 1000).reshape(-1, 1) #   ,       def f(x): return 2 * x + 5 f = np.vectorize(f) #     y = f(x) #   ,  Keras from keras.models import Sequential from keras.layers import Dense def baseline_model(): model = Sequential() model.add(Dense(1, input_dim=1, activation='linear')) model.compile(loss='mean_squared_error', optimizer='sgd') return model #   model = baseline_model() model.fit(x, y, nb_epoch=100, verbose = 0) #        plt.scatter(x, y, color='black', antialiased=True) plt.plot(x, model.predict(x), color='magenta', linewidth=2, antialiased=True) plt.show() #     for layer in model.layers: weights = layer.get_weights() print(weights)</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/038/fb2/612/038fb261227840f8a0ee534a287896b9" alt="image"></div><br>  As you can see, our simplest network coped with the task of approximating a linear function with a linear function with a bang.  Now let's try to complicate the task by taking a more complex function: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> * np.sin(x) + <span class="hljs-number"><span class="hljs-number">5</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c04/ae6/839/c04ae6839aab4a09a4d6f7b3c6efc4fa" alt="image"></div><br>  Again, the result is quite decent.  Let's look at the weights of our model after training: <br><br><pre> <code class="hljs json">[array([[ <span class="hljs-number"><span class="hljs-number">0.69066334</span></span>]], dtype=float32), array([ <span class="hljs-number"><span class="hljs-number">4.99893045</span></span>], dtype=float32)]</code> </pre> <br>  The first number is the weight w, the second is the offset b.  To see this, let's draw the line f (x) = w * x + b: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">line</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> w = model.layers[<span class="hljs-number"><span class="hljs-number">0</span></span>].get_weights()[<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] b = model.layers[<span class="hljs-number"><span class="hljs-number">0</span></span>].get_weights()[<span class="hljs-number"><span class="hljs-number">1</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> w * x + b <span class="hljs-comment"><span class="hljs-comment">#        plt.scatter(x, y, color='black', antialiased=True) plt.plot(x, model.predict(x), color='magenta', linewidth=3, antialiased=True) plt.plot(x, line(x), color='yellow', linewidth=1, antialiased=True) plt.show()</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/9e3/850/651/9e3850651c2d4c02b468a9e4fba47bb4" alt="image"></div><br>  It all fits together. <br><br><h2>  Let's complicate the example </h2><br>  Well, as the line approaches, everything is clear.  But this and the classical linear regression did quite well.  How to capture the nonlinearity of the approximated function by the neural network? <br><br>  Let's try to throw in more neurons, let's say five pieces.  Since  one value is expected at the output, you will have to add one more layer to the network, which will simply summarize all the output values ‚Äã‚Äãfrom each of the five neurons: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/604/876/9ed/6048769ed0d2db503ff74e3d616050d8.png" alt="image"></div><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">baseline_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">5</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">5</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'sgd'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br>  Run: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/67c/9bd/5fd/67c9bd5fd63e48868db105eb9bb01504" alt="image"></div><br>  And ... nothing happened.  All the same straight line, although the matrix weights has grown a bit.  The fact is that the architecture of our network is reduced to a linear combination of linear functions: <br><br>  f (x) = w1 '* (w1 * x + b1) + ... + w5' (w5 * x + b5) + b <br><br>  Those.  again is a linear function.  To make the behavior of our network more interesting, we will add the function ReLU (rectifier, f (x) = max (0, x)) to the neurons of the inner layer, which allows the network to break the line into segments: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">baseline_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">5</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">5</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'sgd'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/d63/cd9/833/d63cd983319f4bbdbb42476a1c32e5c9" alt="image"></div><br>  The maximum number of segments coincides with the number of neurons on the inner layer.  By adding more neurons, you can get a more accurate approximation: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/656/ab4/27c/656ab427c1834524be955513422d0a33" alt="image"></div><br><h2>  Give more accuracy! </h2><br>  Already better, but flaws are visible to the eye - at the bends, where the original function is the least similar to a straight line, the approximation lags behind. <br><br>  As an optimization strategy, we took a rather popular method - SGD (stochastic gradient descent).  In practice, its improved version with inertia (SGDm, m - momentum) is often used.  This allows you to more smoothly turn on sharp bends and the approximation becomes better by eye: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   ,  Keras from keras.models import Sequential from keras.layers import Dense from keras.optimizers import SGD def baseline_model(): model = Sequential() model.add(Dense(100, input_dim=1, activation='relu')) model.add(Dense(1, input_dim=100, activation='linear')) sgd = SGD(lr=0.01, momentum=0.9, nesterov=True) model.compile(loss='mean_squared_error', optimizer=sgd) return model</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/69c/fde/b99/69cfdeb9998a4851baa06d6fb6eb0029" alt="image"></div><br><h2>  Complicate further </h2><br>  Sine is a pretty good feature for optimization.  Mainly because it does not have wide plateaus - i.e.  areas where the function changes very slowly.  In addition, the function itself varies fairly evenly.  To test our configuration for strength, take the function more complicated: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x * np.sin(x * <span class="hljs-number"><span class="hljs-number">2</span></span> * np.pi) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> x &lt; <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> -x * np.sin(x * np.pi) + np.exp(x / <span class="hljs-number"><span class="hljs-number">2</span></span>) - np.exp(<span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/00a/a44/dee/00aa44deef224599b33955a562519ced" alt="image"></div><br>  Alas, oh, here we already run into the ceiling of our architecture. <br><br><h2>  Give more nonlinearity! </h2><br>  Let's try replacing the ReLU (rectifier) ‚Äã‚Äãthat served us in previous examples with a more non-linear hyperbolic tangent: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">baseline_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">20</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">20</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=sgd) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model <span class="hljs-comment"><span class="hljs-comment">#   model = baseline_model() model.fit(x, y, nb_epoch=400, verbose = 0)</span></span></code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f9b/d18/133/f9bd181332d445e39a37f907c966b9c1" alt="image"></div><br><h2>  Scale initialization is important! </h2><br>  Approximation became better on the folds, but our network did not see part of the function.  Let's try to play with one more parameter - the initial distribution of weights.  We use the practical value of 'glorot_normal' (after the researcher Xavier Glorot, in some frameworks it is called XAVIER): <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">baseline_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">20</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, init=<span class="hljs-string"><span class="hljs-string">'glorot_normal'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">20</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>, init=<span class="hljs-string"><span class="hljs-string">'glorot_normal'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=sgd) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/db6/eb1/b06/db6eb1b06f8841c88450479484bb1cf7" alt="image"></div><br>  Already better.  But using 'he_normal' (after the name of the researcher Kaiming He) gives an even more pleasant result: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/297/c2e/b6c/297c2eb6cc55492dbb95fddd0ce30cb1" alt="image"></div><br><h2>  How it works? </h2><br>  Let's take a short pause and see how our current configuration works.  A network is a linear combination of hyperbolic tangents: <br><br>  f (x) = w1 '* tanh (w1 * x + b1) + ... + w5' * tanh (w5 * x + b5) + b <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#            def tanh(x, i): w0 = model.layers[0].get_weights() w1 = model.layers[1].get_weights() return w1[0][i][0] * np.tanh(w0[0][0][i] * x + w0[1][i]) + w1[1][0] #     plt.scatter(x, y, color='black', antialiased=True) plt.plot(x, model.predict(x), color='magenta', linewidth=2, antialiased=True) #   for i in range(0, 10, 1): plt.plot(x, tanh(x, i), color='blue', linewidth=1) plt.show()</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/3ff/adf/820/3ffadf8206fa4e4fbd44caadd4a763f5" alt="image"></div><br>  The illustration clearly shows that each hyperbolic tangent has captured a small area of ‚Äã‚Äãresponsibility and is working on approximating a function in its own small range.  Outside its area, the tangent falls to zero or one and simply gives an offset along the ordinate axis. <br><br><h2>  Abroad area of ‚Äã‚Äãstudy </h2><br>  Let's take a look at what is happening abroad in the network training area, in our case it is [-3, 3] <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9d6/857/8c4/9d68578c4ace415d94bb55dd5f7315cc" alt="image"></div><br>  As was clear from the previous examples, beyond the boundaries of the learning area, all hyperbolic tangents turn into constants (strictly speaking, values ‚Äã‚Äãclose to zero or one).  The neural network is not able to see outside the field of study: depending on the activators chosen, it will be very rude to estimate the value of the function being optimized.  It is worth remembering this when constructing signs and inputs given for a neural network. <br><br><h2>  Go into the depths </h2><br>  Until now, our configuration has not been an example of a deep neural network, since  there was only one inner layer.  Add one more: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">baseline_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">50</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, init=<span class="hljs-string"><span class="hljs-string">'he_normal'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">50</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">50</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, init=<span class="hljs-string"><span class="hljs-string">'he_normal'</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, input_dim=<span class="hljs-number"><span class="hljs-number">50</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'linear'</span></span>, init=<span class="hljs-string"><span class="hljs-string">'he_normal'</span></span>)) sgd = SGD(lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=sgd) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/cc0/f6d/b8d/cc0f6db8d3284ceaa9b9804c5321b589" alt="image"></div><br>  You can see for yourself that the network has better worked out the problem areas in the center and near the lower border along the x-axis: <br><br><div class="spoiler">  <b class="spoiler_title">An example of working with one inner layer</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://habrastorage.org/files/297/c2e/b6c/297c2eb6cc55492dbb95fddd0ce30cb1" alt="image"></div><br></div></div><br>  <b>NB</b> Blind adding layers does not automatically improve, which is called out of the box.  For most practical applications, the two inner layers are quite sufficient, and you will not have to deal with the special effects of too deep networks, such as the problem of a vanishing gradient.  If you do decide to go deep, be prepared to experiment a lot with network training. <br><br><h2>  The number of neurons on the inner layers </h2><br>  Just put a little experiment: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2d5/3bc/8bc/2d53bc8bc70a442fbcc3f10b1d03be4c" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e40/d5a/3b2/e40d5a3b26ca49ff8102929ff03d52f9" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/674/fc2/739/674fc273912f425399370a832285f075"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/29e/8bc/2d1/29e8bc2d118a424b98617d134eb4b502" alt="image"></div><br>  From a certain point on, the addition of neurons to the inner layers does not give a gain in optimization.  A good rule of thumb is to take the average between the number of inputs and outputs of the network. <br><br><h2>  Number of epochs </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/files/274/dd5/a64/274dd5a6472340c3a59acfea8085b783" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/136/f57/8db/136f578dbf3b4e4d8a32fade9101757c" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ee5/045/8b8/ee50458b886a442385d5aa265acb2cba" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9dc/646/9e6/9dc6469e650242009a68ad6a93c9f98a" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/81b/e66/005/81be66005a31440cbe4df9565dc3d029" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2b0/990/07c/2b099007c2864f769e9e274681a9a372" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/746/448/611/7464486117854c3db621d840518a0015" alt="image"></div><br><h2>  findings </h2><br>  Neural networks are a powerful, but non-trivial application tool.  The best way to learn how to build working neural network configurations is to start with simpler models and experiment a lot by gaining experience and intuition in the practice of neural networks.  And, of course, share the results of successful experiments with the community. </div><p>Source: <a href="https://habr.com/ru/post/322438/">https://habr.com/ru/post/322438/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../322426/index.html">How to bring a business online and do not go broke: digest of useful materials for beginners</a></li>
<li><a href="../322428/index.html">Functional security, part 5 of 7. The life cycle of information and functional security</a></li>
<li><a href="../322430/index.html">BBR system: congestion control directly by congestion</a></li>
<li><a href="../322432/index.html">Welcome to Lua in Moscow 2017 March 5</a></li>
<li><a href="../322434/index.html">IOS Localization Guide</a></li>
<li><a href="../322440/index.html">Full automation of the ‚Äúdevelopment‚Äù environment using docker-compose</a></li>
<li><a href="../322442/index.html">MIPSfpga and in-circuit debugging</a></li>
<li><a href="../322444/index.html">Tete-a-tete: ask the right questions</a></li>
<li><a href="../322446/index.html">Emoji.prototype.length - a story about emotional characters in Unicode</a></li>
<li><a href="../322448/index.html">About spaghetti, or how to explore the business processes of an organization</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>