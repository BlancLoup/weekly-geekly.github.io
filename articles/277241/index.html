<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pattern recognition. Beginning theory</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 In this article, I set out to highlight some of the fundamental results of the theory of machine learning in such a way that the concep...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pattern recognition. Beginning theory</h1><div class="post__text post__text-html js-mediator-article"><h2>  Introduction </h2><br>  In this article, I set out to highlight some of the fundamental results of the theory of machine learning in such a way that the concepts are understandable to readers who are somewhat familiar with the problems of classification and regression.  The idea to write such an article was more and more clearly manifested in my mind with each book read, in which the ideas of teaching machine recognition were told as if from the middle and it‚Äôs not at all clear what the authors of this or that method relied on in developing it.  On the other hand, there are a number of books devoted to basic concepts in machine learning, but the presentation of the material in them may seem too complicated for the first reading. <br><a name="habracut"></a><br><h2>  Motivation </h2><br>  Consider this task.  We have apples of two classes - tasty and not tasty, 1 and 0. Apples have signs - color and size.  The color will change continuously from 0 to 1, i.e.  0 - completely green apple, 1 - completely red.  The size can change in the same way, 0 - small apple, 1 - large.  We would like to develop an algorithm that would give color and size to the input, and at the output would give the apple class - whether it is tasty or not.  It is highly desirable that the number of errors in this case be the smaller the better.  At the same time, we have a final list, which contains historical data on the color, size and class of apples.  How could we solve this problem? <br><br><h2>  Logical approach </h2><br>  Solving our problem, the first method that may come to mind may be this: let's manually create rules like if-else and, depending on the values ‚Äã‚Äãof color and size, we will assign an apple to a certain class.  Those.  we have the prerequisites - it's color and size, and there is a consequence - the taste of an apple.  It is quite reasonable when there are few signs and it is possible to assess thresholds by comparison.  But it may happen that it is not possible to come up with clear conditions, and it is not obvious from the data which thresholds to take, and the number of signs may increase in perspective.  And what to do if in our list with historical data, we found two apples with the same color and size, but one is marked as tasty and the other is not?  Thus, our first method is not as flexible and scalable as we would like. <br><br><h2>  Legend </h2><br>  We introduce the following notation.  Will denote <img src="http://tex.s2cms.ru/svg/i" alt="i">  apple like <img src="http://tex.s2cms.ru/svg/x_i" alt="x_i">  .  In turn each <img src="http://tex.s2cms.ru/svg/x_i" alt="x_i">  consists of two numbers - color and size.  This fact we will denote by a pair of numbers: <img src="http://tex.s2cms.ru/svg/x_i%20%3D%20(color%2Csize)" alt="x_i = (color, size)">  .  Class each <img src="http://tex.s2cms.ru/svg/i" alt="i">  th apple we denote as <img src="http://tex.s2cms.ru/svg/y_i" alt="y_i">  .  List with historical data denoted by the letter <img src="http://tex.s2cms.ru/svg/S" alt="S">  , the length of this list is equal to <img src="http://tex.s2cms.ru/svg/N" alt="N">  . <img src="http://tex.s2cms.ru/svg/i" alt="i">  The -th element of this list is the value of the attributes of an apple and its class.  Those. <img src="http://tex.s2cms.ru/svg/S%20%3D%20%5C%7B(x_1%2Cy_1)%2C%20%5Cdots%20%2C(x_N%2Cy_N)%5C%7D%20%3D%20%5C%7B((color%2Csize)_1%2Cy_1)%2C%20%5Cdots%20%2C((color%2Csize)_N%2Cy_N)%5C%7D" alt="S = \ {(x_1, y_1), \ dots, (x_N, y_N) \} = \ {((color, size) _1, y_1), \ dots, ((color, size) _N, y_N) \}">  .  We will also call <img src="http://tex.s2cms.ru/svg/S" alt="S">  by sampling.  Capital letters <img src="http://tex.s2cms.ru/svg/X" alt="X">  and <img src="http://tex.s2cms.ru/svg/Y" alt="Y">  we denote variables that can take values ‚Äã‚Äãof a specific attribute and class.  We're going to introduce a new concept - the decisive rule <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  There is a function that accepts color and size <img src="http://tex.s2cms.ru/svg/x" alt="x">  , and on output returns a class label: <img src="http://tex.s2cms.ru/svg/a%3AX%5Crightarrow%20Y" alt="a: X \ rightarrow Y">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Probabilistic approach </h2><br>  Developing the idea of ‚Äã‚Äãa logical method with prerequisites and consequences, let us ask ourselves the question - what is the probability that <img src="http://tex.s2cms.ru/svg/m" alt="m">  apple that does not belong to our sample <img src="http://tex.s2cms.ru/svg/S" alt="S">  will be tasty, provided the measured values ‚Äã‚Äãof color and size?  In the theory of probability notation, this question can be written as: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)%20%3D%20%3F" alt="p (Y = 1 | X = x_m) =?"></div><br><br>  In this expression can be interpreted <img src="http://tex.s2cms.ru/svg/X" alt="X">  like the parcel <img src="http://tex.s2cms.ru/svg/Y" alt="Y">  as a result, but the transition from the premise to the effect will be subject to probabilistic laws, and not logical.  Those.  instead of a truth table with Boolean values ‚Äã‚Äã0 and 1 for a class, there will be probability values ‚Äã‚Äãthat take values ‚Äã‚Äãfrom 0 to 1. Apply the Bayes formula and get the following expression: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)%20%3D%20%5Cfrac%7Bp(X%3Dx_m%20%7C%20Y%3D1)p(Y%3D1)%7D%7Bp(X%3Dx_m)%7D" alt="p (Y = 1 | X = x_m) = \ frac {p (X = x_m | Y = 1) p (Y = 1)} {p (X = x_m)}"></div><br><br>  Consider the right side of this expression in more detail.  Factor <img src="http://tex.s2cms.ru/svg/p(Y%3D1)" alt="p (Y = 1)">  is called a priori probability and means the probability of meeting a tasty apple among all possible apples.  A priori chance of meeting a tasteless apple <img src="http://tex.s2cms.ru/svg/p(Y%3D0)%20%3D%201%20-p(Y%3D1)" alt="p (Y = 0) = 1 -p (Y = 1)">  .  This probability may reflect our personal knowledge of how delicious and tasteless apples are distributed in nature.  For example, in our past experience, we know that 80% of all apples are tasty.  Or we can estimate this value simply by counting the share of delicious apples in our list with historical data S. The next factor is <img src="http://tex.s2cms.ru/svg/p(X%3Dx_m%20%7C%20Y%3D1)" alt="p (X = x_m | Y = 1)">  shows how likely it is to get a specific color and size value <img src="http://tex.s2cms.ru/svg/x_m" alt="x_m">  for an apple of class 1. This expression is also called a likelihood function and may have the form of some specific distribution, for example, normal.  Denominator <img src="http://tex.s2cms.ru/svg/p(X%3Dx_m)" alt="p (X = x_m)">  we use as a normalization constant, what would be the desired probability <img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)" alt="p (Y = 1 | X = x_m)">  varied from 0 to 1. Our ultimate goal is not a search for probabilities, but a search for a decision rule that would immediately give us a class.  The final form of the decision rule depends on what values ‚Äã‚Äãand parameters are known to us.  For example, we can know only the values ‚Äã‚Äãof a priori probability, and the remaining values ‚Äã‚Äãcannot be estimated.  Then the decisive rule will be this - to put all the apples of the value of the class for which the a priori probability is greatest.  Those.  if we know that 80% of apples in nature are tasty, then we put a class 1 for each apple. Then our error will be 20%.  If, moreover, we can estimate the values ‚Äã‚Äãof the likelihood function $ p (X = x_m | Y = 1) $, then we can find the value of the desired probability <img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)" alt="p (Y = 1 | X = x_m)">  according to the Bayesian formula, as written above.  The decisive rule here would be: put a label of the class for which the probability <img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)" alt="p (Y = 1 | X = x_m)">  maximum: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/a(x_m)%20%3D%201%2C%20%D0%B5%D1%81%D0%BB%D0%B8%20%5C%20p(Y%3D1%7CX%3Dx_m)%20%3E%20p(Y%3D0%7CX%3Dx_m)%2C" alt="a (x_m) = 1 if \ p (Y = 1 | X = x_m) &amp; gt; p (Y = 0 | X = x_m),"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/a(x_m)%20%3D%200%2C%20%D0%B5%D1%81%D0%BB%D0%B8%20%5C%20p(Y%3D1%7CX%3Dx_m)%20%3C%20p(Y%3D0%7CX%3Dx_m)" alt="a (x_m) = 0 if \ p (Y = 1 | X = x_m) &amp; lt; p (Y = 0 | X = x_m)"></div><br><br>  This rule is called the Bayesian classifier.  Since we are dealing with probabilities, even the large probability <img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)" alt="p (Y = 1 | X = x_m)">  does not guarantee that the apple <img src="http://tex.s2cms.ru/svg/x_m" alt="x_m">  does not belong to class 0. We estimate the probability of an error on an apple <img src="http://tex.s2cms.ru/svg/x_m" alt="x_m">  as follows: if the decision rule returns a class value equal to 1, then the probability of a mistake will be <img src="http://tex.s2cms.ru/svg/p(Y%3D0%7CX%3Dx_m)" alt="p (Y = 0 | X = x_m)">  and vice versa: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p(error%7Cx_m)%20%3D%20p(Y%3D0%7CX%3Dx_m)%2C%20%D0%B5%D1%81%D0%BB%D0%B8%20%5C%20a(x_m)%20%3D%201%2C" alt="p (error | x_m) = p (Y = 0 | X = x_m), if \ a (x_m) = 1,"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p(error%7Cx_m)%20%3D%20p(Y%3D1%7CX%3Dx_m)%2C%20%D0%B5%D1%81%D0%BB%D0%B8%20%5C%20a(x_m)%20%3D%200%20" alt="p (error | x_m) = p (Y = 1 | X = x_m), if \ a (x_m) = 0"></div><br><br>  We are interested in the probability of a classifier error not only in this particular example, but in general for all possible apples: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/p(error)%20%3D%20%5Cint%7Bp(error%7Cx)p(x)dx%7D" alt="p (error) = \ int {p (error | x) p (x) dx}"></div><br><br>  This expression is mathematical expect error <img src="http://tex.s2cms.ru/svg/p(error%7Cx)" alt="p (error | x)">  .  So, solving the original problem, we came to the Bayes classifier, but what are its disadvantages?  The main problem is to estimate the conditional probability from the data. <img src="http://tex.s2cms.ru/svg/p(X%3Dx_m%20%7C%20Y%3D1)" alt="p (X = x_m | Y = 1)">  .  In our case, we represent the object as a pair of numbers ‚Äî color and size, but in more complex tasks, the dimension of features may be several times higher and the number of observations from our list with historical data may not be enough to estimate the probability of a multidimensional random variable.  Next, we will try to generalize our notion of a classifier error, as well as see if it is possible to choose any other classifier to solve the problem. <br><br><h2>  Losses from classifier errors </h2><br>  Suppose we already have some decisive rule <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  .  Then it can make two types of errors - the first is to classify an object to class 0, which has real class 1 and vice versa, classify an object to class 1, which has real class 0. In some problems it is important to distinguish these cases.  For example, we suffer more from the fact that the apple, marked as tasty, turned out to be tasteless and vice versa.  The degree of our discomfort from deceived expectations, we formalize the concept <img src="http://tex.s2cms.ru/svg/%5Cit%20%D0%BF%D0%BE%D1%82%D0%B5%D1%80%D1%8F." alt="\ it is a loss.">  More generally, we have a loss function that returns a number for each classifier error.  Let be <img src="http://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  - real class label.  Then the loss function <img src="http://tex.s2cms.ru/svg/%5Clambda(%5Calpha_i%7Ca(x))" alt="\ lambda (\ alpha_i | a (x))">  returns the loss value for a real class label <img src="http://tex.s2cms.ru/svg/%5Calpha_i" alt="\ alpha_i">  and the values ‚Äã‚Äãof our decision rule <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  .  An example of the use of this function - we take from the apple with a known class <img src="http://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  passing an apple to our decisive rule <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  , we obtain the class estimate from the decision rule, if the values <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  and <img src="http://tex.s2cms.ru/svg/%5Calpha" alt="\ alpha">  coincided, we believe that the classifier was not mistaken and there are no losses, if the values ‚Äã‚Äãdo not match, then our function will say the magnitude of the losses <img src="http://tex.s2cms.ru/svg/%5Clambda(%5Calpha_i%7Ca(x))." alt="\ lambda (\ alpha_i | a (x))."><br><br><h2>  Conditional and Bayesian Risk </h2><br>  Now that we have a loss function and we know how much we lose from improperly classifying an object <img src="http://tex.s2cms.ru/svg/x" alt="x">  , it would be nice to understand how much we lose on average, at many sites.  If we know the value <img src="http://tex.s2cms.ru/svg/p(Y%3D1%7CX%3Dx_m)" alt="p (Y = 1 | X = x_m)">  - the probability that <img src="http://tex.s2cms.ru/svg/m" alt="m">  if the apple is tasty, provided that the measured values ‚Äã‚Äãof color and size, as well as the real value of the class (for example, take an apple from sample S, see the beginning of the article), we can introduce the concept of conditional risk.  Conditional risk is the average loss at the facility <img src="http://tex.s2cms.ru/svg/x" alt="x">  for decision rule <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%7Cx)%20%3D%20%5Clambda(%5Calpha%3D0%7Ca(x))p(Y%3D0%7CX%3Dx)%20%2B%20%5Clambda(%5Calpha%20%3D%201%7Ca(x))p(Y%3D1%7CX%3Dx)" alt="R (a (x) | x) = \ lambda (\ alpha = 0 | a (x)) p (Y = 0 | X = x) + \ lambda (\ alpha = 1 | a (x)) p (Y = 1 | X = x)"></div><br><br>  In our case of binary classification when <img src="http://tex.s2cms.ru/svg/a(x)%20%3D%201" alt="a (x) = 1">  it turns out: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%201%7Cx)%20%3D%20%5Clambda(%5Calpha%3D0%7Ca(x)%3D1)p(Y%3D0%7CX%3Dx)%20%2B%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%3D1)p(Y%3D1%7CX%3Dx)" alt="R (a (x) = 1 | x) = \ lambda (\ alpha = 0 | a (x) = 1) p (Y = 0 | X = x) + \ lambda (\ alpha = 1 | a (x) = 1) p (Y = 1 | X = x)"></div><br><br>  When a (x) = 0 <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%200%7Cx)%20%3D%20%5Clambda(%5Calpha%3D0%7Ca(x)%3D0)p(Y%3D0%7CX%3Dx)%20%2B%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%3D0)p(Y%3D1%7CX%3Dx)" alt="R (a (x) = 0 | x) = \ lambda (\ alpha = 0 | a (x) = 0) p (Y = 0 | X = x) + \ lambda (\ alpha = 1 | a (x) = 0) p (Y = 1 | X = x)"></div><br><br>  To calculate the average losses on all possible objects, the concept of Bayesian risk is introduced, which is the mathematical expectation of conditional risk: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x))%20%3D%20%5Cint%7BR(a(x)%7Cx)p(x)dx%7D" alt="R (a (x)) = \ int {R (a (x) | x) p (x) dx}"></div><br><br>  Above, we described the decision rule that relates an object to the class that has the highest probability value. <img src="http://tex.s2cms.ru/svg/p(Y%7CX)." alt="p (Y | X).">  Such a rule delivers a minimum to our average losses (Bayesian risk), so the Bayesian classifier is optimal from the point of view of the risk functional introduced by us.  This means that the Bayesian classifier has the smallest possible classification error. <br><br><h2>  Some typical loss functions </h2><br>  One of the most frequently occurring loss functions is a symmetric function, when the losses from the first and second types of errors are equivalent.  For example, the loss function 1-0 (zero-one loss) is defined as: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%20%3D%200)%20%3D%201%20" alt="\ lambda (\ alpha = 1 | a (x) = 0) = 1"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%200%7Ca(x)%20%3D%201)%20%3D%201%20" alt="\ lambda (\ alpha = 0 | a (x) = 1) = 1"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%20%3D%201)%20%3D%200%20" alt="\ lambda (\ alpha = 1 | a (x) = 1) = 0"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%200%7Ca(x)%20%3D%200)%20%3D%200%20" alt="\ lambda (\ alpha = 0 | a (x) = 0) = 0"></div><br><br>  Then the conditional risk for a (x) = 1 is simply the probability value to get class 0 on the object. <img src="http://tex.s2cms.ru/svg/x" alt="x">  : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%201%7Cx)%20%3D%201*p(Y%3D0%7CX%3Dx)%20%2B%200*p(Y%3D1%7CX%3Dx)%20%3D%20p(Y%3D0%7CX%3Dx)" alt="R (a (x) = 1 | x) = 1 * p (Y = 0 | X = x) + 0 * p (Y = 1 | X = x) = p (Y = 0 | X = x)"></div><br><br>  Similarly for a (x) = 0: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%200%7Cx)%20%3D%20p(Y%3D1%7CX%3Dx)" alt="R (a (x) = 0 | x) = p (Y = 1 | X = x)"></div><br><br>  The loss function 1‚Äì0 takes the value 1 if the classifier makes an error on the object and 0 if it does not.  Now we will do so that the value on the error is not equal to 1, but to another function Q, depending on the decision rule and the real class label: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%200%7Ca(x)%20%3D%201)%20%3D%20Q(%5Calpha%20%3D%200%20%2C%20a(x)%20%3D%201)" alt="\ lambda (\ alpha = 0 | a (x) = 1) = Q (\ alpha = 0, a (x) = 1)"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%20%3D%200)%20%3D%20Q(%5Calpha%20%3D%201%2C%20a(x)%20%3D%200)" alt="\ lambda (\ alpha = 1 | a (x) = 0) = Q (\ alpha = 1, a (x) = 0)"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%201%7Ca(x)%20%3D%201)%20%3D%200%20" alt="\ lambda (\ alpha = 1 | a (x) = 1) = 0"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20%5Clambda(%5Calpha%20%3D%200%7Ca(x)%20%3D%200)%20%3D%200%20" alt="\ lambda (\ alpha = 0 | a (x) = 0) = 0"></div><br><br>  Then the conditional risk can be written as: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%200%7Cx)%20%3D%20%20Q(%5Calpha%20%3D%201%2C%20a(x)%20%3D%200)%20p(Y%3D1%7CX%3Dx)" alt="R (a (x) = 0 | x) = Q (\ alpha = 1, a (x) = 0) p (Y = 1 | X = x)"></div><br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R(a(x)%20%3D%201%7Cx)%20%3D%20%20Q(%5Calpha%20%3D%200%2C%20a(x)%20%3D%201)%20p(Y%3D0%7CX%3Dx)" alt="R (a (x) = 1 | x) = Q (\ alpha = 0, a (x) = 1) p (Y = 0 | X = x)"></div><br><br><h2>  Notation notes </h2><br>  The previous text was written according to the notation adopted in the book of Duda and Hart.  In the original book of V.N.  Vapnik considered such a process: nature selects an object according to the distribution of $ p (x) $, and then gives it a class label according to the conditional distribution of $ p (y | x) $.  Then the risk (expectation of loss) is defined as <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R%20%3D%20%5Cint%7BL(y%2Ca(x))p(x%2Cy)dxdy%7D" alt="R = \ int {L (y, a (x)) p (x, y) dxdy}"></div><br><br>  Where <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  - the function we are trying to approximate an unknown dependency, <img src="http://tex.s2cms.ru/svg/L(y%2Ca(x))" alt="L (y, a (x))">  - loss function for real value <img src="http://tex.s2cms.ru/svg/y" alt="y">  and the values ‚Äã‚Äãof our function <img src="http://tex.s2cms.ru/svg/a(x)" alt="a (x)">  .  This notation is more obvious in order to introduce the following concept - an empirical risk. <br><br><h2>  Empirical risk </h2><br>  At this stage, we have already found out that the logical method does not suit us, because it is not flexible enough, and we cannot use the Bayesian classifier when there are a lot of signs, and there are a limited number of data for training and we cannot recover the probability <img src="http://tex.s2cms.ru/svg/p(X%7CY)" alt="p (X | Y)">  .  We also know that the Bayes classifier has the smallest possible classification error.  Since we cannot use the Bayes classifier, let's take something simpler.  Let's fix some parametric family of functions H and we will select a classifier from this family. <br><br>  Example: Let <img src="http://tex.s2cms.ru/svg/H" alt="H">  set of all functions of the form <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/h(x)%20%3D%20%5Csum_%7Bi%7D%5E%7Bn%7D%7Bw_ix_i%7D" alt="h (x) = \ sum_ {i} ^ {n} {w_ix_i}"></div><br><br>  All functions of this set will differ from each other only by coefficients. <img src="http://tex.s2cms.ru/svg/w" alt="w">  When we chose this family, we assumed that in the color-size coordinates between points of class 1 and points of class 0, we can draw a straight line with coefficients <img src="http://tex.s2cms.ru/svg/w" alt="w">  in such a way that points with different classes are on opposite sides of a line.  It is known that a straight line of this type has a coefficient vector <img src="http://tex.s2cms.ru/svg/w" alt="w">  is normal to straight.  Now we do this - we take our apple, measure its color and size from it and put a point with the obtained coordinates on the graph in color-size axes.  Next, measure the angle between this point and the vector $ w $.  We note that our point can lie either on one or on the other side of the line.  Then the angle between <img src="http://tex.s2cms.ru/svg/w" alt="w">  and the point will be either sharp or dull, and the scalar product will be either positive or negative.  Hence the decisive rule: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/a(x)%20%3D%20sign(%5Csum_%7Bi%7D%5E%7Bn%7D%7Bw_ix_i%7D)" alt="a (x) = sign (\ sum_ {i} ^ {n} {w_ix_i})"></div><br><br>  After we fixed the class of functions $ H $, the question arises - how to choose from it a function with the necessary coefficients?  Answer - let's choose the function that delivers the minimum to our Bayesian risk $ R () $.  Again, the problem is that in order to calculate the Bayesian risk values, you need to know the distribution of $ p (x, y) $, but it is not given to us, and restoring it is not always possible.  Another idea is to minimize the risk not at all possible sites, but only at a sample.  Those.  minimize function: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R_%7Bemp%7D%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%7D%5E%7BN%7D%7BL(y_i%2C%20a(x_i))%7D" alt="R_ {emp} = \ frac {1} {N} \ sum_ {i} ^ {N} {L (y_i, a (x_i))}"></div><br><br>  This feature is called empirical risk.  The next question is why we decided that minimizing the empirical risk, we also minimize the Bayesian risk?  Let me remind you that our task is practical - to allow as few classification errors as possible.  The fewer errors, the less Bayesian risk.  The justification for the convergence of empirical risk to Bayesian with increasing data volume was obtained in the 70s by two scientists - V.N. Vapnik and A. Ya. Chervonenkis. <br><br><h2>  Guarantees of convergence.  The simplest case </h2><br>  So, we have come to the conclusion that the Bayes classifier gives the smallest possible error, but in most cases we cannot train it and we cannot calculate the error (risk) either.  However, we can calculate the approximation to Bayesian risk, which is called empirical risk, and knowing the empirical risk to choose such an approximating function that would minimize the empirical risk.  Let's consider the simplest situation when minimizing empirical risk is provided by the classifier, which also minimizes Bayesian risk.  For the simplest case, we will have to make an assumption, which is rarely carried out in practice, but which can be further weakened.  Fix a finite class of functions. <img src="http://tex.s2cms.ru/svg/H%20%3D%20%5C%7Bh%3AX%5Crightarrow%20Y%5C%7D" alt="H = \ {h: X \ rightarrow Y \}">  from which we will choose our classifier and suppose that the real function that nature uses to mark our apples to taste is in this finite set of hypotheses: <img src="http://tex.s2cms.ru/svg/f%5Cin%20H" alt="f \ in H">  .  We also have a sample. <img src="http://tex.s2cms.ru/svg/S" alt="S">  derived from the distribution <img src="http://tex.s2cms.ru/svg/D" alt="D">  above objects <img src="http://tex.s2cms.ru/svg/x" alt="x">  .  All objects in the sample are assumed to be equally independently distributed (iid).  Then the following will be true <br><br><h3>  Theorem </h3><br>  Choosing a function from class <img src="http://tex.s2cms.ru/svg/H" alt="H">  by minimizing empirical risk, we are guaranteed to find such <img src="http://tex.s2cms.ru/svg/h%5Cin%20H" alt="h \ in H">  that it has a small Bayes risk if the sample on which we perform the minimization is of sufficient size. <br><br>  For the meaning of ‚Äúsmall value‚Äù and ‚Äúsufficient size‚Äù, see the literature below. <br><br><h3>  Idea proof </h3><br>  By the condition of the theorem we get the sample <img src="http://tex.s2cms.ru/svg/S" alt="S">  from distribution <img src="http://tex.s2cms.ru/svg/D" alt="D">  i.e.  the process of selecting objects from nature is random.  Every time we collect a sample, it will be from the same distribution, but the objects themselves can be different in it.  The main idea of ‚Äã‚Äãthe proof is that we can get such an unsuccessful sample. <img src="http://tex.s2cms.ru/svg/S" alt="S">  that the algorithm we choose by minimizing the empirical risk on a given sample would be poor to minimize the Bayesian risk but it would be good to minimize the empirical risk, but the probability of obtaining such a sample is small and as the sample size increases, this probability decreases.  Similar theorems exist for more realistic assumptions, but here we will not consider them. <br><br><h2>  Practical results </h2><br>  Having evidence that the function found while minimizing the empirical risk will not have a big error on previously unobservable data with sufficient training sample size, we can use this principle in practice, for example, as follows - take the expression: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/R_%7Bemp%7D%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%7D%5E%7BN%7D%7BL(y%2C%20a(x))%7D" alt="R_ {emp} = \ frac {1} {N} \ sum_ {i} ^ {N} {L (y, a (x))}"></div><br><br>  And we substitute different loss functions, depending on the problem being solved.  For linear regression: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/L(y%2C%20a(x))%20%3D%20(y-a(x))%5E2" alt="L (y, a (x)) = (y-a (x)) ^ 2"></div><br><br>  For logistic regression: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%20L(y%2C%20a(x))%20%3D%20log(1%2Bexp%5B-y*a(x)%5D)" alt="L (y, a (x)) = log (1 + exp [-y * a (x)])"></div><br><br>  Despite the fact that the method of support vectors is mainly geometrical motivation, it can also be presented as the problem of minimizing empirical risk. <br><br><h2>  Conclusion </h2><br>  Many teaching methods with a teacher can be viewed as particular cases of the theory developed by V.N. Vapnik and A. Ya. Chervonenkis.  This theory provides guarantees regarding the error on the test sample provided that the training sample is sufficiently sized and some hypothetical space requirements in which we search for our algorithm. <br><br><h2>  Used Books </h2><br><ul><li>  The Nature of the Statistical Learning Theory, Vladimir N. Vapnik </li><li>  Pattern Classification, 2nd Edition, Richard O. Duda, Peter E. Hart, David G. Stork </li><li>  Understanding Machine Learning: From Theory to Algorithms, Shai Shalev-Shwartz, Shai Ben-David </li></ul><br>  PS Please write in a personal about all inaccuracies and typos <br><br>  PPS Many thanks to the authors of <a href="https://habrahabr.ru/post/264709">this</a> article for the TeX-editor <br></div><p>Source: <a href="https://habr.com/ru/post/277241/">https://habr.com/ru/post/277241/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../277231/index.html">Training courses. Training</a></li>
<li><a href="../277233/index.html">How graph databases help fight e-commerce fraud</a></li>
<li><a href="../277235/index.html">Preparing ASP.NET Core: how to present static content as resources</a></li>
<li><a href="../277237/index.html">How do we decide what to do?</a></li>
<li><a href="../277239/index.html">How not to recover data, or so lucky for you too</a></li>
<li><a href="../277243/index.html">Useful links for warranty verification</a></li>
<li><a href="../277245/index.html">Implementing multi-level undo / redo functionality using the example of a spreadsheet prototype</a></li>
<li><a href="../277247/index.html">Registration for NeoQUEST-2016 is open</a></li>
<li><a href="../277249/index.html">How to turn your old mobile phone into a home security system</a></li>
<li><a href="../277251/index.html">ARP: Cisco equipment features and interesting cases. Part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>