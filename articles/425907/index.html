<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Making a machine learning project in Python. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A Complete Machine Learning Walkthrough Translation In Python: Part Two 

 Bringing together all parts of a machine learning project can be very diffi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Making a machine learning project in Python. Part 2</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/225/910/6f3/2259106f3ccc19ae2b8b1ec9f316c4f2.png"><br><br>  <i><a href="https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2">A Complete Machine Learning Walkthrough</a> Translation <a href="https://towardsdatascience.com/a-complete-machine-learning-project-walk-through-in-python-part-two-300f1f8147e2">In Python: Part Two</a></i> <br><br>  Bringing together all parts of a machine learning project can be very difficult.  In this series of articles, we will go through all the stages of the implementation of the machine learning process using real data, and learn how the various techniques are combined with each other. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the <a href="https://habr.com/company/nixsolutions/blog/425253/">first article,</a> we cleaned and structured the data, conducted an exploratory analysis, collected a set of attributes for use in the model, and established a baseline for evaluating the results.  With this article, we will learn how to implement in Python and compare several machine learning models, conduct hyperparameter tuning to optimize the best model, and evaluate the performance of the final model on a test dataset. <br><br>  The entire project code is <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough">on GitHub</a> , and <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%2520Learning%2520Project%2520Part%25202.ipynb">here</a> is the second notebook related to the current article.  You can use and modify the code on your own! <br><a name="habracut"></a><br><h2>  Evaluation and selection of a model </h2><br>  Memo: we are working on a controlled regression problem, we use <a href="https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/interpret-your-results/what">information on the energy consumption of buildings in New York</a> to create a model that predicts what <a href="https://www.energystar.gov/buildings/facility-owners-and-managers/existing-buildings/use-portfolio-manager/interpret-your-results/what">Energy Star Score a</a> building will receive.  We are interested in both the accuracy of prediction and the interpretability of the model. <br><br>  Today, you can choose from a <a href="http://scikit-learn.org/stable/supervised_learning.html">variety of machine learning models available</a> , and this abundance is intimidating.  Of course, there are <a href="https://docs.microsoft.com/en-us/azure/machine-learning/studio/algorithm-cheat-sheet">comparative reviews</a> on the net that will help you navigate when choosing an algorithm, but I prefer to try a few at work and see which one is better.  Machine learning for the most part is based on <a href="https://www.quora.com/How-much-of-deep-learning-research-is-empirical-versus-theoretical">empirical, rather than theoretical results</a> , and it is almost <a href="http://www.statsblogs.com/2014/01/25/machine-learning-lesson-of-the-day-the-no-free-lunch-theorem/">impossible to know in advance which model will be more accurate</a> . <br><br>  It is usually recommended to start with simple, interpretable models, such as linear regression, and if the results are unsatisfactory, then move to more complex, but usually more accurate methods.  This graph (very unscientific) shows the relationship between the accuracy and interpretability of some algorithms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a1/602/9a1/1a16029a1b75b5ba4022d477615f352f.png"><br>  <i>Interpretability and accuracy ( <a href="http://blog.fastforwardlabs.com/2017/09/01/LIME-for-couples.html">Source</a> ).</i> <br><br>  We will evaluate five models of varying degrees of complexity: <br><br><ul><li>  Linear regression. </li><li>  The method of k-nearest neighbors. </li><li>  "Random Forest." </li><li>  Gradient boosting. </li><li>  Support vector machine. </li></ul><br>  We will consider not the theoretical apparatus of these models, but their implementation.  If you are interested in theory, you can read <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a> (available for free) or <a href="http://shop.oreilly.com/product/0636920052289.do">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a> .  In both books, the theory is beautifully explained and the effectiveness of using the mentioned methods in the R and Python languages, respectively, is shown. <br><br><h4>  Fill in the missing values </h4><br>  Although during data cleansing we dropped the columns that lack more than half of the values, we still lack many values.  Machine learning models cannot work with missing data, so we need to <a href="https://en.wikipedia.org/wiki/Imputation_(statistics)">fill</a> it <a href="https://en.wikipedia.org/wiki/Imputation_(statistics)">in</a> . <br><br>  First, we count the data and remember what it looks like: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment"># Read in data into dataframes train_features = pd.read_csv('data/training_features.csv') test_features = pd.read_csv('data/testing_features.csv') train_labels = pd.read_csv('data/training_labels.csv') test_labels = pd.read_csv('data/testing_labels.csv') Training Feature Size: (6622, 64) Testing Feature Size: (2839, 64) Training Labels Size: (6622, 1) Testing Labels Size: (2839, 1)</span></span></code> </pre> <br>  Each <code>NaN</code> value is a missing record in the data.  <a href="https://www.omicsonline.org/open-access/a-comparison-of-six-methods-for-missing-data-imputation-2155-6180-1000224.php%3Faid%3D54590">You can fill them in different ways</a> , and we will use a fairly simple method of median filling (median imputation), which replaces the missing data with average values ‚Äã‚Äãfor the corresponding columns. <br><br>  In the code below, we will create a <a href="http://scikit-learn.org/stable/">Scikit-Learn</a> imputer <code>Imputer</code> with a median strategy.  Then we will train it on training data (using <code>imputer.fit</code> ), and apply it to fill in the missing values ‚Äã‚Äãin the training and test sets (using <code>imputer.transform</code> ).  That is, the records that are missing in the <i>test data</i> will be filled with the corresponding median value from the <i>training data</i> . <br><br>  We do the filling and do not train the model on the data as it is, in order to avoid problems with <a href="https://www.kaggle.com/dansbecker/data-leakage">leakage of test data</a> , when information from the test dataset passes into the training one. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Create an imputer object with a median filling strategy imputer = Imputer(strategy='median') # Train on the training features imputer.fit(train_features) # Transform both training data and testing data X = imputer.transform(train_features) X_test = imputer.transform(test_features) Missing values in training features: 0 Missing values in testing features: 0</span></span></code> </pre> <br>  Now all values ‚Äã‚Äãare filled, no gaps. <br><br><h4>  Feature scaling </h4><br>  <a href="https://en.wikipedia.org/wiki/Feature_scaling">Scaling</a> is the general process of changing the range of a feature.  <a href="https://stats.stackexchange.com/questions/121886/when-should-i-apply-feature-scaling-for-my-data">This is a necessary step</a> , because the signs are measured in different units, which means they cover different ranges.  This greatly distorts the results of such algorithms as <a href="https://stats.stackexchange.com/questions/305906/feature-scaling-in-svm-does-it-depend-on-the-kernel">the support vector machine</a> and the k-nearest neighbor method, which take into account the distances between measurements.  And scaling allows to avoid it.  And although methods like <a href="https://stats.stackexchange.com/questions/121886/when-should-i-apply-feature-scaling-for-my-data">linear regression and the ‚Äúrandom forest‚Äù</a> do not require scaling of features, it is better not to neglect this step when comparing several algorithms. <br><br>  We will scale by casting each attribute to the range from 0 to 1. Take all the values ‚Äã‚Äãof the attribute, select the minimum and divide it by the difference between the maximum and minimum (range).  This method of scaling is often called <a href="https://machinelearningmastery.com/normalize-standardize-machine-learning-data-weka/">normalization, and the other main method is standardization</a> . <br><br>  This process is easy to implement manually, so we will use the <code>MinMaxScaler</code> object from Scikit-Learn.  The code for this method is identical to the code for filling in the missing values, only scaling is used instead of the insert.  Recall that we teach the model only on the training set, and then convert all the data. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Create the scaler object with a range of 0-1 scaler = MinMaxScaler(feature_range=(0, 1)) # Fit on the training data scaler.fit(X) # Transform both the training and testing data X = scaler.transform(X) X_test = scaler.transform(X_test)</span></span></code> </pre> <br>  Now, for every attribute, the minimum value is 0, and the maximum is 1. Filling in the missing values ‚Äã‚Äãand scaling of the signs ‚Äî these two steps are needed in almost any machine learning process. <br><br><h4>  We implement machine learning models in Scikit-Learn </h4><br>  After all the preparatory work, the process of creating, training and running models is relatively simple.  We will use the <a href="http://scikit-learn.org/stable/documentation.html">Scikit-Learn</a> library in Python, well-documented and with a well-thought-out syntax for building models.  By learning how to create a model in Scikit-Learn, you can quickly implement all sorts of algorithms. <br><br>  We will illustrate the process of creation, training ( <code>.fit</code> ) and testing ( <code>.predict</code> ) with the help of gradient boosting: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GradientBoostingRegressor <span class="hljs-comment"><span class="hljs-comment"># Create the model gradient_boosted = GradientBoostingRegressor() # Fit the model on the training data gradient_boosted.fit(X, y) # Make predictions on the test data predictions = gradient_boosted.predict(X_test) # Evaluate the model mae = np.mean(abs(predictions - y_test)) print('Gradient Boosted Performance on the test set: MAE = %0.4f' % mae) Gradient Boosted Performance on the test set: MAE = 10.0132</span></span></code> </pre> <br>  Only one line of code to create, train, and test.  To build other models, we use the same syntax, changing only the name of the algorithm. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/215/58f/ab4/21558fab42e2669b96132dff6a5b2691.png"><br><br>  In order to objectively evaluate the models, we calculated the base level using the median value of the target and obtained 24.5.  And the results were much better, so that our problem can be solved using machine learning. <br><br>  In our case, the <a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">gradient boosting</a> (MAE = 10,013) turned out to be slightly better than the ‚Äúrandom forest‚Äù (10,014 MAE).  Although these results can not be considered completely honest, because for hyper parameters we mostly use the default values.  The effectiveness of the models depends heavily on these settings, <a href="http://pyml.sourceforge.net/doc/howto.pdf">especially in the support vector machine</a> .  Nevertheless, based on these results, we will choose a gradient boosting and optimize it. <br><br><h2>  Hyperparametric model optimization </h2><br>  After choosing a model, you can optimize it for the problem being solved by setting up hyper parameters. <br><br>  But first of all, let's <a href="https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/">see what hyperparameters are and how do they differ from ordinary parameters</a> ? <br><br><ul><li>  Hyperparameters of the model can be considered the settings of the algorithm, which we set before the start of its training.  For example, the hyperparameter is the number of trees in a ‚Äúrandom forest‚Äù, or the number of neighbors in the k-nearest neighbors method. </li><li>  The model parameters are what she learns during training, for example, weights in linear regression. </li></ul><br>  By controlling the hyperparameter, we influence the results of the model, changing the balance between its <a href="https://towardsdatascience.com/overfitting-vs-underfitting-a-conceptual-explanation-d94ee20ca7f9">under-training and re-training</a> .  Under-training is a situation when a model is not sufficiently complex (it has too few degrees of freedom) to study the correspondence of features and purpose.  The under-trained model has a <a href="https://en.wikipedia.org/wiki/Bias%25E2%2580%2593variance_tradeoff">high</a> bias, which can be corrected by complicating the model. <br><br>  Retraining is the situation when a model essentially memorizes training data.  The retrained model has a <a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B8%25D1%2581%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25B8%25D1%258F_%25D1%2581%25D0%25BB%25D1%2583%25D1%2587%25D0%25B0%25D0%25B9%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B2%25D0%25B5%25D0%25BB%25D0%25B8%25D1%2587%25D0%25B8%25D0%25BD%25D1%258B">high</a> variance, which can be corrected by limiting the complexity of the model through regularization.  Both under-trained and over-trained models will not be able to generalize well the test data. <br><br>  The difficulty in choosing the right hyperparameters is that for each task there will be a unique optimal set.  Therefore, the only way to choose the best settings is to try different combinations on the new dataset.  Fortunately, Scikit-Learn has a number of methods for effectively evaluating hyperparameters.  Moreover, in projects like <a href="https://epistasislab.github.io/tpot/">TPOT,</a> attempts are being made to optimize the search for hyperparameters using approaches such as <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BD%25D0%25B5%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">genetic programming</a> .  In this article we will limit the use of Scikit-Learn. <br><br><h4>  Random search with crosscheck </h4><br>  Let's implement the method of setting up hyper parameters, which is called random cross-search searches: <br><br><ul><li>  <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization">Random search</a> is a method of choosing hyper parameters.  We define a grid, and then randomly select different combinations from it, unlike grid search, in which we sequentially try each combination.  By the way, <a href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf">random search works almost as well as the grid</a> , but much faster. </li><li>  <a href="https://www.openml.org/a/estimation-procedures/1">Cross checking</a> is a method for evaluating a selected combination of hyperparameters.  Instead of dividing the data into training and test sets, which reduces the amount of data available for training, we will use the K-Fold Cross Validation.  To do this, we divide the training data into k blocks, and then run an iterative process, during which we first train the model on k-1 blocks, and then compare the result when training on the k-th block.  We will repeat the process k times, and at the end we will get the average error value for each iteration.  This will be the final assessment. </li></ul><br>  Here is a vivid illustration of the k-block cross check with k = 5: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e17/94b/51e/e1794b51eded0314afd9f594a8e9ee5e.png"><br><br>  The whole process of random search with cross-checking looks like this: <br><br><ol><li>  Set the grid of hyperparameters. </li><li>  Randomly select a combination of hyperparameters. </li><li>  Create a model using this combination. </li><li>  We estimate the result of the model using a k-block cross check. </li><li>  We decide which hyperparameters give the best result. </li></ol><br>  Of course, all this is not done manually, but with the help of <code>RandomizedSearchCV</code> from Scikit-Learn! <br><br><h4>  A small digression: <a href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient boosting methods</a> </h4><br>  We will use a regression model based on gradient boosting.  This is a composite method, that is, the model consists of numerous "weak learners" (weak learners), in this case from individual decision trees (decision trees).  If pupils learn in parallel <a href="https://machinelearningmastery.com/bagging-and-random-forest-ensemble-algorithms-for-machine-learning/">algorithms like ‚Äúrandom forest‚Äù</a> , then the prediction result is selected by voting, then in <a href="https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/">boosting algorithms</a> like gradient boosting, students learn successively, and each of them ‚Äúfocuses‚Äù on the mistakes made by the predecessors. <br><br>  In recent years, boosting algorithms have become popular and often win machine learning contests.  <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">Gradient boosting</a> is one of the implementations in which to minimize the cost of a function, gradient descent is used (Gradient Descent).  The implementation of gradient boosting in Scikit-Learn is considered not as effective as in other libraries, for example, in <a href="http://xgboost.readthedocs.io/en/latest/model.html">XGBoost</a> , but it works well on small datasets and gives fairly accurate predictions. <br><br><h4>  Back to hyperparameter tuning </h4><br>  There are many hyperparameters in the regression using gradient boosting that need to be customized, refer you to the Scikit-Learn documentation for details.  We will optimize: <br><br><ul><li>  <code>loss</code> : minimization of the loss function; </li><li>  <code>n_estimators</code> : the number of used weak decision trees (decision trees); </li><li>  <code>max_depth</code> : maximum depth of each decision tree; </li><li>  <code>min_samples_leaf</code> : the minimum number of examples that should be in the leaf node of the decision tree; </li><li>  <code>min_samples_split</code> : the minimum number of examples needed to separate a decision tree node; </li><li>  <code>max_features</code> : the maximum number of attributes that are used to separate nodes. </li></ul><br>  I'm not sure that at least someone really understands how it all works, and the only way to find the best combination is to try different options. <br><br>  In this code, we create a grid of hyperparameters, then create a <code>RandomizedSearchCV</code> object and look for using 4-block cross-checking on 25 different combinations of hyperparameters: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Loss function to be optimized loss = ['ls', 'lad', 'huber'] # Number of trees used in the boosting process n_estimators = [100, 500, 900, 1100, 1500] # Maximum depth of each tree max_depth = [2, 3, 5, 10, 15] # Minimum number of samples per leaf min_samples_leaf = [1, 2, 4, 6, 8] # Minimum number of samples to split a node min_samples_split = [2, 4, 6, 10] # Maximum number of features to consider for making splits max_features = ['auto', 'sqrt', 'log2', None] # Define the grid of hyperparameters to search hyperparameter_grid = {'loss': loss, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_leaf': min_samples_leaf, 'min_samples_split': min_samples_split, 'max_features': max_features} # Create the model to use for hyperparameter tuning model = GradientBoostingRegressor(random_state = 42) # Set up the random search with 4-fold cross validation random_cv = RandomizedSearchCV(estimator=model, param_distributions=hyperparameter_grid, cv=4, n_iter=25, scoring = 'neg_mean_absolute_error', n_jobs = -1, verbose = 1, return_train_score = True, random_state=42) # Fit on the training data random_cv.fit(X, y) After performing the search, we can inspect the RandomizedSearchCV object to find the best model: # Find the best combination of settings random_cv.best_estimator_ GradientBoostingRegressor(loss='lad', max_depth=5, max_features=None, min_samples_leaf=6, min_samples_split=6, n_estimators=500)</span></span></code> </pre> <br>  You can use these results for a grid search by choosing parameters for the grid that are close to these optimal values.  But further adjustment is unlikely to significantly improve the model.  There is a general rule: competent design of the signs will have a much greater influence on the accuracy of the model than the most expensive hyperparameter tuning.  This is the <a href="http://www.picnet.com.au/blogs/guido/2018/04/13/diminishing-returns-machine-learning-projects/">law of decreasing profitability in relation to machine learning</a> : the design of signs gives the highest return, and the hyperparameter tuning brings only a modest benefit. <br><br>  To change the number of estimators (estimator) (decision trees) while preserving the values ‚Äã‚Äãof other hyperparameters, you can put one experiment that demonstrates the role of this setting.  The implementation is shown <a href="https://github.com/WillKoehrsen/machine-learning-project-walkthrough/blob/master/Machine%2520Learning%2520Project%2520Part%25202.ipynb">here</a> , and this is the result: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aca/18e/d51/aca18ed519f22d26c6b78af3324b8614.png"><br><br>  As the number of trees used by the model increases, the level of errors during training and testing decreases.  But errors in training are reduced much faster, and as a result, the model is retrained: it shows excellent results on training data, but it works worse on test data. <br><br>  On test data, accuracy is always reduced (after all, the model sees the correct answers for the training dataset), but a significant drop <a href="https://www.kdnuggets.com/2015/01/clever-methods-overfitting-avoid.html">speaks of retraining</a> .  This problem can be solved by increasing the amount of training data or <a href="https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/">reducing the complexity of the model using hyper parameters</a> .  Here we will not touch on the hyperparameters, but I recommend always paying attention to the issue of retraining. <br><br>  For our final model, we will take 800 appraisers, because this will give us the lowest level of error during cross-checking.  Now let's test the model! <br><br><h2>  Evaluation using test data </h2><br>  Being responsible people, we made sure that our model did not in any way get access to test data during training.  Therefore, <a href="https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets">we can use accuracy when working with test data as a</a> quality <a href="https://www.coursera.org/learn/deep-neural-network/lecture/cxG1s/train-dev-test-sets">indicator</a> of a model when it is admitted to real problems. <br><br>  We feed test data to the model and calculate the error.  Here is a comparison of the results of the default gradient boost algorithm and our customized model: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Make predictions on the test set using default and final model default_pred = default_model.predict(X_test) final_pred = final_model.predict(X_test) Default model performance on the test set: MAE = 10.0118. Final model performance on the test set: MAE = 9.0446.</span></span></code> </pre> <br>  Hyperparametric tuning has helped improve model accuracy by about 10%.  Depending on the situation, this can be a very significant improvement, but it takes a lot of time. <br><br>  You can compare the duration of training for both models using the <code>%timeit</code> magic command in Jupyter Notebooks.  First, measure the default duration of the model: <br><br><pre> <code class="python hljs">%%timeit -n <span class="hljs-number"><span class="hljs-number">1</span></span> -r <span class="hljs-number"><span class="hljs-number">5</span></span> default_model.fit(X, y) <span class="hljs-number"><span class="hljs-number">1.09</span></span> s ¬± <span class="hljs-number"><span class="hljs-number">153</span></span> ms per loop (mean ¬± std. dev. of <span class="hljs-number"><span class="hljs-number">5</span></span> runs, <span class="hljs-number"><span class="hljs-number">1</span></span> loop each)</code> </pre> <br>  One second to learn is very decent.  But the tuned model is no longer so nimble: <br><br><pre> <code class="python hljs">%%timeit -n <span class="hljs-number"><span class="hljs-number">1</span></span> -r <span class="hljs-number"><span class="hljs-number">5</span></span> final_model.fit(X, y) <span class="hljs-number"><span class="hljs-number">12.1</span></span> s ¬± <span class="hljs-number"><span class="hljs-number">1.33</span></span> s per loop (mean ¬± std. dev. of <span class="hljs-number"><span class="hljs-number">5</span></span> runs, <span class="hljs-number"><span class="hljs-number">1</span></span> loop each)</code> </pre> <br>  This situation illustrates the fundamental aspect of machine learning: <a href="http://people.inf.ethz.ch/jaggim/meetup/slides/ML-meetup-9-vonRohr-kaggle.pdf">it‚Äôs all a matter of compromise</a> .  Constantly we have to choose a balance between accuracy and interpretability, between <a href="https://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/">displacement and dispersion</a> , between accuracy and working time, and so on.  The right combination is completely determined by the specific task.  In our case, the 12-fold increase in the duration of work in relative terms is large, but insignificant in absolute terms. <br><br>  We obtained the final prediction results, let's now analyze them and find out if there are any noticeable deviations.  On the left is a graph of the density of predicted and real values, on the right is a histogram of the error: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/817/ea7/f23/817ea7f2371b83ff0ae6ae5fa02b5a1e.png" width="350"><img src="https://habrastorage.org/getpro/habr/post_images/f49/f42/5cc/f49f425cc56d717a1e75b9478d1a24d1.png" width="340"><br><br>  The forecast of the model reproduces well the distribution of real values, while on the training data the density peak is closer to the median value (66) than to the real peak of the density (about 100).  The errors have an almost normal distribution, although there are some large negative values ‚Äã‚Äãwhen the model's forecast is very different from the actual data.  In the next article we will take a closer look at the interpretation of the results. <br><br><h2>  Conclusion </h2><br>  In this article, we looked at several stages of solving a machine learning problem: <br><br><ul><li>  Filling missing values ‚Äã‚Äãand scaling features. </li><li>  Evaluation and comparison of the results of several models. </li><li>  Hyperparameter tuning using random grid search and cross-validation. </li><li>  Evaluate the best model using test data. </li></ul><br>  The results show that we can use machine learning to predict Energy Star Score based on available statistics.  With the help of gradient boosting, we managed to achieve an error of 9.1 on the test data.  Hyperparameter tuning can greatly improve results, but at the cost of a significant slowdown.  This is one of the many tradeoffs that need to be taken into account in machine learning. <br><br>  In the next article we will try to figure out how our model works.  We will also consider the main factors affecting the Energy Star Score.  If we know that the model is accurate, then we will try to understand why it predicts this way and what it tells us about the task itself. </div><p>Source: <a href="https://habr.com/ru/post/425907/">https://habr.com/ru/post/425907/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../425897/index.html">Features of using the RxJs library in the online banking system</a></li>
<li><a href="../425899/index.html">An anthill or a fortress? Building a house for the price of an apartment. 1 part</a></li>
<li><a href="../425901/index.html">Weather Station on Arduino from A to Z. Part 1</a></li>
<li><a href="../425903/index.html">Holiday comes to us: SCR has expanded the ISM-range of 868 MHz twice</a></li>
<li><a href="../425905/index.html">How to write an assembler program with overlapping instructions (another bytecode obfuscation technique)</a></li>
<li><a href="../425911/index.html">Moving cloud CRM to boxed version</a></li>
<li><a href="../425915/index.html">How inter-transport communication can replace traffic lights and shorten the way to work</a></li>
<li><a href="../425917/index.html">The fighter for justice did not give Waymo to patent the key technology of lidar</a></li>
<li><a href="../425919/index.html">Hexagon maps in Unity: save and load, textures, distances</a></li>
<li><a href="../425921/index.html">Meet the .NET community at CLRium # 4 + online</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>