<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Load balancing and fault tolerance in Odnoklassniki</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue posts with transcripts of speeches at the HighLoad ++ conference, which was held in Skolkovo, Moscow Region, on November 7‚Äî8, 2016. 

 Hel...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Load balancing and fault tolerance in Odnoklassniki</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/fee/214/006/fee214006fcc4201bc416a3c60549846.jpg" align="left" width="256">  <em>We continue posts with transcripts of speeches at the <a href="http://www.highload.ru/">HighLoad ++</a> conference, which was held in Skolkovo, Moscow Region, on November 7‚Äî8, 2016.</em> <br><br>  Hello, my name is Nikita Dukhovny, and I work as the lead system administrator in the Odnoklassniki project. <br><br>  At the moment, the Odnoklassniki infrastructure is located on more than 11 thousand physical servers.  They are located in 3 main data centers in Moscow.  We also have CDN presence points.  According to the latest data at rush hour, we give our users more than 1 terabit of traffic per second. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the system administration department, we develop and develop automation systems.  We are engaged in many research tasks.  We help developers launch new projects. <br><br>  Today we will talk about load balancing and fault tolerance on the example of our social network. <br><a name="habracut"></a><br>  At the dawn of the project, Odnoklassniki was faced with the task of making load balancing between many frontends.  The project lived explosive growth, and therefore the decision was made as follows.  The user went to <code>www.odnoklassniki.ru</code> , in response received a redirect to a specific server name, for example, <code>wg13.odnoklassniki.ru</code> .  This name was tightly bound to a physical server. <br><br>  This approach has obvious drawbacks.  First, in case of problems with the server, the user received an error in the browser.  Secondly, if we wanted to take the server out of rotation, we were faced with the fact that many users bookmarked the name of this server.  Something needed to be changed. <br><br>  Then we decided to put into operation load balancers.  We had enough to use Layer 4 balancing.  What does it mean?  This means that the balancer is aware of the IP address and port.  That is, the user, coming to virtual IP, port 80, will be sent to port 80 of the real server, coming to port 443, will be sent to port 443. <br><br>  We considered different options.  At that time, the choice was between proprietary solutions ‚Äî both software and hardware and software ‚Äî and the open source LVS project (Linux Virtual Server).  According to the test results, we did not find any reason to dwell on proprietary solutions. <br><br><h1>  How we organized the LVS work scheme </h1><br>  The user request goes to the network equipment, it sends a request to LVS.  LVS, in turn, sends a request to one of the front-end servers.  Moreover, on LVS we use the persistence settings, that is, the next user request will be sent to the same front-end server. <br><br><img src="https://habrastorage.org/files/0a1/531/d2a/0a1531d2ac56497c8a060f02abfd8a49.png"><br><br>  We wanted our LVS cluster to be fault tolerant, so we organized master / standby pairs.  Between master / standby pairs, sessions are constantly synchronized.  What is it for?  So that when the master server is sent out, the user, upon entering the standby server, is sent to the same frontend. <br><br>  VRRP is also running between the servers using the tools of the well-known keepalive daemon.  This is to ensure that the IP address of the LVS server itself is moved to standby.  We have a lot of such master / standby pairs.  Between them we balance using ECMP protocol.  Moreover, our network equipment is also aware of the settings of persistence, and thus the whole chain works.  A user request will be sent to the same LVS pair, and the LVS pair, in turn, will send to the same frontend. <br><br>  It was necessary to somehow manage the balance sheet.  At that time, we chose the popular ldirector solution.  The ldirecor daemon adds real servers to the balancing table based on its config.  It also checks real servers.  And if any of the real servers starts to return an error, ldirector adjusts the balance table, and this server is taken out of rotation. <br><br>  Everything looked good, but the number of servers increased.  At the moment, in each of our data centers, we have two hundred web-based frontends ‚Äî these are the servers that process requests from <code>www.odnoklassniki.ru</code> . <br><br>  The fact is that ldirector performs its checks in single-threaded mode.  In case of problems, when the servers start responding longer than the timeout, the following happens: the request arrives at the first server, waits for the timeout time (for example, 5 seconds), and only after that the test starts the second server.  Thus, with a large-scale problem, for example, when half of the servers are affected, the output of this half takes more than 8 minutes.  It was unacceptable. <br><br>  To solve this problem, we wrote our own solution: ok-lvs-monitor.  It has two main advantages. <br><br><ol><li>  <strong>Performs multi-threaded checks</strong> .  This means that with the above problem, the servers will be taken out of rotation at the same time. </li><li>  <strong>Integrates with our portal configuration system</strong> .  If earlier to change the table we needed to edit configs on the servers, now it is enough to edit the configuration in the same system where we manage the settings of our applications. </li></ol><br>  During the operation of the LVS, we are faced with a number of technical problems. <br><br><ul><li>  We stumbled upon the fact that when synchronizing sessions there is a rather large discrepancy between master and standby. </li><li>  The synchronization process worked only on the first processor core. </li><li>  On a standby server, it consumed over 50% of this kernel. </li><li>  Different things were also noticed.  For example, the ipvsadm balance table management utility did not distinguish between Active / InActive sessions in the case of our configuration. </li></ul><br>  We reported the bugs to the LVS project developer, helped with testing, and all these problems were fixed. <br><br><h1>  Persistence settings </h1><br>  Earlier, I mentioned that we use the settings for persistence.  What is it for us? <br><br>  The fact is that each front-end server stores in its memory information about the user session.  And if the user comes to another frontend, this frontend must follow the user login procedure.  For the user, this is transparent, but nevertheless it is a rather expensive technical operation. <br><br>  LVS can do persistence based on client IP address.  And here we are faced with the following problem.  Mobile Internet gained popularity, and many operators, first of all, the largest operator in Armenia, hid their users for just a few IP addresses.  This led to the fact that our mobile-web servers were very unevenly loaded, and many of them were overloaded. <br><br>  We decided to ensure the persistence of the cookie that we put to the user.  When it comes to a cookie, unfortunately, it is impossible to limit ourselves to Layer 4 balancing, because we are already talking about Layer 7, about the application level.  We reviewed various solutions, and at that time HAProxy was the best.  Popular today nginx then did not have the proper mechanisms of balancing. <br><br><img src="https://habrastorage.org/files/f9b/dcf/413/f9bdcf41363048adbf6baf251212da26.png"><br><br>  A user request, getting on the network equipment, is balanced by the same ECMP protocol, so far nothing has changed.  But then the interesting begins: without any persistence, a pure Round-robin request is sent to any of the HAProxy servers.  Each of these servers in its configuration stores the correspondence of the cookie value to a specific front-end server.  If the user has arrived for the first time, a cookie will be set for him; if the user has arrived for the second and subsequent times, he will be sent to the same mobile frontend. <br><br>  Wait, you say, that is, "Classmates" put a cluster of balancers behind a cluster of balancers?  It looks somehow complicated. <br><br><img src="https://habrastorage.org/files/b82/97d/7f4/b8297d7f44dc409f931197507b2e806a.jpg"><br><br>  In fact, let's see.  Why not? <br><br>  In the case of our situation, the percentage of traffic passing through HAProxy is less than 10% of the traffic passing through the entire LVS.  That is, for us this is not some expensive solution.  But the advantage of this approach is obvious. <br><br>  Imagine that we are conducting some kind of experimental reconfiguration of the HAProxy server.  We want to evaluate the effect.  And in this case, we start this HAProxy server in rotation with low weight.  First, we can collect all the statistics before making a further decision.  Secondly, in the worst case, only a small percentage of our users will notice any problems.  After successful application on a small part, we can continue with the entire cluster. <br><br><h1>  Accidents in the data center </h1><br>  ‚ÄúClassmates‚Äù in their history have repeatedly encountered accidents.  We had a story when both the main and backup optics to the data center burned down.  Data center has become unavailable.  We had a story when electrical wiring in a data center melted due to an incorrect design, which led to a fire.  We had a very exciting story when a hurricane in Moscow tore off a piece of the roof, and he disabled the cooling system in the data center.  We had to decide which servers to shut down and which ones to leave, because overheating had begun. <br><br>  According to the results of these accidents, we formulated for ourselves the rule: ‚ÄúClassmates‚Äù should work in case of failure of any of the data centers. <br><br>  How did we go towards this goal? <br><br><ul><li>  We prepared our data storage systems.  We implemented the functionality that allows our data systems to understand in which data center they are located. </li><li>  We use replication factor = 3.  Together, these two points lead to the fact that in each of the data centers appears its own replica of the data. </li><li>  In our systems we use quorum = 2.  This means that when one of the data centers departs, the system will continue its work. </li></ul><br>  We needed to distribute our frontend servers across three data centers.  We did this, taking into account the power reserve, so that the front-ends in the data center could take on all the requests that had previously been made to the data center, which had now refused. <br><br><img src="https://habrastorage.org/files/40c/efd/d9e/40cefdd9e3c14c8fbb64e44bc096cb9b.png"><br><br>  If you make a DNS request on <code>www.ok.ru</code> , then you will see that you are given 3 IP addresses.  Each of these addresses corresponds to a single data center. <br><br>  We handle the situation with the accident in the data center automatically.  Here you can see an example of such a check: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/993/c55/d5a/993c55d5ae83184a9a67bc7d5a04d825.png"><br><br>  It can be seen that in case 10 of the last 20 checks were unsuccessful, the IP address of this data center will cease to be given.  10 is quite a lot, the output of the data center is a difficult operation.  We want to make sure that the accident has started.  Here you can see that the data center will be automatically rotated in the event of 20 of the 20 successful checks.  Yes, the conclusion - this is a difficult operation, but we want to be sure that everything is in order, before we get users. <br><br>  According to our calculations, with this method, in 5 minutes 80% of the audience will leave the data center.  There are assumptions, where did the figure 5 minutes come from? <br><br><img src="https://habrastorage.org/files/9ef/d26/f68/9efd26f68a4549269371cb312daf0b82.png"><br><br>  Network.  At the network level, we use a rather curious trick.  In fact, the border routers of each of the data centers announce all the networks in the world. <br><br>  Imagine that a user goes to the IP address <code>217.20.156.159</code> .  The user gets into one of the data centers.  Another user, by accessing the same IP address, can actually get into the adjacent data center.  What is it for?  In case of failure of one of the border routers, users will not notice any effect. <br><br>  There is a subtle point.  I just told you about the fact that in each data center we will have our IP address for the service and it seems that I will give you conflicting information.  Not really.  At the core level of the network, each data center still has its own set of networks, and just border routers, using either a direct connection or a ring, have the opportunity to send a request to the desired data center. <br><br><img src="https://habrastorage.org/files/da0/adc/ae3/da0adcae35a44cd4b36e195f4117e5b5.png"><br><br>  About the core network.  All network equipment with us is reserved with such a reserve of power to be able to take on the load of the outgoing neighbor.  Only an accident with the entire network core will really lead to a situation where we will have to output traffic from data centers. <br><br><h1>  Heavy content </h1><br>  By heavy content, I mean music, videos and photos. <br><br>  The fact is that a significant part of the Odnoklassniki audience cannot boast the same quality of Internet connection as the residents of the capital.  First, most of our audience lives outside the Russian Federation.  In many countries, the speed of foreign Internet and domestic very different.  Secondly, users in remote regions, in principle, often cannot boast of a stable channel.  Even if it is stable, it does not mean at all that it is not slow. <br><br>  We wanted to make it so comfortable for users to work with Odnoklassniki.  So we thought about our own CDN solution. <br><br>  The idea behind CDN is pretty simple.  You take the caching servers and put them close to the user.  When constructing a CDN, one of two classical approaches is usually used. <br><br>  <strong>IP Anycast</strong> .  Its essence lies in the fact that each of your sites announces to the world the same network, the same addresses.  And what platform your user gets is determined by the current topology of the Internet.  The advantages of this approach are obvious: you do not need to implement any logic in order for the user to get to the optimal site. <br><br><img src="https://habrastorage.org/files/4d6/9dd/b01/4d69ddb01fc94afdb0dfadfd1c5f07f7.png"><br><br>  Cons are also clearly visible.  Imagine that you have a CDN in Voronezh, you want to do serious service work with it.  To do this, you need to send all users to a specific site, for example, to St. Petersburg.  Using IP Anycast in its purest form, you do not have direct mechanisms for how you can do this.  In addition, you will need to prepare your application for such a turn of events as an unexpected user transition.  In the Internet topology, something has changed, and the user sends the next request to another site. <br><br>  Another classic technology is based on the use of <strong>geolocation and DNS</strong> .  What is the point? <br><br><img src="https://habrastorage.org/files/19c/4f3/a30/19c4f3a30f474262a7b6915d6ce025b3.png"><br><br>  A Muscovite, making a request for static content, for a service name that gives static content to the DNS, will receive in return a Moscow IP address and will be sent to the site in Moscow.  A resident of Novosibirsk, making a similar request, will receive the IP address of the site in Novosibirsk and will be sent there. <br><br>  The advantages of this approach are a high degree of control.  The above task to send a resident of Voronezh to the site of your choice becomes quite simple.  You just need to edit your base.  But there are also disadvantages to this approach.  First, this approach usually uses a geolocation base like GeoIP, and geographic proximity does not mean the fastest connection to this site.  Secondly, this approach does not take into account the change in the topology on the Internet. <br><br>  Odnoklassniki had one more task: we wanted to launch projects like ‚Äútraffic to‚Äú Odnoklassniki ‚Äùfor free‚Äù.  That is, choosing, for example, the mobile connection of our partner, everything that you do through the Odnoklassniki mobile app is not counted as a traffic expense.  To accomplish such a task, our CDN should have the functionality that allows you to send only those networks to the site that our partner wants to see there. <br><br>  Classic solutions for this problem are not suitable.  What have we done? <br><br><img src="https://habrastorage.org/files/707/042/f32/707042f324ee46f2b332013184e65ca9.png"><br><br>  At the CDN site, our BGP server establishes a connection with the provider‚Äôs Route Server, receiving from it a list of the networks that the provider wants to see on it.  Next, the list of prefixes is transmitted to our GSLB DNS, and then the user, by accessing a DNS request, receives an IP address just in the right area.  The site, in turn, of course, takes all the content from the main data centers. <br><br><img src="https://habrastorage.org/files/303/7c1/e77/3037c1e77b3e4897bb8988c07ef2c149.png"><br><br>  Inside the site between the servers we balance, using a rather trivial approach.  Each server has its own IP-address, and both IP-addresses are sent by DNS-request.  Thus, users will be distributed between the servers approximately equally.  In case of failure of one of the servers by means of the same VRRP protocol, the IP address will be transparently transferred to another node.  Users will not notice anything. <br><br><img src="https://habrastorage.org/files/7c7/bc7/757/7c7bc7757b604d7fb6cd837786ddb7b8.png"><br><br>  Of course, we carry out inspections of all our sites.  In case the site ceases to return the status OK, we cease to give its IP address.  In this case, the user from Novosibirsk will go to data centers in Moscow.  It will be a little slower, but it will work. <br><br>  On our CDN sites we use internally developed applications for video, music and photos, as well as in addition to this nginx.  I would like to mention the main point of optimization, which will be applied in our applications.  Imagine that you have several servers that deliver heavy content.  The user comes to one server and asks him for this content.  Content is not there.  Then the server takes this content from the site in Moscow.  But what will happen when the next user comes for the same content to the second server?  We would not want the server to go again for the same content to Moscow.  Therefore, in the first place, he goes to his neighbor server and takes the content from it. <br><br><h1>  Future </h1><br>  I would like to talk about several projects that we would be very interested in doing and which we would be very interested in implementing. <br><br>  We would like to see that each of our services, for example, <code>www.odnoklassniki.ru</code> , still live with <strong>one IP address</strong> .  For what? <br><br><ul><li>  In case of failure of the data center in this case, everything would be transparent for users. </li><li>  Vy would get rid of the problem with incorrectly leading caching DNS, which continue to give the IP address of the derived site, despite the fact that we don‚Äôt give it away. </li><li>  Once in TTL time (that is, in our case, once every 5 minutes), the user can still move to another site. </li></ul><br>  This is not a very technical operation, but nevertheless I would like to get rid of it.  This is a very ambitious project, within its framework it is necessary to solve many problems.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How to ensure persistence settings? How to carry out service work without being noticed? How to display a data center? In the end, how to prepare our application for this turn of events? All these issues to be solved. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Another interesting topic. These are </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Layer 4 balancers that work in user space</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . What is their essence?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, when the balancer uses the Linux network stack, it constantly switches between user space and kernel space. This is quite an expensive operation for a CPU. In addition, network stack Linux is a universal solution. Like any universal solution, it is not ideal for this particular task. User space Layer 4 balancers independently implement the necessary functionality of the network stack that they need. Due to this, according to rumors, everything works very quickly. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I‚Äôll tell you a little secret: in fact, now one of the partners is conducting research and some improvements for us, and we could see for ourselves the huge difference in speed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">These are only two projects from a possible future, which I have just named.</font></font> In fact, they are much more.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And, of course, most of them are connected with other topics, not only with load balancing and fault tolerance. </font></font><br><br><img src="https://habrastorage.org/files/2d6/e07/bfa/2d6e07bfad8b41798c1961a52a46d1f0.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's see where this road will lead us. </font><font style="vertical-align: inherit;">We always welcome new strong colleagues, please visit </font></font><a href="https://v.ok.ru/publishing.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">our technical site</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , you will find many interesting publications and videos of my colleagues' speeches on it.</font></font><br><br>  Thanks for attention! <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/7HTYrx3neJk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br> <em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last nine minutes of the speech, Nikita Dukhovny, answered questions from the audience.</font></font></em> </div><p>Source: <a href="https://habr.com/ru/post/321448/">https://habr.com/ru/post/321448/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321438/index.html">Offer a project for testing with the PVS-Studio analyzer: now on GitHub</a></li>
<li><a href="../321440/index.html">How to enable JTAG debugging via USB</a></li>
<li><a href="../321442/index.html">Openstack. Detective story or where the connection is lost? Part one</a></li>
<li><a href="../321444/index.html">We read Google tables from the web application</a></li>
<li><a href="../321446/index.html">How does it feel to be a developer in Russia when you're forty</a></li>
<li><a href="../321452/index.html">CEF, Angular 2 using .Net Core class events</a></li>
<li><a href="../321454/index.html">Connecting Facebook SDK for Xamarin.Forms</a></li>
<li><a href="../321456/index.html">The method of recursive coordinate bisection for decomposition of computational grids</a></li>
<li><a href="../321458/index.html">How we made the application of the international loyalty program PINS: case</a></li>
<li><a href="../321460/index.html">Games do not have to entertain</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>