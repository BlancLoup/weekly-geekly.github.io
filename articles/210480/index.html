<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We parallelize processes to speed up computations and tasks in Linux.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Almost all personal computers released over the past few years have at least a dual-core processor. If you, the reader, are not very old computer or n...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We parallelize processes to speed up computations and tasks in Linux.</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/810/6cb/706/8106cb7066be2eb8533931b95e688c59.jpg"><br><br>  Almost all personal computers released over the past few years have at least a dual-core processor.  If you, the reader, are not very old computer or not some budget laptop, then most likely you are the owner of a multiprocessor system.  And if you still like to play games, then about a hundred GPU-cores are available to you.  However, the lion's share of time all this power gathering dust.  Let's try to fix it. <br><a name="habracut"></a><br><br><h4>  Introduction </h4><br>  Very rarely, we use the power of several processors (or cores) to solve everyday tasks, and the use of a powerful graphics processor is often reduced to computer games.  Personally, I don‚Äôt like it: I‚Äôm working - why should the processor rest?  Disorder.  It is necessary to adopt all the capabilities and advantages of multiprocessors (multi-core) and parallelize all that is possible, saving them a lot of time.  And if you also connect a powerful video card with a hundred cores on board to work ... which, of course, will fit only for a narrow circle of tasks, but it will nevertheless speed up the calculations quite significantly. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Linux is very strong in this regard.  First, in the majority of distributions out of the box are available good tools for parallel execution, and secondly, a lot of software is written, designed for multi-core systems.  On the flexibility of customization, I think, do not even need to talk.  The only thing that can cause problems is the drivers for the video card, but there's nothing you can do. <br><br>  To begin with we will be defin with methods of parallelization.  There are two of them: by the means of the application itself, which runs several parallel threads (multithreading) to perform the task.  Another method is to run multiple copies of the application, each of which will process a certain portion of data.  The operating system in this case will independently distribute the processes to the cores or processors (multitasking). <br><br><h4>  We parallelize directly in the terminal </h4><br>  Let's begin, perhaps, with parallel start of processes directly from a terminal window.  Running terminal processes running for a long time is not a problem.  And what if you need two such processes?  ‚ÄúIt‚Äôs not a problem either,‚Äù you say, ‚Äúwe‚Äôll just run the second process in another terminal window.‚Äù  And if you want to run ten or more?  Hmm ... The first thing that comes to mind is to use the xargs utility.  If you give it the option --max-procs = n, then the software will execute n processes at the same time, which is of course good for us.  The official manual recommends using grouping by arguments with the --max-procs option (the -n option): without this, problems with parallel launch are possible.  For example, imagine that we need to archive a bunch of large (or small) files: <br><br><pre><code class="bash hljs">$ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> folder/with/files $ ls | xargs -n1 --max-procs=4 gzip</code> </pre> <br>  How much time have we won?  Is it worth bothering with this?  It will be appropriate to bring the numbers.  On my quad-core processor, the usual archiving of five files, each of which weighed about 400 MB, took 1 minute 40 seconds.  Using <code>xargs --max-procs=4</code> elapsed time was reduced almost four times: 34 seconds!  I think the answer to the question is obvious. <br><br>  Let's try something more interesting.  Convert, for example, WAV files to MP3 using lame: <br><br><pre> <code class="bash hljs">$ ls *.wav | xargs -n1 --max-procs=4 -I {} lame {} -o {}.mp3</code> </pre><br>  Looks awkward?  I agree.  But parallel execution of processes is not the main task of xargs, but only one of its capabilities.  In addition, xargs does not behave very well with passing special characters, such as space or quotation marks.  And here we come to the aid of a wonderful utility called GNU Parallel.  The software is available in standard repositories, but I do not recommend installing it from there: in the Ubuntu repositories, for example, I came across a version two years ago.  It is better to compile yourself a fresh version from source: <br><br><pre> <code class="bash hljs">$ wget ftp.gnu.org/gnu/parallel/parallel-latest.tar.bz2 $ tar xjf parallel-latest.tar.bz2 $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> parallel-20130822 $ ./configure &amp;&amp; make <span class="hljs-comment"><span class="hljs-comment"># make install</span></span></code> </pre><br>  The very name of the utility speaks of its narrow specialization.  Indeed, Parallel is much more convenient for parallelization, and its use looks more logical.  The above example using Parallel instead of xargs turns into this: <br><br><pre> <code class="bash hljs">$ ls *.wav | parallel lame {} -o {}.mp3</code> </pre><br>  By the way, if you are sitting under Ubuntu or Kubuntu, then the example will not work, giving out incomprehensible errors.  This is fixed by adding the '--gnu' key (this also applies to the following example).  Read more about the problem <a href="http://bit.ly/--gnu">here</a> . <br><br>  And why do not we set the number of simultaneously running processes?  Because Parallel will do it for us: it will determine the number of cores and will run on the process to the core.  Of course, you can also set this number manually using the -j option.  By the way, if you need to run a task on different machines, then to improve portability, it is convenient to set this option in the -j + 2 format, which in this particular case means ‚Äúto run two more processes at a time than there are computing units in the system‚Äù. <br><br>  If you make friends with Parallel c Python, then we get a powerful tool for parallel execution of tasks.  For example, downloading web pages from the Web and their subsequent processing may look like this: <br><br><pre> <code class="bash hljs">$ python makelist.py | parallel -j+2 <span class="hljs-string"><span class="hljs-string">'wget "{}" -O - | python parse.py'</span></span></code> </pre><br>  But even without Python, this utility has plenty of features.  Be sure to read the man - there are a lot of interesting examples. <br><br>  In addition to Parallel and xargs, of course, there are a lot of other utilities with similar functionality, but they can‚Äôt do anything that the first two don‚Äôt. <br><br>  With this sorted out.  Moving on. <br><br><h4>  Parallel compilation </h4><br>  Collecting something from the source is a common thing for Linux users.  Most often, you have to collect something insignificant, for such projects, no one thinks about any parallel compilation.  But sometimes more projects come across, and it takes almost an eternity to wait until the end of the build: the build, for example, Android (AOSP) from source (in one thread) lasts about five hours!  For this kind of project you need to start all the cores. <br><br>  First of all, I think it is obvious that the actual compilation itself (GCC, for example) is not parallelized.  But large projects are often composed of a large number of independent modules, libraries, and other things that can and should be compiled simultaneously.  Of course, we don‚Äôt need to think about how to parallelize the compilation - make will take care of this, but only if the dependencies are written in the makefile.  Otherwise, make will not know in which sequence to build and what can be built at the same time and what cannot.  And since the makefile is the concern of the developers, for us it all comes down to executing the command <br><br><pre> <code class="bash hljs">$ make -jN</code> </pre><br>  after which make will begin building the project, simultaneously running up to N tasks. <br><br>  By the way, about choosing the value for the -j parameter.  The web often recommends using the number 1.5 * &lt;number of computing units&gt;.  But this is not always true.  For example, the project build with a total weight of 250 MB on my quad core was the fastest with the -j parameter value equal to four (see screenshot). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/130/3de/3a9/1303de3a9ae8433d1fe34b723bf732af.jpg" alt="Dependence of compile time on the value of the -j parameter"><br>  Compilation time versus parameter value <br><br><br>  To win some more time, you can add the key '-pipe' to the GCC.  With this key, data transfer between different stages of compilation occurs through the exchange channels (pipes), and not through temporary files, which slightly (very little) speeds up the process. <br><br>  In addition to make, you can also try <a href="http://pmake.sourceforge.net/">pmake</a> , a parallel build system written in Python.  For users, its use is not much different from make, but for developers it can be quite interesting, since it has more extensive features than the standard tool. <br><br><h4>  Parallel rsync </h4><br>  If you have ever used Rsync to synchronize a huge number of small files with a remote server, you probably noticed a decent delay in the receiving file list stage.  Is it possible to speed up this stage by paralleling?  Sure you may.  Much time is spent on network latency.  To minimize these temporary losses, we will run several copies of Rsync, and in order not to copy the same files - we will set each copy, for example, into a separate directory.  To do this, we use a combination of the --include and --exclude parameters, like this: <br><br><pre> <code class="bash hljs">$ rsync -av --include=<span class="hljs-string"><span class="hljs-string">"/a*"</span></span> --exclude=<span class="hljs-string"><span class="hljs-string">"/*"</span></span> -P login@server:remote /localdir/ $ rsync -av --include=<span class="hljs-string"><span class="hljs-string">"/b*"</span></span> --exclude=<span class="hljs-string"><span class="hljs-string">"/*"</span></span> -P login@server:remote /localdir/</code> </pre><br>  You can manually start multiple copies in different terminals, but you can connect Parallel: <br><br><pre> <code class="bash hljs">$ cat directory_list.txt | parallel rsync -av --include=<span class="hljs-string"><span class="hljs-string">"/{}*"</span></span> --exclude=<span class="hljs-string"><span class="hljs-string">"/*"</span></span> ...</code> </pre><br><br><h4>  Turbojet file copying over SSH </h4><br>  Typically, to synchronize directories between two hosts, Rsync runs on top of SSH.  By speeding up the SSH connection, let's speed up the work of Rsync.  And SSH can be accelerated by using the OpenSSH HPN patch set, eliminating a number of bottlenecks in the server-side and client-side SSH buffering mechanism.  In addition, HPN uses a multi-threaded version of the AES-CTR algorithm, which improves file encryption speed (activated with the <code>-oCipher=aes[128|192|256]-ctr</code> ).  To check if you have OpenSSH HPN installed, drive in the terminal: <br><br><pre> <code class="bash hljs">$ ssh -V</code> </pre><br>  and look for the HPN substring entry.  If you have the usual OpenSSH, you can install the HPN version like this: <br><br><pre> <code class="bash hljs">$ sudo add-apt-repository ppa:w-rouesnel/openssh-hpn $ sudo apt-get update -y $ sudo apt-get install openssh-server</code> </pre><br>  Then add the following lines to <code>/etc/ssh/sshd_config</code> : <br><br><pre> <code class="bash hljs">HPNDisabled no TcpRcvBufPoll yes HPNBufferSize 8192 NoneEnabled yes</code> </pre><br>  then restart the SSH service.  Now create the Rsync / SSH / SCP connection again and evaluate the winnings. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4b1/6b4/b5c/4b16b4b5cb6502787c60b5a33937d06b.jpg" alt="We include support HPN"><br>  We include support HPN <br><br><br><h4>  File compression </h4><br>  All the accelerations that we have done above are based on the simultaneous launch of several copies of the same process.  The process scheduler of the operating system resolved these processes between the cores (processors) of our machine, due to which we received acceleration.  Let's go back to the example where we compressed several files.  But what if you need to compress one huge file, and even slow bzip2?  Fortunately, file compression lends itself very well to parallel processing ‚Äî the file is divided into blocks, and they are compressed independently.  However, standard utilities, like gzip and bzip2, do not have this functionality.  Fortunately, there are many third-party products that can do this.  Consider only two of them: parallel analog gzip - <a href="http://zlib.net/pigz/">pigz</a> and analog bzip2 - <a href="http://compression.ca/pbzip2/">pbzip2</a> .  These two utilities are available in the standard Ubuntu repositories. <br><br>  Using pigz is absolutely no different from working with gzip, except the ability to specify the number of threads and the block size.  In most cases, the block size can be left at default, and as the number of threads, it is desirable to specify a number equal to (or 1‚Äì2 more) the number of processors (cores) of the system: <br><br><pre> <code class="bash hljs">$ pigz -c -p5 backup.tar &gt; pigz-backup.tar.gz</code> </pre><br>  Executing this command on the backup.tar file weighing 620 MB took me 12.8 seconds, the resulting file weighed 252.2 MB.  Processing the same file with gzip: <br><br><pre> <code class="bash hljs">$ gzip -c backup.tar &gt; gzip-backup.tar.gz</code> </pre><br>  took 43 seconds.  The resulting file at the same time weighed only 100 KB less, compared to the previous one: 252.1 MB.  Again, we received almost fourfold acceleration, which is good news. <br><br>  Pigz can only parallelize compression, but not unpacking, which can't be said about pbzip2 - which both can do.  Using the utility is similar to its non-parallel version: <br><br><pre> <code class="bash hljs">$ pbzip2 -c -p5 backup.tar &gt; pbzip-backup.tar.bz2</code> </pre><br>  Processing the same backup.tar file took me 38.8 seconds, the size of the resulting file was 232.8 MB.  Compression using normal bzip2 took 1 min 53 s, with a file size of 232.7 MB. <br><br>  As I said, with pbzip2, you can speed up unpacking.  But here you need to take into account one nuance - in parallel, you can only unpack what was previously packed in parallel, that is, only archives created with pbzip2.  Unpacking in several streams of a regular bzip2 archive will be done in one stream.  Well, some more numbers: <br><br><ul><li>  normal unpacking - 40.1 s; </li><li>  unpacking in five streams - 16.3 s. </li></ul><br>  It remains only to add that the archives created using pigz and pbzip2 are fully compatible with archives created using their non-parallel counterparts. <br><br><h4>  Encryption </h4><br>  By default, eCryptfs is used to encrypt the home directory in Ubuntu and all derivative distributions.  At the time of writing, eCryptfs did not support multi-threading.  And this is especially noticeable in folders with a large number of small files.  So if you have a multi-core, then eCryptfs use impractical.  A better replacement would be to use dm-crypt or Truecrypt systems.  True, they can only encrypt entire partitions or containers, but they do support multi-streaming. <br><br><blockquote><h5>  INFO </h5><br>  The NPF packet filter from NetBSD 6.0 allows you to achieve maximum performance on multi-core systems through parallel multi-stream processing of packets with the minimum number of locks. <br></blockquote><br>  If you want to encrypt only a specific directory, and not a whole disk, then you can try <a href="http://www.arg0.net/encfsintro">EncFS</a> .  It is very similar to eCryptfs, but it works, unlike the latter, not in kernel mode, but using FUSE, which makes it potentially slower than eCryptfs.  But it supports multi-threading, so on multi-core systems there will be a speed advantage.  In addition, it is very easy to install (available in most standard repositories) and use.  All you need to do <br><br><pre> <code class="bash hljs">$ encfs ~/.crypt-raw ~/crypt</code> </pre><br>  enter a passphrase, and that‚Äôs all: encrypted versions of files will be in .crypt-raw, and unencrypted in crypt.  To unmount EncFS, run: <br><br><pre> <code class="bash hljs">$ fusermount -u ~/name</code> </pre><br>  Of course, all this can be automated.  How to do this, you can read <a href="http://wiki.archlinux.org/index.php/EncFS">here</a> . <br><br><h4>  I love to watch kernels plow </h4><br>  We load the processor to the full, but sometimes it is necessary to monitor its work.  In principle, almost every distribution has a good snap to monitor processor utilization, including information about each individual core or processor.  In Kubuntu, for example, KSysGuard very successfully displays the current load of cores (see the screen ‚ÄúKSysGuard on a quad-core system‚Äù). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e96/778/46d/e9677846d7143843ddfb0a0982e1b40e.jpg" alt="KSysGuard on quad-core system"><br>  KSysGuard on quad-core system <br><br><br>  But there are other interesting utilities that allow you to contemplate the work of the processor.  Fans of console solutions will love htop, a more colorful and interactive equivalent of top.  I also advise you to pay attention to <a href="http://conky.sourceforge.net/">Conky</a> - a powerful and easily customizable system monitor.  It is very easy to configure it to monitor the workload of each core and processor as a whole.  For each core, you can display a separate schedule.  In the screenshot you can see my version of the utility configuration. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2ef/b5b/df6/2efb5bdf6b3e6736672b14d4b4a3bb8d.jpg" alt="This is what Conky looks like."><br>  This is what Conky looks like. <br><br><br><img src="https://habrastorage.org/getpro/habr/post_images/665/23d/179/66523d1790bf1da68be85a90678ef30c.jpg" alt="Htop in action"><br>  Htop in action <br><br><br>  I uploaded the contents of the corresponding configuration file <a href="http://pastebin.com/63wgDSj6">here</a> .  On the Web, by the way, is full of interesting configs, which can be used as a basis and remade for yourself. <br><br>  But these utilities provide only information about the workload, which often may not be enough.  Mpstat from the <a href="http://bit.ly/sysstat">sysstat</a> set provides more interesting information, such as the idle time of each kernel, the time spent waiting for I / O, or the time taken to process interrupts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/24e/6ec/18a/24e6ec18af68cba6846c7dd34b614b74.jpg" alt="Mpstat utility output"><br>  Mpstat utility output <br><br><br><h4>  GPU not only for games </h4><br>  It is no secret that modern GPUs have very large processing power.  But due to the fact that the GPU cores have a special architecture and a limited set of available commands, the GPU is only suitable for solving a narrow circle of tasks.  Previously, only shading gurus could perform calculations on the GPU.  Now, video card manufacturers are doing everything possible to simplify the lives of enthusiasts and developers who want to use the power of graphics processors in their projects: CUDA from NVIDIA, AMD FireStream, <a href="http://ru.wikipedia.org/wiki/OpenCL">OpenCL</a> standard.  Every year computing on the GPU is becoming more and more accessible. <br><br><h5>  Calculating hashes </h5><br>  Today, of the tasks launched on the GPU, the most popular is probably the calculation of hashes.  And all this because of Bitcoin mining, which, in fact, lies in the calculation of hashes.  Most bitcoin miners are available for Linux.  If you want to mine Bitcoins and if your graphics processor supports OpenCL (if it supports CUDA, then OpenCL also), then I recommend paying attention to <a href="https://github.com/luke-jr/bfgminer">bfgminer</a> : it is fast, convenient and functional, though not that simple to set up. <br><br><blockquote><h5>  Accelerate Snort with GPU </h5>  A very interesting concept called Gnort was developed by researchers from the Greek FORTH Institute (Foundation for Research and Technology - Hellas).  They offer to increase the efficiency of detecting attacks by Snort by porting the code responsible for checking regular expressions onto the GPU.  According to the graphs given in the official <a href="http://bit.ly/Gnort-pdf">PDF of the study</a> , they achieved an almost twofold increase in the throughput of Snort. </blockquote><br><br>  But not a single bitcoin live.  Nothing prevents the use of GPU power for brute force hashes (in order to find out your forgotten password, of course, no more).  In solving this problem, the <a href="http://hashcat.net/oclhashcat-plus/">oclHashcat-plus</a> utility, a true harvester with hashes, has proven itself well.  It can pick up MD5 hashes with salt and without salt, SHA-1, NTLM, cached domain passwords, MySQL database passwords, GRUB passwords, and this is not even half the list. <br><br><h5>  GPU encryption </h5><br>  Weibin Sun and Xing Lin students from the University of Utah in the framework of the <a href="https://code.google.com/p/kgpu/">KGPU</a> project presented us a very interesting application of graphics processor <a href="https://code.google.com/p/kgpu/">power</a> .  The essence of the project is to transfer the execution of some parts of the Linux kernel code to a CUDA-compatible GPU.  The first developers decided to put the AES algorithm on the GPU.  Unfortunately, the development of the project stopped at this, although the developers promised to continue the work.  But besides this, the existing experience can be used to speed up AES encryption in eCryptfs and dm-crypt, it‚Äôs a pity that the kernel version 3.0 and higher is not supported. <br><br><h5>  GPU Performance Monitoring </h5><br>  Why not?  Of course, the load of each GPU core cannot be found, but at least you can get some information about what is happening on the GPU.  The CUDA-Z program (almost analogous to the GPU-Z Windows program), in addition to various static information about the GPU, can also get dynamic: the current speed of data exchange between the graphics processor and the machine, as well as the overall performance of all GPU cores in flops. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c3/f95/e45/1c3f95e45208aaa3c632ba0b213dcebd.jpg" alt="CUDA-Z tab with GPU performance information"><br>  CUDA-Z tab with GPU performance information <br><br><br><h4>  findings </h4><br>  Multicore or multiprocessor workstations entered our everyday life quite a long time ago - it's time to change our single-threaded approach when working with them.  After all, the parallelization of tasks on such systems gives us a huge time gain, which we have already seen. <br><br><blockquote><h5>  useful links </h5><br><ul><li>  Tuning JVM to increase the performance of Java EE applications on a multi-core system: <br>  <a href="http://bit.ly/JavaTuning">bit.ly/JavaTuning</a> </li><li>  Measuring multi-core CPU performance with MPI tests: <a href="http://bit.ly/MPIbenchmark">bit.ly/MPIbenchmark</a> </li><li>  If you have nothing to occupy your computer, give your computing power to search for drugs, study global warming and other interesting scientific research: <a href="https://boinc.berkeley.edu/">https://boinc.berkeley.edu</a> </li></ul><br></blockquote><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ff1/0c0/0f0/ff10c00f06d672a2454dd89cf5543518.jpg"><br>  <i>First published in the Hacker magazine dated 10/2013.</i> <br><br>  <a href="http://issuu.com/xakep_magazine/docs/xa_10-2013-unixoid-glavnye-parallel">Publication on Issuu.com</a> <br><br>  Subscribe to "Hacker" <br><ul><li>  <a href="http://bit.ly/habr_subscribe_paper">Paper version</a> </li><li>  <a href="http://bit.ly/xakep_on_ipad">Hacker on iOS / iPad</a> </li><li>  <a href="http://bit.ly/habr_android">"Hacker" on Android</a> </li></ul><br><br> <a href="http://bit.ly/xakep_on_ipad"><img src="https://habrastorage.org/getpro/habr/post_images/6d3/e39/90b/6d3e3990b645b719835d84e9c02699ef.png"></a> <br><br> <a href="http://bit.ly/habr_android"><img src="https://habrastorage.org/getpro/habr/post_images/117/fc7/5d3/117fc75d3fe93b8601be69119c2ecd88.jpg"></a> </div><p>Source: <a href="https://habr.com/ru/post/210480/">https://habr.com/ru/post/210480/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../210464/index.html">Samsung and Google enter into patent agreements</a></li>
<li><a href="../210466/index.html">Dev Story: Raildale - a game about the railway for iOS and Mac</a></li>
<li><a href="../210468/index.html">Own chess (with tanks and helicopters)</a></li>
<li><a href="../210472/index.html">What to do if you got an Android with a broken touch screen</a></li>
<li><a href="../210478/index.html">How the infrastructure of Yandex. Mail has grown over 13 years</a></li>
<li><a href="../210482/index.html">Freelancer's view of management: project management and customer communication</a></li>
<li><a href="../210484/index.html">Chrome extension to soften high-profile news headlines</a></li>
<li><a href="../210486/index.html">The State Duma will approve the threshold of 150 euros for duty-free parcels from online stores?</a></li>
<li><a href="../210488/index.html">Duty-free in foreign online stores you can buy goods no more than 150 dollars / euro</a></li>
<li><a href="../210492/index.html">Found new Telegram application bug</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>