<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of graph compression algorithms</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This paper describes methods of compressing primarily social (graphs of connections between users on social networks) and Web graphs (graphs of links ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of graph compression algorithms</h1><div class="post__text post__text-html js-mediator-article">  This paper describes methods of compressing primarily social (graphs of connections between users on social networks) and Web graphs (graphs of links between sites). <br><br>  Most of the algorithms on graphs are well studied and designed on the basis of the fact that random access to graph elements is possible, at the moment the size of social graphs exceed the average RAM of the average machine in size, but at the same time easily fit on the hard disk.  A compromise option is to compress the data with the ability to quickly access certain queries.  We will focus on two: <br><br>  a) get a list of edges for a particular vertex <br>  b) find out if 2 vertices are connected. <br><a name="habracut"></a><br>  Modern algorithms allow compressing data up to 1-5 bits per link, which makes it possible to work with a similar database on an average machine.  I was inspired to write this article precisely the desire to fit the base of vkontakte friends in 32GB of RAM, given that now there are about 300M accounts with an average peak level of about 100, it seems quite realistic.  It is on this basis that I will conduct my experiments. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The general properties of the graphs of interest to us are: <br>  a) the fact that they are large, 16 - 19 or more tops and about 19 edges - which makes it impossible to work with them directly in memory, but allows you to store them even on one hard drive without any problems.  The site <a href="http://law.di.unimi.it/datasets.php">law.di.unimi.it/datasets.php</a> presents the entire vast range of such databases. <br>  b) the distribution of degrees (vertices) of vertices, and indeed, all important frequency characteristics are described by a power dependence (Power-Law) at least asymptotically. <br>  c) they are sparse.  The average degree of the vertex is ~ 100-1000. <br>  d) vertices are similar - the probability that you have a common friend is more for the connected vertices. <br><br><h3>  Boldi and Vigna. </h3><br>  In this paper, the graph in the "natural", uncompressed form will be represented by an array of lists: <br><br><pre><code class="hljs matlab">node1 -&gt; link11,link12,link13... node2 -&gt; link21,link22,link23.... ....  link[<span class="hljs-built_in"><span class="hljs-built_in">i</span></span>,<span class="hljs-built_in"><span class="hljs-built_in">j</span></span>] &lt; link[<span class="hljs-built_in"><span class="hljs-built_in">i</span></span>,<span class="hljs-built_in"><span class="hljs-built_in">j</span></span>+<span class="hljs-number"><span class="hljs-number">1</span></span>].</code> </pre> <br><br><table><tbody><tr><th>  Vertex </th><th>  Degree of vertex </th><th>  Ribs </th></tr><tr><td>  15 </td><td>  eleven </td><td>  13,15,16,17,18,19,23,24,203,315,1034 </td></tr><tr><td>  sixteen </td><td>  ten </td><td>  15,16,17,22,23,24,315,316,317,3041 </td></tr><tr><td>  17 </td><td>  0 </td><td></td></tr><tr><td>  18 </td><td>  five </td><td>  13,15,16,17,50 </td></tr></tbody></table><br>  <i>Table 1. Natural coding of the graph.</i>  <i>(Example taken from [2])</i> <br><br>  Let's call the difference between the indices of the two links ‚Äúgap‚Äù. <br><br><pre> <code class="hljs matlab">Gap[<span class="hljs-built_in"><span class="hljs-built_in">i</span></span>,<span class="hljs-built_in"><span class="hljs-built_in">j</span></span>] = link[<span class="hljs-built_in"><span class="hljs-built_in">i</span></span>,<span class="hljs-built_in"><span class="hljs-built_in">j</span></span><span class="hljs-number"><span class="hljs-number">-1</span></span>] - link[<span class="hljs-built_in"><span class="hljs-built_in">i</span></span>,<span class="hljs-built_in"><span class="hljs-built_in">j</span></span>]</code> </pre> <br><br>  The first of the families of compression algorithms are methods that use the following 2 basic approaches: <br>  a) Exploitation of locality of vertices.  Vertices more often refer to ‚Äúclose‚Äù peaks, rather than to ‚Äúfar‚Äù ones. <br>  b) Operation of vertex similarity.  ‚ÄúClose‚Äù vertices refer to the same vertices. <br><br>  In these two statements lies a paradox - the term ‚Äúproximity‚Äù of peaks can be described in the same way through similarity, then our statements will turn into a truism.  It is in clarifying the concept of "proximity" that is the main difference between the algorithms I have tried. <br><br>  The first work on this topic is apparently K. Randall [1].  It examined the web graph of Altavista still alive, and it was discovered that most of the links (80%) on the graph are local and have a large number of common links, and it was suggested to use the following reference coding: <br>  - the new vertex is encoded as a link to a ‚Äúsimilar‚Äù + list of additions and deletions, which in turn is encoded with gaps + nibble coding. <br>  The guys squeezed the altavist to 5 bits per link (I will try to bring the worst compression results in the works).  This approach has been greatly developed in the works of Paolo Boldi and Sebastiano Vigna. <br><br>  They proposed a more complex method of reference coding, presented in Table 2. Each vertex can have one reference to ‚Äúsimilar‚Äù, RefNr encodes the difference between the current vertex and it, if RefNr is zero, then the vertex has no reference.  Copy list is a bit-list that encodes which vertices must be picked up by reference - if the corresponding Copy list bit is 1, then this vertex also belongs to the encoded one.  Then Copylist is also encoded with RLE by a similar algorithm and turns into CopyBlocks: <br><pre> <code class="hljs erlang-repl"><span class="hljs-number"><span class="hljs-number">01110011010</span></span> -&gt; <span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre><br>  We write the length - 1 for each repeating block + value of the first block (1 or 0), and the zeros remaining at the end can be discarded. <br><br>  The rest of the vertices is converted into gaps, the intervals of zeros are coded separately (here a special property of web graphs is exploited, which will be discussed below) and the remaining gaps (Residuals) are recorded by one of the digitized codes ( <a href="http://en.wikipedia.org/wiki/Golomb_coding">Golomb</a> , <a href="http://en.wikipedia.org/wiki/Huffman_coding">Huffman</a> extremum). <br><br>  As can be seen from table 2, this encoding can be multi-level - the degree of this multi-level is one of the parameters of the encoder <i>L</i> , with a large <i>L,</i> the encoding speed and decoding decreases, but the degree of compression increases. <br><br><table><tbody><tr><th>  Vertex </th><th>  Degree of vertex </th><th>  Refnr </th><th>  number of copy-blocks </th><th>  Copy blocks </th><th>  Number of intervals </th><th>  intervals (relative left index) </th><th>  spacing lengths </th><th>  residue (Residual) </th></tr><tr><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td></tr><tr><td>  15 </td><td>  eleven </td><td>  0 </td><td></td><td></td><td>  2 </td><td>  0.2 </td><td>  3.0 </td><td>  5,189,11,718 </td></tr><tr><td>  sixteen </td><td>  ten </td><td>  one </td><td>  7 </td><td>  0,0,2,1,1,0,0 </td><td>  one </td><td>  600 </td><td>  0 </td><td>  12,3018 </td></tr><tr><td>  17 </td><td>  0 </td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>  18 </td><td>  five </td><td>  3 </td><td>  one </td><td>  four </td><td>  0 </td><td></td><td></td><td>  50 </td></tr><tr><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td><td>  ... </td></tr></tbody></table><br>  <i>Table 2. The method of reference coding proposed by Boldi and Vigna for data from Table 1. [2]</i> <br><br>  The distribution of Gaps in the crawler bases is also subject to Power-Law law. <br><br><img src="//habrastorage.org/files/d64/6b6/306/d646b6306b2e48259fda3aa23446fd87.png"><br>  <i>Figure 1. Distribution of "Gaps" in snapshot on .uk sites - 18.5M pages.</i> <br><br>  What allowed us to say "close" accounts are given to us from above in the form of ordered data from webcrawlers.  And then develop the work on the above two directions: encode the list of edges as a modification of one of the previous vertices (reference coding), store the list of edges as ‚ÄúGaps‚Äù (Gap [i, 0] = Node [i] - link [i, 0]) - and, using the frequency response identified above - to encode this list with one of the many integer codes.  I must say that they did well: 2-5 bit per link [4].  In [2] and [3], even simpler reference coding methods LM and SSL are proposed that encode the data in small blocks.  The result is superior or rather commensurate with the BV algorithm.  From myself I want to add that even a simple coding with gaps gives a comparable result on Web - bases and at the same time all methods using local "similarity" noticeably succumb to social bases. <br><br>  In social graphs, this effect does not seem to be observed ‚Äî in Figure 2, the distribution of gaps in different pieces of the vkontakte database is presented.  It is interesting that for the first million log / log accounts, the law for gaps is actually implemented.  But with the increase in the amount of data.  The distribution of gaps is becoming more and more "white." <br><br><table><tbody><tr><td><img src="//habrastorage.org/files/e8e/728/06d/e8e72806d47a475d9b4ec96bede416f9.png"><br></td><td><img src="//habrastorage.org/files/c79/588/f58/c79588f5854c4843aa8747a2131cdfe5.png"><br></td><td><img src="//habrastorage.org/files/259/396/9ab/2593969ab24e40c2aba963491ac20a0a.png"><br></td></tr><tr><td>  <i>Sample 50k.</i> </td><td>  <i>Sampling 100k.</i> </td><td>  <i>Sample 2M.</i> </td></tr></tbody></table><br>  <i>Figure 2. The distribution of gaps in the base of friends vkontakte.</i> <br><br><img src="//habrastorage.org/files/aec/9a6/a39/aec9a6a39cc4441899bbf6f0ba4fd695.png"><br>  <i>Figure 3. The adjacency matrix of the graph, vkontakte.</i> <br><br><img src="//habrastorage.org/files/e82/da2/2f2/e82da22f26aa497c86c2060f80ca82d2.png"><br>  <i>Figure 4. The adjacency matrix before and after clustering.</i>  <i>100k users.</i> <br><br>  In Figure 3, the adjacency matrix of the friend graph is presented, in a logarithmic form.  She also does not inspire much hope for this kind of compression.  The data looks much more interesting if in some way we pre-cluster our graph.  Figure 4 shows the adjacency matrix after the MCL ( <a href="http://micans.org/mcl/">Markov Cluster Algorithm</a> ) clustering pass.  The correspondence matrix becomes almost diagonal (the color map is logarithmic, so bright yellow means more connections between elements by several orders of magnitude) - and WebGraph and many other algorithms are already excellent for compressing such data.  (Asano [7] - currently being the best as far as I know from compression, but also the slowest in data access). <br><br>  But the problem is that MCL is, at worst, cubic in time and quadratic in memory (http://micans.org/mcl/man/mclfaq.html#howfast).  Everything in life is not so bad for symmetric graphs (which the social graph is almost like) - but it is still far from linearity.  Another big problem is that the graph does not fit in the memory and it is necessary to invent distributed clustering techniques - and this is a completely different story.  An interim solution to this problem was invented by Alberto Apostolico and Guido Drovandi [1] - re-numeration of the graph, simply passing through it search in width (BFS-search).  This way, it is guaranteed that some vertices that link to each other will have close indices.  In their work, they left GAP coding and replaced it with a rather complex model of reference coding, while receiving 1-3 bits per coding reference.  In fact, the intuition is not very clear that BFS should correctly compose vertices, but this method works - I did this coding for the VK database and looked at the histogram for gaps (Figure 5) - it looks very promising.  There are also suggestions to use Deep First Search instead of BFS, as well as more complex re-indexing schemes such as shingle reordering [7] - they give a similar increase.  There is one more reason why BFS re-indexing should / can work - WebArchive databases are well encoded, and they are obtained just by sequential indexing of the live Internet graph. <br><br><img src="//habrastorage.org/files/b8d/5f9/da8/b8d5f9da803d49b99fa8b9bd1a2ee4e4.png" width="50%"><br>  <i>Figure 5. The distribution of gaps in the base of friends vkontakte after BFS indexing.</i>  <i>100k sample</i> <br><br><table><tbody><tr><th>  Gap </th><th>  amount </th><th>  Share </th></tr><tr><td>  one </td><td>  83812263 </td><td>  7.69% </td></tr><tr><td>  2 </td><td>  12872795 </td><td>  1.18% </td></tr><tr><td>  3 </td><td>  10643810 </td><td>  0.98% </td></tr><tr><td>  four </td><td>  9097025 </td><td>  0.83% </td></tr><tr><td>  five </td><td>  7938058 </td><td>  0.73% </td></tr><tr><td>  6 </td><td>  7032620 </td><td>  0.65% </td></tr><tr><td>  7 </td><td>  6322451 </td><td>  0.58% </td></tr><tr><td>  eight </td><td>  5733866 </td><td>  0.53% </td></tr><tr><td>  9 </td><td>  5230160 </td><td>  0.48% </td></tr><tr><td>  ten </td><td>  4837242 </td><td>  0.44% </td></tr><tr><td>  top10 </td><td>  153520290 </td><td>  14.09% </td></tr></tbody></table><br>  <i>Table 2. The distribution of gaps in the base of friends vkontakte after BFS indexing.</i>  <i>1M sample</i> <br><br>  The second work of Boldi and Vigna [5] is devoted to the theoretical justification of coding a list of gaps with various digital codes as well as comparing them with Huffmanman coding as possible upper bound.  The basis is that the encoded value is distributed according <a href="http://en.wikipedia.org/wiki/Zipf%2527s_law">to Zipf's law</a> .  The upper limit of compression for various alpha parameters turned out to be 4-13 bits per link.  For the VKontakte database, this coding method gave 18 bits per link.  Which, of course, is not bad and allows you to fit the entire database in RAM, but very far from the terratic estimates given in the works.  Figure 5 shows a comparison of the gap distribution after BFS indexing, with the zipf distribution as close as possible to practical data (alpha = 1.15).  Figure 6 shows the adjacency matrix for the graph after BFS reindexing - it reflects the reason for poor compression well - the diagonal is well drawn but the overall noise is still very large compared to the clustered matrix.  These "bad" results are also confirmed by the work [6]. <br><br><img src="//habrastorage.org/files/dd1/250/157/dd12501575e14bdc95b7e2dcd4d701ca.png"><br>  <i>Figure 6. The adjacency matrix of the vkontakte base.</i>  <i>After BFS re-indexing.</i> <br><br><h3>  Bikliki </h3><br>  There is another method worthy of special attention - the exploitation of properties of the graph.  Namely, the allocation of biklik (Bipartite graph) and the generation of a virtual vertex connecting the 2 parts of the subgraph.  This allows you to reduce the length of the list of vertices for each side of the bichromatic graph.  This problem is generally NP-complete, but there are many good heuristics for finding dicotyledonous subgraphs.  This method was proposed in [9], as well as the BV algorithm gave rise to many others [10‚Äì11] and deserves more detailed consideration.  Figure 6 describes only its main idea. <br><table><tbody><tr><td><img src="//habrastorage.org/files/fad/626/016/fad6260169e44b7baa939348dbd5f09f.png"></td><td><img src="//habrastorage.org/files/8a1/0d5/cb5/8a10d5cb51d24b3f9a6231548b807a59.png"></td></tr><tr><td>  A1 -&gt; A2 B2 C2 D3 <br>  B1 -&gt; A2 B2 C2 D4 <br>  C1 -&gt; A2 B2 C2 D5 <br></td><td>  A1 -&gt; <b>V</b> D3 <br>  B1 -&gt; <b>V</b> D4 <br>  C1 -&gt; <b>V</b> D5 <br>  <b>V</b> -&gt; A2 B2 C2 <br></td></tr></tbody></table><br>  <i>Figure 6. Biclik coding.</i> <br><br><h4>  Literature </h4><br><ol><li>  A. Alberto and G. Drovandi: Graph compression by BFS. </li><li>  Sz.  Grabowski and W. Bieniecki: Tight and simple Web graph compression. </li><li>  Sz.  Grabowski and W. Bieniecki Merging adjacency lists for efficient Web graph compression </li><li>  P. Boldi and S. Vigna: The webgraph framework I: Compression techniques. </li><li>  P. Boldi and S. Vigna: The webgraph framework II: Codes for the World-Wide-Web.  WebGraphII.pdf </li><li>  P. Boldi and S. Vigna.  Permuting Web and Social Graphs. </li><li>  Flavio Chierichetti, Ravi Kumar.  On Compressing Social Networks. </li><li>  Y. Asano, Y. Miyawaki, and T. Nishizeki: Efficient compression of web graphs. </li><li>  Gregory Buehrer, Kumar Chellapilla.  A Scalable Pattern Mining Approach to Web Graph Compression with Communities </li><li>  Cecilia Hernandez, Gonzalo Navarro.  Compressed Representations for Web and Social Graphs. </li><li>  F. Claude and G. Navarro: Fast and compact web graph representations. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/246325/">https://habr.com/ru/post/246325/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../246309/index.html">Marketing wars (and here Habr)</a></li>
<li><a href="../246311/index.html">Nutanix large family</a></li>
<li><a href="../246313/index.html">Understanding paradigms for building domain models</a></li>
<li><a href="../246315/index.html">TeamCity 9.0: import projects, settings in VCS, background cleaning and pandas</a></li>
<li><a href="../246317/index.html">Processing NBA data for 30 years using MongoDB Aggregation</a></li>
<li><a href="../246329/index.html">New Year's draw</a></li>
<li><a href="../246331/index.html">Expressive JavaScript: Project: Experience Sharing Website</a></li>
<li><a href="../246339/index.html">Some interesting MySQL features</a></li>
<li><a href="../246341/index.html">IPv6, miredo, dynamic DNS AAAA</a></li>
<li><a href="../246349/index.html">Yeoman for newbies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>