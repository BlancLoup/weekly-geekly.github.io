<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of C ++ deep learning libraries Apache.SINGA, tiny-dnn, OpenNN</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Enjoying the creation of models in Python on wonderful Deep Learning frameworks like Keras or Lasagne , from time to time I want to see what interesti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of C ++ deep learning libraries Apache.SINGA, tiny-dnn, OpenNN</h1><div class="post__text post__text-html js-mediator-article">  Enjoying the creation of models in Python on wonderful Deep Learning frameworks like <a href="https://keras.io/">Keras</a> or <a href="http://lasagne.readthedocs.io/en/latest/">Lasagne</a> , from time to time I want to <a href="https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software">see</a> what interesting things have appeared for C ++ developers, in addition to mainstream TensorFlow and Caffe.  I decided to take a closer look at three representatives: <a href="https://github.com/tiny-dnn">tiny-dnn</a> , <a href="https://svn.apache.org/repos//infra/websites/production/singa/content/docs/overview.html">Apache.SINGA</a> and <a href="https://github.com/Artelnics/OpenNN">OpenNN</a> .  A brief description of the experience of installing, building and using under Windows you will find under the cut. <br><br><a name="habracut"></a><h2>  Model problem of binary classification of word strings </h2><br>  I made a comparison of C ++ deep learning libraries as part of experiments with different ways of representing words in a simplified task of building the so-called <a href="https://en.wikipedia.org/wiki/Language_model">language model</a> .  A detailed description of all options is beyond the scope of this publication, so I will formulate the problem briefly. <br><br>  There are n-grams (chains of words) of a pre-selected equal length, in this case 3 words.  If the n-gram is derived from the text corpus, then we assume that this is a valid combination of words and the model should produce the target value y = 1.  Please note that n-grams are extracted from the body without taking into account the syntactic structure of the sentence, so the 3-gram " <i>cat sat on</i> " will be correct, although the border on the right has passed between the preposition and its object.  If the N-gram is obtained by random replacement of one of the words and such a chain is not found in the body, then the target value y = 0 is expected from the model. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Invalid N-grams are generated during the analysis of the hull in the same quantity as the valid ones.  It turns out perfectly balanced dataset, which facilitates the task.  And the absence of the need for manual markup makes it easy to ‚Äúplay‚Äù with such a parameter as the number of records in the training set. <br><br>  Thus, the binary classification problem is solved.  For experimenting with C ++ libraries, I took the simplest version of the word representation through the word2vector combination of individual word representations.  <a href="https://radimrehurek.com/gensim/models/word2vec.html">The w2v model was</a> trained on a 70- <a href="https://radimrehurek.com/gensim/models/word2vec.html">gb</a> corpus of texts with a vector size of 32. As a result, each 3-gram is represented by a length 96 vector. The training set consists of 340,000 entries.  Validation and final assessment is performed on individual sets of approximately the same volume.  The generation of data files for C ++ models is done <a href="https://github.com/Koziev/WordRepresentations/blob/master/PyModels/store_dataset_file.py">by a python script</a> , so that all compared libraries and models are guaranteed to be trained and validated on the same data. <br><br><h2>  Solving the problem of binary classification in C ++ using tiny-dnn </h2><br>  Search on Habr√© for references to <a href="https://github.com/tiny-dnn/tiny-dnn">the tiny-dnn library</a> gives one link to the article: <a href="https://habrahabr.ru/post/319436/">habrahabr.ru/post/319436</a> <br><br>  Installing and connecting tiny-dnn to your project is extremely simple. <br><br>  1. Clone the contents of the <a href="https://github.com/tiny-dnn/tiny-dnn">repository</a> . <br>  2. Compilation of the library is not required, since everything is implemented in the header files. <br>  To use, it is enough to specify the directive #include "tiny_dnn / tiny_dnn.h" in your code. <br><br>  Ease of connection results in a significant increase in compile time.  On the other hand, there is such a thing as precompiled headers, and in this case it should help. <br><br>  Now let's see what can be achieved using tiny-dnn.  I will make a reservation in advance that I saw <br>  tiny-dnn the first time, so I admit that I missed some important functional <br>  opportunities that would improve results.  However, I will describe my <br>  user experience. <br><br>  The source code for the implementation of a simple feed forward grid (MLP) in C ++ and the corresponding <br>  The project for VS 2015 lies <a href="https://github.com/Koziev/WordRepresentations/tree/master/CXXModels/TinyDNN_Model">here</a> .  The construction of the grid consists in calling one function, which indicates the number of inputs, the size of the hidden layer and the number of outputs: <br><br><pre><code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">auto</span></span> nn = make_mlp&lt;sigmoid_layer&gt;({ input_size, num_hidden_units, output_size });</code> </pre> <br>  Then an object is created that performs optimization of the weights, a couple of callback functions to track the learning process, and the training starts: <br><br><pre> <code class="cpp hljs">gradient_descent optimizer; optimizer.alpha = <span class="hljs-number"><span class="hljs-number">0.1</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">auto</span></span> on_enumerate_epoch = [&amp;]() { <span class="hljs-comment"><span class="hljs-comment">/*...*/</span></span> }; <span class="hljs-keyword"><span class="hljs-keyword">auto</span></span> on_enumerate_data = [&amp;]() { <span class="hljs-comment"><span class="hljs-comment">/*...*/</span></span> }; nn.train&lt;mse&gt;(optimizer, X_train, y_train, batch_size, nb_epochs, on_enumerate_data, on_enumerate_epoch);</code> </pre><br>  It hurts the eye a little that the training and verification data transmitted to the training procedure is announced like this: <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;<span class="hljs-keyword"><span class="hljs-keyword">vec_t</span></span>&gt; X_train = ...; <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;<span class="hljs-keyword"><span class="hljs-keyword">label_t</span></span>&gt; y_train = ...;</code> </pre><br>  Wherein: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">typedef</span></span> <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;<span class="hljs-keyword"><span class="hljs-keyword">float_t</span></span>, aligned_allocator&lt;<span class="hljs-keyword"><span class="hljs-keyword">float_t</span></span>, 64&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">vec_t</span></span>;</code> </pre> <br>  A vector consisting of vectors as a rectangular matrix leaves an alarming sense of irregularity, at least in terms of performance. <br><br>  The constructed neural net, in which the activation of all layers is sigmoid, gives an accuracy of approximately 0.58.  This is much worse than the values ‚Äã‚Äãthat can be obtained in the model on Keras or Apache.SINGA.  I assume that by changing the output layer activation to softmax, activating the intermediate layers to relu, and playing with the optimizer settings, you can improve the result. <br><br>  Generally speaking, this library has all the basic architectural solutions needed for experiments with deep learning models.  In particular, there are <a href="">convolutional</a> layers, layers for <a href="">pooling</a> , <a href="">recurrent</a> layers, as well as regularizers - <a href="">dropout</a> and <a href="">batch normalization</a> .  The means for organizing more complex computational graphs are also visible, for example, a layer for <a href="">merging</a> . <br><br>  But still, the library requires adding functionality (perhaps it is, but I did not notice it) at least in terms of implementing early stopping, model checkpoint, since without such basic capabilities it is difficult to use it for serious tasks.  You can see that the on_enumerate_epoch callback function passed to the tiny_dnn :: network &lt;...&gt; :: train &lt;...&gt; method returns void.  It would be wise to make it a return bool.  Then the user code could transmit the exit signal from the learning cycle according to epochs and independently implement its criterion for an early stop, preventing the receipt of a retrained model. <br><br><h2>  Solution of the problem of binary classification in C ++ using Apache.SINGA </h2><br><h3>  Why do I like Apache.SINGA? </h3><br>  First, they have a nice <a href="">logo</a> .  Kitty-maskot - such a sweetness can not be a bad project! <br><br>  Secondly, viewing examples of creating grids in C ++ using this library <br>  ( <a href="">https://github.com/apache/incubator-singa/blob/master/examples/cifar10/alexnet.cc</a> ) demonstrates ideological proximity to the description of grid models, for example, in <a href="https://keras.io/getting-started/sequential-model-guide/">Keras</a> .  A thought slips through - yes, it's almost like that of people, in the sense of pythonists. <br><br><div class="spoiler">  <b class="spoiler_title">Spoiler</b> <div class="spoiler_text">  Well, not exactly like people, all the same, it's C ++ style. <br></div></div><br>  <a href="https://svn.apache.org/repos//infra/websites/production/singa/content/docs/overview.html">The documentation</a> positions SINGA as a library with support for distributed processing of large grid models and implementation of basic deep learning architectures, including convolutional and recurrent layers.  Support for calculations on GPU through CUDA and OpenCL is declared, and this is a very serious application for production with heavy models.  However, I only checked the CPU option, because under Windows I have no GPU, and under Ubuntu I‚Äôm not ready to repeat the entire build cycle (see below). <br><br>  On Habr√© there was <a href="https://habrahabr.ru/company/parallels/blog/317994/">one mention of Apache.SINGA a</a> year ago in the form of a couple of paragraphs, so a more detailed description with installation tips and examples, I think, will be useful to someone. <br><br>  We proceed to the installation and testing. <br><br><h3>  Installing Apache.SINGA from Windows sources </h3><br>  Here we do not have a python but a hardcore C ++, and even under Windows, so be prepared for pain and suffering.  To minimize stress, brew a cup of tea and take a relaxed pose for the coachman. <br><br>  Assembly documentation can be found <a href="https://singa.incubator.apache.org/en/docs/installation.html">here</a> , including a separate <a href="https://singa.incubator.apache.org/en/docs/installation.html">section on the assembly under Windows</a> . <br><br>  Now step by step. <br><br>  0. Toolkit for assembly. <br><br>  0.1 It takes CMake to generate studio sln ( <a href="https://cmake.org/download/">https://cmake.org/download/</a> ) <br>  0.2 Need a git client to download the source. <br>  0.3 I use VisualStudio 2015, how to compile all components with other versions of the studio I did not check. <br>  0.4 Apparently, you need Perl to build OpenBLAS, I put &lt;a href=" <a href="https://www.perl.org/get.html">https://www.perl.org/get.html</a> "&gt; ActivePerl and did not notice any problems. <br><br>  1. Install <a href="https://github.com/google/protobuf">Protobuf</a> . <br><br>  1.1 Download <a href="https://github.com/google/protobuf/releases">Protobuf sources for Windows</a> . <br><br>  1.2 Follow <a href="https://github.com/google/protobuf/tree/master/cmake">the assembly instructions</a> .  In particular, generating a sln file for VS 2015 x64 looks like this: <br><br><pre> <code class="dos hljs">cmake -G "Visual Studio <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">2015</span></span> Win64" ^ -DCMAKE_INSTALL_PREFIX=../../../install ^ -Dprotobuf_BUILD_SHARED_LIBS=ON ^ ../..</code> </pre><br>  Pay particular attention to the -Dprotobuf_BUILD_SHARED_LIBS = ON option.  This is not mentioned in <br>  assembly documentation for SINGA, but somewhere in its source code the use is nailed <br>  dll-dependencies.  A similar approach is needed for GLOG and CBLAS. <br><br>  1.3 Compile the desired configuration - Release or Debug.  I did this in VS using the sln file generated by cmake.  While compiling, listen to ambient, drink tea, read Habr. <br><br>  2. Installing <a href="http://www.openblas.net/">OpenBLAS</a> . <br><br>  2.1 Download the source code of the open-source somewhere: <br><br><pre> <code class="dos hljs">git clone https://github.com/xianyi/OpenBLAS.git</code> </pre> <br>  2.2 Preparing sln using CMake, I had the following command for VS2015 x64: <br><br><pre> <code class="dos hljs">cmake -G "Visual Studio <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">2015</span></span> Win64"</code> </pre> <br>  2.3 We open the generated OpenBLAS.sln in the studio, we start the assembly of the necessary configurations.  Drinking tea.  It compiles for a long time, you have to wait. <br><br>  3. Installing <a href="https://github.com/google/glog">glog</a> . <br><br>  In the SINGA source code, you can see the build version without using GLOG, with some simplified <br>  version of the logger, but under Windows that option does not meet. <br><br>  3.1 Download: <br><br><pre> <code class="dos hljs">git clone https://github.com/google/glog.git</code> </pre> <br>  3.2 Doing <br><br><pre> <code class="dos hljs"><span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> build &amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> build</code> </pre> <br>  Then <br><br><pre> <code class="dos hljs">cmake -G "Visual Studio <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">2015</span></span> Win64" -DBUILD_SHARED_LIBS=<span class="hljs-number"><span class="hljs-number">1</span></span> ..</code> </pre> <br>  About the meaning-DBUILD_SHARED_LIBS = 1 I mentioned earlier in the description of the assembly Protobuf. <br><br>  We do not pay attention to a bunch of "not found" in the console, and then run the studio: <br><br><pre> <code class="dos hljs">glog.sln</code> </pre> <br>  We select the necessary configuration (Release or Debug), we compile.  Drinking tea. <br><br>  3. Build SINGA <br><br>  3.1 We write out the paths to the libraries and leaders from the previous steps, we collect approximately <br>  such portion of commands for the console: <br><br><pre> <code class="dos hljs"><span class="hljs-built_in"><span class="hljs-built_in">set</span></span> CBLAS_H="e:\polygon\SINGA\cblas\OpenBLAS" <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> CBLAS_LIB="e:\polygon\SINGA\cblas\OpenBLAS\lib\RELEASE\libopenblas.lib" <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> PROTOBUF_H=e:\polygon\SINGA\protobuf\protobuf-<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>\src <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> PROTOBUF_LIB=e:\polygon\SINGA\protobuf\protobuf-<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>\cmake\build\solution\Release\ <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> PROTOC_EXE=e:\polygon\SINGA\protobuf\protobuf-<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>\cmake\build\solution\Release\protoc.exe <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> GLOG_H=e:\polygon\SINGA\glog\src\windows\ <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> GLOG_LIB=e:\polygon\SINGA\glog\build\Release\glog.lib ..</code> </pre> <br>  This stage is the most difficult, since no one can immediately guess which paths to <br>  header files must be specified.  I was able to guess from the 4th or 5th attempt, so it‚Äôs not <br>  throw is not halfway, everything will turn out. <br><br><pre> <code class="dos hljs">cmake -G "Visual Studio <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">2015</span></span> Win64" -DUSE_CUDA=OFF -DUSE_PYTHON=OFF ^ -DCBLAS_INCLUDE_DIR=<span class="hljs-variable"><span class="hljs-variable">%CBLAS_H%</span></span> ^ -DCBLAS_LIBRARIES=<span class="hljs-variable"><span class="hljs-variable">%CBLAS_LIB%</span></span> ^ -DProtobuf_INCLUDE_DIR=<span class="hljs-variable"><span class="hljs-variable">%PROTOBUF_H%</span></span> ^ -DProtobuf_LIBRARIES=<span class="hljs-variable"><span class="hljs-variable">%PROTOBUF_LIB%</span></span> ^ -DProtobuf_PROTOC_EXECUTABLE=<span class="hljs-variable"><span class="hljs-variable">%PROTOC_EXE%</span></span> ^ -DGLOG_INCLUDE_DIR=<span class="hljs-variable"><span class="hljs-variable">%GLOG_H%</span></span> ^ -DGLOG_LIBRARIES=<span class="hljs-variable"><span class="hljs-variable">%GLOG_LIB%</span></span> -DUSE_GLOG</code> </pre> <br>  Get studio singa.sln.  Run the studio. <br><br>  It will take another tiny modification of the project.  I did not find a way to specify the required define in the cmake startup parameters, so I simply indicated in the studio project in the C ++ / Preprocessor section: <br><br>  USE_GLOG <br>  PROTOBUF_USE_DLLS <br><br>  Begin compiling.  Tea is no longer fit, just relax. <br><br>  I was wrong several times with the paths to dependencies, so getting a normal sln for VS came about an hour later, but in general, there is nothing supercomplex here. <br><br>  As a result, the coveted file singa.lib appears in the folder build \ lib \ Release \ <br><br>  Colleague, if you get to this place, I congratulate you on the successful completion <br>  The first phase of the mission!  You most likely have assembled an inoperable singa.lib library, now I will explain how to check it and what to do next. <br><br><h3>  Check the performance of singa.lib </h3><br>  For a quick check, you need to build such a program: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"singa/utils/channel.h"</span></span></span><span class="hljs-meta"> int main(int argc, char **argv) { singa::InitChannel(nullptr); std::vector</span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;std::string&gt; rl = singa::GetRegisteredLayers(); //   rl return 0; }</span></span></span></span></code> </pre> <br>  If the singa :: GetRegisteredLayers () function returns an empty list, then the problem is present. <br>  and no neural network will continue to work. <br><br>  Using the debugger you can see the following picture.  The library implements several <br>  factory classes that return the string name of the "singacpp_dense" type <br>  object of the corresponding class.  To initialize the factory, the authors of the library <br>  wrote a macro <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> RegisterLayerClass(Name, SubLayer) \ static Registra</span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;Layer, SubLayer&gt; Name##SubLayer(#Name);</span></span></span></span></code> </pre><br>  Each class for a factory is created through a global static object declaration: <br><br><pre> <code class="cpp hljs">RegisterLayerClass(singa_dense, Dense); RegisterLayerClass(singacpp_dense, Dense); RegisterLayerClass(singacuda_dense, Dense); RegisterLayerClass(singacl_dense, Dense);</code> </pre><br>  For pitonists, sisharperov and other owners of normal reflection, this may sound crazy, but this is the old C ++. <br><br>  And here it is already necessary to guard.  C ++ generally implements the principle of maximum surprise (hi to Python): a design with indefinite behavior always behaves unexpectedly and shoots in the foot.  In particular, experience says that you <a href="https://stackoverflow.com/questions/9459980/c-global-variable-not-initialized-when-linked-through-static-libraries-but-ok">should avoid using static constructors in C ++ libraries</a> , since they may not work at all (in the static library), or be called not in the order that the developer expects, in general, cause many different outrages.  It is better to write a function that explicitly calls the right constructors in the right order and pull it in the client code than to deal with the unported nuances of the compilers for Windows and Linux. <br><br>  In general, I added a separate function to SINGA: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">namespace</span></span> singa { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">initialize_static_ctors</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ RegisterLayerClass(singa_dense, Dense); RegisterLayerClass(singacpp_dense, Dense); <span class="hljs-comment"><span class="hljs-comment">//RegisterLayerClass(singacuda_dense, Dense); //RegisterLayerClass(singacl_dense, Dense); RegisterLayerClass(singa_relu, Activation); RegisterLayerClass(singa_sigmoid, Activation); RegisterLayerClass(singa_tanh, Activation); RegisterLayerClass(singacpp_relu, Activation); //RegisterLayerClass(singacuda_relu, Activation); //RegisterLayerClass(singacl_relu, Activation); RegisterLayerClass(singacpp_sigmoid, Activation); //RegisterLayerClass(singacuda_sigmoid, Activation); //RegisterLayerClass(singacl_sigmoid, Activation); RegisterLayerClass(singacpp_tanh, Activation); //RegisterLayerClass(singacuda_tanh, Activation); //RegisterLayerClass(singacl_tanh, Activation); return; } }</span></span></code> </pre> <br>  With its help, everything worked as it should - the factories were initialized, the grid began to work. <br>  If someone falters and does not want to build SINGA binaries, I put the Win x64 compiled libraries into the <a href="https://github.com/Koziev/WordRepresentations/tree/master/Libs">repository</a> . <br><br><h3>  Implementing simple feed forward neural networks </h3><br>  A cursory analysis of the sources shows that Apache.SINGA supports <a href="">fully connected</a> (dense) <a href="">layers</a> , <a href="">convolutional layers</a> (convolution), <a href="">pooling</a> (pooling), variants of <a href="">recurrent architectures</a> - RNN, GRU, LSTM and bidirectional LSTM.  <a href="">Activation options</a> are the minimum set of tanh, sigmoid and relu.  In general, the entire gentleman's set of deep learning in stock. <br><br>  Immediately I warn those accustomed to the Piton service: in many cases the wrong <br>  the use of SINGA functionality leads to memory fault. <br><br>  Wrong choice of data type for tensor y?  Get a memory fault. <br><br>  Set 1 output instead of 2 on the final layer for binary classification?  Hold the memory fault. <br><br>  And so on.  By the way, it's good that memfault, because you can get on the wrong result, <br>  if you're lucky with the block size. <br><br>  In general, many things in SINGA must be felt, since it is not always possible to guess about them in advance.  As I said, the tensor for the target variable y for learning and validation must necessarily have the data type kInt.  An attempt to create it as well as the tensor X with the type kFloat32 will lead to a segmentation error, because in one section of the code the void * caste is made to an int *, and if there is actually a pointer to float, then we get an error. <br><br>  As a seed for my code, I took the <a href="">alexnet.cc</a> file.  In <a href="">my example of</a> creating a grid, you can see that the sigmoid activations are set on top of fully connected layers explicitly as separate layers: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> FeedForwardNet </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_net</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">size_t</span></span></span></span><span class="hljs-function"><span class="hljs-params"> input_size)</span></span></span><span class="hljs-function"> </span></span>{ FeedForwardNet net; Shape s{ input_size }; net.Add(GenHiddenDenseConf(<span class="hljs-string"><span class="hljs-string">"dense1"</span></span>, <span class="hljs-number"><span class="hljs-number">96</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>), &amp;s); net.Add(GenSigmoidConf(<span class="hljs-string"><span class="hljs-string">"dense1_a"</span></span>)); net.Add(GenOutputDenseConf(<span class="hljs-string"><span class="hljs-string">"dense_output"</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>)); net.Add(GenSigmoidConf(<span class="hljs-string"><span class="hljs-string">"dense2_a"</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> net; }</code> </pre> <br>  If, by virtue of the habit of working with <a href="https://keras.io/layers/core/">Keras,</a> to forget to indicate activation, expecting to get sigmoid, then there will be a surprise, which you can guess from a strange learning curve: a dense layer will be simply linear. <br><br>  As can be seen in the above example, the creation of a neural network is quite a familiar way.  First a container is created - an object of class FeedForwardNet.  In this container layers are added sequentially processing input data.  For the first layer, it is necessary to transfer the dimension - an object of the Shape class so that the library correctly configures the connections between the layers. <br><br>  Training a neural network, as usual, boils down to adjusting the weights and offsets to minimize the loss function.  The optimization process is organized by the optimizer (in part, there is also an Updater), which you specify when creating the model.  The library has the <a href="https://github.com/apache/incubator-singa/tree/master/src/model/optimizer">following options for optimizers</a> (derived classes from Optimizer): <br><br>  <a href="">SGD</a> <br>  <a href="">Nesterov</a> <br>  <a href="">AdaGrad</a> <br>  <a href="">Rmsprop</a> <br><br>  For the basic version of the stochastic gradient descent, you can set your own schedule <br>  on learning speed through callback function: look for reference in my experiment code <br>  SetLearningRateGenerator method.  Thus, we further create and configure the desired version of the optimizer: <br><br><pre> <code class="cpp hljs"> SGD sgd; OptimizerConf opt_conf; opt_conf.set_momentum(<span class="hljs-number"><span class="hljs-number">0.9</span></span>); sgd.Setup(opt_conf);</code> </pre><br>  Finally, we ‚Äúcompile‚Äù the grid, indicating the loss function and the metric.  This stage is also familiar to those who use DL libraries with python: <br><br><pre> <code class="cpp hljs"> SoftmaxCrossEntropy loss; Accuracy acc; net.Compile(<span class="hljs-literal"><span class="hljs-literal">true</span></span>, &amp;sgd, &amp;loss, &amp;acc);</code> </pre><br>  Everything is ready - we start training, indicating the tensors, the number of epochs and the size of the batch: <br><br><pre> <code class="cpp hljs"> <span class="hljs-keyword"><span class="hljs-keyword">size_t</span></span> batch_size = <span class="hljs-number"><span class="hljs-number">128</span></span>; net.Train(batch_size, num_epoch, train_x, train_y, val_x, val_y);</code> </pre><br>  In the source code, you can see some hint at the tools for creating more complex graphs of calculations than the simple sequential addition of layers, for example, <a href="">concate</a> and <a href="">merge</a> . <br><br>  There are also regularizers: <a href="">dropout</a> and <a href="">batch normalization</a> . <br><br>  In principle, all of the above is sufficient for evaluative experiments and acquaintance with this wonderful library.  If you look at the contents of the examples folder, then there are quite complex models written on a Python-style wrapper, such as <a href="https://github.com/apache/incubator-singa/tree/master/examples/char-rnn">Char-RNN</a> or an <a href="https://github.com/apache/incubator-singa/tree/master/examples/imagenet/googlenet">image categorizer based on GoogleNet</a> .  My experiment with binary classification, by the way, gives an estimate of the accuracy in the region of 0.745.  This is somewhat worse than 0.80 for <a href="https://github.com/Koziev/WordRepresentations/blob/master/PyModels/wr_keras.py">the neural version of Keras</a> with Theano backend, but not so much as to sound the alarm, especially since additional tuning of parameters and architecture can improve the model. <br><br><h4>  Subjective evaluation of Apache.SINGA </h4><br>  (+) All major grid architectures are presented, including convolutional and recurrent. <br>  (+) GPU support. <br>  (+) Calculations on float. <br>  (+) More or less familiar workflow in the design of the model, understandable terminology. <br>  (-) Difficult installation and compilation. <br>  (-) Not always intuitive requirements for the choice of parameters and options, manifested through memory faults. <br><br><h2>  Solution of the problem of binary classification in C ++ using OpenNN </h2><br>  This is the last library I tried in this experiment.  Description in the <a href="https://github.com/Artelnics/OpenNN">repository</a> positions the library as a high-performance implementation of neural networks.  I do not dare to say how achieved this goal, but the training of my neural network implementation started on one CPU core (Apache.SINGA, for example, sat on both available cores without additional kicks on my part), and the double accuracy is even more embarrassing. <br><br>  Nevertheless, I recommend installing and trying this library, since this is an extremely simple process. <br><br><h3>  Build opennn </h3><br>  The process is described in the <a href="http://www.opennn.net/documentation/building_opennn.html">assembly documentation</a> , nothing complicated there. <br><br>  1. Download: <br><br><pre> <code class="dos hljs">git clone https://github.com/Artelnics/OpenNN.git</code> </pre><br>  2. Create a sln file for VS 2015: <br><br><pre> <code class="dos hljs"><span class="hljs-built_in"><span class="hljs-built_in">mkdir</span></span> build &amp; <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> build cmake -G "Visual Studio <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">2015</span></span> Win64" ..</code> </pre><br>  3. Open the created OpenNN.sln file in Studio, compile it.  It takes 10-15 minutes.  In the subdirectory ... \ build \ opennn \ Debug (or Release) a static library file opennn.lib appears. <br><br>  At this assembly ends. <br><br><h3>  Using OpenNN </h3><br>  The main problem with this library is the threshold of entry due to unusual terminology and the general organization of work with the model.  Class names are often unintuitive, it is difficult to guess their purpose.  For example, what does the <a href="">LossIndex</a> class <a href="">do</a> ?  This thing obviously has <br>  relation to loss function, but why ‚ÄúIndex‚Äù? <br><br>  Or learning options.  When you start learning neural networks with default parameters <br>  In the console, you can see the message that quasi-Newtonian optimization is being performed - great, but I would like something like stochastic gradient descend.  Switching the optimizer is done like this: <br><br><pre> <code class="cpp hljs">training_strategy.set_main_type( OpenNN::TrainingStrategy::GRADIENT_DESCENT);</code> </pre><br>  At the same time, the corresponding class is independently engaged in adjusting the learning speed, and so tightly that I have not found a way to somehow influence this process, in particular, how to set the initial learning rate. <br><br>  Setting the maximum number of iterations and other parameters looks like this: <br><br><pre> <code class="dos hljs">TrainingStrategy training_strategy; // ... training_strategy.get_gradient_descent_pointer()-&gt;set_maximum_iterations_number(<span class="hljs-number"><span class="hljs-number">100</span></span>); training_strategy.get_gradient_descent_pointer()-&gt;set_maximum_time(<span class="hljs-number"><span class="hljs-number">3600</span></span>); training_strategy.get_gradient_descent_pointer()-&gt;set_display_period(<span class="hljs-number"><span class="hljs-number">1</span></span>);</code> </pre><br>  As you can see, the names of classes and methods are a bit cumbersome. <br><br>  Further, the data for training are presented as double.  It certainly imposes some <br>  overhead on copying tensors, and raises the question of whether a cast is done to float somewhere <br>  under the hood quietly if using a GPU. <br><br>  Another disadvantage is that I did not find any analogs of the sklearn method of the predict for the trained model, so that for the test set I could immediately get the vector of predictions, then to calculate my metric.  I had to write a loop, feed the grid on one test sample and then analyze a single result, updating the metric. <br><br>  There are pluses.  The library has its own class for dataset operations, with the straightforward name <a href="">DataSet</a> .  He is able to download data from a csv file, and even understands the header with column names.  On the other hand, this class is organized in such a way that it loads from the same file both input variables X and target values ‚Äã‚Äãof y.  For me it was a surprise, so I had to add a separate option to save the dataset in the Python script specifically for OpenNN. <br><br>  In general, writing my own neural net implementation to solve my classification problem <br>  using OpenNN took a little longer than SINGA or tiny-dnn.  How transparent <a href="">it turned out</a> - you can evaluate yourself. <br><br><h3>  OpenNN subjective assessment </h3><br>  (+) simple installation and compilation, no dependencies <br>  (+) there seems to be support for CUDA <br>  (¬±) its class for working with datasets DataSet can load from CSV and knows <br>  about the headers, but requires that both X and y be in the same file. <br>  (-) operations with double <br>  (-) extremely unusual terminology, non-intuitive class names, unusual workflow in the description of the model. <br><br><h2>  Continuing with other libraries </h2><br>  At a minimum, it is worth looking at the <a href="https://github.com/Microsoft/CNTK">maycrosoft CNTK</a> .  For this framework, in addition to the standard set of architectural elements of deep learning, the documentation <a href="https://github.com/Microsoft/CNTK/tree/master/Examples/ReinforcementLearning">mentions the implementation of reinforcement learning</a> - the technological foundation of the <a href="https://blog.openai.com/dota-2/">HYIP wave</a> in the last couple of years. </div><p>Source: <a href="https://habr.com/ru/post/335838/">https://habr.com/ru/post/335838/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335826/index.html">How hackers prepare attacks on banks</a></li>
<li><a href="../335828/index.html">Download photos from Instagram using the Vkontakte bot</a></li>
<li><a href="../335830/index.html">On the quality of requirements in IT projects, to be honest (from the standpoint of the development team). Part 1</a></li>
<li><a href="../335834/index.html">DDD in practice. Design a wish list</a></li>
<li><a href="../335836/index.html">How to write normal texts in English, not being a native speaker</a></li>
<li><a href="../335840/index.html">Random RPG damage distribution</a></li>
<li><a href="../335842/index.html">Everything is relative, or the implementation of one simple task in python and tcl</a></li>
<li><a href="../335844/index.html">Automate workflow with the new platform. Part 1. Office. Work with "Incoming Documents"</a></li>
<li><a href="../335846/index.html">Logs as part of the product. How GrayLog Impacted Quality</a></li>
<li><a href="../335848/index.html">MIPSfpga - practical experience</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>