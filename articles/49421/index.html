<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Are the booty guys kinky? writing a morphological analyzer in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Morphological analyzer for the Russian language - is it something abstruse? The program, which leads the word to the initial form, determines the case...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Are the booty guys kinky? writing a morphological analyzer in Python</h1><div class="post__text post__text-html js-mediator-article">  Morphological analyzer for the Russian language - is it something abstruse?  The program, which leads the word to the initial form, determines the case, finds the word forms - it is not clear how to approach?  But in fact, all is not so difficult.  In the article - as I wrote an analogue of mystem, lemmatizer and phpmorphy in Python, and what came of it. <br><a name="habracut"></a><br>  I‚Äôll say right away that a library for morphological analysis in Python has turned out, which you can use and modify according to the MIT license. <br><br><h3>  The first question - why all this? </h3><br>  Slowly I make one hobby project, there is a decision to write it in Python.  And for the project, a more or less high-quality morphological analyzer was vital.  The option with mystem did not fit because of the license and the lack of the ability to dig deeper, lemmatizer and phpmorphy I was slightly strained with my dictionaries not in Unicode, but for some reason I did not find a decent analog in python.  In general, there are many reasons, everything, to be honest, not very serious, in fact, I just wanted to.  I finally decided to invent a bicycle and write my own implementation. <br><br>  Based on the best practices from the site aot.ru.  Dictionaries (LGPL) for Russian and English, as well as ideas - from there.  There were described specific implementation algorithms, but in terms of the theory of automata.  I am familiar with the theory of automata (I even defended my diploma in formal grammars), but it seemed to me that one could do very well without it.  Therefore, the implementation is independent, which has both advantages and disadvantages. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To begin with, the library should, at a minimum, be able to: <br>  1. To reduce a word to a normal form (for example, in units, Ip for nouns) - for the word ‚ÄúPEOPLE‚Äù it must return ‚ÄúPERSON‚Äù <br>  2. Determine the shape of the word (or form).  For example, for the word ‚ÄúPEOPLE‚Äù she must somehow let know that this noun, in the plural, may end up in the genitive or accusative case. <br><br><h3>  We deal with dictionaries </h3><br>  Dictionaries from aot.ru contain the following information: <br>  1. word paradigms and specific rules of education; <br>  2. stress; <br>  3. user sessions; <br>  4. a set of prefixes (productive prefixes); <br>  5. lemmas (non-replaceable parts of a word, a basis); <br>  and there is also 6. grammatical information - in a separate file. <br><br>  From all this, we are interested in the rules of word formation, prefixes, lemmas and grammatical information. <br><br>  All words are formed on the same principle: <br>  <strong>[prefix] + [prefix] + [base] + [ending]</strong> <br><br>  <em>Prefixes</em> are any ‚Äúmega‚Äù, ‚Äúsuper‚Äù, etc.  A set of prefixes is stored simply in a list. <br><br>  <em>Prefixes</em> - refers to prefixes that are inherent in grammatical form, but not inherent in the unchangeable part of the word (‚Äúby‚Äù, ‚Äúna‚Äù).  For example, "nai" in the word "most beautiful", because  without superlatives will be "beautiful." <br><br>  <em>The rules for the formation of words</em> - this is what must be attributed to the front and back of the base, to get some form.  The dictionary stores pairs ‚Äúprefix - ending‚Äù, + ‚Äúnumber‚Äù of the record of grammatical information (which is stored separately). <br><br>  The rules for the formation of words are combined in <em>paradigms</em> .  For example, for any class of nouns it can be described how a word looks in all cases and kinds.  Knowing that the noun belongs to this class, we can correctly get any form of it.  Such a class is a paradigm.  The first in the paradigm is always the normal form of the word (to be honest, the empirical conclusion)) <br><br>  <em>Lemmas</em> are unchangeable parts of words.  The dictionary stores information about which lemma corresponds to which paradigms (which set of rules for the formation of grammatical forms of a word).  Several paradigms can correspond to one lemma. <br><br>  <em>Grammatical information</em> - just pairs ("number" of the record, gram. Information).  "Number" in quotes, because  These are 2 letters, it is simple from, but all different. <br><br>  The file with the dictionary is plain text, for each section, the number of lines in it is first indicated, and then the lines follow, the format is described, so writing the parser was not working. <br><br>  Having understood the structure of the dictionary, it is already easy to write the first version of the morphological analyzer. <br><br><h3>  We write morphological analyzer </h3><br>  In fact, we have the word, and it must be found among all reasonable combinations of the form <br>  <strong>&lt;prefix&gt; + &lt;prefix&gt; + &lt;lemma&gt; + &lt;ending&gt;</strong> and <br>  <strong>&lt;prefix&gt; + &lt;lemma&gt; + &lt;ending&gt;</strong> <br><br>  The matter is simplified by what turned out (as shown by a couple of lines on the python) that there are only 2 ‚Äúprefixes‚Äù in our language (and also in English, too) 2. And the prefixes in the dictionary are about 20 for the Russian language.  Therefore, you can search among the combinations <br>  <strong>&lt;prefix&gt; + &lt;lemma&gt; + &lt;ending&gt;</strong> , combining in mind the list of prefixes and prefixes, and then doing a small test. <br><br>  Let's sort out the prefix in our own way: if the word starts with one of the possible prefixes, then we (the prefix) discard it and try to morphologically analyze the remainder (recursively), and then simply assign the rejected prefix to the received forms. <br><br>  The result is that the task comes down to searching among the combinations. <br>  <strong>&lt;lemma&gt; + &lt;ending&gt;</strong> , which is even better.  We are looking for suitable lemmas, then we look to see if there are suitable endings for them *. <br><br>  Here I decided to greatly simplify my life, and use the standard Python associative array in which I put all the lemmas.  It turned out a lemmas type dictionary: {base -&gt; [rule_id]}, i.e.  the key is a lemma, and the value is a list of numbers of admissible paradigms.  And then we went, first we believe that the lemma is the first letter of the word, then that these are the 2 first letters, etc.  By the lemma we try to get a list of paradigms.  If received, then in each permissible paradigm we run according to all the rules and see if our word will work out, if we apply the rule.  It turns out - we add it to the list of found forms. <br><br>  * There was another option - to compile a dictionary of all possible words at once &lt;lemma&gt; + &lt;ending&gt;, the result was about 5 million words, not much, but I didn‚Äôt like the option at all. <br><br><h3>  We add the morphological analyzer </h3><br>  In fact - everything is ready, we wrote a morphological analyzer, with the exception of some details that took me 90% of the time) <br><br><h4>  Part One </h4>  If we recall the example, which was at the beginning, about "PEOPLE" - "MAN", then it becomes clear that there are words that have an unchangeable part missing.  And then where to look for forms of these words is not clear.  I tried to search among all endings (just like among lemmas), because for such words both ‚ÄúPEOPLE‚Äù and ‚ÄúMAN‚Äù in the dictionary will be stored as endings.  For some words it worked, for some it gave out, besides the correct result, also a bunch of garbage.  As a result, after long experiments, it turned out that there is such a cunning magic lemma "#" in the dictionary, which corresponds to all empty lemmas.  Hurray, the problem is solved, we look for it every time there. <br><br><h4>  Detail Two </h4>  It would be nice to have a "predictor" who could work with words that are not in the dictionary.  These are not only rare words unknown to science, but simply misinformation, for example. <br><br>  There are 2 simple, but quite working approaches: <br>  1. If words differ only in the fact that something is attributed to one of them in front, then, most likely, they will be inclined <br>  2. If 2 words end in the same way, then they will most likely be inclined equally. <br><br>  The first approach is guessing the prefix.  It is implemented very simply: we try to consider first one first letter of a word as a prefix, then 2 first letters, etc.  And what remains is passed to the morphological analyzer.  Well, we do it only for not very long prefixes and not very short residues. <br><br>  The second (prediction at the end of the word) approach is a little more difficult to implement (it is so much more difficult if a good implementation is needed)) and ‚Äúsmarter‚Äù in terms of predictions. <br>  The first difficulty is connected with the fact that the end of a word can consist not only of an end, but also of a part of the lemma.  Here I also decided to ‚Äúcut corners‚Äù and re-used the associative array with previously prepared all possible endings of words (up to 5 letters).  Not so many of them turned out, several hundred thousand.  The key of the array is the end of a word, the value is a list of possible rules.  Further, everything is as if searching for a suitable lemma, only we take the word not from the beginning, but from 1, 2, 3, 4, 5-letter ends, and now instead of the lemmas we now have a new monster array. <br>  The second difficulty - it turns out a lot of deliberate garbage.  This garbage is cut off, if we consider that the words obtained can only be nouns, adjectives, adverbs or verbs. <br>  Even after that, we have too many non-junk rules left.  For definiteness, for each part of speech we leave only the most common rule. <br>  In theory, if the word was not predicted as a noun, it would be good to add the variant with the unchangeable noun in units.  ip, but it is in TODO. <br><br>  The ideal text for testing the predictor's work is, of course, ‚ÄúSyapala Kalush by gunshot,‚Äù about how she rescued the boot-out there and what came of it: <br><br><blockquote>  Syal Kalush on the gun and uvazila butyavku.  And will: <br>  - Kalushata, kalushatochki!  Butyavka! <br>  Kalushata zaypalis and butyavku strung.  And podudonilis. <br>  And Kalush will: <br>  - Oee, oee!  Butyavka something nekuzyavaya! <br>  Kalushat Butyvka learned. <br>  Butyavka rattled, snitched away, and poured from her gun. <br>  And Kalush will: <br>  - Butyavok not twitch.  Butyavki Dybye and zyumo-zyumo nonkazyavye.  From butterflies dudonitsya. <br>  And the booty will come after the gun: <br>  - Kalushata podduonilis!  Kalushata podduonilis!  Zyumo nekuzyavye!  Puski btyaty! <br></blockquote><br>  From the text, the predictor failed to cope with Kalush‚Äôs own name, Kalush (they became Kalush and Kalush peasants), Oee, the mysterious ‚ÄúZyumo-Zyumo‚Äù interjection, and instead of ‚ÄúPusk‚Äù again gave the peasant ‚ÄúPusek ", The rest is, in my opinion, determined correctly.  In general, there is much to strive for, but not bad. <br><br>  About horror, "Habrahabr" is also predicted in any way.  But "Habrahabr" - already understands that "Habrahabr". <br><br>  Here it would be possible, in principle, to summarize if the computers worked instantly.  But it is not, so there is <br><br><h4>  Detail number 3: CPU and RAM resources </h4> The speed of the first version was fine with me.  It turned out, according to estimates, up to 10 words per second for simple Russian words, about a thousand for clever ones.  For English - even faster.  But there were 2 obvious (even before the start of development) ‚Äúbut‚Äù, connected with the fact that all data were loaded into RAM (via pickle / cPickle). <br>  1. the initial download took 3-4 seconds <br>  2. it was eaten about 150 megabytes of RAM with psyco and about 100 without (+ it was possible to reduce a little when Python set and list all sorts of there led to tulpe, where possible) <br><br>  Without hesitation, I did a little refactoring and brought all the data into a separate entity.  And then I came to the aid of the magic of Python and duck typing.  In short, the algorithms used data in the form of associative and simple arrays.  And everything will work without rewriting the algorithms, if instead of ‚Äúreal‚Äù arrays you slip something that behaves like an array, and more specifically, for our task, it supports [] and in.  All) In the standard python delivery, such ‚Äúmassive‚Äù interfaces to several non-relational databases were found.  These bases (bsddb, ndbm, gdbm), in fact, are associative arrays on the disk.  Just what you need.  There was also a couple of high-level superstructures over this economy (anydbm and shelve).  I stopped at inheriting from DbfilenameShelf and added support for keys in unicode and keys-integers.  And, he added caching of the result, which for some reason is in Shelf, but killed in DbfilenameShelf. <br><br>  Speed ‚Äã‚Äãdata on test texts (995 words, Russian dictionary, macbook): <br>  get_pickle_morph ('en', predict_by_prefix = False): 0.214 CPU seconds <br>  get_pickle_morph ('en'): 0.262 CPU seconds <br>  get_shelve_morph ('en', predict_by_prefix = False): 0.726 CPU seconds <br>  get_shelve_morph ('en'): 0.874 CPU seconds <br><br>  Memory option shelve, one might say, did not eat at all. <br><br>  The shelve options seemed to work when dictionaries were already in disk cache.  Without this, the time may be 20 seconds with the predictor turned on.  I also noticed that the prefix prediction is the slowest to work: before screwing caching to my heirs from DbfilenameShelf, this prediction was inhibited 5 times more than everything else taken together.  And in these tests it seems not much already. <br><br>  By the way, taking this opportunity, I want to ask if suddenly someone knows how in the python you can find out the amount of memory occupied by the current process.  If possible, cross-platform somehow.  And then put the delay in the end of the script and measure according to the list of processes. <br><br><h3>  Usage example </h3><br> <code>import pymorphy <br> morph = pymorphy.get_shelve_morph('ru') <br> #       <br> info = morph.get_graminfo(unicode('').upper()) <br></code> <br><br><h3>  So after all, why? </h3><br>  At the very beginning I already explained why I began to write a morphological analyzer. <br>  Now it's time to explain why I started writing this article.  I wrote it in the hope that such a library is interesting and useful, and together we will improve this library.  The fact is that the functionality that has already been implemented is quite sufficient for my needs.  I will support and correct it, but not very actively.  Therefore, this article is not a detailed instruction manual, but an attempt to describe how everything works. <br><br><h3>  In delivery </h3><br>  pymorphy.py - the library itself <br>  shelve_addons.py - heirs from the shelf, who can come in handy <br>  encode_dicts.py is a utility that converts dictionaries from AOT format to pymorphy formats.  Without parameters, it works for a long time, eats 200 meters of memory, starts 1 time.  Converted dictionaries do not distribute because of possible binary incompatibility and large volume. <br>  test.py - unit tests for pymorphy <br>  example.py is a small example and text with those 995 words <br>  dicts / src - folder with source dictionaries downloaded from aot.ru <br>  dicts / converted - the folder where encode_dicts.py will add converted dictionaries <br><br><h3>  At last </h3><br>  link: <br><br>  <a href="http://bitbucket.org/kmike/pymorphy/">bitbucket.org/kmike/pymorphy</a> <br><br>  License - MIT. <br><br>  checked only under Python 2.5. <br><br>  Comments, suggestions, questions on the case - are welcome.  If interested - connect to the development. </div><p>Source: <a href="https://habr.com/ru/post/49421/">https://habr.com/ru/post/49421/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../49407/index.html">Personality 2.0. Part One: Putting up a profile. Interview with the creator of retaggr.com.</a></li>
<li><a href="../49408/index.html">100,000 articles in Knol</a></li>
<li><a href="../49411/index.html">We learn Java. First cup</a></li>
<li><a href="../49414/index.html">Ideal editor or friend of Notepad ++ with SFTP</a></li>
<li><a href="../49418/index.html">Let's beat Ruby together! Drop ten</a></li>
<li><a href="../49428/index.html">We master Python. Ounce Zero. Introduction</a></li>
<li><a href="../49431/index.html">As we were not taken in Yandex.Catalog on a paid basis</a></li>
<li><a href="../49432/index.html">.NET StockTrader Sample Application</a></li>
<li><a href="../49433/index.html">OpenDesktop.org introduces the possibility of selling content.</a></li>
<li><a href="../49434/index.html">Optimization of the process of creating unit tests</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>