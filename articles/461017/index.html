<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>New GPU Tracking Algorithm: Wavefront Path Tracing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, we explore the important concept used in the recently released Lighthouse 2 platform. Wavefront path tracing , as it is called Lane, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>New GPU Tracking Algorithm: Wavefront Path Tracing</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/_x/t8/rw/_xt8rwehj6jymumaisqg5ehgkro.png"></div><br>  In this article, we explore the important concept used in the recently released Lighthouse 2 platform. <a href="https://research.nvidia.com/publication/megakernels-considered-harmful-wavefront-path-tracing-gpus">Wavefront path tracing</a> , as it is called Lane, Karras and Aila from NVIDIA, or streaming path tracing, as it was originally called in Van Antwerp <a href="http://resolver.tudelft.nl/uuid:4a5be464-dc52-4bd0-9ede-faefdaff8be6">‚Äôs master's thesis</a> , plays a crucial role in developing efficient path tracers on the GPU, and potentially path tracers on the CPU.  However, it is quite counterintuitive, therefore, to understand it, it is necessary to rethink ray tracing algorithms. <br><a name="habracut"></a><br><h2>  Occupancy </h2><br>  The path tracing algorithm is surprisingly simple and can be described in just a few lines of pseudocode: <br><br><pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-function">vec3 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Trace</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( vec3 O, vec3 D )</span></span></span><span class="hljs-function"> IntersectionData i </span></span>= Scene::Intersect( O, D ) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i == NoHit) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> vec3( <span class="hljs-number"><span class="hljs-number">0</span></span> ) <span class="hljs-comment"><span class="hljs-comment">// ray left the scene if (i == Light) return i.material.color // lights do not reflect vec3 R, pdf = RandomDirectionOnHemisphere( i.normal ), 1 / 2PI return Trace( i.position, R ) * i.BRDF * dot( i.normal, R ) / pdf</span></span></code> </pre> <br>  The input is the <em>primary ray</em> passing from the camera through the screen pixel.  For this beam, we determine the closest intersection with the scene primitive.  If there are no intersections, then the beam disappears into the void.  Otherwise, if the beam reaches the light source, then we found the light path between the source and the camera.  If we find something else, then we perform reflection and recursion, hoping that the reflected beam will still find the source of illumination.  Note that this process resembles the (return) path of a photon reflecting off the surface of a scene. <br><br>  GPUs are designed to perform this task in multi-threaded mode.  At first it might seem that ray tracing is ideal for this.  So, we use OpenCL or CUDA to create a stream for a pixel, each stream performs an algorithm that actually works as intended, and is pretty fast: just look at a few examples with ShaderToy to understand <a href="https://www.shadertoy.com/view/4sfGDB">how</a> <a href="https://www.shadertoy.com/view/4sXGDs">fast</a> <a href="https://www.shadertoy.com/view/llXSWr">ray tracing</a> <a href="https://www.shadertoy.com/view/4ljGRd">can</a> <a href="https://www.shadertoy.com/view/XdB3Dw">be</a> on the GPU.  But be that as it may, the question is different: are these ray tracers really <em>as fast as possible</em> ? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ce5/61d/07d/ce561d07daa3437927ab8ad5a6744ec9.jpg"></div><br>  This algorithm has a problem.  The primary ray can find the light source immediately, or after one random reflection, or after fifty reflections.  The programmer for the CPU will notice a potential stack overflow here;  the GPU programmer should see <em>the occupancy problem</em> .  The problem is caused by conditional tail recursion: the path may end at the light source or continue on.  Let's transfer this to many threads: some of the threads will stop, and the other part will continue to work.  After a few reflections, we will have several threads that need to continue computing, and most threads will wait for these last threads to finish working.  <em>Employment</em> is a measure of the portion of GPU threads that do useful work. <br><br>  The employment problem applies to the execution model of SIMT GPU devices.  Streams are organized into groups, for example, in Pascal GPU (NVidia equipment class 10xx) 32 threads are combined into a <em>warp</em> .  Threads in warp have a common program counter: they are executed with a fixed step, so each program instruction is executed by 32 threads simultaneously.  SIMT stands for <em>single instruction multiple thread</em> , which describes the concept well.  For a SIMT processor, a code with conditions is complex.  This is clearly shown in the official Volta documentation: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9bb/1c0/cb0/9bb1c0cb0c4916e8a7989edeb466d3dd.jpg"></div><br>  <i>Code execution with conditions in SIMT.</i> <br><br>  When a certain condition is true for some threads in warp, the branches of the <em>if-statement are</em> serialized.  An alternative to the ‚Äúall threads do the same‚Äù approach is ‚Äúsome threads are disabled‚Äù.  In the if-then-else block, the average occupation of warp will be 50%, unless all threads have consistency regarding the condition. <br><br>  Unfortunately, code with conditions in the ray tracer is not so rare.  Rays of shadows are emitted only if the light source is not behind the shading point, different paths may collide with different materials, integration with the Russian roulette method can destroy or leave the path alive, and so on.  It turns out that occupancy is becoming the main source of inefficiency, and it is not so easy to prevent it without emergency measures. <br><br><h2>  Streaming Path Tracing </h2><br>  The streaming path tracing algorithm is designed to address the root cause of the busy problem.  Streaming path tracing divides the path tracing algorithm into four steps: <br><br><ol><li>  <strong>Generate</strong> </li><li>  <strong>Extend</strong> </li><li>  <strong>Shade</strong> </li><li>  <strong>Connect</strong> </li></ol><br>  Each stage is implemented as a separate program.  Therefore, instead of executing a full path tracer as a single GPU program (‚Äúkernel‚Äù, kernel), we will have to work with <em>four</em> cores.  In addition, as we will soon see, they are executed in a loop. <br><br>  <b>Stage 1 (‚ÄúGenerate‚Äù)</b> is responsible for the generation of primary rays.  This is a simple core that creates the starting points and directions of the rays in an amount equal to the number of pixels.  The output of this stage is a large ray buffer and a counter informing the next stage of the number of rays that need to be processed.  For primary rays, this value is equal to the <em>width of the screen</em> times the <em>height of the screen</em> . <br><br>  <strong>Stage 2 (‚ÄúRenew‚Äù)</strong> is the second core.  It is executed only after stage 1 is completed for all pixels.  The kernel reads the buffer generated in step 1 and crosses each ray with the scene.  The output of this stage is the intersection result for each ray stored in the buffer. <br><br>  <strong>Stage 3 (‚ÄúShade‚Äù)</strong> is performed after the completion of stage 2. It receives the result of the intersection from stage 2 and calculates the shading model for each path.  This operation may or may not generate new rays, depending on whether the path has completed.  The paths that generate the new ray (the path ‚Äúextends‚Äù) writes the new ray (the ‚Äúpath segment‚Äù) to the buffer.  Paths that directly sample light sources (‚Äúexplicitly sample lighting‚Äù or ‚Äúcalculate the next event‚Äù) write a shadow beam to a second buffer. <br><br>  <strong>Stage 4 (‚ÄúConnect‚Äù)</strong> traces the shadow rays generated in stage 3. This is similar to stage 2, but with an important difference: the rays of the shadow need to find <em>any</em> intersection, while the extending rays need to find the nearest intersection.  Therefore, a separate core has been created for this. <br><br>  After completing step 4, we get a buffer containing rays that extend the path.  Having taken these rays, we proceed to stage 2. We continue to do this until there are no extension rays or until we reach the maximum number of iterations. <br><br><h2>  Sources of Inefficiency </h2><br>  A programmer concerned about performance will see a lot of dangerous moments in such a scheme of streaming path tracing algorithms: <br><br><ul><li>  Instead of a single kernel call, we now have <em>three calls per iteration</em> , plus a generation kernel.  Challenges of the cores mean a certain increase in load, so this is bad. </li><li>  Each core reads a huge buffer and writes a huge buffer. </li><li>  The CPU needs to know how many threads to generate for each core, so the GPU should tell the CPU how many rays were generated in step 3. Moving information from the GPU to the CPU is a bad idea, and it needs to be done at least once per iteration. </li><li>  How does stage 3 write the rays to the buffer without creating spaces everywhere?  He doesn‚Äôt use an atomic counter for this? </li><li>  The number of active paths is still decreasing, so how can this scheme help at all? </li></ul><br>  Let's start with the last question: if we transfer a million tasks to the GPU, it will not generate a million threads.  The true number of threads executed simultaneously depends on the equipment, but in the general case, tens of thousands of threads are executed.  Only when the load falls below this number will we notice employment problems caused by a small number of tasks. <br><br>  Another concern is the large-scale I / O of buffers.  This is indeed a difficulty, but not as serious as you might expect: data access is highly predictable, especially when writing to buffers, so the delay does not cause problems.  In fact, GPUs were primarily developed for this type of data processing. <br><br>  Another aspect that GPUs handle very well is atomic counters, which is quite unexpected for programmers working in the CPU world.  The z-buffer requires quick access, and therefore the implementation of atomic counters in modern GPUs is extremely effective.  In practice, an atomic write operation is as costly as an uncached write to global memory.  In many cases, the delay will be masked by large-scale parallel execution in the GPU. <br><br>  Two questions remain: kernel calls and two-way data transfer for counters.  The latter is actually a problem, so we need another architectural change: <em>persistent threads</em> . <br><br><h2>  Effects </h2><br>  Before delving into the details, we will look at the implications of using the wavefront path tracing algorithm.  First, let's say about buffers.  We need a buffer to output the data of stage 1, i.e.  primary rays.  For each beam we need: <br><br><ul><li>  Ray origin: three float values, i.e. 12 bytes </li><li>  Ray direction: three float values, i.e. 12 bytes </li></ul><br>  In practice, it is better to increase the size of the buffer.  If you store 16 bytes for the beginning and direction of the beam, the GPU will be able to read them in one 128-bit read operation.  An alternative is a 64-bit read operation followed by a 32-bit operation to get float3, which is almost twice as slow.  That is, for a screen of 1920 √ó 1080 we get: 1920x1080x32 = ~ 64 MB.  We also need a buffer for the intersection results created by the Extend kernel.  This is another 128 bits per element, that is 32 MB.  Further, the ‚ÄúShadow‚Äù kernel can create up to 1920 √ó 1080 path extensions (upper limit), and we cannot write them to the buffer from which we read.  That is another 64 MB.  And finally, if our path tracer emits shadow rays, then this is another 64 MB buffer.  Having summed everything up, we get 224 MB of data, and this is only for the wavefront algorithm.  Or about 1 GB in 4K resolution. <br><br>  Here we need to get used to another feature: we have plenty of memory.  It could seem.  that 1 GB is a lot, and there are ways to reduce this number, but if you approach this realistically, then by the time we really need to trace the paths in 4K, using 1 GB on an 8 GB GPU will be the lesser of our problems. <br><br>  More serious than the memory requirements will be the consequences for the rendering algorithm.  So far, I have suggested that we need to generate one extension ray and, possibly, one shadow ray for each thread in the Shadow core.  But what if we want to perform Ambient Occlusion using 16 rays per pixel?  16 AO rays need to be stored in the buffer, but, even worse, they will appear only in the next iteration.  A similar problem arises when tracing rays in the Whited style: emitting a shadow beam for several light sources or splitting a beam in a collision with glass is almost impossible to realize. <br><br>  Wavefront path tracing, on the other hand, solves the problems that we have listed in the Occupancy section: <br><br><ul><li>  At stage 1, all flows without conditions create primary rays and write them to the buffer. </li><li>  At stage 2, all flows without conditions intersect the rays with the scene and write the results of the intersection to the buffer. </li><li>  In step 3, we begin calculating the intersection results with a 100% occupancy. </li><li>  In step 4, we process a continuous list of shadow rays with no spaces. </li></ul><br>  By the time we return to stage 2 with the surviving rays with a length of 2 segments, we again have a compact ray buffer, which guarantees full employment when the kernel starts. <br><br>  In addition, there is an additional advantage that should not be underestimated.  The code is isolated in four separate steps.  Each core can use all available GPU resources (cache, shared memory, registers) without taking into account other cores.  This may allow the GPU to execute intersection code with the scene in more threads, because this code does not require as many registers as the shader code.  The more threads, the better you can hide the delays. <br><br>  Full-time, enhanced delay masking, streaming recording: all these benefits are directly related to the emergence and nature of the GPU platform.  For the GPU, the wavefront path tracing algorithm is very natural. <br><br><h2>  Is it worth it? </h2><br>  Of course, we have a question: does optimized employment justify I / O from buffers and the cost of invoking additional cores? <br><br>  The answer is yes, but proving this is not so easy. <br><br>  If we return to the path tracers with ShaderToy for a second, we will see that most of them use a simple and hard-coded scene.  Replacing it with a full-blown scene is not a trivial task: for millions of primitives, the intersection of the beam and the scene becomes a complex problem, the solution of which is often left for NVidia ( <a href="https://developer.nvidia.com/optix">Optix</a> ), AMD ( <a href="https://gpuopen.com/gaming-product/radeon-rays/">Radeon-Rays</a> ) or Intel ( <a href="https://www.embree.org/">Embree</a> ).  None of these options can easily replace a hard-coded scene in the CUDA artificial ray tracer.  In CUDA, the closest analogue (Optix) requires control over program execution.  Embree in the CPU allows you to trace individual beams from your own code, but this costs significant performance overhead: he prefers to trace large groups of beams instead of individual beams. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fb4/2a2/409/fb42a240924ba04871abb70421d16cdf.png"></div><br>  <i>Screen from It's About Time rendered with Brigade 1.</i> <br><br>  Will wavefront path tracing be faster than its alternative (the megakernel, as Lane and colleagues call it), depends on the time spent in the cores (large scenes and costly shaders reduce the relative excess of costs by the wavefront algorithm), on the maximum path length , mega-core employment and differences in the load on the registers in four stages.  In an early version of the original <a href="https://www.researchgate.net/publication/258395638_The_Brigade_Renderer_A_Path_Tracer_for_Real-Time_Games">Brigade Path Tracer,</a> we found that even a simple scene with a mix of reflective and Lambert surfaces running on the GTX480 benefited from using wavefront. <br><br><h2>  Streaming Path Tracing in Lighthouse 2 </h2><br>  The Lighthouse 2 platform has two wavefront path tracing tracers.  The first one uses Optix Prime for the implementation of stages 2 and 4 (stages of the intersection of rays and the scene);  the second uses Optix directly to implement that functionality. <br><br>  Optix Prime is a simplified version of Optix that only deals with the intersection of a set of rays with a scene consisting of triangles.  Unlike the full Optix library, it does not support custom intersection code, and only intersects triangles.  However, this is exactly what is required for the wavefront path tracer. <br><br>  Optix Prime based wavefront path tracer is implemented in <code>rendercore.cpp</code> project.  The initialization of Optix Prime starts in the <code>Init</code> function and uses <code>rtpContextCreate</code> .  The scene is created using <code>rtpModelCreate</code> .  Various ray buffers are created in the <code>SetTarget</code> function using <code>rtpBufferDescCreate</code> .  Note that for these buffers we provide the usual device pointers: this means that they can be used both in Optix and in regular CUDA cores. <br><br>  Rendering begins in the <code>Render</code> method.  To fill the primary ray buffer, a CUDA core called <code>generateEyeRays</code> .  After filling the buffer, Optix Prime is called using <code>rtpQueryExecute</code> .  With it, intersection results are written to <code>extensionHitBuffer</code> .  Note that all buffers remain in the GPU: with the exception of kernel calls, there is no traffic between the CPU and GPU.  The Shadow phase is implemented in the regular CUDA <code>shade</code> core.  Its implementation is in <code>pathtracer.cu</code> . <br><br>  Some implementation details for <code>optixprime_b</code> are worth mentioning.  First, shadow rays are traced outside the wavefront cycle.  This is correct: a shadow ray affects a pixel only if it is not blocked, but in all other cases its result is not needed anywhere else.  That is, the shadow beam is <em>disposable</em> , it can be traced at any time and in any order.  In our case, we use this by grouping the rays of the shadow so that the finally traced batch is as large as possible.  This has one unpleasant consequence: with <em>N</em> iterations of the wavefront algorithm and <em>X</em> primary rays, the upper limit of the number of shadow rays is equal to <em>XN</em> . <br><br>  Another detail is the processing of various counters.  The stages ‚ÄúRenew‚Äù and ‚ÄúShadow‚Äù should know how many paths are active.  Counters for this are updated in the GPU (atomically), which means that they are used in the GPU, even without returning to the CPU.  Unfortunately, in one of the cases this is impossible: the Optix Prime library needs to know the number of rays traced.  To do this, we need to return the information of the counters once an iteration. <br><br><h2>  Conclusion </h2><br>  This article explains what wavefront path tracing is and why it is necessary to effectively perform path tracing on the GPU.  Its practical implementation is presented in the Lighthouse 2 platform, which is open source and <a href="">available on Github</a> . </div><p>Source: <a href="https://habr.com/ru/post/461017/">https://habr.com/ru/post/461017/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461003/index.html">Analysis: how the high-frequency trading market on the exchange works</a></li>
<li><a href="../461005/index.html">Testing for the company: asking the right questions at the interview</a></li>
<li><a href="../46101/index.html">Console Python Tricks: Team History + Autocompletion</a></li>
<li><a href="../461013/index.html">Reserving internet connection</a></li>
<li><a href="../461015/index.html">Live and learn. Part 2. University: 5 years or 5 corridors?</a></li>
<li><a href="../461019/index.html">How is life for developers in Iran</a></li>
<li><a href="../461021/index.html">Comet Lake - picture is clearing up</a></li>
<li><a href="../461027/index.html">Java REPL you do not ScriptEngine</a></li>
<li><a href="../461029/index.html">A lake of marketing data - from monstrous tables to reports and visualizations</a></li>
<li><a href="../461031/index.html">We connect online maps to the navigator on the smartphone. Part 1 - standard raster maps</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>