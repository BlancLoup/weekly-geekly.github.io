<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DMP Part 1. Microsegmentation of the audience using keywords</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Authors of the article: Danila Perepechin Danila Perepechin, Dmitry Cheklov dcheklov . 

 Hello. 
 Data management platform (DMP) is our favorite topi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DMP Part 1. Microsegmentation of the audience using keywords</h1><div class="post__text post__text-html js-mediator-article">  Authors of the article: Danila Perepechin Danila Perepechin, Dmitry Cheklov <a href="http://habrahabr.ru/users/dcheklov/" class="user_link">dcheklov</a> . <br><br>  Hello. <br>  Data management platform (DMP) is our favorite topic in the whole story about online advertising.  <i>RTB is all about the data</i> . <br>  In the course of the series of stories about the technological stack <a href="http://targetix.net/">Targetix</a> ( <a href="http://habrahabr.ru/company/targetix/blog/264015/">SSP</a> , <a href="http://habrahabr.ru/company/targetix/blog/261745/">DSP</a> ), today I will describe one of the tools included <br>  in DMP - <b>Keyword Builder</b> . <br><br><img src="https://habrastorage.org/files/46e/056/da1/46e056da1b8f488eb7b9cd917bc9097a.jpg"><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  <b>Microsegmentation</b> </h4><br>  <b>Keyword Builder</b> helps advertisers to create very narrow audience segments (also called microsegments).  To build such audiences, we decided to use the following mechanism: the advertiser sets up a list of keywords, and at the output receives an audience consisting of those users who have visited pages with specified keywords or searched for these words in a search.  On the one hand, this is a very simple tool; on the other hand, it provides marketers with quite large possibilities for experiments. <br><br>  The main advantage of this tool is full control over the creation of the audience.  The advertiser clearly understands which users will eventually see the advertisement.  For example, you can create an audience based on the following list of keywords: ford focus, opel astra, toyota corola. <br><br>  How it looks from the side of the advertiser: <br><br><img src="https://habrastorage.org/files/69b/48f/88c/69b48f88cef24b49bd4948c705b55982.gif"><br><br>  First of all, to solve this problem, we get <b>clickstream</b> from all possible data providers (Raw Data Suppliers) (user‚Äôs browsing history).  The data comes to us in this form: <br><br><pre><code class="java hljs">{ user_id; url }</code> </pre> <br><br><h4>  <b>Goals and requirements</b> </h4><br>  The main requirement that we made to the tool is the speed of creating an audience.  This process should not take more than 5 minutes even for the most high-frequency words.  It is also important that the advertiser in the interface, when specifying a keyword, can estimate the size of the audience.  Size estimation should occur in real time when entering words (no more than 100 ms, as seen in the video above). <br><br>  For a better understanding, we present a complete list of requirements for the instrument: <br><ul><li>  principle of data locality; </li><li>  high availability of a database (high availability); </li><li>  horizontal scalability; </li><li>  high write speed; </li><li>  high speed full-text search; </li><li>  high speed search by key. </li></ul><br>  True, we came to these requirements after creating the first version of this tool :) <br><br><br><h4>  <b>The first architecture, which teaches, "how not to do"</b> </h4><br>  Initially, <b>MongoDB was</b> used and everything went pretty well. <br><br><img src="https://habrastorage.org/files/628/678/3fa/6286783fa6dd4a78943e3e8d060d76e6.jpg"><br><br>  In the <b>Visitor History</b> collection recorded data about the user, in the <b>Page Index</b> - about the pages.  The URL of the page itself does not represent value - the page should be downloaded and extracted keywords.  Then there was the first problem.  The fact is that the <b>Page Compiled</b> collection was not there at first, and the keywords were recorded in the <b>Page Index</b> , but the simultaneous recording of keywords and data from suppliers created too much load on this collection.  The <b>Keywords</b> field is usually large, it needs an index, and in MongoDB of that time (version 2.6) there was a lock on the entire collection during a write operation.  In general, I had to put the keywords into a separate <b>Page Compiled</b> collection.  I had to - so what, the problem is solved - we are happy.  Now it is difficult to remember the number and characteristics of servers ... something about 50 shard'ov. <br><br><img src="https://habrastorage.org/files/f31/b50/36f/f31b5036f9cd4a368fa03cbca3f49158.jpg"><br><br>  To create an audience by keywords, we made a request to the <b>Page Compiled</b> collection, received a list of URLs that met these words, went to the <b>Visitor History</b> collection with this list, and searched for users who visited these pages.  Everything worked well (sarcasm) and we could create 5 or even 10 (!!!) audience segments per day ... unless, of course, nothing falls.  The load at that time was about 800 million datapoint per day, the TTL index was 2 weeks: 800 * 14 ... there was a lot of data.  Work went wrong and for 3 months the load doubled.  But taking more N servers to sustain the life of this strange construction was not comme il faut. <br><br>  The biggest and most obvious disadvantage of this architecture is the above-described request related to obtaining a list of URLs.  The result of this query could be thousands, millions of records.  And most importantly, on this list, it was necessary to make a request to another collection. <br><br>  Other cons of architecture: <br><ul><li>  inconvenient recording due to global lock'a; </li><li>  the huge size of the index, which grew despite the deletion of data; </li><li>  the complexity of monitoring a large cluster; </li><li>  the need to independently create a simplified version of full-text search; </li><li>  the lack of a locality principle in data processing; </li><li>  the inability to quickly assess the size of the audience; </li><li>  the inability to add users to the audience on the fly. </li></ul><br>  Pros: <br><ul><li>  we understood all the minuses. </li></ul><br><br><h4>  <b>What we came to</b> </h4><br>  In general, from the very beginning it was clear that something had gone wrong, but now it became obvious.  We concluded that a table is needed, which will be updated in real time and contain records of the following form: <br><br><pre> <code class="java hljs">user { user_id; keywords[] }</code> </pre><br>  After analyzing possible solutions, we chose not for one database, but their combination with a small duplication of data.  We did this because the cost of data duplication was not comparable with the possibilities that this architecture gave us. <br><br><img src="https://habrastorage.org/files/4c2/6b8/aa2/4c26b8aa22c34c3189cd20699138c6f6.jpg"><br><br>  The first solution is the <b>Solr</b> full-text search platform.  Allows you to create a distributed index of documents.  Solr is a popular solution that has a large amount of documentation and is supported in the <b>Cloudera</b> service.  However, it didn‚Äôt work with him as a full-fledged database, we decided to add a distributed column database <b>HBase</b> to the architecture. <br><br>  At the beginning, it was decided that in Solr a <b>user</b> will appear as a document, in which there will be an indexed <b>keywords</b> field with all keywords.  But since we planned to delete the old data from the table, we decided to use the <b>user + date</b> link as the document, which became the <b>User_id</b> field: that is, each document should store all user keywords for the day.  This approach allows you to delete old entries on the TTL-index, as well as build audiences with varying degrees of "freshness."  The <b>Real_Id</b> field is the real user id.  This field is used for aggregation in queries with a specified duration of interest. <br><br>  By the way, in order not to store unnecessary data in <b>Solr</b> , the <b>keywords</b> field was made only indexable, which allowed us to significantly reduce the amount of stored information.  In this case, the keywords themselves, as you already understood, can be obtained from <b>HBase</b> . <br><br>  Solr features that we found useful in this architecture: <br><ul><li>  fast indexing; </li><li>  compact index; </li><li>  full text search; </li><li>  assessment of the number of documents matching the request; </li><li>  index optimization after data deletion (prevents its growth, as it often happens in other databases). </li></ul><br>  Data duplicated in HBase and Solr, applications are recorded only in HBase, from where the service from Cloudera automatically duplicates the records in Solr with the specified field attributes and TTL index. <br><br>  Thus, we were able to reduce the costs associated with heavy queries.  But these are not all the elements that we needed and became our foundation.  First of all, we needed to handle large amounts of data on the fly, and then <b>Apache Spark</b> with its <b>streaming</b> functionality came in handy.  And as the data queue, we chose <b>Apache Kafka</b> , which is the best suited for this role. <br><br>  Now the working scheme is as follows: <br><br><img src="https://habrastorage.org/files/45a/c1c/ba2/45ac1cba23954b9cbc6c6524e71ae69e.jpg"><br><br>  <b>1.</b> From the <b>Kafka</b> data select two processes.  Queuing functionality allows you to read it to several independent processes, each of which has its own cursor. <br><br>  <b>2.</b> <b>PageIndexer</b> - from the record {user_id: [urls]} uses only the URL.  It works only with the <b>Page Compiled</b> table. <br><ul><li>  When a datapoint arrives, it makes a request to HBase to check if the page has already been downloaded or not. </li><li>  If the page was not found in the database, extracts keywords from it. </li><li>  Writes data to HBase. </li></ul><br>  <b>3.</b> <b>VisitorActionReceiver</b> ‚Äî Collects information about users actions stored in the Kafka in 30 seconds (batch interval). <br><ul><li>  Remove duplicates from the queue </li><li>  Creating a record of the form {user_id: [urls]} </li><li>  Query to HBase in the <b>Page Compiled</b> table and retrieving keywords, creating a record of the form {user_id: [keywords]} </li><li>  Query to HBase in the <b>Visitor Keywords</b> table and retrieving keywords already contained by the user </li><li>  Combining keywords for a user </li><li>  Record in Solr and HBase </li></ul><br>  <b>4.</b> <b>SegmentBuilder</b> builds audiences and adds new users to existing ones, in real time. <br><ul><li>  Timer checks for raw audiences. </li><li>  Generates a request to Solr and receives users for the audience. </li><li>  Writes data to Aerospike </li></ul><br><br>  A feature of <b>PageIndexer</b> is the choice of keywords from a page, for different types of pages - different sets of words. <br>  Works on 1-2 servers 32GB RAM Xeon E5-2620, downloads 15-30K pages per minute.  At the same time choosing from the queue 200-400K records. <br>  And the main advantage of <b>VisitorActionReceiver</b> is that in addition to adding entries to Solr / HBase, data is also sent to Segment Builder and new users are added to the audience in real time. <br><div class="spoiler">  <b class="spoiler_title">The order of calls VisitorActionReceiver:</b> <div class="spoiler_text"><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(String[] args)</span></span></span><span class="hljs-function"> </span></span>{ SparkConf conf = getSparkConf(); JavaStreamingContext jssc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> JavaStreamingContext(conf, <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Duration(STREAMING_BATCH_SIZE)); JavaPairInputDStream&lt;String, <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>[]&gt; rawStream = Utils.createDirectStream(jssc, KAFKA_TOPIC_NAME, KAFKA_BROKERS); JavaPairDStream&lt;String, String&gt; pairUrlVid = rawStream.mapPartitionsToPair(rawMessageIterator -&gt; convertRawMessage(rawMessageIterator)); Utils.GetCountInStreamRDD(pairUrlVid, <span class="hljs-string"><span class="hljs-string">"before_distinct"</span></span>); JavaPairDStream&lt;String,String&gt; reducedUrlVid = pairUrlVid.reduceByKey((old_s, next_s) -&gt; reduceByUrl(old_s, next_s), LEVEL_PARALLELISM).repartition(LEVEL_PARALLELISM).persist(StorageLevel.MEMORY_ONLY_SER()); Utils.GetCountInStreamRDD(reducedUrlVid, <span class="hljs-string"><span class="hljs-string">"after_distinct"</span></span>); JavaDStream&lt;SolrInputDocument&gt; solrDocumentsStream = reducedUrlVid.mapPartitions(reducedMessages -&gt; getKeywordsFromHbase(reducedMessages)).repartition(SOLR_SHARD_COUNT).persist(StorageLevel.MEMORY_ONLY_SER()); Utils.GetCountInStreamRDD(solrDocumentsStream, <span class="hljs-string"><span class="hljs-string">"hbase"</span></span>); SolrSupport.indexDStreamOfDocs(SOLR_ZK_CONNECT, SOLR_COLLECTION_NAME, SOLR_BATCH_SIZE, solrDocumentsStream); jssc.start(); jssc.awaitTermination(); }</code> </pre><br></div></div><br>  <b>Segment Builder</b> generates a complex query in Solr, where words from different tags are assigned different weights, and the duration of the user's interest is taken into account (short-term, medium-term, long-term).  All requests for building an audience are made using the edismax query, which returns the weight of each document relative to the request.  In this way, really relevant users get into the audience. <br>  In <b>HBase</b> per second comes about 20K of requests for reading and 2.5K for writing.  The data storage capacity is ~ 100 GB.  <b>Solr</b> occupies ~ 250 GB, contains ~ 250 million TTL records 7 days index.  (All numbers without replication). <br><br>  Briefly recall the main elements of the infrastructure: <br>  <b>Kafka</b> is a smart and stable queue, <b>HBase</b> is a fast column database, <b>Solr</b> is a long-established search engine, <b>Spark</b> is distributed computing, including streaming, and most importantly, all this is on <b>HDFS</b> , scales well, is monitored and is very stable.  Works surrounded by Cloudera. <br><div class="spoiler">  <b class="spoiler_title">Vivid illustration of sustainability</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/c6d/aa6/47c/c6daa647cae04c8d99ef121a9a2a6d55.png"><br></div></div><br><img src="https://habrastorage.org/files/ba6/290/16d/ba629016d15f405a90265df99b2c2019.jpg"><br><br><h4>  <b>Conclusion</b> </h4><br>  It may seem that we have complicated the working scheme with a wide variety of tools.  In fact, we have taken the path of the greatest simplification.  Yes, we changed one Mongu to the list of services, but they all work in the niche for which they were created. <br>  Now the Keyword Builder really meets all the requirements of advertisers and common sense.  Audiences of 2.5 million people are created in 1-7 minutes.  Evaluation of audience size occurs in real time.  Only 8 servers are involved (i7-4770, 32GB RAM).  Adding servers entails a linear increase in performance. <br><br>  And you can always try the resulting tool in <a href="http://hybrid.ru/">the Hybrid retargeting platform</a> . <br><br>  I express <a href="http://habrahabr.ru/users/dcheklov/" class="user_link">my deep</a> gratitude to the user <a href="http://habrahabr.ru/users/dcheklov/" class="user_link">dcheklov</a> , for help in creating the article and <a href="http://habrahabr.ru/users/dcheklov/" class="user_link">admonishing the</a> true path =). <br>  The DMP theme started and remains open, wait for us in the next issues. <br><br><img src="https://habrastorage.org/files/d50/b61/911/d50b6191160745d49a4726b23d3f5ee1.jpg"></div><p>Source: <a href="https://habr.com/ru/post/264821/">https://habr.com/ru/post/264821/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../264805/index.html">Test lab v.8 - pentest laboratory, built on the basis of a real corporate network</a></li>
<li><a href="../264807/index.html">Does the amount of data on the complexity of the development. Accounting in the anthill</a></li>
<li><a href="../264811/index.html">Clustering graphs and searching for communities. Part 2: k-medoids and modifications</a></li>
<li><a href="../264815/index.html">As we raised the IT infrastructure [from the bottom]</a></li>
<li><a href="../264819/index.html">Asterisk Manager Interface in dialplan</a></li>
<li><a href="../264823/index.html">We write Observer implementation over KVO on objective-c</a></li>
<li><a href="../264827/index.html">Training in the field of practical information security: "Corporate laboratories". New set</a></li>
<li><a href="../264829/index.html">Mobile printing</a></li>
<li><a href="../264839/index.html">15 Important Developer Career Tips</a></li>
<li><a href="../264841/index.html">How to write software for the whole world</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>