<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Operation of rabbits (RabbitMQ) in the "Survive at any cost"</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="" Company " - communication operator of PJSC "Megaphone" 
 Noda is the RabbitMQ server. 
 ‚Äú Cluster ‚Äù is a set, in our case of three, RabbitMQ nodes w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Operation of rabbits (RabbitMQ) in the "Survive at any cost"</h1><div class="post__text post__text-html js-mediator-article">  " <b>Company</b> " - communication operator of PJSC "Megaphone" <br>  <b>Noda</b> is the RabbitMQ server. <br>  ‚Äú <b>Cluster</b> ‚Äù is a set, in our case of three, RabbitMQ nodes working as a whole. <br>  ‚Äú <b>Contour</b> ‚Äù is a collection of RabbitMQ clusters, the rules for working with which are determined by the balancer in front of them. <br>  " <b>Balancer</b> ", " <b>Hap</b> " - Haproxy - balancer, performing the function of switching the load on the clusters within the contour.  A pair of Haproxy servers running in parallel is used for each loop. <br>  ‚Äú <b>Subsystem</b> ‚Äù - publisher and / or consumer of messages transmitted through a rabbit <br>  ‚Äú <b>SYSTEM</b> ‚Äù is a set of Subsystems, which is a unified software and hardware solution used in the Company, characterized by distribution throughout Russia, but having several centers where all information flows and where the main calculations and calculations take place. <br>  <b>SYSTEM</b> - a geographically distributed system - from Khabarovsk and Vladivostok to St. Petersburg and Krasnodar.  Architecturally, these are several central contours, divided by the characteristics of the subsystems connected to them. <br><a name="habracut"></a><br><h3>  What is the task of transport in the realities of telecom? </h3><br>  In a nutshell: for each subscriber action follows the reaction of the Subsystems, which in turn informs the other Subsystems of events and subsequent changes.  Messages are generated by any actions with the SYSTEM, not only on the part of subscribers, but also on the part of the Company's employees, and on the part of the Subsystems (a very large number of tasks are performed automatically). <br><br>  Features of transport in telecom: large, no, not so, BIG stream of various data transmitted through asynchronous transport. <br><br>  Some Subsystems live on separate Clusters due to the heaviness of message flows ‚Äî there is simply no other resources left on the cluster, for example, with a message flow of 5-6 thousand messages / second, the volume of transmitted data can reach 170-190 Megabytes / second.  With such a load profile, someone else will land on this cluster will lead to sad consequences: since there are not enough resources to process all data at the same time, the rabbit will start to drive incoming connections during the <b>flow</b> - simple publishers will start, with all the consequences for all Subsystems and SYSTEMS in whole 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Basic requirements for transport: <br><br><ol><li>  Availability of transport should be 99.99%.  In practice, this translates into a 24/7 job requirement and the ability to automatically respond to any emergencies. </li><li>  Data security:% of lost messages on the transport should strive to 0. </li></ol><br>  For example, the very fact of the call, through the asynchronous transport flies several different messages.  some messages are intended for subsystems that live in the same circuit, and some are intended for transmission to central nodes.  The same message can be claimed by several subsystems, therefore, at the stage of publishing a message in a rabbit, it is copied and sent to different consumers.  And in some cases, copying messages is forcedly implemented on the intermediate circuit - when information must be delivered from the circuit in Khabarovsk, to the circuit in Krasnodar.  The transfer is performed through one of the central Circuits, where copies of the messages are made, for central recipients. <br><br>  In addition to events caused by subscriber actions, service messages go through the transport that Subsystems exchange.  This results in several thousand different message passing routes, some overlap, some exist in isolation.  It is enough to name the number of queues involved in routes on different Contours to understand the approximate scale of the transport map: On central contours 600, 200, 260, 15 ... and on remote Contours 80-100 ... <br><br>  With such transport involvement, the requirements of 100% availability of all transport hubs no longer seem excessive.  We turn to the implementation of these requirements. <br><br><h3>  How we solve the tasks </h3><br>  In addition to <i>RabbitMQ itself</i> , <i>Haproxy is</i> used for load balancing and providing automatic response to emergency situations. <br><br>  A few words about the software and hardware environment in which our rabbits exist: <br><br><ul><li>  All rabbit servers are virtual, with parameters of 8-12 CPU, 16 Gb Mem, 200 Gb HDD.  As experience has shown, even the use of creepy non-virtual servers for 90 cores and a bunch of RAM provides a small performance boost at a significantly higher cost.  Versions used: 3.6.6 (in practice - the most stable of 3.6) with an erlang of 18.3, 3.7.6 with an erlang of 20.1. </li><li>  For Haproxy, the requirements are much lower: 2 CPU, 4 Gb Mem, haproxy version - 1.8 stable.  Resource load on all haproxy servers does not exceed 15% CPU / Mem. </li><li>  The entire zoo is located in 14 data centers at 7 sites throughout the country, united in a single network.  In each of the data centers there is a cluster of three nods and one hap. </li><li>  For remote circuits, 2 data centers are used; for each of the central circuits, 4 each. </li><li>  Central Contours interact both with each other and with remote Contours, in turn, remote Contours work only with central ones, they do not have a direct connection between each other. </li><li>  The configurations of Hapov and Clusters within one Contour are completely identical.  The entry point for each Contour is a pseudonym for several A-DNS records.  Thus, in order not to happen, at least one hap and at least one of the clusters (at least one node in the cluster) will be available in each circuit.  Since the case of failure of even 6 servers in two data centers at the same time is extremely unlikely, the availability is close to 100%. </li></ul><br>  It looks conceived (and realized) all this about like this: <br><br><img src="https://habrastorage.org/webt/f2/rm/9i/f2rm9i0kd5jk1ikaapl2vc63yhc.jpeg" alt="image"><br><br><img src="https://habrastorage.org/webt/az/5t/ks/az5tkse8qkz0fl3ml6znbg4pz18.jpeg" alt="image"><br><br>  Now some configs. <br><br><div class="spoiler">  <b class="spoiler_title">Haproxy configuration</b> <div class="spoiler_text"><table><tbody><tr><td>  frontend center-rmq_5672 </td><td></td></tr><tr><td></td><td>  bind </td><td>  *: 5672 </td></tr><tr><td></td><td>  mode </td><td>  tcp </td></tr><tr><td></td><td>  maxconn </td><td>  10,000 </td></tr><tr><td></td><td>  timeout client </td><td>  3h </td></tr><tr><td></td><td>  option </td><td>  tcpka </td></tr><tr><td></td><td>  option </td><td>  tcplog </td></tr><tr><td></td><td>  default_backend </td><td>  center-rmq_5672 </td></tr><tr><td>  frontend center-rmq_5672_lvl_1 </td><td></td></tr><tr><td></td><td>  bind </td><td>  localhost: 56721 </td></tr><tr><td></td><td>  mode </td><td>  tcp </td></tr><tr><td></td><td>  maxconn </td><td>  10,000 </td></tr><tr><td></td><td>  timeout client </td><td>  3h </td></tr><tr><td></td><td>  option </td><td>  tcpka </td></tr><tr><td></td><td>  option </td><td>  tcplog </td></tr><tr><td></td><td>  default_backend </td><td>  center-rmq_5672_lvl_1 </td></tr><tr><td>  backend center-rmq_5672 </td></tr><tr><td></td><td>  balance </td><td>  leastconn </td></tr><tr><td></td><td>  mode </td><td>  tcp </td></tr><tr><td></td><td>  fullconn </td><td>  10,000 </td></tr><tr><td></td><td>  timeout </td><td>  server 3h </td></tr><tr><td></td><td>  server </td><td>  srv-rmq01 10.10.10.10II672 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr><tr><td></td><td>  server </td><td>  srv-rmq03 10.10.10.11 Lower672 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr><tr><td></td><td>  server </td><td>  srv-rmq05 10.10.10.12 Low672 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr><tr><td></td><td>  server </td><td>  localhost 127.0.0.1 Redu6721 check inter 5s rise 2 fall 3 backup on-marked-down shutdown sessions </td></tr><tr><td>  backend center-rmq_5672_lvl_1 </td></tr><tr><td></td><td>  balance </td><td>  leastconn </td></tr><tr><td></td><td>  mode </td><td>  tcp </td></tr><tr><td></td><td>  fullconn </td><td>  10,000 </td></tr><tr><td></td><td>  timeout </td><td>  server 3h </td></tr><tr><td></td><td>  server </td><td>  srv-rmq02 10.10.10.13 Low672 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr><tr><td></td><td>  server </td><td>  srv-rmq04 10.10.10.14II672 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr><tr><td></td><td>  server </td><td>  srv-rmq06 10.10.10.5.56767 check inter 5s rise 2 fall 3 on-marked-up shutdown-backup-sessions </td></tr></tbody></table><br></div></div><br>  The first section of the front describes the entry point - leading to the main cluster, the second section is designed for balancing the reserve level.  If you simply describe in the backend section all backup servers of rabbits (backup instruction), then it will work the same way - if the main cluster is completely unavailable, the connections will go to the backup, however, all connections will go to FIRST in the backup server list.  To ensure load balancing on all backup nodes, we are introducing one more front, which we make available only from localhost and assign it as the backup server. <br><br>  The given example describes the balancing of the remote Circuit - which operates within two data centers: the server srv-rmq {01.03.05} - live in data center No. 1, srv-rmq {02.04.06} - in data center No. 2.  Thus, to implement the four-zod solution, we only need to add two more local fronts and two backend sections of the corresponding rabbit servers. <br><br>  The balancer behavior with this configuration is the following: While at least one primary server is alive, we use it.  If the main servers are not available, we work with the reserve.  If at least one primary server becomes available, all connections to the backup servers are broken and when the connection is restored, they already fall on the main cluster. <br><br>  Operating experience of this configuration shows almost 100% availability of each of the circuits.  This solution requires the Subsystems to be completely legitimate and simple: to be able to reconnect with the rabbit after breaking the connection. <br><br>  So, we have provided load balancing for an arbitrary number of Clusters and automatic switching between them, it's time to go directly to the rabbits. <br><br>  Each Cluster is created from three nodes, as practice shows - the most optimal number of nodes, which ensures the optimal balance of availability / resiliency / speed.  Since the rabbit does not scale horizontally (cluster performance is equal to the performance of the slowest server), we create all nodes with the same, optimal parameters using CPU / Mem / Hdd.  We arrange the servers as close as possible to each other - in our case, we write down virtual machines within the same farm. <br><br>  As for the preconditions, following which by the Subsystems will ensure the most stable operation and fulfillment of the requirement to preserve incoming messages: <br><br><ol><li>  Work with the rabbit is only under the protocol amqp / amqps - through balancing.  Authorization under local accounts - within each Cluster (and of the Outline as a whole) </li><li>  Subsystems are connected to the rabbit in the passive mode: No manipulation of the entities of the rabbits (creation of queues / exchanges / binds) is allowed and limited to the level of account rights - we simply do not give permission to configure. </li><li>  All necessary entities are created centrally, not by means of Subsystems, and on all Contour Clusters are done in the same way - to ensure automatic switching to the backup Cluster and back.  Otherwise, we can get a picture: they have switched to the reserve, but there is no queue or bind there, and we can get either a connection error or a message loss to choose from. </li></ol><br><h3>  <b>Now directly settings on rabbits:</b> </h3><br><ol><li>  Local KMs do not have access to the Web interface. </li><li>  Access to the Web is organized through LDAP - we integrate with AD and we get logging who and where on the webcam went.  At the configuration level, we restrict the rights of the AD accounts, not only do we require being in a certain group, so we give only the rights to ‚Äúlook‚Äù.  Monitoring groups are more than enough.  And we assign the rights of the administrator to another group in AD, thus the range of influence on transport is strongly limited. </li><li>  To facilitate administration and tracking: <br>  At all VHOST, we immediately hang a level 0 policy with application to all queues (pattern:. *): <br><br><ul><li>  <b><i>ha-mode: all</i></b> - store all data on all nodes of the cluster, slows down the processing of messages, but ensures their safety and availability. </li><li>  <b><i>ha-sync-mode: automatic</i></b> - we instruct the crawler to automatically synchronize data on all nodes of the cluster: the safety and availability of data also increases. </li><li>  <b><i>queue-mode: lazy</i></b> - perhaps one of the most useful options that appeared in rabbits from version 3.6 - the immediate recording of messages on the HDD.  This option drastically reduces the consumption of RAM and increases the data integrity when the node stops or falls or the cluster as a whole. </li></ul><br></li><li>  Settings in the configuration file ( <i>rabbitmq-main / conf / rabbitmq.config</i> ): <br><br><ul><li>  Section <b>rabbit</b> : <i>{vm_memory_high_watermark_paging_ratio, 0.5}</i> - the threshold for downloading messages to disk is 50%.  When <b>lazy</b> is on, it serves more like insurance when we draw a policy, for example, level 1, in which we forget to include <b>lazy</b> . </li><li>  <i>{vm_memory_high_watermark, 0.95}</i> - we limit the crawl to 95% of all RAM, since only the rabbit lives on the servers, there is no point in imposing more stringent restrictions.  5% ‚Äúbroad gesture‚Äù so be it - leave the OS, monitoring and other useful trifles.  Since this value is the upper limit - there is enough resources for everyone. </li><li>  <i>{cluster_partition_handling, pause_minority}</i> - describes the behavior of the cluster when a Network Partition occurs, for three or more node clusters, it is recommended that this flag - allows the cluster to recover itself. </li><li>  <i>{disk_free_limit, "500MB"}</i> - everything is simple when there is 500 MB of free disk space - the publication of messages will be stopped, only reading will be available. </li><li>  <i>{auth_backends, [rabbit_auth_backend_internal, rabbit_auth_backend_ldap]}</i> - authorization order in rabbits: First, check for the presence of KM in the local database and if not found - go to the LDAP server. </li><li>  Section <b>rabbitmq_auth_backend_ldap</b> - configuration of interaction with AD: <i>{servers, ["srv_dc1", "srv_dc2"]}</i> - list of domain controllers on which authentication will take place. </li><li>  The parameters that directly describe the user in AD, the LDAP port, etc., are very individual and are described in detail in the documentation. </li><li>  The most important thing for us is a description of the rights and restrictions on the administration and access to the web page interface: tag_queries: <br>  <i>[{administrator, {in_group, "cn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}},</i> <i><br></i>  <i>{monitoring,</i> <i><br></i>  <i>{in_group, "cn = rabbitmq-web, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = en"}</i> <i><br></i>  <i>}]</i> - this construction provides administrative privileges to all users of the rabbitmq-admins group and monitoring rights (minimum sufficient for access to view) for the rabbitmq-web group. <br></li><li>  <b>resource_access_query</b> : <br>  <i>{for,</i> <i><br></i>  <i>[{permission, configure, {in_group, "cn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru"}},</i> <i><br></i>  <i>{permission, write, {in_group, ‚Äúcn = rabbitmq-admins, ou = GRP, ou = GRP_MAIN, dc = My_domain, dc = ru‚Äù}},</i> <i><br></i>  <i>{permission, read, {constant, true}}</i> <i><br></i>  <i>]</i> <i><br></i>  <i>}</i> - we provide configuration and write permissions only to a group of administrators, all others who successfully authorize permissions are read-only - may well read messages via the Web interface. <br></li></ul></li></ol><br>  We get a configured (at the level of the configuration file and settings in the rabbit itself) cluster, which maximally ensures the availability and safety of data.  By this we implement the requirement - ensuring the availability and security of data ... in most cases. <br><br>  There are several points that should be taken into account when operating such highly loaded systems: <br><br><ol><li>  All additional properties of queues (TTL, expire, max-length, etc.) are better organized by politicians, and not hung with parameters when creating queues.  It turns out a flexibly customizable structure that can be customized on the fly to changing realities. </li><li>  Using TTL.  The longer the queue, the higher the load on the CPU.  In order to prevent the ‚Äúpunching of the ceiling‚Äù, it is better to limit the queue length in max-length. </li><li>  In addition to the rabbit itself, a certain number of service applications are spinning on the server, which, oddly enough, also requires CPU resources.  A greedy rabbit, by default, occupies all the available cores ... It may turn out to be an unpleasant situation: the struggle for resources, which will easily lead to brakes on a rabbit.  To avoid the occurrence of such a situation, you can, for example, like this: Change the erlang launch parameters ‚Äî enter a forced limit on the number of used cores.  We do this as follows: find the <i>rabbitmq-env</i> file, look for the SERVER_ERL_ARGS = parameter and add the + sct L0-Xc0-X + SY: Y to it.  Where X is the number of 1 cores (counting starts from 0), Y is the Number of cores -1 (counting from 1).  + sct L0-Xc0-X - changes the binding to the cores, + SY: Y - changes the number of shearlers triggered by the Erlang.  So for a system of 8 cores, the added parameters will take the form: + sct L0-6c0-6 + S 7: 7.  By this, we give the rabbit only 7 cores and expect that the OS will launch other processes optimally and hang them on an unloaded core. </li></ol><br><h3>  <b>The nuances of operating the resulting zoo</b> </h3><br>  What no setting can protect against is a collapsed mnesia base - unfortunately, it is happening with a non-zero probability.  Not global failures (for example, complete failure of the entire data center - the load will simply switch to another cluster) lead to this deplorable result, but failures are more local - within the same network segment. <br><br>  And it is terrible local network failures, because  emergency shutdown of one or two nodes will not lead to fatal consequences - just all requests will go to one node, and as we remember, performance depends on the performance of the node itself.  Network failures (we do not take into account small interruptions of communication - they are experienced without serious consequences), lead to a situation where the nodes start the synchronization process between themselves and then the connection breaks again and again for a few seconds. <br><br>  For example, multiple blinking of the network, and with a periodicity of more than 5 seconds (this is exactly the timeout set in the Hap settings, you can of course play around, but to check the effectiveness you will need to repeat the failure, which nobody wants). <br><br>  One - two such iterations a cluster can still withstand, but more - the chances are already minimal.  In such a situation, stopping a dropped node can save, but it's almost impossible to do it manually.  More often, the result is not just a dropout of the node from the cluster with the <b>‚ÄúNetwork Partition‚Äù</b> message, but also a picture when the data on the part of the queues lived just this node and did not have time to synchronize for the others.  Visually - in the data on the queues is <b><i>NaN</i></b> . <br><br>  And this is already an unambiguous signal - to switch to the backup cluster.  Switching will provide hap, you only need to stop the rabbits on the main cluster - a matter of a few minutes.  As a result, we get the restoration of transport performance and we can safely proceed to the analysis of the accident and its elimination. <br><br>  In order to remove a damaged cluster from under load, in order to prevent further degradation, the simplest thing is to make the rabbit work on ports other than 5672. Since we have Hapa who monitor the rabbits in the regular port, its offset, for example, at 5673 in the settings of the rabbit, it will allow the cluster to run completely painlessly and try to restore its working capacity and the messages remaining on it. <br><br>  We do in a few steps: <br><br><ol><li>  We stop all the nodes of the failed cluster - hap will switch the load to the backup cluster </li><li>  Add RABBITMQ_NODE_PORT = 5673 to the file <i>rabbitmq-env</i> - when the rabbit is started, these settings will be pulled, and the Web interface will still work on 15672. </li><li>  We specify the new port on all nodes of the untimely deceased cluster and launch them. </li></ol><br>  At startup, there will be a restructuring of the indexes and in the overwhelming majority of cases all data is restored in full.  Unfortunately, failures occur as a result of which one has to physically delete all messages from the disk, leaving only the configuration ‚Äî the directories <i>msg_store_persistent</i> , <i>msg_store_transient</i> , <i>queues</i> (for version 3.6) or <i>msg_stores</i> (for version 3.7) are deleted in the database folder. <br><br>  After such radical therapy, the cluster is launched with preservation of the internal structure, but without messages. <br><br>  And the most unpleasant option (it was observed once): The damage to the base was such that it was necessary to completely remove the entire base and rebuild the cluster from scratch. <br><br>  For the convenience of managing and updating rabbits, not a ready-made assembly in rpm is used, but a rabbit disassembled using cpio and reconfigured (changed paths in scripts).  The main difference: it does not require root rights for installation / configuration, is not installed on the system (the reassembled rabbit is perfectly packaged in tgz) and runs from any user.  This approach allows flexibility to update versions (if it does not require a complete cluster shutdown - in this case, simply switch to the backup cluster and update, not forgetting to specify the offset port for operation).  It is even possible to launch several instances of RabbitMQ on one machine ‚Äî the test is very convenient for tests ‚Äî you can deploy a reduced architectural copy of a combat zoo. <br><br>  As a result, shamanism with cpio and paths in scripts received an assembly option: two rabbitmq-base folders (in the original assembly - mnesia folder) and rabbimq-main - put all the necessary scripts for both the rabbit and erlang. <br><br>  In rabbimq-main / bin - symlinks to the rabbit and erlang scripts and the rabbit tracking script (description below). <br><br>  In rabbimq-main / init.d - the rabbitmq-server script through which the logs start / stop / rotate;  in lib, the rabbit itself;  in lib64 - erlang (a truncated version is used, only for rabbit operation, erlang version). <br><br>  It is extremely easy to update the resulting assembly when new versions are released - add the contents of rabbimq-main / lib and rabbimq-main / lib64 from new versions and replace the symlinks in the bin.  If the update also affects control scripts, simply changing the paths to ours in them. <br><br>  The solid advantage of this approach is the complete continuity of versions - all paths, scripts, control commands remain unchanged, which allows you to use any self-written service scripts without finishing each version. <br><br>  Since the fall of rabbits, an event, though rare, is happening, it was necessary to embody a mechanism for tracking their well-being - raising in case of a fall (while maintaining the logs of the reasons for the fall).  The fall of the node in 99% of cases is accompanied by a log entry, even kill and it leaves traces; this allowed monitoring the state of the crawl with a simple script. <br><br>  For versions 3.6 and 3.7, the script is slightly different due to the differences in the log entries. <br><br><div class="spoiler">  <b class="spoiler_title">For version 3.6</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/python import subprocess import os import datetime import zipfile def LastRow(fileName,MAX_ROW=200): with open(fileName,'rb') as f: f.seek(-min(os.path.getsize(fileName),MAX_ROW),2) return (f.read().splitlines())[-1] if os.path.isfile('/data/logs/rabbitmq/startup_log'): if b'FAILED' in LastRow('/data/logs/rabbitmq/startup_log'): proc = subprocess.Popen("ps x|grep rabbitmq-server|grep -v 'grep'", shell=True, stdout=subprocess.PIPE) out = proc.stdout.readlines() if str(out) == '[]': cur_dt=datetime.datetime.now() try: os.stat('/data/logs/rabbitmq/after_crush') except: os.mkdir('/data/logs/rabbitmq/after_crush') z=zipfile.ZipFile('/data/logs/rabbitmq/after_crush/repair_log'+'-'+str(cur_dt.day).zfill(2)+str(cur_dt.month).zfill(2)+str(cur_dt.year)+'_'+str(cur_dt.hour).zfill(2)+'-'+str(cur_dt.minute).zfill(2)+'-'+str(cur_dt.second).zfill(2)+'.zip','a') z.write('/data/logs/rabbitmq/startup_err','startup_err') proc = subprocess.Popen("~/rabbitmq-main/init.d/rabbitmq-server start", shell=True, stdout=subprocess.PIPE) out = proc.stdout.readlines() z.writestr('res_restart.log',str(out)) z.close() my_file = open("/data/logs/rabbitmq/run.time", "a") my_file.write(str(cur_dt)+"\n") my_file.close()</span></span></code> </pre> <br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">For 3.7, only two lines change</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (os.path.isfile(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> (os.path.isfile(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_err'</span></span>)): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> ((<span class="hljs-string"><span class="hljs-string">b' OK '</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> (<span class="hljs-string"><span class="hljs-string">b'FAILED'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_log'</span></span>))) <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> (<span class="hljs-string"><span class="hljs-string">b'Gracefully halting Erlang VM'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> LastRow(<span class="hljs-string"><span class="hljs-string">'/data/logs/rabbitmq/startup_err'</span></span>)):</code> </pre><br></div></div><br><br>  We start in the crontab account under which the rabbit will work (by default rabbitmq) the execution of this script (script name: check_and_run) every minute (for a start we ask the admin to give the account the rights to use crontab, we do it ourselves): <br>  <b><i>* / 1 * * * * ~ / rabbitmq-main / bin / check_and_run</i></b> <br><br>  The second point in the use of the reassembled crawl is the logging of logs. <br><br>  Since we are not tied to the logrotate of the system - we use the functionality provided by the developer: <b>rabbitmq-server</b> script from init.d (for version 3.6) <br>  Making minor changes to <i>rotate_logs_rabbitmq ()</i> <br>  Add: <br><br><pre> <code class="bash hljs"> find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.*.back -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f | xargs -i gzip {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete</code> </pre><br>  The result of running the rabbitmq-server script with the rotate-logs key: logs are compressed with gzip and stored only for the last 30 days.  <b>http_api</b> - the path where the rabbit adds http logs - is configured in the configuration file: <i>{rabbitmq_management, [{rates_mode, detailed}, {http_log_dir, path_to_logs / http_api "}]}</i> <br><br>  At the same time I pay attention to <i>{rates_mode, <b>detailed</b> }</i> - the option somewhat increases the load, but it allows you to see on the WEB interface (and thus get through the API) information about who publishes the messages in the exchanges.  Information is extremely necessary, because  all connections go through the balancer - we will see only the IP of the balancers themselves.  And if all the Subsystems that work with the rabbit are puzzled to fill in the ‚ÄúClient properties‚Äù parameters in the properties of their connections to rabbits, then it will be possible at the connection level to get detailed information about who exactly where and with what intensity publishes messages. <br><br>  With the release of new versions 3.7, there was a complete rejection of the <b>rabbimq-server</b> script in init.d.  In order to facilitate operation (uniformity of control commands regardless of the rabbit version) and smoother transition between versions, we continue to use this script in the reassembled rabbit.  It is true again: we <i>‚Äôll</i> change <i>rotate_logs_rabbitmq () a</i> bit, since in 3.7 the log naming mechanism after rotation was changed: <br><br><pre> <code class="bash hljs"> mv <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/<span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>.log.0 <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/<span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.$(date +%Y%m%d-%H%M%S).back mv <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/$(<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>)_upgrade.log.0 <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/$(<span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$NODENAME</span></span>)_upgrade.log.$(date +%Y%m%d-%H%M%S).back find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.* -maxdepth 0 -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f ! -name <span class="hljs-string"><span class="hljs-string">"*.gz"</span></span> | xargs -i gzip --force {} find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete find <span class="hljs-variable"><span class="hljs-variable">${RABBITMQ_LOG_BASE}</span></span>/http_api/*.gz -<span class="hljs-built_in"><span class="hljs-built_in">type</span></span> f -mtime +30 -delete</code> </pre><br>  Now it remains only to add a task to rotate logs in crontab - for example, every day at 23-00: <br>  <b><i>00 23 * * * ~ / rabbitmq-main / init.d / rabbitmq-server rotate-logs</i></b> <br><br>  Let us turn to the tasks that need to be addressed in the framework of the operation of the "rabbit farm": <br><br><ol><li>  Manipulations with entities of rabbits - creation / removal of entities of a rabbit: ekschendzhey, turns, binds, chauvelov, users, the politician.  And to do this is absolutely identical on all Contour Clusters. </li><li>  After switching to / from the backup Cluster, it is required to transfer messages that remain on it to the current Cluster. </li><li>  Backup configurations of all Clusters of all Contours </li><li>  Full synchronization of Cluster configurations within the contour </li><li>  Stop / start rabbits </li><li>  Analyze current data streams: do all messages go and, if they do, go where they should or‚Ä¶ </li><li>  Find and catch passing messages by any criteria. </li></ol><br>  The operation of our zoo and the solution of the voiced tasks by means of the supplied <i>rabbitmq_management</i> staff plug-in is possible, but extremely inconvenient, which is why the shell was developed and implemented to <b><a href="https://github.com/spirtus/rabbitmq_Mazay">control the entire diversity of rabbits</a></b> . </div><p>Source: <a href="https://habr.com/ru/post/434016/">https://habr.com/ru/post/434016/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../434004/index.html">Wargaming Platform: Hello World</a></li>
<li><a href="../434006/index.html">Do we need cookies banners in the era of GDPR - we discuss the situation and the requirements of the law</a></li>
<li><a href="../434008/index.html">How to stop worrying and start writing tests based on properties</a></li>
<li><a href="../434010/index.html">To yourself devops or configure Nginx proxy for Apache Tomcat on Ubuntu in 5 minutes with https and firewall</a></li>
<li><a href="../434012/index.html">Free PVS-Studio for those who develop open projects</a></li>
<li><a href="../434018/index.html">We get a certificate from Google Associate Android Developer</a></li>
<li><a href="../434022/index.html">Uber resumed tests of its robomobiles nine months after the fatal accident</a></li>
<li><a href="../434024/index.html">Horsepower in Android or once again about RecyclerView.LayoutManager</a></li>
<li><a href="../434026/index.html">Price of change: how much will the processing of the code actually cost</a></li>
<li><a href="../434028/index.html">Alexa developers have integrated digital assistant with Wolfram Alpha</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>