<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Mountain Car: solve the classic problem with the help of training with reinforcements</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As a rule, modifications of algorithms that rely on the features of a particular task are considered less valuable, since they are difficult to genera...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Mountain Car: solve the classic problem with the help of training with reinforcements</h1><div class="post__text post__text-html js-mediator-article">  As a rule, modifications of algorithms that rely on the features of a particular task are considered less valuable, since they are difficult to generalize to a wider class of problems.  However, this does not mean that such modifications are not needed.  Moreover, they can often significantly improve the result even for simple classical problems, which is very important in the practical application of algorithms.  As an example in this post, I will solve the Mountain Car problem with the help of reinforcement training and show that using knowledge of how a problem is structured, it can be solved much faster. <br><br><img src="https://habrastorage.org/webt/xy/ju/ai/xyjuaivxj9j2c5hp2o-2x3cem2y.png"><br><a name="habracut"></a><br><h2>  About myself </h2><br>  My name is Oleg Svidchenko, now I study at the School of Physical-Mathematical and Computer Science of the St. Petersburg HSE, before that I had been studying at SPbAU for three years.  I also work at JetBrains Research as a researcher.  Before entering the university, I studied at the SSC of Moscow State University and became the winner of the All-Russian Olympiad in Informatics for Schoolchildren in the Moscow team. <br><br><h2>  What do we need? </h2><br>  If you are interested in trying out exercises with reinforcements, the Mountain Car challenge is great for this.  Today, we will need Python with the <a href="https://gym.openai.com/">Gym</a> and <a href="https://pytorch.org/">PyTorch</a> libraries installed, as well as basic knowledge of neural networks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Task Description </h2><br>  In a two-dimensional world, a car needs to climb from the depression between two hills to the top of the right hill.  Everything is complicated by the fact that it does not have enough engine power to overcome the force of gravity and enter there at the first attempt.  We are invited to train an agent (in our case, a neural network), who, while managing it, can climb the right hill as quickly as possible. <br><br>  The machine is controlled through interaction with the medium.  It is divided into independent episodes, and each episode is carried out step by step.  At each step, the agent receives from the environment a state <i>s</i> and a reward <i>r</i> in response to the action he performed <i>a</i> .  In addition, sometimes the medium may additionally report that the episode is over.  In this task, <i>s</i> is a pair of numbers, the first of which is the position of the car on the curve (one coordinate is enough, since we cannot detach from the surface), and the second is its speed on the surface (with a sign).  The reward <i>r</i> is a number, always equal to -1 for this task.  Thus, we encourage the agent to complete the episode as quickly as possible.  There are only three possible actions: push the car to the left, do nothing and push the car to the right.  The numbers from 0 to 2 correspond to these actions. The episode can be completed if the car reaches the top of the right hill or if the agent has taken 200 steps. <br><br><h2>  Some theory </h2><br>  On Habr√© there was already an <a href="https://habr.com/ru/post/279729/">article about DQN</a> , in which the author described all the necessary theory quite well.  Nevertheless, for the convenience of reading, I will repeat it here in a more formal form. <br><br>  The learning task with reinforcement is given by a set of state space S, action space A, coefficient <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.237ex" height="1.817ex" viewBox="0 -520.7 3546.5 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-67" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="730" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="1260" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="2138" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="3017" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> \ gamma </script>  , transition functions T and reward functions R. In general, the transition function and the reward function can be random values, however, we now consider a simpler version in which they are uniquely defined.  The goal is to maximize the cumulative reward. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msub><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.676ex" height="3.021ex" viewBox="0 -883.9 10193.7 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-75" x="719" y="0"></use><g transform="translate(1292,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-54" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-3D" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-30" x="1140" y="0"></use></g></g><g transform="translate(3430,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-72" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="638" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-63" x="4487" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-64" x="4921" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="5444" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="5930" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-67" x="6541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="7022" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="7551" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="8430" y="0"></use><g transform="translate(9308,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="748" y="513"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>s</mi><mi>u</mi><msubsup><mi>m</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>=</mo><mn>0</mn></mrow><mrow class="MJX-TeXAtom-ORD"><mi>T</mi></mrow></msubsup><msub><mi>r</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msub><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><msup><mi>a</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-2"> \ sum_ {t = 0} ^ {T} r_ {t} \ cdot \ gamma ^ {t} </script>  , where t is the step number in the environment, and T is the number of steps in the episode. <br><br>  To solve this problem, we define the value-function V of the state s as the value of the maximum cumulative reward, provided that we start in the state s.  Knowing such a function, we can solve the problem simply by moving each step to s with the maximum possible value.  However, not everything is so simple: in most cases we do not know what action will lead us to the desired state.  Therefore, we add the action a as the second parameter of the function.  The resulting function is called the Q-function.  It shows the maximum possible cumulative reward we can receive by performing the action a in the state s.  But we can already use this function to solve the problem: being in the state s, we simply choose a such that Q (s, a) is maximal. <br><br>  In practice, we do not know the real Q-function, but we can bring it closer using various methods.  One such method is the Deep Q Network (DQN).  His idea is that for each of the actions we approximate the Q-function using a neural network. <br><br><h2>  Environment </h2><br>  We now turn to practice.  First we need to learn how to emulate the MountainCar environment.  The Gym library will help us with this task, which provides a large number of standard environments for reinforcement learning.  To create an environment, we need to call the make method of the gym module, passing it the name of the desired environment as a parameter: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym env = gym.make(<span class="hljs-string"><span class="hljs-string">"MountainCar-v0"</span></span>)</code> </pre> <br>  Detailed documentation can be found <a href="https://github.com/openai/gym/blob/master/gym/core.py">here</a> , and a description of the environment - <a href="https://github.com/openai/gym/wiki/MountainCar-v0">here</a> . <br>  Let's take a closer look at what we can do with the environment we created: <br><br><ul><li>  <code>env.reset()</code> - ends the current episode and starts a new one.  Returns the initial state. </li><li>  <code>env.step(action)</code> - performs the specified action.  It returns a new state, a reward, whether the episode has ended and additional information that can be used for debugging. </li><li>  <code>env.seed(seed)</code> - sets a random seed.  It determines how initial states will be generated during env.reset (). </li><li>  <code>env.render()</code> - displays the current state of the environment. </li></ul><br><h2>  Implement DQN </h2><br>  DQN is an algorithm that uses a neural network to evaluate the Q-function.  The <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">original</a> DeepMind <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">article</a> identified the standard architecture for Atari games using convolutional neural networks.  Unlike these games, Mountain Car does not use the image as a state, so we‚Äôll have to define the architecture ourselves. <br><br>  Take, for example, an architecture with two hidden layers of 32 neurons each.  After each hidden layer, we will use <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLU</a> as an activation function.  Two numbers describing the state are fed to the input of the neural network, and at the output we get the estimate of the Q-function. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ae/jl/mk/aejlmktkosv-jpbne9hqi96enxw.png" alt="Neural network architecture"></div><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights)</span></span></code> </pre><br>  Since we will train the neural network on the GPU, we need to upload our network there: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     CPU,  ‚Äúcuda‚Äù    ‚Äúcpu‚Äù device = torch.device("cuda") model.to(device) target_model.to(device)</span></span></code> </pre><br>  The variable device will be global, since we also need to load the data. <br><br>  We also need to define an optimizer that will update model weights using a gradient descent.  Yes, there are more than one. <br><br><pre> <code class="python hljs">optimizer = optim.Adam(model.parameters(), lr=<span class="hljs-number"><span class="hljs-number">0.00003</span></span>)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Together</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch.nn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> nn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> torch device = torch.device(<span class="hljs-string"><span class="hljs-string">"cuda"</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_new_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> model = nn.Sequential( nn.Linear(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>), nn.ReLU(), nn.Linear(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>) ) target_model = copy.deepcopy(model) <span class="hljs-comment"><span class="hljs-comment">#    def init_weights(layer): if type(layer) == nn.Linear: nn.init.xavier_normal(layer.weight) model.apply(init_weights) #   ,     (GPU  CPU) model.to(device) target_model.to(device) #  ,        optimizer = optim.Adam(model.parameters(), lr=0.00003) return model, target_model, optimizer</span></span></code> </pre><br></div></div><br>  Now we will declare the function that will read the error function, the gradient along it, and apply the descent.  But before that you need to download data from the batch to the GPU: <br><br><pre> <code class="python hljs">state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device)</span></span></code> </pre><br>  Next, we need to calculate the real values ‚Äã‚Äãof the Q-function, but since we do not know them, we will evaluate them using the values ‚Äã‚Äãfor the following state: <br><br><pre> <code class="python hljs">target_q = torch.zeros(reward.size()[<span class="hljs-number"><span class="hljs-number">0</span></span>]).float().to(device) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> torch.no_grad(): <span class="hljs-comment"><span class="hljs-comment">#     Q-function    target_q[done] = target_model(next_state).max(1)[0].detach()[done] target_q = reward + target_q * gamma</span></span></code> </pre><br>  And the current prediction: <br><br><pre> <code class="python hljs">q = model(state).gather(<span class="hljs-number"><span class="hljs-number">1</span></span>, action.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>))</code> </pre><br>  Using target_q and q, we consider the loss function and update the model: <br><br><pre> <code class="python hljs">loss = F.smooth_l1_loss(q, target_q.unsqueeze(<span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-comment"><span class="hljs-comment">#      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">Together</b> <div class="spoiler_text"><pre> <code class="python hljs">gamma = <span class="hljs-number"><span class="hljs-number">0.99</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch, model, target_model, optimizer)</span></span></span><span class="hljs-function">:</span></span> state, action, reward, next_state, done = batch <span class="hljs-comment"><span class="hljs-comment">#       state = torch.tensor(state).to(device).float() next_state = torch.tensor(next_state).to(device).float() reward = torch.tensor(reward).to(device).float() action = torch.tensor(action).to(device) done = torch.tensor(done).to(device) #  ,       target_q = torch.zeros(reward.size()[0]).float().to(device) with torch.no_grad(): #     Q-function    target_q[done] = target_model(next_state).max(1)[0].detach()[done] target_q = reward + target_q * gamma #   q = model(state).gather(1, action.unsqueeze(1)) loss = F.smooth_l1_loss(q, target_q.unsqueeze(1)) #      optimizer.zero_grad() #     loss.backward() #   . ,       for param in model.parameters(): param.grad.data.clamp_(-1, 1) #    optimizer.step()</span></span></code> </pre><br></div></div><br>  Since the model only considers the Q-function, and does not perform actions, we need to define a function that will decide which actions the agent will perform.  As a decision-making algorithm, we take <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> \ varepsilon </script>  -greedy policy.  Her idea is that the agent usually performs actions greedily, choosing the maximum of the Q function, however with probability <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ varepsilon </script>  he will perform a random act.  Random actions are needed so that the algorithm can investigate those actions that it would not have committed guided only by greedy politics - this process is called exploration. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">select_action</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(state, epsilon, model)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> random.random() &lt; epsilon: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model(torch.tensor(state).to(device).float().unsqueeze(<span class="hljs-number"><span class="hljs-number">0</span></span>))[<span class="hljs-number"><span class="hljs-number">0</span></span>].max(<span class="hljs-number"><span class="hljs-number">0</span></span>)[<span class="hljs-number"><span class="hljs-number">1</span></span>].view(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>).item()</code> </pre><br>  Since we use batchy for learning the neural network, we will need a buffer in which we will store the experience of interaction with the environment and from where we will choose batchy: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">Memory</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, capacity)</span></span></span><span class="hljs-function">:</span></span> self.capacity = capacity self.memory = [] self.position = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">push</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, element)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(self.memory) &lt; self.capacity: self.memory.append(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) self.memory[self.position] = element self.position = (self.position + <span class="hljs-number"><span class="hljs-number">1</span></span>) % self.capacity <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, batch_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> list(zip(*random.sample(self.memory, batch_size))) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__len__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> len(self.memory)</code> </pre><br><h2>  Naive solution </h2><br>  To begin with, we will declare constants that we will use in the learning process, and create a model: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 #    memory = Memory(5000) model, target_model, optimizer = create_new_model()</span></span></code> </pre><br>  Despite the fact that the interaction process would be logical to divide into episodes, to describe the learning process it is more convenient for us to divide it into separate steps, since we want to take one step of the gradient descent after each step of the environment. <br><br>  Let's talk more about how one step of learning looks like.  We will assume that we are now making a step with the step number from the max_steps steps and the current state.  Then committing an action with <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-5"> \ varepsilon </script>  -greedy policies would look like this: <br><br><pre> <code class="python hljs">epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action)</code> </pre><br>  Immediately add the experience to memory and start a new episode, if the current one is over: <br><br><pre> <code class="python hljs">memory.push((state, action, reward, new_state, done)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> done: state = env.reset() done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: state = new_state</code> </pre><br>  And take a step gradient descent (if, of course, we can already collect at least one batch): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer)</code> </pre><br>  Now it remains to update target_model: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model)</code> </pre><br>  However, we would also like to follow the learning process.  To do this, we will play an additional episode after each update of target_model with epsilon = 0, memorizing the total award in the rewards_by_target_updates buffer: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> step % target_update == <span class="hljs-number"><span class="hljs-number">0</span></span>: target_model = copy.deepcopy(model) state = env.reset() total_reward = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> done: action = select_action(state, <span class="hljs-number"><span class="hljs-number">0</span></span>, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> state = env.reset() rewards_by_target_updates.append(total_reward)</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Together</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  model   target model target_update = 1000 #  ,      batch_size = 128 #   max_steps = 100001 #  exploration max_epsilon = 0.5 min_epsilon = 0.1 def fit(): #    memory = Memory(5000) model, target_model, optimizer = create_new_model() for step in range(max_steps): #    epsilon = max_epsilon - (max_epsilon - min_epsilon)* step / max_steps action = select_action(state, epsilon, model) new_state, reward, done, _ = env.step(action) #  ,  ,   memory.push((state, action, reward, new_state, done)) if done: state = env.reset() done = False else: state = new_state #  if step &gt; batch_size: fit(memory.sample(batch_size), model, target_model, optimizer) if step % target_update == 0: target_model = copy.deepcopy(model) #Exploitation state = env.reset() total_reward = 0 while not done: action = select_action(state, 0, target_model) state, reward, done, _ = env.step(action) total_reward += reward done = False state = env.reset() rewards_by_target_updates.append(total_reward) return rewards_by_target_updates</span></span></code> </pre><br></div></div><br>  Run this code and get something like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7c7/4c5/7a3/7c74c57a34c63c0c6b42742414c35c86.png" alt="Straight line y = -200"><br><br><h2>  Something went wrong? </h2><br>  Is this a bug?  Is this the wrong algorithm?  Are these bad options?  Not really.  In fact, the problem is in the task, namely in the function of the reward.  Let's look at it more attentively.  At each step, our agent receives a -1 reward, and this happens until the episode ends.  This reward motivates the agent to complete the episode as quickly as possible, but at the same time does not tell him how to do it.  Because of this, the only way to learn how to solve a problem in such a formulation for an agent is to solve it many times, using exploration. <br><br>  Of course, one could try using more complex algorithms to explore the environment instead of our <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.348ex" height="2.419ex" viewBox="0 -780.1 4886 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-76" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="735" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-72" x="1265" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="1716" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-70" x="2183" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="2686" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="3156" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6C" x="3501" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="3800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6E" x="4285" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>v</mi><mi>a</mi><mi>r</mi><mi>e</mi><mi>p</mi><mi>s</mi><mi>i</mi><mi>l</mi><mi>o</mi><mi>n</mi></math></span></span><script type="math/tex" id="MathJax-Element-6"> \ varepsilon </script>  -greedy policies.  However, firstly, because of their application, our model will become more complex, which we would like to avoid, and secondly, not the fact that they will work well enough for this task.  Instead, we can eliminate the source of the problem by modifying the task itself, namely by changing the reward function, i.e.  applying what is called reward shaping. <br><br><h2>  Accelerate convergence </h2><br>  Our intuitive knowledge tells us that you need to accelerate to get on the hill.  The greater the speed, the closer the agent is to the task.  You can tell him about it, for example, by adding a speed module with some coefficient to the reward: <pre>  modified_reward = reward + 10 * abs (new_state [1]) </pre><br><br>  Accordingly, the line in the function fit <pre>  memory.push ((state, action, reward, new_state, done)) </pre>  should be replaced by <pre>  memory.push ((state, action, modified_reward, new_state, done)) </pre>  Now let's take a look at the new chart (it shows the <b>original</b> award without modifications): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e93/0bc/67f/e930bc67f90785f0e0b138aeb92742db.png" alt="Baseline comparison with RS"><br>  <i>Here RS is short for Reward Shaping.</i> <br><br><h2>  Is it good to do that? </h2><br>  The progress is obvious: our agent obviously learned to drive up the hill, because the reward began to differ from -200.  There is only one question left: if by changing the reward function, we changed the task itself, would the solution of the new problem we found be good for the old task? <br><br>  To begin with, we will understand what ‚Äúgoodness‚Äù means in our case.  Solving the problem, we are trying to find the optimal policy - one that maximizes the total reward for the episode.  In this case, we can replace the word ‚Äúgood‚Äù with the word ‚Äúoptimal‚Äù, since we are looking for it.  We also optimistically hope that our DQN will sooner or later find the optimal solution for the modified problem, and not get stuck in a local maximum.  So, the question can be reformulated as follows: if by changing the reward function, we changed the task itself, would the optimal solution we found be optimal for the old problem? <br><br>  As it turns out, we cannot provide such a guarantee in the general case.  The answer depends on how we changed the function of the reward, how it was arranged earlier and how the environment itself is arranged.  Fortunately, there is <a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf">an article</a> whose authors investigated how changing the reward function affects the optimality of the found solution. <br><br>  First, they found a whole class of ‚Äúsafe‚Äù changes that are based on the potential method: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy=&quot;false&quot;>(</mo><mtext>&amp;#xA0;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&amp;#xA0;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mi>e</mi><mi>w</mi><msub><mtext>&amp;#xA0;</mtext><mi>s</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2212;</mo><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy=&quot;false&quot;>)</mo><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="55.164ex" height="2.66ex" viewBox="0 -832 23751.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-28" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-67" x="5010" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="5490" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="6020" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6D" x="6898" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="7777" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-63" x="8556" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-64" x="8990" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6F" x="9513" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="9999" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-50" x="10610" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-68" x="11362" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="11938" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-28" x="12284" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-6E" x="12673" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="13274" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-77" x="13740" y="0"></use><g transform="translate(14457,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="353" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="15139" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="15500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="16030" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="16391" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-29" x="16858" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2212" x="17470" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-50" x="18720" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-68" x="19472" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="20048" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-28" x="20394" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="20783" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="21253" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="21614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-74" x="22144" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-65" x="22505" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-29" x="22972" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-29" x="23361" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mo stretchy="false">(</mo><mtext>&nbsp;</mtext><mi>g</mi><mi>a</mi><mi>m</mi><mi>m</mi><mi>a</mi><mtext>&nbsp;</mtext><mi>c</mi><mi>d</mi><mi>o</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>w</mi><msub><mtext>&nbsp;</mtext><mi>s</mi></msub><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy="false">)</mo><mo>‚àí</mo><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi><mo stretchy="false">(</mo><mi>s</mi><mi>t</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-7"> R ‚Äô= R + (\ gamma \ cdot \ Phi (new \ _state) - \ Phi (state)) </script>  where <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.468ex" height="2.057ex" viewBox="0 -780.1 1923.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-50" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-68" x="1001" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-69" x="1578" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>P</mi><mi>h</mi><mi>i</mi></math></span></span><script type="math/tex" id="MathJax-Element-8"> \ Phi </script>  - potential, which depends only on the state.  For such functions, the authors were able to prove that if the solution for the new problem is optimal, then for the old problem it is also optimal. <br><br>  Second, the authors showed that for any other <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>R</mi><mo>&amp;#x2032;</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="17.056ex" height="2.66ex" viewBox="0 -832 7343.5 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-52" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2032" x="1074" y="513"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-3D" x="1332" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-52" x="2388" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2B" x="3370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-46" x="4370" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-28" x="5120" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-73" x="5509" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-2C" x="5979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMATHI-61" x="6424" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/hsespb/blog/444428/&amp;xid=17259,15700002,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhhSf-x2JQQK8nbcMJ4vaSJen9LSSg#MJMAIN-29" x="6953" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>R</mi><mo>‚Ä≤</mo></msup><mo>=</mo><mi>R</mi><mo>+</mo><mi>F</mi><mo stretchy="false">(</mo><mi>s</mi><mo>,</mo><mi>a</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-9"> R ‚Äô= R + F (s, a) </script>  there is such a problem, the reward function R and the optimal solution of the modified problem, that this solution is not optimal for the original problem.  This means that we cannot guarantee the goodness of the solution we found if we use a change that is not based on the method of potentials. <br><br>  Thus, the use of potential functions to modify the reward function can only change the rate of convergence of the algorithm, but does not affect the final decision. <br><br><h2>  Accelerate the convergence correctly </h2><br>  Now that we know how to safely change the reward, we will try to modify the task again using the potential method instead of naive heuristics: <pre>  modified_reward = reward + 300 * (gamma * abs (new_state [1]) - abs (state [1])) </pre><br>  Look at the graph of the original award: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b8f/e05/c21/b8fe05c211b3bcd166b7d768da48f6ea.png" alt="Graphic comparison of baseline, RS and RS with potentials"><br><br>  As it turned out, in addition to the availability of theoretical guarantees, modification of the reward with the help of potential functions also significantly improved the result, especially in the early stages.  Of course, there is a possibility that it would be possible to choose more optimal hyperparameters (random seed, gamma and other coefficients) for training the agent, however, reward shaping still significantly increases the rate of convergence of the model. <br><br><h2>  Afterword </h2><br>  Thank you for reading to the end!  I hope you enjoyed this small practice-oriented excursion to training with reinforcements.  It is clear that Mountain Car is a ‚Äútoy‚Äù task, however, as we could see, it can be difficult to teach an agent to solve even this seemingly simple task from a human point of view. </div><p>Source: <a href="https://habr.com/ru/post/444428/">https://habr.com/ru/post/444428/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../444416/index.html">HyperX Alloy CORE - when the membrane can in games</a></li>
<li><a href="../444418/index.html">Millions of binaries later. How to get stronger Linux</a></li>
<li><a href="../444420/index.html">How to ride the two wheels to work</a></li>
<li><a href="../444422/index.html">As it was in 2018: Industrial FDM printing at Top 3D Expo</a></li>
<li><a href="../444426/index.html">Lyft and Uber go IPO. Why invest in Lyft?</a></li>
<li><a href="../444430/index.html">Parsing: how to properly use Present Perfect in English</a></li>
<li><a href="../444432/index.html">The use of Linux and open source software in our school: to be or not to be?</a></li>
<li><a href="../444434/index.html">The time has come java 12! Review of hot JEPs</a></li>
<li><a href="../444436/index.html">What is the Mirai botnet, and how can I protect my devices?</a></li>
<li><a href="../444438/index.html">A brief history of open source - how free software has fought proprietary</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>