<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. EMC XtremIO</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the middle of 2012, EMC paid $ 430 million for an Israeli startup that was opened three years earlier. At the development stage, in fact, half a ye...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. EMC XtremIO</h1><div class="post__text post__text-html js-mediator-article">  In the middle of 2012, EMC paid $ 430 million for an Israeli startup that was opened three years earlier.  At the development stage, in fact, half a year before the expected appearance of the first XtremIO device.  To order, the first devices became available only at the end of 2013. <br><br>  The main distinguishing feature of XtremIO lies in its <a href="http://russia.emc.com/storage/xtremio/technology.htm">architecture and functionality</a> .  First, the architecture initially includes constantly running and non-switchable services, such as inline deduplication, compression, and thin provisioning, which save space on the SSD.  Secondly, XtremIO is a horizontally-scalable cluster of modules (X-Bricks), between which data is automatically and evenly distributed.  At the same time, the standard x86 equipment and SSD are used, and the functionality is implemented by software.  As a result, it turns out not just a fast disk, but an array that allows you to save capacity due to deduplication and compression, especially in such tasks as server virtualization, VDI or databases with multiple copies. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9f9/63e/fd7/9f963efd7bbb46f966ecae72c57fb77b.jpg" height="310" width="640"></div><br>  Love for different kinds of tests is not EMC's strength.  Nevertheless, thanks to the initiative assistance of the local office, for us, in the depths of the remote laboratory, a stand was assembled that included 2 X-Brick systems.  That allowed us to conduct a series of tests as close as possible to the <a href="http://habrahabr.ru/company/inline_tech/blog/227885/">method developed by us</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Testing was conducted on version 2.4 code, version 3.0 is now available, in which half the delay is stated. <br><a name="habracut"></a><br><br><h3>  <b>Testing method</b> </h3><br>  During testing, the following tasks were solved: <br><ul><li>  studies of the process of storage degradation during long-term write load (Write Cliff); </li><li>  performance study of EMC XtremIO storage systems under various load profiles; </li></ul><br><h4>  Testbed Configuration </h4><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f7/6d6/0a6/5f76d60a6af8d8f417024bc2131a3873.jpg" height="401" width="640"></div></td></tr><tr><td>  Figure 1. The block diagram of the test bench. </td></tr></tbody></table>  The test bench consists of 4 servers, each of which is connected by four 8Gb connections to 2 FC switches.  Each switch has 4 8Gb FC connections to EMC Xtream-IO storage.  On the FC switches, zones are created in such a way that each initiator is in the zone with each storage port. <br>  <abbr title="Cisco C260M1 Processor: Intel Xeon E5 8 Core - 4 pcs RAM: 128GbFC adapter: Emulex 12002e 8Gb 2 port (fw U3D2.01A12) - 2 pcs: RHEL 6.3 x86-64 Kernel: 2.6.32-279.el6.x86_64">Server</abbr> ; <br>  <abbr title="EMC XtremIO (Dual-Brick ie 2 X-Brick blocks) Raw Capacity: ~ 20TB (10TB on X-Brick) Controllers: 4 pieces (2 on X-Brick) Formatted Capacity: 14.9TB (7 each , 45 on X-Brick) Cache size: 512 GB (256 for X-Brick, 128 for each controller) Connection interfaces: total 8x8Gb FC, 8x10Gb iSCSI, 8x40Gb Infiniband (for interconnect when using 2-4 X-Brick in the system), 4x1Gb Ethernet (for management) Number of flash modules (data + HS): 46 + 4 (23 + 2 on the X-Brick) Firmware version: 2.4.1 build 12 - the last at the time of testing.">Storage system</abbr> <br>  As additional software, Symantec Storage Foundation 6.1 is installed on the test server, which implements: <br><ul><li>  Functional logical volume manager (Veritas Volume Manager); </li><li>  Functional fault-tolerant connection to disk arrays (Dynamic Multi Pathing). </li></ul><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  On the test server, the following settings were made to reduce disk I / O latency: <br><ul><li> Changed the I / O scheduler from ‚Äúcfq‚Äù to ‚Äúnoop‚Äù by assigning the value to the noop parameter; <code>/sys/&lt;___Symantec_VxVM&gt;/queue/scheduler</code> </li><li>  The following parameter has been added to <code>/etc/sysctl.conf</code> minimizes the queue size at the level of the Symantec logical volume manager: <code>¬´vxvm.vxio.vol_use_rq = 0¬ª</code> ; </li><li>  The limit of simultaneous I / O requests to the device is increased to 1024 by assigning the value 1024 to the parameter <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nr_requests</code> </li><li>  Disabled checking of the possibility of merging i / v operations (iomerge) by assigning the value 1 to the parameter <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nomerges</code> </li><li>  Disable read-ahead by setting the value 0 to <code>/sys/&lt;___Symantec_VxVM&gt;/queue/read_ahead_kb</code> </li><li>  The default (30) queue size on the FC HBA is used; </li></ul><br>  On the storage system, the following configuration settings are performed for partitioning disk space: <br><ul><li>  On storage, by default, all space is marked up and there is the possibility of only a logical partitioning of physical capacity. </li><li>  32 LUNs of the same size are created on the storage system, cumulatively occupying 80% of the storage capacity, the moons are presented with 8 each server. </li></ul><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (performance of synthetic tests) on the storage system, the Flexible IO Tester (fio) version 2.1.4 utility is used.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li>  direct = 1 </li><li>  size = 3T </li><li>  ioengine = libaio </li><li>  group_reporting = 1 </li><li>  norandommap = 1 </li><li>  time_based = 1 </li><li>  randrepeat = 0 </li></ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  interface and means of monitoring and diagnostics storage; </li><li>  fio version 2.1.4 to generate a summary report for each load profile </li></ul><br></div></div><br><h3>  Testing program. </h3><br>  The tests were performed by creating a synthetic load simultaneously from four servers using the fio program on a Block Device, which is a logical volume of type <code>stripe, 8 column, stripewidth=1MiB</code> , created using Veritas Volume Manager from 8 LUNs presented from the system under test on each server.  The created volumes are previously completely filled with data. <br>  Testing consisted of 2 groups of tests: <br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h5>  <b>Group 1: Tests that implement long-term random write.</b> </h5><br>  When creating a test load, the following additional parameters of the fio program are used: <br><ul><li>  rw = randwrite </li><li>  blocksize = 4K </li><li>  numjobs = 10 </li><li>  iodepth = 8 </li></ul><br>  The test duration is 18 hours. <br>  According to the test results, based on the data output by the vxstat command, graphs are generated that combine the test results: <br><ul><li>  IOPS as a function of time; </li><li>  Latency as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  The presence of performance degradation during long-term load on the record and read; </li><li>  The performance of the service processes storage (Garbage Collection) limiting the performance of the disk array to write during a long peak load; </li></ul><br><h5>  <b>Group 2: Disk array performance tests under various types of load.</b> </h5><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters fio: randomrw, rwmixedread): </li></ul><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter fio: blocksize); </li><li>  methods of processing I / O operations: asynchronous (variable software parameter fio: ioengine); </li></ul><br>  Tests are performed in 3 stages: <br><ul><li>  For each combination of the above types of load, by varying the fio numjobs and iodepth parameters, there is a storage saturation point, that is, a combination of jobs and iodepth, at which the maximum iops is reached, but the delay is minimal.  Recorded by the program fio indicators iops, latency, numjobs and qdepth. </li><li>  then the tests are carried out similarly to the previous stage, only the point at which approximately twice the performance is achieved is searched. </li><li>  then similar tests are carried out, the point is still half the performance. </li></ul><br>  Such an algorithm allows testing to determine the maximum performance of the disk array for a given load profile, as well as the dependence of latency on the load. <br>  According to the test results, graphs and tables of the obtained iops and latency are built, the results are analyzed and conclusions are drawn about the performance of the storage system. <br></div></div><br><h2>  <b>Test results</b> </h2><br>  Investigation of disk array performance on synthetic tests. <br><br><h5>  <b>Group 1: Tests that implement long-term random write.</b> </h5><br>  The test results are presented in the form of graphs (Figure 2 and 3). <br><div class="spoiler">  <b class="spoiler_title">View graphics.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/075/0aa/56e/0750aa56e2a9c66e227a63b758bdd626.jpg" height="262" width="640"></div></td></tr><tr><td>  Figure 2. IOPS during long-term recording (4K block) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/550/2aa/23e/5502aa23ef61257341b79573934cc35b.jpg" height="262" width="640"></div></td></tr><tr><td>  Figure 3. Delay during long recording (4K block) </td></tr></tbody></table><br></div></div><br>  Main conclusions: <br>  During long-term load, the performance drop was not recorded with time.  The phenomenon of "Write Cliff" is missing.  Therefore, when choosing a disk subsystem (sizing), you can count on stable performance regardless of the load duration (usage history of the disk array). <br><br><h5>  <b>Group 2:</b> <b>Disk array performance tests under various types of load.</b> </h5><br>  The test results are presented in the form of graphs (Fig. 4-9) and summarized in Tables 1-3. <br><div class="spoiler">  <b class="spoiler_title">View charts and tables.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db5/def/d79/db5defd790f4573c08261268b3d3ded2.jpg" height="324" width="640"></div></td></tr><tr><td>  Figure 4. IOPS with random write. </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e50/0fb/ae2/e500fbae2c07208f925f50c6a04b706e.jpg" height="322" width="640"></div></td></tr><tr><td>  Figure 5. Delay in random recording.  (ms) </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/694/e12/a99/694e12a99f763487f8ecd33d9583dd1a.png" height="240" width="640"></div></td></tr><tr><td>  Table 1. Random write performance </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ca/7e0/eab/3ca7e0eaba2675ace219cd015216f0a6.jpg" height="322" width="640"></div></td></tr><tr><td>  Figure 6. IOPS with mixed IV (70% read 30% write) </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b4c/168/47b/b4c16847beb1b946beff1c249c2d72b6.jpg" height="326" width="640"></div></td></tr><tr><td>  Figure 7. Delay in mixed I / O (70% read 30% write) (ms) </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3f3/826/b9e/3f3826b9e9b5367bd64194b5930f25ab.png" height="240" width="640"></div></td></tr><tr><td>  Table 2. Performance with mixed IV (70% reading 30% record) </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/432/7ed/36b/4327ed36b6b601ed5e05ccffe903f0d6.jpg" height="320" width="640"></div></td></tr><tr><td>  Figure 8. IOPS with random reading. </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ac5/0e9/eee/ac50e9eeeaac4e892d25e77a7f98a82e.jpg" height="316" width="640"></div></td></tr><tr><td>  Figure 9. Delay in random reading.  (ms) </td></tr></tbody></table><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/967/5e1/4ef/9675e14ef59543457128d6cd2596dfbf.png" height="260" width="640"></div></td></tr><tr><td>  Table 3. Random read performance. </td></tr></tbody></table><br></div></div><br><h6>  <b>Maximum recorded storage performance parameters:</b> </h6><br>  Record: <br><ul><li>  160000 IOPS with latency 1.2ms (4K block); </li></ul><br>  Reading: <br><ul><li>  455000 IOPS with latency 1,4ms (4K jobs = 10 qd = 16); </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  264000 IOPS with latency 1,15ms (4K jobs = 8 qd = 10); </li></ul><br><br><ul><li>  The storage system provides declared performance with acceptable latency. </li><li>  The delays obtained cannot be considered minimal, since  Testing was performed only in an asynchronous manner, in / in with a queue depth greater than 1. </li><li>  On the 1MB block, the performance is lower than expected.  Further studies showed that with tests of 1MB block on the storage system, 256KB blocks of operations were performed.  This means that the data blocks before sending were shared on the server, which led to a drop in performance.  The reason for this was the use of the swidth = 1m parameter instead of stripeunit = 1m when creating a Symantec VxVM volume. </li><li>  In case of random recording, the performance change with increasing block size occurs linearly, which allows to conclude that the storage bandwidth (recording bandwidth) is limited to approximately 700-750MB / s. </li></ul><br>  As an added bonus, we were shown the performance of ‚Äúsnapshots‚Äù.  The 32 snapshot shots taken simultaneously from the 32-max, maximally loaded for recording, the moons did not have a visible impact on performance, which can not but arouse admiration of the system architecture. <br><br><h2>  <b>findings</b> </h2><br>  Summing up, we can say that the array made a favorable impression and demonstrated the iops declared by the manufacturer.  Compared to other flash solutions, it is distinguished by: <br><ol><li>  No performance degradation on write operations (write cliff) </li><li>  Online deduplication, allowing not only to save in volume, but also to win in the recording speed. </li><li>  The snapshot function and their impressive performance. </li><li>  Scalability from 1 to 4 nodes (X-brick) </li></ol><br>  The unique architecture and data processing algorithms of the array allow you to implement great functionality (deduplication, snapshots, thin moons). <br><br>  Unfortunately, the testing was not complete.  After all, maximum performance is expected from a system consisting of 4 X-Brick.  In addition, version 2.4 was provided to us, while version 3.0 was already released, with half the delay stated.  As before, the issues of the operation of an array with large blocks (its maximum throughput) and work with synchronous I / O, where latency is critical, remain unclear.  We hope that soon we will be able to close all these "white spots" with additional research. <br><br><br>  <font color="green">PS The author expresses cordial thanks to Pavel Katasonov, Yuri Rakitin and all other company employees who participated in the preparation of this material.</font> </div><p>Source: <a href="https://habr.com/ru/post/252069/">https://habr.com/ru/post/252069/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../252059/index.html">High Frequency Trading Neighborhood - Part V (Final)</a></li>
<li><a href="../252061/index.html">Chinese HID USBISP Programmer (USBASP) in Linux. Preprogramming</a></li>
<li><a href="../252063/index.html">How many interface designs do you really need to draw for iPhone 4, 5, 6, and 6+?</a></li>
<li><a href="../252065/index.html">Tarantool 1.6 POV</a></li>
<li><a href="../252067/index.html">Writing a search plugin for Elasticsearch</a></li>
<li><a href="../252073/index.html">11 cool sites for iOS developers</a></li>
<li><a href="../252075/index.html">An interesting and at the same time simple slider on pure CSS3</a></li>
<li><a href="../252077/index.html">Discrete structures: matan for IT professionals</a></li>
<li><a href="../252079/index.html">DuoCode: we translate C # in JavaScript</a></li>
<li><a href="../252083/index.html">GoogleFit API - start and see the result</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>