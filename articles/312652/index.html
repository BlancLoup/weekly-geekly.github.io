<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating shared storage based on CEPH RBD and GFS2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Most cluster system software assumes that the file system is accessible from all cluster nodes. This file system is used to store software, data, to o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating shared storage based on CEPH RBD and GFS2</h1><div class="post__text post__text-html js-mediator-article">  Most cluster system software assumes that the file system is accessible from all cluster nodes.  This file system is used to store software, data, to organize the work of some cluster subsystems, etc.  The performance requirements of such an FS can vary greatly for different tasks, however, the higher it is, the more it is considered that the cluster is more stable and versatile.  The NFS server on the master node is the minimum variant of such an FS.  For large clusters, NFS is complemented by the deployment of LusterFS, a high-performance, specialized distributed file system that uses multiple servers as file storage and several meta-information servers.  However, this configuration has a number of properties that make it difficult to work with it when clients use independent virtualized clusters.  The HPC HUB vSC system uses the well-known CEPH solution and the GFS2 file system to create a shared FS. <br><img src="https://habrastorage.org/files/cdf/5b6/bc6/cdf5b6bc69734193b2c4cf36a8f6e5e7.jpg" alt="main"><br><a name="habracut"></a><br>  As part of the work on the HPC HUB project, based on OpenStack, the task arose of creating fault-tolerant and high-performance shared storage for several groups of virtual machines, each of which is a small virtual HPC cluster.  Each such cluster is issued to a separate client, which from the inside completely controls it.  Thus, the task cannot be solved within the framework of a ‚Äúnormal‚Äù distributed system assembled on physical computers, since  Ethernet networks of clusters are isolated from each other at the L2 level to protect customer data and their network traffic from each other.  At the same time, data cannot be placed on each of the virtual machines within the framework of the classic shared system, because each machine lacks the capabilities and wants to free up valuable processor resources immediately after the calculations, without spending it on ‚Äúdata transfusion‚Äù, especially taking into account the use of the pay-per model -use.  Fortunately, not everything is so bad.  In light of the fact that shared storage is supposed to be used for HPC calculations, you can make several assumptions about the load: <br><br><ul><li>  operations will be performed in large blocks; </li><li>  write operations will not be frequent, but will be interspersed with time significant intervals; </li><li>  write operations can begin immediately from all nodes almost simultaneously, but even in this case, you want to finish the recording as quickly as possible so that the nodes continue counting. </li></ul><br>  A peer-to-peer system with nodes on which the calculation is made and data are simultaneously stored contradicts the HPC paradigm in principle, since  Two un-synchronized activities appear on the node: an account and data recording from some other node.  We were forced to abandon it initially.  It was decided to build a two-tier system.  There are several approaches to such a repository: <br><br><ol><li>  Distributed file system mounted on a virtual node through a specially organized network device to avoid performance losses from OpenStack network virtualization. <br><img src="https://habrastorage.org/files/496/07d/369/49607d36900e414d9d8e24439e441624.png" alt="sch1"></li><li>  A distributed file system mounted on a physical node and accessed from the guest system via a virtual file system. <br><img src="https://habrastorage.org/files/3ca/517/276/3ca5172767fa4ec8a63d65843d30c6c0.png" alt="sch2"></li><li>  Classic distributed file system inside a virtual cluster on top of a guest block device. <br><img src="https://habrastorage.org/files/76c/f61/b67/76cf61b670bf41d984eae889becee102.png" alt="sch3"></li><li>  A distributed block device, common to each virtual node, and a pre-configured file system within a virtual cluster, running on top of this shared device. <br><img src="https://habrastorage.org/files/9ad/5bc/b05/9ad5bcb0500c44f1907e034079628a94.png" alt="sch4"></li></ol><br>  Let's try to analyze the advantages and disadvantages of these approaches. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In option 1) the classic distributed file system (or some subtree) is mounted to the guest OS.  In this case, the file system servers are visible from each system and each other.  This breaks network isolation at L2 level and potentially allows different users to see each other‚Äôs traffic with stored data.  In addition, for the case of CEPH, only cooperative quotas are possible (that is, the user can reuse the allowable disk space at the expense of other users) in the file system mount mode via FUSE (filesystem in userspace), and not directly in the kernel.  When mounted directly in the kernel, quotas are not supported at all. <br><br>  In option 2) - a virtual file system (virtio-9p in QEMU) on top of a distributed file system mounted in a host - to work, you need to run QEMU on behalf of the superuser without lowering privileges, which reduces the overall security of the system or configure specific ACLs to store guest IDs users and guest access rights. <br><br>  In option 3) in the absence of disk space on the physical servers, we can use only remote block devices of some kind of distributed storage.  This option is not advisable for several reasons: <br><br><ul><li>  data access will require 2 times more network bandwidth.  Data is first transferred from the distributed storage to a specific virtual node and only then sent to the final consumer; <br><br></li><li>  as a rule, all such systems support data redundancy, which is unacceptable.  For example, with the default settings for CEPH, the usable storage capacity is already reduced by 4 times the available disk space.  All network loads that support distributed FS will also be inside the virtual network space.  And, what is the most difficult, the latter option will actually require the allocation of physical nodes for storage virtualke, which leads to a sharp decrease in overall efficiency. </li></ul><br>  In option 4) we decided to try a distributed block device based on CEPH and some kind of file system within the cluster working with such a device.  In fact, the guest, of course, sees the device not as a CEPH block device, but as an ordinary virtual VirtIO SCSI or, optionally, VirtIO block.  We chose VirtIO SCSI for a very simple reason - because of the SCSI support for the unmap command, an analog of the well-known SATA TRIM command, which is sent by the guest when deleting files and freeing up space in the virtual storage. <br><br>  Choosing a file system for a guest is also easy.  There are only two options: <br><br><ul><li>  OCFS2, </li><li>  Gfs2 </li></ul>  Exotics, which are not supported in mainstream Linux, were not used for commercial exploitation.  Since CentOS 7 is taken as the base system, OCFS2 also disappears.  There was no desire to engage in a painstaking reassembly of the core.  In addition, GFS2 supports resizing the file system in a mounted state, which can be useful for our case. <br><br><h2>  <font color="#808080">Shared Storage Configuration</font> </h2><br>  The installation of CEPH is perfectly described in the <a href="http://docs.ceph.com/docs/master/start/">docs.ceph.com/docs/master/start</a> documentation and did not cause any special problems.  Currently, the system works with one redundant copy of data and without authorization.  The CEPH network is isolated from the client network.  Creating a block device also did not cause problems.  Everything is standard. <br><br><h2>  <font color="#808080">GFS2 configuration</font> </h2><br>  Unfortunately, setting up with GFS2 took a lot longer than originally estimated.  Explanatory descriptions in the network is extremely small and they are confused.  The general idea looks something like this.  The work of the shared file system in the kernel is controlled by several demons, the correct setting of which requires nontrivial effort or knowledge. <br><br>  The first and most important thing to know is that you most likely will not be able to raise this file system on Ubuntu.  Or you will have to rebuild a lot of little-used packages outside of RedHat under Ubuntu, allowing a bunch of non-trivial API-level conflicts. <br><br>  Under a RedHat-like system, the sequence is more or less clear.  Below are the commands for the guest CentOS 7. <br><br>  First off, turn off SELinux.  GFS2 will not work with it.  This is official information from the manufacturer. <br><br><pre><code class="bash hljs">vi /etc/sysconfig/selinux <span class="hljs-comment"><span class="hljs-comment">#  ,     </span></span></code> </pre> <br>  Put the basic software: <br><br><pre> <code class="bash hljs">yum -y install ntp epel-release vim openssh-server net-tools</code> </pre> <br>  Enable / disable the necessary services: <br><br><pre> <code class="bash hljs">chkconfig ntpd on chkconfig firewalld off</code> </pre> <br>  NTP is necessary for the operation of the system; all distributed locks of areas of the block device are tied to it.  The system is configured for a cluster isolated from the outside world, therefore, we disable firewalls.  Next, we put the necessary software binding on each of the cluster nodes: <br><br><pre> <code class="bash hljs">yum -y install gfs2-utils lvm2-cluster pcs fence-agents-all chkconfig pcsd on <span class="hljs-comment"><span class="hljs-comment">#  PaceMaker -  RedHat  lvmconf --enable-cluster #    GFS2    echo &lt;PASSWORD&gt; | passwd --stdin hacluster #    .   :) reboot #   SELinux    LVM</span></span></code> </pre> <br>  Please note that you do not need to create the user 'hacluster', it is created by itself when installing packages.  In the future, you will need his password to create a network of mutual authorization between the cluster machines. <br><br>  You can now create distributed storage if this has not been done before.  It is done on any CEPH node, that is, in our case, on a physical node: <br><br><pre> <code class="bash hljs">rbd create my-storage-name --image-format 2 --size 6291456 <span class="hljs-comment"><span class="hljs-comment">#   ! sudo rbd map rbd/my-storage-name sudo mkfs.gfs2 -p lock_dlm -t gfs2:fs -j17 /dev/rbd0 sudo rbd unmap /dev/rbd0</span></span></code> </pre> <br>  When formatting a file system, the cluster name is specified - in our case, 'gfs2' and the name of the file system within this cluster is 'fs', the number of journals is '-j17', which should be equal to the number of nodes (in our case, 17) that simultaneously work with this cluster and lock protocol for allocating disk space (in our case, 'lock_dlm' is distributed locking).  The names of the cluster and file system will have to be specified when mounting the partition.  In an isolated network within a cluster, you can use the same names for different clusters.  This is not a problem. <br><br>  Now it remains only to configure the mount in the guest OS.  The configuration is performed with one of the cluster machines once. <br><br><pre> <code class="bash hljs">pcs cluster destroy --all <span class="hljs-comment"><span class="hljs-comment">#   ,    </span></span></code> </pre> <br>  Creating a mutual authorization network: <br><br><pre> <code class="bash hljs">pcs cluster auth master n01 n02 n03 n04 -u hacluster -p 1q2w3e4r --force</code> </pre> <br>  here, master and n01..n04 are virtual hosts on which a shared partition will be available. <br><br>  Create a default cluster.  Note that the cluster name must match the one used when creating the file system in the previous step. <br><pre> <code class="bash hljs">pcs cluster setup --name gfs2 master n01 n02 n03 n04 pcs cluster start --all <span class="hljs-comment"><span class="hljs-comment">#     pcs cluster enable --all #      </span></span></code> </pre> <br>  Running service daemons - clvmd &amp; dlm: <br><br><pre> <code class="bash hljs">pcs property <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> no-quorum-policy=ignore pcs stonith create local_stonith fence_kdump pcmk_monitor_action=<span class="hljs-string"><span class="hljs-string">"metadata"</span></span> pcs resource create dlm ocf:pacemaker:controld op monitor interval=30s \ on-fail=fence <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> ordered=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs resource create clvmd ocf:heartbeat:clvm op monitor interval=30s \ on-fail=fence <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> ordered=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs constraint order start dlm-clone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> clvmd-clone pcs constraint colocation add clvmd-clone with dlm-clone</code> </pre> <br>  Mounting a GFS2 partition in / shared, shared block device - sdb: <br><br><pre> <code class="bash hljs">pcs resource create clusterfs Filesystem device=<span class="hljs-string"><span class="hljs-string">"/dev/sdb"</span></span> \ directory=<span class="hljs-string"><span class="hljs-string">"/shared"</span></span> fstype=<span class="hljs-string"><span class="hljs-string">"gfs2"</span></span> <span class="hljs-string"><span class="hljs-string">"options=noatime,nodiratime"</span></span> op \ monitor interval=10s on-fail=fence <span class="hljs-built_in"><span class="hljs-built_in">clone</span></span> interleave=<span class="hljs-literal"><span class="hljs-literal">true</span></span> pcs constraint order start clvmd-clone <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> clusterfs-clone pcs constraint colocation add clusterfs-clone with clvmd-clone</code> </pre> <br>  From this point on, you can enjoy the start of the whole system, raising services one by one with the help of the command: <br><br><pre> <code class="bash hljs">pcs status resources</code> </pre> <br>  In the end, if everything worked out right for you, you should see that the / shared file system will be available on each node. <br><br><h2>  <font color="#808080">Performance tests</font> </h2><br>  For the tests, a simple script was used that sequentially reads and writes data from each of the nodes via dd, for example: <br><br><pre> <code class="bash hljs">dd <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=/shared/file of=/dev/null bs=32M iflag=direct dd <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=/root/file of=/shared/file bs=32M oflag=direct</code> </pre> <br>  The block size is set large, reading is done in the 'direct' mode to test the file system itself, and not the speed of working with disk cache.  The results were as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/93c/ffb/010/93cffb010d2442fca69224700aa45765.png" alt="pic1"></div><br>  <em>Fig.1.</em>  <em>Simultaneous reading from all nodes of virtual clusters of different sizes.</em>  <em>Performance per node.</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/5cf/89a/5d2/5cf89a5d2e104846a556065a8269d583.png" alt="pic2"></div><br>  <em>Fig.2.</em>  <em>Simultaneous reading from all nodes of virtual clusters of different sizes.</em>  <em>Total performance</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e8b/989/e5f/e8b989e5f6f74d67a4131169b399e7cb.png" alt="pic3"></div><br>  <em>Fig.3.</em>  <em>Simultaneous recording from all nodes of virtual clusters of different sizes.</em>  <em>Performance per node.</em> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/c43/78d/c6b/c4378dc6bdd64771ba70bd720c09ae46.png" alt="pic4"></div><br>  <em>Fig.4.</em>  <em>Simultaneous recording from all nodes of virtual clusters of different sizes.</em>  <em>Total performance</em> <br><br><h2>  <font color="#808080">findings</font> </h2><br>  What conclusions can be drawn: <br><br><ul><li>  reading almost rests on network bandwidth (~ 9 Gbps, Infiniband, IPoIB) and scales well with the increase in the number of nodes of the virtual cluster; <br><br></li><li>  the record rests on the practical ceiling and does not scale.  But given the assumptions made, it suits us so far.  The mechanism that led to the existence of such a ceiling remains unclear to us. </li></ul><br><h2>  <font color="#808080">Underwater rocks</font> </h2><br>  The pitfalls include the need to correctly shut down at least the last of all virtual machines using a cluster.  Otherwise, the file system may be in a state that requires recovery.  A serious problem is also the correct setting of fencing to disconnect from the pcs cluster of a node that has lost synchronization with others.  But more about that in the following articles. <br><br>  The material was prepared by Andrey Nikolaev, Denis Lunev, Anna Subbotina. </div><p>Source: <a href="https://habr.com/ru/post/312652/">https://habr.com/ru/post/312652/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../312638/index.html">Using ES6 generators using the example of koa.js</a></li>
<li><a href="../312640/index.html">How we filtered bots and lowered the bounce rate from 90% to 42%</a></li>
<li><a href="../312642/index.html">Unicode: the necessary practical minimum for each developer</a></li>
<li><a href="../312646/index.html">Anti-pager for paranoiac in Safari</a></li>
<li><a href="../312648/index.html">Administrator's summary: the difference between a microscope and a hammer when building a SAN (updated)</a></li>
<li><a href="../312654/index.html">Blockchain decomposition</a></li>
<li><a href="../312656/index.html">Two-factor authentication in Redmine</a></li>
<li><a href="../312658/index.html">Why is it important to check the values ‚Äã‚Äãreturned by the function?</a></li>
<li><a href="../312660/index.html">Free seminar "Operation of the data center: what you need to do yourself", October 27, Moscow</a></li>
<li><a href="../312662/index.html">PHP multithreaded server implementation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>