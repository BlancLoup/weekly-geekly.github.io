<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Generate images from text using AttnGAN</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I present to you the translation of the article " AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Net...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Generate images from text using AttnGAN</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  I present to you the translation of the article " <a href="https://arxiv.org/pdf/1711.10485.pdf">AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks</a> " by Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He. <br><br>  In this publication, I want to talk about my experiments with the AttnGAN architecture for generating images from a text description.  This architecture was already mentioned on Habr√© after the release of the original article in early 2018, and I was interested in the question - how difficult would it be to train such a model on my own? <br><br><h3>  Architecture Description </h3><br>  For those who are not familiar with AttnGAN and the classic GAN, I will briefly describe the essence.  Classic GAN consists of at least 2 neural networks - a generator and a discriminator.  The task of the generator is to generate some data (images, text, audio, video, etc.), ‚Äúsimilar‚Äù to real data from dataset.  The task of the discriminator is to evaluate the generated data, an attempt to compare them with the real ones and reject them.  The rejected result of the generator stimulates it to produce the best result in order to ‚Äúfool‚Äù the discriminator, which, in turn, learns to better recognize the fakes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      There are a great many modifications of GAN, and the AttnGAN authors approached the issue of architecture very ingeniously.  The model consists of 9 neural networks that are finely tuned for interaction.  It looks like this: <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/tk/eq/nq/tkeqnqzoqcw9dt9giju0rdsh4zg.png"><br><br>  Text and image encoders (text / image encoder) convert the source text description and the actual images into some kind of internal representation.  Characteristically, in this case, the text is considered as a sequence of individual words, the representation of which is processed together with the representation of the image, which makes it possible to compare individual words to separate parts of the image.  Thus, the attention mechanism, named by the authors of the DAMSM article, is realized. <br><br>  Fca - creates a concise view of the overall scene in the image, based on the entire text description.  The value of C at the output is concatenated with a vector from the normal distribution of Z, which defines the variability of the scene.  This information is the basis for the operation of the generator. <br><br>  The generator is the largest network consisting of three levels.  Each level generates images of increasing resolution, from 64 * 64 to 256 * 256 pixels, and the result of work at each level is corrected using Fattn attention networks, which carry information about the correct location of individual objects in the scene.  In addition, the results at each level are checked by three separately working discriminators, which assess the realism of the image and the consistency of its overall view of the scene. <br><br><h3>  Training </h3><br>  To test the architecture, I used the standard dubase CUB with photos and textual description of the birds. <br><br>  Training of the entire model takes place in two stages.  The first stage is the pre-training of DAMSM networks consisting of a text and image encoder.  During this stage, as described above, a ‚Äúattention map‚Äù is created, which looks like this: <br><br><img src="https://habrastorage.org/webt/of/w7/6f/ofw76fxdvl6bbuohon_okmsgfro.png"><br><br>  As can be seen from the figure, DAMSM manages to very accurately capture the relationship between individual words from the text description and the elements of the image; it is especially easy for the model to recognize colors.  I must say that there is no additional information about what a ‚Äúred‚Äù, ‚Äúyellow‚Äù or ‚Äúwings‚Äù, ‚Äúbeak‚Äù is.  There is only a set of texts and images. <br><br>  DAMSM training takes place without any problems, the training time on this dataset is 150-200 epochs, which corresponds to several hours on a high-power GPU. <br><br>  The second and main stage is generator training using the DAMSM model. <br>  The generator at each level generates an image of higher resolution - it looks like this: <br><br><img src="https://habrastorage.org/webt/xz/go/cw/xzgocw0eswwfeku7kxogp2jhuqy.png"><br><br>  The training of the generator takes much longer and is not always so stable, the recommended training time for this dataset is 300-600 epochs, which corresponds to about 4-8 days on a high-power GPU. <br><br>  The main problem in the training of the generator, in my opinion, in the absence of sufficiently good metrics that would allow to evaluate the quality of training in a more formal form.  I studied several implementations of the Inception score, which, in theory, is positioned as a universal metric for such tasks - but they did not seem convincing enough to me.  If you decide to train such a generator - you will need to constantly monitor the course of training visually, according to intermediate results.  However, this rule is valid for any such tasks, visual inspection is always necessary. <br><br><h3>  results </h3><br>  Now the fun part.  With the help of a trained model, we try to generate images, let's start with simple sentences: <br><br><img src="https://habrastorage.org/webt/7g/ri/x-/7grix-945iwxoysnibzph4yjd0w.png"><br><br>  Let's try more complex descriptions: <br><br><img src="https://habrastorage.org/webt/8n/kp/eu/8nkpeuqwf4wiqk_c6fn8bynmxiq.png"><br><br>  All text descriptions are invented, I intentionally did not use phrases from datasets for tests.  Of course, not all of these images were taken on the first try.  The model is wrong, the authors themselves are talking about it.  As the text of the description and the elements to be displayed increases, it becomes increasingly difficult to maintain the realism of the whole scene.  However, if you want to use something similar in production, say, generate pictures of certain objects for the designer, you can train and customize the system to your requirements, which can be quite strict. <br><br>  For each text description, you can generate a lot of options for images (including unrealistic), so there will always be something to choose from. <br><br><h3>  Technical details </h3><br>  In this paper, I used a low-power GPU for prototyping and the Google Cloud cloud server from the Tesla K80 installed during the training phase. <br><br>  The source code was taken from the repository of the authors of the article and underwent a serious refactoring.  The system was tested in Python 3.6 with Pytorch 0.4.1 <br><br>  Thank you for your attention! <br><br>  <i>Original article: <a href="https://arxiv.org/pdf/1711.10485.pdf">AttnGAN: Fine-Grained Text Generation with Attentional Generative Adversarial Networks</a> , 2018 - Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, Xiaodong He.</i> </div><p>Source: <a href="https://habr.com/ru/post/424957/">https://habr.com/ru/post/424957/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424945/index.html">Smartphone drives a toy car</a></li>
<li><a href="../424947/index.html">Gesture Recognition with APDS-9960</a></li>
<li><a href="../424949/index.html">PHP Digest 140 (September 17 - 30, 2018)</a></li>
<li><a href="../424951/index.html">Hooray! It was not paranoia</a></li>
<li><a href="../424955/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ332 (September 24 - 30, 2018)</a></li>
<li><a href="../424961/index.html">MTA-STS for Postfix</a></li>
<li><a href="../424963/index.html">Zuckerberg Finance: Building Tools for Science Together</a></li>
<li><a href="../424965/index.html">Develop React applications using ReasonReact</a></li>
<li><a href="../424967/index.html">Closures in javascript for beginners</a></li>
<li><a href="../424969/index.html">Node.js Part 9 Guide: Working with the File System</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>