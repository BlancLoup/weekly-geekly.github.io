<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A review of research in the field of deep learning: natural language processing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is the third article in the ‚ÄúDeep Learning Research Review‚Äù series of UCLA student Adit Deshpande. Every two weeks Adit publishes a review and in...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A review of research in the field of deep learning: natural language processing</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/57c/3aa/4df/57c3aa4dfb884b50be2f2c96dfd36ea1.png"><br><br>  This is the third article in the ‚ÄúDeep Learning Research Review‚Äù series of UCLA student Adit Deshpande.  Every two weeks Adit publishes a review and interpretation of research in a specific area of ‚Äã‚Äãin-depth training.  This time, he focused on applying deep learning to natural language processing. <br><a name="habracut"></a><br><h2>  <font color="#c75733">Introduction to natural language processing</font> </h2><br><h3>  Introduction </h3><br>  Natural language processing (NLP) is the creation of systems that process or ‚Äúunderstand‚Äù the language in order to accomplish certain tasks.  These tasks may include: <br><br><ul><li>  Question Answering (what Siri, Alexa and Cortana do) </li><li>  Analysis of the emotional coloration of statements (Sentiment Analysis) (determination of whether the statement has a positive or negative connotation) </li><li>  Finding the text corresponding to the image (Image to Text Mappings) (generating a signature for the input image) </li><li>  Machine Translation (text paragraph translation from one language to another) </li><li>  Speech Recognition </li><li>  Part of Speech Tagging (definition of parts of speech in a sentence and their annotation) </li><li>  Extract Entities (Name Entity Recognition) </li></ul><br>  The traditional approach to NLP suggested a deep knowledge of the subject area - linguistics.  The understanding of such terms as phonemes and morphemes was obligatory, since there are whole disciplines of linguistics devoted to their study.  Let's see how traditional NLP would recognize the following word: <br><img src="https://habrastorage.org/web/e6e/5bd/f54/e6e5bdf5434f49c4be1a8a5cb01fd0c4.png"><br>  Suppose our goal is to collect some information about this word (determine its emotional coloring, find its meaning, etc.).  Using our knowledge of the language, we can break this word into three parts. <br><img src="https://habrastorage.org/web/5ba/edd/072/5baedd0723ca470d99087f3c953a5de4.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We understand that the prefix (prefix) un- means negation, and we know that -ed can mean the time to which a given word belongs (in this case, the past tense).  Recognizing the meaning of the single-parent word interest, we can easily conclude about the meaning and emotional coloring of the whole word.  It seems to be simple.  However, if you take into account the diversity of prefixes and suffixes of the English language, you will need a very skilled linguist to understand all the possible combinations and their meanings. <br><br><img src="https://habrastorage.org/web/596/81d/265/59681d2655054e58bcb4761eef4d59d0.png"><br>  An example showing the number of suffix prefixes and roots in English <br><br><h3>  How to use deep learning </h3><br>  At the heart of deep learning is the teaching of ideas.  For example, convolutional neural networks (CNN) include a combination of different filters designed to categorize objects into categories.  Here we will try to apply a similar approach by creating representations of words in large data sets. <br><br><h3>  Article structure </h3><br>  This article is organized so that we can go through the basic elements from which you can build deep networks for NLP, and then proceed to discuss some of the applications that are related to recent scientific work.  It's okay if you don‚Äôt know exactly why, for example, we use RNN, or LSTM is useful, but hopefully, having studied these works, you will understand why deep learning is so important for NLP. <br><br><h2>  <font color="#c75733">Word vectors</font> </h2><br>  Since deep learning cannot live without mathematics, let us represent each word as a d-dimensional vector.  Take d = 6. <br><img src="https://habrastorage.org/web/7f9/eb0/325/7f9eb03259ba4ce6805c6f7ceb355c75.png"><br>  Now we will think how to fill the values.  We want the vector to be filled in such a way that it somehow represents the word and its context, meaning, or semantics.  One of the ways is to build a co-occurrence matrix.  Consider the following sentence: <br><img src="https://habrastorage.org/web/872/b74/f08/872b74f088434461bea0bd83f34cda2c.png"><br>  We want to get a vector representation for each word. <br><img src="https://habrastorage.org/web/bda/672/d34/bda672d341634c0e8616665fde214362.png"><br><br>  The co-occurrence matrix contains the number of times each word has been encountered in the corpus (training set) after each other word of this corpus. <br><br><img src="https://habrastorage.org/web/1b4/e79/55e/1b4e7955eca542128ddad3b6af4708cc.png"><br><br>  The rows of this matrix can serve as vector representations of our words. <br><img src="https://habrastorage.org/web/c70/e73/f63/c70e73f635d94c0089b4ca11d6615bd4.png"><br>  Please note that even from this simple matrix we can gather some rather important information.  For example, note that the vectors of the words ‚Äúlove‚Äù and ‚Äúlike‚Äù contain units in the cells responsible for their proximity to nouns (‚ÄúNLP‚Äù and ‚Äúdogs‚Äù).  They also have ‚Äú1‚Äù where they are next to ‚ÄúI‚Äù, indicating that this word is most likely a verb.  You can imagine how much easier it is to identify similar similarities when a data set is more than one sentence: in this case, the vectors of verbs such as ‚Äúlove‚Äù, ‚Äúlike‚Äù and other synonyms will be similar, since these words will be used in similar contexts. <br><br>  Good for a start, but here we note that the dimension of the vector of each word will increase linearly depending on the size of the body.  In the case of a million words (which is a little for standard NLP tasks), we would get a million-million-by-one matrix, which, moreover, would be very sparse (with a lot of zeros).  This is definitely not the best option in terms of data storage efficiency.  In the issue of finding the optimal vector representation of words, several serious moves were made.  The most famous of them is Word2Vec. <br><br><h2>  <font color="#c75733">Word2vec</font> </h2><br>  The main goal of all methods for initializing a word vector is to store as much information as possible in this vector, while maintaining a reasonable dimension (ideally, from 25 to 1000).  At the heart of Word2Vec is the idea of ‚Äã‚Äãlearning how to predict the surrounding words for each word.  Consider the sentence from the previous example: ‚ÄúI love NLP and I like dogs‚Äù.  Now we are only interested in the first three words.  Let the size of our window be three. <br><img src="https://habrastorage.org/web/f30/490/396/f3049039619243bc90c5f95dbfa33195.png"><br>  Now we want to take the central word ‚Äúlove‚Äù and predict the words going before and after it.  How do we do this?  Of course, by maximizing and optimizing the function!  Formally, our function tries to maximize the logarithmic probability of each word context for the current central word. <br><img src="https://habrastorage.org/web/f70/229/824/f70229824e7647d5b77e8c7d0479766e.png"><br>  We study the above formula in more detail.  It follows from this that we add the logarithmic probability of joint occurrence of both ‚ÄúI‚Äù and ‚Äúlove‚Äù, and ‚ÄúNLP‚Äù and ‚Äúlove‚Äù (in both cases, ‚Äúlove‚Äù is the central word).  The variable T denotes the number of training offers.  Consider the logarithmic probability closer. <br><img src="https://habrastorage.org/web/426/fb3/e73/426fb3e73cd54505b1482d7fea94ddc5.png"><br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>V</mi><mi>c</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.299ex" height="2.419ex" viewBox="0 -780.1 990 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-56" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-63" x="825" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>V</mi><mi>c</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-1"> V_c </script>  - vector representation of the central word.  Each word has two vector representations: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>o</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.617ex" height="2.419ex" viewBox="0 -780.1 1126.8 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-55" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-6F" x="966" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>U</mi><mi>o</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> U_o </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>U</mi><mi>w</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.996ex" height="2.419ex" viewBox="0 -780.1 1290.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-55" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-77" x="966" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>U</mi><mi>w</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-3"> U_w </script>  , one for the case when the word occupies a central position, the other for the case when this word is ‚Äúexternal‚Äù.  Vectors are trained by stochastic gradient descent.  This is definitely one of the most difficult to understand equations, so if you still have a hard time imagining what is happening, you can find additional information <a href="https://www.quora.com/How-does-word2vec-work">here</a> and <a href="https://www.youtube.com/watch%3Fv%3DD-ekE-Wlcds">here</a> . <br><br>  <b>To summarize in one sentence</b> : Word2Vec searches for vector representations of various words, maximizing the logarithmic probability of the occurrence of context words for a given central word and transforming vectors using the method of stochastic gradient descent. <br><br>  (Optional: further, the authors of the <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">work</a> talk in detail about how using negative sampling and subsampling to get more accurate word vectors). <br><br>  Perhaps the most interesting contribution of Word2Vec to the development of NLP was the emergence of linear relationships between different word vectors.  After learning, vectors reflect various grammatical and semantic concepts. <br><img src="https://habrastorage.org/web/3b8/4b3/b37/3b84b3b37169464893b9f7d5812323fc.png"><br>  It's amazing how such a simple objective function and simple optimization technique could reveal these linear relationships. <br><br>  <b>Bonus</b> : another cool method to initialize word vectors is <a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe</a> (Global Vector for Word Representation) (combines the ideas of the co-occurrence matrix with Word2Vec). <br><br><h2>  <font color="#c75733">Recurrent Neural Networks (Recurrent Neural Networks, RNN)</font> </h2><br>  Now let's see how the recurrent neural network will work with our vectors.  RNN is a magic wand for most modern natural language processing tasks.  The main advantage of RNN is that they can efficiently use data from previous steps.  This is what a small piece of RNN looks like: <br><img src="https://habrastorage.org/web/c45/065/d20/c45065d2091049ae969474e5c0c21fad.png"><br>  Below are word vectors ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="12.736ex" height="1.937ex" viewBox="0 -520.7 5483.5 834" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-78" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="809" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2C" x="928" y="0"></use><g transform="translate(1373,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-78" x="0" y="0"></use><g transform="translate(572,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2C" x="3205" y="0"></use><g transform="translate(3650,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-78" x="0" y="0"></use><g transform="translate(572,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2B" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mi>t</mi></msub><mo>,</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-4"> x_t, x_ {t-1}, x_ {t + 1} </script>  ).  Each vector at each step has a hidden state vector (hidden state vector) ( <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="12.764ex" height="2.539ex" viewBox="0 -780.1 5495.5 1093.4" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2C" x="932" y="0"></use><g transform="translate(1377,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><g transform="translate(576,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2C" x="3213" y="0"></use><g transform="translate(3658,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><g transform="translate(576,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2B" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub><mo>,</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo>,</mo><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-5"> h_t, h_ {t-1}, h_ {t + 1} </script>  ).  We will call this pair a module. <br><img src="https://habrastorage.org/web/556/1c2/05e/5561c205e7564734af1449edea1c20e9.png"><br>  The hidden state in each RNN module is a <i>function</i> of the word vector and the hidden state vector from the last step. <br><img src="https://habrastorage.org/web/93c/1b3/d56/93c1b3d56a2a443381b8f567e727443c.png"><br>  If we take a closer look at superscripts, we will see that there is a weights matrix <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>h</mi><mi>x</mi></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.627ex" height="2.419ex" viewBox="0 -935.7 1992.2 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><g transform="translate(1079,362)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-78" x="576" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mi>x</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-6"> W ^ {hx} </script>  , which we multiply by the input value, and there is a recurrent matrix of weights <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>h</mi><mi>h</mi></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.634ex" height="2.419ex" viewBox="0 -935.7 1995 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><g transform="translate(1079,362)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="576" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mrow class="MJX-TeXAtom-ORD"><mi>h</mi><mi>h</mi></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-7"> W ^ {hh} </script>  which is multiplied by the latent state vector from the previous step.  Keep in mind that these recurrent weights matrices are the same at each step.  <b>This is the key point of the RNN</b> .  If you think carefully, then this approach is significantly different from, say, traditional two-layer neural networks.  In this case, we usually choose a separate matrix W for each layer: <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>1</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.248ex" height="2.298ex" viewBox="0 -780.1 1398.4 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1335" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-8"> W_1 </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>W</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.248ex" height="2.298ex" viewBox="0 -780.1 1398.4 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-32" x="1335" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-9"> W_2 </script>  .  Here, the recurrent weights matrix is ‚Äã‚Äãthe same for the entire network. <br><br>  To obtain the output values ‚Äã‚Äãof each module (Yhat) is another matrix of weights - <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mi>s</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.511ex" height="2.057ex" viewBox="0 -780.1 1511.7 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-73" x="1526" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mi>s</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-10"> W ^ s </script>  multiplied by h. <br><br><img src="https://habrastorage.org/web/3ea/ed3/57c/3eaed357c44d4415b68fb1db58d6e95b.png"><br>  Now let's look from the side and see what the benefits of RNN are.  The most obvious difference between the RNN and the traditional neural network is that the RNN accepts input data <i>sequences</i> (in our case, words).  In this way, they differ, for example, from typical CNN, to which the whole image is fed to the input.  For RNN, however, the input data can be either a short sentence or a five-paragraph essay.  In addition, the <i>order</i> in which data is supplied can influence how the weight matrices and the vectors of hidden states change in the learning process.  By the end of the training, information from past steps should accumulate in the vectors of hidden states. <br><br><h2>  <font color="#c75733">Managed recurrent neurons (Gated recurrent units, GRU)</font> </h2><br>  Now let's get acquainted with the concept of a controlled recurrent neuron, which is used to calculate the vectors of hidden states in RNN.  This approach allows you to save information about more remote dependencies.  Let's speculate why distant dependencies for ordinary RNNs can be a problem.  At the time of the back propagation of the error (backpropagation) method, the error will move along the RNN from the last step to the earliest.  With a sufficiently small initial gradient (say, less than 0.25) to the third or fourth module, the gradient will almost disappear (since, according to the rule of the derivative of a complex function, the gradients multiply), and then the hidden states of the very first steps will not be updated. <br><br>  In normal RNN, the vector of hidden states is calculated by the following formula: <br><img src="https://habrastorage.org/web/7e2/d88/949/7e2d88949ceb4233913adf676d65f95a.png"><br>  The GRU method allows us to calculate h (t) differently.  The calculations are divided into three blocks: an update gate filter, a reset gate filter, and a new memory container.  Filter filters are functions of the input vector representation of the word and the hidden state in the previous step. <br><img src="https://habrastorage.org/web/798/a4b/4f7/798a4b4f74974266803a73e9c7f021c4.png"><br>  The main difference is that each filter uses its own weight.  This is indicated by different superscripts.  Update filter uses <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mi>z</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.51ex" height="2.057ex" viewBox="0 -780.1 1511 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="1526" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mi>z</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-11"> W ^ z </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>U</mi><mi>z</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.843ex" height="2.057ex" viewBox="0 -780.1 1224 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-55" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="1121" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mi>z</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-12"> U ^ z </script>  and the state reset filter is <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>W</mi><mi>r</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.482ex" height="2.057ex" viewBox="0 -780.1 1499 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-57" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-72" x="1526" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>W</mi><mi>r</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-13"> W ^ r </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>U</mi><mi>r</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.815ex" height="2.057ex" viewBox="0 -780.1 1212 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-55" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-72" x="1121" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>U</mi><mi>r</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-14"> U ^ r </script>  . <br>  Now let's calculate the memory container: <br><img src="https://habrastorage.org/web/603/8ca/751/6038ca75158e47c8a5a1b7acbeea2817.png"><br>  (an empty circle here refers to the <a href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">work of Hadamard</a> ). <br><br>  Now, if you look at the formula, you can see that if the factor of the reset filter is close to zero, then the whole product will also approach zero, and thus, the information from the previous step <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.265ex" height="2.539ex" viewBox="0 -780.1 1836.5 1093.4" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><g transform="translate(576,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-15"> h_ {t-1} </script>  will not be counted.  In this case, the neuron is just a function of the new word vector. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>x</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.256ex" height="1.937ex" viewBox="0 -520.7 1832.5 834" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-78" x="0" y="0"></use><g transform="translate(572,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMAIN-31" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-16"> x_ {t-1} </script>  . <br><br>  The final formula h (t) can be written as <br><img src="https://habrastorage.org/web/1f2/fb5/978/1f2fb5978044466fbab93cdb7e5a5f3f.png"><br><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-17"> h_t </script>  - a function of all three components: an update filter, a state reset filter, and a memory container.  You can better understand this by visualizing what happens to a formula when <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-18"> z_t </script>  nearing 1 and when <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-19"> z_t </script>  close to 0. In the first case, the hidden state vector <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-20"> h_t </script>  largely depends on the previous hidden state, and the current memory container is not taken into account, since (1 - <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-21"> z_t </script>  ) tends to 0. When <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-22"> z_t </script>  nearing 1, the new vector of the hidden state <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-23"> h_t </script>  , on the contrary, depends mainly on the memory container, and the previous hidden state is not taken into account.  So, our three components can be intuitively described as follows: <br><br><ul><li>  Update filter <br><ul><li>  If a <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-24"> z_t </script>  ~ 1, then <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-25"> h_t </script>  does not take into account the current vector of the word and simply copies the previous hidden state. </li><li>  If a <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>z</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.907ex" height="1.817ex" viewBox="0 -520.7 821.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-7A" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="658" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>z</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-26"> z_t </script>  ~ 0, then <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>h</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.165ex" height="2.419ex" viewBox="0 -780.1 932.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-68" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="815" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>h</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-27"> h_t </script>  does not take into account the previous hidden state and depends only on the memory container. </li><li>  This filter allows the model to control how much information from the previous hidden state should affect the current hidden state. </li></ul></li><li>  State reset filter <br><ul><li>  If a <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.875ex" height="1.817ex" viewBox="0 -520.7 807.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-72" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="638" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>r</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-28"> r_t </script>  ~ 1, the memory container stores information from the previous hidden state. </li><li>  If a <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-29-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>r</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.875ex" height="1.817ex" viewBox="0 -520.7 807.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-72" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="638" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>r</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-29"> r_t </script>  ~ 0, the memory container does not take into account the previous hidden state. </li><li>  This filter allows you to discard some of the information, if in the future it will not interest us. </li></ul></li><li>  Memory Container: Depends on the reset filter. </li></ul><br>  Let us give an example illustrating the work of GRU.  Suppose we have the following several sentences: <br><img src="https://habrastorage.org/web/4be/a28/c5d/4bea28c5dfad4864b3bbf81386d12b73.png"><br>  and the question: ‚ÄúWhat is the sum of two numbers?‚Äù Since the sentence in the middle does not affect the answer, the reset and update filters will allow the model to ‚Äúforget‚Äù this sentence and understand that only certain information can change the hidden state (in this case, numbers) . <br><br><h2>  <font color="#c75733">Neurons with long short term memory (LSTM)</font> </h2><br>  If you are done with GRU, then LSTM will not be difficult for you.  LSTM also consists of a sequence of filters. <br><img src="https://habrastorage.org/web/6a6/969/484/6a69694843454c84b6fba80e257a65b3.png"><br>  LSTM definitely takes in more information.  Since it can be considered an extension of GRU, I will not analyze it in detail, but to get a detailed description of each filter and each step of the calculations, you can refer to the beautifully written <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">blog post by</a> Chris Olah.  At the moment, this is the most popular LSTM tutorial, and will definitely help those of you who are looking for a clear and intuitive explanation of how this method works. <br><br><h2>  <font color="#c75733">Comparison of LSTM and GRU</font> </h2><br>  First, consider the common features.  Both of these methods are designed to preserve distant dependencies in sequences of words.  Distant dependencies mean situations where two words or phrases can occur at different time steps, but the relationship between them is important for achieving the ultimate goal.  LSTM and GRUs track these relationships using filters that can save or discard information from the sequence being processed. <br><br>  The difference between the two methods is in the number of filters (GRU - 2, LSTM - 3).  This affects the amount of nonlinearity that comes from the input data and ultimately affects the calculation process.  In addition, there is no memory in the GRU. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-30-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>c</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.833ex" height="1.817ex" viewBox="0 -520.7 789.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-63" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-74" x="613" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>c</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-30"> c_t </script>  as in lstm. <br><br><h2>  <font color="#c75733">Before going into articles</font> </h2><br>  I would like to make a small note.  If other models of deep learning, useful in NLP.  In practice, recursive and convolutional neural networks are sometimes used, although they are not as common as the RNNs that underlie most NLP deep learning systems. <br><br>  Now that we have begun to understand well the recurrent neural networks with reference to NLP, let's take a look at some of the work in this area.  Since NLP includes several different areas of tasks (from machine translation to forming answers to questions), we could consider quite a lot of work, but I chose the three that I found particularly informative.  In 2016, there was a number of significant advances in the field of NLP, but we will start with one work in 2015. <br><br><h2>  <font color="#c75733"><a href="https://arxiv.org/pdf/1410.3916v11.pdf">Neural networks with memory (Memory Networks)</a></font> </h2><br><h3>  Introduction </h3><br>  The first work that we will discuss has had a great influence on the development of the field of forming answers to questions.  In this publication, authorship by Jason Weston, Sumit Chopra and Antoine Bordes first described the class of models called ‚Äúmemory network‚Äù. <br><br>  The intuitive idea is this: in order to accurately answer a question relating to a piece of text, you must somehow store the source information provided to us.  If I asked you: ‚ÄúWhat does the abbreviation RNN mean?‚Äù, You could answer me, because the information that you learned while reading the first part of the article was stored somewhere in your memory.  You only need a few seconds to find this information and voice it.  I have no idea how it works in the brain, but the idea that space is needed to store this information remains unchanged. <br><br>  The memory network described in this paper is unique because it has an associative memory to which it can write and from which it can read.  It is interesting to note that neither CNN, nor Q-Network (for reinforcement learning), nor traditional neural networks use such memory, this is partly due to the fact that the task of forming answers to questions relies heavily on the ability to model or track remote dependencies, for example, keep track of the heroes of the story, or memorize the sequence of events.  It would be possible to use RNN or LSTM, but usually they are not able to remember input data from the past (which is critical for the tasks of forming answers to questions). <br><br><h3>  Network architecture </h3><br>  Now let's see how this network handles the source code.  Like most machine learning algorithms, the first step is to convert the input data into a feature space representation.  This may include the use of vector representations of words, morphological markup, syntactic analysis, etc., at the discretion of the programmer. <br><img src="https://habrastorage.org/web/e4b/c6c/1e7/e4bc6c1e7a074630b9d23bb16df5a35a.png"><br>  The next step is to take a representation in the feature space I (x) and read into memory the new portion of the input data x. <br><img src="https://habrastorage.org/web/6c8/6e7/5ee/6c86e75ee3ba4863a878b1be42ecbe06.png"><br>  Memory m can be considered as the similarity of an array made up of individual blocks of memory. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-31-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>m</mi><mi>i</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.84ex" height="1.817ex" viewBox="0 -520.7 1222.8 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-69" x="1242" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>m</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-31"> m_i </script>  .  Every such block <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-32-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>m</mi><mi>i</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.84ex" height="1.817ex" viewBox="0 -520.7 1222.8 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/wunderfund/blog/330194/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhhS-ARZ9-66B3drtAZFNOfET2MM3A#MJMATHI-69" x="1242" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>m</mi><mi>i</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-32"> m_i </script>  may be a function of the entire memory m, the representation in the feature space I (x) and / or itself.  The G function can simply store the entire representation I (x) in the memory block mi.  Function G can be modified to update the memory of the past based on new input data.  The third and fourth steps include reading from memory, taking into account the question, to find a representation of the signs o, and decoding it, to get the final answer r. <br><img src="https://habrastorage.org/web/991/491/2fd/9914912fde234aa2b6eb2037bb40c133.png"><br>  RNN can serve as a function of R, which transforms the presentation of signs into human-readable and accurate answers to questions. <br><br>  Now let's take a closer look at step 3. We want the function O to return a representation in the feature space that best matches the possible answer to the asked question x.           ,        . <br><img src="https://habrastorage.org/web/eae/4eb/a19/eae4eba1979341faacf68a401d0a1b47.png"><br>     (argmax)  ,   ,    (        ,    ).              ( )  (     ).           ,  ,   .      RNN, LSTM    ,    . <br><br>       ,        , ,     . ,    : <br><br><img src="https://habrastorage.org/web/708/802/c9c/708802c9c3ec4ad6965d8ef1e607f0d0.png"><br><br>  ,  ,    ,      : <br><br><ul><li> <a href="https://arxiv.org/pdf/1503.08895v5.pdf">End to End Memory Networks</a> </li><li> Dynamic Memory Networks </li><li> <a href="https://arxiv.org/pdf/1611.01604v2.pdf">Dynamic Coattention Networks</a> (   2016           Stanford's Question Answering) </li></ul><br><h2> <font color="#c75733"><a href="https://arxiv.org/pdf/1503.00075v3.pdf">Tree-LSTM     </a></font> </h2><br><h3>  Introduction </h3><br>           ‚Äì  ,       ().       ‚Äú        ‚Äù.             LSTM.      (Kai Sheng Tai),   (Richard Socher)    (Christopher Manning)       LSTM-   . <br><br>       ,          .  ,     ,   ,        .    ,    LSTM-     ,        . <br><br><h3>   </h3><br>    Tree-LSTM   LSTM   ,      ‚Äì            .  Tree-LSTM   ‚Äì           . <br><img src="https://habrastorage.org/web/c48/f33/aac/c48f33aac0d5496aa53d1cba3ab19291.png"><br>     ‚Äì  ‚Äì       , ,       .       .       ,       LSTM. <br><br>  Tree-LSTM            .   ,          -.      ,    (,  ‚Äú‚Äù  ‚Äú‚Äù)        .              . <br><br><h2> <font color="#c75733"><a href="https://arxiv.org/pdf/1609.08144v2.pdf">   (Neural Machine Translation, NMT)</a></font> </h2><br><h3>  Introduction </h3><br>  ,    ,       .    ‚Äì  Google      (Jeff Dean),   (Greg Corrado),   (Orial Vinyals)   ‚Äì    ,        Google Translate.            60%     ,  Google. <br><br>           .                 .        ,      . ,       (   NMT)  ,               . <br><br><h3>   </h3><br>       LSTM,            .       :  RNN,  RNN   ‚Äú‚Äù (attention module).          ,    ,     ,          (      ). <br><img src="https://habrastorage.org/web/bcf/aa5/f7a/bcfaa5f7a19748e284629f08e01eafb8.png"><br>     ,       .     ,   ,      . <br><br><h2>  <font color="#c75733">Conclusion</font> </h2><br>       ,           . ,               ,    , ,             (,  ). <br><br><blockquote><div class="spoiler">  <b class="spoiler_title">Oh, and come to work with us?</b>  <b class="spoiler_title">:)</b> <div class="spoiler_text">  <a href="http://wunderfund.io/"><b>wunderfund.io</b></a> is a young foundation that deals with <a href="https://en.wikipedia.org/wiki/High-frequency_trading">high-frequency algorithmic trading</a> .  High-frequency trading is a continuous competition of the best programmers and mathematicians of the whole world.  By joining us, you will become part of this fascinating fight. <br><br>  We offer interesting and challenging data analysis and low latency tasks for enthusiastic researchers and programmers.  Flexible schedule and no bureaucracy, decisions are quickly made and implemented. <br><br>  Join our team: <a href="http://wunderfund.io/">wunderfund.io</a> </div></div></blockquote></div><p>Source: <a href="https://habr.com/ru/post/330194/">https://habr.com/ru/post/330194/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../330180/index.html">Dell EMC Cloud for Microsoft Azure key features</a></li>
<li><a href="../330182/index.html">Pygest # 10. Releases, articles, interesting projects from the world of Python [May 23, 2017 - June 5, 2017]</a></li>
<li><a href="../330186/index.html">The digest of interesting materials for the mobile developer # 206 (May 29 - June 4)</a></li>
<li><a href="../330188/index.html">Inperfo - minimal network monitoring</a></li>
<li><a href="../330190/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ265 (May 29 - June 4, 2017)</a></li>
<li><a href="../330196/index.html">Concepts: set, type, attribute</a></li>
<li><a href="../330198/index.html">Useful utilities when working with Kubernetes</a></li>
<li><a href="../330204/index.html">Preventing Negative Impacts when Developing Artificial Intelligence Systems Beating the Human Mind</a></li>
<li><a href="../330206/index.html">Creating a design ecosystem for dozens of related IT solutions: a word to designers</a></li>
<li><a href="../330208/index.html">Understanding the new architectural components in Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>