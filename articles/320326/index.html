<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>All the best from Lean Startup methodology, and how testers live with it</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚ÄúThis should be done yesterday,‚Äù ‚ÄúTest it somehow quickly,‚Äù ‚ÄúThe time from the start of development to the production calculation should be minimal, a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>All the best from Lean Startup methodology, and how testers live with it</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/0a1/98d/32e/0a198d32eefc421fb3d9be6c5ecb643c.jpg" align="right" width="400">  ‚ÄúThis should be done yesterday,‚Äù ‚ÄúTest it somehow quickly,‚Äù ‚ÄúThe time from the start of development to the production calculation should be minimal, and if possible, even less‚Äù - probably, many are familiar with similar quotes.  And as long as we (testers) are one of the last links in the development chain, we most often have to balance between the speed at which features are released and their quality. <br><br>  In this article I want to share how we in our company apply successful practices from Lean Startup (despite the fact that many of our projects are fully formed and settled), what problems testers face when using this methodology and how we cope with these difficulties. <br><br>  A few words about myself: I am a tester, I had experience in projects of various scales, I was the only tester on a project and worked in teams that used different approaches and methodologies.  In my experience, working on Lean Startup is cool, but there are also pitfalls for testing, which are not bad to know in advance. <br><a name="habracut"></a><br>  For a start, it's worth to tell a little about what our company does.  <a href="https://www.tutu.ru/">Tutu.ru</a> is a service for online purchase of train, air and bus tickets, tours and other things related to travel.  One of our projects, <a href="https://tours.tutu.ru/">Tours</a> , is still under active development, it is very dynamic, and the functionality is rapidly changing.  Our PO (product-products) practice Lean Startup, and in particular, we conduct a lot of experiments. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Lean Startup, the desires of managers and the reality of a simple tester </h2><br>  Of course, we are not exactly a startup, but in the Lean Startup concept there are a lot of interesting things, including for us.  What does Lean Startup mean? <br><br><ol><li>  Minimize unnecessary costs and save all possible funds. </li><li>  Involvement in the process of creating a product of each employee.  In other words, about how much this decision costs;  when it can be done faster, and when - add more functions, carefully check the result - not only managers, ROs or some other mythical people, but also ourselves should think.  Each of us.  Before testing, you should ask yourself: what is the purpose of a feature / project?  Do not test for the sake of the testing process, and test in accordance with some goals, features. </li><li>  Orientation to the user and getting the maximum amount of information about the client per unit of time.  Yes, we think about our users and care about them.  We try to do not only as it is convenient for us, the engineers, from the technical side, but it is for them - the people who will use our final product. </li></ol><br>  And in order to understand who our users are, what they want, what they like and what doesn't, we use experiments with fast iterations. <br><br>  The point of all this is to make the minimally viable functionality, to test any hypothesis, and then, according to the results of the analytics, to find out whether it is necessary to develop this hypothesis (feature) further.  Experimenting, we reduce the risk of developing features that nobody needs.  At the same time, we need as short an iteration as possible: to get the necessary information as quickly as possible, to finalize everything in accordance with the data obtained - as quickly as possible. <br><br>  From the point of view of management, all this, of course, is very fun and healthy: we quickly saw the necessary functionality, quickly check hypotheses and variants, quickly change the functionality according to the results.  But what about all of this we do - testers?  When we constantly hear: ‚ÄúFaster, even faster!  This feature should have been released on production yesterday! ‚Äù <br><br>  We can use many different techniques, infinitely increase coverage - then we will ensure the maximum level of quality.  And of course, we would like our project and our new features to be of the highest quality and finest.  But you need to understand that all this costs certain resources: effort, money and time. <br><br>  Under the conditions of frequent experiments, we usually do not have these resources: after all, we must release and analyze the results of what has been done quickly.  In addition, at this stage, the project may <b>not need the</b> maximum level of quality that we would like to ensure in our dreams and secret desires.  At the same time, we cannot completely throw out or greatly reduce the amount of testing, because our new feature should still be of high quality, up to a certain level.  But where is this border?  How to keep this balance?  How to scale testing for the needs of the project? <br><br>  To do this, we use the following methodologies and tools: <br><br><ol><li>  Business analysis task </li><li>  Risk management </li><li>  Gradations of quality </li><li>  Developer Checklists </li><li>  ‚ÄúDelayed‚Äù writing documentation </li></ol><br>  And now we will consider each of these points in more detail: <br><br><h2>  Business analysis task </h2><br>  Of course, one cannot say that we (testers) conduct a full-fledged business analysis: after all, there are special people for this, and the task is quite extensive.  But we try to apply this to our realities and our situation.  We concentrate on the objectives of the project. <br><br>  For example, if we know that the number of users who buy tours for two adults and two infants (children under 2 years old) tends to zero - we most likely will not check what type of room the tour operator offers us in this case - because it‚Äôs easier no one will become of it.  But at the same time we will definitely check the search and the proposed type of room for 2 adults and one child - because such a story happens much more often, and because when searching with children, mistakes are often seen in general - we can say that this case is a boundary case. <br><br>  Or, for example, testing the API of connecting a new provider, we will not pay special attention to the prices of tours with business class flights (the main thing is that they are not substituted by default instead of economic ones!), Because we know that we should fly business class when buying a tour I wished only one tourist for the whole time of our section. <br><br>  The temptation to check clever cases for a tester is always great, but the main thing is not to forget about reality. <br><br><h2>  Risk management </h2><br>  The goal of quality assurance is risk reduction.  Even if we owned unlimited resources, we would not be able to completely eliminate all risks. <br>  If we are limited by time, means or some other things, then we divide the risks into 2 groups: <br><br><ol><li>  Valid </li><li>  Invalid </li></ol><br>  <b>The first category</b> includes things that may not work, work incorrectly, we may not even know that something is wrong with them (this is the worst thing!) - and for us this is normal.  We can take these risks for the sake of speeding out of tasks or another purpose. <br><br>  What can be attributed here?  Some additional scenarios, places where, according to statistics, ‚Äúalmost no one pokes‚Äù (0.1% of users, for example), work in unpopular browsers, etc. <br><br>  <b>The second category</b> - unacceptable risks - can be attributed to what should work for sure and of which we should be sure.  We need to provide the user with minimal, but <b>working functionality</b> .  If we make a feature that should help the user cross the ravine and eventually release half of the bridge, this is no good because it does not solve the user's problem.  Such an experiment will always show a negative result, this applies to our unacceptable risks.  And if we pull a rope with ropes over the sides of the ravine, it will be possible to switch to the other side, which means that the user's problem can be solved. <br><br>  We <b>manage</b> acceptable risks and <b>reduce</b> unacceptable. <br><br><h3>  Gradations of quality </h3><br>  Graduations generally follow from the preceding paragraph and were made for convenience by both testers and ROs.  They agree on the rules of the game between us.  We have 5 gradations of quality, each of which corresponds to a certain number of things that we look at. <br><br><img src="https://habrastorage.org/files/b08/a51/38e/b08a5138ef354120b3348a9cae948a8d.jpg"><br><br>  For example, in 2 gradations we need to make sure that the functionality behaves adequately in one, the most popular browser among our users, the main user scenarios are executed, the main business function is working. <br><br>  At quality level 5, in addition to all of the above, we draw attention to: <br><br><ul><li>  additional user scenarios and business functions; </li><li>  UI; </li><li>  display in all supported browsers and mobile devices; </li><li>  the maximum level of automation of the main and additional user scenarios; </li><li>  testability </li></ul><br>  This is also convenient because you can set a different and quite certain level of quality for different projects and even different tasks. <br><br>  For experimental tasks, the quality level is set lower, and if such problems are decided to be developed further, then the quality gradation grows with them.  For example, we assume that users will find it interesting and useful if we offer them prices for the next days and months, as well as to similar countries (compared to the current search).  But technically, this task is quite complicated, so we execute and run it, so to speak, in demo mode, to see if there will be interest from our users.  Suppose, in this form, the task falls into our testing.  And we suddenly find out that if the search is performed for two people (our default option, the most popular one), then the prices for the next months / similar countries are adequate and do not lie.  But if you search not for two, but for three or one - then ‚Äúoh‚Äù - not everything is so rosy.  And at this stage, we, as testers, after consulting with the developers and learning that the editing is quite laborious, we decide to release the task as it is. <br><br>  Next, we collect analytics and understand that about 40% of our visitors use this new block.  Then we start the next iteration of the task, and here the quality gradation will be higher - we realized that the functionality needs to be developed and we can‚Äôt afford the ‚Äúwrong prices‚Äù when searching for a different number of people. <br><br>  Here another question may arise: what is the main scenario in this case, and what is additional. <br><br><img src="https://habrastorage.org/files/12e/1ed/eb9/12e1edeb983b49a8bc592dedd33d978b.jpg"><br><br>  If you go back to Lean, then everything very much depends on the stage of the project where we are. <br><br>  This figure on the left shows the stages of the Lean Analytics project, and the so-called ‚Äúgates‚Äù (on the right), which you must go through to go to the next stage.  Here is a brief description of these stages: <br><br><ul><li>  Empathy.  The purpose of this stage is to inform potential buyers about product development and your experiments.  The gate in this case is a clear definition of the real, actual and unresolved problem.  Now that you have decided on this, you can go on. </li><li>  Stickiness.  Now do something such a feature that will force users to return.  Gate is a formulated problem solving technique that is ready to pay for. </li><li>  Virality.  Maximize the number of users (preferably without or with minimal use of paid methods).  Gate - the number of users, as well as the quality and demand for features is growing. </li><li>  Revenue.  The project should bring money, and more than you spend on it.  Gate - you have found and found your niche in the market. </li></ul><br>  Thus, in order to understand which scenarios are now considered additional and basic, it is worth considering at what stage the project is now. <br><br>  For example, at the Stickiness stage, various ‚Äútricks‚Äù and distinctive features will be important for us, at the Virality stage, we cannot do without SEO - and if we know what the project‚Äôs priorities are at the moment - we can more effectively approach the quality assurance process and ‚Äúapply special benefit "only where it is really needed. <br><br><h2>  Check-lists for developers (acceptance criteria). </h2><br>  Here we will talk about increasing the speed of the entire cycle of work on the task. <br><br>  In a perfect world, each team member is responsible for quality.  There are no raw tasks for testing.  Testers do not find critical bugs, because  the developers have already taken everything into account.  Our world is not perfect.  We were faced with the fact that the task came to us with obvious bugs.  But there are no obvious things.  Such situations were transparent only for us - testers. <br><br>  Developers think differently - this is normal and natural.  And that is why we would like to contribute a part of our ‚Äúvision‚Äù of the project as early as possible, preferably even before the start of testing itself. <br><br>  We found this way: we use the so-called ‚Äúacceptance criteria‚Äù. <br><br>  This checklist is a list of checks with specific parameters, data and links, as well as describing the expected behavior of objects.  At the same time, we try to minimize the number of passes and put the maximum number of checks in them so that the check list is easier, faster and more fun to use.  The checklist should be simple enough and concise, so that developers do not miss items from it.  It almost never contains negative checks.  Writing a checklist makes sense only when the developer has not finished the task, because our goal: the minimum number of switchings from task to task.  If we did not have time to write a check-list, we simply do not write it at all. <br><br>  In addition, if the developer passed the checklist, this does not mean that when testing, we do not need to look at the things that were described in the checklist.  Always worth a quick run through them just in case.  Here the difference in our perception of the product plays a role. <br><br>  An example of a fragment of our checklist: <br><br><blockquote><ol><li>  Go to / hotel / 3442 / (page without search parameters) </li><li>  There are no dice "send link to tours" </li><li>  Go to XXX (sample link to hotel page with parameters) </li><li>  Dice present </li><li>  You can go to the next step by clicking on the bid price. </li><li>  There are inscriptions about the possibility of partial payment and that included in the tour </li><li>  Opposite them, under the buttons on prices, there is a die with the pseudo reference ‚ÄúFind out if the price decreases‚Äù, in it the percentage is active and turns red when you hover </li><li>  Click on the link ‚ÄúShow more N offers‚Äù </li><li>  Below the subscription, already outside the locale in the right column is a die with the inscription "Send a link to tours to this hotel by e-mail" </li></ol></blockquote><br>  We use checklists for more than a year and have already been able to draw some conclusions.  Both testers and developers say that checklists are uniquely useful and convenient to use.  So what are the advantages? <br><br><h3>  Why do testers like it? </h3><br><ul><li>  fewer raw tasks; </li><li>  Testers used to work on the task before. </li></ul><br>  When a development methodology is flexible, there is usually very little documentation and specifications on the project.  If the project is dynamic and the ROs are constantly experimenting (as it happens with us), then the ‚Äúpreliminary‚Äù documentation does not make much sense - usually, we only have user-experience and approximate solutions.  Therefore, in this situation, the stage ‚Äútesting requirements‚Äù as a separate unit is simply absent.  But it is important for testers to take part in the formation of requirements and we do it just at the stage of creating checklists for developers.  Of course, there are also separate meetings for discussing tasks, discussing layouts, but even this is not enough. <br><br><ul><li>  Testing tasks with checklists takes less time than testing similar ones without checklists. </li><li>  We learn to formulate concretely and clearly, we highlight the problem areas in the product. </li><li>  We have the opportunity to look at the problem from different points of view. </li></ul><br>  It often happens that one person tests a task, and another checklist writes to it.  We all have slightly different styles, views, testing methodology.  When you manage to look at the task with a ‚Äúfresh‚Äù look, it is very healthy and useful. <br><br><h3>  Why do developers like it? </h3><br><ul><li>  a clear list of scenarios with specific data to check; </li><li>  preliminary analysis of the task; </li><li>  developers get statistics and feedback. </li></ul><br>  We take into account the nature of the bugs found on the checklists and we can say that, for example, someone makes mistakes of a certain nature: logic, code, etc.  When people know where they are wrong, they have something to strive for and it is easier to correct their mistakes. <br><br>  <b>What we all like:</b> we do not throw the task from development to testing a hundred thousand times.  Many things are corrected at the check-lists stage.  And we are getting closer to the ideal world, where everything is taken into account and done right the first time.  Maybe one day. <br><br>  Of the minuses it can be noted that time is still spent on writing checklists and their passage.  But, judging by our statistics, it takes less time than throwing tasks between testing and development in their absence.  Testers write one checklist for an average of half an hour, developers pass it in 10-30 minutes (depending on the complexity of the task). <br><br>  In addition, not all tasks in our sprints "deserve" checklists.  On planning, we estimate whether we need a checklist in a task or not.  As a result, we have 6-7 checklists per sprint (on average, we do about 30 tasks per sprint).  About 67% of checklists were passed successfully (the developers did not find bugs for them).  In 35% of tasks with checklists, no bugs were found during testing. <br><br><img src="https://habrastorage.org/files/3c0/322/2a0/3c03222a03be42e7926b402bd4cc987e.png"><br><br><h2>  "Deferred" documentation </h2><br>  We do not set a goal to write detailed test cases for all new features.  And the reason is the same: experimental tasks.  We do not want to write documentation that is not useful to anyone.  It is better to describe the functionality, when it will be known for sure that it is waiting for further development.  We write cases without too much detail to make them easier to maintain.  In addition, we try to think a few times before writing a case to avoid, again, unnecessary wastage of time. <br><br>  All of the above tools in our realities really work - we checked. <br><br>  They help us maintain a balance between quality and speed. <br><br>  ‚Üí <a href="http://sqadays.com/ru/talk/40453">Based on my performance on SQA Days-19</a> <br><br>  Video of the performance can be seen here: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/178143861" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p>Source: <a href="https://habr.com/ru/post/320326/">https://habr.com/ru/post/320326/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../320316/index.html">Docker implementation for a small project in Production</a></li>
<li><a href="../320318/index.html">Seven excellent site accelerators for Linux and Unix</a></li>
<li><a href="../320320/index.html">How is the League of Legends frame rendered?</a></li>
<li><a href="../320322/index.html">How to make localization for the Japanese market</a></li>
<li><a href="../320324/index.html">Wonderful sketches of the game Super Mario Bros</a></li>
<li><a href="../320330/index.html">How to "make friends" Mitel SIP-DECT and Asterisk</a></li>
<li><a href="../320336/index.html">Labor market overview in the field of big data and data science</a></li>
<li><a href="../320338/index.html">Observed models in Realm Xamarin</a></li>
<li><a href="../320342/index.html">Tale of the impossible bug: big.LITTLE and caching</a></li>
<li><a href="../320344/index.html">Olympiad of MIPT on electronics for schoolchildren</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>