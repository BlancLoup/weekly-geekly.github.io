<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Entropy and WinRAR</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The concept of entropy is used in almost all areas of science and technology, 
 from designing boiler rooms to models of human consciousness. 
 The ba...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Entropy and WinRAR</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/307/d92/62c/307d9262c34cef8dd164774e12e171aa.png" align="right" alt="image"><br>  The concept of entropy is used in almost all areas of science and technology, <br>  from designing boiler rooms to models of human consciousness. <br>  The basic definitions of both <a href="http://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D1%2580%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25BC%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%258D%25D0%25BD%25D1%2582%25D1%2580%25D0%25BE%25D0%25BF%25D0%25B8%25D1%258F">thermodynamics</a> and <a href="http://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BD%25D1%2584%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D1%2586%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%258D%25D0%25BD%25D1%2582%25D1%2580%25D0%25BE%25D0%25BF%25D0%25B8%25D1%258F">dynamic systems</a> and methods of computation are not difficult to understand.  But the farther into the forest - the more firewood.  For example, I recently found out (thanks to R. Penrose, ‚ÄúThe Path to Reality‚Äù, pp. 592-593) that not only solar energy is important for life on Earth, but its <i>low entropy.</i> <br><br>  If we confine ourselves to simple dynamical systems or one-dimensional data arrays (which can be obtained as the ‚Äútrace‚Äù of the system‚Äôs motion), then at least three definitions of entropy can be counted as measures of randomness. <br>  The deepest and most complete of them (Kolmogorov-Sinai) can be clearly studied, <br>  <i>using file archiver programs.</i> <br><a name="habracut"></a><br><h5>  Introduction </h5><br>  Referring to the informational entropy, they often simply give Shannon's ‚Äúmagic‚Äù formula: <br><img src="https://habrastorage.org/storage2/cb9/072/dae/cb9072dae3d7a925c4c9121f2c0f3d46.png" alt="image"><br>  where N is the number of possible realizations, b is the units of measurement (2 are bits, 3 are trits, ..), p <sub>i</sub> are the probabilities of realizations. <br>  Of course, everything is not so simple (but interesting!).  For the first (as well as the second, and even the third) acquaintance, I recommend a <a href="http://yadi.sk/d/gKP8GQWx57Jt0">book</a> that is not outdated by A. Yagl and I. Yagl, Probability and Information, 1973. <br>  In response to the "childish" question: " <i>What does the number obtained by the formula mean?"</i> <br>  Such a graphic illustration will do. <br><br>  Suppose there are N = 16 cells, one of which is marked with a cross: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      [] [] [] [] [] [] [] [] [] [] [] [x] [] [] [] []] <br><br>  The choice is random;  the probability that the i-th cell is marked is equal to p <sub>i</sub> = 1/16 <br>  Shannon's entropy is <br><img src="https://habrastorage.org/storage2/49c/a04/896/49ca0489654b84d3d39e38b4512945c8.png"><br>  Now imagine a game;  one player draws cells, puts a cross and tells the second only <br>  cell count.  The second is to find out which one is marked and asks questions; <br>  but the first one will answer <i>only ‚ÄúYes‚Äù or ‚ÄúNo‚Äù.</i> <br>  How many (minimum!) Questions need to be asked to find out where the cross is? <br>  The answer is " <b>4</b> ".  In all such games, the <i>minimum number of questions will be equal to the information entropy.</i> <br>  It does not matter what questions to ask.  An example of the right strategy is <br>  divide the cells in half (if N is odd, then with an excess / deficit of 1 cell) and ask: <br>  ‚ÄúCross in the first half?‚Äù Having learned that in such and such, we also divide it in half, etc. <br><br>  Another kind of ‚Äúnaive‚Äù, but a deep question: <i>why is there a logarithm in the Shannon formula?</i> <br>  And what exactly is a logarithmic function?  Inverse exponential ... And what is exponential? <br>  In fact, rigorous definitions are given in the course of mathematical analysis, but they are not complicated at all. <br>  It often happens that increasing the "input" at times, we get the "output" only by a few more units. <br>  Augustin Cauchy proved that the equation <br><br>  f (x ¬∑ y) = f (x) + f (y) x, y&gt; 0 <br><br>  has <i>only one continuous solution</i> ;  this is a logarithmic function. <br>  Imagine now that we are conducting two independent tests;  the number of outcomes they have is n and m, they are all equally probable (for example, we throw both the dice and the coin at the same time; n = 6, m = 2) <br>  Obviously, the number of outcomes of double experience is equal to the product n ¬∑ m. <br>  The uncertainties of the experiences add up;  and entropy, as a measure of this uncertainty, should <br>  satisfy the condition h (m ¬∑ n) = h (m) + h (n), i.e.  be logarithmic. <br><br><h5>  Entropy of Kolmogorov-Sinai </h5><br>  The strict definition of this fundamental concept is rather difficult to understand. <br>  We will approach it gradually; <br>  our "phase space" will be one-dimensional arrays of natural numbers. <br>  For example, the decimal digits of the number "Pi": {3, 1, 4, 1, 5, 9, 2, 6, ...} <br>  In fact, any data can be brought into this form (with losses, of course). <br>  It is enough to split the entire range of values ‚Äã‚Äãinto N intervals and, if it hits the k-th interval, record the number k (as with the sound sampling). <br>  <i>How random, stochastic is this or that sequence?</i> <br>  Informational entropy, of course, is a good candidate for the ‚Äúchaos meter‚Äù. <br>  It is not difficult to prove that it is maximal when all probabilities of the appearance of different numbers are equal. <br><br>  But then the difficulties begin, which I mentioned in a previous post. <br>  For example, the array ... 010101010101010101 ... has the <i>maximum</i> entropy, being the <i>least random</i> sequence of 0 and 1. <br>  If the data is not originally digital (sound, pictures), then you can convert them into numbers from 0 to N in different ways.  And probabilities in a numerical array can be measured not only for individual elements, but also for pairs, triples, odd in order, etc. <br>  Well, since there are many options, we <b>use them all!</b> <br>  We will display the "experimental" array in various ways - in different number systems, breaking into pairs and triples, etc.  Obviously, its size L and Shannon's entropy H will depend on the mapping method. <br>  For example, considering it as a solid, indivisible object, we get L = 1, H = 0. <br>  Given that H grows with increasing L, we introduce the value <br><br><img src="https://habrastorage.org/storage2/821/2bd/9de/8212bd9de24f00522e5d73ab05cfeea3.png"><br><br>  This is the (somewhat simplified) definition of Kolmogorov-Sinai entropy ( <a href="http://www.scholarpedia.org/article/Kolmogorov-Sinai_entropy">strict</a> here).  In a theoretical sense, it is perfect. <br>  But it is very difficult to technically find the top edge over all partitions.  In addition, for clarity, it will be necessary to arrange all the results in an orderly manner, and this is a separate serious task. <br>  Fortunately, there is an approach that almost does not weaken the strictness of the theory, but is much more visual and <br>  practical. <br><br><h5>  Archiving and entropy </h5><br>  About methods and algorithms of information compression on Habr√© were told more than once <br>  <a href="http://habrahabr.ru/post/142242/">habrahabr.ru/post/142242</a> <br>  <a href="http://habrahabr.ru/post/132683/">habrahabr.ru/post/132683</a> <br>  not to mention the original sources. <br>  At one time, the possibility of compressing up to 50% with full recovery made a shocking impression on me. <br>  He was no less surprised when he found out that the <i>relationship between the degree of data compression and their Kolmogorov-Sinai entropy was strictly proved</i> .  Intuitively, this is understandable - the more ordered the data, the more possibilities for compression encoding. <br>  However, rigorous mathematical considerations do not always confirm ‚Äúcommon sense‚Äù; <br>  and although the articles on this topic are quite <a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.31.2725">intricate</a> (and the <a href="http%253A%252F%252Fieeexplore.ieee.org%252Fxpls%252Fabs_all.jsp%253Farnumber%253D179344">main one</a> is also paid), we will believe theorists that everything is legal and proceed to the experiments. <br><br>  Create data of 4 types: <br><ul><li>  decimal digits of pi (œÄ) </li><li>  GPSN Wolfram Mathematica (Random) </li><li>  the brightest and simplest example of deterministic chaos - <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2582%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">logistic mapping</a> (Logistic) </li><li>  array of one "1" (Const) </li><li>  some kind of glitzy formula that at first glance generates random numbers (Map) </li></ul><br>  Of course, we can calculate entropies using the Shannon formula (embedded in Mathematica as Entropy []). <br>  However, as mentioned above, the result will strongly depend on the number system, the form of data presentation, etc. <br>  Here, for example, is a logistic entropy graph for an ever-increasing amount of data: <br><br><img src="https://habrastorage.org/storage2/0df/bd2/b3d/0dfbd2b3d78cf8cdd8f032bcf0105b16.png"><br><br>  It is not very clear whether the entropy tends to a certain value as the size of the array increases. <br>  And now let's try to measure entropy using compression.  For each example, we will make arrays of different sizes and construct graphs of the ‚Äúsize after compression / size before compression‚Äù relationship depending on the array length (to be honest, I used Mathematica zip archiver, which was not included in the WinRAR header). <br><br><img src="https://habrastorage.org/storage2/bed/16e/e6d/bed16ee6d5b01879296ecb0a84ebdca1.jpeg"><br><br>  These are the right, ‚Äúwine‚Äù pictures!  It is clearly seen how the compression ratio goes to the limit value, which characterizes the measure of randomness of the original data. <br>  Let me remind you that the less ordered the data, the harder it is to encode it. <br>  The largest residual volume is in the digits of Pi and random numbers (almost merged blue and red <br>  graphics). <br><br><h5>  An example of the development of ideas. </h5><br>  Instead of ‚Äúboring‚Äù numbers, we take meaningful literary texts.  The degree of their stochasticity and methods of compression is a very broad topic (see <a href="http://cs.fit.edu/~mmahoney/compression/text.html">for example</a> ) <br>  Here we just make sure that measurements with the help of archivers also make sense. <br>  Approximately equal fragments of Russian fairy tales, spiritual conversations of Jidda Krishnamurti and LI speeches were taken as examples.  Brezhnev. <br><br><img src="https://habrastorage.org/storage2/22b/cec/bbd/22bcecbbdb0238603f0e3d3e26b5985b.jpg"><br><br>  Interestingly, for small passages, the compression ratio (that is, the ordering) of the texts is noticeably different, while Brezhnev is the leader (!) But for large arrays, the entropy of the texts is about the same. What this means and whether it always means is unknown. </div><p>Source: <a href="https://habr.com/ru/post/180803/">https://habr.com/ru/post/180803/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../180787/index.html">The path of a business samurai: the maximum effect for one weekend</a></li>
<li><a href="../180789/index.html">Google Code does not allow file placement.</a></li>
<li><a href="../180791/index.html">40 years of Ethernet</a></li>
<li><a href="../180793/index.html">Unmanned combat robot</a></li>
<li><a href="../180799/index.html">A quick tour of new google maps</a></li>
<li><a href="../180805/index.html">Interview with Boris Kim (QIWI) at the conference # MBLT13</a></li>
<li><a href="../180807/index.html">How Blackberry cares about developers</a></li>
<li><a href="../180811/index.html">Out of 5,335,200 LEGO parts, a full-size X-Wing star fighter was built</a></li>
<li><a href="../180813/index.html">Ask questions to Intel experts. Everything related to video processing on CPU and GPU</a></li>
<li><a href="../180815/index.html">Video review monoblock (tablet) ASUS Transformer P1801</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>