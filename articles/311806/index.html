<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How the use of redundancy codes in SDS helps Yandex keep data cheaply and securely</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Yandex, like any other large Internet company, stores a lot, or rather a lot of data. This includes user data from different services, and sites that ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How the use of redundancy codes in SDS helps Yandex keep data cheaply and securely</h1><div class="post__text post__text-html js-mediator-article"><p>  Yandex, like any other large Internet company, stores a lot, or rather a lot of data.  This includes user data from different services, and sites that are inked, and intermediate data for calculating weather, and backup copies of databases.  Storage cost ($ / GB) is one of the important indicators of the system.  In this article I want to tell you about one of the methods that allowed us to seriously reduce the cost of storage. </p><br><p> <a href="https://habrahabr.ru/company/yandex/blog/311806/"><img src="https://habrastorage.org/files/192/4cf/95d/1924cf95d4014819965ffbab467e81c4.jpg"><br></a> </p><br><p>  In 2015, as you all remember, the dollar rose strongly.  More precisely, it began to grow at the end of 2014, but we ordered new batches of iron already in 2015.  Yandex earns in rubles, and therefore the cost of iron for us has increased along with the exchange rate.  This made us once again think about how to make it possible to put more data into the current cluster.  Of course, we do this regularly, but this time the motivation was especially strong. </p><br><p>  Each cluster server provides us with the following resources: processor, RAM, hard drives, and the network.  The network here is a more complex concept than just a network card.  It is also the entire infrastructure inside the data center, and the connectivity between different data centers and traffic exchange points.  In a cluster, replication was used to ensure reliability, and the total cluster size was determined solely through the total capacity of the hard drives.  It was necessary to figure out how to exchange the remaining resources for an increase in space.  By the way, if after the post you still have questions that you would like to discuss in person, come to our <a href="https://events.yandex.ru/events/meetings/15-oct-2016/%3Futm_source%3Dhabr%26utm_medium%3Darticles%26utm_campaign%3DInfr">meeting</a> . </p><br><p><a name="habracut"></a>  RAM is difficult to exchange.  We use disk shelves, and the difference in the amount of RAM to disk capacity is more than three orders of magnitude.  It can be used to speed up access within a single machine, but this is a story for a separate article. </p><br><p>  The processor exchanges quite obviously through archiving.  But you need to pay attention to several pitfalls.  First, the degree of compression is highly dependent on the combination of the archiver and the stored data, that is, you need to take a representative sample of data and estimate how much you will save.  Secondly, the archiver should provide the ability to read data from almost any place, otherwise you will have to forget about the Range header in HTTP (and customers with not very good Internet, who will no longer be able to download large files, will be offended by this).  Thirdly, the speed of compression and decompression, as well as the associated CPU consumption, are important.  You can take an archiver that will compress the data quite effectively, but the number of processors in your cluster is not enough to provide the current write speed.  When unpacking, in turn, latency requests will suffer.  Not so long ago, Facebook officially released its archiver Zstd - we recommend it to try.  It is very fast and compresses data pretty well. </p><br><p>  Of all the resources, the network remains, and with it we have a lot of room for creativity.  How to exchange the network for storage capacity will be discussed further. </p><br><p>  Yandex has quite strict storage requirements.  It must remain operational even if one data center is lost entirely.  This condition imposes quite strong restrictions on the technologies that can be applied, but in exchange we get higher reliability than from the data center itself.  For us as storage system developers, this means that the redundancy factor must be greater than one when any data center drops out. </p><br><p>  In general, it is more correct to talk about accessibility zones.  In our case, it is the entire data center, but it can be a separate machine, a rack, a machine room, or even a continent.  That is, if we have four availability zones and we evenly distribute data between them, then the degree of redundancy cannot be less than 1. (3), 0. (3) in each of the zones.  So, there are N availability zones and you need to somehow decompose the data on them. </p><br><h3>  Replicas </h3><br><p>  Obviously, the easiest way is to make full replicas.  Most often, two or three replicas are made, and for many years we have been living according to this scheme.  The disadvantages are obvious: you have to use two (or, respectively, three) gigabytes of hard disk space to store one gigabyte of data.  But there are also advantages: a full-fledged file is stored in each replica, it is easy to read, and at the same time it lies entirely on one machine, on one hard disk.  Recovering data when a disk is lost is also quite simple - by regular copying over the network. </p><br><p>  For the storage of user content, Yandex.Music data, various avatars and other similar data, we are responsible for the MDS - Media Storage service.  It is based on Elliptics, Eblob and HTTP proxy.  Elliptics provides network routing, Eblob is needed for storing data on a disk, and a proxy is used for terminating user traffic.  All this cluster is controlled by a system called Mastermind.  In our storage, we departed from the concept of large DHT-rings, which we tried in Elliptics earlier, and divided the entire space into small shards of 916 gigabytes.  We call them "drops" (from the English. Couple).  The figure 916 is chosen because we need to place a multiple of the number of replicas of the drops on one hard disk (disk manufacturers, in turn, love marketing and consider the volume in decimal terabytes).  Mastermind ensures that replicas of a single drop are always located in different DCs, starts the data recovery procedure, defragmentation, and generally automates the work of system administrators. </p><br><img src="https://habrastorage.org/files/39a/97c/96b/39a97c96b899489c820538b079ef2f6b.png"><br><br><p>  To restore consistency, we have a special procedure that runs through all the keys and appends the missing keys to those replicas where they are not.  This procedure is not very fast, but we run it only where there is a discrepancy in the number of living keys between replicas.  Side plus - creates a load on the hard drive, even if the data there is cold and the users behind them do not come.  As a result, the disk begins to die in advance, and not at the moment when we are trying to recover the data, having discovered that another replica has already died.  We quietly change it, after which the recovery starts automatically and the user data remains safe and sound. </p><br><h3>  Redundancy codes </h3><br><p>  To make an analogy, this kind of data replication is RAID 1, simple, reliable, and not very efficient in terms of space consumption.  We want to create something similar to RAID 5 or RAID 6. Again, we go from simple to complex: we take three availability zones, somehow block our data into blocks, write even blocks to availability zone 1, and odd numbers to the second, in the third - the result of a byte XOR between blocks.  For error detection, we consider for each block a checksum that is negligible in comparison with the block size.  Data recovery is elementary: if a ^ b = c, then b = a ^ c.  The redundancy factor with this approach is 1.5.  If any block is lost, you will need to read two others, and from different access zones.  Recovery is possible with the loss of no more than one disk, which is much worse than in the case of three replicas, and is comparable to two replicas.  This is how the XOR result for the string ‚ÄúHello, habrahabr‚Äù is considered (the bottom numbers are the decimal representation of the byte): </p><br><img src="https://habrastorage.org/files/6ec/243/66c/6ec24366cf904d6f99102b7901f5d7d2.png"><br><br><p>  Here it is necessary to introduce the concept of stripe.  A stripe is N consecutive blocks, and the beginning of the first stripe coincides with the beginning of the data stream, N depends on the selected coding scheme, and in the case of RAID 1 N = 2.  To effectively use redundancy codes, you need to combine all files into one continuous stream of bytes, and break it into stripes already.  Nearby you should save the markup, in which type and with which byte each file starts, as well as its size.  If the length of the data stream is not a multiple of the stripe size, then the rest should be filled with zeros.  Schematically, this can be represented as: </p><br><img src="https://habrastorage.org/files/1e4/372/19b/1e437219baba4748922d6a59e7993f78.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  When choosing a stripe size, you can use the following considerations: </p><br><ul><li>  the larger the block size, the more files will fit in one block and the smaller the readings from the two availability zones at once, </li><li>  the larger the block size, the more data must be dragged across the network in the event of a block being temporarily unavailable or lost. </li></ul><br><p>  Thus, it is necessary to consider the probabilities of these events.  It turned out that the optimal size is two medians of the file size.  In addition, it is advisable to re-sort the files so that the block boundaries fall into larger files that would not fit in one block anyway.  This will also reduce the load on the network.  And to simplify the code, the function of storing parity blocks is assigned to one of the accessibility zones. </p><br><h3>  Reed-Solomon codes </h3><br><p> But what if you want more reliability?  Correct - use more powerful redundancy codes.  Now one of the most common codes is the Reed-Solomon code.  It is used when recording DVDs, in digital TV (DVB-T), in QR codes, and also in RAID 6. We are not going to talk about the mathematics of the Galois fields here - an engineering approach is waiting for you.  For all calculations, we will use the jerasure library, which has a difficult fate, but which works very quickly and has all the functions we need. </p><br><p>  The first thing worth noting: for an effective result, you need to work in the fields 2 ^ 8, 2 ^ 16, 2 ^ 32, that is, in machine words.  Further, for simplicity, we will use the field 2 ^ 8 and work with bytes.  To make the example more specific, let's try to achieve a replication ratio of 1.5, but with two parity blocks.  To do this, you need to split the data into stripes of 4 blocks each, and generate 2 parity blocks.  If we take the first byte from each data block, we can compose a vector of dimension 4 and, similarly, a vector of dimension 2 for parity blocks.  In order to get a vector of dimension 2 from a vector of dimension 4, it must be multiplied by a 2x4 coding matrix, and multiplied by the rules of operation in the Galois fields, if viewed from an engineering point of view.  The matrix that we need is called the Vandermond matrix.  For the selected field, such a matrix guarantees a property similar to the absence of linear combinations in ordinary algebra.  When restoring data, it will also play an important role. </p><br><p>  Let's take one stripe of data ‚ÄúHello, habrahabr‚Äù.  It is very well divided into 4 blocks of 4 bytes each, with one byte corresponding to one encoding word. </p><br><img src="https://habrastorage.org/files/5db/cbd/96a/5dbcbd96ab274aa49e204e9b1d2b9ac6.png"><br><br><p>  So, it turns out the following picture: </p><br><img src="https://habrastorage.org/files/6bb/478/7dd/6bb4787dd24945a2aea3e2926194420b.png"><br><br><p>  We calculate the parity blocks in this way, word by word (in our case - byte by byte). </p><br><p>  If you slightly change the picture and add the identity matrix above the coding matrix, then the output data will appear in the output vector: </p><br><img src="https://habrastorage.org/files/368/24a/462/36824a462474471188f679d76cf5a552.png"><br><br><p>  Suppose we have lost block number 2 and block number 4. Delete the corresponding rows from the matrix and from the right vector: </p><br><img src="https://habrastorage.org/files/b6b/7ea/d81/b6b7ead8144343e8b9ff62d4b8b4493b.png"><br><br><p>  Then we reverse the resulting square matrix and multiply both sides of the equality by: </p><br><img src="https://habrastorage.org/files/a89/bb6/a00/a89bb6a000544c5d898371f3b3b6887f.png"><br><img src="https://habrastorage.org/files/e1b/cb9/d98/e1bcb9d981fe460fade3f1e0e889c2ea.png"><br><br><p>  It turns out that to obtain the original data you only need to carry out the same multiplication operation as in the encoding!  If only one block is lost, then from the coding matrix, in order for it to become square, you need to cross out one of the lines corresponding to the parity blocks.  Note that the first line consists of ones and has a special magic: the counting is equivalent to calculating XOR between all elements and is performed several times faster than the counting of any other line.  Therefore, throwing this line is not worth it. </p><br><h3>  Local Recovery Codes </h3><br><p>  It turned out quite simple and, I hope, understandable.  But can anything else be improved?  Yes, colleagues from Microsoft Azure tell us in <a href="https://www.microsoft.com/en-us/research/publication/erasure-coding-in-windows-azure-storage/">their publication</a> .  This method is called the Local Reconstruction Codes (LRC).  If you break all the data blocks into several groups (for example, into two groups), you can encode parity blocks in such a way as to localize error corrections within the groups.  For the former replication ratio of 1.5, the scheme looks like this: we divide the stripe into two groups of four blocks, for each group there will be a local parity block plus two global parity blocks.  This scheme allows you to correct any three errors and about 96% of situations with four errors.  The remaining 4% are cases when all four errors fall within one group, which includes 4 data blocks and a local parity block. </p><br><img src="https://habrastorage.org/files/4ef/78b/73d/4ef78b73de2b4911876cb75733a15b6f.png"><br><br><p>  Once again applying the ‚Äúfrom simple to complex‚Äù approach, we took and divided the row of units in the coding matrix into two as follows: </p><table><tbody><tr><td>  one </td><td>  one </td><td>  one </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td></tr><tr><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  one </td><td>  one </td><td>  one </td><td>  one </td></tr><tr><td>  one </td><td>  55 </td><td>  39 </td><td>  73 </td><td>  84 </td><td>  181 </td><td>  225 </td><td>  217 </td></tr><tr><td>  one </td><td>  172 </td><td>  70 </td><td>  235 </td><td>  143 </td><td>  34 </td><td>  200 </td><td>  101 </td></tr></tbody></table>  Unfortunately, this time our approach failed.  At first everything was fine, but when we wrote a test combinatorially sorting through all the options for theoretically recoverable errors, we found that some of them our matrix does not allow restoring.  I had to dive into the study of publications.  The answer was found in the same publication from Microsoft, in section 2.2.1.  The matrix needs to be made a little more cunning - fortunately, jerasure allows us to do this with ease. <p></p><br><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">8</span></span>): k = <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-keyword"><span class="hljs-keyword">index</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> * k + <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> is_first_half = <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> &lt; k / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: matrix[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>] = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_first_half <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> elif <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> == <span class="hljs-number"><span class="hljs-number">1</span></span>: matrix[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>] = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_first_half <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> elif <span class="hljs-keyword"><span class="hljs-keyword">row</span></span> == <span class="hljs-number"><span class="hljs-number">2</span></span>: shift = <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_first_half <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> ** (k / <span class="hljs-number"><span class="hljs-number">2</span></span>) relative_column = <span class="hljs-keyword"><span class="hljs-keyword">column</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> is_first_half <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">column</span></span> - k / <span class="hljs-number"><span class="hljs-number">2</span></span>) matrix[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>] = shift * (<span class="hljs-number"><span class="hljs-number">1</span></span> + relative_column) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: prev = <span class="hljs-keyword"><span class="hljs-keyword">array</span></span>[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span> - k] matrix[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>] = libjerasure.galois_single_multiply(prev, prev, <span class="hljs-number"><span class="hljs-number">8</span></span>)</code> </pre> <br><p>  With such a matrix, the test is successful: </p><table><tbody><tr><td>  one </td><td>  one </td><td>  one </td><td>  one </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td></tr><tr><td>  0 </td><td>  0 </td><td>  0 </td><td>  0 </td><td>  one </td><td>  one </td><td>  one </td><td>  one </td></tr><tr><td>  one </td><td>  2 </td><td>  3 </td><td>  four </td><td>  sixteen </td><td>  32 </td><td>  48 </td><td>  64 </td></tr><tr><td>  one </td><td>  four </td><td>  five </td><td>  sixteen </td><td>  29 </td><td>  116 </td><td>  105 </td><td>  205 </td></tr></tbody></table>  So we formed groups.  But we must not forget that the system must remain operational when the entire availability zone falls.  Using LRC, this can be achieved if the blocks are arranged in zones as follows: <p></p><br><img src="https://habrastorage.org/files/43a/2f9/2f6/43a2f92f6b5f470eb9b4d546d18f59f5.png"><br><br><p>  Here it can be seen that whatever line we cross out, there will be no more than three errors in each locality group, which means that the data can be read.  If only one disk on which one block lies breaks, the data can be read by requesting only one additional block from another access zone.  Considerations about the block size are about the same as in the XOR scheme, with one exception: if we assume that the reading will be too expensive only between accessibility zones, then the block size can be made four times smaller, since a continuous sequence of within one zone. </p><br><h3>  Practice </h3><br><p>  Now you understand that it is a fairly simple task to program the calculation of redundancy codes, and you can apply this knowledge in your projects.  The options that we considered in the article: </p><br><ul><li>  Replicas - provide multiple redundancy, are easily realizable (often - the default option), work both at the block and at the file level. </li><li>  XOR, redundancy - 1.5, can be implemented both at the block and file level, requires three availability zones. </li><li>  Reed-Solomon codes allow flexible selection of the degree of redundancy, but to restore one data block, you need to read as many blocks as one strip contains.  Work well only at the block level. </li><li>  LRCs are similar to Reed-Solomon codes, but with single failures they allow reading less data, although they provide data recovery in a smaller percentage of cases. </li></ul><br><p>  In MDS, we applied the LRC-8-2-2 scheme (8 data blocks, 2 local parity blocks and 2 global parity blocks).  As a result, 1 drop, which had 2 replicas and lived on 2 hard drives, began to be located on 12 hard drives.  This significantly complicated the reading procedure, and recovery after losing the hard drive also became more difficult.  But we got 25% savings in disk space, which outweighed all the disadvantages.  In order to load the network less, data recording is performed in ordinary drops with replicas.  We convert them only when they are completely filled and the readings become smaller - that is, when the data "cool". </p><br><p>  We will tell about the problems that we had during the implementation of the scheme <a href="https://events.yandex.ru/events/meetings/15-oct-2016/%3Futm_source%3Dhabr%26utm_medium%3Darticles%26utm_campaign%3DInfr">on October 15 at an event</a> in our Moscow office.  Come, it will be interesting! </p></div><p>Source: <a href="https://habr.com/ru/post/311806/">https://habr.com/ru/post/311806/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../311794/index.html">Gamification aka gamification in business</a></li>
<li><a href="../311796/index.html">Banking Trojan Qadars returned and attacks banks in the UK</a></li>
<li><a href="../311800/index.html">Design as a coder</a></li>
<li><a href="../311802/index.html">Service Desk - quick start. 3 part. Creating a single entry point</a></li>
<li><a href="../311804/index.html">Once again about the promises</a></li>
<li><a href="../311808/index.html">learnopengl. Lesson 1.4 - Hello Triangle</a></li>
<li><a href="../311812/index.html">Deep Learning Course Overview</a></li>
<li><a href="../311816/index.html">How to stop being afraid and fall in love with mbed [Part 5]</a></li>
<li><a href="../311818/index.html">Barrier control with Arduino UNO and 433 MHz radio transmitter</a></li>
<li><a href="../311820/index.html">Meet the dictator! Small and kind</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>