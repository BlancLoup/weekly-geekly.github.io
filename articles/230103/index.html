<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic models: LDA, part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue the conversation. Last time, we took the first step in the transition from the naive Bayes classifier to the LDA: we removed the need to m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic models: LDA, part 2</h1><div class="post__text post__text-html js-mediator-article">  We continue the conversation.  <a href="http://habrahabr.ru/company/surfingbird/blog/228249/">Last time,</a> we took the first step in the transition from the naive Bayes classifier to the LDA: we removed the need to mark up the training set from the naive Bayes, making it a clustering model that you can train with the EM algorithm.  Today, I no longer have any excuses - I‚Äôll have to talk about the LDA model itself and show how it works.  Once we <a href="http://habrahabr.ru/company/surfingbird/blog/150607/">talked</a> about LDA in this blog, but then the story was very short and without very significant details.  I hope that this time it will be possible to tell more and more clearly. <br><img src="https://habrastorage.org/getpro/habr/post_images/f2e/890/d9a/f2e890d9abf70fd2241d90699112c381.jpg"><br><a name="habracut"></a><br><br><h3>  LDA: intuition </h3><br>  So, we start talking about the LDA.  Let me remind you that the main goal of our generalization is to ensure that the document is not rigidly tied to one topic, and the words in the documents could come from several topics at once.  What does this mean in (mathematical) reality?  This means that the document is a <i>mixture</i> of topics, and each word can be generated from one of the topics in this mixture.  Simply put, we first throw a dice-document, defining a theme <i>for each word</i> , and then pull the word out of the corresponding bag. <br><br>  Thus, the basic intuition of the model looks like this: <br><ul><li>  there are <i>themes</i> in the world (unknown in advance) that reflect what parts of a document can be about; </li><li>  each topic is a probability distribution in words, i.e.  a bag of words from which you can with different probabilities pull out different words; </li><li>  each document is a mixture of topics, i.e.  probability distribution on topics, a die that can be thrown; </li><li>  The process of generating each word is to first select a topic in the distribution corresponding to the document, and then select a word from the distribution corresponding to that topic. </li></ul>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Since intuition is the most important thing, and here it is not the simplest, I will repeat the same thing again in other words, now a little more formally.  It is convenient to understand and represent probabilistic models in the form of generative processes, when we consistently describe how one unit of data is generated, introducing along the way all probabilistic assumptions that we make in this model.  Accordingly, the generating process for the LDA must consistently describe how we generate each word of each document.  And this is how it happens (hereinafter I will assume that the length of each document is set ‚Äî you can also add it to the model, but usually this does not give anything new): <br><ul><li>  for each topic <i>t</i> : <br><ul><li>  choose the vector <i>œÜ <sub>t</sub></i> (distribution of words in the topic) according to the distribution <img src="https://habrastorage.org/getpro/habr/post_images/cc6/a38/198/cc6a38198d29b86e67226e2bfab267fa.png">  ; </li></ul></li><li>  for each document <i>d</i> : <br><ul><li>  choose vector <img src="https://habrastorage.org/getpro/habr/post_images/5fc/443/f81/5fc443f8187edc3579708df4823f14e4.png">  - the vector of "severity" of each topic in this document; </li><li>  for each word in the document <i>w</i> : <br><ul><li>  select a topic <i>z</i> <sub><i>w</i></sub> by distribution <img src="https://habrastorage.org/getpro/habr/post_images/a95/1fb/e9b/a951fbe9bfa79ced8a7416e0773953b0.png">  ; </li><li>  choose a word <img src="https://habrastorage.org/getpro/habr/post_images/28f/29c/b16/28f29cb160cf59fa50b9924ea2b77460.png">  with probabilities given in Œ≤. </li></ul></li></ul></li></ul><br><br>  Here, the process of choosing a word with the data <i>Œ∏</i> and <i>œÜ</i> should already be clear to the reader - we throw a die for each word and choose the topic of the word, then pull the word out of the corresponding bag: <br><br><img width="700" src="https://habrastorage.org/getpro/habr/post_images/46e/401/465/46e4014652374181426834ec71190123.png"><br><br>  In the model, it remains to explain only where these <i>Œ∏</i> and <i>œÜ</i> come from and what the expression <img src="https://habrastorage.org/getpro/habr/post_images/5fc/443/f81/5fc443f8187edc3579708df4823f14e4.png">  ;  it also explains why this is called Dirichlet allocation. <br><br><h3>  What does Dirichlet have to do with it? </h3><br>  Johann Peter Gustav Lejeun Dirichlet was not so much concerned with the theory of probability;  he mainly specialized in mathematical analysis, number theory, Fourier series, and other similar mathematics.  In probability theory, he was not so often seen, although, of course, his results in probability theory would make a brilliant career in modern mathematics (the trees were greener, the sciences were less studied, and, in particular, the mathematical Klondike was also not exhausted).  However, in his studies of mathematical analysis, Dirichlet, in particular, managed to take this classical integral, which as a result was called the Dirichlet integral: <br><img src="https://habrastorage.org/getpro/habr/post_images/a3e/c55/758/a3ec55758007f76d13ff60bdc20fb485.png"><br>  where Œì (Œ± <sub>i</sub> ) is a gamma function, the continuation of factorial (for natural numbers <img src="https://habrastorage.org/getpro/habr/post_images/e52/034/86c/e5203486c6449702a67c1a17f0d7a5ed.png">  ).  Dirichlet himself derived this integral for the needs of celestial mechanics, then Liouville found a simpler conclusion, which was generalized to more complex forms of integrand functions, and wrap everything up. <br><br>  The Dirichlet distribution, motivated by the Dirichlet integral, is the distribution on the ( <i>n</i> -1) -dimensional vector <i>x</i> <sub>1</sub> , <i>x</i> <sub>2</sub> , ..., <i>x</i> <sub><i>n</i> -1</sub> , defined as follows: <br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/8ca/ae4/e5b8caae4000a0417fb9e88167a28c36.png"><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/50d/44a/a14/50d44aa14f22be3638505d61e2a6b459.png">  , and the distribution is given only on the ( <i>n</i> -1) -dimensional simplex, where <i>x</i> <sub><i>i</i></sub> &gt; 0.  From the Dirichlet integral in it appeared normalization constant.  And the meaning of the Dirichlet distribution is quite elegant - it is a <i>distribution on distributions</i> .  Look - as a result of vector sketching over the Dirichlet distribution, a vector with non-negative elements is obtained, the sum of which is equal to one ... ba, and this is a discrete probability distribution, actually a dishonest cube with <i>n</i> edges. <br><br>  Of course, this is not the only possible class of distributions on discrete probabilities, but the Dirichlet distribution also has a number of other attractive properties.  The main thing is that it is a <i>conjugate prior distribution</i> for that same dishonest cube;  I will not explain in detail what this means (perhaps later), but, in general, it is reasonable to take the Dirichlet distribution as an a priori distribution in probabilistic models, when you want to get an unfair cube as a variable. <br><br>  It remains to discuss what, in fact, will be obtained for different values ‚Äã‚Äãof Œ± <sub><i>i</i></sub> .  First of all, let us immediately confine ourselves to the case when all Œ± <sub><i>i are the</i></sub> same - we a priori have no reason to prefer some topics to others, we at the beginning of the process still do not know where which topics stand out.  Therefore, our distribution will be symmetrical. <br><br>  What does it mean - the distribution on the simplex?  What does it look like?  Let's draw several such distributions - we can draw a maximum in three-dimensional space, so the simplex will be two-dimensional - a triangle, in other words, - and the distribution will be over this triangle (the picture is taken <a href="http://cos.name/2013/01/lda-math-beta-dirichlet/">from here</a> ; in principle, you understand what it is about, but very interesting why Uniform (0,1) is signed under Pandora's box - if anyone understands, tell me :)): <br><img src="https://habrastorage.org/getpro/habr/post_images/891/6ab/680/8916ab68010dac3bb199fa6c8bd2e70d.png"><br>  But similar distributions in two-dimensional format, in the form of heatmap (as they can be drawn, explained, for example, <a href="http://blog.bogatron.net/blog/2014/02/02/visualizing-dirichlet-distributions/">here</a> );  however, here 0.1 would be completely invisible, so I replaced it with 0.995: <br><img width="700" src="https://habrastorage.org/getpro/habr/post_images/332/94c/472/33294c472239e99c9af3869286f84163.png"><br><br>  And this is what happens: if Œ± = 1, we get a uniform distribution (which is logical - then all the powers in the density function are zero).  If Œ± is greater than one, then with a further increase in Œ±, the distribution becomes more and more concentrated in the center of the simplex.  For large Œ±, we will almost always get almost completely honest cubes.  But the most interesting thing here is what happens when Œ± is less than one ‚Äî in this case, the distribution focuses on the corners and edges / sides of the simplex!  In other words, for small Œ±, we will obtain <i>sparse</i> cubes ‚Äî distributions in which almost all probabilities are equal or close to zero, and only a small part of them are essentially nonzero.  This is exactly what we need - and in the document we would like to see some essentially expressed topics, and the topic would be well enough to clearly outline a relatively small subset of words.  Therefore, in LDA, Œ± and Œ≤ hyper parameters are usually less than one, usually Œ± = 1 / <i>T</i> (inverse to the number of topics) and Œ≤ = Const / <i>W</i> (a constant like 50 or 100 divided by the number of words). <br><br>  Disclaimer: my presentation follows the simplest and most straightforward approach to the LDA, I try to convey a general understanding of the model with large strokes.  In fact, of course, there are more detailed studies on how to choose the Œ± and Œ≤ hyperparameters, what they give and what they lead to;  in particular, hyperparameters are not at all required to be symmetrical.  See, for example, the article Wallach, Nimho, McCallum " <a href="https://people.cs.umass.edu/~wallach/publications/wallach09rethinking.pdf">Rethinking LDA: Why Priors Matter</a> ". <br><br><h3>  LDA: graph, co-distribution, factorization </h3><br>  Let's summarize.  We have already explained all parts of the model ‚Äî the Dirichlet a priori distribution produces dice-documents and sack-themes, throwing a cube-document determines its own theme for each word, and then the word itself is selected from the corresponding sack-theme.  Formally speaking, it turns out that the graphical model looks like this: <br><img width="400" src="https://habrastorage.org/getpro/habr/post_images/899/71e/e52/89971ee5218140f17e73849ad86849d6.png"><br>  Here the inner plate corresponds to the words of one document, and the outer plate corresponds to the documents in the body.  No need to be deceived by the external simplicity of this picture, there are more than enough cycles here;  here, for example, a fully expanded factor graph for two documents, one of which has two words, and the other three: <br><img width="700" src="https://habrastorage.org/getpro/habr/post_images/c3e/704/d7f/c3e704d7fd17cf44d85481a16a364963.png"><br><br>  And the joint probability distribution, it turns out, is factorized as follows: <br><img src="https://habrastorage.org/getpro/habr/post_images/252/b7b/55e/252b7b55e503af11e4338efef48a404a.png"><br><br>  What is the task of learning the LDA model now?  It turns out that only words are given to us, nothing is given any more, and we must somehow train all-all-all these parameters.  The task looks very scary - nothing is known and everything must be pulled out of the set of documents: <br><img width="700" src="https://habrastorage.org/getpro/habr/post_images/0f0/117/bf9/0f0117bf9ced94708b6bab12eb6ff61a.png"><br><br>  Nevertheless, it is solved with the help of the very methods of approximate inference that we talked about earlier.  Next time we will talk about how to solve this problem, apply the Gibbs sampling method (see <a href="http://habrahabr.ru/company/surfingbird/blog/226677/">one of the previous installations</a> ) to the LDA model and derive the update equations for variables <i>z <sub>w</sub></i> ;  we will see that in fact all this terrible mathematics is not so terrible at all, and the LDA is quite understandable not only conceptually, but also algorithmically. </div><p>Source: <a href="https://habr.com/ru/post/230103/">https://habr.com/ru/post/230103/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../230091/index.html">WEXLER.TAB 8q Review</a></li>
<li><a href="../230093/index.html">Return of nicknames: Google+ canceled the requirement to specify only real names</a></li>
<li><a href="../230097/index.html">Various Visual Studio profiles and installation of extensions to them</a></li>
<li><a href="../230099/index.html">Mozilla Releases Improved JPEG Encoder</a></li>
<li><a href="../230101/index.html">I twist, I twist, I twist, I twist pedals</a></li>
<li><a href="../230105/index.html">The history of "Titans"</a></li>
<li><a href="../230109/index.html">The interactive map of the solar system was translated into Russian</a></li>
<li><a href="../230111/index.html">Templates in XtraLayoutControl 14.1.5</a></li>
<li><a href="../230115/index.html">Data security on Android, deep links from Facebook and the new MOBA to mobile phones - the main mobile news of the week</a></li>
<li><a href="../230117/index.html">Theory of Relativity in the Real World: GPS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>