<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Comparison of 7 photogrammetry systems. What is better to choose?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Greetings, Habr. This is my first attempt at translating an article. I hope it will be useful not only to the residents of Habr, but also to archaeolo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Comparison of 7 photogrammetry systems. What is better to choose?</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/55d/47a/9a9/55d47a9a984d4aa8a08fe7e230d4fa0d.jpg"><br><br>  <i>Greetings, Habr.</i>  <i>This is my first attempt at translating an article.</i>  <i>I hope it will be useful not only to the residents of Habr, but also to archaeologists, on whom the original article is oriented.</i> <br><br>  When I explain to people that photogrammetry is similar to the process of 3D scanning with photos, they always trust me, because it seems too fantastic to be true.  Just imagine, take a few pictures of the object, send them to the algorithm and get a textured 3D model.  Wow 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      After presenting the model, the second question asked is accuracy.  What is the accuracy of 3D scanning by photo?  Answer: <a href="http://www.dailymail.co.uk/sciencetech/article-3678651/Animal-avengers-rescue-Adorable-puppy-eat-3D-printed-tooth-replaces-one-broke-chewing.html">submillimeter range</a> .  Again, I am surprised by the expression of distrust.  Fortunately, our team wrote a <a href="http://www.portaldeperiodicos.unisul.br/index.php/JR_Dentistry/article/viewFile/1993/1415">scientific article</a> about an experiment that showed an average deviation of 0.78 mm, that is, less than one millimeter compared to a scanned 3D model made with a laser scanner. <br><a name="habracut"></a><br>  Just like in the laser scanner market, photogrammetry has many different test software options.  They range from proprietary and proprietary solutions, to open and free.  And precisely, among such programs and solutions, comes the third question, which is still unanswered, at least officially: which photogrammetry software is the best? <br><br>  This question is difficult to answer, because the answer largely depends on the situation.  But thinking about this, among the many approaches that I have taken over a long time, I decided to answer in such a way as to give a simple and lengthy answer. <br><br><h2>  Lord Sipan's Skull </h2><br>  In July 2016, I traveled to Lambayeque, Peru, where I met face to face with Lord Sipan‚Äôs skull.  Analyzing it, I realized that it would be possible to restore his face using the forensic technique of face reconstruction.  The skull, however, was broken and distorted by years of pressure, which he endured in his grave, found complete in 1987, one of the great archaeological expeditions led by Dr. Walter Alva. <br><br><img src="https://habrastorage.org/files/ad3/cbe/21b/ad3cbe21bb474101906363903b664d58.jpg"><br><br>  To restore the skull, I took 120 photos on the ASUS Zenphone 2 smartphone and with these photos, I resumed reconstruction work.  Parallel to this process, professional photographer Ra√∫l Martin from the marketing department of Inca University Garcilaso de la Vega (sponsor of my trip) took 96 photos on a Canon EOS 60D camera.  Of these, I selected 46 images to continue the experiment. <br><br><img src="https://habrastorage.org/files/9f1/f17/967/9f1f1796787c4d85bcbbce8a60315e15.jpg"><br>  <i>Specialist of the Ministry of Culture of Peru, begins the process of digitizing the skull (center)</i> <br><br>  A day after the photographic survey, the Peruvian Ministry of Culture sent specialists in the field of laser scanning to scan Lord Sipan's skull, using Leica ScanStation C10 equipment.  The final point cloud was sent after 15 days, that is, when I received the data from the laser scanner, all the models obtained using photogrammetry were ready. <br><br>  We had to wait all this time, since the model obtained with the help of equipment is the gold standard, that is, all the grids obtained using photogrammetry will be compared, one by one, with it. <br><br><img src="https://habrastorage.org/files/60e/b87/560/60eb87560dae4f4f97364e08f8013eb4.png"><br>  <i>Full point cloud imported to MeshLab after conversion made to CloudCompare</i> <br><br>  The point cloud from the scan was in the .LAS and .E57 files ... and I had never heard of them.  I had to do a lot of research to figure out how to open them on Linux using free software.  It was decided to do this in CloudCompare, which offers the ability to import .E57 files.  Then I exported the model as .ply to be able to open the MeshLah and reconstruct the 3D mesh using the Poisson algorithm. <br><br><img src="https://habrastorage.org/files/e3f/710/a6d/e3f710a6db464e378cee1a3b64968ba5.jpg"><br>  <i>3D mesh reconstructed from point cloud.</i>  <i>The color of the vertex (above) and the surface with only one color (below).</i> <br><br>  As noted above, the jaw and the surface of the table where the parts were placed were also scanned.  The part associated with the skull was isolated and removed for the experiment to be performed.  I will not deal with these details here, since the volume is different.  I have already written other materials explaining how to remove irrelevant parts of a point / mesh cloud. <br><br>  For scanning using photogrammetry, the following systems were selected: <br><br>  1) <a href="http://imagine.enpc.fr/~moulonp/openMVG/">OpenMVG</a> (Open Multiple View Geometry library) + <a href="https://github.com/cdcseacave/openMVS">OpenMVS</a> (Open Multi-View Stereo reconstruction library): A thin point cloud is calculated in OpenMVG and a dense point cloud in OpenMVS. <br>  2) <b>OpenMVG</b> + <a href="http://www.di.ens.fr/pmvs/">PMVS</a> (Patch-based Multi-view Stereo Software): A discharged point cloud is calculated in OpenMVG, and then a dense point cloud using PMVS. <br>  3) <a href="http://www.gcc.tu-darmstadt.de/home/proj/mve/">MVE</a> (Multi-View Environment): Complete photogrammetry system. <br>  4) <a href="http://www.agisoft.com/">Agisoft Photoscan</a> : Complete and closed photogrammetry system. <br>  5) <a href="http://recap360.autodesk.com/">Autodesk Recap 360</a> : Complete online photogrammetry system. <br>  6) <a href="http://www.123dapp.com/catch">Autodesk 123D Catch</a> : A complete online photogrammetry system. <br>  7) <a href="http://184.106.205.13/arcteam/ppt.php">PPT-GUI</a> (Python Photogrammetry Toolbox with a graphical user interface): A discharged cloud of points is generated by the <a href="https://www.cs.cornell.edu/~snavely/bundler/">Bundler</a> , and later <b>PMVS</b> creates a dense cloud. <br><br><img src="https://habrastorage.org/files/588/19d/16b/58819d16b4f941629ecd83fdc19f6ef8.png"><br><br>  Above we have a table containing important aspects of each of the systems.  In general, at least apparently, there is not a single system that stands out much more than others. <br><br><img src="https://habrastorage.org/files/bf8/8fe/175/bf88fe175e1b478a9178fe4e520fd53b.png"><br><br>  Generation of a discharged cloud + generation of a dense cloud + 3D mesh + textures, a little time to upload photos and 3D meshes (in cases with Recap 360 and 123D Catch). <br><br><img src="https://habrastorage.org/files/241/bd6/0fb/241bd60fbc0346d3a1b150d15ec191c6.png"><br>  <i>Alignment based on common points</i> <br><br><img src="https://habrastorage.org/files/529/0cb/23a/5290cb23a5444d26b5eb37441e38ee62.png"><br>  <i>Aligned Skulls</i> <br><br>  All meshes were imported into Blender and combined with laser scanning. <br><br><img src="https://habrastorage.org/files/af9/667/a12/af9667a125c64c378c54b36c02b5d2ec.jpg"><br><br>  Above, we see that all the grids are nearby.  We can see that some surfaces are so dense that we notice only the edges, as is the case with 3D scanning and OpenMVG + PMVS.  Initially very important information ... the texture in the scanned meshes, as a rule, deceives us in relation to the quality of the scan, so in this experiment I decided to ignore the texturing results and focus on the 3D surface.  Therefore, I exported all the original models to the .stl format, which, as you know, has no information about the texture. <br><br><img src="https://habrastorage.org/files/7b0/31f/30d/7b031f30da234bb2938bc46b69141f0e.jpg"><br><br>  Looking closely, we will see that the result is consistent with a less dense result of units in the grid.  The ultimate goal of scanning, at least in my work, is to obtain a grid that is consistent with the original object.  If this grid is simplified, because it is in harmony with the real volumetric aspect, it is even better, because when the 3D grid has fewer faces, it will be processed faster when released. <br><br><img src="https://habrastorage.org/files/d3c/1d6/f1a/d3c1d6f1a44c461194381ca642a325d9.png"><br><br>  If we look at the file sizes (.stl exported without texture), which is a good comparison indicator, we see that the net mesh created in OpenMVG + OpenMVS is 38.4 MB in size and Recap 360 is only 5.1 MB! <br><br>  After several years of working with photogrammetry, I realized that the best thing to do when we are faced with a very dense mesh is to simplify the mesh so that we can process it in real time.  It‚Äôs hard to say whether this is true or not, as it is a patented and closed solution, but I believe that Recap 360 and 123D Catch generate complex grids, but at the end of the process they simplify it significantly, so they work on any hardware (PCs and smartphones ), preferably with WebGL support (interactive 3D in an Internet browser). <br><br>  Soon we will return to the discussion of this situation associated with the simplification of the grids, and now let's compare them. <br><br><h2>  How 3D Grid Comparison Works </h2><br>  After all the skulls were cleaned and aligned with the gold standard (laser scanning), it's time to compare the grids in CloudCompare.  But how does the 3D grid comparison technology work? <br><br>  For illustration, I created some didactic elements.  Let's go back to them. <br><br><img src="https://habrastorage.org/files/d03/ec1/d41/d03ec1d411c0460588d50f2c22659668.png"><br>  This didactic element deals with two planes with surfaces of zero thickness (this is possible in 3D modeling), forming X. <br><br><img src="https://habrastorage.org/files/db2/714/608/db271460804647789a6d9a9e7c9738fa.png"><br><br>  Then we have object A and object B. In the final part of both sides, the ends of the planes are separated by millimeters.  Where there is an intersection, the distance is, of course, 0 mm. <br><br><img src="https://habrastorage.org/files/20f/831/002/20f831002fc340a995d4c7d288a418a1.png"><br><br>  When comparing two grids in CloudCompare, they turn out to be pigmented with a color spectrum that goes from blue to red.  The picture above shows two already pigmented plans, but we must remember that they are two different elements and the comparison is made in two points, one in relation to the other. <br><br><img src="https://habrastorage.org/files/971/962/87b/97196287b9cf4586bbe0291504f8a027.png"><br><br>  Now we have a clear idea of ‚Äã‚Äãhow this works.  Basically, what happens is this: we set the distance limit, in this case 5 mm.  The ‚Äúout‚Äù grid tries to be pigmented red, and the one that ‚Äúin‚Äù tends to be colored blue and what is at the intersection, that is, on the same line, is usually pigmented in green. <br><br><img src="https://habrastorage.org/files/a44/c62/db6/a44c62db6e47456ab02d7a73198adbc6.png"><br><br>  Now I will explain the approach used in this experiment.  See above, we have an element with a central area that tends to zero, and the ends are set to +1 mm and -1 mm.  This does not appear in the image, but the element that we use for comparison is a simple plane located in the center of the scene, right around the base of the 3D bell, or those that are ‚Äúpointing up‚Äù when those that are ‚Äúlooking down‚Äù. <br><br><img src="https://habrastorage.org/files/39a/e6b/6e3/39ae6b6e385b408c97178a9ffc82f927.png"><br><br>  As I said earlier, we have set a comparison limit.  It was initially set at + 2mm and -2mm.  What if we change this limit to + 1mm and -1mm?  See that this happened in the figure above, and the part that goes beyond the boundaries. <br><br><img src="https://habrastorage.org/files/550/c82/a8f/550c82a8feb740338c9c15ca66fe1542.png"><br><br>  We can remove parts that go beyond rendering, so that they do not interfere with us. <br><br><img src="https://habrastorage.org/files/571/512/fab/571512fab17c4c28880d651ec559c2f4.png"><br><br>  Thus, as a result, in the grid there is only a percentage of the structure <br>  For those who understand a little more 3D modeling, it is clear that the comparison is made on the vertices, not the faces.  Because of this, we have a jagged edge. <br><br><h2>  Skull comparison </h2><br>  Photogrammetry was compared with a laser scanner within +1 mm and -1 mm.  Everything outside the spectrum has been erased. <br><br>  OpenMVG + OpenMVS <br><img src="https://habrastorage.org/files/89c/e73/54c/89ce7354c4bf458388eb21c8ed163e5f.png"><br><br>  OpenMVG + PMVS <br><img src="https://habrastorage.org/files/4ee/c88/2e9/4eec882e95ad4dda9045fbae6a8f1d3e.png"><br><br>  Photoscan <br><img src="https://habrastorage.org/files/f5e/c01/5a3/f5ec015a314647c591b6fd7c7297f2eb.png"><br><br>  MVE <br><img src="https://habrastorage.org/files/5e8/e24/990/5e8e24990f904b6fbc91f2e4a1fe1ac3.png"><br><br>  Recap 360 <br><img src="https://habrastorage.org/files/052/de6/43c/052de643cbd3439cac3f269056bddac2.png"><br><br>  123D Catch <br><img src="https://habrastorage.org/files/0e4/877/323/0e4877323c804402a87ed9671aacb1ff.png"><br><br>  Ppt-gui <br><img src="https://habrastorage.org/files/451/cc8/043/451cc804317d4bf491de8574ecd762c3.png"><br><br><img src="https://habrastorage.org/files/c0b/efe/83e/c0befe83e25a44f08b3604fad5dd98e5.png"><br><br>  By putting everything in comparison, we see that there is a strong tendency to reduce the error to zero.  All seven photogrammetry systems are effectively aligned with laser scanning! <br><br><img src="https://habrastorage.org/files/a86/c6f/0d9/a86c6f0d9ef84b4fb228bf18e5f010ff.png"><br><br>  We now turn to the question of the size of files.  One thing that always bothered me in comparison with photogrammetry results was the consideration of polygons generated by the grid reconstruction algorithm.  As I said above, this does not make much sense, since in the case of the skull we can simplify the surface and nevertheless it retains the information necessary for work during the anthropological examination and forensic reconstruction of a person. <br><br>  In light of this, I decided to align all the files, leaving them compatible in size and polygons.  To do this, I took as a basis the smaller file size that 123D Catch generates and used the MeshLab Quadratic Edge Collapse Detection filter to 25,000. This made 7 STL files of 1.3 MB each. <br><br>  With this alignment, we have a fair comparison between photogrammetry systems. <br><br><img src="https://habrastorage.org/files/409/dad/237/409dad23790d447d8305c104b644e7a2.jpg"><br><br>  Above we can visualize the stages of work.  In <b>Original, the</b> skulls are laid, aligned initially.  Then in <b>Compared,</b> we only see areas of interest in the skull, and finally, in <b>Decimated,</b> we have skulls aligned in size.  For an unsuspecting reader, this seems to be one image placed side by side. <br><br><img src="https://habrastorage.org/files/c90/1a3/3ce/c901a33ce1ae45d7b2971cd59e734dda.jpg"><br><br>  When we imagine the comparison in the "solid" model, we understand how all of them are compatible.  Now let's get to the conclusions. <br><br><h2>  findings </h2><br>  The most obvious conclusion is that all programs, with the exception of MVE, showed smaller definitions in the grid, and all photogrammetry systems had very similar visual results. <br><br>  <i>Does this mean that MVE is inferior to others?</i> <br><br>  No, quite the opposite.  MVE is a very reliable and practical system.  In another embodiment, I will present its use in the case of the manufacture of a prosthesis with millimeter quality.  In addition to this occasion, it was also used in other projects that make prosthetics, and this is an area that requires great precision, and it was successful.  Case was even published on the <a href="https://www.informatik.tu-darmstadt.de/de/studieninteressierte/news-und-ankuendigungen/einzelansicht/artikel/schnabel-aus-dem-3d-drucker-mit-technologie-aus-darmstadt/">official website of Darmstadt University</a> , the institute that develops it. <br><br>  <i>Which photogrammetry system is the best?</i> <br><br>  It is very difficult to answer this question, because a lot depends on the user style. <br><br>  <i>Which system is best for beginners?</i> <br><br>  Undoubtedly, this is Autodesk Recap 360. This is an online platform that can be accessed from any operating system that has an Internet browser that supports WebGL.  I have already tested directly on the smartphone, and it worked.  In the courses that I conducted on photogrammetry, I used this solution more and more, because students tend to understand the process much faster than other options. <br><br>  <i>What is the best system for modeling and animation for professionals?</i> <br><br>  I would like to point at Agisoft PhotoScan.  It has a graphical interface that allows, among other things, to create masks in the area of ‚Äã‚Äãinterest for photogrammetry, and also allows you to limit the area of ‚Äã‚Äãcalculation, which dramatically reduces the processing time on the machine.  In addition, it exports in a wide variety of formats, offering the opportunity to show where the cameras were while photographing the scene. <br><br>  <i>Which system do you like the most?</i> <br><br>  Well, personally, I appreciate everything in certain situations.  My favorite mix for today is OpenMVG + OpenMVS.  Both are open and can be accessed using the command line, which allows me to control a number of properties, adjusting the scanning process if necessary, whether it be a reconstruction of a face, a skull or any other piece.  Although I like this solution very much, it has some problems, such as not combining cameras with respect to models, when rare clouds of a scene are imported into Blender.  To solve this problem, I use PPT-GUI, which generates a rarefied cloud from the Bundler and matches, that is, it aligns the cameras with the cloud more accurately.  Another problem associated with OpenMVG + OpenMVS is that it ultimately does not create a full dense cloud, even if sparse data shows that all cameras are aligned.  To solve this problem, I use PMVS, which, although it generates less dense grids than OpenMVS, it is very reliable and works in almost all cases.  Another problem with open source options is the need to compile programs.  Everything works very well on my computers, but when I have to transfer the solutions to students or those interested, it becomes a big headache.  For the end user, it is important to have software in which images are loaded on one side, and on the other side a 3D model is obtained and this is offered by patented solutions.  In addition, the licenses of the models obtained are more understandable in these applications, and I feel safer in the field of professional modeling than, for example, using the templates generated in PhotoScan.  Technically, you pay for the license and can generate templates at will, using them in your work.  So, this solution is similar to Autodesk solutions. </div><p>Source: <a href="https://habr.com/ru/post/320766/">https://habr.com/ru/post/320766/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../320756/index.html">PHP Digest number 101 - interesting news, materials and tools (January 15 - 29, 2017)</a></li>
<li><a href="../320758/index.html">The revival of the IT-questionnaire or how to follow the questions toster and stackoverflow v2</a></li>
<li><a href="../320760/index.html">Is it time to quit?</a></li>
<li><a href="../320762/index.html">A small comparison of the performance of the MongoDB vs ClickHouse DBMS</a></li>
<li><a href="../320764/index.html">Dell EMC: Convergence Conversion</a></li>
<li><a href="../320768/index.html">BGP Fake-AS</a></li>
<li><a href="../320772/index.html">The difference between statistics and data science</a></li>
<li><a href="../320776/index.html">Google Chrome Extensions: a quick do-it-yourself translator</a></li>
<li><a href="../320778/index.html">Recheck SharpDevelop: What's New?</a></li>
<li><a href="../320780/index.html">Vysor is a handy tool for testing.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>