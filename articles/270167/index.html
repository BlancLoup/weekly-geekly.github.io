<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of the first elastic data warehouse Snowflake Elastic Data Warehouse</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In our company, we regularly test and analyze new and interesting technologies in the field of storage and management of big data. In April, represent...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of the first elastic data warehouse Snowflake Elastic Data Warehouse</h1><div class="post__text post__text-html js-mediator-article">  In our company, we regularly test and analyze new and interesting technologies in the field of storage and management of big data.  In April, representatives of Snowflake Computing contacted us and offered to try their product Snowflake Elastic Data Warehouse - a cloud-based data warehouse.  They are working to create an elastic system that could easily expand as needed - with an increase in data volume, workload, and other troubles. <br><br>  Usually DBMS work in conditions when the amount of available resources is limited by the available equipment.  To add resources, you need to add or replace servers.  In the cloud, resources are available at the moment when they are needed, and they can be returned if they are no longer needed.  The architecture of Snowflake allows you to take full advantage of the cloud: data storage can instantly expand and contract without interrupting your running requests. <br><a name="habracut"></a><br>  There are other data warehouses that work in the clouds, the most famous is Amazon Redshift.  But for their expansion it is still necessary to add servers, albeit virtual ones.  What entails the redistribution of data, which means in one way or another downtime.  About the compression of such storage is not at all, no one wants to once again shift all the data from place to place. <br><br>  Snowflake provides the client with an elastic data warehouse as a service (Data Warehouse as a Service).  This is a high-performance column DBMS that supports standard SQL and meets the requirements of ACID.  The data is accessed via the Snowflake Web UI, the Snowflake Client command-line interface, as well as ODBC and JDBC. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We conducted a full cycle of testing Snowflake, which includes testing the performance of data loading, test SQL queries, system scaling, as well as basic operating scenarios. <br><br>  But before proceeding to the test results, it is worthwhile to dwell on the architecture in more detail. <br><br><h4>  Architecture </h4><br>  Snowflake uses Amazon Web Services as its platform.  A DBMS consists of three components: a data storage level (Database Storage), a data processing level (Processing), and cloud services (Cloud Services). <br><br><img src="https://habrastorage.org/files/050/5cc/707/0505cc7079cb45db83c20ccb3f5ee6a0.png"><br><br>  <b>The data storage level is</b> responsible for the safe, secure and resilient data storage.  These wonderful properties are not provided by Snowflake itself, but by S3.  Data is stored in S3, customers do not have direct access to them.  To load data into Snowflake, special S3 buckets (staging area) are created, where you need to add files, from which you can then load data using Snowflake SQL.  To create a staging area and copy files to it, you can use standard Amazon Web Services services or special Snowflake SQL functions.  You can connect existing S3 buckets as Staging area, then files can be downloaded without prior copying.  When loading, data is compressed (gzip) and converted to column format.  Snowflake indexes are not provided. <br><br>  The distribution of data (data distribution) is carried out automatically based on statistics on the use of data.  There is no paring. <br><br>  Data access is carried out through <b>the data processing level</b> - this is a set of virtual servers that can access S3 files.  ‚ÄúVirtual Clusters‚Äù can consist of 1 (X-Small), 2 (Small), 4 (Medium), 8 (Large) or 16 (X-Large) virtual servers (EC2).  All servers in a virtual cluster are the same and equivalent, the client does not have direct access to them.  From a client‚Äôs point of view, a virtual cluster is a single unit.  EC2 servers can be of two types: Standard and Enterprise.  At the time of testing, EC2 with 16Gb RAM, 160Gb SSD, 8 cores were used as Standard.  Enterprise - 2 times more.  Snowflake has an agreement with Amazon, they receive new instances within 5 minutes.  Those.  expanding a virtual cluster or creating a new one takes 1-5 minutes.  Since EC2 does not store any data, their loss is not at all critical.  If a failure occurs, EC2 is simply recreated. <br>  SSD virtual servers are used as a cluster cache.  When executing a query, the data from S3 will go to the cluster cache, and all subsequent queries will use this cache.  When data is available in the cache, queries are executed up to 10 times faster. <br><br>  You can create multiple virtual clusters that will simultaneously access the same data.  This allows you to better distribute the load.  For example, you can use different virtual clusters to access different tables - in this case, the data of different tables will be cached by different clusters, which will actually increase the database cache. <br><br>  Data management uses <b>cloud services</b> Snowflake.  Using Snowflake UI, the client creates databases and tables, users, and roles.  Metadata is used at the data processing level to determine whether you have the necessary access rights, to compile queries, etc.  How and where metadata is stored, Snowflake does not explain. <br><br>  For each virtual cluster, you need to select a database with which it will work.  Before executing the request, the user must specify which cluster needs to be executed.  You can set a default cluster. <br>  You can start the cluster on a schedule, you can automatically when you receive a request, you can automatically stop if no requests were received within a certain time.  You can also increase or decrease the number of servers in a running cluster. <br><br>  As a result, the client receives an elastic storage, which is in no way limited in size.  No installation, configuration, and maintenance procedures are required ‚Äî Snowflake provides all of this.  You just need to go to the site, create tables, download data, and run the query. <br><br><h5>  Special features </h5><br>  Snowflake has several interesting features.  For example, it is possible to restore a recently deleted object: a table, a schema, and even a database. <br><br>  Another feature allows you to see the results of a recently executed query.  For re-viewing the data is available within 24 hours only to the user who executed the request.  To view the saved results do not need a virtual cluster, because  Snowflake stores them with metadata. <br><br>  Snowflake is very proud of the ability to process partially structured data, such as JSON or Avro.  Data is loaded as is, without any transformation and without a specific scheme.  Then you can refer to them in SQL, indicating certain "fields" of the record.  As part of testing, we did not check the speed of such calls, but usually in such cases, performance is sacrificed for convenience.  For example, in Vertica, this functionality works much slower than querying regular tables. <br><br><h4>  Price </h4><br>  The cost of service consists of payment for the amount of data in S3 and hourly payment for used virtual servers.  Those.  if the client has 2 virtual clusters of 4 servers each, and they worked 9 hours and 3 minutes, then you will have to pay for 2 * 4 * 10 = 80 service hours.  It is important that virtual servers are paid only for the time.  This allows you to significantly save if the load is uneven throughout the day or the system is used only sporadically. <br><br><h4>  Testing </h4><br>  Our test dataset consisted of one large table (just over 1 billion rows) and several small (from 100 to 100 thousand rows) - a simple star-type diagram.  Data downloaded from text files in CSV format.  Snowflake advises downloading data in small chunks, one file on the CPU core - so the resources will be used most efficiently.  To do this, you need to split the file into pieces before it got to the Staging area.  Faster to copy data from one table to another.  When data is changed, the table is locked exclusively (reading is possible, all DML requests are queued). <br><br><div class="spoiler">  <b class="spoiler_title">Load time of 1 billion lines, from a text file and from a table</b> <div class="spoiler_text"><table><tbody><tr><th>  Cluster size </th><th>  Insert from file </th><th>  Time </th></tr><tr><td>  Medium (4 EC2) </td><td>  1 file 8 GB <br>  copy into table1 from @ ~ / file / file.txt.gz file_format = 'csv' </td><td>  42m 49.663s </td></tr><tr><td>  Medium (4 EC2) </td><td>  11 files of 750 MB <br>  copy into table1 from @ ~ / file / file_format = 'csv' </td><td>  3m 45.272s </td></tr><tr><td>  Small (2 EC2) </td><td>  11 files of 750 MB <br>  copy into table1 from @ ~ / file / file_format = 'csv' </td><td>  4m 33.432s </td></tr><tr><th></th><th>  Copy from another table </th><th></th></tr><tr><td>  Medium (4 EC2) </td><td>  insert into table2 select * from table1 </td><td>  1m 30.713s </td></tr><tr><td>  Small (2 EC2) </td><td>  insert into table2 select * from table1 </td><td>  2m 42.358s </td></tr></tbody></table><br></div></div><br>  Then we proceeded to simple queries.  Here we were pleased with Snowflake, requests for two or three tables with filters, both for large and small, were performed fairly quickly. <br><div class="spoiler">  <b class="spoiler_title">For example, select count (*), sum (float1) from table</b> <div class="spoiler_text"><table><tbody><tr><th>  Cluster size </th><th>  Size table </th><th>  First run (S3) </th><th>  Restart (cache) </th></tr><tr><td>  Small (2 EC2) </td><td>  1 billion lines, 24.4 GB </td><td>  22.48 seconds </td><td>  1.91 seconds </td></tr><tr><td>  Small (2 EC2) </td><td>  5 billion lines </td><td>  109 seconds </td><td>  7.34 seconds </td></tr><tr><td>  Medium (4 EC2) </td><td>  1 billion lines, 24.4 GB </td><td>  10.67 seconds </td><td>  1.2 seconds </td></tr><tr><td>  Medium (4 EC2) </td><td>  5 billion lines </td><td></td><td>  3.65 seconds </td></tr></tbody></table><br></div></div><br>  It can be seen that the table scan is performed 10 or more times faster if the data has been cached.  You can also see that Snowflake scales well, i.e.  An increase in the number of servers in a cluster gives an almost linear increase in performance. <br><br>  To analyze the results obtained, a reference is always needed.  In our case, these are the results of conducting the same test on a Vertica cluster of 5 servers.  All five servers are the same: 2xE5-2630, 32GB RAM, 8x1000GB R10 SATA. <br><div class="spoiler">  <b class="spoiler_title">Compare test runtime between Snowflake and Vertica</b> <div class="spoiler_text"><table><tbody><tr><th></th><th>  Snowflake (first) </th><th>  Snowflake (second) </th><th>  Vertica </th></tr><tr><th>  Cluster size </th><th>  Medium (4 EC2) </th><th>  Medium (4 EC2) </th><th>  cluster of 5 servers </th></tr><tr><th>  Size of table (rows) </th><th>  1 billion </th><th>  1 billion </th><th>  1 billion </th></tr><tr><td>  select count (*) from table1 </td><td>  0.2 </td><td>  0.27 </td><td>  0.69 </td></tr><tr><td>  select count (*), sum (float1) from table1 </td><td>  10.6 </td><td>  1.33 </td><td>  2.89 </td></tr><tr><td>  select count (*), sum (float1) from table1 where <b>country_key = 1</b> </td><td>  5.8 </td><td>  1.41 </td><td>  3.63 </td></tr><tr><td>  select count (*), sum (float1) from table1 a, <b>country c</b> <br>  where <b>c.country_code = 'US'</b> and a.country_key = c.country_key </td><td>  2.3 </td><td>  1.70 </td><td>  2.14 </td></tr><tr><td>  select count (*), sum (float1) from table1 a, country c <br>  where <b>c.country_code = 'ZA'</b> and a.country_key = c.country_key; </td><td>  2.1 </td><td>  1.51 </td><td>  1.94 </td></tr><tr><td>  select count (*), sum (float1), <b>c.country_code</b> from table1 a, country c <br>  where <b>a.country_key = 1</b> and a.country_key = c.country_key <br>  <b>group by c.country_code</b> </td><td>  2.3 </td><td>  1.99 </td><td>  2.17 </td></tr><tr><td>  select count (*), sum (float1), c.country_code from table1 a, country c <br>  where <b>a.country_key&gt; -1 and a.country_key &lt;100000</b> and a.country_key = c.country_key <br>  group by c.country_code; </td><td>  4.4 </td><td>  4.17 </td><td>  3.26 </td></tr><tr><td>  select count (*), sum (float1), c.country_code from table1 a, country c <br>  where <b>c.country_code in ('US', 'GB')</b> and a.country_key = c.country_key <br>  group by c.country_code; </td><td>  2.3 </td><td>  2.22 </td><td>  2.42 </td></tr><tr><td>  select count (*), sum (float1), c.country_code from table1 a, country c <br>  where <b>c.country_code in ('US', 'GB')</b> and a.country_key = c.country_key and <b>a.time_key &lt;45000</b> <br>  group by c.country_code; </td><td>  3.8 </td><td>  1.47 </td><td>  1.03 </td></tr><tr><td>  select count (*), sum (float1), c.country_code from table1 a, country c, <b>time t</b> <br>  where <b>c.country_code in ('US', 'GB')</b> and a.country_key = c.country_key <br>  and <b>t.date&gt; = '2013-03-01' and t.date &lt;'2013-04-01'</b> and t.time_key = a.time_key <br>  group by c.country_code; </td><td>  3.3 </td><td>  1.97 </td><td>  1.23 </td></tr><tr><td>  select count (*), sum (float1), c.country_code, <b>r.revision_name</b> <br>  from table1 a, country c, time t, <b>revision r</b> <br>  where c.country_code in ('US', 'GB') and a.country_key = c.country_key <br>  and t.date&gt; = '2013-03-01' and t.date &lt;'2013-04-01' and t.time_key = a.time_key <br>  and r.revision_key = a.revision_key group by c.country_code, <b>r.revision_name</b> ; </td><td>  4.4 </td><td>  2.66 </td><td>  1.49 </td></tr><tr><th>  Total test time, seconds </th><th>  41.5 </th><th>  20.69 </th><th>  22.89 </th></tr></tbody></table><br></div></div><br>  Thus, with enough cache, the Snowflake Medium cluster of 4 EC2 can easily compete with the Vertica cluster of 5 servers.  Moreover, on scans Snowflake is quite far ahead of Vertica, but with the complexity of requests it begins to lag behind.  Unfortunately, it is not clear to what extent you can count on the cache.  Monitoring is not available, it is also impossible to see usage statistics.  All you can see is how much data you had to read to fulfill a particular request, but you don‚Äôt see where the data was read from the cache or from S3.  It should also be borne in mind that when the table data changes, the cache is invalid.  However, synthetic test data should never be used to predict actual performance. <br><br>  In the next step, we tried to reproduce the data loading procedure that we use in Vertica.  First, the data from the file is loaded into a very wide table (about 200 fields), and then aggregated with varying degrees of detail and passed on to other tables.  This is where problems began to arise.  Queries with a large number of tables or columns could be compiled for several minutes.  If there was not enough memory to execute the request, no messages were output, instead, an incorrect result was simply returned.  Often the error messages were not informative, it was especially difficult to diagnose the format mismatch at boot.  We stopped the tests because it became clear that Snowflake is not yet ready to perform our tasks. <br><br><h4>  Conclusion </h4><br>  Testing lasted about a month.  During this time, specialists from Snowflake gave us support, helped with advice, even fixed a couple of bugs.  The technology is interesting, the ability to change the amount of resources on the fly looks very attractive. <br><br>  Cloud storage may be a good option if the project is new and there is not much data yet.  Especially if the fate of the project is unknown, and do not want to invest in infrastructure for storing and processing data.  But it‚Äôs still too early to plan the transfer of all data and systems to the cloud. </div><p>Source: <a href="https://habr.com/ru/post/270167/">https://habr.com/ru/post/270167/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../270155/index.html">We write the simplest plugin for ReSharper</a></li>
<li><a href="../270159/index.html">Significance of SPF</a></li>
<li><a href="../270161/index.html">Plugin for gulp - collect files in pieces</a></li>
<li><a href="../270163/index.html">As we for the first time in Ukraine held a security event on a grand scale</a></li>
<li><a href="../270165/index.html">Isolate the demons with systemd or ‚Äúyou don't need a docker for this!‚Äù</a></li>
<li><a href="../270169/index.html">Data ONTAP 8.3 ADP: FlashPool StoragePools</a></li>
<li><a href="../270171/index.html">Product Design Digest October 2015</a></li>
<li><a href="../270173/index.html">AST analysis using patterns</a></li>
<li><a href="../270175/index.html">Microsoft will refuse to support digital certificates based on SHA-1</a></li>
<li><a href="../270179/index.html">The most important argument against MySQL?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>