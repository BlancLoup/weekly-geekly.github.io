<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Highly reliable PostgreSQL cluster based on Patroni, Haproxy, Keepalived</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I recently got up a task: to configure the most reliable cluster of PostgreSQL 9.6 servers. 

 As planned, I wanted to get a cluster that is...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Highly reliable PostgreSQL cluster based on Patroni, Haproxy, Keepalived</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  I recently got up a task: to configure the most reliable cluster of PostgreSQL 9.6 servers. <br><br>  As planned, I wanted to get a cluster that is experiencing the loss of any server, or even several servers, and is able to automatically commission servers after crashes. <br><br>  Planning a cluster, I studied many articles, both from the main documentation for PostgreSQL, and various howto, including from Habr, and tried to set up a standard cluster with RepMgr, experimented with pgpool. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In general, it worked, but I occasionally had problems with switching, I needed manual intervention to recover from accidents, etc.  In general, I decided to look for more options. <br><br>  As a result, somewhere (no longer remember exactly where) I found a link to the wonderful project <a href="https://github.com/zalando/patroni">Zalando Patroni</a> , and wrap everything up ... <br><a name="habracut"></a><br><h3>  Introduction </h3><br>  Patroni is a python daemon that allows you to automatically serve PostgreSQL clusters with different types of replication and automatic role switching. <br><br>  Its special beauty, in my opinion, is that distributed <a href="https://en.wikipedia.org/wiki/Distributed_control_system">DCS</a> storages are used to maintain the cluster's relevance and choices (supported by Zookeeper, etcd, Consul). <br><br>  Thus, the cluster is easily integrated into almost any system, you can always find out who is the master at the moment, and the status of all servers with requests in DCS, or directly to Patroni via http. <br><br>  Well, it's just beautiful :) <br><br>  I tested the work of Patroni, tried to drop the wizard and other servers, tried to pour different bases (~ 25 GB base automatically rises from scratch to 10GB network in a few minutes), and in general I really liked the Patroni project.  After the complete implementation of the scheme described below, I conducted testing with a simple bencher, who went to the database at a single address, and experienced the crashes of all cluster elements (server master, haproxy, keepalived). <br><br>  The delay in transferring the role to the new master was a couple of seconds.  When the former master returns to the cluster, or a new server is added, the roles are not changed. <br><br>  To automate the deployment of the cluster and add new servers, it was decided to use the familiar Ansible (I will give links to the resulting roles at the end of the article).  As the DCS is the Consul already used in our country. <br><br>  The article has two main goals: to show PostgreSQL users that there is such a wonderful thing as Patroni (there are practically no references in Runet in general and in Habr√© in particular), and at the same time share some experience using Ansible using a simple example for those who are just starting to work with it. <br><br>  I will try to explain all the action at once on the example of the analysis of Ansible roles and playbooks.  Those who do not use Ansible will be able to transfer all actions to their favorite automated server management tool, or perform them manually. <br><br>  Since most of the yaml scripts will be long, I will wrap them in a spoiler. <br>  The story will be divided into two parts - preparing the servers and deploying the cluster directly. <br><br>  For those who are familiar with Ansible, the first part will not be interesting, so I recommend to go directly to the second. <br><br><h3>  Part I </h3><br>  For this example, I use Centos 7 based virtual machines. Virtual machines are deployed from a template that is periodically updated (kernel, system packages), but this topic is beyond the scope of this article. <br><br>  I will only note that no application or server software on virtual computers is pre-installed.  Also any cloud resources are suitable, for example with AWS, DO, vScale, etc.  For them, there are scripts for dynamic inventory and integration with Ansible, or you can fasten Terraform, so that the whole process of creating and deleting servers from scratch can be automated. <br><br>  First you need to create an inventory of used resources for Ansible.  Ansible is (and by default) located in / etc / ansible.  Create an inventory in the / etc / ansible / hosts file: <br><br><pre><code class="hljs pgsql">[pgsql] <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-01.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-02.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-03.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span></code> </pre> <br>  We use the internal domain zone .local, so the servers have such names. <br><br>  Next, you need to prepare each server to install all the necessary components and working tools. <br><br>  For this purpose we create a playbook in / etc / ansible / tasks: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/tasks/essentialsoftware.yml</b> <div class="spoiler_text"><pre> <code class="hljs perl">--- - name: Install essential software yum: name={{ item }} <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>=latest tags: software with_items: - ntpdate - bzip2 - zip - unzip - openssl-devel - mc - vim - atop - wget - mytop - screen - net-tools - rsync - psmisc - gdb - subversion - htop - <span class="hljs-keyword"><span class="hljs-keyword">bind</span></span>-utils - sysstat - nano - iptraf - nethogs - ngrep - tcpdump - lm_sensors - mtr - s3cmd - psmisc - gcc - git - python2-pip - python-devel - name: install the <span class="hljs-string"><span class="hljs-string">'Development tools'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">package</span></span> group yum: name: <span class="hljs-string"><span class="hljs-string">"@Development tools"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>: present</code> </pre><br></div></div><br>  The Essential package set is used to create a familiar working environment on any server. <br>  The development tools package group, some libraries -devel and python need pip-u to build Python modules for PostgreSQL. <br><br>  We use virtual machines based on VmWare ESXi, and for ease of administration, they need to run a vmware agent. <br><br>  To do this, we will launch the open agent vmtoolsd, and describe its installation in a separate playbook (since not all servers are virtual, and for some of them this task will not be necessary): <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/tasks/open-vm-tools.yml</b> <div class="spoiler_text"><pre> <code class="hljs perl">--- - name: Install <span class="hljs-keyword"><span class="hljs-keyword">open</span></span> VM tools <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> VMWARE yum: name={{ item }} <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>=latest tags: <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>-vm-tools with_items: - <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>-vm-tools - name: VmWare service start <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> enabling service: name=vmtoolsd.service <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>=started enabled=yes tags: <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>-vm-tools</code> </pre><br></div></div><br>  In order to complete the preparation of the server for installing the main part of the software, in our case, the following steps will be needed: <br><br>  1) set up time synchronization using ntp <br>  2) install and run zabbix agent for monitoring <br>  3) roll out the required ssh keys and authorized_keys. <br><br>  In order not to inflate the article too much with details not related to the cluster proper, I will briefly quote ansible playbooks performing these tasks: <br><br>  NTP: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/tasks/ntpd.yml</b> <div class="spoiler_text"><pre> <code class="hljs perl">--- - name: setting default timezone set_fact: timezone: name=Europe/Moscow <span class="hljs-keyword"><span class="hljs-keyword">when</span></span>: timezone is <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: setting TZ timezone: name={{ timezone }} <span class="hljs-keyword"><span class="hljs-keyword">when</span></span>: timezone is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> tags: - tz - tweaks - ntp - ntpd - name: Configurating cron <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ntpdate cron: name=<span class="hljs-string"><span class="hljs-string">"ntpdate"</span></span> minute=<span class="hljs-string"><span class="hljs-string">"*/5"</span></span> job=<span class="hljs-string"><span class="hljs-string">"/usr/sbin/ntpdate pool.ntp.org"</span></span> tags: - tz - tweaks - ntp - ntpd - name: ntpd stop <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> disable service: name=ntpd <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>=stopped enabled=<span class="hljs-keyword"><span class="hljs-keyword">no</span></span> tags: - tz - tweaks - ntp - ntpd ignore_errors: yes - name: crond restart <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> enabled service: name=crond <span class="hljs-keyword"><span class="hljs-keyword">state</span></span>=restarted enabled=yes tags: - tz - tweaks - ntp - ntpd</code> </pre><br></div></div><br>  First, it checks whether the server has a personal time zone, and if not, it sets Moscow time (we have the majority of such servers). <br><br>  We do not use ntpd because of problems with time sailing on ESXi virtual machines, after which ntpd refuses to synchronize time.  (And tinker panic 0 does not help).  Therefore, we simply run the ntp client cron once every 5 minutes. <br><br>  Zabbix-agent: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/tasks/zabbix.yml</b> <div class="spoiler_text"><pre> <code class="hljs dos">--- - name: <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> zabbix ip external set_fact: zabbix_ip: <span class="hljs-number"><span class="hljs-number">132</span></span>.xx.xx.<span class="hljs-number"><span class="hljs-number">98</span></span> tags: zabbix - name: <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> zabbix ip internal set_fact: zabbix_ip: <span class="hljs-number"><span class="hljs-number">192</span></span>.<span class="hljs-number"><span class="hljs-number">168</span></span>.xx.<span class="hljs-number"><span class="hljs-number">98</span></span> when: ansible_all_ipv4_addresses | ipaddr('<span class="hljs-number"><span class="hljs-number">192</span></span>.<span class="hljs-number"><span class="hljs-number">168</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>/<span class="hljs-number"><span class="hljs-number">16</span></span>') tags: zabbix - name: Import Zabbix3 repo yum: name=http://repo.zabbix.com/zabbix/<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>/rhel/<span class="hljs-number"><span class="hljs-number">7</span></span>/x86_64/zabbix-release-<span class="hljs-number"><span class="hljs-number">3</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>-<span class="hljs-number"><span class="hljs-number">1</span></span>.el7.noarch.rpm state=present tags: zabbix - name: Remove old zabbix yum: name=zabbix2* state=absent tags: zabbix - name: Install zabbix-agent software yum: name={{ item }} state=latest tags: zabbix with_items: - zabbix-agent - zabbix-release - name: Creates directories file: <span class="hljs-built_in"><span class="hljs-built_in">path</span></span>={{ item }} state=directory tags: - zabbix - zabbix-mysql with_items: - /etc/zabbix/externalscripts - /etc/zabbix/zabbix_agentd.d - /var/lib/zabbix - name: <span class="hljs-built_in"><span class="hljs-built_in">Copy</span></span> scripts <span class="hljs-built_in"><span class="hljs-built_in">copy</span></span>: src=/etc/ansible/templates/zabbix/{{ item }} dest=/etc/zabbix/externalscripts/{{ item }} owner=zabbix group=zabbix <span class="hljs-built_in"><span class="hljs-built_in">mode</span></span>=<span class="hljs-number"><span class="hljs-number">0755</span></span> tags: zabbix with_items: - netstat.sh - iostat.sh - iostat2.sh - iostat_collect.sh - iostat_parse.sh - php_workers_discovery.sh - name: <span class="hljs-built_in"><span class="hljs-built_in">Copy</span></span> .my.cnf <span class="hljs-built_in"><span class="hljs-built_in">copy</span></span>: src=/etc/ansible/files/mysql/.my.cnf dest=/var/lib/zabbix/.my.cnf owner=zabbix group=zabbix <span class="hljs-built_in"><span class="hljs-built_in">mode</span></span>=<span class="hljs-number"><span class="hljs-number">0700</span></span> tags: - zabbix - zabbix-mysql - name: remove default configs file: <span class="hljs-built_in"><span class="hljs-built_in">path</span></span>={{ item }} state=absent tags: zabbix with_items: - /etc/zabbix_agentd.conf - /etc/zabbix/zabbix_agentd.conf - name: put zabbix-agentd.conf to default place template: src=/etc/ansible/templates/zabbix/zabbix_agentd.tpl dest=/etc/zabbix_agentd.conf owner=zabbix group=zabbix force=yes tags: zabbix - name: link zabbix-agentd.conf to /etc/zabbix file: src=/etc/zabbix_agentd.conf dest=/etc/zabbix/zabbix_agentd.conf state=link tags: zabbix - name: zabbix-agent <span class="hljs-built_in"><span class="hljs-built_in">start</span></span> and enable service: name=zabbix-agent state=restarted enabled=yes tags: zabbix</code> </pre><br></div></div><br>  When installing Zabbix, the agent config is rolled out of the template, you only need to change the server address. <br><br>  Servers located within our network go to 192.168.x.98, and servers that do not have access to it, to the real address of the same server. <br><br>  The transfer of ssh keys and the ssh setting are in a separate role, which can be found, for example, on an ansible-galaxy. <br><br>  There are many options, but the essence of the changes is quite trivial, so I don‚Äôt see any point in quoting all its content here. <br><br>  It is time to roll on the server configuration.  In general, I install all the components and the cluster itself in one step, already with the full config, but it seems to me that for the purposes of this tutorial, it would be better to divide it into two steps, respectively, by chapters. <br><br>  Create a playbook for a group of servers: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/cluster-pgsql.yml</b> <div class="spoiler_text"><pre> <code class="hljs actionscript">--- - hosts: pgsql pre_tasks: - name: Setting system hostname hostname: name=<span class="hljs-string"><span class="hljs-string">"{{ ansible_host }}"</span></span> - <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta">: tasks/essentialsoftware.yml - </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta">: tasks/open-vm-tools.yml - </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta">: tasks/ntpd.yml post_tasks: - </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta">: tasks/zabbix.yml roles: - ssh.role - ansible-role-patroni</span></span></code> </pre><br></div></div><br>  We start processing all servers: <br><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text">  ~ # ansible-playbook cluster-pgsql.yml --skip-tags patroni </div></div><br>  If you have completely downloaded my example from the githab repository, then you will also have the role of Patroni, which we do not need to work out yet. <br><br>  The --skip-tags argument causes Ansible to skip steps marked with this tag, so the role of ansible-role-patroni will not be executed now. <br><br>  If it is not on the disk, then nothing terrible will happen, Anisble will simply ignore this key. <br><br>  Ansible comes to my server immediately as root, and if you need to let the ansible under an unprivileged user, you should add the special ‚Äúbecome: true‚Äù flag to the steps requiring root rights, which will cause the ansible to use sudo calls for these steps. <br><br>  Preparation is over. <br><br><h3>  Part II </h3><br>  We proceed to the deployment of the cluster itself. <br><br>  Since it takes a lot of work to set up a cluster (install PostgreSQL and all components, upload individual configs for them), I selected the whole process into a separate role. <br><br>  Roles in Ansible allow you to group sets of related tasks, and thus simplify the writing of scripts and support them in working condition. <br><br>  I took the role template for installing Patroni here: <a href="https://github.com/gitinsky/ansible-role-patroni">https://github.com/gitinsky/ansible-role-patroni</a> , for which I thank its author. <br>  For my purposes, I reworked the existing one and added my haproxy and keepalived playbooks. <br><br>  My roles are in the / etc / ansible / roles directory.  Create a directory for a new role, and subdirectories for its components: <br><br><pre> <code class="hljs pgsql">~# mkdir /etc/ansible/roles/ansible-<span class="hljs-keyword"><span class="hljs-keyword">role</span></span>-patroni/tasks ~# mkdir /etc/ansible/roles/ansible-<span class="hljs-keyword"><span class="hljs-keyword">role</span></span>-patroni/templates</code> </pre> <br>  In addition to PostgreSQL, our cluster will consist of the following components: <br><br>  1) haproxy to monitor the status of servers and redirect requests to the master server. <br>  2) keepalived to ensure the presence of a single point of entry into the cluster - virtual IP. <br><br>  All playbooks executed by this role are listed in a file that is run by ansible by default: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/tasks/main.yml</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">- <span class="hljs-keyword"><span class="hljs-keyword">include</span></span>: postgres.yml - <span class="hljs-keyword"><span class="hljs-keyword">include</span></span>: haproxy.yml - <span class="hljs-keyword"><span class="hljs-keyword">include</span></span>: keepalived.yml</code> </pre><br></div></div><br>  Next, we begin to describe the individual task. <br><br>  The first playbook installs PostgreSQL 9.6 from the native repository, and additional packages required by Patroni, and then downloads Patroni itself from GitHub: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/tasks/postgres.yml</b> <div class="spoiler_text"><pre> <code class="hljs cmake">--- - name: Import Postgresql96 repo yum: name=https://download.postgresql.org/pub/repos/yum/<span class="hljs-number"><span class="hljs-number">9.6</span></span>/redhat/rhel-<span class="hljs-number"><span class="hljs-number">7</span></span>-x86_64/pgdg-centos96-<span class="hljs-number"><span class="hljs-number">9.6</span></span>-<span class="hljs-number"><span class="hljs-number">3</span></span>.noarch.rpm state=present tags: patroni when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: <span class="hljs-keyword"><span class="hljs-keyword">Install</span></span> PGsql96 yum: name={{ item }} state=latest tags: patroni with_items: - postgresql96 - postgresql96-contrib - postgresql96-server - python-psycopg2 - repmgr96 when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: checkout patroni git: repo=https://github.com/zalando/patroni.git dest=/opt/patroni tags: patroni when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: create /etc/patroni <span class="hljs-keyword"><span class="hljs-keyword">file</span></span>: state=directory dest=/etc/patroni tags: patroni when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: put postgres.yml template: src=postgres0.yml dest=/etc/patroni/postgres.yml backup=yes tags: patroni when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> python packages pip: name={{ item }} tags: patroni with_items: - python-etcd - python-consul - dnspython - boto - mock - requests - six - kazoo - click - tzlocal - prettytable - PyYAML when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: put patroni.service systemd unit template: src=patroni.service dest=/etc/systemd/system/patroni.service backup=yes tags: patroni when: <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> is <span class="hljs-keyword"><span class="hljs-keyword">defined</span></span> - name: Reload daemon definitions <span class="hljs-keyword"><span class="hljs-keyword">command</span></span>: /usr/bin/systemctl daemon-reload tags: patroni - name: restart service: name=patroni state=restarted enabled=yes tags: patroni</code> </pre><br></div></div><br>  In addition to installing the software, this playbook also fills in the configuration for the current Patroni server, and the systemd unit to start the daemon in the system, and then starts the Patroni daemon.  Config templates and systemd units should be in the templates directory inside the role. <br><br>  Patroni config template: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/templates/postgres.yml.j2</b> <div class="spoiler_text"><pre> <code class="hljs lua">name: {{ patroni_node_name }} scope: &amp;scope {{ patroni_scope }} consul: host: consul.services.<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>:<span class="hljs-number"><span class="hljs-number">8500</span></span> restapi: listen: <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>:<span class="hljs-number"><span class="hljs-number">8008</span></span> connect_address: {{ ansible_default_ipv4.address }}:<span class="hljs-number"><span class="hljs-number">8008</span></span> auth: <span class="hljs-string"><span class="hljs-string">'username:{{ patroni_rest_password }}'</span></span> bootstrap: dcs: ttl: &amp;ttl <span class="hljs-number"><span class="hljs-number">30</span></span> loop_wait: &amp;loop_wait <span class="hljs-number"><span class="hljs-number">10</span></span> maximum_lag_on_failover: <span class="hljs-number"><span class="hljs-number">1048576</span></span> # <span class="hljs-number"><span class="hljs-number">1</span></span> megabyte <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bytes postgresql: use_pg_rewind: <span class="hljs-literal"><span class="hljs-literal">true</span></span> use_slots: <span class="hljs-literal"><span class="hljs-literal">true</span></span> parameters: archive_mode: <span class="hljs-string"><span class="hljs-string">"on"</span></span> wal_level: hot_standby archive_command: mkdir -p ../wal_archive &amp;&amp; cp %p ../wal_archive/%f max_wal_senders: <span class="hljs-number"><span class="hljs-number">10</span></span> wal_keep_segments: <span class="hljs-number"><span class="hljs-number">8</span></span> archive_timeout: <span class="hljs-number"><span class="hljs-number">1800</span></span>s max_replication_slots: <span class="hljs-number"><span class="hljs-number">5</span></span> hot_standby: <span class="hljs-string"><span class="hljs-string">"on"</span></span> wal_log_hints: <span class="hljs-string"><span class="hljs-string">"on"</span></span> pg_hba: # Add following <span class="hljs-built_in"><span class="hljs-built_in">lines</span></span> to pg_hba.conf after <span class="hljs-built_in"><span class="hljs-built_in">running</span></span> <span class="hljs-string"><span class="hljs-string">'initdb'</span></span> - host replication replicator <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">16</span></span> md5 - host all all <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span> md5 postgresql: listen: <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>:<span class="hljs-number"><span class="hljs-number">5432</span></span> connect_address: {{ ansible_default_ipv4.address }}:<span class="hljs-number"><span class="hljs-number">5432</span></span> data_dir: /var/lib/pgsql/<span class="hljs-number"><span class="hljs-number">9.6</span></span>/data pg_rewind: username: superuser password: {{ patroni_postgres_password }} pg_hba: - host all all <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span> md5 - hostssl all all <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span> md5 replication: username: replicator password: {{ patroni_replicator_password }} network: <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">16</span></span> superuser: username: superuser password: {{ patroni_postgres_password }} admin: username: admin password: {{ patroni_postgres_password }} restore: /opt/patroni/patroni/scripts/restore.py</code> </pre><br></div></div><br>  Since each server in the cluster requires an individual configuration of Patroni, its config is in the form of a jinja2 template (postgres0.yml.j2 file), and the template step forces the ansible to broadcast this template with the replacement of variables, the values ‚Äã‚Äãof which are taken from a separate description for each server. <br><br>  Variables that are common to the entire cluster will be specified in the inventory, which now takes the following form: <br><br><div class="spoiler">  <b class="spoiler_title">/ etc / ansible / hosts</b> <div class="spoiler_text"><pre> <code class="hljs pgsql">[pgsql] <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-01.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-02.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> <span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>-pgsql<span class="hljs-number"><span class="hljs-number">-03.</span></span><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> [pgsql:vars] patroni_scope: "cluster-pgsql" patroni_rest_password: flsdjkfasdjhfsd patroni_postgres_password: flsdjkfasdjhfsd patroni_replicator_password: flsdjkfasdjhfsd cluster_virtual_ip: <span class="hljs-number"><span class="hljs-number">192.</span></span>xx.xx<span class="hljs-number"><span class="hljs-number">.125</span></span> &lt;/spoiler&gt;      -   host_vars/_: &lt;spoiler title="/etc/ansible/host_vars/pgsql-cluster-01.local/main.yml"&gt; &lt;source lang="yaml"&gt; patroni_node_name: cluster_pgsql_01 keepalived_priority: <span class="hljs-number"><span class="hljs-number">99</span></span></code> </pre><br></div></div><br>  Decipher why some variables are needed: <br><br>  patroni_scope - cluster name when registering with Consul <br>  patroni_node_name - server name when registering with Consul <br>  patroni_rest_password - password for http Patroni interface (required for sending commands to change the cluster) <br>  patroni_postgres_password: password for postgres user.  It is installed in the case of creating a new patroni base. <br>  patroni_replicator_password - password for user replicator.  On its behalf, replication is performed on the slaves. <br><br>  Also in this file are some other variables used in other playbooks or roles, in particular, it can be setting ssh (keys, users), timezone for the server, server priority in the cluster keepalived, etc. <br><br>  The configuration for other servers is similar, the server name and priority change accordingly (for example, 99-100-101 for three servers). <br><br>  Install and configure haproxy: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/tasks/haproxy.yml</b> <div class="spoiler_text"><pre> <code class="hljs delphi">--- - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: Install haproxy yum: <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=<span class="hljs-comment"><span class="hljs-comment">{{ item }</span></span>} state=latest tags: - patroni - haproxy with_items: - haproxy when: install <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> defined - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: put config template: src=haproxy.cfg.j2 dest=/etc/haproxy/haproxy.cfg backup=yes tags: - patroni - haproxy - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: restart <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> enable service: <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=haproxy state=restarted enabled=yes tags: - patroni - haproxy</code> </pre><br></div></div><br>  Haproxy is installed on each host, and contains in its config links to all PostgreSQL servers, checks which server is currently the master, and sends requests to it. <br>  For this check, a beautiful feature of Patroni is used - the REST interface. <br><br>  When accessing the URL <a href="http://server/">server</a> : 8008 (8008 is the default port) Patroni returns a report on the cluster status in json, and also reflects the response code http whether this server is a master.  If it is, there will be a response with code 200. If not, the answer is with code 503. <br><br>  I strongly advise you to go to the documentation for Patroni, the http interface is quite interesting there, it is also allowed to forcibly switch roles, and manage the cluster. <br>  Similarly, this can be done using the console utility patronyctl.py, supplied by Patroni. <br><br>  The haproxy configuration is fairly simple: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/templates/haproxy.cfg</b> <div class="spoiler_text"><pre> <code class="hljs sql">global maxconn 800 defaults log global mode tcp retries 2 timeout client 30m timeout connect 4s timeout server 30m timeout <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span>s frontend ft_postgresql bind *:<span class="hljs-number"><span class="hljs-number">5000</span></span> default_backend postgres-patroni backend postgres-patroni <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> httpchk <span class="hljs-keyword"><span class="hljs-keyword">http</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">check</span></span> expect <span class="hljs-keyword"><span class="hljs-keyword">status</span></span> <span class="hljs-number"><span class="hljs-number">200</span></span> <span class="hljs-keyword"><span class="hljs-keyword">default</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span> inter <span class="hljs-number"><span class="hljs-number">3</span></span>s fall <span class="hljs-number"><span class="hljs-number">3</span></span> rise <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> {{ patroni_node_name }} {{ patroni_node_name }}.local:<span class="hljs-number"><span class="hljs-number">5432</span></span> maxconn <span class="hljs-number"><span class="hljs-number">300</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">8008</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> {{ patroni_node_name }} {{ patroni_node_name }}.local:<span class="hljs-number"><span class="hljs-number">5432</span></span> maxconn <span class="hljs-number"><span class="hljs-number">300</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">8008</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> {{ patroni_node_name }} {{ patroni_node_name }}.local:<span class="hljs-number"><span class="hljs-number">5432</span></span> maxconn <span class="hljs-number"><span class="hljs-number">300</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">8008</span></span></code> </pre><br></div></div><br>  In accordance with this configuration, haproxy listens to port 5000, and sends traffic from it to the master server. <br><br>  The status check occurs at 1 second intervals, 3 failed responses (code 500) are required to transfer the server to Down, and 2 successful responses (with a code 200) to switch the server back. <br>  At any time, you can apply directly to any haproxy, and it will correctly route traffic to the master server. <br><br>  Also included with Patroni is a template for configuring the confd daemon, and an example of its integration with etcd, which allows you to dynamically change the haproxy config when removing or adding new servers. <br><br>  I'm still doing quite a static cluster, too much automation in this situation, IMHO, can lead to unforeseen problems. <br><br>  We wanted clients to make special changes to the logic, tracking servers lively, etc.  not required, so we make a single entry point to the cluster using keepalived. <br><br>  The keepalived daemon works under the vrrp protocol with its neighbors, and as a result of selecting one of the daemons as the main one (the priority is specified in the config, and templated into the keepalived_priority variable in host_vars for each server), it raises the virtual ip address. <br>  The rest of the demons are waiting patiently.  If the current main server keepalived for some reason dies or signals the neighbors a crash, re-election will occur, and the next highest priority server will pick up a virtual ip address. <br><br>  To protect against haproxy crashes, keepalived demons perform a scan by running the ‚Äúkillall -0 haproxy‚Äù command once a second.  It returns the code 0 if the haproxy process is, and 1 if it is not. <br>  If haproxy disappears, the keepalived daemon will signal the vrrp crash, and remove the virtual ip. <br>  Virtual IP will immediately pick up the next highest priority server, with live haproxy. <br><br>  Install and configure keepalived: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/tasks/keepalived.yml</b> <div class="spoiler_text"><pre> <code class="hljs delphi">--- - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: Install keepalived yum: <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=<span class="hljs-comment"><span class="hljs-comment">{{ item }</span></span>} state=latest tags: - patroni - keepalived with_items: - keepalived when: install <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> defined - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: put alert script template: src=alert.sh.j2 dest=/usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/sbin/alert.sh backup=yes mode=<span class="hljs-number"><span class="hljs-number">755</span></span> tags: - patroni - keepalived when: install <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> defined - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: put config template: src=keepalived.conf.j2 dest=/etc/keepalived/keepalived.conf backup=yes tags: - patroni - keepalived - <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>: restart <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> enable service: <span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=keepalived state=restarted enabled=yes tags: - patroni - keepalived</code> </pre><br></div></div><br>  In addition to the keepalived installation, this playbook also copies a simple script for sending alerts via telegrams.  The script accepts the message as a variable, and simply jerks the curl's telegram API. <br><br>  In this script, you only need to specify your token and telegram group ID to send alerts. <br><br>  The keepalived configuration is described as a jinja2 template: <br><br><div class="spoiler">  <b class="spoiler_title">/etc/ansible/roles/ansible-role-patroni/templates/keepalived.conf.j2</b> <div class="spoiler_text"><pre> <code class="hljs django"><span class="xml"><span class="xml">global_defs { router_id </span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ patroni_node_name }}</span></span><span class="xml"><span class="xml"> } vrrp_script chk_haproxy { script "killall -0 haproxy" interval 1 weight -20 debug fall 2 rise 2 } vrrp_instance </span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ patroni_node_name }}</span></span><span class="xml"><span class="xml"> { interface ens160 state BACKUP virtual_router_id 150 priority </span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ keepalived_priority }}</span></span><span class="xml"><span class="xml"> authentication { auth_type PASS auth_pass secret_for_vrrp_auth } track_script { chk_haproxy weight 20 } virtual_ipaddress { </span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ cluster_virtual_ip }}</span></span><span class="xml"><span class="xml">/32 dev ens160 } notify_master "/usr/bin/sh /usr/local/sbin/alert.sh '</span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ patroni_node_name }}</span></span><span class="xml"><span class="xml"> became MASTER'" notify_backup "/usr/bin/sh /usr/local/sbin/alert.sh '</span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ patroni_node_name }}</span></span><span class="xml"><span class="xml"> became BACKUP'" notify_fault "/usr/bin/sh /usr/local/sbin/alert.sh '</span></span><span class="hljs-template-variable"><span class="hljs-template-variable">{{ patroni_node_name }}</span></span><span class="xml"><span class="xml"> became FAULT'" }</span></span></code> </pre><br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The variables patroni_node_name, cluster_virtual_ip and keepalived_priority translate the corresponding data from host_vars. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Also in the keepalived config, a script is specified for sending status change messages to the telegram channel. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We roll the entire cluster configuration on the server:</font></font><br><br><pre> <code class="hljs objectivec">~<span class="hljs-meta"><span class="hljs-meta"># ansible-playbook cluster-pgsql.yml</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Since Ansible is idempotent, i.e. performs the steps only if they have not been performed previously, you can start the playbook without additional parameters. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you do not want to wait any longer, or you are sure that the servers are fully ready, you can start the ansible-playbook with the -t patroni key. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then only the steps from the role of Patroni will be performed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I note that I do not specify separately the role of servers - master or slave. This configuration will create an empty database, and the first configured server will simply become the master. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When adding new servers, Patroni will see through DCS what the cluster master already has, automatically copy the base from the current master, and connect the slave to it.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the case of the start of the slave who has lagged behind for some time from the master, Patroni will automatically inject changes with the help of pg_rewind. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We are convinced that all servers have started and chose roles for themselves:</font></font><br><br><pre> <code class="hljs objectivec">~<span class="hljs-meta"><span class="hljs-meta"># journalctl -f -u patroni</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Messages from the slave (server cluster-pgsql-01): </font></font><br><br><div class="spoiler">  <b class="spoiler_title">spoiler</b> <div class="spoiler_text"> <code>Feb 17 23:50:32 cluster-pgsql-01.local patroni.py[100626]: 2017-02-17 23:50:32,254 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_01 <br> Feb 17 23:50:32 cluster-pgsql-01.local patroni.py[100626]: 2017-02-17 23:50:32,255 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_01 <br> Feb 17 23:50:32 cluster-pgsql-01.local patroni.py[100626]: 2017-02-17 23:50:32,255 INFO: does not have lock <br> Feb 17 23:50:32 cluster-pgsql-01.local patroni.py[100626]: 2017-02-17 23:50:32,255 INFO: no action. i am a secondary and i am following a leader <br></code> <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Messages from the wizard (in this case, the server is cluster-pgsql-02): </font></font><br><div class="spoiler">  <b class="spoiler_title">spoiler</b> <div class="spoiler_text"> <code>Feb 17 23:52:23 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:23,457 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:52:23 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:23,874 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:52:24 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:24,082 INFO: no action. i am the leader with the lock <br> Feb 17 23:52:33 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:33,458 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:52:33 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:33,884 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:52:34 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:52:34,094 INFO: no action. i am the leader with the lock <br></code> <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The logs clearly show that each server constantly monitors its status and master status. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's try to stop the master:</font></font><br><br><pre> <code class="hljs objectivec">~<span class="hljs-meta"><span class="hljs-meta"># systemctl stop patroni</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">spoiler</b> <div class="spoiler_text"> <code>Feb 17 23:54:03 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:54:03,457 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:54:03 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:54:03,880 INFO: Lock owner: cluster_pgsql_02; I am cluster_pgsql_02 <br> Feb 17 23:54:04 cluster-pgsql-02.local patroni.py[4913]: 2017-02-17 23:54:04,092 INFO: no action. i am the leader with the lock <br> Feb 17 23:54:11 cluster-pgsql-02.local systemd[1]: Stopping Runners to orchestrate a high-availability PostgreSQL... <br> Feb 17 23:54:13 cluster-pgsql-02.local patroni.py[4913]: waiting for server to shut down.... done <br> Feb 17 23:54:13 cluster-pgsql-02.local patroni.py[4913]: server stopped <br></code> <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> But what happened at this moment on the slave: </font></font><br><br><div class="spoiler">  <b class="spoiler_title">spoiler</b> <div class="spoiler_text"> <code>Feb 17 19:54:12 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:12,353 INFO: does not have lock <br> Feb 17 19:54:12 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:12,776 INFO: no action. i am a secondary and i am following a leader <br> Feb 17 19:54:13 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:13,440 WARNING: request failed: GET http://192.xx.xx.121:8008/patroni (HTTPConnectionPool(host='192.xx.xx.121', port=8008 <br> ): Max retries exceeded with url: /patroni (Caused by NewConnectionError('&lt;requests.packages.urllib3.connection.HTTPConnection object at 0x1f12750&gt;: Failed to establish a new connection: [Er <br> rno 111] Connection refused',))) <br> Feb 17 19:54:13 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:13,444 INFO: Got response from cluster_pgsql_03 http://192.xx.xx.122:8008/patroni: {"database_system_identifier": "63847 <br> 30077944883705", "postmaster_start_time": "2017-02-17 05:36:52.388 MSK", "xlog": {"received_location": 34997272728, "replayed_timestamp": null, "paused": false, "replayed_location": 34997272 <br> 728}, "patroni": {"scope": "clusters-pgsql", "version": "1.2.3"}, "state": "running", "role": "replica", "server_version": 90601} <br> Feb 17 19:54:13 cluster-pgsql-01 patroni.py: server promoting <br> Feb 17 19:54:13 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:13,961 INFO: cleared rewind flag after becoming the leader <br> Feb 17 19:54:14 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:14,179 INFO: promoted self to leader by acquiring session lock <br> Feb 17 19:54:23 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:23,436 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_01 <br> Feb 17 19:54:23 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:23,857 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_01 <br> Feb 17 19:54:24 cluster-pgsql-01 patroni.py: 2017-02-17 23:54:24,485 INFO: no action. i am the leader with the lock <br></code> <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This server has intercepted the master role on itself. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And now we will return server 2 back to the cluster:</font></font><br><br><pre> <code class="hljs objectivec">~<span class="hljs-meta"><span class="hljs-meta"># systemctl start patroni</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Spoiler header</b> <div class="spoiler_text"> <code>Feb 18 00:02:11 cluster-pgsql-02.local systemd[1]: Started Runners to orchestrate a high-availability PostgreSQL. <br> Feb 18 00:02:11 cluster-pgsql-02.local systemd[1]: Starting Runners to orchestrate a high-availability PostgreSQL... <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,186 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,190 WARNING: Postgresql is not running. <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,190 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,398 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,400 INFO: starting as a secondary <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,412 INFO: rewind flag is set <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,609 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,609 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,609 INFO: changing primary_conninfo and restarting in progress <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:13,631 INFO: running pg_rewind from user=superuser host=192.xx.xx.120 port=5432 dbname=postgres sslmode=prefer sslcompression=1 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: servers diverged at WAL position 8/26000098 on timeline 25 <br> Feb 18 00:02:13 cluster-pgsql-02.local patroni.py[56855]: rewinding from last common checkpoint at 8/26000028 on timeline 25 <br> Feb 18 00:02:14 cluster-pgsql-02.local patroni.py[56855]: Done! <br> Feb 18 00:02:14 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:14,535 INFO: postmaster pid=56893 <br> Feb 18 00:02:14 cluster-pgsql-02.local patroni.py[56855]: &lt; 2017-02-18 00:02:14.554 MSK &gt; LOG: redirecting log output to logging collector process <br> Feb 18 00:02:14 cluster-pgsql-02.local patroni.py[56855]: &lt; 2017-02-18 00:02:14.554 MSK &gt; HINT: Future log output will appear in directory "pg_log". <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: localhost:5432 - accepting connections <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: localhost:5432 - accepting connections <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:15,790 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:15,791 INFO: Lock owner: cluster_pgsql_01; I am cluster_pgsql_02 <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:15,791 INFO: does not have lock <br> Feb 18 00:02:15 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:15,791 INFO: establishing a new patroni connection to the postgres cluster <br> Feb 18 00:02:16 cluster-pgsql-02.local patroni.py[56855]: 2017-02-18 00:02:16,014 INFO: no action. i am a secondary and i am following a leader <br></code> <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni discovered that she was connecting to the cluster with the existing master, and by updating the database to the current state, she correctly assumed the role of a slave. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's try to create an error on another layer of the cluster by stopping haproxy on the main server keepalived. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">By priority, this role is taken by the second server:</font></font><br><br><blockquote> <code>[root@cluster-pgsql-02 ~]# ip a <br> 2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 <br> link/ether 00:50:56:a9:b8:7b brd ff:ff:ff:ff:ff:ff <br> inet 192.xx.xx.121/24 brd 192.168.142.255 scope global ens160 <br> valid_lft forever preferred_lft forever <br> <b>inet 192.xx.xx.125/32 scope global ens160 &lt;----   </b> <br> valid_lft forever preferred_lft forever <br> inet6 fe80::xxx::4895:6d90/64 scope link <br> valid_lft forever preferred_lft forever <br></code> <br></blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Stop haproxy: </font></font><br><br><pre> <code class="hljs objectivec">~<span class="hljs-meta"><span class="hljs-meta"># systemctl stop haproxy ; journalctl -fl</span></span></code> </pre> <br><blockquote> <code>Feb 18 00:18:54 cluster-pgsql-02.local Keepalived_vrrp[25018]: VRRP_Script(chk_haproxy) failed <br> Feb 18 00:18:56 cluster-pgsql-02.local Keepalived_vrrp[25018]: VRRP_Instance(cluster_pgsql_02) Received higher prio advert <br> Feb 18 00:18:56 cluster-pgsql-02.local Keepalived_vrrp[25018]: VRRP_Instance(cluster_pgsql_02) Entering BACKUP STATE <br> Feb 18 00:18:56 cluster-pgsql-02.local Keepalived_vrrp[25018]: VRRP_Instance(cluster_pgsql_02) removing protocol VIPs. <br> Feb 18 00:18:56 cluster-pgsql-02.local Keepalived_vrrp[25018]: Opening script file /usr/bin/sh <br> Feb 18 00:18:56 cluster-pgsql-02.local Keepalived_healthcheckers[25017]: Netlink reflector reports IP 192.xx.xx.125 removed <br></code> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keepalived caught the problem, removed the virtual address from itself, and signaled its neighbors about it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Look what happened on the second server:</font></font><br><blockquote> <code>Feb 18 00:18:56 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) forcing a new MASTER election <br> Feb 18 00:18:56 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) forcing a new MASTER election <br> Feb 18 00:18:56 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) forcing a new MASTER election <br> Feb 18 00:18:56 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) forcing a new MASTER election <br> Feb 18 00:18:57 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Transition to MASTER STATE <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Entering MASTER STATE <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) setting protocol VIPs. <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Sending gratuitous ARPs on ens160 for 192.xx.xx.125 <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: Opening script file /usr/bin/sh <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Received lower prio advert, forcing new election <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Sending gratuitous ARPs on ens160 for 192.xx.xx.125 <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_healthcheckers[41189]: Netlink reflector reports IP 192.xx.xx.125 added <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Received lower prio advert, forcing new election <br> Feb 18 00:18:58 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Sending gratuitous ARPs on ens160 for 192.xx.xx.125 <br> Feb 18 00:19:03 cluster-pgsql-01.local Keepalived_vrrp[41190]: VRRP_Instance(cluster_pgsql_01) Sending gratuitous ARPs on ens160 for 192.xx.xx.125 <br></code> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Twice there were re-elections (because the third server of the cluster managed to send its announcement before the first election), server 1 assumed the role of moderator, and set up a virtual IP. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We are convinced of this:</font></font><br><blockquote> <code>[root@cluster-pgsql-01 log]# ip a <br> 2: ens160: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000 <br> link/ether 00:50:56:a9:f0:90 brd ff:ff:ff:ff:ff:ff <br> inet 192.xx.xx.120/24 brd 192.xx.xx.255 scope global ens160 <br> valid_lft forever preferred_lft forever <br> <b>inet 192.xx.xx.125/32 scope global ens160 &lt;----    !</b> <br> valid_lft forever preferred_lft forever <br> inet6 fe80::1d75:40f6:a14e:5e27/64 scope link <br> valid_lft forever preferred_lft forever <br></code> </blockquote><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now virtual IP is present on a server that is not a replication master. However, this does not matter, because we access haproxy through the database, and it monitors the cluster status independently, and always sends requests to the master. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When haproxy returns to the system on the second server, re-election occurs again (keepalived with a higher priority is put into operation), and the virtual IP returns to its place. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In rare cases it happens that the slave cannot catch up to the master (for example, he fell a long time ago and the wal magazine managed to partly retire). In this case, you can completely clear the directory with the base on the slave: </font></font><br><br> <s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"rm -rf /var/lib/pgsql/9.6/data", and restart Patroni. It will merge the base from the master entirely.</font></font><br> <i><u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(Carefully with cleaning up "unnecessary" databases, carefully look at which server you are running the command !!!)</font></font></u></i></s> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In this case, you need to use the patronictl utility. </font><font style="vertical-align: inherit;">The reinit command allows you to safely clean a specific cluster node; it will not be executed by the wizard. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thanks for the addition</font></font><a href="https://habrahabr.ru/users/cyberdemon/" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CyberDemon</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The patronictl utility itself allows you to see the current situation with the cluster via the command line, without contacting DCS, and manage the cluster. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example cluster status report:</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">/opt/patroni/patronictl.py -c /etc/patroni/postgres.yml list cluster-pgsql:</font></font><br><blockquote><pre><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">+ --------------- + ------------------ + -------------- --- + -------------- + ------------------ + ----------- +
</font></font> |<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Cluster | </font><font style="vertical-align: inherit;">Member | </font><font style="vertical-align: inherit;">Host | </font><font style="vertical-align: inherit;">Role | </font><font style="vertical-align: inherit;">State | </font><font style="vertical-align: inherit;">Lag in MB |</font></font><font></font>
+---------------+------------------+-----------------+--------------+------------------+-----------+<font></font>
 | cluster-pgsql | cluster_pgsql_01 | 192.xxx.xxx.120 | Leader | running | 0.0 |
 | cluster-pgsql | cluster_pgsql_02 | 192.xxx.xxx.121 | Sync standby | running | 0.0 |
 | cluster-pgsql | cluster_pgsql_03 | 192.xxx.xxx.122 |  | creating replica | 33712.0 |<font></font>
+---------------+------------------+-----------------+--------------+------------------+-----------+ </pre><br></blockquote><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this case, the third node is poured, its gap from the master is 33 GB. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After completing this process, it also enters the Running state with a zero lag. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can also note that the State field is empty. This is because the cluster in my case is running in synchronous mode. To reduce the lag of synchronous replication, one slave operates in synchronous mode, and the other in the usual asynchronous mode. In the case of the loss of the master role will shift, and the second slave will go into synchronous mode to become the master, the first slave.</font></font><br><br><h3>  Afterword </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The only thing that this cluster, in my opinion, is not enough for happiness is the pooling of connections and proxying read requests to all slaves to improve read performance, and insertion and update requests only to the master. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a configuration with asynchronous replication, unfolding the load on the read can lead to unexpected responses, if the slave falls behind the master, this should be taken into account. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Streaming (asynchronous) replication does not provide cluster consistency at any time, and this requires synchronous replication.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this mode, the master server will wait for confirmations about copying and applying transactions to the slaves, which will slow down the operation of the database. However, if transaction losses are invalid (for example, some financial applications), synchronous replication is your choice. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni supports all options, and if synchronous replication suits you better, you just need to change the value of several fields in the Patroni configs. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Questions of different replication techniques are perfectly sorted out in the Patroni documentation. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Someone will probably suggest using pgpool, which itself, in fact, covers all the functionality of this system. It can monitor databases, proxy requests, and set up virtual IP, as well as pool connections to clients.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yes he can. But in my opinion, the scheme with Patroni is much more transparent (of course this is just my opinion), and during experiments with pgpool I caught strange behavior with his search and virtual addresses, which did not debit too deep yet, deciding to look for another solution. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course it is possible that the problem is only mine in my hands, and later I plan to return to testing pgpool. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, in any case, pgpool will not be able to fully automatically manage the cluster, introduce new and (especially) return failed servers, work with DCS. In my opinion, this is the most interesting functionality of Patroni. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thank you for your attention, I will be glad to see suggestions for further improvement of this solution, and answer questions in the comments.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Many thanks to Zalando for Patroni, and to the authors of the original </font></font><a href="https://github.com/compose/governor"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Governor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> project </font><font style="vertical-align: inherit;">, which served as the basis for Patroni, and to </font></font><a href="https://github.com/alexclear"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alex Chistyakov</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for the role template for Ansible. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The full code of playbooks and Ansible templates described in the article </font></font><a href="https://github.com/imcitius/ansible-pgsql_patroni_cluster"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . I would be grateful for the improvements from the gurus Ansible and PostgreSQL.</font></font> :) <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Main articles and sources used: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Several PgSQL clusters: </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font><a href="https://habrahabr.ru/post/301370/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://habrahabr.ru/post/301370/</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí </font></font><a href="https://habrahabr.ru/post/213409/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://habrahabr.ru/post/213409/</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí </font></font><a href="https://habrahabr.ru/company/etagi/blog/314000/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://habrahabr.ru/company/etagi/ blog / 314000 /</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí a </font></font><a href="https://tech.zalando.com/blog/zalandos-patroni-a-template-for-high-availability-postgresql/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">post about Patroni on the Zalando blog</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí </font></font><a href="https://github.com/zalando/patroni"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Patroni project</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí </font></font><a href="https://github.com/gitinsky/ansible-role-patroni"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ansible-role-patroni Alex Chistyakov</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Üí </font></font><a href="https://github.com/compose/governor"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Governor</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - unfortunately the development has long been frozen. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Üí </font></font><a href="https://www.ansiblefordevops.com/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The book Ansble for Devops</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a great tutorial with lots of examples of using Ansible.</font></font></div><p>Source: <a href="https://habr.com/ru/post/322036/">https://habr.com/ru/post/322036/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../322026/index.html">Kotlin compiler message types to use in Suppress</a></li>
<li><a href="../322028/index.html">Friday format: myths about the ideal workplace</a></li>
<li><a href="../322030/index.html">How I got a job at the Stack Exchange</a></li>
<li><a href="../322032/index.html">Laboratory: ProContent 2.0. Closer to the user</a></li>
<li><a href="../322034/index.html">Interesting clustering algorithms, part two: DBSCAN</a></li>
<li><a href="../322040/index.html">Five things that mobile application developers need to know about cross-marketing</a></li>
<li><a href="../322042/index.html">Wiring diagrams in Autocad using .NET</a></li>
<li><a href="../322044/index.html">How IT professionals work. Boris Tikhomirov, Director of PROMT Mobile and Internet Projects</a></li>
<li><a href="../322046/index.html">Containers and safety: seccomp</a></li>
<li><a href="../322048/index.html">MOOC ‚ÄúProgramming and development of web applications‚Äù: announcement and interview with the author of the course of ITMO University</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>