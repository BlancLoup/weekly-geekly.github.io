<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating a safe AI: specifications, reliability and warranty</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Among the authors of the article are the safety team from the DeepMind company. 

 Build a rocket hard. Each component requires careful study and test...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating a safe AI: specifications, reliability and warranty</h1><div class="post__text post__text-html js-mediator-article">  <font color="gray">Among the authors of the article are the safety team from the DeepMind company.</font> <br><br>  Build a rocket hard.  Each component requires careful study and testing, with the basis of security and reliability.  Rocket scientists and engineers get together to design all systems: from navigation to control, engines and chassis.  Once all the parts are assembled, and the systems are checked, only then can we board the astronauts with the confidence that everything will be fine. <br><br>  If artificial intelligence (AI) is a <a href="https://www.ted.com/talks/max_tegmark_how_to_get_empowered_not_overpowered_by_ai">rocket</a> , then someday we all will get tickets on board.  And, as in rockets, safety is an important part of creating artificial intelligence systems.  Ensuring security requires careful system design from scratch to ensure that the various components work together as intended, while at the same time creating all the tools to monitor the successful operation of the system after it is commissioned. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      At a high level, security research at DeepMind focuses on designing reliable systems, while detecting and mitigating possible short-term and long-term risks.  <b>The technical safety of AI</b> is a relatively new, but rapidly developing field, the content of which varies from a high theoretical level to empirical and specific research.  The goal of this blog is to contribute to the development of the field and encourage substantive conversation about technical ideas, thereby promoting our collective understanding of the security of AI. <br><a name="habracut"></a><br>  In the first article, we will discuss three areas of AI technical safety: <b>specifications</b> , <b>reliability,</b> and <b>warranties</b> .  Future articles will generally conform to the boundaries outlined here.  Although our views inevitably change over time, we believe that these three areas cover a wide enough range to provide useful categorization for current and future research. <br><br><img src="https://habrastorage.org/webt/sv/8c/se/sv8cseuw2rlofm85zszbk6nhv6k.png"><br>  <i><font color="gray">Three problem areas of AI security.</font></i>  <i><font color="gray">Each block lists some relevant issues and approaches.</font></i>  <i><font color="gray">These three areas are not isolated, but interact with each other.</font></i>  <i><font color="gray">In particular, a specific security issue may include problems from several blocks.</font></i> <br><br><h1>  Specifications: system task definition </h1><br><h4>  Specifications ensure that the behavior of the AI ‚Äã‚Äãsystem is consistent with the true intentions of the operator. </h4><br>  Perhaps you know the myth of <a href="https://www.youtube.com/watch%3Fv%3Dnn8YGPZdCvA">King Midas</a> and the golden touch.  In one embodiment, the Greek god Dionis promised Midas any reward he wished, as a sign of gratitude that the king tried his best to show hospitality and mercy to his friend Dionysus.  Then <b>Midas asked that everything he touches turns into gold</b> .  He was beside himself with the joy of this new power: the oak branch, the stone and the roses in the garden ‚Äî everything turned into gold at his touch.  But he soon discovered the stupidity of his desire: even food and drink turned into gold in his hands.  In some versions of the story, even his daughter fell victim to a blessing that turned out to be a curse. <br><br>  This story illustrates the problem of specifications: how to correctly formulate our desires?  Specifications should ensure that the AI ‚Äã‚Äãsystem is committed to acting in accordance with the true wishes of the creator, rather than being tuned to a poorly defined or incorrect target.  Formally, there are three types of specifications: <br><br><ul><li>  <b>ideal specification</b> (" <b>wishes</b> "), corresponding to a hypothetical (but difficult to formulated) description of an ideal AI system, fully consistent with the desires of the human operator; </li><li>  <b>the project specification</b> (" <b>blueprint</b> "), which corresponds to the specification we <i>actually use</i> to create an AI system, for example, a specific reward function, to maximize which the reinforced learning system is programmed; </li><li>  <b>the identified specification</b> (" <b>behavior</b> "), which best describes the <i>actual behavior of the</i> system.  For example, the reward function revealed as a result of the reverse development after observing the behavior of the system (inverse reinforced learning).  This compensation function and specification are usually different from those programmed by the operator, because AI systems are not ideal optimizers or due to other unforeseen consequences of using the design specification. </li></ul><br>  <b>The specification problem</b> arises when there is a mismatch between the <b>ideal specification</b> and the <b>identified specification</b> , that is, when the AI ‚Äã‚Äãsystem does not do what we want from it.  Studying the problem from the technical security point of view of AI means: how to design more fundamental and general objective functions and help agents figure out if the goals are not defined?  If problems generate a discrepancy between the ideal and project specification, then they fall into the ‚ÄúDesign‚Äù subcategory, if between the design and the identified, then the ‚ÄúEmergence‚Äù subcategory. <br><br>  For example, in our scientific article <a href="https://arxiv.org/abs/1711.09883">AI Safety Gridworlds</a> (where other definitions of specifications and reliability problems are presented, as compared to this article), we give agents a reward function for optimization, but then we evaluate their actual performance by the ‚Äúsafety performance‚Äù, which is hidden from agents.  Such a system models these differences: the security function is an ideal specification that is incorrectly formulated as a remuneration function (design specification), and then implemented by the agents who create the specification, which is implicitly disclosed through their resulting policy. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pw/vd/cm/pwvdcm0ra3_bo4qzpu9gdc_nfco.gif"></div><br>  <i><font color="gray">From OpenAI <a href="https://blog.openai.com/faulty-reward-functions/">'s Defective Reward Functions in the Wild</a> : A reinforcement training agent found a random strategy for gaining more points.</font></i> <br><br>  As another example, consider the CoastRunners game, analyzed by our colleagues at OpenAI (see the animation above from ‚ÄúDefective reward functions in the wild‚Äù).  For most of us, the goal of the game is to quickly finish the track and get ahead of other players - this is our ideal specification.  However, translating this goal into an exact reward function is difficult, so CoastRunners rewards players (design specification) for hitting targets along the route.  Teaching an agent to play through reinforcement training leads to surprising behavior: the agent drives the boat in a circle to capture re-appearing targets, repeatedly breaking and catching fire rather than ending the race.  From this behavior, we conclude (the identified specification) that the game has an imbalance between instant reward and full circle reward.  There <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml">are many more such examples</a> when AI systems find loopholes in their objective specification. <br><br><h1>  Reliability: developing systems that resist disruption </h1><br><h4>  Reliability ensures that the AI ‚Äã‚Äãsystem continues to operate safely with interference </h4><br>  In real conditions, where AI systems work, a certain level of risk, unpredictability and volatility is necessarily present.  Artificial intelligence systems must be resistant to unforeseen events and hostile attacks that may damage or manipulate these systems.  Research into the <b>reliability</b> of artificial intelligence systems is aimed at ensuring that our agents remain within safe boundaries, regardless of the conditions that arise.  This can be achieved by avoiding risks ( <b>prevention</b> ) or by self-stabilization and smooth degradation ( <b>restoration</b> ).  Security issues arising from <b>distributive shift</b> , <b>hostile input data</b> (adversarial inputs) and <b>unsafe research</b> (unsafe exploration) can be classified as reliability problems. <br><br>  To illustrate the solution to the problem of <b>distribution shear</b> , consider a home cleaning robot that usually cleans rooms without pets.  Then the robot was launched into the house with a pet - and artificial intelligence collided with it during cleaning.  A robot that has never seen cats and dogs before, will wash them with soap, which will lead to undesirable results ( <a href="https://arxiv.org/pdf/1606.06565v1.pdf">Amodei and Olah et al., 2016</a> ).  This is an example of a reliability problem that can arise when the distribution of data during testing is different from the distribution during training. <br><br><img src="https://habrastorage.org/webt/oi/k0/lc/oik0lc_srvx7tovbrec-dmbzqsa.gif"><br>  <i><font color="gray">From the work of <a href="https://deepmind.com/blog/specifying-ai-safety-problems/">AI Safety Gridworlds</a> .</font></i>  <i><font color="gray">The agent learns to avoid lava, but when testing in a new situation, when the location of the lava has changed, he is not able to generalize knowledge - and runs straight into the lava</font></i> <br><br>  Hostile input is a specific case of distribution shear where the input data are specifically designed to trick the AI ‚Äã‚Äãsystem. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/550/89a/6d5/55089a6d587e1745783257a0c898b046.png"><br>  <i><font color="gray">A hostile entry superimposed on ordinary images may cause the classifier to recognize the sloth as a racing car.</font></i>  <i><font color="gray">Two images differ by a maximum of 0.0078 in each pixel.</font></i>  <i><font color="gray">The first is classified as a three-toed sloth with a probability of more than 99%.</font></i>  <i><font color="gray">The second - as a racing car with a probability of more than 99%</font></i> <br><br>  <b>An unsafe study</b> can demonstrate a system that seeks to maximize its performance and goal achievement, with no guarantee that safety will not be compromised during the study, as it studies and explores in its environment.  An example is a cleaning robot that pokes a wet mop into an electrical outlet, learning the best cleaning strategies ( <a href="http://www.jmlr.org/papers/volume16/garcia15a/garcia15a.pdf">Garc√≠a and Fern√°ndez, 2015</a> ; <a href="https://arxiv.org/pdf/1606.06565.pdf">Amodei and Olah et al., 2016</a> ). <br><br><h1>  Warranties: monitoring and control of system activity </h1><br><h4>  Assurance guarantees that we are able to understand and control AI systems during operation. </h4><br>  Although an elaborate safety precaution can eliminate many risks, it is difficult to do everything right from the start.  After the commissioning of AI systems, we need tools for their continuous monitoring and configuration.  Our last category, assurance, deals with these problems from two sides: <b>monitoring</b> and enforcing. <br><br>  <b>Monitoring</b> includes all methods of checking systems for analyzing and predicting their behavior, both with the help of human inspections (summary statistics) and with the help of automated inspections (to analyze a huge number of logs).  On the other hand, <b>submission</b> involves the development of mechanisms to control and limit the behavior of systems.  Issues such as <b>interpretability</b> and <b>interruptibility</b> belong to the subcategories of control and subordination, respectively. <br><br>  Artificial intelligence systems are not like us either in appearance or in the way of data processing.  This creates <b>interpretative</b> problems.  Well-designed measurement tools and protocols allow you to evaluate the quality of decisions made by an artificial intelligence system ( <a href="https://arxiv.org/abs/1702.08608">Doshi-Velez and Kim, 2017</a> ).  For example, a medical artificial intelligence system would ideally make a diagnosis along with an explanation of how it arrived at such a conclusion ‚Äî so that doctors could test the reasoning process from beginning to end ( <a href="https://www.nature.com/articles/s41591-018-0107-6">De Fauw et al., 2018</a> ).  In addition, to understand more complex systems of artificial intelligence, we could even use automated methods for constructing models of behavior using <b>machine theory of mind</b> ( <a href="https://arxiv.org/abs/1802.07740">Rabinowitz et al., 2018</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sp/ff/ei/spffeiaxzptaap4ghzl_xmcao1o.png"></div><br>  <i><font color="gray">ToMNet detects two subspecies of agents and predicts their behavior (from the <a href="https://arxiv.org/abs/1802.07740">‚ÄúMachine Theory of Mind‚Äù</a> )</font></i> <br><br>  Finally, we want to be able to turn off the AI ‚Äã‚Äãsystem if necessary.  This is an <b>interruptibility</b> issue.  <a href="https://www.ijcai.org/proceedings/2017/0032.pdf">Designing a robust</a> switch is very difficult: for example, because an AI system with maximizing rewards usually has strong incentives to prevent this ( <a href="https://www.ijcai.org/proceedings/2017/0032.pdf">Hadfield-Menell et al., 2017</a> );  and because such interruptions, especially frequent ones, ultimately change the original task, forcing the AI ‚Äã‚Äãsystem to draw wrong conclusions from experience ( <a href="http://www.auai.org/uai2016/proceedings/papers/68.pdf">Orseau and Armstrong, 2016</a> ). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/6d/dn/fl6ddnu5joy9i6jnya2wdrzpyeq.png"></div><br>  <i><font color="gray">The problem with interruptions: human intervention (i.e. pressing the stop button) can change the task.</font></i>  <i><font color="gray">In the figure, the interrupt adds a transition (in red) to the Markov decision process that changes the original problem (in black).</font></i>  <i><font color="gray">See <a href="http://auai.org/uai2016/proceedings/papers/68.pdf">Orseau and Armstrong, 2016</a></font></i> <br><br><h1>  Looking to the future </h1><br>  We are building a foundation of technology that will be used for many important applications in the future.  It should be borne in mind that some solutions that are not critical to security at system startup may become such when the technology becomes widespread.  Although at one time these modules were integrated into the system for convenience, it would be difficult to fix the problems without complete reconstruction. <br><br>  Two examples from the history of computer science can be cited: this is a null pointer that Tony Hoar <a href="https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare">called his ‚Äúbillion-dollar error‚Äù</a> , and the gets () procedure in C. If early programming languages ‚Äã‚Äãwere designed with security in mind, progress would be slowed, but it is likely that This would have a very positive impact on modern information security. <br><br>  Now, having carefully thought out and planned everything, we are able to avoid similar problems and vulnerabilities.  We hope that the categorization of problems from this article will serve as a useful basis for such methodical planning.  We strive to ensure that in the future, AI systems will not only work according to the principle ‚ÄúI hope, safely‚Äù, but really reliably and verifiably safely, because we built them this way! <br><br>  We look forward to continuing exciting progress in these areas, in close collaboration with the broader AI research community, and encourage people from different disciplines to consider contributing to AI security research. <br><br><h1>  Resources </h1><br>  For reading on this topic, below is a selection of other articles, programs, and taxonomies that helped us in compiling our categorization or contain a useful alternative view on the technical security issues of AI: <br><br><ul><li>  <a href="http://humancompatible.ai/publications" rel="noopener nofollow">Annotated bibliography of recommended materials</a> (Center for Human-Compatible AI, 2018) </li><li>  <a href="http://inst.eecs.berkeley.edu/~cs294-149/fa18/" rel="noopener nofollow">Safety and Control for Artificial General Intelligence</a> (UC Berkeley, 2018) </li><li>  <a href="https://vkrakovna.wordpress.com/ai-safety-resources/" rel="noopener nofollow">AI Safety Resources</a> (Victoria Krakovna, 2018) </li><li>  <a href="https://arxiv.org/abs/1805.01109" rel="noopener nofollow">AGI Safety Literature Review</a> (Everitt et al., 2018) </li><li>  <a href="https://arxiv.org/abs/1802.07228" rel="noopener nofollow">Preparing for Malicious Uses of AI</a> (2018) </li><li>  <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRPiprOaC3HsCf5Tuum8bRfzYUiKLRqJmbOoC-32JorNdfyTiRRsR7Ea5eWtvsWzuxo8bjOxCG84dAg/pubhtml" rel="noopener nofollow">Specification gaming examples in AI</a> (Victoria Krakovna, 2018) </li><li>  <a href="https://ai-alignment.com/directions-and-desiderata-for-ai-control-b60fca0da8f4" rel="noopener nofollow">Directions and desiderata for AI alignment</a> (Paul Christiano, 2017) </li><li>  <a href="https://docs.google.com/document/d/1NIg4OnQyhWGR01fMVTcxpz8jDd68JdDIyQb0ZZyB-go/edit" rel="noopener nofollow">Funding for Alignment Research</a> (Paul Christiano, 2017) </li><li>  <a href="https://intelligence.org/files/TechnicalAgenda.pdf" rel="noopener nofollow">Agent Intelligence for Machine Interests: A Technical Research Agenda</a> (Machine Intelligence Research Institute, 2017) </li><li>  <a href="https://arxiv.org/abs/1711.09883" rel="noopener nofollow">AI Safety Gridworlds</a> (Leike et al., 2017) </li><li>  <a href="https://futureoflife.org/wp-content/uploads/2017/01/Nick_Bostrom.pdf%3Fx17807" rel="noopener nofollow">Governance Problem</a> (Nick Bostrom, 2017) </li><li>  <a href="https://intelligence.org/files/AlignmentMachineLearning.pdf" rel="noopener nofollow">Alignment for Advanced Machine Learning Systems</a> (Machine Intelligence Research Institute, 2017) </li><li>  <a href="https://agentfoundations.org/item%3Fid%3D1388" rel="noopener nofollow">AI safety issues</a> (Stuart Armstrong, 2017) </li><li>  <a href="https://arxiv.org/abs/1606.06565" rel="nofollow noopener">Concrete Problems in AI Safety</a> (Dario Amodei et al, 2016) </li><li>  <a href="https://intelligence.org/files/ValueLearningProblem.pdf" rel="noopener nofollow">The Value Learning Problem</a> (Machine Intelligence Research Institute, 2016) </li><li>  (Future of Life Institute, 2015) </li><li>  <a href="https://futureoflife.org/data/documents/research_priorities.pdf" rel="noopener nofollow">Research Priorities for Robust and Beneficial Artificial Intelligence</a> (Future of Life Institute, 2015) </li></ul><ul><li>  <a href="https://medium.com/tag/artificial-intelligence%3Fsource%3Dpost">Artificial Intelligence</a> </li><li>  <a href="https://medium.com/tag/machine-learning%3Fsource%3Dpost">Machine learning</a> </li><li>  <a href="https://medium.com/tag/deepmind%3Fsource%3Dpost">Deepmind</a> </li><li>  <a href="https://medium.com/tag/ai-safety%3Fsource%3Dpost">Ai Safety</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/425387/">https://habr.com/ru/post/425387/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../425375/index.html">Another way to see application communications</a></li>
<li><a href="../425377/index.html">If digital product designers were creating real things.</a></li>
<li><a href="../425379/index.html">Charles Nutter. How to transfer an ancient monolithic project to JRuby and is it worth it?</a></li>
<li><a href="../425383/index.html">Jet Infosystems, Rosreestr, NLMK and Utkonos launch AI hackathon</a></li>
<li><a href="../425385/index.html">A programmer with a head: how coding affects thinking</a></li>
<li><a href="../425389/index.html">FadeObjects - Hide objects between camera and character</a></li>
<li><a href="../425391/index.html">Proof of the Collatz hypothesis</a></li>
<li><a href="../425393/index.html">QIWI server party 3.0: report + full videos of all reports</a></li>
<li><a href="../425395/index.html">10 physical facts that you should have learned in school, but may not have learned</a></li>
<li><a href="../425397/index.html">10 libraries every Android developer should know about</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>