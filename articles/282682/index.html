<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Test automation: Acronis Kernel ‚Äúdrone‚Äù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="( http://bp-la.ru/bespilotnyj-apparat-danem ) 


 Build => Test => Not passed => and kilometers of logs scattered across different systems, and tens o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Test automation: Acronis Kernel ‚Äúdrone‚Äù</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/648/ee6/b4b/648ee6b4bc144ea3b2c5ab7a095c2b73.jpg"><br><p>  ( <a href="http://bp-la.ru/bespilotnyj-apparat-danem">http://bp-la.ru/bespilotnyj-apparat-danem</a> ) </p><br><p>  Build =&gt; Test =&gt; Not passed =&gt; and kilometers of logs scattered across different systems, and tens of minutes mixing ends together in search of the cause of the failure.  Familiar? </p><br><p>  And if not? </p><br><p>  Build =&gt; Test =&gt; Failed =&gt; Ticket in JIRA - and the developer takes the bug to work, because he already has all the information. </p><br><p>  Working in the Acronis Kernel team, I set out to create just such an autotest. <br>  Under the cut - my story. </p><a name="habracut"></a><br><h2>  Introduction </h2><br><p>  Software testing is a study to provide interested parties (Stakeholders, hereinafter Customers) with information about the quality of a product or service ( <a href="https://en.wikipedia.org/wiki/Software_testing">from Wikipedia</a> ). <br>  Customers perceive test results in different ways: </p><br><ul><li>  <strong>A product manager</strong> looks at which product features are ready for release, and which ones will have to be simplified \ postponed \ discarded. </li><li>  <strong>The test manager is</strong> interested in the details of the study: types of tests performed, code / requirements coverage, elapsed time, detailed results, as well as any failures \ errors \ complexity that may adversely affect the objectivity of the test results. </li><li>  <strong>The developer</strong> needs defects: clearly described, reproducible, including all the information necessary for fixing. </li><li>  <strong>The tester</strong> receives the task, performs the test, analyzes the result, reports the bugs.  If possible, expands the test coverage by adding new tests or test environments (environments). </li></ul><br><p>  Data should be available as soon as possible, ideally in real time, immediately after the appearance of a new product assembly. </p><br><p>  Now we will present a workflow sufficient to fulfill the specified requirements: </p><br><ol><li>  Tests will start as soon as a new product is available; </li><li>  The test execution time is determined by the development process adopted by the company, but must not exceed the average time between appearances of a new assembly; </li><li>  Detected errors are automatically analyzed, already known errors are attributed to existing defects, the rest are recorded as new defects - within a few minutes after detection; </li><li>  Test results are marked on the "Product Quality Card". </li></ol><br><p>  Here, in the Acronis Kernel team, we built such a process - not immediately, of course. <br>  First I will tell you where we started. </p><br><h2>  Prehistoric </h2><br><img src="https://habrastorage.org/files/13f/18f/a89/13f18fa89d58429bab42cbbf90af0215.png"><br><p>  ( <a href="http://spongebob.wikia.com/wiki/Primitive_Sponge">http://spongebob.wikia.com/wiki/Primitive_Sponge</a> ) </p><br><h4>  Machinery </h4><br><ul><li>  [bike] Control Center (SS) - a task scheduler written in Python, also stores test plans and draws reports. </li><li>  [bike] AutoTest Management System (ATMS) - java-based virtual and physical resource manager </li><li>  [bike] Custom DSL to set up a test environment </li><li>  [bike] Custom Python unittest-based framework for writing tests with XML configs.  Here and there, the logic of the test was built into the configs. </li><li>  [bike] Custom version TestLink - here, in theory, should live detailed descriptions of test cases and the results of their implementation.  In fact, it was used mainly to get a unique script ID (group of tests) </li><li>  ESX-i virtual machines </li></ul><br><h4>  It all worked like this </h4><br><ol><li>  There is a new build. </li><li>  CC noted the appearance of the assembly, and, according to the test plan stored in it, created new tasks in Testlink. </li><li>  ATMS found tasks in TestLink and requested resources for them from the hypervisor.  There were no queues of tasks: who managed to seize the resource, he is right. </li><li>  Having obtained the required VM set, ATMS configured the Guest OS in them.  The configuration recipe was defined as a custom DSL. </li><li>  Then the control was transferred to the Python library, which completed the configuration of the environment, deployed the build and ran the tests. </li><li>  Upon completion of the tests, the ATMS collected logs and test results, updated the task status in TestLink. </li><li>  CC saw the completion of the task in TestLink, retrieved the results, updated its statistics database and sent a report on the test results by letter.  Later, the Control Center took over the functions of TestLink, and the tasks were created in its internal database, which emulates Testlink for the client - ATMS. </li></ol><br><p>  Tests went on for several hours, often giving a random (non-replicable) result.  For the analysis of the files, there was a whole quest with a visit to ATMS, CC, balls with logs, a detailed analysis of the logs and a search for similar bugs in Jira - all hand to hand. </p><br><p>  Registered failures are sometimes reproduced, more often - not.  For most of the bug fixes, the developers asked to clarify the steps, provide a virtual machine or attach forgotten logs. </p><br><p>  About once a week, the ATMS fell.  If the test hung, or for some other reason the resources did not become free, you had to manually delete the virtual machines, remove the task in the ATMS and reset the host busy count. </p><br><p>  It was possible to compare the results of tests on different assemblies by static email reports with a graph of the Result Type / Build Number, or by selecting the results manually in the CC.  To compare the results of the same test on different operating systems, I had to manually view the test logs from each OS. </p><br><p>  As a result, developers did not trust autotests, relying more on manual launching of their own tests on their environment.  This "mechanization" did not suit me at all, the situation had to be corrected. </p><br><h2>  Brave new world </h2><br><img src="https://habrastorage.org/files/b78/bf1/f39/b78bf1f397c7410da94adc6509eb3ed9.png"><br><p>  ( <a href="http://dkrack.wikispaces.com/Brave%2BNew%2BWorld">http://dkrack.wikispaces.com/Brave+New+World</a> ) </p><br><h4>  The architecture of the new autotest system was based on: </h4><br><ul><li>  <strong>Jenkins</strong> - task scheduler, resource manager, custodian of test history and detailed results </li><li>  <strong>ESX-i</strong> virtual machines </li><li>  <strong>Python, Pytest</strong> - search for tests by tags, parameterized launch, execution control and output of the result in junit.xml format (standard format for Jenkins) </li><li>  <strong>JIRA</strong> - test results in the form of bugs, project success metrics </li></ul><br><p>  0 (zero) bicycles. </p><br><h4>  Task path </h4><br><ol><li>  With a successful build, the build server (also Jenkins) starts the project on the test Jenkins, putting the test in the queue. </li><li>  Test Jenkins reserves resources (VM linked clone), downloads the latest test code from SVN, runs the CMD script to set up the environment, and calls pytest. </li><li><p>  Pytest using the built-in test discovery function selects cases and starts the test.  The framework code is executed on the Gate VM, the control machine, and the System Under Test (in our case, the kernel driver) is deployed on the Test VM, so as not to lose results in the BSOD case. </p><br><ul><li><p>  The standard python logging library writes the info log and the debug log into two different files: <br>  a) Info log contains test steps and meets two requirements: 1) human readable format, 2) there is enough information to reproduce the failure. <br>  b) Debug log includes timestamp, address \ line number of the executable code and the expanded message.  The log allows you to track a detailed history of events that are not directly related to the essence of the test, but affecting the result: whether it was possible to establish a connection, how much time the reboot took, etc. </p><br></li><li>  The test stops when the first failure is detected (the result is assert = False).  Pytest writes the result + trace to junit xml. </li></ul><br></li><li>  Jenkins (JUnit Plugin) publishes a report and starts the python script for reporting bugs. </li><li>  The script searches for already known open bugs in Jira, if it finds it - leaves the comment "Reproduced there somewhere", if not - it registers a new bug.  The error message (pytest assert) goes into the header, the steps from the Info log into the description, the test logs and the drivers themselves will attach to the bug. </li></ol><br><p>  I will give the scheme for clarity: </p><br><img src="https://habrastorage.org/files/1ea/336/bf5/1ea336bf57884e55a744b767861bf8e6.png"><br><p>  (¬© Acronis) </p><br><p>  The name of the bug is added with a suffix to the name of the VM, so developers can easily find a car if necessary.  The machine on which the already known bug was reproduced will be automatically removed after three days.  The machine with the new bug will be automatically removed after the developer translates it into Resolved status, and the corresponding test passes without errors. </p><br><h4>  An example of an automatically activated bug </h4><br><img src="https://habrastorage.org/files/dff/1ea/f2e/dff1eaf2e54c48799f5fc2079293cedd.png"><br><p>  (¬© Acronis) </p><br><p>  Previously, the automator had to spend 80-90% of the time on manual analysis of test results.  Now just look at the list of bugs in Jira.  The product bug goes to the developers, the automator takes the test fails.  If there is not enough information in the bug report, you don‚Äôt need to teach people to get bugs differently - just change the code. </p><br><h4>  An example of developer communication with an automatic bug reporter </h4><br><img src="https://habrastorage.org/files/ad3/590/75a/ad359075a02f42e88fc722a4e6ba9502.png"><br><p>  (¬© Acronis) </p><br><p>  Support for tests has been reduced to processing in the code of yet unaccounted types of failures.  Corner cases will always be there, this should be understood, and you should not aim at getting rid of 100% of the failures of the auto-test / test infrastructure.  It is enough to turn these failures into specific action items - bugs in Jira, in our case, and fix them one by one. </p><br><h4>  Product Quality Card </h4><br><p>  A general overview of the state of the tested components can now be obtained by looking at the Jenkins dashboard: </p><br><img src="https://habrastorage.org/files/8cc/111/660/8cc1116605ce42c69163926abd0709f1.png"><br><p>  (¬© Acronis) </p><br><p>  Dashboard implemented using the plugin <a href="https://wiki.jenkins-ci.org/display/JENKINS/Dashboard%2BView">https://wiki.jenkins-ci.org/display/JENKINS/Dashboard+View</a> . </p><br><p>  Maybe not all readers are familiar with Jenkins, so I‚Äôll explain the values ‚Äã‚Äãof the columns: </p><br><ul><li>  <strong>S (Status)</strong> - the result of the last build (in our case, the test); </li><li>  <strong>Name</strong> - the name of the test; </li><li>  <strong>W (Weather)</strong> - iconography, showing the history of the quality of the assembly, 5 assemblies back.  The sun means that all 5 assemblies are successful, a thundercloud ‚Äî all 5 assemblies are bad; </li><li>  <strong>Build Parameters</strong> - in our case the path is specified, corresponding to the code branch and containing the build number; </li><li>  <strong>Last Duration</strong> - the execution time of the last build, starting from the moment the order is placed in the queue, until the moment when the logging from the last environment is completed and the report about the test results is sent; </li><li> <strong>Build Description</strong> - in the description of the assembly, the autotest adds the numbers of the bugs automatically instituted in Jira, indicating whether it is new (new) or already known (upd); </li><li>  <strong>Last Success, Last Failure</strong> - how long ago the last successful / unsuccessful build was made. </li></ul><br><h2>  results </h2><br><p>  We built and debugged the system I described above by the end of last fall, and then actively added new scripts for testing.  From February 2016, I switched full time to another project. </p><br><p>  During my absence (six months): </p><br><ul><li>  129 correct bugs were found and automatically generated - approximately one new bug every working day. </li><li>  From other sources, 48 ‚Äã‚Äãbugs. </li></ul><br><p>  The project has lived for six months and has been developed by the efforts of developers only, without a single tester.  The developers have independently added a new component, creating Jenkins projects and Pyhton code by analogy with existing ones. </p><br><p>  Incorrect bugs during this time, too, quite a lot, mostly duplicates, born of an incorrect new test setup or test server failures.  However, this is a topic for a separate article. </p><cut></cut></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/282682/">https://habr.com/ru/post/282682/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../282672/index.html">The benefits of inspections</a></li>
<li><a href="../282674/index.html">How does it feel to be a developer when you're forty</a></li>
<li><a href="../282676/index.html">Inheriting tables in Postgresql with Ruby on Rails</a></li>
<li><a href="../282678/index.html">Python: a programming language created by the community</a></li>
<li><a href="../282680/index.html">Color gap</a></li>
<li><a href="../282684/index.html">PHP 7 Checker</a></li>
<li><a href="../282688/index.html">Node.JS v6.0 released</a></li>
<li><a href="../282692/index.html">We are deploying Cisco ISE in a Hyper-V environment and beyond.</a></li>
<li><a href="../282694/index.html">Deploy the MEAN stack (MongoDB, Express, AngularJS, Node.js) in Microsoft Azure</a></li>
<li><a href="../282696/index.html">Xamarin Forms in action. Medchest Assistant</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>