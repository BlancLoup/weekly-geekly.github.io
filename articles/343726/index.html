<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hinton's capsule nets</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="On October 27, 2017, an article by Dr. Jofri Hinton and co-authors from Google Brain appeared . Hinton is more than a renowned machine learning scient...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hinton's capsule nets</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/pj/qv/5b/pjqv5bfj1qbs_elbjxuei10p7mc.jpeg"><br><br>  On October 27, 2017, an <a href="https://arxiv.org/pdf/1710.09829.pdf">article by Dr. Jofri Hinton and co-authors from Google Brain appeared</a> .  Hinton is more than a renowned machine learning scientist.  He once developed the math of backpropagation of errors, was the scientific adviser of Yang Lekun, author of the architecture of convolutional networks. <br><br>  Although the presentation was quite modest, it is correct to talk about a revolutionary change in the approach to artificial neural networks (INS).  They called the new approach "capsule networks".  While in the Russian segment of the Internet there is little information about them, therefore, I will fill this gap. <br><a name="habracut"></a><br>  So, as it turned out, Hinton in recent years has been looking for ideas that will make it possible to do something cooler than convolutional networks in computer vision.  What did he not like in CNN (convolutional neural networks, convolutional neural networks)?  Basically, the max-pooling layer and the invariance of detection only to the position in the image.  It is used to reduce the dimension of the outputs of the convolutional layers, ignoring the small differences in the spatial structure of the images (or other types of data). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/4bc/ea0/29d/4bcea029de3730862ab8721e47d9bc25.png" alt="image"><br><br>  From the image it is clear how this layer works: it selects the maximum in the exit window of the convolutional layer.  This part of the information is lost.  In fact, it is not so obvious to prove, and there may even be classes of objects for which information loss does not happen.  In addition, in a number of architectures convolutional networks, the max-pooling layer simply does not exist.  But, nevertheless, - max-pooling is really quite a layer vulnerable to criticism. <br><br>  Even more criticism of convolutional networks is at the end of one of my past <a href="https://habrahabr.ru/company/recognitor/blog/277163/">articles</a> .  Hinton talks about similar problems. <br><br>  In addition, a large "elephant", which we, the engineers, try not to notice in our room is a mini column in the cerebral cortex, which clearly must have an understandable, limited and not too primitive function. <br><br><img src="https://habrastorage.org/webt/5l/kk/zj/5lkkzjsu90kg2gntesxopuuaf3k.jpeg" alt="image"><br><br>  So Hinton says: neural capsules are minicolumns, they can correspond to recognizable objects, their characteristics.  The total activity of neurons characterizes the probability of recognition.  The output information inside the capsule should be enough to restore the input almost without loss. <br><br>  <b>Architecture</b> <br><br>  For those who are a little involved in convolutional networks, this picture explains almost everything: <br><br><img src="https://habrastorage.org/webt/yx/zu/py/yxzupyihw2eoxxvgyw9bmpll05e.png"><br><br>  So, first comes the standard selection of features that are invariant to translation in the image using a convolutional layer.  Even if you do not know what a convolutional layer is, you can still read further - this is not critical here. <br><br>  Then 32 more convolutional layers are distinguished, each of which looks at the first convolutional layer.  This is already unusual, and the picture does not explain why.  For <u>dynamic routing</u> (of course, routing, but let it be routing, otherwise it looks like speech about network technologies).  This is one of the two "chips" that distinguishes this architecture from the rest, so it deserves a separate chapter. <br><br>  Then comes the ‚ÄúdigitCaps‚Äù layer - the capsules that Hinton is talking about.  Although he calls the previous layer ‚ÄúprimaryCaps‚Äù, i.e.  primary capsules, they do not carry a particularly new function in themselves.  What matters is how they are then connected to digitCaps. <br><br>  So, each capsule (or minicolumn) has a strict meaning: the amplitude of the vector in each of them corresponds to the probability of having one of the numbers in the image. <br><br>  A rather important part of the architecture was hidden at the bottom of the image: a decoder, to the input of which a layer of DigitCaps is fed, while all the capsules are zeroed out except for one that ‚Äúwon‚Äù in amplitude.  Decoders are essentially an inverted ANN (artificial neural network), the input is a small vector, and the output is the original image.  This decoder provides the second most important "chip" of this architecture: the contents of one capsule must fully describe the subset of a specific digit supplied to the input.  Otherwise, the decoder will not be able to restore the original image.  There is another useful property of a decoder - it is a regularization method.  Close codes in capsules will be in Euclid's close images. <br><br>  For example, the article presents the result of the "movement" component in one of the capsules (in this case responsible for the number 6): <br><br><img src="https://habrastorage.org/webt/wc/iy/8n/wciy8n5zvxpaynytqknpqlhvzlu.png"><br><br>  The components of the capsule vector are responsible for the Hinton digit representation form. <br><br>  <b>Dynamic routing</b> <br><br>  Hinton, Frosst and Sara Sabour write: ‚ÄúThere are many ways to realize the idea of ‚Äã‚Äãcapsule networks.  And dynamic routing as proposed is just one example. ‚Äù  Therefore, it may be that soon more elegant solutions will appear.  But so far everything works as follows: <br><br><img src="https://habrastorage.org/webt/fs/4o/vk/fs4ovk5yze98v3nxsiiuro8-wwo.png"><br><br>  Each capsule from the previous layer is connected with each next one.  Here "connected" means that there is a weight matrix between each capsule from the previous layer and each next one.  These weights are trained, as in a conventional artificial neural network using the method of back propagation of errors.  But over these weights by coefficients a factor for each pair is superimposed, with the factor ultimately must tend to one or zero.  In the limit, each capsule from the top level ‚Äúlistens‚Äù to only one capsule from the bottom at a time.  In the image above, the first capsule from the top level draws attention only to the second capsule of the bottom one, and the second one at the top one - only to the third one from the bottom one.  Note that this matrix is ‚Äã‚Äãtemporary and must be calculated anew for each example.  Further, I will call it a ‚Äútemporary matrix‚Äù.  But fully connected weight matrices between all capsules are trained and their weights always exist, despite the fact that they are not implemented every time. <br><br>  How is this temporary matrix calculated?  A rather tricky iterative process: when a new example of a capsule network is presented, it is estimated what contribution to the top level capsule vector is given by each connection to the lower level.  The compound that makes a greater contribution increases its weight in the time matrix, and then everything is normalized so that the coefficients do not go to infinity.  And this is repeated 3 times. <br><br>  It is not obvious here, but, in fact, it is the realization of the principle ‚Äúthe winner takes everything‚Äù.  Well, because of the iterative principle, here "the winner takes almost everything."  And so is the distribution of roles at the lower level: each element begins to better describe some of its situation for each of the top-level capsules. <br><br>  Thus, during recognition (as well as during training), for each capsule responsible for a specific figure, the problem is solved: which model of the lower level capsules describes the observed image better.  And then the amplitude of each of the vectors in the upper level capsules is estimated to decide which figure is most likely. <br><br>  If you want to understand the mathematics in detail, then it is described in the original article.  And in more detail <a href="https://medium.com/ai%25C2%25B3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418">here</a> . <br><br>  <b>Experimental results</b> <br><br>  Experimented on a well-known database of images MNIST, in which 10 numbers are presented in words in different variations. <br><br><img src="https://habrastorage.org/webt/bd/e9/eq/bde9eq70enpj3yb0o5flbfby7vy.png"><br><br>  What are these lines about?  Adding a decoder and dynamic routing with three iterations helps.  In the second column, the result is on MNIST when 2 digits are superimposed on each other.  This is an interesting ability of capsule networks to choose not one capsule of the winner, but two at once.  The only thing that doesn't work is if two identical numbers are drawn on top of each other in a slightly different way. <br><br>  IMHO on MNIST you can get any accuracy.  The test sample size is 10,000, which means that with 99.7% accuracy there are only 30 errors.  And changing these or other parameters of the algorithm, you can always reduce this error due to only accidents.  Therefore, we can only talk about the contribution of this or that mechanism to the result (and then, in fact, it will be controversial and statistically unreliable).  For example, the entire dynamic routing mechanism gives an improvement of only 0.04%, i.e.  on 4 images.  So, there were 29 errors, it became 25. The probability that dynamic routing did not help is still very serious. <br><br>  Of course, I think it all works and helps.  Just do not pay attention to the result on MNIST.  The concept itself is important, and MNIST allowed to debug and check its implementation.  And this is an important step - now we can all touch the implementation, <a href="https://github.com/naturomics/CapsNet-Tensorflow/blob/master/capsLayer.py">for example in tensorflow</a> , and understand how it works, personally. <br><br>  <b>What does all this mean?</b>  <b>Strong AI, which will enslave us all?</b> <br><br>  Because of the dynamic development of the concept of convolutional networks (and still recurrent, but Hinton does not explain how his ideas can absorb them, although they can) a couple of years ago, everyone in the industry began to think that nothing more was needed.  Everything is training - give the data!  And it is only a matter of time when all tasks are solved.  But it is not.  And I think that it is very important that a person who is considered the father of deep learning speaks of the severe inferiority of existing approaches, looking for new directions. <br><br>  This means that from October 27, 2017 more research groups will start searching for the next step in artificial intelligence, more people will formulate what is the difference between ‚Äúweak‚Äù AI and ‚Äústrong‚Äù, competition will begin among previously marginal, and later trend theories which should replace existing models.  And this is all very good!  Now opens a new Wild West in the field of AI. <br><br>  <b>Announcement, take this opportunity</b> <br><br>  Not so long ago, <a href="https://habrahabr.ru/company/recognitor/blog/277163/">in this article</a> , armed with the ideas of Alexei Redozubov, which are almost orthogonal to the existing established consensus, I tried to show an alternative approach using the example of car numbers. <br><br>  Last year, our surveys also did not stand still.  For example, we wrote a demonstration on MNIST on Keras, which is now not even ashamed to share with the community.  And the capsule networks only convinced that the formulation of the function of minicolumns is extremely important at the current stage, and here it is somewhat different from what Hinton gives (for the better, of course). <br><br>  Therefore, in the coming days I will publish a similar article in the wake of the MNIST training session and articles on arxiv within the framework of the concept of ‚Äúcompletely non-neural networks‚Äù (we‚Äôll not think of a name). </div><p>Source: <a href="https://habr.com/ru/post/343726/">https://habr.com/ru/post/343726/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343714/index.html">Local automation of builds (Crashlytics + Slack + FastLane)</a></li>
<li><a href="../343716/index.html">Experiment to promote the game on Google Play. Part 1</a></li>
<li><a href="../343718/index.html">Sound settings in Ubuntu</a></li>
<li><a href="../343720/index.html">Strong typing in non-strict tests</a></li>
<li><a href="../343724/index.html">PLATO: the history of the world's first e-learning system</a></li>
<li><a href="../343728/index.html">Unity game, open source</a></li>
<li><a href="../343730/index.html">We write DSL in Koltin</a></li>
<li><a href="../343732/index.html">Corporate "stuffing" for small networks</a></li>
<li><a href="../343736/index.html">Avtomatny workshop - 2. Example "Crossing", the mathematical transformations of TK in OA</a></li>
<li><a href="../343738/index.html">We understand what was discovered there in the problem of queens</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>