<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Parallelism in PostgreSQL: not spherical, not a horse, not in a vacuum</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Scaling a DBMS is a continually coming future. DBMSs are being improved and better scaled on hardware platforms, while hardware platforms themselves i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Parallelism in PostgreSQL: not spherical, not a horse, not in a vacuum</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/b0/s3/rq/b0s3rqkffufh7baruji0jc_vbgq.jpeg"><br><br>  Scaling a DBMS is a continually coming future.  DBMSs are being improved and better scaled on hardware platforms, while hardware platforms themselves increase productivity, the number of cores, and memory ‚Äî Achilles catches up with a turtle, but still has not caught up.  The problem of scaling the DBMS is standing upright. <br><br>  Postgres Professional faced the problem of scaling not only theoretically, but also practically: from its customers.  And more than once.  About one of these cases and will be discussed in this article. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      PostgreSQL scales well on NUMA systems if it is a single motherboard with multiple processors and multiple data buses.  Some optimizations can be found <a href="https://habr.com/company/postgrespro/blog/270827/">here</a> and <a href="http://akorotkov.github.io/blog/2016/05/09/scalability-towards-millions-tps/">here</a> .  However, there is another class of systems, they have several motherboards, the data exchange between them is carried out using interconnect, while they have one instance of the operating system and for the user this design looks like a single machine.  And although formally such systems can also be attributed to NUMA, but in essence they are closer to supercomputers, since  access to the local memory of a node and access to the memory of a neighboring node differ radically.  The PostgreSQL community believes that the only Postgres instance running on such architectures is a source of problems, and there is no systematic approach to solving them. <br><a name="habracut"></a><br>  This is explained by the fact that the software architecture that uses shared memory is fundamentally designed to ensure that the access time of different processes to its own and remote memory is more or less comparable.  In the case when we work with many nodes, the rate on shared memory as a fast communication channel ceases to justify itself, because due to delays (latency) it is much ‚Äúcheaper‚Äù to send a request to perform a specific action to the node (node) where data of interest than to send this data on the bus.  Therefore, cluster solutions are relevant for supercomputers and systems with many nodes in general. <br><br>  This does not mean that the combination of multisite systems and a typical Postgres shared memory architecture must be put in a cross.  After all, if the postgres processes spend most of the time doing complex calculations locally, then this architecture will even be very efficient.  In our situation, the client had already purchased a powerful multi-node server, and we had to solve PostgreSQL problems on it. <br><br>  And the problems were serious: the simplest write requests (change several field values ‚Äã‚Äãin one record) were executed from a few minutes to an hour.  As was later confirmed, these problems manifested themselves in all their glory precisely because of the large number of cores and, consequently, radical parallelism in the execution of requests with a relatively slow exchange between nodes. <br><br>  Therefore, the article will turn out as a dual purpose: <br><br><ul><li>  Share experience: what to do if the base slows down in earnest in a multi-node system.  Where to start, how to diagnose, where to go. </li><li>  Tell how the problems of the PostgreSQL DBMS itself can be solved with a high level of parallelism.  This includes how changing a lock algorithm affects PostgreSQL performance. </li></ul><br><h3>  Server and DB </h3><br>  The system consisted of 8 blades with 2 sockets in each.  In total, more than 300 cores (excluding hypertreaming).  The fast tire (manufacturer's proprietary technology) connects the blades.  Not that a supercomputer, but for a single instance of a DBMS, the configuration is impressive. <br>  The load is also rather big.  More than 1 terabyte of data.  About 3000 transactions per second.  Over 1000 connections to postgres. <br><br>  Starting to deal with the hourly wait recording, the first thing we have excluded as a reason for the delay recording on the disk.  As soon as incomprehensible delays began, tests began to be done exclusively on <code>tmpfs</code> .  The picture has not changed.  The disk has nothing to do with it. <br><br><h3>  Getting mining diagnoses: submission </h3><br>  Since problems arose, most likely due to the high competition of processes that ‚Äúknock‚Äù on the same objects, the first thing to check is blocking.  In PostgreSQL, for such a check, there is a <code>pg.catalog.pg_locks</code> and <code>pg_stat_activity</code> .  In the second, in version 9.6, information was added about what the process was waiting for ( <i>Amit Kapila, Ildus Kurbangaliev</i> ) - <code>wait_event_type</code> .  Possible values ‚Äã‚Äãfor this field are described <a href="https://postgrespro.ru/docs/postgresql/10/monitoring-stats">here</a> . <br><br>  But first, just count: <br><br><pre> <code class="sql hljs">postgres=<span class="hljs-comment"><span class="hljs-comment"># SELECT COUNT(*) FROM pg_locks; count ‚Äî---‚Äî 88453 (1 row) postgres=# SELECT COUNT(*) FROM pg_stat_activity; count ‚Äî---‚Äî 1826 (1 row) postgres=# SELECT COUNT(*) FROM pg_stat_activity WHERE state ='active'; count ‚Äî---‚Äî 1005</span></span></code> </pre> <br>  These are real numbers.  Reached up to 200,000 locks. <br>  At the same time, there were such locks on the ill-fated request: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">COUNT</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">mode</span></span>), <span class="hljs-keyword"><span class="hljs-keyword">mode</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pg_locks <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> pid =<span class="hljs-number"><span class="hljs-number">580707</span></span> <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-keyword"><span class="hljs-keyword">mode</span></span>; count | mode ‚Äî<span class="hljs-comment"><span class="hljs-comment">-----+---------------‚Äî 93 | AccessShareLock 1 | ExclusiveLock</span></span></code> </pre> <br>  When reading the buffer, the DBMS uses the <code>share</code> lock, while writing it is <code>exclusive</code> .  That is, write locks accounted for less than 1% of all requests. <br>  In the <code>pg_locks</code> types of locks do not always appear as described <a href="https://postgrespro.ru/docs/postgrespro/10/explicit-locking">in the</a> user <a href="https://postgrespro.ru/docs/postgrespro/10/explicit-locking">documentation</a> . <br><br>  Here is a label: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">AccessShareLock</span></span> = LockTupleKeyShare RowShareLock = LockTupleShare ExclusiveLock = LockTupleNoKeyExclusive AccessExclusiveLock = LockTupleExclusive</code> </pre> <br>  The SELECT mode FROM pg_locks query showed that executions of the CREATE INDEX command (without CONCURRENTLY) are waiting for 234 INSERTs and 390 INSERTs are waiting for <code>buffer content lock</code> .  A possible solution is to ‚Äúteach‚Äù INSERTs from different sessions to less intersect across buffers. <br><br><h3>  It's time to use perf </h3><br>  The <b><code>perf</code></b> utility collects a lot of diagnostic information.  In <code>record</code> mode ... it records system event statistics in files (by default they are in <code>./perf_data</code> ), and in <code>report</code> mode it maligns the collected data, you can, for example, filter events related only to <code>postgres</code> or this <code>pid</code> : <br><br><pre> <code class="hljs ruby">$ perf record -u postgres  $ perf record -p <span class="hljs-number"><span class="hljs-number">76876</span></span>  ,  $ perf report &gt; ./my_results</code> </pre> <br>  As a result, we'll see something like <br><br><img src="https://habrastorage.org/webt/rn/ta/rv/rntarvj2jticq7glciiqockk3-y.jpeg"><br><br>  How to use <code>perf</code> to diagnose PostgreSQL is described, for example, <a href="https://blog.2ndquadrant.com/tracing-postgresql-perf/">here</a> , as well as in the <a href="https://wiki.postgresql.org/wiki/Profiling_with_perf">pg-wiki</a> . <br><br>  In our case, even the simplest mode gave important information - <code>perf top</code> , which works, naturally, in the spirit of <code>top</code> operating system.  Using <code>perf top</code> we saw that the processor spends most of the time in kernel <code>PinBuffer()</code> , as well as in the <code>PinBuffer()</code> and <code>LWLockAttemptLock().</code> functions <code>LWLockAttemptLock().</code>  . <br><br>  <code>PinBuffer()</code> is a function that increases the count of buffer references (displaying a data page to RAM), thanks to which postgres processes know which buffers can be preempted and which cannot. <br><br>  <code>LWLockAttemptLock()</code> - the <code>LWLock</code> 's capture function.  <code>LWLock</code> is a kind of <code>LWLock</code> with two levels of <code>shared</code> and <code>exclusive</code> , without defining <code>deadlock</code> , locks are first allocated in <code>shared memory</code> , waiting processes are waiting in the queue. <br><br>  These functions have already been seriously optimized in PostgreSQL 9.5 and 9.6.  Spinlock inside them were replaced by direct use of atomic operations. <br><br><h3>  Flame graphs </h3><br>  It is impossible to do without them: even if they were useless, they would still be worth telling about them - they are extraordinarily beautiful.  But they are useful.  Here is an illustration from <code>github</code> , not from our case (neither we nor the client are ready for the disclosure of details yet). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/75b/909/e8d/75b909e8de5177a48fa7d73f53eff437.svg"><br><br>  These beautiful pictures very clearly show what the processor cycles are going on.  The data can be collected by the same <code>perf</code> , but the <code>flame graph</code> lucidly visualizes the data, and builds trees based on the collected call stacks.  Details about profiling with flame graphs can be found, for example, <a href="https://queue.acm.org/detail.cfm%3Fid%3D2927301">here</a> , and download everything you need <a href="https://github.com/brendangregg/FlameGraph">here</a> . <br><br>  In our case, the flame graphs showed a huge amount of <code>nestloop</code> .  Apparently, the JOINs of a large number of tables in numerous parallel read requests caused a large number of <code>access share</code> locks. <br><br>  The statistics collected by <code>perf</code> show where the processor cycles go.  And although we have seen that most of the CPU time is spent on locks, we did not see what exactly leads to such long waits for locks, because we don‚Äôt see where locks wait exactly because  pending processor time is not wasted. <br><br>  In order to see the expectations, you can build a query to the system view <code>pg_stat_activity</code> . <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> wait_event_type, wait_event, <span class="hljs-keyword"><span class="hljs-keyword">COUNT</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pg_stat_activity <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> wait_event_type, wait_event;</code> </pre> <br>  revealed that: <br><br><pre> <code class="hljs markdown">LWLockTranche | buffer<span class="hljs-emphasis"><span class="hljs-emphasis">_content | UPDATE ************* LWLockTranche | buffer_</span></span>content | INSERT INTO <span class="hljs-strong"><span class="hljs-strong">*****</span></span><span class="hljs-strong"><span class="hljs-strong">*** LWLockTranche | buffer_content | \r | | insert into B4_MUTEX | | values (nextval('hib | | returning ID Lock | relation | INSERT INTO B4_*</span></span><span class="hljs-strong"><span class="hljs-strong">**** LWLockTranche | buffer_content | UPDATE **</span></span><span class="hljs-strong"><span class="hljs-strong">*****</span></span><span class="hljs-strong"><span class="hljs-strong">*****</span></span><span class="hljs-bullet"><span class="hljs-bullet">* Lock | relation | INSERT INTO *</span></span><span class="hljs-strong"><span class="hljs-strong">*****</span></span><span class="hljs-strong"><span class="hljs-strong">** LWLockTranche | buffer_mapping | INSERT INTO **</span></span><span class="hljs-strong"><span class="hljs-strong">*****</span></span>* LWLockTranche | buffer_content | \r</code> </pre> <br>  (asterisks here simply replace the details of the request that we do not disclose). <br><br>  <code>buffer_content</code> values ‚Äã‚Äãare <code>buffer_content</code> (locking the contents of buffers) and <code>buffer_mapping</code> (locks on the components of the <code>shared_buffers</code> hash labels). <br><br><h3>  For help with GDB </h3><br>  But why so many expectations for these types of locks?  For more detailed information about expectations, I had to use the <code>GDB</code> debugger.  With <code>GDB</code> we can get a call stack of specific processes.  Applying the sampling, i.e.  By collecting a certain number of random call stacks, you can get an idea of ‚Äã‚Äãwhich stacks have the longest expectations. <br><br>  Consider the process of building statistics.  We will consider the ‚Äúmanual‚Äù collection of statistics, although in real life special scripts are used that do this automatically. <br><br>  First, <code>gdb</code> must be attached to the PostgreSQL process.  To do this, find the <code>pid</code> server process, say from <br><br><pre> <code class="hljs dos">$ ps <span class="hljs-built_in"><span class="hljs-built_in">aux</span></span> | grep postgres</code> </pre> <br>  Suppose we discovered: <br><br><pre> <code class="hljs pgsql">postgres <span class="hljs-number"><span class="hljs-number">2025</span></span> <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-number"><span class="hljs-number">172428</span></span> <span class="hljs-number"><span class="hljs-number">1240</span></span> pts/<span class="hljs-number"><span class="hljs-number">17</span></span>  S   <span class="hljs-number"><span class="hljs-number">23</span></span>  <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">00</span></span> /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/pgsql/bin/postgres -D /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/pgsql/data</code> </pre> <br>  and now insert the <code>pid</code> into the debager: <br><br><pre> <code class="hljs perl">igor_le:~$gdb -p <span class="hljs-number"><span class="hljs-number">2025</span></span></code> </pre> <br>  Once inside the debugger, we write <code>bt</code> [i.e. <code>backtrace</code> ] or <code>where</code> .  And we get a lot of information like this: <br><br><pre> <code class="hljs swift">(gdb) bt #<span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0x00007fbb65d01cd0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> __write_nocancel () from /lib64/libc.so.<span class="hljs-number"><span class="hljs-number">6</span></span> #<span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">0x00000000007c92f4</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> write_pipe_chunks ( data=<span class="hljs-number"><span class="hljs-number">0x110e6e8</span></span> <span class="hljs-string"><span class="hljs-string">"2018‚Äê06‚Äê01 15:35:38 MSK [524647]: [392‚Äê1] db=bp,user=bp,app=[unknown],client=192.168.70.163 (http://192.168.70.163) LOG: relation 23554 new block 493: 248.389503\n2018‚Äê06‚Äê01 15:35:38 MSK [524647]: [393‚Äê1] db=bp,user=bp,app=["</span></span>..., len=<span class="hljs-number"><span class="hljs-number">409</span></span>, dest=dest@entry=<span class="hljs-number"><span class="hljs-number">1</span></span>) at elog.<span class="hljs-built_in"><span class="hljs-built_in">c</span></span>:<span class="hljs-number"><span class="hljs-number">3123</span></span> #<span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">0x00000000007cc07b</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> send_message_to_server_log (edata=<span class="hljs-number"><span class="hljs-number">0xc6ee60</span></span> &lt;errordata&gt;) at elog.<span class="hljs-built_in"><span class="hljs-built_in">c</span></span>:<span class="hljs-number"><span class="hljs-number">3024</span></span> #<span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-type"><span class="hljs-type">EmitErrorReport</span></span> () at elog.<span class="hljs-built_in"><span class="hljs-built_in">c</span></span>:<span class="hljs-number"><span class="hljs-number">1479</span></span></code> </pre> <br>  Collecting statistics, including call stacks from all postgre processes collected repeatedly at different points in time, we saw that for 3706 seconds (about an hour) the <code>buffer partition lock</code> waiting inside the <code>relation extension lock</code> , that is, locking on a piece of the hash table manager, which was necessary to displace the old buffer, to later replace it with a new, corresponding to the extended part of the table.  A number of <code>buffer content lock</code> was also noticeable, which corresponded to the expectation of locking the <code>B-tree</code> index pages for the implementation of the insert. <br><br><img src="https://habrastorage.org/webt/6c/y6/cw/6cy6cwmfye29jw84dkzoabjlgvg.jpeg"><br><br>  At first, there were two explanations for such a monstrous waiting time: <br><br><ul><li>  Someone else took this <code>LWLock</code> and <code>LWLock</code> stuck.  But this is unlikely.  Because nothing complicated inside the buffer partition lock happens. </li><li>  We are faced with some pathological behavior <code>LWLock</code> 'a.  That is, in spite of the fact that no one took a lock for too long, his wait lasted unnecessarily long. </li></ul><br><h3>  Diagnostic patches and treatment of trees </h3><br>  By reducing the number of simultaneous connections, we would surely discharge the request flow for locks.  But it would be like a surrender.  Instead, <i>Alexander Korotkov</i> , the chief architect of Postgres Professional (of course, he helped prepare this article), suggested a series of patches. <br><br>  First of all, it was necessary to get a more detailed picture of the disaster.  No matter how good the finished tools are, the diagnostic patches of their own making will be useful. <br><br>  A patch was written to add detailed logging of the time spent in the <code>relation extension</code> , what happens inside the <code>RelationAddExtraBlocks()</code> function. So we will know what time is spent inside <code>RelationAddExtraBlocks().</code> <br><br>  And another patch was written to him in support, reporting to <code>pg_stat_activity</code> about what we are currently doing in the <code>relation extension</code> .  It was done this way: when the <code>relation</code> extended, <code>application_name</code> becomes <code>RelationAddExtraBlocks</code> .  This process is now conveniently analyzed with maximum details using <code>gdb bt</code> and <code>perf</code> . <br><br>  Actually therapeutic (and not diagnostic) patches were written two.  The first patch changed the behavior of the blocking of leaves of <code>B‚Äêtree</code> : earlier, when requesting to insert, the sheet was blocked as <code>share</code> , and after that it received <code>exclusive</code> .  Now he immediately gets <code>exclusive</code> .  Now this patch is <a href="https://git.postgresql.org/gitweb/%3Fp%3Dpostgresql.git%3Ba%3Dcommit%3Bh%3Dd2086b08b023c0749a53d617ff3fe0f052646254">already commited</a> for <b>PostgreSQL 12</b> .  Fortunately, this year <i>Alexander Korotkov</i> received <a href="https://wiki.postgresql.org/wiki/Committers">committer status</a> - the second PostgreSQL committer in Russia and the second in the company. <br><br>  The value of <code>NUM_BUFFER_PARTITIONS</code> was also increased from 128 to 512 to reduce the load on mapping locks: the hash table of the buffer manager was divided into smaller pieces, in the hope that the load on each specific piece would decrease. <br><br>  After the application of this patch, the blocking for the buffers are gone, however, despite the increase in <code>NUM_BUFFER_PARTITIONS</code> , <code>buffer_mapping</code> remained, that is, we remind you that the buffer manager <code>buffer_mapping</code> pieces of the hash table: <br><br><pre> <code class="hljs ruby">locks_count <span class="hljs-params"><span class="hljs-params">| active_session |</span></span> buffer_content <span class="hljs-params"><span class="hljs-params">| buffer_mapping ----‚Äê‚Äê‚Äê--‚Äê‚Äê‚Äê+‚Äê------‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê‚Äê------‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê------‚Äê‚Äê‚Äê 12549 |</span></span> <span class="hljs-number"><span class="hljs-number">1218</span></span> <span class="hljs-params"><span class="hljs-params">| 0 |</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span></code> </pre> <br>  And that is not much.  B-tree has ceased to be a bottleneck.  The <code>heap-</code> expansion came to the fore. <br><br><h3>  Conscience treatment </h3><br>  Next, Alexander put forward the following hypothesis and solution: <br><br>  We have a lot of time waiting on the <code>buffer parittion lock</code> 'e when the buffer is preempted.  Perhaps, at the same <code>buffer parittion lock</code> , there is some very demanded page, for example, the root of some <code>B‚Äêtree</code> .  In this place there is a non-stop stream of requests for <code>shared lock</code> from reading requests. <br><br>  The wait queue in <code>LWLock</code> 'e is ‚Äúnot fair.‚Äù  Since <code>shared lock</code> 's can be taken as many as you like at once, then if <code>shared lock</code> already taken, then subsequent <code>shared lock</code> ' and pass without a queue.  Thus, if the shared lock stream has enough intensity so that there are no ‚Äúwindows‚Äù between them, then the waiting of the <code>exclusive lock</code> goes almost to infinity. <br><br>  To fix this, you can try to offer - a patch of "gentleman's" behavior of locks.  It awakens a <code>shared locker</code> conscience and they honestly queue when an <code>exclusive lock</code> already there (interestingly, heavy locks - <code>hwlock</code> - have no problems with conscience: they always honestly queue) <br><br><pre> <code class="hljs ruby">locks_count <span class="hljs-params"><span class="hljs-params">| active_session |</span></span> buffer_content <span class="hljs-params"><span class="hljs-params">| buffer_mapping |</span></span> reladdextra <span class="hljs-params"><span class="hljs-params">| inserts&gt;30sec ‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê-‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê--‚Äê-‚Äê+‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê-‚Äê‚Äê‚Äê‚Äê‚Äê‚Äê+‚Äê‚Äê‚Äê‚Äê------ 173985 |</span></span> <span class="hljs-number"><span class="hljs-number">1802</span></span> <span class="hljs-params"><span class="hljs-params">| 0 |</span></span> <span class="hljs-number"><span class="hljs-number">569</span></span> <span class="hljs-params"><span class="hljs-params">| 0 |</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br>  All is well!  There is no long <code>insert</code> .  Although the locks on the pieces of hash tablets remained.  But what to do, these are the properties of the bus of our little supercomputer. <br><br>  This patch has also been <a href="">proposed to the community</a> .  But whatever the fate of these patches in the community, nothing prevents them from getting into the next version of <b>Postgres Pro Enterprise</b> , which is designed just for customers with heavily loaded systems. <br><br><h3>  Morality </h3><br>  The highly moral, lightweight <code>share</code> -blocks ‚Äî which allow <code>exclusive</code> -blocks to pass ‚Äî solved the problem of hourly delays in a multi-node system.  The <code>buffer manager</code> hash label did not work because of the too large flow of <code>share lock</code> locks that did not leave a chance for locks needed to wipe out the old buffers and load new ones.  Problems with expanding the buffer for database tables were only a consequence of this.  Before that, we managed to embroider a bottleneck with access to the <code>B-tree</code> root. <br><br>  PostgreSQL was not designed for NUMA architectures and supercomputers.  Adapting Postgres to such architectures is a huge job that would require (and possibly require) the coordinated efforts of many people and even companies.  But the unpleasant consequences of these architectural problems can be mitigated.  And we have to: the types of load that led to delays, similar to those described, are quite typical, we continue to receive similar distress signals from other places.  Similar troubles were manifested before - on systems with a smaller number of cores, the consequences were simply not so monstrous, and the symptoms were treated in other ways and other patches.  Now another medicine has appeared - not universal, but obviously useful. <br><br>  So, when PostgreSQL works with the memory of the entire system as if it were local, no high-speed bus between nodes compares to the access time to local memory.  Tasks arise because of this difficult, often urgent, but interesting.  And the experience of solving them is useful not only decisive, but the entire community. <br><br><img src="https://habrastorage.org/webt/od/n1/sf/odn1sf_id7l60ezlyo-padxymmi.jpeg"></div><p>Source: <a href="https://habr.com/ru/post/423685/">https://habr.com/ru/post/423685/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../423673/index.html">Risks in software development</a></li>
<li><a href="../423675/index.html">Security Week 35: Winter Is Near, How to Assemble a Tesla Trojan</a></li>
<li><a href="../423677/index.html">Jetpack Pilots: Frankie Zapata</a></li>
<li><a href="../423679/index.html">The challenge with the skyscraper and the eggs - not binomial Newton?</a></li>
<li><a href="../423683/index.html">Based on common sense: we grow DevOps from scratch</a></li>
<li><a href="../423687/index.html">HyperX Pulsefire FPS Pro - faster, meaner, more affordable</a></li>
<li><a href="../423689/index.html">ROS MAX - free? We plan to open a license for free commercial use.</a></li>
<li><a href="../423693/index.html">Another way to use Webpack 4 and code sharing</a></li>
<li><a href="../423695/index.html">How to retire to 40 years with a million dollars in a bank account</a></li>
<li><a href="../423697/index.html">Introducing Spring Data JDBC</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>