<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognizing scenes in images using deep convolutional neural networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Many products of our company work with images. Some time ago we decided to add to these services a ‚Äúsmart‚Äù search through photos, their tagging. Such ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognizing scenes in images using deep convolutional neural networks</h1><div class="post__text post__text-html js-mediator-article">  Many products of our company work with images.  Some time ago we decided to add to these services a ‚Äúsmart‚Äù search through photos, their tagging.  Such functionality will be included in the Computer Vision API for further use in the company's products.  One of the important ways to tag images is to tag by scenes, when we end up with something like this: <br><br><img src="https://habrastorage.org/webt/rv/cg/vd/rvcgvdytio3to_ud-5bwxyveh64.png"><br><a name="habracut"></a><br>  In the Computer Vision community, the object recognition problem has been well studied.  One of the main competitions in the field of machine vision is <a href="http://www.image-net.org/">ImageNet</a> , and algorithms (convolutional neural networks since 2012), which won in the object recognition discipline, as a rule, became state-of-the-art.  However, the recognition of scenes is studied much less, only from last year this task is included in ImageNet.  The main difference between scene recognition (Scene recognition, Place recognition, Place classification) and object recognition is that scenes are more complex entities.  For example, in order to determine that the picture shows a kitten, the algorithm needs to detect and recognize one entity - the kitten itself. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/33b/eb0/9ea/33beb09ea0f200b3ef12674669ad3107.jpg" width="380"><img src="https://habrastorage.org/getpro/habr/post_images/ad8/07f/65c/ad807f65c27d946c3ee6253854173fd2.jpg" width="380">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      However, to determine that the restaurant has a picture, you need to find various objects characteristic of it: a table, chairs, counter, kitchen, illuminated menu, visitors, etc. Thus, for the scene recognition it is necessary to determine the context, several objects and interaction with each other. <br><br>  Modern Computer Vision API can work with scenes.  We will see this with examples of the CV API from Google and Microsoft.  Both include, among other things, tagging by scene.  Examples of the Google CV API: <br><br><img src="https://habrastorage.org/webt/vd/mf/jc/vdmfjcjuscwlxrw5radpbbmgxyq.png"><br><br>  And Microsoft CV API: <br><br><img src="https://habrastorage.org/webt/ri/-0/li/ri-0lic06lwosb2pop65aiwip-m.png"><br><br>  As can be seen from the examples above, both services include scene recognition and find a restaurant in the test picture.  Our task was to add similar functionality to our Computer Vision API. <br><br><h2>  Data </h2><br>  Scene recognition began to actively develop in parallel with the onset of the convolutional neural networks (CNN) boom, that is, around the beginning of the 2010s, when databases began to appear in the public domain.  Since our model is a deep CNN, we needed fairly large and diverse datasets to train her.  The table shows the main database. <br><br><table><tbody><tr><td width="80">  Base </td><td width="80">  <a href="http://vision.princeton.edu/projects/2010/SUN/">SUN</a> </td><td width="150">  <a href="http://lsun.cs.princeton.edu/2017/">LSUN</a> </td><td width="150">  <a href="http://places2.csail.mit.edu/">Places2-Standard</a> </td><td width="150">  <a href="http://places2.csail.mit.edu/">Places2-Challenge</a> </td></tr><tr><td>  Number of categories </td><td>  397 </td><td>  ten </td><td>  365 </td><td>  365 </td></tr><tr><td>  Number of images </td><td>  108 754 </td><td>  9 895 373 </td><td>  1,803,486 </td><td>  8,026,628 </td></tr><tr><td>  Images per class </td><td>  &gt; 100 </td><td>  168 103 - 3 033 042 </td><td>  3068 - 5000 </td><td>  3068 - 40,000 </td></tr></tbody></table><br>  In 2014, the <a href="http://places.csail.mit.edu/">Places</a> base appeared, and in 2016, its second, expanded version of <a href="http://places2.csail.mit.edu/">Places2</a> from MIT.  This database can be downloaded in two forms: Places2-Standard and Places2-Challenge.  Places2-Standard is a smaller version of Places2-Challenge in which classes are more balanced.  The Scene classification competition based on Places2-Challenge was included in ImageNet 2016.  The best result in this competition with a top-5 error of 9.01% was shown by the Chinese team Hikvision Research Institute.  Examples from the Places2 database are given below, on such data our network was trained. <br><br><img src="https://habrastorage.org/webt/ct/jx/-3/ctjx-3rcw3zjntar_sml68urfvm.jpeg"><br><br>  Learning modern deep CNN even on Places2-Standard takes several days.  In order to test and debug various architectures and select the parameters of the model and training in a reasonable time, we decided to take the path that is actively used in Object recognition.  There, the models are first tested and debugged on small CIFAR-10 and CIFAR-100 bases, and then the best of them are trained on a large dataset from ImageNet.  We cut down the SUN base to 89 classes, 50 thousand images in the training set and 10 thousand in the validation.  On the received SUN Reduce dataset, the network learns for about 6-10 hours and gives an idea of ‚Äã‚Äãits behavior when training on Places2. <br><br>  In a detailed study of Places2, we decided to get rid of some classes.  The reasons for this were the lack of need for these classes in production and too little data, which created a bias in the training set.  Examples of such categories are aqueduct, tree house, barndoor.  As a result, after cleaning, the base is Places Sift, which consists of 314 classes, the Standard train set includes about 1.5 million images, the Challenge train set includes about 7.5 million images.  Also, when studying the categories, it became clear that there were too many of them and they were too detailed to be used in battle.  Therefore, we have combined some of them into one, calling this procedure Scene mapping.  Here are examples of combining classes: <br><br><ul><li>  bamboo forest, forest broadleaf, forest path, forest road, rainforest -&gt; forest; </li><li>  hospital, hospital room, nursery, operating room -&gt; hospital; </li><li>  hotel outdoor, hotel room, outdoor, youth hostel -&gt; hotel. </li></ul><br>  Scene mapping is applied only when outputting results to the user and only in some tasks where it makes sense.  CNN training takes place on the original Places Sift classes.  Also in the CV API there is an issue of embedding from the network so that new ones can be retrained based on our model. <br><br><h2>  Approaches, solutions </h2><br>  Since 2012 (winning the ImageNet team at the University of Toronto, which used CNN <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> ), convolutional neural networks have become the de facto state-of-the-art in many areas of Computer Vision and demonstrate significant potential for further progress, therefore in our experiments we considered only this type of models.  I will not go into the description of the basics of CNN, some posts are already devoted to this, <a href="https://habrahabr.ru/company/mailru/blog/311706/">for example</a> .  For a general presentation, the picture below depicts the classic LeNet network from Jan Lekun: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/675/c97/304/675c9730412857cf1dabf44028c43143.png"><br><br>  To determine the network architecture, we selected CNN based on the results of ImageNet and Places2 for the last couple of years.  The top models in these two competitions can be divided with some degree of conventionality into the Inception family (GoogLeNet, Inception 2-4, Inception-ResNet and their derivatives) and the Residual Networks family (ResNet and various ways to improve it).  As a result of our experiments on the recognition of scenes, models from the ResNet family showed themselves best of all, so further narration will be associated with them. <br><br>  <a href="https://arxiv.org/pdf/1512.03385v1.pdf">Residual Networks</a> , abbreviated ResNet (can be translated as ‚Äúresidual networks‚Äù) appeared at the end of 2015 and won ImageNet 2015. Their inventors are a team from the Asian division of Microsoft Research.  They were able to build and successfully train networks of very great depth, that is, with a large number of layers.  Comparison of winners of different years in this parameter from the authors: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/157/a98/f7b/157a98f7bfa8e43554607975d7d3f511.png"><br><br>  The main element of ResNet is a Residual block (residual block) with a shortcut connection, through which the data passes unchanged.  The res block consists of several convolutional layers with activations that convert the input signal <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.33ex" height="1.455ex" viewBox="0 -520.7 572.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> x </script>  at <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.88ex" height="2.66ex" viewBox="0 -832 2101 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-46" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-28" x="749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="1139" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-29" x="1711" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2"> F (x) </script>  .  A shortcut connection is the same transformation. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi><mtext>&amp;#xA0;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="14.505ex" height="2.419ex" viewBox="0 -780.1 6245 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-72" x="822" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-69" x="1274" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-67" x="1619" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-68" x="2100" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="2676" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-61" x="3038" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-72" x="3567" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-72" x="4019" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-6F" x="4470" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-77" x="4956" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="5672" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mtext>&nbsp;</mtext><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mi>a</mi><mi>r</mi><mi>r</mi><mi>o</mi><mi>w</mi><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> x \ rightarrow x </script>  . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a3c/d09/08d/a3cd0908d8d61a4d8f02d4f6561ce16e.png"></div><br><br>  As a result of this design, the Res-block teaches how to input <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.33ex" height="1.455ex" viewBox="0 -520.7 572.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-4"> x </script>  differs from <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>F</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.88ex" height="2.66ex" viewBox="0 -832 2101 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-46" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-28" x="749" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="1139" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-29" x="1711" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-5"> F (x) </script> .  Therefore, if on some layer the network has already sufficiently well approximated the original function generating the data, then on further layers the optimizer can make the weights close to zero in the Res blocks, and the signal will pass through the shortcut connection almost unchanged.  In a sense, it can be said that CNN itself determines its depth. <br><br>  The most important issue in such a network architecture is the construction of the Res-block.  Most of the research and improvements in ResNet are related to this topic.  In 2016, <a href="https://arxiv.org/pdf/1603.05027v2.pdf">an article by the same authors was published</a> , in which they proposed a new way of building the Res-block. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/065/252/095/065252095bef91c6d1cbc2336ea9daeb.png"></div><br><br>  With this approach, it is proposed to make the last activation (ReLU) in the Res-block and move the layers of normalization (batch normalization) and activation before the convolutional layers.  This design allows the signal to flow unchanged from one Res block to another.  <a href="https://arxiv.org/pdf/1603.05027v2.pdf">In the article, the</a> authors give a mathematical explanation to the fact that this trick contributes to combating the damped gradient and, therefore, allows you to build networks of very great depth.  For example, the authors successfully trained ResNet with 1001 layers. <br><br>  For our research, the PyTorch framework was chosen because of its flexibility and speed.  The comparison of the trained models was carried out according to the measurements of the top-1 and top-5 errors on three tests: <br><br><ol><li>  Places val is a validation set from Places Sift. </li><li>  Places val Scene map is a validation set from Places Sift + Scene mapping. </li><li>  Manual test - images provided by our colleagues from their clouds, marked up manually.  This test is closest to the ‚Äúcombat‚Äù use of CNN, but because of the relatively small amount of data, it is less robust. </li></ol><br>  As a result of the experiments, two models were trained: ResNet-50 and ResNet-200.  Both networks were connected with ImageNet, as this approach showed a significant advantage in our task before learning from scratch.  We compared the trained models on tests with the benchmark model ResNet-152, which was provided by the authors of the base Places2.  Top-1 errors: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/300/57e/108/30057e1085fc5f77a340f77cb28e90f3.jpg"><br><br>  And top-5 errors: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/14c/399/093/14c399093cf4d022694b56584fd90c77.jpg"><br><br>  As can be seen from the graphs, ResNet-200 almost everywhere wins, which, in principle, is not surprising. <br><br><h2>  Wide Residual Network </h2><br>  Of course, we did not stop at the standard ResNet and, after receiving the results, we continued research on improving the quality of our CNN.  A simple increase in the number of layers does not improve.  In the <a href="https://arxiv.org/pdf/1605.06431v1.pdf">article, the</a> authors show that the Residual Network is an ensemble of less deep grids.  Below is an illustration of an article on the presentation of ResNet as an ensemble. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ee4/493/85b/ee449385b4ff7e58b19d1afaaf840213.png"><br><br>  This paper argues that the last Res-blocks make a small contribution to the formation of the final result of the entire network, so a simple increase in the number of blocks does not give the expected result.  From these considerations, the idea arose to increase not the depth, but the width of the Residual-block, that is, the number of filters in convolutions. <br><br>  Launched in 2016, the <a href="https://arxiv.org/pdf/1605.07146v1.pdf">Wide Residual Network</a> does just that.  The author of the work takes the usual ResNet and increases the number of channels in the convolutions in Res-blocks, reaching with a smaller number of quality parameters deeper ResNet.  We used the Wide ResNet-50-2 model (WRN-50-2), trained on ImageNet, which is a ResNet-50 with a doubling of channels.  The author himself clearly illustrated the difference of the Wide Res block (left) from the classic Res block (right): <br><br><img src="https://habrastorage.org/webt/ma/l9/lf/mal9lftlgneorjbwh-wxjkmrdla.png" height="300"><img src="https://habrastorage.org/webt/uz/2v/u3/uz2vu309fqrusmmclv9s9w8q8uo.png" height="300"><br><br>  WRN-50-2 shows close results for ResNet-200 on ImageNet: 21.9% top-1 errors versus 21.7%.  At the same time, the WRN-50-2 is almost twice as fast as the ResNet-200. <br><br>  For further analysis of the Wide ResNet architecture, let's go back a couple of steps back to the standard Res-block.  To increase the efficiency of computations and reduce the number of parameters for large networks, such as ResNet-50/101/152, the authors applied the approach that appeared in the <a href="https://arxiv.org/pdf/1312.4400v3.pdf">Network In Network</a> model and then was implemented in the <a href="https://arxiv.org/pdf/1409.4842v1.pdf">Inception</a> model.  The idea is to use 1 √ó 1 convolutions to reduce the number of channels before the more expensive 3 √ó 3 convolutions, and then restore the original number of channels with another 1 √ó 1 convolution.  The res block with this trick is called a bottleneck.  Below is a comparison of the original (left) and bottleneck-blocks (right). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/48b/2fb/dab/48b2fbdaba9bbd97bcd1f4a99103d5d2.png"></div><br><br>  In Wide ResNet, the increase in the number of channels occurs just for ‚Äúinternal‚Äù bundles in bottleneck blocks.  In the WRN-50-2, the number 64 in the figure on the right increases to 128. <br><br>  We give a comparison of the trained WRN on our tests.  Top-1 errors: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f3e/aca/c96/f3eacac96d3886525b67e2a266154667.jpg"><br><br>  Top 5 errors: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/140/2f9/4ce/1402f94ce9ce3402d666fab57719c5e3.jpg"><br><br>  The best model was the WRN-50-2, dubbed with ImageNet and trained on the Places Sift Challenge.  It surpassed ResNet-200, while working almost twice as fast.  We measured the speed of work on a single GPU Maxwell Titan X, cuDNN 5.0.05 on a batch of 16 images.  WRN-50-2 showed 83 ms versus 154 ms for ResNet-200.  Such an increase in speed with a close number of parameters can be explained by the ‚Äúwidth‚Äù of the Res-block and, consequently, by the great possibilities for parallelizing the calculations in it.  Training WRN-50-2 took about two weeks. <br><br><h2>  Work examples </h2><br>  Here are some examples of how our CNN works using Scene mapping.  Examples of successful work (in the format of the prediction and its corresponding speed): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad8/07f/65c/ad807f65c27d946c3ee6253854173fd2.jpg"><br><br><table><tbody><tr><td>  Predicted </td><td>  Scene mapping </td></tr><tr><td><ul><li>  restaurant 0.33 </li><li>  pub / indoor 0.11 </li><li>  beer_hall 0,1 </li><li>  food_court 0.08 </li><li>  dining_hall 0,07 </li></ul><br></td><td><ul><li>  restaurant 0.62 </li><li>  dining room 0,07 </li></ul><br></td></tr></tbody></table><br><img src="https://habrastorage.org/getpro/habr/post_images/56c/211/add/56c211addf726d12ab762206ccbf1b9c.jpg"><br><br><table><tbody><tr><td>  Predicted </td><td>  Scene mapping </td></tr><tr><td><ul><li>  mountain_path 0,2 </li><li>  rainforest 0.17 </li><li>  forest_path 0.08 </li><li>  creek 0,07 </li><li>  valley 0.06 </li></ul><br></td><td><ul><li>  forest 0,22 </li><li>  mountain path 0,2 </li><li>  creek 0,07 </li><li>  valley 0.06 </li></ul><br></td></tr></tbody></table><br><img src="https://habrastorage.org/getpro/habr/post_images/5b6/8be/096/5b68be096c7888ef2c0ce0fdd51b66bb.jpg"><br><br>  Predicted: <br><br><ul><li>  harbor 0,42 </li><li>  coast 0.13 </li><li>  cliff 0.12 </li><li>  promenade 0.07 </li><li>  ocean 0.04 </li></ul><br>  Example of unsuccessful work: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c8c/8dd/95b/c8c8dd95bf5f0df5595bd8f5cfa7f5a9.jpg"><br><br>  Predicted: <br><br><ul><li>  palace 0.21 </li><li>  museum 0.16 </li><li>  plaza 0.12 </li><li>  yard 0,1 </li><li>  church 0,13 </li></ul><br><h2>  Other ResNet </h2><br>  The ResNet family continues to grow.  We tried using the last few members of this family to recognize scenes.  One of them is the <a href="https://arxiv.org/pdf/1610.02915.pdf">PyramidNet</a> network, which showed promising results on CIFAR-10/100 and on ImageNet.  The main idea of ‚Äã‚Äãthis CNN is to gradually increase the number of channels in convolutions, and not sharply several times, as it happens in ordinary ResNet.  PyramidNet options: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/156/6c4/f60/1566c4f60177e398e248c75e94642ca9.png"><br><br>  The authors of the original ResNet are also thinking of expanding the network.  They created the <a href="https://arxiv.org/pdf/1611.05431.pdf">ResNeXt</a> model, which offers a ‚Äúsmart‚Äù Res-block extension.  The basic idea is to decompose the number of channels in the Res block into several parallel streams, for example, 32 blocks with 4 channels = 128 channels instead of the original 64 from the usual Res block (left), as shown in the picture below. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/db2/cc9/87a/db2cc987a20dbca68bb53843b9afb715.png"></div><br><br>  This approach is similar to Inception, but all parallel blocks are the same.  With the same complexity of the model as the ResNet, we get an improvement in quality. <br><br>  Unfortunately, neither PyramidNet nor ResNeXt were able to outperform the WRN-50-2 in our tests and showed similar results. <br><br><h2>  "Creative" approaches to improving CNN </h2><br>  In the course of the research, we tried several approaches to improving the quality of the network, based on the fact that for each class of scenes there are characteristic objects, and if we learn to distinguish them, then this will help correct the mistakes that the main CNN allows. <br><br>  The first approach is based on the <a href="http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf">Class activation map</a> (CAM).  Consider the Global average pooling (GAP) layer of our network, followed by the Softmax layer.  Denote <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.607ex" height="2.66ex" viewBox="0 -832 2844.7 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-2C" x="1512" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-79" x="1957" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-29" x="2455" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6"> f (x, y) </script>  as the output of the convolution layer before the GAP, <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>k</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.211ex" height="2.057ex" viewBox="0 -780.1 521.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-6B" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>k</mi></math></span></span><script type="math/tex" id="MathJax-Element-7"> k </script>  - channel number <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>x</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.33ex" height="1.455ex" viewBox="0 -520.7 572.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-78" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-8"> x </script>  and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.155ex" height="1.817ex" viewBox="0 -520.7 497.5 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-79" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi></math></span></span><script type="math/tex" id="MathJax-Element-9"> y </script>  - spatial coordinates, <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>w</mi><mi>c</mi></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.608ex" height="2.057ex" viewBox="0 -780.1 1123 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-77" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-63" x="1013" y="513"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>w</mi><mi>c</mi></msup></math></span></span><script type="math/tex" id="MathJax-Element-10"> w ^ c </script>  - weights in the layer after GAP, corresponding to <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>c</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.007ex" height="1.455ex" viewBox="0 -520.7 433.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-63" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>c</mi></math></span></span><script type="math/tex" id="MathJax-Element-11"> c </script>  th class.  Then: <br><br><img src="https://habrastorage.org/webt/tz/om/kj/tzomkjwda5gpuveqby9mlnvyv20.png"><br><br>  The basic idea is that instances of one scene should have similar objects as CAM.  CAM example for some classes from Places2 after displaying them on the original image: <br><br><img src="https://habrastorage.org/webt/s-/8w/6k/s-8w6kv-qwe_ohxhcfqgjx3ox7g.png"><br><br>  To implement this approach, we used two networks.  Let Network 1 be the new CNN, for example ResNet-50, trained on ImageNet, and Network 2 is our Places Sift WRN-50-2.  We tried two types of training.  The first algorithm is: <br><br><ol><li>  The input image is run through network 2, and a CAM is displayed on it. </li><li>  The resulting CAM is run through network 1. </li><li>  The result of network 1 is added to the loss function of network 2. </li><li>  Network 2 is learning with this new loss feature. </li></ol><br>  The second algorithm consists of the following steps: <br><br><ol><li>  The input image is run through network 2, and a CAM is displayed on it. </li><li>  The resulting CAM is run through network 1. </li><li>  Only network 1 is trained. </li><li>  When outputting, an ensemble from networks 1 and 2 is used. </li></ol><br>  We experimented with both algorithms, but failed to achieve quality improvement. <br><br>  The next approach we applied to our model is <a href="https://arxiv.org/pdf/1406.6247.pdf">Visual recurrent attention</a> (VRA).  The algorithm is rather complicated, and I suggest turning to the article for details, and here I will tell you about its main steps and how we used it in our task.  The main idea of ‚Äã‚ÄãVRA is that it ‚Äúlooks‚Äù at various image patches, the sequence of which is determined by a recurrent network, and concludes that the entire image is classified according to this sequence.  In general, the algorithm looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a32/779/a90/a32779a908ae88e4f3cdaa89b3306c75.png"><br><br>  In step A of the image <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>X</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.75ex" height="2.419ex" viewBox="0 -780.1 1184.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-58" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="1171" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>X</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-12"> X_t </script>  several patches are cut in scale with the center at the incoming point <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>l</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.62ex" height="2.539ex" viewBox="0 -780.1 1558.5 1093.4" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-6C" x="0" y="0"></use><g transform="translate(298,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-31" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>l</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-13"> l_ {t-1} </script>  .  At stage B, these patches, along with the coordinates of the point <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>l</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>t</mi><mo>&amp;#x2212;</mo><mn>1</mn></mrow></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.62ex" height="2.539ex" viewBox="0 -780.1 1558.5 1093.4" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-6C" x="0" y="0"></use><g transform="translate(298,-150)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-2212" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMAIN-31" x="1140" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>l</mi><mrow class="MJX-TeXAtom-ORD"><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></math></span></span><script type="math/tex" id="MathJax-Element-14"> l_ {t-1} </script>  run through two small networks, resulting in their overall vector representation <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>g</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.935ex" height="1.817ex" viewBox="0 -520.7 833.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="675" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>g</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-15"> g_t </script>  .  It enters the recurrent network. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>h</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.318ex" height="2.419ex" viewBox="0 -780.1 998.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-66" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-68" x="693" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>h</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-16"> f_h </script>  which at each iteration issues a classification decision <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>a</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.056ex" height="1.817ex" viewBox="0 -520.7 885.1 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="748" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>a</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-17"> a_t </script>  and the new point of the center of the patches <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>l</mi><mi>t</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.519ex" height="2.419ex" viewBox="0 -780.1 654.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-6C" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-74" x="422" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>l</mi><mi>t</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-18"> l_t </script>  . <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>f</mi><mi>h</mi></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.318ex" height="2.419ex" viewBox="0 -780.1 998.1 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-66" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/mailru/blog/359214/&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjVL9mDYbPlK0yToFwiPKsKwcmgJg#MJMATHI-68" x="693" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mi>h</mi></msub></math></span></span><script type="math/tex" id="MathJax-Element-19"> f_h </script>  passes a certain number of steps. <br><br>  We experimented with VRA, giving it not an image to the input, but a feature map from one of the layers of the WRN-50-2 network after running the input image through it.  Our idea was that the Attention mechanism would help to highlight the objects specific to the scene.  However, such an algorithm studies for a very long time, and we were not able to raise the quality above 20% in a reasonable time. <br><br>  In the <a href="https://arxiv.org/pdf/1605.06431v1.pdf">article</a> that we considered in the section about Wide ResNet, there is an observation that some Res-blocks can be thrown out without a great loss in quality.  We conducted experiments to remove several Res-blocks from ResNet-200, however, even with an increase in the speed of its work, the WRN-50-2 remained the best choice for the model. <br><br><h2>  Conclusion </h2><br>  Now our team continues to experiment in Scene recognition.  We train new CNN architectures (mainly from the ResNet family), try other CAM variants, set up Visual Recurrent Attention and try different approaches with more sophisticated image patches. <br><br>  In this post, we looked at the basic data sets for Scene recognition, different approaches to solving this problem, found out that Wide ResNet still shows the best result, and looked at some methods to improve the model. <br><br>  In conclusion, I can say that Scene recognition is a necessary, but so far relatively unexplored area of ‚Äã‚ÄãComputer Vision.  The task is very interesting, and you can experiment with different approaches that may not be suitable for classic Object recognition (for example, CAM or VRA). </div><p>Source: <a href="https://habr.com/ru/post/359214/">https://habr.com/ru/post/359214/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../359204/index.html">Build projects with dapp. Part 2: JavaScript (frontend)</a></li>
<li><a href="../359206/index.html">Caution. How to use process metrics without harming process health</a></li>
<li><a href="../359208/index.html">How to quickly and efficiently work with priorities according to the Lean Prioritization method?</a></li>
<li><a href="../359210/index.html">VPNFilter infected more than 500,000 devices worldwide</a></li>
<li><a href="../359212/index.html">Add Distribution to SObjectizer-5 using MQTT and libmosquitto</a></li>
<li><a href="../359216/index.html">Create, configure and use your own Git-server</a></li>
<li><a href="../359218/index.html">Continuing the theme of automating the output of files on the template. Excel</a></li>
<li><a href="../359220/index.html">Economy of semiconductor production in Russia: we are analyzing one news</a></li>
<li><a href="../359222/index.html">Weak HTTPS. Part 1</a></li>
<li><a href="../359224/index.html">How does PandaDoc work effectively with lead scoring?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>