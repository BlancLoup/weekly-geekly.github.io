<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Automatic textual key detection (Sentiment Analysis)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="For a short time of my learning process, I realized one thing - knowledge needs to be shared. I realized this a long time ago, but laziness to overcom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Automatic textual key detection (Sentiment Analysis)</h1><div class="post__text post__text-html js-mediator-article">  For a short time of my learning process, I realized one thing - knowledge needs to be shared.  I realized this a long time ago, but laziness to overcome and find time does not always work. <br><br>  This article will discuss the use of various machine learning methods to solve problems associated with the processing of natural language (NLP).  One of these problems is the automatic determination of the emotional color (positive, negative, neutral) of textual data, that is, sentiment analysis.  The purpose of this task is to determine whether the given text (say, a film review or commentary) is positive, negative or neutral in its effect on the reputation of a particular object.  The difficulty of analyzing the tonality lies in the presence of an emotionally enriched language - slang, polysemy, uncertainty, sarcasm, all these factors mislead not only people, but also computers. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8e4/768/568/8e47685686001174765c7b00bb4b8b44.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      On Habr√© more than once appeared articles related to the definition of tonality <a href="http://habrahabr.ru/post/149605/">1</a> , <a href="http://habrahabr.ru/post/247299/">2</a> , <a href="http://habrahabr.ru/post/146903/">3</a> .  Anyway, this topic is one of the most discussed around the world recently [1, 2, 3, 4]. <br><br>  Immediately I will discuss that you will not find any particular innovations in this article, this material can most likely serve as a tutorial for newcomers in the field of machine learning and NLP, which I am.  The main material that I used you can find <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial">on this link</a> .  The entire source code can be found <a href="https://github.com/wendykan/DeepLearningMovies/blob/master/">at this link</a> . <br><br>  So, what is the problem and how to solve it? <br><a name="habracut"></a><br>  Suppose we have a text message (film description, review, comments): <br><br>  <b>‚ÄúThis film made me upset.</b>  <b>It‚Äôs just that you‚Äôve taken your time.</b> <br><br>  Or: <br><br>  <b>‚ÄúThe best movie I've ever seen !!!</b>  <b>Musical composition, actors, scenario, etc.</b>  <b>all this stuff are just amazing !!! ‚Äù</b> <br><br>  In the first example, the system should produce a negative result, since the comment is negative, and in the second, respectively, positive.  Such tasks in machine learning are called classification, and the method is learning with a teacher.  That is, at first, the algorithm on the training sample is ‚Äútrained‚Äù, preserving the necessary coefficients and other data of the model, then, upon entering new data, with a certain probability classifies them.  By coefficient, I mean something like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7aa/25e/afa/7aa25eafac719a5ee0ba4899c5c1d3f4.png"><br><br>  Where beta values ‚Äã‚Äãare our coefficients derived from learning from test data.  As we see, this formula ultimately returns a value from 0 to 1 (see <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25B3%25D0%25BC%25D0%25BE%25D0%25B8%25D0%25B4%25D0%25B0">sigmoid</a> for more details), that is, the closer to 0, the greater the likelihood that the text carries negative information. <br><br>  For the training sample, we used an open dataset from <a href="http://www.kaggle.com/">www.kaggle.com</a> , namely, a dataset that includes data from 50,000 IMDB movie reviews, specially selected for tonality analysis.  The tonality metric is a binary value, that is, IMDB rating &lt;5 is awarded a value of 0, and rating&gt; = 7 is awarded a value of 1. <br><br>  Each record of this dataset consists of the following fields: <br><ul><li>  ID - a unique identifier for each review; </li><li>  Sentiment - the tonality of the review;  1 or 0; </li><li>  Review - Review text. </li></ul><br><h3>  Algorithm </h3><br>  So, we proceed directly to solving the problem.  The entire algorithm described in this article is implemented in python (v. 2.7).  For readability, I have broken the algorithm into the following steps: <br><br><h3>  Step 1. Pretreatment </h3><br>  Pre-processing is required before any data processing.  In this stage, all html tags, punctuation, symbols are removed.  This operation is performed using the python library - ‚ÄúBeautiful Soup‚Äù.  Also, all numbers and links in the text are replaced by tags,.  Further in the text there are so-called ‚Äústop words‚Äù - these are frequent words in a language that basically do not carry any semantic meaning (for example, in English these are words like ‚Äúthe, at, about ...‚Äù).  Stop words are removed using the Python Natural Language Toolkit (NLTK) package.  After preprocessing the source text, we get the following: <br><br>  [biography, part, feature, film, remember, going, see, cinema, originally] - That is, a set of words. <br><br>  At this stage, it is possible to further refine further by modifying each word to its initial form (stemming), etc.  But for this experiment, I decided to limit myself to this. <br><br><h3>  Step 2. Vector representation </h3><br><h5>  Approach 1 </h5><br>  The fact is that the computer, as well as mathematical formulas, is easier to work with numbers, and not a set of words.  Therefore, we need to present any text as a vector of numbers.  To do this, you can create a dictionary with all the words, i.e.  combine all the words found in the texts into one large dictionary, or use ready-made dictionaries (Dahl or Zaliznyak), and replace the words from the text with an index in the dictionary.  That is, suppose we have only three reviews with the following pre-processed word vectors: <br><ol><li>  [biography, part, feature] </li><li>  [film, remember, going] </li><li>  [see, cinema, originally] </li></ol><br>  Combining all the words from the list into one we get the following sorted dictionary (let's call it as a basis vector): <br><br>  [biography, cinema, feature, film, going, originally, part, remember, see] <br><br>  Replacing the previous vectors with the index of the word in the dictionary, we get the following: <br><ol><li>  [1, 0, 1, 0, 0, 0, 1, 0, 0] </li><li>  [0, 0, 0, 1, 1, 0, 0, 1, 0] </li><li>  [0, 1, 0, 0, 1, 0, 0, 0, 1] </li></ol><br>  Having done this work for all reviews, we can get a fairly large list (in my example, I took 5,000 of the most common words).  These vectors are called ‚Äúproperty vectors‚Äù or ‚Äúfeatures vector‚Äù.  In this way, we get vectors for each test recall, then we can compare these vectors using standard metrics such as Euclidean distance, cosine distance, etc.  This approach is called ‚Äúbag of words‚Äù or ‚ÄúBag-Of-Words‚Äù. <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> CountVectorizer <span class="hljs-comment"><span class="hljs-comment">#   sklearn       ‚ÄúBag-Of-Words‚Äù vectorizer = CountVectorizer(analyzer = "word", \ tokenizer = None, \ preprocessor = None, \ stop_words = None, \ max_features = 5000) train_data_features = vectorizer.fit_transform(clean_train_reviews) train_data_features = train_data_features.toarray()</span></span></code> </pre> <br><h5>  Approach 2 </h5><br>  The first approach is a fairly common method and is quite simple to implement, but it is not excluded from the shortcomings.  When comparing two vectors, an exact match of words is used, and we lose important information.  One of these "missing" information is the semantics of the word.  For example, we can easily replace the word ‚Äúblack‚Äù with the word ‚Äúdark‚Äù, since their meaning is very similar.  Such words can be called - semantic similar words.  The group of such words includes synonyms, hyponyms, hyperonyms, etc. <br><br>  In an alternative approach, we will try to replace each word in the list with the number of its semantic group.  As a result, we get something like a "bag of words", but with a deeper meaning.  For this, Google‚Äôs Word2Vec technology is used.  It can be found in the gensim library package, with built-in Word2Vec models. <br><br>  The essence of the Word2Vec model is as follows: a large amount of text is given at the entrance (in our case approximately 10,000 reviews), at the output we get a weighted vector for each word, a fixed length (the length of the vector is set manually), which is found in dataset.  For example, for the word men, comparing with all words and sorting in descending order, I got the following result (I chose the cosine distance for the proximity measure): <br><br>  <b>Semantic words close to 'man'</b> <br><br><table border="1"><tbody><tr><td>  Words </td><td>  Measuring </td></tr><tr><td>  woman </td><td>  0.6056 </td></tr><tr><td>  guy </td><td>  0.4935 </td></tr><tr><td>  boy </td><td>  0.4893 </td></tr><tr><td>  men </td><td>  0.4632 </td></tr><tr><td>  person </td><td>  0.4574 </td></tr><tr><td>  lady </td><td>  0.4487 </td></tr><tr><td>  himself </td><td>  0.4288 </td></tr><tr><td>  girl </td><td>  0.4166 </td></tr><tr><td>  his </td><td>  0.3853 </td></tr><tr><td>  he </td><td>  0.3829 </td></tr></tbody></table><br>  You can find out more about how the Word2Vec model works <a href="https://code.google.com/p/word2vec/">at this link</a> . <br><br>  Further, clustering is used to combine similar words.  Yes, there is another abstruse word - clustering.  I will not dwell on this in detail, the article in the wiki ( <a href="https://en.wikipedia.org/wiki/Cluster_analysis">sigmoid</a> ) I think will explain everything well.  But let me tell you the essence of the most primitive clustering algorithm (K-means): suppose we have a certain number of clusters N, the algorithm learning the training data divides them into clusters and finds the centers of each of them, then when entering the test data, the algorithm assigns it a cluster number, center which is the closest to him.  In this case, I just took the number of words in the dictionary and divided it into 5, assuming that there would be an average of 5 words in each cluster.  On average, I had ~ 3000 clusters.  Next, we do the same thing as in the first ‚ÄúBag-Of-Words‚Äù approach, replacing each word with a cluster index, only this time we have something like ‚ÄúBag-Of-Clusters‚Äù.  Full source code with explanations on this method, you can get <a href="https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors">at this link</a> . <br><br><h3>  Step 3. Classification of texts </h3><br>  So, at the bathing stage, we have already deleted all unnecessary things, transformed the text into a vector, and now we are entering the home straight.  The Random Forest Classifications Algorithm is used for document classifications in this experiment.  The algorithm is already implemented in the scikit-learn package, all we can do is feed our text data and specify the number of trees.  Then the algorithm takes over, trains on the training set, saves all the necessary data. <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier <span class="hljs-comment"><span class="hljs-comment">#    - 100 forest = RandomForestClassifier(n_estimators = 100) forest = forest.fit( train_data_features, train["sentiment"] )</span></span></code> </pre><br><h3>  results </h3><br>  In short, I launched a classifier based on both approaches for obtaining eigenvectors.  Here are some interesting results: <br><table border="1"><tbody><tr><td>  Method </td><td>  precision </td><td>  recall </td><td>  F-measure </td><td>  accuracy </td></tr><tr><td>  bag-of-words </td><td>  85.2% </td><td>  83.7% </td><td>  84.4% </td><td>  84.5% </td></tr><tr><td>  Word2vec </td><td>  90.3% </td><td>  87.2% </td><td>  88.7% </td><td>  89.8% </td></tr></tbody></table><br><br>  Considering the fact that the launch of Word2Vec took me 2 hours on my old laptop, it showed a relatively better result than the good old Bag-Of-Words. <br><br><h3>  Used materials: </h3><br>  [1] I. Chetviorkin, P. Braslavskiy, N. Loukachevich, ‚ÄúSentiment Analysis Track at ROMIP 2011,‚Äù In Computational Linguistics and Intellectual Technologies: Proceedings of the International Conference ‚ÄúDialog 2012‚Äù, Bekasovo, 2012, pp.  1-14. <br>  [2] AA Pak, SS Narynov, AS Zharmagambetov, SN Sagyndykova, ZE Kenzhebayeva, I. Turemuratovich, ‚ÄúThe method of synonyms extraction from unannotated corpus,‚Äù In proc.  of DINWC2015, Moscow, 2015, pp.  1-5 <br>  [3] T. Mikolov, K. Chen, G. Corrado, J. Dean, ‚ÄúEfficient Estimation of Word Representations in Vector Space,‚Äù In Proc.  of Workshop at ICLR, 2013. <br>  [4] P. Bo and L. Lee, ‚ÄúA Sentimental Education: Using Subjectivity Analysis‚Äù, ‚ÄúIn Proceedings of the ACL, 2004‚Äù <br>  [5] T. Joachims, ‚ÄúText categorization with support vector machines: Learning with many relevant features,‚Äù In European Conference on Machine Learning (ECML), Springer Berlin / Heidelberg, 1998, pp.  137-142 <br>  [6] PD Turney, ‚ÄúThumbs up or thumbs down?  Semantic orientation applied to unsupervised classification of reviews, ‚Äú40th Annual Meeting of the Association for Computational Linguistics (ACL'02), Philadelphia, Pennsylvania, 2002, pp.  417-424. <br>  [7] A. Go, R. Bhayani, L. Huang, ‚ÄúTwitter Sentiment Classification Using Distant Supervision,‚Äù Technical report, Stanford.  2009 <br>  [8] J. Furnkranz, T. Mitchell, and E. Riloff, ‚ÄúA Categorization Case on the WWW,‚Äù A AAI / ICML Workshop for Learning for Text Categorization, 1998, pp.  5-12. <br>  [9] MF Caropreso, S. Matwin, F. Sebastiani, ‚ÄúPhDorses for automated textual categorization,‚Äù ‚Äúpp.  78-102. </div><p>Source: <a href="https://habr.com/ru/post/263171/">https://habr.com/ru/post/263171/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../263161/index.html">Universal Analytics Magic</a></li>
<li><a href="../263163/index.html">We connect the backup channel of the Internet in a router with firmware dd-wrt</a></li>
<li><a href="../263165/index.html">Budget Failover Solution for Remote Offices for Microsoft UC</a></li>
<li><a href="../263167/index.html">OData REST API - minor tricks (part 2)</a></li>
<li><a href="../263169/index.html">12 little-known facts about CSS</a></li>
<li><a href="../263175/index.html">Magic Ctrl-C Ctrl-V, or how to stop saving pictures and start living</a></li>
<li><a href="../263179/index.html">Days of field testing technology iBeacon in Luzhniki</a></li>
<li><a href="../263181/index.html">How to lose and find your account on AWS</a></li>
<li><a href="../263183/index.html">Microsoft advised Skype users to change passwords</a></li>
<li><a href="../263187/index.html">Historical budgets from 1866 and a long way to turn them into open data</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>