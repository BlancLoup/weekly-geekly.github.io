<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Generative adversarial networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the last article, we looked at the simplest linear generative model PPCA. The second generative model we‚Äôll look at is Generative Adversarial Netwo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Generative adversarial networks</h1><div class="post__text post__text-html js-mediator-article">  In the <a href="https://habrahabr.ru/post/347184/">last article,</a> we looked at the simplest linear generative model PPCA.  The second generative model we‚Äôll look at is Generative Adversarial Networks, abbreviated GAN.  In this article, we will look at the most basic version of this model, leaving the advanced versions and comparison with other approaches in generative modeling for the following chapters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ed/ec1/d7b/3edec1d7bd4babf13b32535a43e29169.gif"></div><br><a name="habracut"></a><br><h2>  Story </h2><br>  Generative modeling assumes approximation of noncomputable a posteriori distributions.  Because of this, most effective methods developed for teaching discriminative models do not work with generative models.  Past methods for solving this problem are computationally difficult and mainly based on the use of <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">Markov Chain Monte Carlo</a> , which is poorly scalable.  Therefore, a method based on such scalable techniques as <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">Stochastic Gradient Descent (SGD)</a> and <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a> was needed to train generative models.  One such method is Generative Adversarial Networks (GAN).  For the first time GANs were proposed <a href="https://arxiv.org/pdf/1406.2661.pdf">in this</a> article in 2014.  High-level, this model can be described as two sub-models that compete with each other, and one of these models (the generator) tries to learn in a certain sense to deceive the second (the discriminator).  For this, the generator generates random objects, and the discriminator tries to distinguish these generated objects from real objects from the training sample.  In the process of learning, the generator generates objects that are more and more similar to the sample, and it becomes increasingly difficult for the discriminator to distinguish them from the real ones.  Thus, the generator turns into a generative model that generates objects from a certain complex distribution, for example, from the distribution of photographs of human faces. <br><br><h2>  Model </h2><br>  To begin with, we introduce the necessary terminology.  Through <img src="https://habrastorage.org/getpro/habr/post_images/893/8f7/479/8938f7479ea72465602bb25b05952684.gif" alt="image">  we will denote some space of objects.  For example, pictures <img src="https://habrastorage.org/getpro/habr/post_images/b5d/bb0/6d1/b5dbb06d1014d7da290335af95c579e8.gif" alt="image">  pixel.  On some probability space <img src="https://habrastorage.org/getpro/habr/post_images/7e0/838/1ee/7e08381eec55a31db8263ce4d9b04120.gif" alt="image">  set vector random variable <img src="https://habrastorage.org/getpro/habr/post_images/6da/1d6/4f3/6da1d64f302a295b65cba84d4078eada.gif" alt="image">  with a probability distribution having a density <img src="https://habrastorage.org/getpro/habr/post_images/105/3d4/c7a/1053d4c7aa4f3d5ba8c16cdecd3c1e2f.gif" alt="image">  such that a subset of space <img src="https://habrastorage.org/getpro/habr/post_images/893/8f7/479/8938f7479ea72465602bb25b05952684.gif" alt="image">  , on which <img src="https://habrastorage.org/getpro/habr/post_images/105/3d4/c7a/1053d4c7aa4f3d5ba8c16cdecd3c1e2f.gif" alt="image">  accepts nonzero values ‚Äã‚Äã‚Äî these are, for example, photographs of human faces.  We are given a random iid sample of faces for size <img src="https://habrastorage.org/getpro/habr/post_images/d24/f91/728/d24f9172866ec514bf85399df48a304a.gif" alt="image">  .  Additionally, we define the auxiliary space <img src="https://habrastorage.org/getpro/habr/post_images/821/c3d/b46/821c3db464fa76f58820d05fe084484d.gif" alt="image">  and random variable <img src="https://habrastorage.org/getpro/habr/post_images/b97/4ec/e0a/b974ece0a10b49115fce5ee7c47dd54e.gif" alt="image">  with a probability distribution having a density <img src="https://habrastorage.org/getpro/habr/post_images/623/c53/913/623c539132e71fa79a9088c940c4bf33.gif" alt="image">  . <img src="https://habrastorage.org/getpro/habr/post_images/673/2bc/f9a/6732bcf9a310e0ebf6749e251410e2bf.gif" alt="image">  - discriminator function.  This function accepts an object as input. <img src="https://habrastorage.org/getpro/habr/post_images/1cc/824/7af/1cc8247aff11798c616d0244d53d8b21.gif" alt="image">  (in our example, a picture of the appropriate size) and returns the probability that the input picture is a photograph of a human face. <img src="https://habrastorage.org/getpro/habr/post_images/3c0/6b0/e07/3c06b0e07e260645021c4540797885ab.gif" alt="image">  - generator function.  It takes value <img src="https://habrastorage.org/getpro/habr/post_images/243/4e1/058/2434e105856dc8e845dc751fa6ae62c8.gif" alt="image">  and gives the space object <img src="https://habrastorage.org/getpro/habr/post_images/893/8f7/479/8938f7479ea72465602bb25b05952684.gif" alt="image">  , that is, in our case, the picture. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Suppose we already have the perfect discriminator <img src="https://habrastorage.org/getpro/habr/post_images/266/1c2/f73/2661c2f73236ce62700299f2115fb4d9.gif" alt="image">  .  For any example <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" alt="image">  it gives the true probability that this example belongs to a given subset <img src="https://habrastorage.org/getpro/habr/post_images/893/8f7/479/8938f7479ea72465602bb25b05952684.gif" alt="image">  the sample from <img src="https://habrastorage.org/getpro/habr/post_images/c98/29d/33e/c9829d33e75d0e160aeb9d630b88f847.gif" alt="image">  .  Rephrasing the problem of deception of a discriminator in a probabilistic language, we obtain that it is necessary to maximize the probability given out by the ideal discriminator using the generated examples.  Thus, the optimal generator is as <img src="https://habrastorage.org/getpro/habr/post_images/b88/2a7/fc1/b882a7fc1b66fcef8c201db725188dcd.gif" alt="image">  .  Because <img src="https://habrastorage.org/getpro/habr/post_images/4d9/dc3/556/4d9dc355616b7cec38466e8925b9b012.gif" alt="image">  - a monotonically increasing function and does not change the position of the extrema of the argument; this formula is rewritten as <img src="https://habrastorage.org/getpro/habr/post_images/40a/56d/8b4/40a56d8b4c4f820ccd684ec48bf429d8.gif" alt="image">  that will be convenient in the future. <br><br>  In reality, there is usually no perfect discriminator and it needs to be found.  Since the discriminator's task is to provide a signal for training the generator, instead of the ideal discriminator, it is enough to take a discriminator that ideally separates the real examples from those generated by the current generator, i.e.  perfect only on subset <img src="https://habrastorage.org/getpro/habr/post_images/893/8f7/479/8938f7479ea72465602bb25b05952684.gif" alt="image">  from which examples are generated by the current generator.  This task can be reformulated as the search for such a function. <img src="https://habrastorage.org/getpro/habr/post_images/266/1c2/f73/2661c2f73236ce62700299f2115fb4d9.gif" alt="image">  , which maximizes the likelihood of properly classifying examples as genuine or generated.  This is called a binary classification task and in this case we have an infinite training set: a finite number of real examples and a potentially infinite number of examples generated.  Each example has a label: it is real or generated.  The <a href="https://habrahabr.ru/post/343800/">first article</a> described the solution of the classification problem using the maximum likelihood method.  Let's write it for our case. <br><br>  So, our sample <img src="https://habrastorage.org/getpro/habr/post_images/0d2/567/d97/0d2567d97be2e80bd868156fea8a9806.gif" alt="image">  .  Determine the density of distribution <img src="https://habrastorage.org/getpro/habr/post_images/639/273/1c1/6392731c1c8f6d6049321fd0d574f497.gif" alt="image">  then <img src="https://habrastorage.org/getpro/habr/post_images/302/a20/7cb/302a207cbe200555687ef8cebbccf4eb.gif" alt="image">  - this is a reformulation of the discriminator. <img src="https://habrastorage.org/getpro/habr/post_images/266/1c2/f73/2661c2f73236ce62700299f2115fb4d9.gif" alt="image">  probability class <img src="https://habrastorage.org/getpro/habr/post_images/559/537/f1e/559537f1e11c68d8ba3d9f6d540de6b0.gif" alt="image">  (real example) as a distribution on classes <img src="https://habrastorage.org/getpro/habr/post_images/7c2/468/be5/7c2468be5a889e6c0883822f8201200e.gif" alt="image">  .  Because <img src="https://habrastorage.org/getpro/habr/post_images/706/96a/524/70696a524f8f24249d50379a2b4b5bcc.gif" alt="image">  This definition specifies the correct probability density.  Then the optimal discriminator can be found as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7ec/6a9/b40/7ec6a9b400ec1590e4142188157478dd.gif" alt="image"></div><br>  Group the multipliers for <img src="https://habrastorage.org/getpro/habr/post_images/c16/183/b68/c16183b68b3634ddf1c923978d874290.gif" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/e6d/390/4db/e6d3904db75f7bb0ac6aa70a1f98ca72.gif" alt="image">  : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/60c/790/dae/60c790dae0a875869d9a4fcaef70f01a.gif" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a3/077/ffd/0a3077ffd5ee46217b5d647ede155b27.gif" alt="image"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9e0/208/886/9e020888672b5079090049fba4e7d8aa.gif" alt="image"></div><br>  And when the sample size tends to infinity, we get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/412/0a1/0b3/4120a10b33f2f2de578a2c66d514b097.gif" alt="image"></div><br>  So, we get the following iterative process: <br><br><ol><li>  Set arbitrary initial <img src="https://habrastorage.org/getpro/habr/post_images/724/73d/cdc/72473dcdc783e8baf459286236c09bfd.gif" alt="image">  . </li><li>  Begins <img src="https://habrastorage.org/getpro/habr/post_images/65a/912/036/65a9120364a862f3e7abfc1c106738bc.gif" alt="image">  iteration, <img src="https://habrastorage.org/getpro/habr/post_images/431/6b4/145/4316b4145ef1d3dd8ec1df76b9e7592c.gif" alt="image">  . </li><li>  We are looking for an optimal discriminator for the current generator: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/154/9d0/2dc/1549d02dc7348e85d3d404a1a530c1ef.gif" alt="image">  . </li><li>  We improve the generator using the optimal discriminator: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/83a/0a4/525/83a0a4525e7a29e274c70bee69ca022c.gif" alt="image">  .  It is important to be in the vicinity of the current generator.  If you move far away from the current generator, the discriminator will no longer be optimal and the algorithm will no longer be correct. </li><li>  The task of learning a generator is considered solved when <img src="https://habrastorage.org/getpro/habr/post_images/ef2/02b/847/ef202b8478fcc337dfe800f8809b6fac.gif" alt="image">  for anyone <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" alt="image">  .  If the process is not converged, then go to the next iteration in paragraph (2). </li></ol><br>  In the original article, this algorithm is summed up in one formula, which in a sense defines a minimax game between the discriminator and the generator: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fdb/96e/afb/fdb96eafbdde0ae877d1f68693508d6a.gif" alt="image"></div><br>  Both functions <img src="https://habrastorage.org/getpro/habr/post_images/61a/255/b19/61a255b192b4c3724a022df2318cddaf.gif" alt="image">  can be represented as neural networks: <img src="https://habrastorage.org/getpro/habr/post_images/707/91d/147/70791d147a66c8667459d0eeaf5d2931.gif" alt="image">  after which the task of finding optimal functions is reduced to the problem of optimization by parameters and it can be solved using traditional methods: backpropagation and SGD.  Additionally, since the neural network is a universal function approximator, <img src="https://habrastorage.org/getpro/habr/post_images/913/1c4/02e/9131c402e363b77609dd249913be0b57.gif" alt="image">  can approximate an arbitrary probability distribution, which removes the question of choosing a distribution <img src="https://habrastorage.org/getpro/habr/post_images/623/c53/913/623c539132e71fa79a9088c940c4bf33.gif" alt="image">  .  It can be any continuous distribution in some reasonable limits.  For example, <img src="https://habrastorage.org/getpro/habr/post_images/d0e/0b3/cf0/d0e0b3cf083c71ad4f23bfe459af9603.gif" alt="image">  or <img src="https://habrastorage.org/getpro/habr/post_images/849/5db/09f/8495db09f5277456c0aae34e5a2a0106.gif" alt="image">  .  The correctness of this algorithm and the convergence <img src="https://habrastorage.org/getpro/habr/post_images/3eb/591/939/3eb591939e41b6eeeaa6195c41cd0614.gif" alt="image">  to <img src="https://habrastorage.org/getpro/habr/post_images/105/3d4/c7a/1053d4c7aa4f3d5ba8c16cdecd3c1e2f.gif" alt="image">  under rather general assumptions, it is proved in the original article. <br><br><h2>  Finding Normal Distribution Parameters </h2><br>  We've dealt with math, now let's see how it works.  Let's say <img src="https://habrastorage.org/getpro/habr/post_images/ac5/1c5/313/ac51c531383d21316f58a4064d514ca9.gif" alt="image">  i.e.  solve one-dimensional problem. <img src="https://habrastorage.org/getpro/habr/post_images/5d7/ff5/38e/5d7ff538e723f4f5aa10bc82298555b0.gif" alt="image">  .  Let's use a linear generator <img src="https://habrastorage.org/getpro/habr/post_images/c69/3de/0b3/c693de0b3fab4b4d66967fd2883c315c.gif" alt="image">  where <img src="https://habrastorage.org/getpro/habr/post_images/420/958/d06/420958d06b05fe0366df5ad7f5794151.gif" alt="image">  .  The discriminator will be a fully connected three-layer neural network with a binary classifier at the end.  The solution to this problem is <img src="https://habrastorage.org/getpro/habr/post_images/34a/8cf/e8e/34a8cfe8e65947b95ed998a9b0f2accf.gif" alt="image">  , i.e, <img src="https://habrastorage.org/getpro/habr/post_images/dfe/b09/aa5/dfeb09aa5780a898ef1112eded612b2d.gif" alt="image">  .  Let us now try to program a numerical solution of this problem using Tensorflow.  The full code can be found <a href="https://github.com/Monnoroch/generative/tree/master/gan_model_data">here</a> , in the article only key points are covered. <br><br>  The first thing to ask is the input sample: <img src="https://habrastorage.org/getpro/habr/post_images/ecc/fe0/dba/eccfe0dbaa209b9e33f8ae306f02bb3b.gif" alt="image">  .  Since the training goes on minicabits, we will generate a vector of numbers at once.  Additionally, the sample is parameterized by the mean and standard deviation. <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">data_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(hparams)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Input data are just samples from N(mean, stddev). """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.random_normal( [hparams.batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>], hparams.input_mean, hparams.input_stddev)</code> </pre> <br>  Now we will set random inputs for the generator <img src="https://habrastorage.org/getpro/habr/post_images/041/fad/9ba/041fad9ba5cd2f5e2824034f35ce000e.gif" alt="image">  : <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generator_input</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(hparams)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">""" Generator input data are just samples from N(0, 1). """</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.random_normal([hparams.batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">1.</span></span>)</code> </pre><br>  We define the generator.  Take the absolute value of the second parameter to give it a meaning of the standard deviation: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(input, hparams)</span></span></span><span class="hljs-function">:</span></span> mean = tf.Variable(tf.constant(<span class="hljs-number"><span class="hljs-number">0.</span></span>)) stddev = tf.sqrt(tf.Variable(tf.constant(<span class="hljs-number"><span class="hljs-number">1.</span></span>)) ** <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> input * stddev + mean</code> </pre><br>  Let's create a vector of real examples: <br><br><pre> <code class="python hljs">generator_input = generator_input(hparams) generated = generator(generator_input)</code> </pre><br>  And the vector of generated examples: <br><br><pre> <code class="python hljs">generator_input = generator_input(hparams) generated = generator(generator_input)</code> </pre><br>  Now let's run all the examples through the discriminator.  Here it is important to remember that we do not want two different discriminators, but one, because Tensorflow should be asked to use the same parameters for both inputs: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"discriminator"</span></span>): real_ratings = discriminator(real_input, hparams) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.variable_scope(<span class="hljs-string"><span class="hljs-string">"discriminator"</span></span>, reuse=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>): generated_ratings = discriminator(generated, hparams)</code> </pre><br>  The loss function on real examples is the cross-entropy between the unit (the expected response of the discriminator on real examples) and the discriminator estimates: <br><br><pre> <code class="python hljs">loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.ones_like(real_ratings), logits=real_ratings))</code> </pre><br>  The loss function in fake examples is the cross-entropy between zero (the expected discriminator response in fake examples) and the discriminator scores: <br><br><pre> <code class="python hljs">loss_generated = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.zeros_like(generated_ratings), logits=generated_ratings))</code> </pre><br>  The discriminator's loss function is the sum of losses on real examples and on fake examples: <br><br><pre> <code class="python hljs">discriminator_loss = loss_generated + loss_real</code> </pre><br>  The generator loss function is the cross-entropy between the unit (the desired discriminator error response on fake examples) and the evaluations of these fake examples by the discriminator: <br><br><pre> <code class="python hljs">generator_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( labels=tf.ones_like(generated_ratings), logits=generated_ratings))</code> </pre><br>  Optionally, L2-regularization is added to the loss function of the discriminator. <br><br>  Training of the model is reduced to alternate training of the discriminator and the generator in a cycle to convergence: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> step <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(args.max_steps): session.run(model.discriminator_train) session.run(model.generator_train)</code> </pre><br>  Below are graphs for four discriminator models: <br><br><ul><li>  three-layer neural network. </li><li>  three-layer neural network with L2 regularization .. </li><li>  three-layer neural network with dropout regularization. </li><li>  three-layer neural network with L2 and dropout regularization. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0e8/ea4/87f/0e8ea487f840b72cfdcef8fdec3244d9.png"></div><br>  Fig.  1. Probability of classification by the discriminator of a real example as a real one. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc7/813/ff8/dc7813ff8734f265a0d8ca68dc061e8d.png"></div><br>  Fig.  2. Probability of classification by the discriminator of the generated example as real. <br><br>  All four models quite quickly converge to the fact that the discriminator produces <img src="https://habrastorage.org/getpro/habr/post_images/4fb/d7c/0cf/4fbd7c0cf722cd4bfbeb119457c483ba.gif" alt="image">  on all entrances.  Because of the simplicity of the problem that the generator solves, there is almost no difference between the models.  From the graphs it can be seen that the mean and standard deviation converge fairly quickly to the values ‚Äã‚Äãfrom the data distribution: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/265/b7f/d19/265b7fd197f3c0a15b04300a0510bbe3.png"></div><br>  Fig.  3. Mean generated distributions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5f0/a3c/4a7/5f0a3c4a756c9c10869065f103e0f978.png"></div><br>  Fig.  4. The standard deviation of the generated distributions. <br><br>  Below are the distribution of these and generated examples in the learning process.  It can be seen that by the end of the training the generated examples are practically indistinguishable from the real ones (they are distinguishable in the graphs because Tensorboard chose different scales, but if you look at the values, they are the same). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/107/dc8/c4e/107dc8c4e3c34981c29b72481de0a7b3.png"></div><br>  Fig.  5. Distribution of real data.  Does not change in time.  The learning step is postponed on the vertical axis. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/963/f24/da5/963f24da596b544ed1dc7346179daf4a.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/aad/a60/aa1/aada60aa1eb8a5ee395cafc306f9a923.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/91e/732/706/91e73270603daeea307a4977921ec96d.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/62b/544/b62/62b544b62d00074fc72c63da66cba5d2.png" width="230"><br>  Fig.  6. Distribution of real data.  Does not change in time.  The learning step is postponed on the vertical axis. <br><br>  Let's look at the learning process of the model: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ed/ec1/d7b/3edec1d7bd4babf13b32535a43e29169.gif"></div><br>  Fig.  7. Visualization of the model learning process.  The motionless Gaussian is the distribution density of real data, the moving Gaussian is the distribution density of the generated examples, the blue curve is the result of the work of the discriminator, i.e.  the likelihood of an example being present. <br><br>  It can be seen that at the beginning of training the discriminator very well shares the data, but the distribution of the generated examples very quickly ‚Äúcreeps‚Äù to the distribution of the real examples.  In the end, the generator approximates the data so well that the discriminator becomes a constant <img src="https://habrastorage.org/getpro/habr/post_images/4fb/d7c/0cf/4fbd7c0cf722cd4bfbeb119457c483ba.gif" alt="image">  and the task converges. <br><br><h2>  Approximation of a mixture of normal distributions I </h2><br>  Let's try to replace <img src="https://habrastorage.org/getpro/habr/post_images/ecc/fe0/dba/eccfe0dbaa209b9e33f8ae306f02bb3b.gif" alt="image">  on <img src="https://habrastorage.org/getpro/habr/post_images/8ae/ee3/620/8aeee3620cd9b6163d6cdd588c980e8c.gif" alt="image">  , thereby simulating a multimodal distribution of the source data.  For this model, you only need to change the code for generating real examples.  Instead of returning a normally distributed random variable, we return a mixture of several: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">data_batch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(hparams)</span></span></span><span class="hljs-function">:</span></span> count = len(hparams.input_mean) componens = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(count): componens.append( tf.contrib.distributions.Normal( loc=hparams.input_mean[i], scale=hparams.input_stddev[i])) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.contrib.distributions.Mixture( cat=tf.contrib.distributions.Categorical( probs=[<span class="hljs-number"><span class="hljs-number">1.</span></span>/count] * count), components=componens) .sample(sample_shape=[hparams.batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br>  Below are graphs for the same models as in the previous experiment, but for data with two modes: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/35f/e77/cd8/35fe77cd82c1298376ac678cbb5e3547.png"></div><br>  Fig.  8. Probability of classification by the discriminator of a real example as a real one. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/138/224/3ce/1382243ce1a1983a19d6c1614c7ef3e7.png"></div><br>  Fig.  9. Probability of classification by the discriminator of the generated example as real. <br><br>  It is interesting to note that regularized models show themselves to be significantly better than non-regularized ones.  However, regardless of the model, it can be seen that now the generator fails to deceive the discriminator so well.  Let's understand why this happened. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/376/55d/975/37655d97500082bc0b85f214e12972ad.png"></div><br>  Fig.  10. Mean generated distributions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/df5/bd4/4ae/df5bd44aee03f2ceda79879c9a55c262.png"></div><br>  Fig.  11. The standard deviation of the generated distributions. <br><br>  As in the first experiment, the generator approximates the data with a normal distribution.  The reason for the decline in quality is that now the data cannot be accurately approximated by a normal distribution, because they are sampled from a mixture of two normal ones.  The modes of the mixture are symmetric about zero, and it can be seen that all four models approximate the data with a normal distribution centered near zero and a fairly large dispersion.  Let's look at the distribution of real and fake examples to see what is going on: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/085/0a9/ac6/0850a9ac68105d5b85672f601d6aa441.png"></div><br>  Figure 12. Distribution of real data.  Does not change in time.  The learning step is postponed on the vertical axis. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/802/4fd/8458024fd6b3ab1cee50c9824a852640.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/518/a2d/64b/518a2d64bf8228c776296058e9dba381.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/16f/327/e24/16f327e24ad37581b23f6762e1ccd024.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/071/b4f/d9c/071b4fd9c359dca62b4c3db7a109ec50.png" width="230"><br>  Figure 13. Distributions of the generated data from four models.  The learning step is postponed on the vertical axis. <br><br>  This is how the model is taught: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1de/645/574/1de645574127682f54ad0604d101ab23.gif"></div><br>  Fig.  14. Visualization of the model learning process.  The motionless mixture of Gaussians is the distribution density of real data, the moving Gaussian is the distribution density of the generated examples, the blue curve is the result of the work of the discriminator, i.e.  the likelihood of an example being present. <br><br>  This animation shows in detail the case studied above.  The generator, not possessing sufficient expressiveness and having the opportunity to approximate data only to a Gaussian, spreads into a wide Gaussian, trying to cover both modes of data distribution.  As a result, the generator reliably deceives the discriminator only in places where the areas under the generator and source data curves are close, that is, in the area of ‚Äã‚Äãintersection of these curves. <br><br>  However, this is not the only possible case.  Let's move the right-hand mode a little more to the right so that the initial approximation of the generator does not capture it. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/43a/29a/7a7/43a29a7a7c7eefb32e343e5b8a97f286.gif"></div><br>  Fig.  15. Visualization of the model learning process.  The motionless mixture of Gaussians is the distribution density of real data, the moving Gaussian is the distribution density of the generated examples, the blue curve is the result of the work of the discriminator, i.e.  the likelihood of an example being present. <br><br>  It is seen that in this case it is most advantageous for the generator to try to bring the left distribution mode closer.  After this happens, the generator tries to make an attempt to capture the left fashion too.  It looks like oscillations of the generator standard deviation in the second half of the animation.  But all these attempts fail, because the discriminator locks the generator somehow and in order to capture the left mode it needs to overcome the barrier of the high loss function, which it cannot do due to the insufficiently high learning speed.  This effect is called fashion collapse. <br><br>  In the two examples described above, we saw two types of problems that arise if the generator is not powerful enough to express the initial distribution of data: mode averaging, when the generator approximates the entire distribution, but everywhere is bad enough;  and the collapse of the mode, when the generator learns a subset of modes, and those that he did not learn, do not affect him in any way. <br><br>  In addition to the fact that both of these problems lead to the divergence of the discriminator to <img src="https://habrastorage.org/getpro/habr/post_images/4fb/d7c/0cf/4fbd7c0cf722cd4bfbeb119457c483ba.gif" alt="image">  they also lead to a decrease in the quality of the generative model.  The first problem leads to the fact that the generator produces examples ‚Äúbetween‚Äù modes, which should not be, the second problem leads to the fact that the generator gives examples only from some modes, thereby reducing the richness of the original data distribution. <br><br><h2>  Approximation of a mixture of normal distributions II </h2><br>  The reason that the previous section did not succeed until the end of the deception of the discriminator was the triviality of the generator, which simply did a linear transformation.  Let us now try to use a fully connected three-layer neural network as a generator: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generator</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, input, hparams)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     256 . input_size = 1 features = 256 weights = tf.get_variable( "weights_1", initializer=tf.truncated_normal( [input_size, features], stddev=0.1)) biases = tf.get_variable( "biases_1", initializer=tf.constant(0.1, shape=[features])) hidden_layer = tf.nn.relu(tf.matmul(input, weights) + biases) #     256 . features = 256 weights = tf.get_variable( "weights_2", initializer=tf.truncated_normal( [input_size, features], stddev=0.1)) biases = tf.get_variable( "biases_2", initializer=tf.constant(0.1, shape=[features])) hidden_layer = tf.nn.relu(tf.matmul(input, weights) + biases) #   ,  . output_size = 1 weights = tf.get_variable( "weights_out", initializer=tf.truncated_normal( [features, output_size], stddev=0.1)) biases = tf.get_variable( "biases_out", initializer=tf.constant(0.1, shape=[output_size])) return tf.matmul(hidden_layer, weights) + biases</span></span></code> </pre><br>  Let's look at the training schedules. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bd1/f03/12b/bd1f0312b84fb2bc546a4a22d2cc0bf0.png"></div><br>  Fig.  16. Probability of classification by the discriminator of a real example as real. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/941/cd4/475/941cd4475a8812695b1aced4f8f45593.png"></div><br>  Fig.  17. Probability of classification by the discriminator of the generated example as real. <br><br>  It can be seen that because of the large number of parameters, the training has become much more noisy.  Discriminators of all models converge to about <img src="https://habrastorage.org/getpro/habr/post_images/4fb/d7c/0cf/4fbd7c0cf722cd4bfbeb119457c483ba.gif" alt="image">  but behave unstably around this point.  Let's look at the form of the generator. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/085/0a9/ac6/0850a9ac68105d5b85672f601d6aa441.png"></div><br>  Figure 18. Distribution of real data.  Does not change in time.  The learning step is postponed on the vertical axis. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ab2/6cd/25a/ab26cd25a04177e70b21f74fb2e0aefe.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f76/4c0/999/f764c0999ab7b651b5413ccbf35b515d.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/c62/d0c/ce4/c62d0cce49524f565f28bd5232d6284c.png" width="230"><img src="https://habrastorage.org/getpro/habr/post_images/342/dc8/31d/342dc831dfb534f7959ba53c40a478a1.png" width="230"><br>  Figure 19. Distributions of the generated data from four models.  The learning step is postponed on the vertical axis. <br><br>  It can be seen that the distribution of the generator does not at least coincide with the distribution of data, but is quite similar to it.  The most regularized model again showed itself better than anyone.  It can be seen that she learned two modes that roughly coincide with the modes of data distribution.  Peak sizes are also not very accurate, but approximate the distribution of data.  Thus, the neural network generator is able to learn the multimodal distribution of data. <br><br>  This is how the model is taught: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/74c/e28/1e6/74ce281e65e8d50d56217d2f5583c279.gif"></div><br>  Fig.  20. Visualization of the learning process of a model with similar mods.  The motionless mixture of Gaussians is the distribution density of real data, the moving Gaussian is the distribution density of the generated examples, the blue curve is the result of the work of the discriminator, i.e.  the likelihood of an example being present. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f42/fbf/c8c/f42fbfc8c8ca149f9c94a2548864dcb1.gif"></div><br>  Fig.  21. Visualization of the learning process of a model with distant modes.  The motionless mixture of Gaussians is the distribution density of real data, the moving Gaussian is the distribution density of the generated examples, the blue curve is the result of the work of the discriminator, i.e.  the likelihood of an example being present. <br><br>  These two animations show learning on the data distributions from the previous section.  From these animations it can be seen that when using a sufficiently large generator with a variety of parameters, it is, albeit rather crudely, but capable of approximating the multimodal distribution, thereby indirectly confirming that the problems from the previous section arise from the insufficiently complex generator.  The discriminators in these animations are much more noisy than in the section on finding the parameters of the normal distribution, but by the end of the training they begin to resemble a noisy horizontal line. <img src="https://habrastorage.org/getpro/habr/post_images/3fe/d23/6ce/3fed236ce01b5b6b572811db8f3dd810.gif" alt="image">  . <br><br><h2>  Results </h2><br>  GAN is a model for approximating an arbitrary distribution only by sampling from this distribution.  In this article, we looked at the details of how the model works on a trivial example of finding parameters of a normal distribution and on a more complex example of approximating a bimodal distribution by a neural network.  Both tasks were solved with good accuracy, which required only the use of a rather complex generator model.  In the next article, we will move from these model examples to real examples of generating samples from complex distributions using the example of image distribution. <br><br><h2>  Thanks </h2><br>  Thank you <a href="https://www.linkedin.com/in/olga-talanova-b319b761/">Olga Talanova</a> and <a href="https://www.linkedin.com/in/ruslan-login-68bb2676/">Ruslan Login</a> for the review of the text.  Thank you <a href="https://www.linkedin.com/in/ruslan-login-68bb2676/">Ruslan Login</a> for help in preparing images and animations.  Thanks to <a href="https://github.com/andrewtar">Andrei Tarashkevich</a> for help with the layout of this article. </div><p>Source: <a href="https://habr.com/ru/post/352794/">https://habr.com/ru/post/352794/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352782/index.html">Debugging multithreaded programs based on FreeRTOS</a></li>
<li><a href="../352784/index.html">ESET: Turla Mosquito backdoor used in Eastern Europe</a></li>
<li><a href="../352786/index.html">How to use Azure for free (life hacking for students)</a></li>
<li><a href="../352790/index.html">Public Key Infrastructure: Certification Center based on the OpenSSL utility, SQLite3 and Tcl / Tk</a></li>
<li><a href="../352792/index.html">Conference DEFCON 22. ‚ÄúTraveling on the dark side of the Internet. Introduction to Tor, Darknet and Bitcoin ¬ª</a></li>
<li><a href="../352798/index.html">Reality of reuse</a></li>
<li><a href="../352800/index.html">Release Rust 1.25</a></li>
<li><a href="../352802/index.html">Intel Summer School 0x7E2 - there is a reason to learn</a></li>
<li><a href="../352804/index.html">MobileNet: smaller, faster, more accurate</a></li>
<li><a href="../352806/index.html">Arduino for beginners. Part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>