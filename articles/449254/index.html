<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>FAQ on architecture and work VKontakte</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The history of creating VKontakte is in Wikipedia, it was told by Pavel himself. It seems that everyone already knows her. Pavel told about interiors,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>FAQ on architecture and work VKontakte</h1><div class="post__text post__text-html js-mediator-article">  The history of creating VKontakte is in Wikipedia, it was told by Pavel himself.  It seems that everyone already knows her.  Pavel <a href="http://profyclub.ru/docs/103">told</a> about interiors, architecture and device of the site at HighLoad ++ <a href="http://profyclub.ru/docs/103">back in 2010</a> .  A lot of servers have flowed since then, so we will update the information: dissect, pull out the insides, weigh it - look at the VC device from a technical point of view. <br><br><img src="https://habrastorage.org/webt/_x/zc/wp/_xzcwpb5ze_4e-yx_jw_-8nvnei.jpeg"><br><br>  <strong>Alexey Akulovich</strong> ( <a href="https://habr.com/ru/users/atercattus/" class="user_link">AterCattus</a> ) is a backend developer on the VKontakte team.  Deciphering this report is a collective answer to frequently asked questions about the operation of the platform, infrastructure, servers, and interaction between them, but not about the development, namely <strong>about hardware</strong> .  Separately - about the databases and what is in their place at the VC, about collecting logs and monitoring the entire project as a whole.  Details under the cut. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/_GqcriadL-s" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  For over four years I have been involved in all sorts of tasks related to the backend. <br><br><ul><li>  Loading, storage, processing, distribution of media: video, live streaming, audio, photos, documents. </li><li>  Infrastructure, platform, monitoring by the developer, logs, regional caches, CDN, own RPC protocol. </li><li>  Integration with external services: push emails, external links parsing, RSS feed. </li><li>  Helping colleagues on various issues, for the answers to which you have to dive into an unknown code. </li></ul><br>  During this time I had a hand in many components of the site.  I want to share this experience. <br><br><h2>  General architecture </h2><br>  Everything, as usual, begins with a server or group of servers that accept requests. <br><br><h3>  Front server </h3><br>  The front-server accepts requests via HTTPS, RTMP and WSS. <br><br>  <strong>HTTPS</strong> are requests for the main and mobile web versions of the site: vk.com and m.vk.com, and other official and unofficial clients of our API: mobile clients, messengers.  We have <strong>RTMP</strong> traffic for live broadcasts with separate front servers and <strong>WSS</strong> connections for the Streaming API. <br><br>  For HTTPS and WSS on servers is <strong>nginx</strong> .  For RTMP broadcasts, we recently switched to our own solution, <strong>kive</strong> , but it is outside of the report.  For fault tolerance, these servers announce public IP addresses and act as groups so that in case of a problem on one of the servers, user requests are not lost.  For HTTPS and WSS, the same servers encrypt traffic in order to take some of the CPU load on themselves. <br><br>  We will not talk further about WSS and RTMP, but only about standard HTTPS requests, which are usually associated with a web project. <br><br><h3>  Backend </h3><br>  Backend servers are usually behind the front.  They handle requests that the front-server receives from clients. <br><br>  These are the <strong>kPHP servers</strong> on which the HTTP daemon is running, because HTTPS is already decrypted.  kPHP is a server that works on the <strong>prefork-model</strong> : it starts the master process, a bundle of child processes, sends them listening sockets and they process their requests.  In this case, the processes are not restarted between each request from the user, but simply reset their state to the initial zero-value state - request by request, instead of restarting. <br><br><h4>  Load distribution </h4><br>  All our backends are not a huge pool of machines that can handle any request.  We <strong>divide</strong> them <strong>into separate groups</strong> : general, mobile, api, video, staging ... The problem on a separate group of machines will not affect all the others.  In case of problems with the video, the user who listens to music will not even know about the problems.  On what backend to send a request, decides nginx on the front by configuration. <br><br><h4>  Metrics collection and rebalancing </h4><br>  To understand how many cars you need to have in each group, we <strong>do not rely on QPS</strong> .  The backends are different, they have different requests, each request has different complexity of calculating QPS.  Therefore, we use the <strong>concept of server load as a whole - on the CPU and perf</strong> . <br><br>  We have thousands of such servers.  The kPHP group is running on each physical server in order to dispose of all cores (because kPHP is single-threaded). <br><br><h3>  Content Server </h3><br>  <strong>CS or Content Server is a repository</strong> .  CS is a server that stores files and also handles filled files, all kinds of background synchronous tasks that the main web frontend sets to it. <br><br>  We have tens of thousands of physical servers that store files.  Users love to upload files, and we love to store and distribute them.  Some of these servers are closed by special pu / pp servers. <br><br><h3>  pu / pp </h3><br>  If you opened the network tab in VK, you saw pu / pp. <br><br><img src="https://habrastorage.org/webt/fd/am/xo/fdamxolkfxlplnc5h5flbihru3g.png"><br><br>  What is pu / pp?  If we close one server after another, then there are two options for uploading and uploading a file to a server that has been closed: <strong>directly</strong> through <code>http://cs100500.userapi.com/path</code> or <strong>via an intermediate server</strong> - <code>http://pu.vk.com/c100500/path</code> . <br><br>  <strong>Pu is the historical name for photo upload, and pp is photo proxy</strong> .  That is, one server to upload photos, and the other - to give.  Now not only photos are loaded, but the name is preserved. <br><br>  These servers <strong>terminate HTTPS sessions</strong> to remove the CPU load from the storage.  Also, since user files are processed on these servers, the less sensitive information is stored on these machines, the better.  For example, the keys to encrypt HTTPS. <br><br>  Since the machines are closed by our other machines, we can afford not to give them ‚Äúwhite‚Äù external IPs, and <strong>give them ‚Äúgray‚Äù ones</strong> .  So saved on the IP pool and guaranteed to protect the machine from outside access - there is simply no IP to get into it. <br><br>  <strong>Failover through shared IP</strong> .  In terms of fault tolerance, the scheme works the same way - several physical servers have a common physical IP, and the piece of iron in front of them chooses where to send the request.  Later I will talk about other options. <br><br>  The controversial point is that in this case the <strong>client keeps fewer connections</strong> .  If there is the same IP on several machines - with the same host: pu.vk.com or pp.vk.com, the client‚Äôs browser has a limit on the number of simultaneous requests to one host.  But during the ubiquitous HTTP / 2, I believe that this is no longer so relevant. <br><br>  The obvious disadvantage of the scheme is that you have to <strong>pump all the traffic</strong> that goes to the repository through another server.  Since we are pumping traffic through the machines, we cannot yet pump heavy traffic in the same way, for example, video.  We give it directly to it - a separate direct connection for separate storage specifically for video.  Lighter content we pass through proxy. <br><br>  Not so long ago, we had an improved proxy version.  Now I will tell you how they differ from ordinary ones and why it is needed. <br><br><h3>  Sun </h3><br>  In September 2017, Oracle, which had previously bought Sun, <a href="https://habr.com/ru/post/337682/">sacked a huge number of Sun employees</a> .  We can say that at this moment the company ceased to exist.  Choosing a name for the new system, our admins decided to pay homage to the memory of this company, and called the new system Sun.  Between ourselves, we call it simply "suns." <br><br><img src="https://habrastorage.org/webt/d3/6f/0j/d36f0jjqbwlst9mk2-lcncltkq4.png"><br><br>  Pp had a few problems.  <strong>One IP per group is an inefficient cache</strong> .  Several physical servers have a common IP address, and there is no way to control which server the request will come to.  Therefore, if different users come for the same file, then if there is a cache on these servers, the file is deposited in the cache of each server.  This is a very inefficient scheme, but nothing could be done. <br><br>  As a result, <strong>we cannot shard content</strong> , because we cannot select a specific server of this group - they have a common IP.  Also, for some internal reasons, we <strong>did not have the opportunity to install such servers in the regions</strong> .  They stood only in St. Petersburg. <br><br>  With the suns, we changed the system of choice.  Now we have <strong>anycast routing</strong> : dynamic routing, anycast, self-check daemon.  Each server has its own individual IP, but a common subnet.  Everything is set up so that in the event of a single server falling out, traffic is smeared over the other servers of the same group automatically.  Now it is possible to select a specific server, <strong>there is no redundant caching</strong> , and reliability has not suffered. <br><br>  <strong>Weights support</strong> .  Now we can afford to put machines of different capacities as needed, as well as in case of temporary problems, to change the weights of the working ‚Äúsuns‚Äù to reduce the load on them, so that they ‚Äútake a rest‚Äù and start working again. <br><br>  <strong>Sharding by content id</strong> .  The funny thing about sharding is that we usually shard content so that different users follow the same file through the same ‚Äúsun‚Äù so that they have a common cache. <br><br>  We recently launched the application ‚ÄúClover‚Äù.  This is an online quiz in live broadcast where the presenter asks questions and users respond in real time by choosing options.  The application has a chat, where users can pofludit.  <strong>More than 100 thousand people</strong> can be connected to the broadcast simultaneously.  They all write messages that are sent to all participants, along with the message comes another avatar.  If 100 thousand people come for one avatar in one "sun", then it can sometimes roll behind a cloud. <br><br>  In order to withstand the bursts of requests for the same file, it is for some type of content that we turn on a stupid scheme that spreads the files across all the existing ‚Äúsuns‚Äù of the region. <br><br><h4>  Sun inside </h4><br>  Reverse proxy on nginx, cache either in RAM or on fast Optane / NVMe disks.  Example: <code>http://sun4-2.userapi.com/c100500/path</code> - a link to the "sun", which stands in the fourth region, the second server group.  It closes the path file, which physically lies on the server 100500. <br><br><h3>  Cache </h3><br>  In our architectural scheme, we add another node - the caching environment. <br><br><img src="https://habrastorage.org/webt/jp/4p/xv/jp4pxvydhstms-z9bpbmpjy0htk.png"><br><br>  Below is the layout of the <strong>regional caches</strong> , there are about 20 of them.  These are the places where exactly caches and ‚Äúsuns‚Äù stand, which can cache traffic through themselves. <br><br><img src="https://habrastorage.org/webt/yh/9i/qk/yh9iqkv3dyqd3uox9wgd2cmgqba.png"><br><br>  This is caching of multimedia content, user data is not stored here - just music, video, photos. <br><br>  To determine the region of the user, we <strong>collect the BGP prefixes of networks announced in the regions</strong> .  In the case of a fallback, we still have geoip parsing if we could not find the IP by the prefixes.  <strong>By user IP define region</strong> .  In the code, we can look at one or several regions of the user - those points to which it is closest geographically. <br><br><h4>  How it works? </h4><br>  <strong>We consider the popularity of files by region</strong> .  There is a regional cache number where the user is located, and the file identifier - take this pair and increment the rating with each download. <br><br>  In this case, the demons - services in the regions - from time to time come to the API and say: "I am such a cache, give me a list of the most popular files of my region that I don‚Äôt have yet."  The API gives a bunch of files, sorted by rating, the demon downloads them, takes them to the regions, and from there sends the files.  This is the fundamental difference between pu / pp and Sun from caches: they give the file through themselves immediately, even if there is no file in the cache, and the cache first downloads the file to itself, and then starts to give it away. <br><br>  At the same time, we get the <strong>content closer to the users</strong> and the spreading of the network load.  For example, only from the Moscow cache, we distribute more than 1 Tbit / s during the busy hours. <br><br>  But there are problems - <strong>cache servers are not rubber</strong> .  For super popular content, sometimes there is not enough network to a separate server.  Cache servers are 40-50 Gbit / s, but there is content that clogs such a channel completely.  We are moving towards storing more than one copy of popular files in the region.  I hope that by the end of the year we will implement it. <br><br>  We reviewed the overall architecture. <br><br><ul><li>  Front servers that accept requests. </li><li>  Backends that process requests. </li><li>  Stores that are closed by two types of proxy. </li><li>  Regional caches. </li></ul><br>  What is missing from this scheme?  Of course, the databases in which we store data. <br><br><h2>  Databases or engines </h2><br>  We call them not databases, but engines - Engines, because we have practically no databases in the conventional sense. <br><br><img src="https://habrastorage.org/webt/n6/zm/lj/n6zmlj5pwxsnqoqp0xgfhxza_ic.png"><br><br>  <strong>This is a necessary measure</strong> .  This happened because in 2008-2009, when VK had an explosive growth in popularity, the project worked completely on MySQL and Memcache and there were problems.  MySQL liked to fall and mess up the files, after which it did not rise, and Memcache gradually degraded in performance, and had to restart it. <br><br>  It turns out that in the increasingly popular project there was a persistent storage that corrupts data, and a cache that slows down.  In such conditions it is difficult to develop a growing project.  It was decided to try to rewrite the critical things in which the project rested on their own bikes. <br><br>  <strong>The decision was successful</strong> .  The ability to do this was, like the extreme necessity, because at that time there were no other ways to scale.  There were no heaps of databases, NoSQL did not exist yet, there were only MySQL, Memcache, PostrgreSQL - and that‚Äôs all. <br><br>  <strong>Universal operation</strong> .  The development was led by our team of C-developers and everything was done in a uniform way.  Regardless of the engine, everywhere there was approximately the same format of files written to disk, the same launch parameters, the signals were processed the same way and they behaved approximately the same in the case of edge situations and problems.  With the growth of engines, it is convenient for admins to exploit the system - there is no zoo to support, and to re-learn to exploit every new third-party base, which made it possible to quickly and conveniently increase their number. <br><br><h3>  Types of engines </h3><br>  The team wrote quite a lot of engines.  Here are just a few of them: friend, hints, image, ipdb, letters, lists, logs, memcached, meowdb, news, nostradamus, photo, playlists, pmemcached, sandbox, search, storage, likes, tasks, ... <br><br>  For each task that requires a specific data structure or processes atypical requests, the C-team writes a new engine.  Why not. <br><br>  We have a separate <strong>memcached</strong> engine, which is similar to the usual one, but with a bunch of buns, and which does not slow down.  Not ClickHouse, but it works too.  There is a separate <strong>pmemcached</strong> - this is a <strong>persistent memcached</strong> , which can also store data on the disk, and more than it gets into the RAM so as not to lose data when it is restarted.  There are a variety of engines for individual tasks: queues, lists, sets - all that is required by our project. <br><br><h3>  Clusters </h3><br>  From the point of view of the code, there is no need to imagine the engines or databases as some processes, entities or instances.  The code works with clusters, with groups of engines - <strong>one type per cluster</strong> .  Suppose there is a cluster of memcached - this is just a group of machines. <br><br><blockquote>  The code does not need to know the physical location, size and number of servers.  He goes to the cluster by some kind of identifier. </blockquote><br>  For this to work, you need to add another entity, which is located between the code and the engines - <strong>proxy</strong> . <br><br><h3>  RPC-proxy </h3><br>  Proxy is a <strong>connecting tire</strong> on which almost the entire site operates.  At the same time, we <strong>have no service discovery</strong> - instead there is a config for this proxy, which knows the location of all the clusters and all the shards of this cluster.  This is done by admins. <br><br>  Programmers do not care how much, where and what they cost - they just go to the cluster.  It allows us a lot.  When a request is received, the proxy forwards the request, knowing where it is - it determines this itself. <br><br><img src="https://habrastorage.org/webt/7k/pf/ia/7kpfiagxzy2a4mrosc_f4otqnw8.png"><br><br>  At the same time proxy is a point of protection against service failure.  If any engine slows down or falls, then the proxy understands this and responds accordingly to the client side.  This allows you to remove the timeout - the code does not wait for the response of the engine, and understands that it does not work and you need to somehow behave differently.  The code must be ready for the fact that databases do not always work. <br><br><h4>  Specific implementations </h4><br>  Sometimes we still really want to have some kind of non-standard solution as an engine.  It was decided not to use our ready-made rpc-proxy, created specifically for our engines, but to make a separate proxy for the task. <br><br>  For MySQL, which we still have in some places we use db-proxy, and for ClickHouse - <strong>Kittenhouse</strong> . <br><br>  This works in general.  There is a certain server, it runs kPHP, Go, Python - in general, any code that can walk on our RPC protocol.  The code runs locally on RPC-proxy - on each server where there is code, its own local proxy is running.  Upon request, the proxy understands where to go. <br><br><img src="https://habrastorage.org/webt/f-/dx/ro/f-dxrox3o97ckejzygz8mgf4tcs.png"><br><br>  If one engine wants to go to another, even if it is a neighbor, it goes through a proxy, because a neighbor can stand in another data center.  The engine should not be tied to the knowledge of the location of anything other than itself - we have this standard solution.  But of course there are exceptions :) <br><br>  An example of a TL scheme, according to which all engines work. <br><br><pre> <code class="plaintext hljs">memcache.not_found = memcache.Value; memcache.strvalue value:string flags:int = memcache.Value; memcache.addOrIncr key:string flags:int delay:int value:long = memcache.Value; tasks.task fields_mask:# flags:int tag:%(Vector int) data:string id:fields_mask.0?long retries:fields_mask.1?int scheduled_time:fields_mask.2?int deadline:fields_mask.3?int = tasks.Task; tasks.addTask type_name:string queue_id:%(Vector int) task:%tasks.Task = Long;</code> </pre> <br>  This is a binary protocol, the closest analogue of which is <strong>protobuf.</strong>  The scheme describes in advance optional fields, complex types ‚Äî extensions of built-in scalars, and queries.  Everything works according to this protocol. <br><br><h4>  RPC over TL over TCP / UDP ... UDP? </h4><br>  We have an RPC protocol for executing queries of the engine that runs on top of a TL scheme.  This all works on top of a TCP / UDP connection.  TCP is understandable, but why do we often ask UDP. <br><br>  UDP helps to <strong>avoid the problem of a huge number of connections between servers</strong> .  If on each server there is an RPC-proxy and it, in general, can go to any engine, then tens of thousands of TCP connections to the server are obtained.  There is a load, but it is useless.  In the case of UDP, this problem does not exist. <br><br>  <strong>No excessive TCP-handshake</strong> .  This is a typical problem: when a new engine or a new server rises, many TCP connections are established at once.  For small lightweight requests, for example, UDP payload, all communication with the engine is <strong>two UDP packets:</strong> one flies in one direction, the other in the other.  One round trip - and the code received a response from the engine without a handshake. <br><br>  Yes, this all works only <strong>with a very small percentage of packet loss</strong> .  The protocol has support for retransmitters, timeouts, but if we lose a lot, we get almost TCP, which is not profitable.  Over the oceans UDP do not drive. <br><br>  We have thousands of such servers, and there is the same scheme: a pack of engines is put on each physical server.  Basically, they are single-threaded in order to work as quickly as possible without locks, and are shaded as single-threaded solutions.  At the same time, we have nothing more reliable than these engines, and a lot of attention is paid to persistent data storage. <br><br><h3>  Persistent storage </h3><br>  <strong>Engines write binlogs</strong> .  Binlog is a file at the end of which an event is added to a state or data change.  In different solutions it is called differently: binary log, <a href="https://en.wikipedia.org/wiki/Write-ahead_logging">WAL</a> , <a href="https://redis.io/topics/persistence">AOF</a> , but the principle is the same. <br><br>  So that the engine does not re-read the entire binlog during many years during the restart, the engines write <strong>snapshots - the current state</strong> .  If necessary, they read first from it, and then read it out from binlog.  All binlog are written in the same binary format - according to the TL scheme, so that admins can administer them in the same way with their tools.  There is no need for snapshots.  There is a general header that indicates whose snapshot is an int, the magic of the engine, and which body does not matter to anyone.  This is the problem of the engine that recorded the snapshot. <br><br>  I will briefly describe the principle of work.  There is a server running the engine.  He opens a new empty binlog post, writes an event of change to it. <br><br><img src="https://habrastorage.org/webt/dd/w9/9p/ddw99p7g6upg9hci9ou6aln6d_c.png"><br><br>  At some point, he either decides to take a snapshot, or a signal comes to him.  The server creates a new file, writes its entire state to it, appends the current binlog size - offset to the end of the file, and continues to write further.  New binlog is not created. <br><br><img src="https://habrastorage.org/webt/ec/fq/yt/ecfqytibh2tsm5ncd8mfli-b1ta.png"><br><br>  At some point, when the engine has restarted, there will be a binlog and snapshot on the disk.  The engine reads completely snapshot, raises its state at a certain point. <br><br><img src="https://habrastorage.org/webt/bg/ph/-u/bgph-uu68nqedhby4a2kf3r9c5u.png"><br><br>  Subtracts the position that was at the time of the snapshot creation, and the binlog size. <br><br><img src="https://habrastorage.org/webt/ar/gs/lq/argslqv8ewosmtic8-zaobq4g5o.png"><br><br>  Reads the end of binlog to get the current state and continues to write further events.  This is a simple scheme, all our engines work on it. <br><br><h4>  Data replication </h4><br>  As a result, the data is replicated to us from <strong>statement-based</strong> ‚Äî we do not write to the binlog any changes to the pages, but <strong>requests for changes</strong> .  Very similar to what comes over the network, only slightly modified. <br><br>  The same scheme is used not just for replication, but also <strong>for creating backups</strong> .  We have a writing engine that writes to binlog.  In any other place where admins are configured, the copy of this binlog is being raised, and that‚Äôs all - we have a backup. <br><br><img src="https://habrastorage.org/webt/og/al/sz/ogalszm0wfe3f_064sbjnpo4p9c.png"><br><br>  If you need a <strong>reading replica</strong> to reduce the load on reading on the CPU, the reading engine simply rises, which finishes the end of the binlog and executes these commands in itself locally. <br><br>  The gap here is very small, and there is an opportunity to find out how much the replica is behind the master. <br><br><h3>  Sharding data in RPC-proxy </h3><br>  How does sharding work?  How does the proxy understand which cluster shard to send to?  The code does not say: ‚ÄúSend to shard 15!‚Äù - no, this is done by the proxy. <br><br>  <strong>The simplest scheme is firstint</strong> - the first number in the query. <br><br> <code>get(photo100_500) =&gt; 100 % N.</code> <br> <br>  This is an example for a simple memcached text protocol, but, of course, requests are complex, structured.  In the example, the first number in the request and the remainder of the division by the cluster size is taken. <br><br>  This is useful when we want to have the locality of data of one entity.  Suppose 100 is a user or group ID, and we want all complex data to be on the same shard for complex queries. <br><br>  If we do not care how requests are spread over a cluster, there is another option - <strong>hashing the shard entirely</strong> . <br><br> <code>hash(photo100_500) =&gt; 3539886280 % N</code> <br> <br>  We also get a hash, the remainder of the division and the number of the shard. <br><br>  Both of these options only work if we are prepared for the fact that when we increase the size of a cluster, we will split it up or increase it multiple times.  For example, we had 16 shards, we do not have enough, we want more - you can safely get 32 ‚Äã‚Äãwithout downtime.  If we want to increase more than once - it will be downtime, because it will not be possible to pererobit accurately all without loss.  These options are useful, but not always. <br><br>  If we need to add or remove an arbitrary number of servers, we use <strong>consistent hashing on the a la Ketama ring</strong> .  But at the same time, we completely lose the locality of the data, we have to do a merge request to the cluster, so that each piece will return its small response, and already combine the answers to the proxy. <br><br>  There are super specific requests.   : RPC-proxy  , ,       .     , ,     ,      .    proxy. <br><br><img src="https://habrastorage.org/webt/jx/6t/f9/jx6tf9jlkkmva1qfifzmwrx58wc.png"><br><br><h2>  </h2><br>     .     ‚Äî <strong>   memcache</strong> . <br><br> <code>ring-buffer: prefix.idx = line</code> <br> <br>    ‚Äî  , ,      ‚Äî  .     0     1.   memcache ‚Äî       .        . <br><br>    ,   <strong>Multi Get</strong>  ,   ,         .  ,   -      ,   ,         ,      . <br><br>         <strong>logs-engine</strong> .      ,       .       600   . <br><br>   ,  ,    6‚Äì7 .    ,    , ,    ClickHouse   . <br><br><h3>    ClickHouse </h3><br>   ,      . <br><br><img src="https://habrastorage.org/webt/jm/-j/s0/jm-js04tjh8lb8pii1_dzl_sfa4.png"><br><br>  ,   RPC    RPC-proxy,   ,    .       ClickHouse,        : <br><br><ul><li>  -   ClickHouse; </li><li>  RPC-proxy,      ClickHouse,  - ,  ,   RPC. </li></ul><br>    ‚Äî          ClickHouse. <br><br>     ClickHouse,   <strong>KittenHouse</strong> .      KittenHouse  ClickHouse ‚Äî   .   ,  HTTP-     .   ,    ClickHouse <strong>  reverse proxy</strong> ,   ,     .         . <br><br><img src="https://habrastorage.org/webt/zj/fy/5y/zjfy5yuay9-6wqe3nrgjkeznvny.png"><br><br>      RPC-   , ,  nginx.   KittenHouse      UDP. <br><br><img src="https://habrastorage.org/webt/hq/wl/v_/hqwlv_vnujb-maxakxksbmrf6xo.png"><br><br>         ,    UDP-      .       RPC     ,      UDP.      . <br><br><h2>  Monitoring </h2><br>     : ,        ,     .     : <strong>  </strong> . <br><br><h3>   </h3><br>       <a href="https://my-netdata.io/">Netdata</a> ,        <strong>Graphite Carbon</strong> .      ClickHouse,   Whisper, .       ClickHouse,   <strong>Grafana</strong>  ,   .  ,   Netdata  Grafana  . <br><br><h3>   </h3><br>      . ,    ,    Counts, UniqueCounts   ,   - . <br><br><pre> <code class="plaintext hljs">statlogsCountEvent ( 'stat_name', $key1, $key2, ‚Ä¶) statlogsUniqueCount ( 'stat_name', $uid, $key1, $key2, ‚Ä¶) statlogsValuetEvent ( 'stat_name', $value, $key1, $key2, ‚Ä¶) $stats = statlogsStatData($params)</code> </pre><br>      ,    ,     ‚Äî  ,  Wathdogs. <br><br>    <strong> ,</strong>    600   1   .       <strong>   </strong> ,     .     ‚Äî  ,     . ,      . <br><br>    ,     <strong>  memcache</strong> ,    .         <strong>stats-daemon</strong>   .         <strong>logs-collectors</strong> ,       ,      . <br><br><img src="https://habrastorage.org/webt/ih/ab/oy/ihaboy4luh5hriorej9seodbx6u.png"><br><br>        logs-collectors. <br><br><img src="https://habrastorage.org/webt/fq/ta/bj/fqtabjgq556wqfdz5_kfq3mj94c.png"><br><br>          stas-daemom ‚Äî   ,      collector.  ,    -        memcache stats-daemon,   ,    . <br><br>  logs-collectors    <strong>meowDB</strong> ‚Äî   ,      . <br><br><img src="https://habrastorage.org/webt/v_/gb/_y/v_gb_ya-9ywkra7xdh5h_qtqsc4.png"><br><br>      ¬´-SQL¬ª  . <br><br><img src="https://habrastorage.org/webt/1q/gw/wp/1qgwwpyj3ewcwuonshvty_zcfhc.png"><br><br><h3>  Experiment </h3><br>  2018     ,          -,      ClickHouse.      ClickHouse ‚Äî    ? <br><br><img src="https://habrastorage.org/webt/wg/mz/kl/wgmzklw41x7ilj0-5hbr_kfdif8.png"><br><br>    ,     KittenHouse. <br><br><img src="https://habrastorage.org/webt/kq/s7/uj/kqs7ujzbhnqzt5f8djepldmwxia.png"><br><br>   <strong>     ¬´*House¬ª</strong> ,        ,       UDP.   *House    inserts,  ,   KittenHouse.        ClickHouse,     . <br><br><img src="https://habrastorage.org/webt/ff/k3/th/ffk3thypln9exuhyuhr-nuj9hr4.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><img src="https://habrastorage.org/webt/r4/g3/e9/r4g3e9yakpzbx5gscmgyl6keqsa.png"><br><br>   memcache, stats-daemon  logs-collectors    . <br><br><ul><li>     ,     StatsHouse. </li><li> StatsHouse   KittenHouse UDP-,    SQL-inserts, . </li><li> KittenHouse    ClickHouse. </li><li>     ,      StatsHouse ‚Äî   ClickHouse  SQL. </li></ul><br>    <strong></strong> ,   ,  .    , , ,    .     . <br><br>  <strong>  </strong> .   ,    stats-daemons  logs-collectors,  ClickHouse   ,  ,     . <strong>  ,       </strong> . <br><br><h2>  </h2><br>     PHP.    <strong>git</strong> :  <strong>GitLab</strong>  <strong>TeamCity</strong>  .     -,       ,   ‚Äî  . <br><br>        ,     diff  ‚Äî : , , .     binlog   copyfast,          .     ,  <strong>gossip replication</strong> ,       ,  ‚Äî  ,   .            .      ,       <strong>  </strong> .       . <br><br>     kPHP         <strong>git</strong>   .    <strong> HTTP-</strong> ,      diff ‚Äî     .     ‚Äî    <strong>binlog copyfast</strong> .     ,      .  <strong>  </strong> .  copyfast' ,   binlog   ,     gossip replication     ,    -,      .   <strong>graceful </strong>   . <br><br>   ,     ,   : <br><br><ul><li> git master branch; </li><li>   <strong>.deb</strong> ; </li><li>    binlog copyfast; </li><li>   ; </li><li>     .dep; </li><li> <strong>dpkg -i</strong> ; </li><li> graceful    . </li></ul><br>   ,        <strong>.deb</strong> ,     <strong>dpkg -i</strong>   .    kPHP  ,   ‚Äî dpkg?  .  ‚Äî  . <br><br> <b> :</b> <br><br><ul><li>    <a href="https://www.youtube.com/watch%3Fv%3DPV2xpLB6KG0">¬´  Vkontakte. ?¬ª</a>    copyfast  gossip. </li><li>    <a href="https://habr.com/ru/company/vk/blog/430168/">¬´ VK    CLickHouse    ¬ª</a> . </li><li>   <a href="https://habr.com/ru/post/322562/">¬´     ¬ª</a> ,     ,   . </li></ul><br><blockquote>     ,       <a href="https://phprussia.ru/2019">PHP Russia</a>  17          PHP-. ,     ,  <a href="https://phprussia.ru/2019/abstracts"></a> (     PHP!) ‚Äî ,      PHP,   . </blockquote></div><p>Source: <a href="https://habr.com/ru/post/449254/">https://habr.com/ru/post/449254/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../44924/index.html">Human possibilities are endless</a></li>
<li><a href="../449240/index.html">The story of one young service Daida (art by subscription)</a></li>
<li><a href="../449244/index.html">Medium - the first decentralized Internet provider in Russia</a></li>
<li><a href="../449246/index.html">AX200 - Intel's Wi-Fi 6</a></li>
<li><a href="../449252/index.html">Zombie projects - merge user data even after his death</a></li>
<li><a href="../449256/index.html">I read 80 resumes, I have questions</a></li>
<li><a href="../449260/index.html">What is Automated Machine Learning (AutoML)</a></li>
<li><a href="../449262/index.html">Latest IRM - Siebel upgrade to IP17 +</a></li>
<li><a href="../449264/index.html">Creating a reporting system for 1C: ERP based on OLAP and Excel</a></li>
<li><a href="../449266/index.html">3 reports with RusCrypto: conferences with experience</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>