<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An overview of clustering algorithms for numeric data spaces</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The clustering task is a special case of a learning task without a teacher, which is reduced to splitting the existing set of data objects into subset...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An overview of clustering algorithms for numeric data spaces</h1><div class="post__text post__text-html js-mediator-article">  The clustering task is a special case of a learning task without a teacher, which is reduced to splitting the existing set of data objects into subsets in such a way that the elements of one subset differed significantly in some set of properties from the elements of all other subsets.  A data object is usually considered as a point in a multidimensional metric space, each dimension of which corresponds to a certain property (attribute) of the object, and the metric is a function of the values ‚Äã‚Äãof these properties.  The choice of the data clustering algorithm and the metric used depends on the types of measurements of this space, which can be both numerical and categorical.  This choice is dictated by differences in the nature of different types of attributes. <br><br>  This article provides a brief overview of clustering methods for numeric data spaces.  It will be useful to those who are just beginning to study Data Mining and cluster analysis and will help orient themselves in the diversity of modern clustering algorithms and get a general idea about them.  The article does not claim to be a complete presentation of the material; on the contrary, the description of the algorithms in it is as simple as possible.  For a more detailed study of an algorithm, it is recommended to use the scientific work in which it was presented (see the list of references at the end of the article). <br><a name="habracut"></a><br><br><h5>  Partition methods </h5><br>  The most famous representatives of this family of methods are the k-means [1] and k-medoids [2] algorithms.  They take the input parameter <i>k</i> and divide the data space into <i>k</i> clusters such that the similarity between objects of one cluster is maximum, and between objects of different clusters is minimal.  The similarity is measured in relation to a certain cluster center as the distance from the object under consideration to the center.  The main difference between these methods lies in the way the cluster center is defined. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the k-means algorithm, the similarity is considered with respect to the cluster center of mass, the average value of the coordinates of cluster objects in the data space.  First, <i>k</i> objects are selected at random, each of which is a cluster prototype and represents its center of mass.  Then for each of the remaining objects is attached to the cluster with which the similarity is greater.  After that, the center of mass of each cluster is recalculated.  For each partition obtained, a certain evaluation function is calculated, the values ‚Äã‚Äãof which at each step form a converging series.  The process continues until the specified series converges to its limit value.  In other words, moving objects from a cluster to a cluster ends when, with each iteration, the clusters remain unchanged.  Minimizing the evaluation function allows the resulting clusters to be as compact and separate as possible.  The k-means method works well when clusters are compact ‚Äúclouds‚Äù that are significantly separated from each other.  It is effective for processing large amounts of data, but is not applicable for detecting clusters of non-convex shape or a very different size.  Moreover, the method is very sensitive to noise and isolated points of space, since even a small number of such points can significantly affect the calculation of the center of mass of the cluster. <br><br>  To reduce the influence of noise and discrete points of space on the clustering result, the k-medoids algorithm, in contrast to k-means, uses not the center of mass to represent the center of the cluster, but a representative object ‚Äî one of the cluster objects.  As in the k-means method, <i>k</i> representative objects are selected at random.  Each of the remaining objects is combined into a cluster with the nearest representative object.  Then it is iteratively replaced for each representative object by replacing it with an arbitrary unrepresentative data space object.  The replacement process continues until the quality of the resulting clusters improves.  The quality of clustering is determined by the sum of deviations between each object and the representative object of the corresponding cluster, which the method seeks to minimize. Thus, the iterations continue until its representative object in each cluster becomes the medoid - the object closest to the cluster center.  The algorithm is poorly scalable for processing large amounts of data, but this problem is solved by the CLARANS algorithm that complements the k-medoids method [3].  For clustering multidimensional spaces based on CLARANS, a PROCLUS algorithm was built [4]. <br><br><h5>  Hierarchical methods </h5><br>  The general idea of ‚Äã‚Äãthe methods of this group is a consistent hierarchical decomposition of a set of objects.  Depending on the direction of building the hierarchy, the divisional and agglomerative methods are distinguished.  In the case of the agglomerative method (bottom-up), the decomposition process begins with the fact that each object is an independent cluster.  Then at each iteration, pairs of nearby clusters are successively combined into a single cluster.  Iterations continue until all objects are merged into a single cluster or until a certain stopping condition is fulfilled.  The divisional method (from top to bottom), on the contrary, implies that at the initial stage all objects are combined into a single cluster.  At each iteration, it is divided into smaller ones until each object is in a separate cluster or the stop condition is satisfied.  As a stop condition, you can use the threshold number of clusters that you want to get, but usually the threshold value of the distance between the clusters is used. <br><br>  The main problem of hierarchical methods lies in the difficulty of determining the condition of a stop in such a way as to isolate ‚Äúnatural‚Äù clusters and at the same time prevent their splitting.  Another problem with hierarchical clustering methods is choosing the point of separation or merging of clusters.  This choice is critical, because after splitting or merging clusters at each subsequent step, the method will operate only on newly formed clusters, therefore, the wrong choice of a merge or split point at any step can lead to poor-quality clustering.  In addition, hierarchical methods cannot be applied to large data sets, because deciding whether to divide or merge clusters requires analyzing a large number of objects and clusters, which leads to a large computational complexity of the method.  Examples of algorithms based on the hierarchical method are BIRCH [5] and CHAMELEON [6]. <br><br><h5>  Density methods </h5><br>  Clusters are considered as regions of data space with a high density of objects, which are separated by regions with a low density of objects. <br><br>  The DBSCAN algorithm [7] is one of the first clustering algorithms by the density method.  This algorithm is based on several definitions: <br><br><ul><li>  <i>The Œµ-neighborhood of an</i> object is called a neighborhood of the radius <i>Œµ of</i> some object. </li><li>  <i>A root object</i> is an object whose <i>Œµ-</i> neighborhood contains at least some minimum number of <i>MinPts</i> objects. </li><li>  <i>An object p is directly densely accessible</i> from an object <i>q</i> if <i>p</i> is in an <i>Œµ-</i> neighborhood of <i>q</i> and <i>q</i> is a root object. </li><li>  <i>An object p is densely reachable</i> from an object <i>q</i> for given <i>Œµ</i> and <i>MinPts</i> , if there is a sequence of objects <i>p <sub>1</sub> , ..., p <sub>n</sub></i> , where <i>p <sub>1</sub> = q</i> and <i>p <sub>n</sub> = p</i> , such that <i>p <sub>i</sub> +1 is</i> directly reachable from <i>p <sub>i</sub></i> , <i>1 ‚â§ i ‚â§ n</i> . </li><li>  <i>An object p is tightly connected</i> to an object <i>q</i> for given <i>Œµ</i> and <i>MinPts</i> , if there exists an object <i>o</i> such that <i>p</i> and <i>q are</i> tightly reachable from <i>o</i> . </li></ul><br>  To search for clusters, the DBSCAN algorithm checks the Œµ-neighborhood of each object.  If the <i>Œµ-</i> neighborhood of the object <i>p</i> contains more points than <i>MinPts</i> , then a new cluster is created with the root object <i>p</i> .  DBSCAN then iteratively collects objects directly densely achievable from root objects, which can lead to the union of several densely achievable clusters.  The process ends when no new objects can be added to any cluster. <br><br>  Although, in contrast to the partitioning methods, DBSCAN does not require an indication of the number of clusters obtained in advance, it requires an indication of the values ‚Äã‚Äãof the parameters <i>Œµ</i> and <i>MinPts</i> , which directly affect the result of clustering.  The optimal values ‚Äã‚Äãof these parameters are difficult to determine, especially for multidimensional data spaces.  In addition, the distribution of data in such spaces is often asymmetrical, which makes it impossible to use global density parameters for their clustering.  For clustering multidimensional data spaces based on DBSCAN, the SUBCLU algorithm was created [8]. <br><br><h5>  Network methods </h5><br>  The general idea of ‚Äã‚Äãthe methods is that the object space is divided into a finite number of cells that form a network structure, within which all clustering operations are performed.  The main advantage of the methods of this group is in small execution time, which usually does not depend on the number of data objects, but depends only on the number of cells in each space dimension. <br><br>  The CLIQUE algorithm [9], adapted for clustering high-dimensional data, is one of the classic network algorithms.  The method is based on the assumption that if in a multidimensional data space the distribution of objects is not uniform - there are density and rarefaction regions, then the projection of a density region into a subspace with a lower dimension will be part of the density region in this subspace.  The CLIQUE algorithm clusterizes a multidimensional data space in the following way: the data space is divided into non-intersecting fixed-size cells, among them dense cells are identified - those whose density of data objects exceeds a given threshold value.  Further, from the found cells, a space is formed in which dense cells of higher dimension can exist.  The process begins with one-dimensional spaces (the described procedure is performed for each dimension), followed by a transition to higher-dimensional subspaces. <br><br>  This algorithm is scalable to process large amounts of data, however, with a large number of measurements, the number of considered combinations grows nonlinearly, therefore, heuristics are required to reduce the number of considered combinations.  In addition, the result obtained depends very much on the choice of the cell size and the threshold value of the density of objects in the cell.  This is a big problem, since the same values ‚Äã‚Äãof these parameters are used when considering all combinations of measurements.  This problem is solved by the MAFIA algorithm [10], which operates according to a similar principle, but uses the adaptive cell size when splitting subspaces. <br><br><h5>  Model methods </h5><br>  The methods of this family assume that there is a certain mathematical model of the cluster in the data space and seek to maximize the similarity of this model and the available data.  Often this uses the apparatus of mathematical statistics. <br><br>  The EM algorithm [11] is based on the assumption that the set of data under study can be modeled using a linear combination of multidimensional normal distributions.  Its purpose is to estimate distribution parameters that maximize the likelihood function used as a measure of model quality.  In other words, it is assumed that the data in each cluster obey a certain distribution law, namely, the normal distribution.  With this assumption, it is possible to determine the optimal parameters of the distribution law ‚Äî the expectation and variance at which the likelihood function is maximal.  Thus, we assume that any object belongs to all clusters, but with different probability.  Then the task will be to ‚Äúfit‚Äù the set of distributions to the data, and then to determine the probabilities of the object belonging to each cluster.  Obviously, the object should be assigned to the cluster for which this probability is higher. <br><br>  The EM algorithm is simple and easy to implement, is not sensitive to isolated objects and quickly converges with successful initialization.  However, it requires for initialization to indicate the number of clusters k, which implies a priori knowledge of the data.  In addition, if the initialization failed, the convergence of the algorithm may be slow or a poor-quality result may be obtained. <br>  It is obvious that such algorithms are not applicable to spaces with high dimensionality, since in this case it is extremely difficult to assume a mathematical model for the distribution of data in this space. <br><br><h5>  Conceptual clustering </h5><br>  Unlike traditional clustering, which detects groups of similar objects based on a measure of similarity between them, conceptual clustering defines clusters as groups of objects belonging to the same class or concept ‚Äî a specific set of attribute-value pairs. <br><br>  The COBWEB algorithm [12] is a classic incremental conceptual clustering method.  It creates hierarchical clustering in the form of a classification tree: each node of this tree refers to a concept and contains a probabilistic description of this concept, which includes the probability that the concept belongs to this node and conditional probabilities of the form: <i>P (A <sub>i</sub> = v <sub>ij</sub> | C <sub>k</sub> ),</i> where <i>A <sub>i</sub> = v <sub>ij</sub></i> is an attribute-value pair, <i>C <sub>k</sub></i> is a concept class. <br>  Nodes that are at a certain level of a classification tree are called a slice.  The algorithm uses to construct a classification tree a heuristic evaluation measure, called a category utility ‚Äî an increase in the expected number of correct assumptions about attribute values ‚Äã‚Äãwith knowledge of their belonging to a certain category relative to the expected number of correct assumptions about attribute values ‚Äã‚Äãwithout this knowledge.  To embed a new object in the classification tree, the COBWEB algorithm iteratively goes through the entire tree in search of the ‚Äúbest‚Äù node to which this object belongs.  The node is selected on the basis of placing the object in each node and calculating the utility category of the resulting slice.  The category utility is also calculated for the case when the object belongs to the newly created node.  As a result, the object refers to the node for which the utility category is greater. <br>  However, COBWEB has a number of limitations.  First, it assumes that the probability distribution of the values ‚Äã‚Äãof different attributes is statistically independent of each other.  However, this assumption is not always true, because there is often a correlation between the values ‚Äã‚Äãof the attributes.  Secondly, the probabilistic representation of clusters makes it very difficult to update them, especially in the case when the attributes have a large number of possible values.  This is because the complexity of the algorithm depends not only on the number of attributes, but also on the number of their possible values. <br><br><h5>  Bibliography </h5><br><ol><li>  <a href="http://www-m9.ma.tum.de/foswiki/pub/WS2010/CombOptSem/kMeans.pdf">MacQueen, J. Multicariate observations / J. MacQueen // In Proc.</a>  <a href="http://www-m9.ma.tum.de/foswiki/pub/WS2010/CombOptSem/kMeans.pdf">5th Berkeley Symp.</a>  <a href="http://www-m9.ma.tum.de/foswiki/pub/WS2010/CombOptSem/kMeans.pdf">On Math.</a>  <a href="http://www-m9.ma.tum.de/foswiki/pub/WS2010/CombOptSem/kMeans.pdf">Statistics and Probability, 1967. -C.281-297.</a> </li><li>  Kaufman, PJ Rousseeuw, Y. Dodge, 1987. -C.405-416. Kaufman, L. Clustering. </li><li>  <a href="http://www.vldb.org/conf/1994/P144.PDF">Ng, RT Efficient and Effective Clustering Methods for Spatial Data Mining / RT Ng, J. Han // Proc.</a>  <a href="http://www.vldb.org/conf/1994/P144.PDF">20th int.</a>  <a href="http://www.vldb.org/conf/1994/P144.PDF">Conf.</a>  <a href="http://www.vldb.org/conf/1994/P144.PDF">on Very Large Data Bases.</a>  <a href="http://www.vldb.org/conf/1994/P144.PDF">Morgan Kaufmann Publishers, San Francisco, CA, 1994. -C.144-155.</a> </li><li>  <a href="http://dbs.informatik.uni-halle.de/Lehre/Aktiv_2000/Paper/aggarwal_sigmod99.pdf">Aggarwal, CC Fast Algorithms for Projected Clustering / CC Aggarwal, C. Procopiuc // In Proc.</a>  <a href="http://dbs.informatik.uni-halle.de/Lehre/Aktiv_2000/Paper/aggarwal_sigmod99.pdf">ACM SIGMOD Int.</a>  <a href="http://dbs.informatik.uni-halle.de/Lehre/Aktiv_2000/Paper/aggarwal_sigmod99.pdf">Conf.</a>  <a href="http://dbs.informatik.uni-halle.de/Lehre/Aktiv_2000/Paper/aggarwal_sigmod99.pdf">on Management of Data, Philadelphia, PA, 1999. 12 p.</a> </li><li>  <a href="http://idb.csie.ncku.edu.tw/tsengsm/COURSE/DM/Paper/BIRCH.PDF">Zhang, T. BIRCH: An Efficient Data Clustering Method for Very Large Databases / T. Zhang, R. Ramakrishnan, M. Linvy // In Proc.</a>  <a href="http://idb.csie.ncku.edu.tw/tsengsm/COURSE/DM/Paper/BIRCH.PDF">ACM SIGMOD Int.</a>  <a href="http://idb.csie.ncku.edu.tw/tsengsm/COURSE/DM/Paper/BIRCH.PDF">Conf.</a>  <a href="http://idb.csie.ncku.edu.tw/tsengsm/COURSE/DM/Paper/BIRCH.PDF">on Management of Data.</a>  <a href="http://idb.csie.ncku.edu.tw/tsengsm/COURSE/DM/Paper/BIRCH.PDF">ACM Press, New York, 1996. -C.103-114.</a> </li><li>  <a href="http://www-users.cs.umn.edu/~han/dmclass/chameleon.pdf">Karypis, G. CHAMELEON: A Hierarchical Clustering Algorithm Using Dynamic Modeling / G. Karypis, E.-H.</a>  <a href="http://www-users.cs.umn.edu/~han/dmclass/chameleon.pdf">Han, V. Kumar // Journal Computer Volume 32 Issue 8. IEEE Computer Society Press Los Alamitos, CA, 1999. ‚ÄìC.68-75</a> </li><li>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">Ester, M. A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise / M. Ester, H.-P.</a>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">Kriegel, J. Sander, X. Xu // In Proc.</a>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">ACM SIGMOD Int.</a>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">Conf.</a>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">on Management of Data, Portland, OR, 1996. ‚ÄìC.</a>  <a href="http://dns2.icar.cnr.it/manco/Teaching/2005/datamining/articoli/KDD-96.final.frame.pdf">226-231.</a> </li><li>  <a href="http://www.siam.org/proceedings/datamining/2004/dm04_023kailingk.pdf">Kailing, K. Density-Connected Subspace Clustering for High-Dimensional Data / K. Kailing, H.-P.</a>  <a href="http://www.siam.org/proceedings/datamining/2004/dm04_023kailingk.pdf">Kriegel, P. Kr√∂ger // In Proceedings of the 4th SIAM International Conference on Data Mining (SDM), 2004. ‚ÄìC.246-257.</a> </li><li>  <a href="http://www.cs.cornell.edu/johannes/papers/1998/sigmod1998-clique.pdf">Agrawal, R. Automatic Subspace for Data Mining Applications / R. Agrawal, J. Gehrke, D. Gunopulos, P. Raghavan // In Proc.</a>  <a href="http://www.cs.cornell.edu/johannes/papers/1998/sigmod1998-clique.pdf">ACM SIGMOD Int.</a>  <a href="http://www.cs.cornell.edu/johannes/papers/1998/sigmod1998-clique.pdf">Conf.</a>  <a href="http://www.cs.cornell.edu/johannes/papers/1998/sigmod1998-clique.pdf">on Management of Data, Seattle, Washington, 1998.-C.94-105.</a> </li><li>  <a href="http://mrl.cecsresearch.org/Resources/papers/goil99mafia.pdf">Nagesh, H. MAFIA: Efficient and Scalable Subspace Clustering for Very Large Data Sets / H. Nagesh, S. Goil, A. Choudhary // Technical Report Number of Center for Parallel and Distributed Computing, Northwestern University 1999. 20 p.</a> </li><li>  <a href="http://web.mit.edu/6.435/www/Dempster77.pdf">Demster, A. Maximum Likelihood from Incomplete Data via the EM Algorithm / AP Demster, NM Laird, DB Rubin // JOURNAL OF THE ROYAL STATISTICAL SOCIETY, SERIES B, Vol.</a>  <a href="http://web.mit.edu/6.435/www/Dempster77.pdf">39, No.</a>  <a href="http://web.mit.edu/6.435/www/Dempster77.pdf">1, 1977.-C.1-38.</a> </li><li>  <a href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/Fisher_Cobweb.pdf">Fisher, DH Knowledge acquisition through incremental conceptual clustering / DH Fisher // Machine Learning 2, 1987. -C.139-172.</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/164417/">https://habr.com/ru/post/164417/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../164403/index.html">How do you feel about New Year's mailing lists?</a></li>
<li><a href="../164409/index.html">Hover effect CSS masks</a></li>
<li><a href="../164411/index.html">XIAOMI will open the source code MIUI in 2013</a></li>
<li><a href="../164413/index.html">Android application distribution points</a></li>
<li><a href="../164415/index.html">A little more about Zabbix - we set up ICQ notifications</a></li>
<li><a href="../164419/index.html">You calendar 2013</a></li>
<li><a href="../164423/index.html">Simple-Science - Simple Experiments (Digest # 12)</a></li>
<li><a href="../164425/index.html">Munchhausen Style, or one abnormal way to run Java programs on Android</a></li>
<li><a href="../164427/index.html">LED standby lighting</a></li>
<li><a href="../164431/index.html">Happy New Year! or Demoscene on the calculator</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>