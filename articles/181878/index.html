<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognition of psychological testing forms from scratch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Three months ago, I was approached by a good friend and colleague with a request to write a small program for psychological testing. I, who previously...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognition of psychological testing forms from scratch</h1><div class="post__text post__text-html js-mediator-article"> Three months ago, I was approached by a good friend and colleague with a request to write a small program for psychological testing.  I, who previously wrote exclusively for the small needs of office automation on vba, vb, vb.net, decided to seize the moment and learn C # during the project.  By the way, the project is simple, only 5 psychodiagnostic methods.  Later it turned out that his dream was a form recognition system for these techniques.  The situation has become complicated.  It became clear that the main amount of time I spend on recognition. <br><a name="habracut"></a><br>  Of course, I had no experience with images, and recognition, and began searching for libraries for letter recognition.  Unfortunately, it was not possible to find anything free.  So I rolled up my sleeves and started writing on my own. <br><br>  I will not write code here, as I believe that the main problem in creating such a system is in the algorithm of work. <br>  So‚Ä¶ <br>  For a start, forms were created.  Each method has its own, but they differ only in the number of questions and the number of answers in the questions. <br><img src="https://habrastorage.org/storage2/fd5/96d/b09/fd596db0910555383282655b8e7f2790.png" alt="image"><br><br>  Forms with my words created a good friend and co-worker.  They were originally created in Excel and saved as PDF. <br>  The most important detail on the form is black squares (I called them markers).  The upper central marker is needed to determine where the top and where the bottom of the form.  In psychodiagnostics, the subject is asked to draw a diagonal cross in the desired cell.  It is assumed that: <br>  1. The level of education of the subject may be low. <br>  2. This is not an exam, and the subject makes a mistake, crosses them out many times. <br>  3. The subject uses pencils, ballpoint, gel pens of different colors of ink. <br>  4. The test subject is poorly instructed, and instead of the crosses, ticks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Now, step by step formalization of the task itself of working with the test form recognition system: <br>  1. Remove the blank image from the input device. <br>  2. Bring the image into a standard form. <br>  3. Find the cells in the image that need to be recognized. <br>  4. Recognize cells. <br>  5. Save the answers. <br><br>  Now about everything in order. <br>  <b>1. Obtaining images for processing.</b> <br>  I chose a flatbed scanner as the cheapest and most common means of obtaining images.  In the native C # operating system, there are two main APIs for working with scanners: <a href="http://ru.wikipedia.org/wiki/TWAIN">TWAIN</a> and <a href="http://ru.wikipedia.org/wiki/WIA">WIA</a> .  No problems with WIA.  The technology is well supported in Windows, well documented and there are many <a href="http://cgeers.com/2011/05/15/windows-image-acquisition-wia/">examples</a> on the web.  The most difficult thing was to set <a href="http://www.nakov.com/blog/2009/11/17/playing-with-the-scanner-with-wia-and-c/">the scan parameters</a> . <br><br>  The unpleasant thing about WIA technology is the <a href="http://support.microsoft.com/kb/264598/ru">list of supported devices</a> .  They are few.  Therefore, I had to add the ability to work with TWAIN scanners.  I used the free <a href="https://github.com/tmyroadctfig/twaindotnet">TwainDotNet</a> library.  Its only drawback is that the scanner is checked for compatibility at the beginning of the scan.  Older scanners, for example, do not pass inspection due to the lack of auto-rotate image features.  Given the open source, I quickly corrected the situation. <br><br>  When using both APIs, I disabled their standard GUIs.  Set the size of the A4 image and resolution of 100 DPI.  For completeness, I encapsulated into my Scanner class the ability to select an image from a file. <br><br>  <b>2. Image normalization.</b> <br>  Reduction of the image to the universal view is divided into stages: <br>  1. Reduction of the image to the resolution of 100 DPI. <br>  2. Image binarization. <br>  3. Finding markers. <br>  4. Rotate the image. <br>  I used the <a href="http://www.aforgenet.com/framework/">AForge.NET Framework</a> to work with images.  This is free and convenient, since you do not need to write numerous algorithms for working with images, especially since you would have to write unsafe code for speed, and I'm new to C #.  In the future, I will refer to the classes of this particular library. <br>  So, at the beginning of the incoming image is reduced to 100 DPI.  Smaller resolution threatens with binarization problems, and more with processing speed problems.  Then we translate the image into the Grayscale color mode.  For this operation, we use the <a href="http://www.aforgenet.com/framework/docs/html/d7196dc6-8176-4344-a505-e7ade35c1741.htm">GrayScale</a> class.  For image binarization, I used <a href="http://www.aforgenet.com/framework/docs/html/0ad5b988-5613-d62a-22a9-cf41e39c139f.htm">BradleyLocalThresholding</a> .  The algorithm does an excellent job with small heaps of brightness. <br>  One of the difficult nuances of successful recognition is the correct orientation of the image.  Rotate by 3-4 degrees, and everything is gone.  There is no stream feed, the tablet scanner, the forms are crumpled.  In general, to determine the angle of rotation of the form, I used <a href="http://www.aforgenet.com/framework/docs/html/18d8bd4d-b15e-8507-a480-b2fe6eaeab44.htm">DocumentSkewChecker</a> .  And to rotate the image <a href="http://www.aforgenet.com/framework/docs/html/8620c864-b860-3b5e-e413-de516f50b939.htm">RotateBilinear</a> , because it is fast.  This rotation leveled the image in the range of 0-45 degrees, but it could remain inverted.  Therefore, it is necessary to find the markers and determine the top of the form by the central one. <br>  To search for objects in the image, I successfully applied <a href="http://www.aforgenet.com/framework/docs/html/d7d5c028-7a23-e27d-ffd0-5df57cbd31a6.htm">BlobCounter</a> .  To determine the markers, I filtered all the objects found by size and <a href="http://www.aforgenet.com/framework/docs/html/9efbb83b-0bad-8739-6073-1566b04e96a4.htm">Fullness</a> property, setting the threshold to 0.8.  And if I did not find three markers on top, I turned the image 180 degrees. <br>  With such simple manipulations, all images are brought to a single form and format. <br><br>  <b>3. Finding Recognized Response Cells</b> <br>  The easiest way was used to find the answer cells.  I took the reference blank form of each technique and saved the center of the first (left) cell of each question in the database.  It also saved the distance between the centers of the cells of each question and the size of the cells.  Thus, to determine the areas of recognition, I simply took the data from the database.  Here you need to make two very important observations.  If the scanned form is crumpled, then the recognition areas will be incorrect, since the reference form is even.  The percentage of errors in this case will be too large.  This is what happens if you ignore it: <br><img src="https://habrastorage.org/storage2/dfd/9fb/3be/dfd9fb3bef4f0a613645fa2e7bcc61c3.png"><br>  It is solved quite simply.  The area of ‚Äã‚Äãthe first cell expands from the center.  Then in this area I look for an object using <a href="http://www.aforgenet.com/framework/docs/html/d7d5c028-7a23-e27d-ffd0-5df57cbd31a6.htm">BlobCounter</a> .  After finding the cell inside the expanded area, the area is compressed relative to the center found.  It looks like this: <br><img src="https://habrastorage.org/storage2/551/5fc/7f8/5515fc7f8feaf501d633acf28b09911a.png"><br>  These manipulations are made only with the first cell of each question, the coordinates of the remaining cells are recalculated relative to it. <br>  Also, the image dimensions (recognition areas) may slightly change, so the data on the cells should not be saved in absolute coordinates on the original image but rather their position relative to the markers.  This is the second important function of markers. <br>  It is worth noting that it is possible to find recognition areas without knowing their coordinates.  This is done on the basis of vertical and horizontal brightness histograms.  However, I did not use this algorithm, and I will not describe it. <br><br>  <b>4. Cell recognition</b> <br>  Here I am stuck.  Recognition is essentially a classification task.  Given the task at hand, there are 3 classes of cells: <br>  1. Empty (free). <br>  2. Marked by the subject (cross). <br>  3. Erroneously marked by the test (miss). <br>  With the free class, everything is clear.  This is an absolutely empty cell, untouched by the test subject's pen.  But the cross class already has problems: anything can be found in the cell.  In spite of the excellent instructions, the persons undergoing testing do not carefully write out diagonal crosses.  Additionally, the subjects are instructed to shade a cell if they make a mistake and want to correct a variant of their answer.  This is the miss class. <br>  By and large, I had two approaches to the classification: separation of cells according to the level of brightness and the use of machine learning methods.  As an old fan of neural networks (I wrote something on vba), I took the <a href="http://www.heatonresearch.com/encog">Encog</a> library and trained a multilayer perceptron on 20 completed test forms.  And I got an error of 15-35% wrong answers.  It became clear that it was necessary to use more subtle algorithms (convolutional networks, ensembles of networks, a combination of learning algorithms).  At the same time, it was necessary to present the networks with various variants of the classes of cells, for example: <br><img src="https://habrastorage.org/storage2/400/8bb/b99/4008bbb99b8f01d21b9d3d5cd056eb43.png"><br>  As a result of all trial and error, I returned to the classification by brightness.  But not in the sum of the brightness of all the pixels of the cell, as it turns out not too sensitive.  I used the <a href="http://www.aforgenet.com/framework/docs/html/053a995c-5aa6-2bdf-3c0b-03f4a04eea85.htm">VerticalIntensityStatistics</a> class for the vertical histogram of the cell pixel brightness sums.  Below is the cross class histogram and the free class histogram: <br><img src="https://habrastorage.org/storage2/0a8/a49/b9a/0a8a49b9a30d104f91ce176b81620a55.png"><br>  I took the histogram values ‚Äã‚Äãfor the vertical pixels from 8 to 17 (cut off the minima) and used the formula: threshold = 1 - average (histogram value for brightness) / maximum brightness.  By the value of the threshold, I determined the class of the cell.  As a rule, classes were divided as follows: <br>  from 0 to 0.18 class free, <br>  from 0.18 to 0.6 class cross, <br>  from 0.6 to 1 miss class. <br><br>  <b>5. Saving answers.</b> <br>  Since the system is not automatic, the automated errors that occur during recognition can be corrected manually.  After this, the cell ID of the class cross is sent to the database. <br>  Externally, the recognition interface looks like this: <br><img src="https://habrastorage.org/storage2/d56/9c4/d48/d569c4d482882189802882d74906d92a.png"><br><br>  <b>Eventually:</b> <br>  For three months of learning the language C #, I made a free application for recognition of forms of psychological testing.  Prior use showed a throughput of approximately 1 form in a minute and a half.  This is very good, given the manual entry of personal data before recognition. <br>  Now I am happy to correct my <s>horror</s> code, and even, to be honest, I am writing documentation to give the product to people.  And at the same time optimizing the classification process. <br>  Or maybe this article will spur any pro to write a great free framework for recognition of forms, at least for such simple needs as my good friend and colleague ... </div><p>Source: <a href="https://habr.com/ru/post/181878/">https://habr.com/ru/post/181878/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../181868/index.html">Juniper Wireless Systems - First Meet</a></li>
<li><a href="../181870/index.html">I am writing a toy OS (about the implementation of sleep)</a></li>
<li><a href="../181872/index.html">Battleship as a recognition task</a></li>
<li><a href="../181874/index.html">Managed random in javascript</a></li>
<li><a href="../181876/index.html">What touch technologies are used on large screens?</a></li>
<li><a href="../181880/index.html">Comparison of the effectiveness of minimizers CSS-and JavaScript-code</a></li>
<li><a href="../181882/index.html">AngularJS best practices</a></li>
<li><a href="../181886/index.html">6 indicators of web production efficiency</a></li>
<li><a href="../181890/index.html">Lower limit for CAPTCHA, White House</a></li>
<li><a href="../181896/index.html">Solving a problem of Russian open source projects</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>