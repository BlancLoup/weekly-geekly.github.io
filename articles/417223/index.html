<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Capsule Neural Networks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In 2017, Jeffrey Hinton (one of the founders of the back-propagation error approach) published an article that described capsular neural networks and ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Capsule Neural Networks</h1><div class="post__text post__text-html js-mediator-article">  In 2017, Jeffrey Hinton (one of the founders of the back-propagation error approach) published an article that described capsular neural networks and proposed an algorithm for dynamic routing between capsules for learning the proposed architecture. <br><br>  The classical convolutional neural networks have drawbacks.  The internal representation of convolutional neural network data does not take into account the spatial hierarchies between simple and complex objects.  So, if the image randomly depicts the eyes, nose and lips for a convolutional neural network, this is a clear sign of the presence of a face.  And the rotation of an object impairs the quality of recognition, whereas the human brain easily solves this problem. <br><br><img src="https://habrastorage.org/webt/6y/cp/9f/6ycp9faaapjxfuhwflzs76i1-_e.png"><br>  For a convolutional neural network, 2 images are similar [2] <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/pm/pc/zt/pmpcztp7wilcwljmiveqo37evcg.jpeg"><br>  Thousands of examples will be needed to teach object recognition from various angles of CNN. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/-m/ek/sz/-mekszzwa4bdl7z6eszrfno7kjw.png"><br>  Capsule networks reduce object recognition error from a different angle by 45%. <br><br><h3>  Capsule Assignment </h3><br>  Capsules encapsulate information about the state of a function, which is found in vector form.  Capsules encode the probability of detecting an object as the length of the output vector.  The state of the detected function is encoded as the direction in which the vector indicates (‚Äúinstance creation parameters‚Äù).  Therefore, when the detected function moves through the image or the state of the image changes, the probability remains the same (the length of the vector does not change), but the orientation changes. <br><br>  Imagine that a capsule detects a face in an image and displays a 3D vector of 0.99 length.  Then, move the face in the image.  The vector will rotate in its space, representing a changing state, but its length will remain fixed, because the capsule is sure that it has detected the face. <br><br><img src="https://habrastorage.org/webt/nw/ky/qt/nwkyqtzc1l2dpyfpqzdakpfdfy8.png"><br><br>  Differences between capsules and neurons. [2] <br><br>  An artificial neuron can be described in three steps: <br><br>  1. scalar weighting of input scalars <br>  2. sum of weighted input scalars <br>  3. nonlinear scalar transformation. <br><br>  The capsule has the vector shapes of the above 3 steps, in addition to the new phase of the affine input transformation: <br><br>  1. matrix multiplication of input vectors <br>  2. scalar weighting of input vectors <br>  3. sum of weighted input vectors <br>  4. vector nonlinearity. <br><br>  Another innovation, which is presented in CapsNet, is a new non-linear activation function, which takes a vector and then "gives out" its length not more than 1, but does not change direction. <br><br><img src="https://habrastorage.org/webt/c8/p5/zz/c8p5zzuf4qe1f8cdvywiub1o2jk.png"><br><br>  The right side of the equation (blue rectangle) scales the input vector so that the vector will have a block length, and the left side (red rectangle) performs additional scaling. <br><br>  The design of the capsule is based on the device of an artificial neuron, but expands it to a vector shape in order to provide more powerful representative capabilities.  Matrix weights are also introduced to encode the hierarchical relationships between features of different layers.  The equivalence of neural activity in relation to changes in input data and invariance in the probabilities of detecting signs is achieved. <br><br><h3>  Dynamic routing between capsules </h3><br><br><img src="https://habrastorage.org/webt/14/dr/9q/14dr9qzg_8mvvntwgiq5l8r23u0.png"><br><br>  Dynamic routing algorithm [1]. <br><br>  The first line says that this procedure takes capsules at the lower level l and their outputs u_hat, as well as the number of iterations of routing r.  The last line says that the algorithm will output a higher level of the v_j capsule. <br><br>  The second line contains the new coefficient b_ij, which we have not seen before.  This coefficient is a temporary value that will be updated iteratively, and after the procedure is completed, its value will be stored in c_ij.  At the beginning of training, the value of b_ij is initialized to zero. <br><br>  Line 3 states that steps 4-7 will be repeated r times. <br>  The step in line 4 calculates the value of the vector c_i, which is all the routing weights for the capsule i of the lower level. <br><br>  After the weights c_ij are calculated for lower level capsules, go to line 5, where we look at higher level capsules.  This step calculates a linear combination of the input vectors weighted using the routing coefficients c_ij defined in the previous step. <br><br>  Then, in line 6, the vectors of the last step pass through a nonlinear transformation, which ensures that the direction of the vector is preserved, but its length should not exceed 1. This step creates an output vector v_j for all higher levels of the capsule. <br>  The basic idea is that the similarity between the input and the output is measured as the scalar product between the input and output of the capsule, and then the routing coefficient changes.  Best practice is to use three iterations of routing. <br><br><h3>  Conclusion </h3><br>  Capsule neural networks are a promising neural network architecture that improves image recognition with changing angles and hierarchical structure.  Capsule neural networks are trained using dynamic routing between capsules.  Capsule nets reduce object recognition error from a different angle by 45% compared to CNN. <br><br><div class="spoiler">  <b class="spoiler_title">Shortcuts</b> <div class="spoiler_text">  [1] MATRIX CAPSULES WITH EM ROUTING.  Geoffrey Hinton, Sara Sabour, Nicholas Frosst.  2017 <br>  [2] Understanding Hinton's Capsule Networks.  Max pechyonkin <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/417223/">https://habr.com/ru/post/417223/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417209/index.html">Neural networks from scratch. Overview of courses and articles in Russian, free of charge and without registration</a></li>
<li><a href="../417211/index.html">Nastolka for learning the basics of electrical circuits. Why not?</a></li>
<li><a href="../417215/index.html">Everything you need to know about the Python garbage collector</a></li>
<li><a href="../417219/index.html">Forget burger king! There is a leak of documents is more dangerous</a></li>
<li><a href="../417221/index.html">3DTouch - Scales on the iPhone: Getting Started</a></li>
<li><a href="../417225/index.html">Boschernitsana theorem</a></li>
<li><a href="../417227/index.html">What happened to the Fermi paradox</a></li>
<li><a href="../417229/index.html">Some reasons to forget PascalABC.Net</a></li>
<li><a href="../417231/index.html">Corporate merchandise with human UI</a></li>
<li><a href="../417233/index.html">Google Code-in 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>