<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Screen Space Ambient Occlusion</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Further, it will be discussed how to implement the Screen Space Ambient Occlusion method for calculating diffused lighting in the C ++ programming lan...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Screen Space Ambient Occlusion</h1><div class="post__text post__text-html js-mediator-article">  Further, it will be discussed how to implement the Screen Space Ambient Occlusion method for calculating diffused lighting in the C ++ programming language using the API DirectX11. <br><a name="habracut"></a><br>  Consider the formula for calculating the color of a pixel on the screen when using, for example, a parallel light source: <br><blockquote>  LitColor = Ambient + Diffuse + Specular </blockquote>  Or, more formally, the sum of diffused, absorbed and specular illuminations.  Each of them is calculated as follows: <br><blockquote>  (material color) * (source color) * (intensity coefficient) </blockquote>  For a long time in applications of interactive graphics, the coefficient of the intensity of diffused (ambient) lighting was constant, but now we can calculate it in real time.  I would like to talk about one of these methods - ambient occlusion, or rather its optimization - screen space ambient occlusion.  Let's talk first about the ambient occlusion method.  Its essence is as follows - for each top of the scene to form a factor that will determine the degree of "visibility" of the rest of the scene. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59d/139/2a7/59d1392a725cd9ac9941dfc75e96f534.png" alt="image"></div><br>  <i><font color="#999999">Fig.1 - a drawing with a room and two points, the ‚Äúvisibility‚Äù of each point is depicted as a sphere</font></i> <br><br>  So, for each vertex in random directions, we add rays and find their intersection with the geometry of the scene.  Next, we calculate the length of the resulting line (if the intersection was not found, we will assume that the beam has a certain maximum length for the given scene) and compare it with the threshold value.  If the length exceeds the threshold value - then the beam passes the ‚Äúvisibility‚Äù test. The number of tests passed divided by the number of rays launched will be the ‚Äúvisibility‚Äù factor. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Obviously, the high computational complexity of the algorithm makes it inapplicable in real time or for scenes with high dynamics of objects.  Also, the effectiveness of the method strongly depends on the polygonal complexity of the scene.  This approach is reasonable to use when it is possible to calculate in advance the "visibility" and save it as part of the vertices or in the texture. <br><br>  Fortunately, the guys at CryTeck (at least I heard that they were the first) came up with a way to calculate the coefficient in real time.  It is called Screen Space Ambient Occlusion. <br><br>  The algorithm of my implementation is as follows: <br><br><ul><li>  <b>1.</b> Take NDC (normalized device coordinates) or pixel texture coordinates and convert them to a point in the camera space, using the depth data; <br><br></li><li>  <b>2.</b> From this point in the random directions we let N rays; <br><br></li><li>  <b>3.</b> For each of the N rays: <br><br><ul><li>  <b>3-a.</b>  multiply (scale) our ray (vector) by a certain number (scalar) and add it to the point from item 1; </li><li>  <b>3-b.</b>  Transform the resulting point into NDC space, and then into texture coordinates; </li><li>  <b>3-c.</b>  From the texture, we get the depth value for this point; </li><li>  <b>3rd.</b>  If the resulting value is less than the depth of the point obtained in paragraph 3-a, then there is an ‚Äúoverlap‚Äù (see Fig. 2).  It is necessary to take into account that these values ‚Äã‚Äãbelong to the same coordinate system; </li><li>  <b>3-f.</b>  We obtain the ‚Äúoverlap‚Äù factor based on the dependence ‚ÄúThe farther a point from p. 3 is on the point from p. 1, the less potential overlap from this point‚Äù.  Accumulate its value. </li></ul><br></li><li>  <b>4.</b> We obtain the general factor of ‚Äúoverlap‚Äù, which is equal to the total total overlap of the / N rays.  Since the common factor belongs to [0,1], and visibility is inversely proportional to the overlap, then it is equal to 1 - overlap. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ed5/702/bd0/ed5702bd0b67d667217c508f1e2151b4.png" alt="image"></div><br>  <i><font color="#999999">Fig.2 - in blue is depicted the normal vector, in red is the vector obtained in step 3-a.</font></i>  <i><font color="#999999">A light green vector is the direction of the Z axis. If the depth value at point A is greater than at point B, this is an overlap.</font></i>  <i><font color="#999999">For clarity, the figure uses an orthogonal projection (therefore, the AB line is a straight line)</font></i> <br><br>  By applying this algorithm in a pixel shader, we can get the visibility data if we write the rendering result to a texture.  The data from this texture can be further used when calculating the illumination of the scene. <br><br>  So, let's begin. <br><br><h3>  1. Conversion </h3><br>  In order to get screen coordinates from three-dimensional coordinates, we need to perform a series of matrix transformations. <br><br>  In the general form of such transformations, there are three: <br><br><ul><li>  <b>1.</b> From local coordinates to world ones - transfer all objects to a common coordinate system </li><li>  <b>2.</b> From world coordinates to view coordinates - to orient all objects relative to the ‚Äúcamera‚Äù </li><li>  <b>3.</b> From view coordinates to projection coordinates - project the vertices of the objects on the plane.  We use the perspective projection, which implies the so-called homogeneous divide - the division of the components x and at the apex by its depth - the component z. </li></ul><br>  Items 1 and 2 are not important for us, so we proceed immediately to p.3.  Let's look at the projection matrix: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/464/22e/f96/46422ef962f4a839e27bedc8230ef1d0.png" alt="image"></div><br>  After multiplying by this matrix, the coordinates from the camera space go to the projection space <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b55/682/44d/b5568244da635e9ea58fe8188da763d0.png" alt="image"></div><br>  This is followed by a homogeneous division, as a result of this we move to the space NDC <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb3/266/a1a/bb3266a1aa69fca3e444647219561fc4.png" alt="image"></div><br>  Now let's see how to do the inverse transform.  Obviously, we first need pixel coordinates in the shader.  I think it is most convenient to use a square covering the entire screen area in NDC space with texture coordinates from (0,0) to (1,1).  Here is the vertex data: <br><br><pre><code class="hljs go"><span class="hljs-keyword"><span class="hljs-keyword">struct</span></span> ScreenQuadVertex { D3DXVECTOR3 pos = {<span class="hljs-number"><span class="hljs-number">0.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}; D3DXVECTOR2 tc = {<span class="hljs-number"><span class="hljs-number">0.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}; ScreenQuadVertex(){} ScreenQuadVertex(<span class="hljs-keyword"><span class="hljs-keyword">const</span></span> D3DXVECTOR3 &amp;Pos, <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> D3DXVECTOR2 &amp;Tc) : pos(Pos), tc(Tc){} }; std::vector&lt;ScreenQuadVertex&gt; vertices = { {{<span class="hljs-number"><span class="hljs-number">-1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}, {<span class="hljs-number"><span class="hljs-number">0.0f</span></span>, <span class="hljs-number"><span class="hljs-number">1.0f</span></span>}}, {{<span class="hljs-number"><span class="hljs-number">-1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}, {<span class="hljs-number"><span class="hljs-number">0.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}}, {{ <span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}, {<span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}}, {{ <span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">0.0f</span></span>}, {<span class="hljs-number"><span class="hljs-number">1.0f</span></span>, <span class="hljs-number"><span class="hljs-number">1.0f</span></span>}}, };</code> </pre> <br>  You also need to set the point interpolation of the texture data, for example D3D11_FILTER_MIN_MAG_MIP_POINT.  By drawing this square, we can either "forward" the vertex data to the pixel shader like this: <br><br><pre> <code class="hljs lua">VOut <span class="hljs-built_in"><span class="hljs-built_in">output</span></span>; <span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.posN = float4(<span class="hljs-built_in"><span class="hljs-built_in">input</span></span>.posN, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f); <span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.tex = <span class="hljs-built_in"><span class="hljs-built_in">input</span></span>.tex; <span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.eyeRayN = float4(<span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.posN.xy, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f);</code> </pre> <br>  Or, directly in the pixel shader, convert the interpolated texture coordinates into the NDC space like this (for more details on this conversion, see Chapter 3): <br><br><pre> <code class="hljs go">float4 posN; posN.x = (Input.tex.x * <span class="hljs-number"><span class="hljs-number">2.0f</span></span>) - <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; posN.y = (Input.tex.y * <span class="hljs-number"><span class="hljs-number">-2.0f</span></span>) + <span class="hljs-number"><span class="hljs-number">1.0f</span></span>;</code> </pre><br>  The coordinates of the pixel in the NDC space we have - now we need to go to the view space.  Based on the properties of matrices: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/afe/2e2/9d8/afe2e29d8f6de7cdaca74946fcf9c445.png" alt="image"></div><br>  For our purposes, we must have an inverse projection matrix.  She looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2e6/50f/537/2e650f53776564a07de2650e83cccb04.png" alt="image"></div><br>  But it is not enough for us to simply multiply the two-dimensional point in it in the NDC space and make a uniform division - we also need to have data on the depth of the point that we transform.  I want to use depth in the form of space - let's do some algebraic transformations and find out if this is possible.  First, we express the transition of a point from the species to the NDC space: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4c0/3cb/1fd/4c03cb1fd0b6d1e9a777261ab2f42d7f.png" alt="image"></div><br>  Now, we multiply by the inverse projection matrix: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/08a/cbf/ece/08acbfece63359c3620b757ca7c4d805.png" alt="image"></div><br>  Then, simplify X and Y and expand the brackets in W: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d21/824/97a/d2182497a9e0e385b7cdcf526dfa5d82.png" alt="image"></div><br>  Further we will continue simplification in W: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/082/0da/88e/0820da88e2ef8ae103e0fcb0f4cb27c5.png" alt="image"></div><br><br>  And the final touch - cut 1 / n: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5da/64e/f5d/5da64ef5d8e10ecf7a5655ee8538bee6.png" alt="image"></div><br>  It turns out that after multiplying by the inverse matrix of the projection, we need to multiply the result by the depth in the view space. So we proceed.  First, prepare the data in the NDC space in the vertex shader: <br><br><pre> <code class="hljs lua"><span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.eyeRayN = float4(<span class="hljs-built_in"><span class="hljs-built_in">output</span></span>.posN.xy, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f);</code> </pre> <br>  Then we will do the main work in the pixel shader: <br><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">float4</span></span> normalDepthData = normalDepthTex.Sample(normalDepthSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex); float3 viewRay = mul(<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.eyeRayN, invProj).xyz; viewRay *= normalDepthData.w;</code> </pre> <br>  We have coordinates in space of the form.  Go ahead. <br><br><h3>  2. Ray tracing </h3><br>  <b>2.1 Offset data</b> <br><br>  So, we have the coordinates of the pixel being processed in the view space.  Further from the point with these coordinates, we need to send N rays in random directions.  Unfortunately, the HLSL API does not have a tool with which we could get a random or pseudo-random value during the execution of a shader regardless of external data (well, or I just don‚Äôt know about the existence of such technologies) - therefore, we will prepare such data in advance.  In order to get them in a shader, the easiest way is to use a texture.  Obviously, its ‚Äúweight‚Äù and the limit of data values ‚Äã‚Äãdepend on the pixel format.  For our purposes, the DXGI_FORMAT_R8G8B8A8_UNORM format is quite suitable.  Now let's deal with the size.  Probably the easiest, descriptive and at the same time non-optimal way is to create a texture with a length and width equal to the screen resolution.  In this case, we simply select the data by the value of the texture coordinates of the square, which, recall, are in the range of (0,0) to (1,1).  But what will happen if we go beyond these limits?  Then the rules specified in the D3D11_TEXTURE_ADDRESS_MODE enumeration come into play.  In this case, we are interested in the value of D3D11_TEXTURE_ADDRESS_MIRROR.  The result of this addressing rule is shown in Figure 3. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6cc/7a4/9ff/6cc7a49ff736fd5b8fb6806b3a7f317e.png" alt="image"></div><br>  <i><font color="#999999">Figure 3 - an example of using D3D11_TEXTURE_ADDRESS_MIRROR "</font></i> <br><br>  If we use this approach, then for our purposes, differences will be acceptable (see Figure 4). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3bc/628/850/3bc628850fbec094f0f1c7aaf7061482.png" alt="image"></div><br>  <i><font color="#999999">Fig.4 - primitive with texture overlay 256x256 and with coordinates from 0 to 1 and primitive with texture overlay 4x4 with coordinates from 0 to 64 and addressing D3D11_TEXTURE_ADDRESS_MIRROR</font></i> <br><br>  Now, finally, let's fill the texture with data.  In the shader, we will form a random direction vector from the R, G, and B texel components, so we do not use the alpha channel (you can consider it as a component of W, which is zero for vectors in a homogeneous space).  As a result, the code is something like this: <br><br><pre> <code class="hljs cpp"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> y = <span class="hljs-number"><span class="hljs-number">0</span></span>; y &lt; texHeight; y++){ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> x = <span class="hljs-number"><span class="hljs-number">0</span></span>; x &lt; texWidth; x++){ <span class="hljs-keyword"><span class="hljs-keyword">char</span></span>* channels = <span class="hljs-keyword"><span class="hljs-keyword">reinterpret_cast</span></span>&lt;<span class="hljs-keyword"><span class="hljs-keyword">char</span></span>*&gt;(&amp;data[y * texWidth + x]); channels[<span class="hljs-number"><span class="hljs-number">0</span></span>] = rand() % <span class="hljs-number"><span class="hljs-number">255</span></span>; <span class="hljs-comment"><span class="hljs-comment">//r channels[1] = rand() % 255; //g channels[2] = rand() % 255; //b channels[3] = 0; //a } }</span></span></code> </pre> <br>  I also want to draw attention to the fact that the smaller the texture size, the clearer the image will be (see Figure 5): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6b6/91b/420/6b691b420b96e8d0ab6a204123c6d9ef.png" alt="image"></div><br>  <i><font color="#999999">Fig.5 - a demonstration of the difference between textures of 128x128 and 4x4 offsets</font></i> <br><br>  Well, our texture is ready - it remains only to get this data in the shader.  But we remember that we have texture coordinates from 0 to 1, and we need to use coordinates from 0 to N, where N&gt; 0. This problem is solved very simply at the stage of preparing the shader - you need to know how much you need to multiply the length and How much you need to multiply the width of the texture so that it takes up the entire screen.  Suppose that the screen resolution is 1024x768, and the size of the texture is 2x4, then we get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bac/335/6f0/bac3356f034ae5f1c6f55b69d19b97a5.png" alt="image"></div><br>  Now we will express the coefficients: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eec/99f/950/eec99f950088eef4b3ce8de11b10210b.png" alt="image"></div><br>  As a result, we get the following code: <br><br><pre> <code class="hljs lisp">float2 rndTexFactor(<span class="hljs-name"><span class="hljs-name">fWidth</span></span>, fHeight)<span class="hljs-comment"><span class="hljs-comment">; float3 rndData = tex.Sample(randomOffsetsSampler, input.tex * rndTexFactor).rgb;</span></span></code> </pre> <br>  Perhaps, in your case, it will be more rational to store these coordinates of the sample from the displacement texture as vertex data, thereby obtaining a ready-made interpolation value. <br><br>  Further, since we chose the DXGI_FORMAT_R8G8B8A8_UNORM format, our offset is in the space from 0 to 1. Transfer it to the space (-1, 1) (for a detailed description of the transformation, see Chapter 3): <br><br><pre> <code class="hljs go">rndData = normalize(<span class="hljs-number"><span class="hljs-number">2.0f</span></span> * rndData - <span class="hljs-number"><span class="hljs-number">1.0f</span></span>);</code> </pre> <br>  Now we have a displacement vector! <br><br>  <b>2.2 The core of the displacement vectors</b> <br><br>  One vector is nice, but we need to start up N vectors.  We can get a certain factor of the displacement of texture coordinates ranging from 0 to N and do something like this: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; N; i++){ float3 rndData = tex.Sample(randomOffsetsSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex * rndTexFactor + <span class="hljs-keyword"><span class="hljs-keyword">Offset</span></span> * i).rgb; rndData = normalize(<span class="hljs-number"><span class="hljs-number">2.0</span></span>f * rndData - <span class="hljs-number"><span class="hljs-number">1.0</span></span>f); <span class="hljs-comment"><span class="hljs-comment">/*...*/</span></span> }</code> </pre> <br>  This option is too resource intensive.  Let's try to get an acceptable result using only one sample from the texture.  Our goal is to achieve a relatively heterogeneous distribution of vectors both within the processed pixel and relative to the neighboring ones.  Let's take N prearranged random vectors and each of them is applicable to our displacement vector with a specific mathematical operation.  This set is called the "Core of the displacement vectors".  I assure you, it's easier than I described) <br><br>  Prepare our core: <br><br><pre> <code class="hljs cpp"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;D3DXVECTOR4&gt; kernel(KernelSize); <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(D3DXVECTOR4 &amp;k : kernel){ kx = Math::RandSNorm(); ky = Math::RandSNorm(); kz = Math::RandSNorm(); kw = <span class="hljs-number"><span class="hljs-number">0.0f</span></span>; D3DXVec4Normalize(&amp;k, &amp;k); FLOAT factor = (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)i / KernelSize; k *= Math::Lerp(<span class="hljs-number"><span class="hljs-number">0.1f</span></span>, <span class="hljs-number"><span class="hljs-number">0.9f</span></span>, factor); i++; }</code> </pre><br>  Values ‚Äã‚Äãfor each component are generated from -1 to 1. Please note that vectors are not of unit length.  This is important because it significantly affects the final image.  In fig.  6 that the vectors of non-unit length, when projected, form a more concentrated set of points. <br><br><div style="text-align:center;"><img src="http://i66.tinypic.com/2wn7906.png" alt="image"></div><br>  <i><font color="#999999">Fig.6 - projection of vectors of non-unit length forms a more concentrated set of points.</font></i>  <i><font color="#999999">For greater clarity, orthogonal projection is used.</font></i> <br><br>  Well, the kernel is ready - it remains to use it in the shader.  As a mathematical operation, I decided to use "Reflection of the vector."  This tool is very useful and widely used - for example, if we need to get the reflected vector to the light source when calculating the specular lighting or if we need to know which way the ball will fly, bounced off the wall.  The formula for calculating the reflected vector is as follows: <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/2dvv434.png" alt="image"></div><br>  where v is the vector that we are going to reflect, n is the normal to the surface, relative to which we will reflect the vector (see fig.7) <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/abk9y.png" alt="image"></div><br>  <i><font color="#999999">Fig.7 - visualization of the formula of the reflected vector</font></i> <br><br>  The last thing we need to do with the vector is to ensure that it is within the normal-oriented hemisphere.  To do this, we will change its direction if its scalar product with a normal is less than zero.  As a result, we got the following code: <br><br><pre> <code class="hljs pgsql">//float3 kernel[N] -    //normalV -      float3 rndData = tex.Sample(randomOffsetsSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex * rndTexFactor + <span class="hljs-keyword"><span class="hljs-keyword">Offset</span></span> * i).rgb; rndData = normalize(<span class="hljs-number"><span class="hljs-number">2.0</span></span>f * rndData - <span class="hljs-number"><span class="hljs-number">1.0</span></span>f); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; N; i++){ float3 samplingRayL = reflect(kernel[N], rndData); samplingRayL *= sign(dot(samplingRayL, normalV)); <span class="hljs-comment"><span class="hljs-comment">/*...*/</span></span> }</code> </pre> <br>  Please note that we do not normalize the result of the reflect () operation. <br><br><h3>  3. From ray to spot on screen </h3><br>  Let's look back and see what happened.  So: <br><br><ul><li>  Using the depth data, we obtained the pixel coordinates in the view space. </li><li>  We got N rays randomly distributed relative to each other. </li></ul><br>  Now we have everything we need in order to finally know what is around us.  We continue.  Multiply our vector by a certain scalar occlusionRadius and add it to the point of our pixel in the view space.  It is reasonable to allow the artist to regulate the value of occlusionRadius. <br><br><pre> <code class="hljs objectivec"><span class="hljs-comment"><span class="hljs-comment">//viewRay -      float3 samplingPosV = viewRay + (samplingRayL * occlusionRadius);</span></span></code> </pre><br>  Formally speaking, in the view space we obtained a samplingPosV point, which is located at a distance from our pixel in the direction of samplingRayL.  Next, we project the resulting point onto the screen, while not forgetting to produce a ‚Äúuniform division‚Äù in order to take into account the depth: <br><br><pre> <code class="hljs lisp">float4 samplingPosH = mul(<span class="hljs-name"><span class="hljs-name">float4</span></span>(<span class="hljs-name"><span class="hljs-name">samplingPosV</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f), proj)<span class="hljs-comment"><span class="hljs-comment">; float2 samplingRayN = samplingRayH.xy / samplingRayH.w;</span></span></code> </pre> <br>  We are in the NDC space.  Now we need to go to the texture coordinate space.  To do this, we transform our point from the range of values ‚Äã‚Äãfrom -1 to 1 to the area from 0 to 1. Note that the Y axis is directed in the opposite direction.  (see figure 8) <br><br><div style="text-align:center;"><img src="http://i63.tinypic.com/2db5uns.png" alt="image"></div><br>  <i><font color="#999999">Fig.8 - Demonstration of coordinate axes for NDC and texture coordinate space</font></i> <br><br>  Let's first convert the X coordinate. In general, one-dimensional transformations of this kind can be performed as follows: first we subtract the minimum value of the range, then divide by the width of the range (maximum minus minimum), then multiply the resulting coefficient by the width of the range of the new space and add to the result minimum value of new space.  I assure you it is easier to do than to say.  For our case, suppose that Nx is the X coordinate in the NDC space, Tx is the X coordinate in the texture coordinate space.  It turns out the following: <br><br><div style="text-align:center;"><img src="http://i64.tinypic.com/2rxfeqd.png" alt="image"></div><br>  Since the Y coordinate in the NDC space is directed in the opposite direction, it is necessary to act somewhat differently.  We cannot simply take the value with the opposite sign, since we will immediately go beyond the permissible limits.  Hmm ... Imagine a point in the lower right corner of the screen - in the NDC space its coordinates will be (1, -1), and in the coordinate space of the texture - (1, 1).  Now imagine a point in the upper left corner - in its NDC space the coordinates will be (-1, 1), and in the coordinate space of the texture - (0, 0).  The following pattern emerges: for the boundary regions, Y takes the maximum value in one coordinate system and the minimum in the other and vice versa.  Therefore, when we get our coefficient - we will subtract it from the unit. <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/2ztd7w3.png" alt="image"></div><br>  We can solve this problem in another way.  The solution is presented in Appendix 1. <br><br>  As a result, in the shader we get the following code: <br><br><pre> <code class="hljs go">float2 samplingTc; samplingTc.x = <span class="hljs-number"><span class="hljs-number">0.5f</span></span> * samplingRayN.x + <span class="hljs-number"><span class="hljs-number">0.5f</span></span>; samplingTc.y = <span class="hljs-number"><span class="hljs-number">-0.5f</span></span> * samplingRayN.y + <span class="hljs-number"><span class="hljs-number">0.5f</span></span>;</code> </pre> <br>  I add that you can combine the transformation to the texture coordinate and projecting in one matrix as follows (P is the projection matrix): <br><br><div style="text-align:center;"><img src="http://i66.tinypic.com/334lyl2.png" alt="image"></div><br><h3>  4. Work with depth data </h3><br>  Very little is left!  Hurry, hurry!  According to the coordinates obtained in the previous paragraph, we make a sample of the texture with the data. <br><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> sampledDepth = normalDepthTex.Sample(normalDepthSampler, samplingTc).w;</code> </pre> <br>  Component w stores depth data - take it and!  And ... And what should we do with them !?  Let's think about it.  We are in the form of space - the Z axis coincides with the direction of the camera.  Therefore, the smaller the obtained depth, the closer the object is to us.  Let me remind you that we have projected a point, which is located at some distance from our pixel in the view space.  The texture also stores depth in view space.  What do we learn if we compare the depth of the texture with the depth of our point?  If the depth value from the texture is less than the depth of the point, then something is located closer to the camera and our point will not be visible. Accordingly, our point is visible if it is closer to the camera than this ‚Äúsomething‚Äù.  By the way, about also works ShadowMapping.  It‚Äôs as if you need to make a difficult maneuver by car, and you don‚Äôt see what‚Äôs going on below and you‚Äôre asking a friend to adjust your movement.  But he was drunk and thought that it would be very funny to tell you the data opposite to what you expected ... But this is not our case) <br><br>  So, the fewer points from the N set can be seen by the camera, the less diffuse lighting our pixel receives.  You can consider the situation a little differently - let's imagine that we are looking from our pixel in the direction of its normal (because the rays are distributed within the hemisphere oriented by the normal).  The fewer points from the N set are visible to the camera, the smaller the number of scene objects we can see from our pixel (because more and more ‚Äúgeometry‚Äù of the scene objects blocks our view) - hence the less access to the ambient light of the scene (Damn! Dad made my poster "Iron maiden" with your skis! Pikachu! I challenge you !!) <br><br>  It should also be noted that a certain object of the scene, the depth of which we received, may be so far that it does not affect access to diffused light to a pixel point (see. Fig. 9) <br><br><div style="text-align:center;"><img src="http://i66.tinypic.com/rcjajk.png" alt="image"></div><br>  <i><font color="#999999">Fig.9 - The point q, though closer to the camera, is located too far from the pixel point P and cannot affect its illumination.</font></i> <br><br>  I suggest not just adding 1, but a certain coefficient depending on the distance: <br><br><pre> <code class="hljs lisp">float distanceFactor = (<span class="hljs-number"><span class="hljs-number">1.0</span></span>f - saturate(<span class="hljs-name"><span class="hljs-name">abs</span></span>(<span class="hljs-name"><span class="hljs-name">viewRay</span></span>.z - sampledDepth) / occlusionRadius)) * harshness<span class="hljs-comment"><span class="hljs-comment">;</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Notice that we form the distance coefficient based on the depth of the pixel point, and not the point we projected ‚Äî we used it to see if there is something in front of us, but now we need to understand how far this ‚Äúsomething‚Äù This is from us. </font><font style="vertical-align: inherit;">I also added the ability to adjust the intensity through the harshness parameter. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, this is the main part of the algorithm, so to say heart of it all. </font><font style="vertical-align: inherit;">Let's look at the whole cycle of working with displacement vectors:</font></font><br><br><pre> <code class="hljs pgsql">//viewRay -      //normalV -      //float3 kernel[N] -    //<span class="hljs-keyword"><span class="hljs-keyword">offset</span></span> -   ,    <span class="hljs-type"><span class="hljs-type">float</span></span> totalOcclusion = <span class="hljs-number"><span class="hljs-number">0.0</span></span>f; [unroll] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; <span class="hljs-number"><span class="hljs-number">16</span></span>; i++){ float3 samplingRayL = reflect(kernel[i].xyz, <span class="hljs-keyword"><span class="hljs-keyword">offset</span></span>); samplingRayL *= sign(dot(samplingRayL, normalV)); float3 samplingPosV = viewRay + (samplingRayL * occlusionRadius); <span class="hljs-type"><span class="hljs-type">float4</span></span> samplingPosH = mul(<span class="hljs-type"><span class="hljs-type">float4</span></span>(samplingPosV, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f), proj); samplingPosH.xy /= samplingPosH.w; float2 samplingTc; samplingTc.x = <span class="hljs-number"><span class="hljs-number">0.5</span></span>f * samplingPosH.x + <span class="hljs-number"><span class="hljs-number">0.5</span></span>f; samplingTc.y = <span class="hljs-number"><span class="hljs-number">-0.5</span></span>f * samplingPosH.y + <span class="hljs-number"><span class="hljs-number">0.5</span></span>f; <span class="hljs-type"><span class="hljs-type">float</span></span> sampledDepth = normalDepthTex.Sample(normalDepthSampler, samplingTc).w; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(sampledDepth &lt; samplingPosV.z){ <span class="hljs-type"><span class="hljs-type">float</span></span> distanceFactor = (<span class="hljs-number"><span class="hljs-number">1.0</span></span>f - saturate(abs(viewRay.z - sampledDepth) / occlusionRadius)); totalOcclusion += distanceFactor * harshness; } }</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let's look at the result! </font></font><br><br><div style="text-align:center;"><img src="http://i63.tinypic.com/24zfwvs.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">"Hey! </font><font style="vertical-align: inherit;">What the heck is that! </font><font style="vertical-align: inherit;">And where is FarCry?! ‚Äù- you ask. </font><font style="vertical-align: inherit;">"Easy!" - I will answer you. </font><font style="vertical-align: inherit;">‚ÄúChip and Dale rush to the rescue!‚Äù Oh, this is not from that article - ‚ÄúBlur hurries to the rescue!‚Äù</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 5. Use Blur </font></font></h3><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5.1 is the easiest option.</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Blur effect, or Blur, is a very useful tool that is used in many areas of graphics. I would compare it with electrical tape (blue! This is important) - with its help, you can fix or improve something, but you can hardly fix the phone that fell on the tile from the height of the cabinet (although instructions like ‚ÄúWrap it with insulation, and everything will be fine "Met more than once). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The essence of the effect is simple: for each texel, get the arithmetic average of the colors of its neighbors. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, suppose we have a texel with coordinates P - let's calculate the arithmetic average of the colors of its neighbors in R (AreaWidth by AreaHeight pixels). Something like this (I deliberately do not check for exceeding the array bounds. About this below):</font></font><br><br><pre> <code class="hljs django"><span class="xml"><span class="xml">D3DXCOLOR **imgData = ...; //  D3DXCOLOR avgColor(0.0f, 0.0f, 0.0f, 0.0f); for(INT x = Px - AreaWidth / 2; x </span><span class="hljs-tag"><span class="xml"><span class="hljs-tag">&lt;</span></span><span class="hljs-name"><span class="xml"><span class="hljs-tag"><span class="hljs-name">=</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">Px</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">-</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">AreaWidth</span></span></span></span><span class="xml"><span class="hljs-tag"> / </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">2</span></span></span></span><span class="xml"><span class="hljs-tag">; </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">x</span></span></span></span><span class="xml"><span class="hljs-tag">++) </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">for</span></span></span></span><span class="xml"><span class="hljs-tag">(</span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">INT</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">y</span></span></span></span><span class="xml"><span class="hljs-tag"> = </span></span><span class="hljs-string"><span class="xml"><span class="hljs-tag"><span class="hljs-string">Py</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">-</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">AreaHeight</span></span></span></span><span class="xml"><span class="hljs-tag"> / </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">2</span></span></span></span><span class="xml"><span class="hljs-tag">; </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">y</span></span></span></span><span class="xml"><span class="hljs-tag"> &lt;= </span></span><span class="hljs-string"><span class="xml"><span class="hljs-tag"><span class="hljs-string">Py</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">-</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">AreaHeight</span></span></span></span><span class="xml"><span class="hljs-tag"> / </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">2</span></span></span></span><span class="xml"><span class="hljs-tag">; </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">y</span></span></span></span><span class="xml"><span class="hljs-tag">++) </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">avgColor</span></span></span></span><span class="xml"><span class="hljs-tag"> += </span></span><span class="hljs-string"><span class="xml"><span class="hljs-tag"><span class="hljs-string">imgData[x][y];</span></span></span></span><span class="xml"><span class="hljs-tag"> </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">avgColor</span></span></span></span><span class="xml"><span class="hljs-tag"> /= </span></span><span class="hljs-string"><span class="xml"><span class="hljs-tag"><span class="hljs-string">AreaWidth</span></span></span></span><span class="xml"><span class="hljs-tag"> * </span></span><span class="hljs-attr"><span class="xml"><span class="hljs-tag"><span class="hljs-attr">AreaHeight</span></span></span></span><span class="xml"><span class="hljs-tag">;</span></span></span></span></code> </pre> <br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5.2 Gauss filter</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Now let's do the following: we will multiply the color of each neighbor by the value from the matrix whose dimension is equal to AreaWidth by AreaHeight. We will also ensure that the sum of all elements of the matrix is ‚Äã‚Äãequal to 1 - this will save us from having to divide by the size of the region, because now it will be a special case of the arithmetic average weighted. Such a matrix is ‚Äã‚Äãformally called the ‚ÄúConvolution Matrix‚Äù, also called the ‚ÄúCore‚Äù, and its elements are called ‚Äúweights‚Äù.</font></font> Why do you need it?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So we have more opportunities - by controlling the value of the scales, we can achieve, for example, the effect of pulsation or gradual blurring. There is also a whole family of filters based on the convolution matrix ‚Äî a clarity enhancement filter, a median filter, erosion filters, and a build-up. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The most common filter for blurring is a Gaussian filter. Its important property is linear separability - This allows us to first blur the input image in rows, then the image blurred in rows to blur in columns, performing one cycle with the values ‚Äã‚Äãof a one-dimensional filter, the formula of which looks like:</font></font><br><br><div style="text-align:center;"><img src="http://i68.tinypic.com/2vcyr95.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">where x is an integer from -AreaWidth / 2 to AreaWidth / 2, q is the so-called "Standard deviation of the Gaussian distribution" (the standard deviation of the Gaussian distribution) </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I implemented the function that forms the filter matrix:</font></font><br><br><pre> <code class="hljs cpp"><span class="hljs-keyword"><span class="hljs-keyword">typedef</span></span> <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::<span class="hljs-built_in"><span class="hljs-built_in">vector</span></span>&lt;<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>&gt; KernelStorage; <span class="hljs-function"><span class="hljs-function">KernelStorage </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">GetGaussianKernel</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(INT Radius, FLOAT Deviation)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> a = (Deviation == <span class="hljs-number"><span class="hljs-number">-1</span></span>) ? Radius * Radius : Deviation * Deviation; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> f = <span class="hljs-number"><span class="hljs-number">1.0f</span></span> / (sqrtf(<span class="hljs-number"><span class="hljs-number">2.0f</span></span> * D3DX_PI * a)); <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> g = <span class="hljs-number"><span class="hljs-number">2.0f</span></span> * a; <span class="hljs-function"><span class="hljs-function">KernelStorage </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">outData</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Radius * </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">2</span></span></span></span><span class="hljs-function"><span class="hljs-params"> + </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(INT x = -Radius; x &lt;= Radius; x++) outData[x + Radius] = f * expf(-(x * x) / a); <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> summ = <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::accumulate(outData.begin(), outData.end(), <span class="hljs-number"><span class="hljs-number">0.0f</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span> &amp;w : outData) w /= summ; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> outData; }</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I use a radius of 5, and the deviation is 5 squared. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5.3 Shader and everything connected with it.</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We will use two textures - one with the original data, the other for storing the intermediate result. </font><font style="vertical-align: inherit;">Create both textures with dimensions corresponding to the screen resolution and pixel format DXGI_FORMAT_R32G32B32A32_FLOAT. </font><font style="vertical-align: inherit;">You can, of course, not having lost much in quality, reduce the size of the textures, but in this case I decided not to. </font><font style="vertical-align: inherit;">We will work with textures according to the following scheme:</font></font><br><br><pre> <code class="hljs ruby">/<span class="hljs-regexp"><span class="hljs-regexp">/    { /</span></span><span class="hljs-regexp"><span class="hljs-regexp">/     } /</span></span><span class="hljs-regexp"><span class="hljs-regexp">/      { /</span></span><span class="hljs-regexp"><span class="hljs-regexp">/       }</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As before, we will work with a square in the NDC space, which occupies the entire screen area, with texture coordinates from 0 to 1. Now is the time to think about how to handle the output beyond the texture boundaries. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementing checks directly in the shader code is too resource intensive. Let's see what options we have if we still go beyond the boundaries of the area. As I said earlier, in this case, the rules specified in the D3D11_TEXTURE_ADDRESS_MODE enumeration come into play. The D3D11_TEXTURE_ADDRESS_CLAMP rule is appropriate. The following happens: each of the coordinates is limited to the range [0, 1]. That is, if we do a sample with coordinates (1.1, 0), we get the data of the texel with coordinates (1, 0), if we choose (-0.1, 0) for the coordinates, we get the data in (0, 0). The same for Y (see fig. 12).</font></font><br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/2ywj18k.png" alt="image"></div><br> <i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig.12 demonstration of D3D11_TEXTURE_ADDRESS_CLAMP with regard to the size of the filter</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The last thing left to know is how much we need to move in order to move one pixel in the texture coordinate space. This problem is solved simply - suppose that the screen resolution is 1024 by 768 pixels, then, for example, the center of the screen in space from 0 to 1 will be (512/1024, 384/768) = (0.5 0.5), and the point located by one pixel from the upper left corner - (1/1024, 1/768). You can also express the solution of this problem in the form of an equation. Let (Sx, Sy) be our starting position, (Ex, Ey) be the ending position, then the answer to the question ‚ÄúTo which part of the screen do we need to move in order to move from S to E?‚Äù Will look like this:</font></font><br><br><div style="text-align:center;"><img src="http://i63.tinypic.com/2e4i4j5.png" alt="image"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Suppose we want to move one pixel from the top left corner of the screen, then the equation will look like this: </font></font><br><br><div style="text-align:center;"><img src="http://i64.tinypic.com/2lndeg1.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Express for F and get: </font></font><br><br><div style="text-align:center;"><img src="http://i68.tinypic.com/2vt7cqh.png" alt="image"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Now, perhaps, we can give the pixel shader code: </font></font><br><br><pre> <code class="hljs pgsql">cbuffer Data : register(b0) { <span class="hljs-type"><span class="hljs-type">float4</span></span> weights[<span class="hljs-number"><span class="hljs-number">11</span></span>]; float2 texFactors; float2 padding; }; cbuffer Data2 : register(b1) { <span class="hljs-type"><span class="hljs-type">int</span></span> isVertical; float3 padding2; }; struct PIn { <span class="hljs-type"><span class="hljs-type">float4</span></span> posH : SV_POSITION; float2 tex : TEXCOORD0; }; Texture2D colorTex :register(t0); SamplerState colorSampler :register(s0); <span class="hljs-type"><span class="hljs-type">float4</span></span> ProcessPixel(PIn <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>) : SV_Target { float2 texOffset = (isVertical) ? float2(texFactors.x, <span class="hljs-number"><span class="hljs-number">0.0</span></span>f) : float2(<span class="hljs-number"><span class="hljs-number">0.0</span></span>f, texFactors.y); <span class="hljs-type"><span class="hljs-type">int</span></span> halfSize = <span class="hljs-number"><span class="hljs-number">5</span></span>; <span class="hljs-type"><span class="hljs-type">float4</span></span> avgColor = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-type"><span class="hljs-type">int</span></span> i = -halfSize; i &lt;= halfSize; ++i){ float2 texCoord = <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex + texOffset * i; avgColor += colorTex.Sample(colorSampler, texCoord) * weights[i + halfSize].x; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> avgColor; }</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The vertex shader did not give here, because nothing special happens there - just ‚Äúforward‚Äù the data further. </font><font style="vertical-align: inherit;">Let's see what we did:</font></font><br><br><div style="text-align:center;"><img src="http://i64.tinypic.com/5vppxv.png" alt="image"></div><br> <i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig.13 demonstration of blur without facets</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Not bad, but now we need to solve the problem of blurry edges. In our case, to make it easier than it might seem at first glance - after all, we already have all the necessary data! If we are in screen space, then it is enough for us to track the abrupt changes in the data of the normal and depth. If the scalar product of the normals or the absolute value of the difference between the depths of the neighboring and processed pixel is more or less than certain values, then we assume that the pixel being processed belongs to the line of the face. I wrote a small shader that highlights the faces in yellow. The principle of operation is the same as for blurring - we pass first vertically, then horizontally. We will compare the two neighbors either to the left and to the right, or at the top and bottom, depending on the direction. It turned out like this:</font></font><br><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">bool</span></span> CheckNeib(float2 Tc, <span class="hljs-type"><span class="hljs-type">float</span></span> DepthV, float3 NormalV) { <span class="hljs-type"><span class="hljs-type">float4</span></span> normalDepth = normalDepthTex.Sample(normalDepthSampler, Tc); <span class="hljs-type"><span class="hljs-type">float</span></span> neibDepthV = normalDepth.w; float3 neibNormalV = normalize(normalDepth.xyz); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> dot(neibNormalV, NormalV) &lt; <span class="hljs-number"><span class="hljs-number">0.8</span></span>f || abs(neibDepthV - DepthV) &gt; <span class="hljs-number"><span class="hljs-number">0.2</span></span>f; } <span class="hljs-type"><span class="hljs-type">float4</span></span> ProcessPixel(PIn <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>) : SV_Target { float2 texOffset = (isVertical) ? float2(<span class="hljs-number"><span class="hljs-number">0.0</span></span>f, texFactors.y) : float2(texFactors.x, <span class="hljs-number"><span class="hljs-number">0.0</span></span>f); <span class="hljs-type"><span class="hljs-type">float4</span></span> normalDepth = normalDepthTex.Sample(normalDepthSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex); <span class="hljs-type"><span class="hljs-type">float</span></span> depthV = normalDepth.w; float3 normalV = normalize(normalDepth.xyz); <span class="hljs-type"><span class="hljs-type">bool</span></span> onEdge = CheckNeib(<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex + texOffset, depthV, normalV) || CheckNeib(<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex - texOffset, depthV, normalV); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> onEdge ? <span class="hljs-type"><span class="hljs-type">float4</span></span>(<span class="hljs-number"><span class="hljs-number">1.0</span></span>f, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f, <span class="hljs-number"><span class="hljs-number">0.0</span></span>f, <span class="hljs-number"><span class="hljs-number">1.0</span></span>f).rgba : colorTex.Sample(colorSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex) ; }</code> </pre> <br><div style="text-align:center;"><img src="http://i68.tinypic.com/2ij3zpj.png" alt="image"></div><br> <i><font color="#999999"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 14 Demonstrating facet selection.</font></font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How do we use this when blurring?</font></font> Very simple!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We will not take into account the color of those neighbors that are very far away or significantly differ in the direction of the normals from the pixel, the color of which we consider. </font><font style="vertical-align: inherit;">Only now it is necessary to divide the result by the amount of weights processed. </font><font style="vertical-align: inherit;">Here is the code:</font></font><br><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">float4</span></span> ProcessPixel(PIn <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>) : SV_Target { float2 texOffset = (isVertical) ? float2(<span class="hljs-number"><span class="hljs-number">0.0</span></span>f, texFactors.y) : float2(texFactors.x, <span class="hljs-number"><span class="hljs-number">0.0</span></span>f); <span class="hljs-type"><span class="hljs-type">float4</span></span> normalDepth = normalDepthTex.SampleLevel(normalDepthSampler, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex, <span class="hljs-number"><span class="hljs-number">0</span></span>); <span class="hljs-type"><span class="hljs-type">float</span></span> depthV = normalDepth.w; float3 normalV = normalize(normalDepth.xyz); <span class="hljs-type"><span class="hljs-type">int</span></span> halfSize = <span class="hljs-number"><span class="hljs-number">5</span></span>; <span class="hljs-type"><span class="hljs-type">float</span></span> totalWeight = <span class="hljs-number"><span class="hljs-number">0.0</span></span>f; <span class="hljs-type"><span class="hljs-type">float4</span></span> totalColor = <span class="hljs-number"><span class="hljs-number">0.0</span></span>f; [unroll] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(<span class="hljs-type"><span class="hljs-type">int</span></span> i = -halfSize; i &lt;= halfSize; ++i){ float2 texCoord = <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.tex + texOffset * i; <span class="hljs-type"><span class="hljs-type">float4</span></span> normalDepth2 = normalDepthTex.SampleLevel(normalDepthSampler, texCoord, <span class="hljs-number"><span class="hljs-number">0</span></span>); <span class="hljs-type"><span class="hljs-type">float</span></span> neibDepthV = normalDepth2.w; float3 neibNormalV = normalize(normalDepth2.xyz); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(dot(neibNormalV, normalV) &lt; <span class="hljs-number"><span class="hljs-number">0.8</span></span>f || abs(neibDepthV - depthV) &gt; <span class="hljs-number"><span class="hljs-number">0.2</span></span>f) <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; <span class="hljs-type"><span class="hljs-type">float</span></span> weight = weights[halfSize + i].x; totalWeight += weight; totalColor += colorTex.Sample(colorSampler, texCoord) * weight; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> totalColor / totalWeight; }</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The result was this: </font></font><br><br><div style="text-align:center;"><img src="http://i63.tinypic.com/339t4ww.png" alt="image"></div><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 6. Unexpected changes </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When I was finishing work on this article, as a result of another test, I found out that the result of the overlap is unstable with respect to the camera direction. </font><font style="vertical-align: inherit;">In order to fix this, we need to translate the coordinates of the pixel being processed and the points from clause 3 from the view space to the world one. </font><font style="vertical-align: inherit;">In my case, the problem is aggravated by the fact that all the important data I store in the space of the form, so in a cycle over all the rays we need to convert either the depth of the point into the space of the form, or the depth from the tekstrura into the world space. </font><font style="vertical-align: inherit;">We also need to remember to transfer the normal to the world space, but not so critical, since it does not depend on the cycle.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 7. Conclusion </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I want to thank the reader for the attention to my article, and I hope that the information contained in it was accessible, interesting and useful. </font><font style="vertical-align: inherit;">I would also like to thank Leonid </font></font><a href="https://habrahabr.ru/users/forhaxed/" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ForhaxeD</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for his </font></font><a href="https://habrahabr.ru/post/248313"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">article</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - I took a lot from it and tried to improve it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The source code of the sample can be downloaded at </font></font><a href="https://github.com/AlexWIN32/SSAODemo"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexWIN32/SSAODemo</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Suggestions and comments regarding the work of the example as a whole or its individual subsystems can be sent to me </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">by mail</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or leave as comments. </font><font style="vertical-align: inherit;">I wish you success!</font></font><br><br><div class="spoiler">  <b class="spoiler_title">Annex 1</b> <div class="spoiler_text">         : <br><br><div style="text-align:center;"><img src="http://i67.tinypic.com/a3zjn9.png" alt="image"></div><br>  MinVal ‚Äî   , Range ‚Äî  , Factor ‚Äî ,      0  1.      NDC ,  ,    ,  ,       .  NDC   <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/efrtxy.png" alt="image"></div><br>   : <br><br><div style="text-align:center;"><img src="http://i64.tinypic.com/2zs47zo.png" alt="image"></div><br>   : <br><br><div style="text-align:center;"><img src="http://i68.tinypic.com/2ywb70o.png" alt="image"></div><br>   : <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/121fitt.png" alt="image"></div><br> , ,                ,   : <br><br><div style="text-align:center;"><img src="http://i67.tinypic.com/w8n8fb.png" alt="image"></div><br>      : <br><br><div style="text-align:center;"><img src="http://i67.tinypic.com/6i64gg.png" alt="image"></div><br><div style="text-align:center;"><img src="http://i65.tinypic.com/f3zuio.png" alt="image"></div><br>   : <br><br><div style="text-align:center;"><img src="http://i65.tinypic.com/j5jg49.png" alt="image"></div></div></div></div><p>Source: <a href="https://habr.com/ru/post/310360/">https://habr.com/ru/post/310360/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../310348/index.html">The number of sent and received letters by day</a></li>
<li><a href="../310352/index.html">Badoo opens source code Live Streaming Daemon</a></li>
<li><a href="../310354/index.html">The art of design opponents in the game Zelda: A Link to the Past</a></li>
<li><a href="../310356/index.html">The path of electricity in the data center</a></li>
<li><a href="../310358/index.html">Who, where and why they go: trends in mobile design 2016</a></li>
<li><a href="../310362/index.html">Simultaneous translation: how it works</a></li>
<li><a href="../310366/index.html">How we made even load balancing on the frontend cluster</a></li>
<li><a href="../310368/index.html">Jigsaw project in Java 9. A modular future that cannot be avoided</a></li>
<li><a href="../310370/index.html">TEST Labs 2016. Online conference for testers. September 24</a></li>
<li><a href="../310372/index.html">Examples of real patches in PostgreSQL: part 2 of N</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>