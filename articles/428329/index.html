<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Training with reinforcements: parse on video games</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the AI ‚Äã‚ÄãConference Vladimir Ivanov will tell about the application of training with reinforcement vivanov879 , Sr. Deep learning engineer in Nvidi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Training with reinforcements: parse on video games</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/webt/ls/va/9_/lsva9_rsrvkdmceogvse3sexdog.png"><br>  At the AI ‚Äã‚ÄãConference <b>Vladimir Ivanov</b> will tell about the application of training with reinforcement <b><a href="https://habr.com/users/vivanov879/" class="user_link">vivanov879</a> , Sr.</b>  <b>Deep learning engineer in Nvidia</b> .  The expert is engaged in machine learning in the testing department: ‚ÄúI analyze the data that we collect during the testing of video games and hardware.  For this I use machine learning and computer vision.  The main part of the work is image analysis, data cleaning before training, data marking and visualization of the solutions obtained. ‚Äù <br><br>  In today's article, Vladimir explains why reinforced training is used in autonomous cars and tells how an agent is trained to act in a changing environment ‚Äî using video game examples. <br><br>  In the past few years, humanity has accumulated a huge amount of data.  Some datasets spread in general access and mark manually.  For example, dataset CIFAR, where each picture is signed, to which class it belongs. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/16/ko/lp/16kolpeo1k3j1j9ppdh9vfjb6vc.jpeg"></div><br><br>  Datasets appear, where it is necessary to assign a class not just to the picture as a whole, but to each pixel in the image.  As, for example, in CityScapes. <br><br><img src="https://habrastorage.org/webt/eq/hz/b0/eqhzb0gt9v2demntxriobd5ewji.png"><br><br>  What unites these tasks is the fact that the learning neural network only needs to remember the patterns in the data.  Therefore, with sufficiently large amounts of data, and in the case of CIFAR it is 80 million images, the neural network is learning to generalize.  As a result, she copes well with the classification of pictures that she has never seen before. <br><br>  But acting within the framework of a teaching technique with a teacher that works for marking pictures, it is impossible to solve problems where we want not to predict the label, but to make decisions.  As, for example, in the case of autonomous driving, where the task is to safely and reliably get to the end point of the route. <a name="habracut"></a><br><br>  In the classification tasks, we used the technique of teaching with the teacher - when each picture was assigned a certain class.  But what if we do not have such a markup, but there is an agent and environment in which he can perform certain actions?  For example, let it be a video game, and we can click on the control arrows. <br><br><img src="https://habrastorage.org/webt/bn/-c/ad/bn-cadnljvu7relew1mfes6p0xw.jpeg"><br><br>  This kind of task is worth solving with the help of reinforcement training.  In the general formulation of the problem, we want to learn how to perform the correct sequence of actions.  It is crucial that the agent has the ability to perform actions again and again, thus exploring the environment in which he is located.  And instead of the correct answer, how to act in a given situation, he receives a reward for a correctly completed task.  For example, in the case of an autonomous taxi, the driver will receive a bonus for each completed trip. <br><br>  Let's return to a simple example - to a video game.  Take something unpretentious, for example, the Atari table tennis game. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hg/ev/0b/hgev0bl9kpvpzlllfzyyroentow.jpeg"></div><br><br>  We will manage the small plate on the left.  We will play against the computer-programmed rules of the player on the right.  Since we work with the image, and neural networks cope most successfully with the extraction of information from images, let's submit a picture to the input of a three-layer neural network with a core size of 3x3.  At the exit she will have to choose one of two actions: move the plank up or down. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mw/o_/5z/mwo_5zeixn2llke6ethglty43ao.png"></div><br><br>  We train the neural network to perform actions that lead to victory.  The technique of learning is as follows.  We let the neural network play a few rounds of table tennis.  Then we start to sort the games played.  In those games where she won, we mark the pictures with the label ‚ÄúUp‚Äù where she raised the racket, and ‚ÄúDown‚Äù where she lowered it.  In the lost games, we do the opposite.  We mark those pictures, where she lowered the plate, labeled ‚ÄúUp‚Äù, and where she raised - ‚ÄúDown‚Äù.  Thus, we reduce the task to the approach already known to us - learning with a teacher.  We have a set of pictures with tags. <br><br><img src="https://habrastorage.org/webt/al/16/48/al1648vo1sfp72qj_8n2fpif3va.png"><br><br>  Using this training technique, in a couple of hours our agent will learn how to beat the computer-programmed rules-based player. <br><br>  How to be in the case of autonomous driving?  The fact is that table tennis is a very simple game.  And it can produce thousands of frames per second.  Our network is now only 3 layers.  Therefore, the learning process is lightning.  The game generates a huge amount of data, and we instantly process them.  In the case of autonomous driving data to collect much longer and more expensive.  Cars are expensive, and from one car we will receive only 60 frames per second.  In addition, the cost of error increases.  In a video game, we could afford to play batch after batch at the very beginning of training.  But we can not afford to spoil the car. <br><br>  In this case, let's help the neural network at the very beginning of training.  We will fix the camera on the car, put an experienced driver in it and record photos from the camera.  For each picture we will sign the angle of the vehicle steering wheel.  We will teach the neural network to copy the behavior of an experienced driver.  Thus, we again reduced the task to the already well-known learning with the teacher. <br><br><img src="https://habrastorage.org/webt/pl/d0/oc/pld0oc75oafeojutvborl_upb8a.png"><br><br>  With a sufficiently large and diverse dataset, which will include different landscapes, seasons and weather conditions, the neural network will learn to precisely control the car. <br><br>  However, there is a problem with the data.  They are very long and expensive to collect.  Let's use the simulator, which will be implemented all the physics of the movement of the car - for example, DeepDrive.  We can learn in it, without fear of losing the car. <br><br><img src="https://habrastorage.org/webt/fe/c0/4v/fec04vmzbamtd60xxv3ypctf2ua.jpeg"><br><br>  In this simulator, we have access to all indicators of the car and the surrounding world.  In addition, all people, cars, their speeds and distances to them are marked up. <br><br><img src="https://habrastorage.org/webt/hq/37/1k/hq371k3xvab9kfcworznwlkelai.jpeg"><br><br>  From the point of view of the engineer in such a simulator, you can safely try new teaching techniques.  What should the researcher do?  For example, the student of various options for gradient descent in learning tasks with reinforcement.  To test a simple hypothesis, you do not want to shoot a cannon on sparrows and launch an agent in a complex virtual world, and then wait for days for the simulation results.  In this case, let's more efficiently use our computing power.  Let the agents be simpler.  Take, for example, the model of a spider on four legs.  In the Mujoco simulator, it looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yp/ad/dg/ypaddgwogfekvvjemaf3vseohdi.png"></div><br><br>  We assign him the task of running as fast as possible in a given direction ‚Äî for example, to the right.  The number of observed parameters for a spider is a 39-dimensional vector, where the position and velocity indicators of all its limbs are recorded.  Unlike the neural network for table tennis, where there was only one neuron at the exit, here at the exit there are eight (since the spider has 8 joints in this model). <br><br>  In such simple models one can test various hypotheses about the teaching technique.  For example, let's compare the speed of learning to run, depending on the type of neural network.  Let it be a single-layer neural network, a three-layer neural network, a convolutional network, and a recurrent network: <br><br><img src="https://habrastorage.org/webt/sq/zl/xu/sqzlxuj-k_nts13x7x5veh-56ay.png"><br><br>  The conclusion can be drawn as follows: since the spider model and the task are fairly simple, the results of the training are approximately the same for different models.  A three-layer network is too complicated, and therefore learns worse. <br><br>  Despite the fact that the simulator works with a simple spider model, depending on the task that is put before the spider, training can last for days.  In this case, let's animate several hundred spiders instead of one on the same surface and learn from the data we will receive from all.  So we will speed up the training several hundred times.  Here is an example of the Flex engine. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/p4/6l/4j/p46l4jyulhhspwa9e5z_4oe40oq.png"></div><br><br>  The only thing that has changed in terms of neural network optimization is data collection.  When we ran only one spider, we received data sequentially.  One run after another. <br><br><img src="https://habrastorage.org/webt/8t/cd/hr/8tcdhrcxqbx4mk3w99lacw2tfbu.png"><br><br>  Now it may happen that some spiders are just starting the race, while others have been running for a long time. <br><br><img src="https://habrastorage.org/webt/6u/i6/jv/6ui6jv9y3zmbdy3cwfweuv1drt4.png"><br><br>  We will take this into account during the optimization of the neural network.  Otherwise, everything remains the same.  As a result, we get acceleration in training hundreds of times, according to the number of spiders that are simultaneously on the screen. <br><br>  Once we have an effective simulator, let's try to solve more complex tasks.  For example, cross-country running. <br><br><img src="https://habrastorage.org/webt/zo/sq/dn/zosqdnbygiz_2p-cvxl4hvtuvqw.png"><br><br>  Since the environment in this case has become more aggressive, let us change and complicate tasks during the training.  It's hard to learn, but easy in battle.  For example, every few minutes to change the terrain.  In addition, let's give the agent more external influences.  For example, let's throw balls into it and turn the wind on and off.  Then the agent learns to run even on surfaces that he has never met.  For example, climb the stairs. <br><br><img src="https://habrastorage.org/webt/zc/-7/bq/zc-7bqzt5kcltfgrwfc9piw3mli.png"><br><br>  Since we have learned so effectively to run in simulators, let's test reinforcement training techniques in competitive disciplines.  For example, in shooters.  The VizDoom platform offers a world in which you can shoot, collect weapons and replenish health.  In this game we will also use a neural network.  Only now she will have five outs: four to move and one to fire. <br><br>  In order for the training to go effectively, let's approach this gradually.  From simple to complex.  At the entrance, the neural network receives an image, and before starting to do something conscious, it must learn to understand what the world consists of.  By learning simple scenarios, she will learn to understand what objects inhabit the world and how you can interact with them.  Let's start with a shooting gallery: <br><br><img src="https://habrastorage.org/webt/99/cy/xm/99cyxm9xxkmd3wg9zryiguyqnoe.png"><br><br>  Having mastered this scenario, the agent will figure out that there are enemies, and they should be shot, because you get points for them.  Then we will train him in a scenario where health is constantly decreasing, and you need to replenish it. <br><br><img src="https://habrastorage.org/webt/06/gs/jr/06gsjr-tuvruizcpmy3da0gsyq4.jpeg"><br><br>  Here he will learn that he has health, and it needs to be replenished, because in case of death, the agent receives a negative reward.  In addition, he will learn that if you move in the direction of the subject, it can be collected.  In the first scenario, the agent could not move. <br><br>  And in the final, third scenario, let's leave it to shoot with game-programmed bots from the game so that he can hone his skills. <br><br><img src="https://habrastorage.org/webt/dn/pz/iq/dnpziqqditttkptyp7zflcdh4ag.png"><br><br>  During training in this scenario, the correct selection of awards that an agent receives plays a very important role.  For example, if you give a reward only for defeated rivals, the signal will be very rare: if there are few players around, then we will receive points every few minutes.  So let's use a combination of awards that were before.  The agent will receive a reward for every useful action, whether it be improving health, selecting cartridges or hitting an opponent. <br><br><blockquote>  As a result, an agent trained with well-chosen rewards turns out to be stronger than his more computationally demanding opponents.  In 2016, such a system won the VizDoom competition with a margin of more than half the points scored from second place.  The second place team also used a neural network, only with a large number of layers and additional information from the game engine during training.  For example, information about whether there are enemies in the field of view of the agent. </blockquote><br>  We have examined approaches to solving problems where it is important to make decisions.  But many problems with this approach will remain unresolved.  For example, the quest game Montezuma Revenge. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s9/vp/_9/s9vp_9botnvsnfqcaxutbv-2pn0.jpeg"></div><br><br>  Here it is necessary to search for keys in order to open the doors to the next rooms.  We rarely get keys, and we open rooms even less often.  It is also important not to be distracted by foreign objects.  If we teach the system the way we did in the previous tasks, and give rewards for the beaten enemies, it will simply start knocking out the rolling skull again and again and will not explore the map.  About solving such problems, if interested, I can tell in a separate article. <br><br>  <b>You can listen to Vladimir Ivanov at the AI ‚Äã‚ÄãConference on November 22</b> .  Detailed program and tickets - on the <a href="https://bit.ly/2zbNZis">official website of the</a> event. <br><br>  Read the interview with Vladimir <a href="https://bit.ly/2Pw89xY">here</a> . </div><p>Source: <a href="https://habr.com/ru/post/428329/">https://habr.com/ru/post/428329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428317/index.html">React hooks - win or lose?</a></li>
<li><a href="../428319/index.html">"Yandex" was thinking about protection from acquisitions, if Sberbank allows</a></li>
<li><a href="../428321/index.html">Predictive data analytics - modeling and validation</a></li>
<li><a href="../428325/index.html">Roskomnadzor interested in leaking the base of employees of Sberbank</a></li>
<li><a href="../428327/index.html">What to do: the eIDAS European Electronic Identification Regulation</a></li>
<li><a href="../428331/index.html">Roscosmos has established the cause of the accident "Union-FG"</a></li>
<li><a href="../428333/index.html">The results of the AI ‚Äã‚Äãhackathon RAIF Hackathon 2018</a></li>
<li><a href="../428335/index.html">Siri Shortcut Update</a></li>
<li><a href="../428337/index.html">Entertaining javascript: no braces</a></li>
<li><a href="../428339/index.html">Do not automate it: bad business advice</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>