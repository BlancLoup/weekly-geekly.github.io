<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. IBM RamSan FlashSystem 820</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The basics of the topic we discussed earlier in the article " Testing flash storage. The theoretical part ." Today we will move on to practice. Our fi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. IBM RamSan FlashSystem 820</h1><div class="post__text post__text-html js-mediator-article">  The basics of the topic we discussed earlier in the article " <a href="http://itg-td.blogspot.com/2014/04/ibm-ramsan-flashsystem-820.html">Testing flash storage. The theoretical part</a> ."  Today we will move on to practice.  Our first patient will be the <a href="http://www.redbooks.ibm.com/abstracts/redp5027.html">IBM RamSan FlashSystem 820</a> .  Excellent working system, released in April 2013.  It was the top model until January of this year, giving way to the FlashSystem 840. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/31c/e65/fa1/31ce65fa186c47cf47882c408465894a.jpg" height="80" width="640"></div><br><h2>  <b>Testing method</b> </h2><br>  During testing, the following tasks were solved: <br><ul><li>  explore the process of storage performance degradation during long-term write write and read; </li><li>  explore the performance of IBM FlashSystem 820 storage systems under various load profiles; </li></ul><a name="habracut"></a><br><h4>  Testbed Configuration </h4><br>  Scheme and configuration of the stand are shown in the figure. <br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d6/fad/b99/9d6fadb99bea3b965d671be75ea0fb18.jpg"></div></td></tr><tr><td>  Figure 1 Block diagram of the test bench.  (clickable) </td></tr></tbody></table><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  As an additional software, Symantec Storage Foundation 6.1 with Hot Fix 6.1HF100 is installed on the test server, which implements: <br><ul><li>  Logical volume manager (Veritas Volume Manager); </li><li>  Vxfs file system; </li><li>  Functional fault-tolerant connection to disk arrays (Dynamic Multi Pathing) </li></ul><br>  On the test server, settings are made to reduce the disk I / O latency: <br><ul><li> Changed the I / O scheduler from <code>cfq</code> to <code>noop</code> by adding the kernel boot parameter <code>elevator=noop</code> ; </li><li>  Added a parameter in /etc/sysctl.conf that minimizes the queue size at the level of the Symantec logical volume manager: <code>vxvm.vxio.vol_use_rq = 0</code> ; </li><li>  Increased queue size on FC adapters;  by adding the options <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> to the configuration file /etc/modprobe.d/modprobe.conf <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> ; </li></ul><br>  On the storage configuration settings are performed on the partitioning of disk space: <br><ul><li>  The configuration of Flash modules of RAID5 (10 + P) &amp; HS is realized - the effective capacity is 9.37TiB; </li><li>  For all tests, except test 2 of the first group, 8 LUNs of the same volume are created with a total volume covering the entire usable capacity of the disk array.  For test 2, 8 LUNs of the same volume are created with a total volume covering 80% of the usable capacity of the disk array.  Block size LUN - 512 byte.  Created LUNs are presented to the test server. </li></ul><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (performance of synthetic tests) on the storage system, the Flexible IO Tester (fio) version 2.1.4 utility is used.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li>  thread = 0 </li><li>  direct = 1 </li><li>  group_reporting = 1 </li><li>  norandommap = 1 </li><li>  time_based = 1 </li><li>  randrepeat = 0 </li><li>  ramp_time = 6 </li></ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  iostat, part of the sysstat version 9.0.4 package with <code>txk</code> keys; </li><li>  vxstat, which is part of Symantec Storage Foundation 6.1 with <code>svd</code> keys; </li><li>  vxdmpadm, part of Symantec Storage Foundation 6.1 with the <code>-q iostat</code> keys; </li><li>  fio version 2.1.4, to generate a summary report for each load profile. </li></ul><br>  Removal of performance indicators during the test with the utilities iostat, vxstat, vxdmpstat is performed at 5 s intervals. <br></div></div><br><h3>  Testing program. </h3><br>  Testing consisted of 2 groups of tests.  Tests were performed by creating a synthetic load on the block device using fio, which is a <code>stripe, 8 column, stripe unit size=1MiB</code> logical volume <code>stripe, 8 column, stripe unit size=1MiB</code> , created using Veritas Volume Manager from 8 LUNs on the system under test and presented to the test server . <br><br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h4>  Group 1: Tests that implement long-term load. </h4>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When creating a test load, the following additional parameters of the fio program are used (except those already described): <br><ul><li>  rw = randwrite </li><li>  blocksize = 4K </li><li>  numjobs = 64 </li><li>  iodepth = 64 </li></ul><br>  Group 1 consists of four tests that differ in the total volume of LUNs presented with the tested storage system, the size of the block of input-output operations and the direction of input-output (write or read): <br><ul><li>  Test 1. Record test performed on a fully-marked storage system.  The total volume of the presented LUN is equal to the effective storage capacity of the storage system, the test duration is 12 hours; </li><li>  Test 2. Recording test performed on storage marked at 80%.  The total volume of the presented LUN is 80% of the effective storage capacity, the test duration is 5 hours. </li><li>  Test 3. Test for reading performed on a fully-marked storage system.  The test duration is 4 hours. </li><li>  Test 4. Recording tests with varying block size: 4,8,16,32,64,1024 KiB, performed on a fully-marked storage system, the duration of each test is 2 hours. </li></ul><br>  According to the test results, based on the data output by the vxstat team, the graphs are formed combining the results of two tests: <br><ul><li>  IOPS as a function of time; </li><li>  Latency, as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  the presence of performance degradation during long-term load on the record and reading; </li><li>  the degree of influence of the volume of the marked storage disk space on the performance; </li><li>  performance of storage service processes (garbage collection), limiting the performance of the disk array to write during a long peak load; </li><li>  the degree of influence of the size of the block of I / O operations on the performance of the storage service processes; </li><li>  the amount of space reserved on the storage system for leveling the impact of service processes. </li></ul><br><h4>  Group 2: Disk array performance tests with different types of load, executed at the block device level. </h4><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters <code>fio: randomrw, rwmixedread</code> ): </li></ul><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter <code>fio: blocksize</code> ); </li><li>  methods of processing I / O operations: synchronous, asynchronous (variable parameter in <code>fio: ioengine</code> software <code>fio: ioengine</code> ); </li><li>  the number of load generating processes: 1, 2, 4, 8, 16, 32, 64, 128 (changeable software parameter <code>fio: numjobs</code> ); </li><li>  queue depth (for asynchronous I / O operations): 32, 64 (variable parameter in <code>fio: iodepth</code> ). </li></ul><br>  A group consists of a set of tests representing all possible combinations of the above types of load.  To reduce the impact of garbage collection service processes on test results, a pause is implemented between tests equal to the amount of information recorded during the test, divided by the performance of the storage service processes (determined by the results of the first group of tests). <br><br>  Based on the test results, based on the output of fio software, upon completion of each test, graphs are generated for each combination of the following load types: load profile, method of processing I / O operations, queue depth, which combine tests with different I / O block values: <br><ul><li>  IOPS as a function of the number of load generating processes; </li><li>  Bandwidth as a function of the number of processes that generate the load; </li><li>  Latitude (clat) as a function of the number of load generating processes; </li></ul><br>  The analysis of the obtained results is carried out, conclusions are drawn on the load characteristics of the disk array at latency &lt;1ms. <br><br></div></div><br><h2>  <b>Test results</b> </h2><br>  Unfortunately, the scope of the article will not allow us to cite the entire amount of the data obtained, but we will certainly show you the basic materials. <br><br><h5>  Group 1: Tests that implement long-term load. </h5><br><div class="spoiler">  <b class="spoiler_title">1. With a long write load, a significant degradation of storage performance associated with the inclusion of garbage collection (GC) processes is recorded.</b>  <b class="spoiler_title">The performance of the disk array, fixed with running GC processes, can be considered as the maximum average performance of the disk array.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8e6/4ce/736/8e64ce736046a31d37494eb04a6ee3ed.jpg" height="251" width="640"></div></td></tr><tr><td>  Changing the speed of I / O operations (iops) during long-term 4K recording </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/027/203/409/027203409bd51aca016a5548c7a00b24.jpg" height="241" width="640"></div></td></tr><tr><td>  Changing delays during long-term 4K recording </td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">2. With a long reading load, the performance of the disk array does not change over time.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/798/fe6/f04/798fe6f04a1e12ec40af34bc69231d90.jpg" height="640" width="451"></div></td></tr><tr><td>  Changing performance parameters during long-term reading with a 4K block </td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">3. Storage density does not affect the operation of the disk array.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abe/e02/3bc/abee023bc9f657cd77bc7794a4221c27.jpg" height="480" width="640"></div></td></tr><tr><td>  Change in the speed of I / O operations (iops) and delays (Latency) during long-term recording at 100% and 80% storage density. </td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">4. The block size affects the performance during long-term load on the record, which results in a change in the point of a significant drop in storage performance and the amount of data recorded before the indicated moment.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b22/a2a/cd0/b22a2acd0f944c88e1358ff0ebd62ad6.jpg" height="640" width="452"></div></td></tr><tr><td>  Change of the I / O speed (iops) and data transfer rate (bandwidth) during long-term recording with various block sizes. </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9b3/722/f21/9b3722f21f5399d052b452991d19c695.png" height="316" width="640"></div></td></tr><tr><td>  The dependence of storage performance on block size during long-term recording load </td></tr></tbody></table><br></div></div><br><h5>  Group 2: Disk array performance tests with different types of load, executed at the block device level. </h5><br><div class="spoiler">  <b class="spoiler_title">Block device performance tables.</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/09d/82f/98e/09d82f98e2808c648039b39a2a4d8412.png" height="536" width="640"></div></td></tr><tr><td>  Storage performance with one load generating process (jobs = 1) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee7/d8d/3f7/ee7d8d3f73da11f8ffd3a083bf02b17d.png" height="542" width="640"></div></td></tr><tr><td>  Maximum storage performance with delays less than 1ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a9/a85/d18/7a9a85d181980fc2f6e4883bb7052d20.png" height="538" width="640"></div></td></tr><tr><td>  Maximum storage performance at a different load profile. </td></tr></tbody></table><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Block device performance graphs.</b> <div class="spoiler_text">  (All pictures are clickable) <br><br><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td><br></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ad0/379/e46/ad0379e4648ad857a00af919727330d1.jpg" height="200" width="141"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f29/e92/029/f29e92029483b89584345be62931faff.jpg" height="200" width="141"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eaa/6d2/e7d/eaa6d2e7dd953be0019cb6dbbc363933.jpg" height="200" width="141"></div><br></td></tr><tr><td>  With random recording </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4a2/53a/752/4a253a7528071f88a70a15c7e10ca8d0.jpg" height="200" width="141"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/669/cb8/fac/669cb8facf84844253941d2e1e02deef.jpg" height="200" width="141"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c7/501/b92/2c7501b920acef30a26d49ac65c1fba7.jpg" height="200" width="141"></div><br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a0/160/324/7a016032483d21703ab7b17954eaa3a8.jpg" height="200" width="141"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5a4/301/d50/5a4301d50d019ca751c69a693dc512a0.jpg" height="200" width="141"></div><br><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fe6/554/dff/fe6554dffbc7fdaadacfd93feeec135f.jpg" height="200" width="141"></div><br><br></td></tr></tbody></table><br></div></div><br><h4>  Maximum recorded performance parameters for FlashSystem 820: </h4><br>  Record: <br><ul><li>  180,000 IOPS with latency 0.2ms (4KB sync block) </li><li>  Bandwidth: 1520MB / c for 1MB block </li></ul><br>  Reading: <br><ul><li>  306,000 IOPS with latency of 0.8ms (8KB async qd 32 block); </li><li>  453,000 IOPS with latency 2.2ms (4KB async qd 64 block); </li><li>  Bandwidth: 3150MB / s for the 1MB block. </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  280,000 IOPS with latency 0.8ms (4KB async qd 32 block); </li><li>  390,000 IOPS with latency 2.6ms (4KB async qd 64 block); </li><li>  Bandwidth 2430MB / s for 1MB block </li></ul><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.08ms for 4K block, jobs = 8 with a performance of 100,000 IOPS; </li><li>  With a reading of 0.166ms for a 4K block, jobs = 16 with a capacity of 95,000 IOPS. </li></ul><br>  When the system enters the saturation mode on the read load, there is a decrease in the performance of the storage system as the load increases (this is probably related to the overhead of the storage system for processing large I / O queues). <br><br><h2>  <b>findings</b> </h2><br>  According to the subjective impressions and the sum of indicators, FlashSystem 820 turned out to be an excellent working tool.  The data obtained by us, in general, coincide with the <a href="http://www.redbooks.ibm.com/redpapers/pdfs/redp5027.pdf">declared by the manufacturer</a> .  Of the significant differences, only lower write performance can be noted, which can be explained by the different configuration of test benches.  We used RAID-5.  IBM most likely uses the standard RAID-0 algorithm. <br><br>  The disadvantages are, perhaps, the lack of additional functions standard for an enterprise storage system, such as snapshot, replication, deduplication, etc.  On the other hand, all these are only additional benefits, far from always necessary. <br><br>  I hope you were just as interesting as me. <br>  Soon I will write about <a href="http://habrahabr.ru/topic/edit/231057/">Violin Memory 6000</a> and HDS HUS VM.  At different stages - testing of several more systems from different manufacturers.  If circumstances allow, I hope to familiarize you with their results soon. <br><br><br>  <font color="green">PS The author expresses cordial thanks to Pavel Katasonov, Yuri Rakitin and all other company employees who participated in the preparation of this material.</font> </div><p>Source: <a href="https://habr.com/ru/post/227887/">https://habr.com/ru/post/227887/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../227875/index.html">IT Worker Desktop: Robots, Gadgets, and Dinosaur Skull</a></li>
<li><a href="../227879/index.html">New Imaging SDK and SensorCore SDK for Lumia are already available for download.</a></li>
<li><a href="../227881/index.html">Development of cross-browser extensions</a></li>
<li><a href="../227883/index.html">Indexing Android application content is now available to all developers.</a></li>
<li><a href="../227885/index.html">Testing flash storage. Theoretical part</a></li>
<li><a href="../227893/index.html">Install and configure openchange under CentOS 6.5</a></li>
<li><a href="../227897/index.html">Open networks and switches without OS</a></li>
<li><a href="../227903/index.html">Linux dominates supercomputers like never before</a></li>
<li><a href="../227907/index.html">Any Packt Publishing eBook can be purchased for $ 10.</a></li>
<li><a href="../227911/index.html">As I wrote checkers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>