<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Big Data from A to Z. Part 4: Hbase</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Finally, the long-awaited fourth article in our cycle about big data. In this article we will talk about such a wonderful tool like Hbase, w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Big Data from A to Z. Part 4: Hbase</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  Finally, the long-awaited fourth article in our cycle about big data.  In this article we will talk about such a wonderful tool like Hbase, which has recently gained great popularity: for example, Facebook <a href="https://www.facebook.com/notes/facebook-engineering/the-underlying-technology-of-messages/454991608919/">uses</a> it as the basis of its messaging system, and we use hbase as the main raw data storage for our platform in the <a href="http://datacentric.ru/">data-centric alliance</a> Data Management <a href="http://facetz.net/">Facetz.DCA</a> <br><br>  The article will cover the Big Table concept and its free implementation, features and difference from both classic relational databases (such as MySQL and Oracle), and key-value repositories, such as Redis, Aerospike, and memcached. <br>  Interested?  Welcome under cat. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d8/2ad/35c/2d82ad35c1616888fddbe49190c56fd8.png"></div><br><a name="habracut"></a><br><h2>  Who and why came up with Hbase </h2><br>  As usual - let's start with the background.  Like many other projects from the BigData field, Hbase originated from a concept that was developed by Google.  The principles underlying Hbase were described in the article " <a href="http://static.googleusercontent.com/media/research.google.com/ru//archive/bigtable-osdi06.pdf">Bigtable: A Distributed Storage System for Structured Data</a> ". 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      As we have seen in <a href="https://habrahabr.ru/company/dca/blog/267361/">previous</a> <a href="https://habrahabr.ru/company/dca/blog/270453/">articles</a> , regular files are pretty good for batch processing using the MapReduce paradigm. <br><br>  On the other hand, the information stored in files is rather inconvenient to update;  Files are also deprived of random access.  For fast and convenient random access, there is a class of nosql-systems such as key-value storage, such as <a href="http://www.aerospike.com/">Aerospike</a> , <a href="http://redis.io/">Redis</a> , <a href="http://www.couchbase.com/">Couchbase</a> , <a href="https://memcached.org/">Memcached</a> .  However, usually in these systems batch processing is very inconvenient.  Hbase is an attempt to combine the convenience of batch processing and the convenience of updating and random access. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cdb/4c1/75f/cdb4c175f0b8bf9683becdf9dceaaf5c.png"></div><br><br><h2>  Data model </h2><br>  Hbase is a distributed, column-oriented, multi-version key-value database. <br>  The data is organized into <font color="#0000ff"><b>tables</b></font> indexed by the primary key, which in Hbase is called <font color="#00d100"><b>RowKey</b></font> . <br>  For each <font color="#00d100"><b>RowKey</b></font> key, an unlimited set of <font color="#00d9d9"><b>attributes (or columns)</b></font> can be stored. <br><br>  <font color="#00d9d9">Columns are</font> organized into <font color="#7ca8dc"><b>groups of columns</b></font> called <font color="#7ca8dc"><b>Column Family</b></font> .  As a rule, columns with the same usage and storage pattern are combined into one <font color="#7ca8dc"><b>Column Family</b></font> . <br><br>  For each <font color="#00d9d9"><b>attribute</b></font> can be stored several different <font color="#6aa84f"><b>versions</b></font> .  Different versions have different <font color="#ff9900"><b>timestamp</b></font> . <br><br>  Records are physically stored in the order sorted by <font color="#00d100"><b>RowKey</b></font> .  In this case, the data corresponding to different <font color="#7ca8dc"><b>Column Family</b></font> are stored separately, which allows, if necessary, to read data only from the desired column family. <br><br>  When you physically delete a certain <font color="#00d9d9"><b>attribute,</b></font> it is not immediately removed physically, but is only marked with a special <font color="#ff00ff"><b>tombstone</b></font> flag <b>.</b>  Physical deletion of data will occur later when performing a Major Compaction operation. <br><br>  <font color="#00d9d9"><b>Attributes</b></font> belonging to the same <font color="#7ca8dc"><b>column group</b></font> and corresponding to the same <font color="#00d100"><b>key are</b></font> physically stored as a sorted list.  Any <font color="#00d9d9"><b>attribute</b></font> may be missing or present for each key, while if the <font color="#00d9d9"><b>attribute is</b></font> missing, it does not cause the overhead of storing null values. <br><br>  The list and names <font color="#7ca8dc"><b>of the column groups are</b></font> fixed and have a clear outline.  At the column group level, parameters such as time to live (TTL) and the maximum number of stored <font color="#6aa84f"><b>versions are set</b></font> .  If the difference between the <font color="#ff9900"><b>timestamp</b></font> for a specific <font color="#6aa84f"><b>version</b></font> and the current time is more than the TTL - the record is marked <font color="#ff00ff"><b>for deletion</b></font> .  If the number of <font color="#6aa84f"><b>versions</b></font> for a particular <font color="#00d9d9"><b>attribute has</b></font> exceeded the maximum number of versions - the entry is also marked <font color="#ff00ff"><b>for deletion</b></font> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/72f/db4/418/72fdb44187d02c8affdc9740eb691115.png"></div><br>  The Hbase data model can be remembered as matching key value: <br><br><blockquote>  &lt; <font color="#0000ff"><b>table</b></font> , <font color="#00d100"><b>RowKey</b></font> , <font color="#7ca8dc"><b>Column Family</b></font> , <font color="#00d9d9"><b>Column</b></font> , <font color="#ff9900"><b>timestamp</b></font> &gt; -&gt; <font color="#6aa84f"><b>Value</b></font> </blockquote><br><h2>  Supported operations </h2><br>  The list of supported operations in hbase is quite simple.  4 basic operations are supported: <br><br>  <b>- Put</b> : add a new entry to hbase.  The timestamp of this record can be set manually, otherwise it will be set automatically as the current time. <br><br>  <b>- Get</b> : get data on a specific RowKey.  You can specify the Column Family, from which we will take the data and the number of versions that we want to read. <br><br>  <b>- Scan</b> : read records one by one.  You can specify the record with which we start reading, the record before which to read, the number of records that must be read, the Column Family from which reading will be made, and the maximum number of versions for each record. <br><br>  <b>- Delete</b> : mark a specific version for deletion.  There will be no physical deletion; it will be delayed until the next Major Compaction (see below). <br><br><h2>  Architecture </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/c0d/1a9/217/c0d1a9217b4790f891aaa591cec266de.png"><br><br>  Hbase is a distributed database that can run on dozens and hundreds of physical servers, ensuring uninterrupted operation, even if some of them fail.  Therefore, the hbase architecture is quite complex compared to classic relational databases. <br><br>  Hbase uses two main processes for its work: <br><br>  <b>1. Region Server</b> - serves one or more <b>regions.</b>  A region is a range of records corresponding to a specific range of consecutive RowKey.  Each region contains: <br><br><ul><li>  <b>Persistent Storage</b> - the main data storage in Hbase.  Data is physically stored on HDFS, in a special <b>HFile</b> format.  The data in HFile is stored in an order sorted by RowKey.  One pair (region, column family) corresponds to at least one HFIle. <br><br></li><li>  <b>MemStore</b> - write buffer.  Since the data is stored in HFile d in a sorted order, it is quite expensive to update HFile for each record.  Instead, the data when recording falls into a special memory area MemStore, where they accumulate for some time.  When filling MemStore to a certain critical value, the data is written to the new HFile. <br><br></li><li>  <b>BlockCache</b> - read cache.  Allows you to significantly save time on data that is read frequently. <br><br></li><li>  <b>Write Ahead Log (WAL).</b>  Since the data when recording falls into the memstore, there is some risk of data loss due to failure.  In order for this not to happen, all operations before the actual implementation of the manipulations get into a special log file.  This allows you to recover data after any failure. </li></ul><br>  <b>2. Master Server</b> - the main server in the hbase cluster.  The master manages the distribution of regions across Region Servers, keeps a register of regions, manages the launch of regular tasks and does other useful work. <br><br>  To coordinate actions between services, Hbase uses <a href="https://zookeeper.apache.org/">Apache ZooKeeper</a> , a special service for managing configurations and synchronization of services. <br><br>  As the amount of data in a region increases and it reaches a certain size, Hbase starts split, the operation that divides the region by 2. In order to avoid permanent divisions of the regions, you can pre-set the boundaries of the regions and increase their maximum size. <br><br>  Since data for one region can be stored in several HFiles, Hbase periodically merges them together to speed up the work.  This operation in Hbase is called <b>compaction.</b>  Compactions are of two types: <br><br><ul><li>  <b>Minor Compaction.</b>  Runs automatically, runs in the background.  It has a low priority compared to other Hbase operations. <br><br></li><li>  <b>Major Compaction.</b>  It is started by hands or upon the occurrence of certain triggers (for example, by timer).  It has a high priority and can significantly slow down the work of the cluster.  <b>Major Compaction</b> 's is best done during a time when the cluster load is low.  During Major Compaction, physical deletion of data labeled with tombstone also occurs. <br></li></ul><br><h2>  Ways to work with Hbase </h2><br><h3>  Hbase shell </h3><br>  The easiest way to get started with Hbase is to use the <b>hbase shell</b> utility.  It is available immediately after installing hbase on any hbase cluster node. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/388/e01/39a/388e0139a820a105d0a8df4b65575780.png"><br><br>  Hbase shell is a jruby-console with built-in support for all basic operations with Hbase.  Below is an example of creating a users table with two column families, performing some manipulations with it and deleting the table at the end in the hbase shell language: <br><br><div class="spoiler">  <b class="spoiler_title">Bed sheet code</b> <div class="spoiler_text"><pre><code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">create</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, {<span class="hljs-keyword"><span class="hljs-keyword">NAME</span></span> =&gt; <span class="hljs-string"><span class="hljs-string">'user_profile'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">VERSIONS</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">5</span></span>}, {<span class="hljs-keyword"><span class="hljs-keyword">NAME</span></span> =&gt; <span class="hljs-string"><span class="hljs-string">'user_posts'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">VERSIONS</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">1231231231</span></span>} put <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:name'</span></span>, <span class="hljs-string"><span class="hljs-string">'alexander'</span></span> put <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span>, <span class="hljs-string"><span class="hljs-string">'alexander'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span> put <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span>, <span class="hljs-string"><span class="hljs-string">'kuznetsov'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span>, {<span class="hljs-keyword"><span class="hljs-keyword">COLUMN</span></span> =&gt; <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">VERSIONS</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">5</span></span>} put <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id2'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:name'</span></span>, <span class="hljs-string"><span class="hljs-string">'vasiliy'</span></span> put <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id2'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span>, <span class="hljs-string"><span class="hljs-string">'ivanov'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">scan</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, {<span class="hljs-keyword"><span class="hljs-keyword">COLUMN</span></span> =&gt; <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">VERSIONS</span></span> =&gt; <span class="hljs-number"><span class="hljs-number">5</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">delete</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span>, <span class="hljs-string"><span class="hljs-string">'user_profile:second_name'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span>, <span class="hljs-string"><span class="hljs-string">'id1'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">disable</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">drop</span></span> <span class="hljs-string"><span class="hljs-string">'users'</span></span></code> </pre> </div></div><br><h3>  Native api </h3><br>  Like most other hadoop-related projects, hbase is implemented in the java language, so the native api is available for the java language.  Native API is pretty well documented on the <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/package-summary.html">official site</a> .  Here is an example of using the Hbase API taken from the same place: <br><br><div class="spoiler">  <b class="spoiler_title">Bed sheet code</b> <div class="spoiler_text"><pre> <code class="hljs actionscript"><span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> java.io.IOException;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.HBaseConfiguration;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.TableName;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Connection;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.ConnectionFactory;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Get;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Table;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Put;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Result;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.ResultScanner;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.client.Scan;</span></span> <span class="hljs-meta"><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">import</span></span></span><span class="hljs-meta"> org.apache.hadoop.hbase.util.Bytes;</span></span> <span class="hljs-comment"><span class="hljs-comment">// Class that has nothing but a main. // Does a Put, Get and a Scan against an hbase table. // The API described here is since HBase 1.0. public class MyLittleHBaseClient { public static void main(String[] args) throws IOException { // You need a configuration object to tell the client where to connect. // When you create a HBaseConfiguration, it reads in whatever you've set // into your hbase-site.xml and in hbase-default.xml, as long as these can // be found on the CLASSPATH Configuration config = HBaseConfiguration.create(); // Next you need a Connection to the cluster. Create one. When done with it, // close it. A try/finally is a good way to ensure it gets closed or use // the jdk7 idiom, try-with-resources: see // https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html // // Connections are heavyweight. Create one once and keep it around. From a Connection // you get a Table instance to access Tables, an Admin instance to administer the cluster, // and RegionLocator to find where regions are out on the cluster. As opposed to Connections, // Table, Admin and RegionLocator instances are lightweight; create as you need them and then // close when done. // Connection connection = ConnectionFactory.createConnection(config); try { // The below instantiates a Table object that connects you to the "myLittleHBaseTable" table // (TableName.valueOf turns String into a TableName instance). // When done with it, close it (Should start a try/finally after this creation so it gets // closed for sure the jdk7 idiom, try-with-resources: see // https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html) Table table = connection.getTable(TableName.valueOf("myLittleHBaseTable")); try { // To add to a row, use Put. A Put constructor takes the name of the row // you want to insert into as a byte array. In HBase, the Bytes class has // utility for converting all kinds of java types to byte arrays. In the // below, we are converting the String "myLittleRow" into a byte array to // use as a row key for our update. Once you have a Put instance, you can // adorn it by setting the names of columns you want to update on the row, // the timestamp to use in your update, etc. If no timestamp, the server // applies current time to the edits. Put p = new Put(Bytes.toBytes("myLittleRow")); // To set the value you'd like to update in the row 'myLittleRow', specify // the column family, column qualifier, and value of the table cell you'd // like to update. The column family must already exist in your table // schema. The qualifier can be anything. All must be specified as byte // arrays as hbase is all about byte arrays. Lets pretend the table // 'myLittleHBaseTable' was created with a family 'myLittleFamily'. p.add(Bytes.toBytes("myLittleFamily"), Bytes.toBytes("someQualifier"), Bytes.toBytes("Some Value")); // Once you've adorned your Put instance with all the updates you want to // make, to commit it do the following (The HTable#put method takes the // Put instance you've been building and pushes the changes you made into // hbase) table.put(p); // Now, to retrieve the data we just wrote. The values that come back are // Result instances. Generally, a Result is an object that will package up // the hbase return into the form you find most palatable. Get g = new Get(Bytes.toBytes("myLittleRow")); Result r = table.get(g); byte [] value = r.getValue(Bytes.toBytes("myLittleFamily"), Bytes.toBytes("someQualifier")); // If we convert the value bytes, we should get back 'Some Value', the // value we inserted at this location. String valueStr = Bytes.toString(value); System.out.println("GET: " + valueStr); // Sometimes, you won't know the row you're looking for. In this case, you // use a Scanner. This will give you cursor-like interface to the contents // of the table. To set up a Scanner, do like you did above making a Put // and a Get, create a Scan. Adorn it with column names, etc. Scan s = new Scan(); s.addColumn(Bytes.toBytes("myLittleFamily"), Bytes.toBytes("someQualifier")); ResultScanner scanner = table.getScanner(s); try { // Scanners return Result instances. // Now, for the actual iteration. One way is to use a while loop like so: for (Result rr = scanner.next(); rr != null; rr = scanner.next()) { // print out the row we found and the columns we were looking for System.out.println("Found row: " + rr); } // The other approach is to use a foreach loop. Scanners are iterable! // for (Result rr : scanner) { // System.out.println("Found row: " + rr); // } } finally { // Make sure you close your scanners when you are done! // Thats why we have it inside a try/finally clause scanner.close(); } // Close your table and cluster connection. } finally { if (table != null) table.close(); } } finally { connection.close(); } } }</span></span></code> </pre> </div></div><br><h3>  Thrift, REST and support for other programming languages. </h3><br>  To work from other programming languages, Hbase provides the <a href="https://wiki.apache.org/hadoop/Hbase/ThriftApi">Thrift API</a> and <a href="http://hbase.apache.org/book.html">Rest API</a> .  Based on them, clients for all major programming languages ‚Äã‚Äãare built: <a href="https://github.com/wbolster/happybase">python</a> , <a href="https://github.com/pop/pop_hbase">PHP</a> , <a href="https://github.com/alibaba/node-hbase-client">Java Script,</a> and so on. <br><br><h2>  Some features of working with HBase </h2><br>  <b>1.</b> Hbase out of the box is integrated with MapReduce, and can be used as input and output data with the help of special <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html">TableInputFormat</a> and <a href="https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableOutputFormat.html">TableOutputFormat</a> . <br><br>  <b>2.</b> It is very important to choose the right RowKey.  RowKey should ensure a good even distribution across regions, otherwise there is a risk of so-called ‚Äúhot regions‚Äù - regions that are used much more often than others, which leads to inefficient use of system resources. <br><br>  <b>3.</b> If data is not uploaded singlely, but immediately in large batches - Hbase supports a special <a href="http://blog.cloudera.com/blog/2013/09/how-to-use-hbase-bulk-loading-and-why/">BulkLoad</a> mechanism, which allows you to fill in data much faster than using single Put'y.  BulkLoad is essentially a two-step operation: <br><br>  - Formation of HFile without the participation of put'ov using a special MapReduce job'a <br><br>  - Putting these files directly in Hbase. <br><br>  <b>4.</b> Hbase supports the <a href="http://www.cloudera.com/documentation/enterprise/5-5-x/topics/admin_hbase_ganglia.html">output of its metrics</a> to the monitoring server Ganglia.  This can be very useful when administering Hbase to understand the nature of the problems with hbase. <br><br><h2>  Example </h2><br>  As an example, we can consider the main data table that we use in <a href="http://datacentric.ru/">Data-Centric Aliance</a> to store information about user behavior on the Internet. <br><br><h3>  Rowkey </h3><br>  As the RowKey, the user ID is used, which is the <a href="https://en.wikipedia.org/wiki/Globally_unique_identifier">GUUID</a> , a <a href="https://en.wikipedia.org/wiki/Globally_unique_identifier">string</a> specially generated to be unique throughout the world.  GUUIDs are evenly distributed, which gives a good distribution of data across servers. <br><br><h3>  Column family </h3><br>  Our repository uses two column families: <br><br>  - <b>Data</b> .  This column group stores data that is no longer relevant for advertising purposes, such as when a user visits a specific URL.  The TTL for this Column Family is set at 2 months, the limit on the number of versions is 2000. <br><br>  - <b>LongData</b> .  This column group stores data that does not lose its relevance for a long time, such as gender, date of birth, and other ‚Äúeternal‚Äù user characteristics. <br><br><h3>  Columns </h3><br>  Each type of user fact is stored in a separate column.  For example, in the Data: _v column, the URLs visited by the user are stored, and in the LongData: gender column ‚Äî the gender of the user. <br><br>  The timestamp of the registration of this fact is stored as the timestamp.  For example, in the Data column: _v - as the timestamp, the time the user uses to access a specific URL is used. <br><br>  This user data storage structure fits our usage pattern very well and allows you to quickly update user data, quickly retrieve all the necessary information about users, and, using MapReduce, quickly process all users' data at once. <br><br><h2>  Alternatives </h2><br>  Hbase is quite complicated in administration and use, so before using hbase it makes sense to pay attention to alternatives: <br><br><ul><li>  <b>Relational databases</b> .  Very good alternative, especially in the case when the data fit on one machine.  Also, first of all, relational databases should be considered in the case when transaction indices other than primary are important. <br><br></li><li>  <b>Key-Value Storage</b> .  Repositories such as <a href="http://redis.io/">Redis</a> and <a href="http://www.aerospike.com/">Aerospike are</a> better suited when minimizing latency is needed and batch processing is less important. <br><br></li><li>  <b>Files and their processing using MapReduce.</b>  If the data is only added, and rarely updated / changed, then it is better not to use Hbase, but simply to store the data in files.  To simplify work with files, you can use tools such as Hive, Pig and Impala, which will be discussed in the following articles. </li></ul><br><h2>  Hbase checklist </h2><br>  Using Hbase is justified when: <br><br>  - There is a lot of data and they do not fit on one computer <br>  - Data is frequently updated and deleted. <br>  - In the data there is an obvious "key" for which it is convenient to tie everything else <br>  - Need batch processing <br>  - We need random access to data on certain keys <br><br><h2>  Conclusion </h2><br>  In this article, we looked at Hbase - a powerful tool for storing and updating data in the hadoop ecosystem; we showed the Hbase data model, its architecture, and the specifics of working with it. <br><br>  The following articles will focus on tools that simplify working with MapReduce, such as <a href="https://hive.apache.org/">Apache Hive</a> and <a href="https://pig.apache.org/">Apache Pig</a> . <br><br>  <a href="https://www.youtube.com/channel/UCOvuB83CWNZ0yz8qeNpWIIQ">Youtube Channel about data analysis</a> <br><br><h2>  Links to other articles of the cycle </h2><br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/267361/">Big Data from A to Z. Part 1: Principles of working with big data, the MapReduce paradigm</a> <br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/268277/">Big Data from A to Z. Part 2: Hadoop</a> <br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/270453/">Big Data from A to Z. Part 3: Techniques and strategies for developing MapReduce-applications</a> </div><p>Source: <a href="https://habr.com/ru/post/280700/">https://habr.com/ru/post/280700/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../280688/index.html">Security Week 13: crypto-parade parade, FBI hacked iPhone without Apple‚Äôs help; more details about Badlock</a></li>
<li><a href="../280690/index.html">Improving work in the console or as I wrote the sshcdvim command</a></li>
<li><a href="../280692/index.html">PyCon Russia moves to Moscow</a></li>
<li><a href="../280694/index.html">On which framework will you write a PHP application in 2016?</a></li>
<li><a href="../280696/index.html">RUVDS launches affiliate program</a></li>
<li><a href="../280702/index.html">HPE 3PAR StoreServ cloud array monitoring service</a></li>
<li><a href="../280706/index.html">Blacklists: Cyber ‚Äã‚ÄãDefense in the Age of Advanced Sustainable Threats</a></li>
<li><a href="../280708/index.html">IBM's cognitive service recognizes a photo and tells what is shown in the pictures.</a></li>
<li><a href="../280710/index.html">Automate Personnel Changes on PowerShell</a></li>
<li><a href="../280712/index.html">Bank payment infrastructure protection</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>