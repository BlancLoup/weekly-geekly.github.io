<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Lossy image compression</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The idea behind all lossy compression algorithms is quite simple: in the first stage, delete irrelevant information, and in the second stage, apply th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Lossy image compression</h1><div class="post__text post__text-html js-mediator-article">  The idea behind all lossy compression algorithms is quite simple: in the first stage, delete irrelevant information, and in the second stage, apply the most appropriate lossless compression algorithm to the remaining data.  The main difficulty lies in the allocation of this irrelevant information.  The approaches here vary significantly depending on the type of compressible data.  For sound, frequencies that a person is simply not able to perceive are most often removed, the sampling rate is reduced, and some algorithms remove quiet sounds immediately following loud ones, only moving objects are encoded for video data, and minor changes on fixed objects are simply discarded.  Methods for extracting irrelevant information on images will be discussed in detail later. <br><a name="habracut"></a><br><h1>  Quality criteria for compressed images </h1><br>  Before talking about lossy compression algorithms, it is necessary to agree on what is considered acceptable loss.  It is clear that the main criterion is the visual assessment of the image, but also changes in the compressed image can be quantified.  The easiest way to estimate is to calculate the immediate difference between the compressed and original images.  We agree that under <img src="https://habrastorage.org/files/7ed/bb7/75a/7edbb775a26b44f0ba3b352429707ddf.PNG">  we will understand the pixel located at the intersection of the i-th row and the j-th column of the original image, and by <img src="https://habrastorage.org/files/307/6cd/cee/3076cdcee8fb4002b3971beb7b272f4c.PNG">  - the corresponding pixel of the compressed image.  Then for any pixel you can easily determine the coding error <img src="https://habrastorage.org/files/5ba/215/f41/5ba215f41c534e1d95f9ac2dc801469d.PNG">  , and for the entire image consisting of N rows and M columns, it is possible to calculate the total error <img src="https://habrastorage.org/files/18c/7a1/b39/18c7a1b392a54dc793f85849efb8b86e.PNG">  .  Obviously, the larger the total error, the greater the distortion in the compressed image.  However, this value is extremely rarely used in practice, because  It does not take into account the size of the image.  Much more widely applied estimate using the standard deviation <img src="https://habrastorage.org/files/f16/72d/710/f1672d710dfb4a65854b725339dd485d.PNG">  . <br>  Another (albeit similar in essence) approach is as follows: the pixels of the final image are considered as the sum of the pixels of the original image and noise.  The quality criterion for this approach is called the signal-to-noise ratio (SNR), calculated as follows: <img src="https://habrastorage.org/files/188/65e/a4d/18865ea4d9ad480583e9849ba2e73300.PNG">  . <br>  Both of these estimates are so-called.  objective criteria of fidelity reproduction, because  depend solely on the original and compressed image.  However, these criteria do not always correspond to subjective assessments.  If the images are intended for human perception, the only thing that can be argued is that poor indicators of objective criteria most often correspond to low subjective evaluations, while good indicators of objective criteria do not guarantee high subjective evaluations. <br><br><h1>  Compression through quantization and discretization </h1><br>  The concept of visual redundancy is associated with the quantization and discretization process.  Much of the information in the image can not be perceived by man: for example, a person is able to notice slight differences in brightness, but is much less sensitive to chromaticity.  Also, starting from a certain moment, increasing the sampling accuracy does not affect the visual perception of the image.  Thus, some of the information can be deleted without compromising visual quality.  This information is called visually redundant. <br>  The easiest way to remove visual redundancy is to reduce the sampling accuracy, but in practice this method can only be used for images with a simple structure, because  distortions that occur in complex images are too noticeable (see. Table 1) <br><br><img src="https://habrastorage.org/files/45c/85f/467/45c85f467ec3432591a6df722a46dac7.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To remove redundant information, quantization accuracy is often reduced, but it cannot be reduced thoughtlessly, since  This leads to a sharp deterioration in image quality. <br>  Consider the image already known to us with.  Suppose that the image is represented in the RGB color space, the results of encoding this image with reduced quantization accuracy are presented in Table.  2 <br><br><img src="https://habrastorage.org/files/677/0be/a09/6770bea093734dfd841a91855fd2d303.PNG"><br><br>  In the above example, uniform quantization is used as the easiest way, but if it is important to preserve color reproduction more accurately, you can use one of the following approaches: either use uniform quantization, but choose the mean of the segment, but the expected value of the segment, but use uneven splitting of the entire range of brightness. <br><br>  Having carefully studied the obtained images, one can notice that on the compressed images there are distinct false contours, which significantly impair visual perception.  There are methods based on transferring the quantization error to the next pixel, which can significantly reduce or even completely remove these contours, but they lead to image noise and graininess.  These shortcomings severely limit the direct use of quantization for image compression. <br><br>  Most modern methods for removing visually abundant information use information about the features of human vision.  Everyone knows the different sensitivity of the human eye to information about the color and brightness of the image.  Tab.  3 shows an image in the YIQ color space, encoded with different quantization depth of the color difference signals IQ: <br><br><img src="https://habrastorage.org/files/fd0/66f/9bf/fd066f9bf12045bfb8443ea4f9299515.PNG"><br><br>  As can be seen from Tab.  3, the quantization depth of color difference signals can be lowered from 256 to 32 levels with minimal visual changes.  At the same time, the losses in the I and Q components are very significant and are shown in Table.  four <br><br><img src="https://habrastorage.org/files/070/5b7/28f/0705b728f02349b0b7333593fd61eb23.PNG"><br><br>  Despite the simplicity of the methods described, in their pure form they are rarely used, most often they serve as one of the steps of more efficient algorithms. <br><br><h1>  Predictive coding </h1><br>  We have already considered predictive coding as a very efficient method of lossless data compression.  If you combine coding with prediction and compression by quantization, you get a very simple and effective lossy image compression algorithm.  In this method, the prediction error will be quantized.  It is the accuracy of its quantization that will determine both the compression ratio and the distortions introduced into the compressed image.  The choice of optimal algorithms for prediction and for quantization is a rather difficult task.  In practice, the following universal (ensuring acceptable quality on most images) predictors are widely used: <br><br><img src="https://habrastorage.org/files/934/98a/9c0/93498a9c0d0d48af96e421a27c1e0d37.PNG"><br><br>  We, in turn, take a detailed look at the simplest and at the same time very popular lossy coding method - the so-called.  delta modulation.  This algorithm uses prediction based on one previous pixel, i.e. <img src="https://habrastorage.org/files/c42/5c9/08a/c425c908ae524668bba8424f5ab2a708.PNG">  .  The error obtained after the prediction stage is quantized as follows: <br><br><img src="https://habrastorage.org/files/976/43f/2ee/97643f2eeddf417fa8ef85622f5be259.PNG"><br><br>  At first glance, this is a very crude method of quantization, but it has one indisputable advantage: the result of the prediction can be encoded with a single bit.  Tab.  Figure 5 shows images encoded with delta modulation (row-wrap) with different Œæ values. <br><br><img src="https://habrastorage.org/files/d90/6b1/247/d906b1247f2443d2afb159e74e0b45e2.PNG"><br><br>  Two types of distortion are most noticeable - blurring of contours and a certain graininess of the image.  This is the most typical distortion arising due to overload on the slope and because of the so-called.  noise granularity.  Such distortions are characteristic of all lossy coding variants with prediction, but using the example of delta modulation they are best seen. <br>  Noise of granularity arises mainly on monochromatic areas, when the value of Œæ is too large to correctly display small fluctuations in brightness (or their absence).  In Fig.  1, the yellow line indicates the original brightness, and the blue bars indicate the noise that occurs when quantizing the prediction error. <br><br><img src="https://habrastorage.org/files/3f1/417/31a/3f141731a4ae4b89bdfddaa02380d2a5.png"><br><br>  The situation of overload on a slope is fundamentally different from the situation with noise of granularity.  In this case, the value of Œæ is too small to transmit a sharp brightness difference.  Due to the fact that the brightness of the encoded image can not grow as fast as the brightness of the original image, there is a noticeable blur of contours.  The situation with overload on the slope is explained in Fig.  2 <br><br><img src="https://habrastorage.org/files/e16/895/944/e16895944c034c2ab25d23413b793bd0.png"><br><br>  It is easy to see that the granularity noise decreases with decreasing Œæ, but at the same time the distortions due to the overload due to the steepness grow and vice versa.  This leads to the need to optimize the value of Œæ.  For most images, it is recommended to choose Œæ‚àà [5; 7]. <br><br><h1>  Transformational coding </h1><br>  All the previously discussed methods of image compression worked directly with the pixels of the original image.  This approach is called spatial coding.  In the current section, we consider a fundamentally different approach, called transformational coding. <br><br>  The basic idea of ‚Äã‚Äãthe approach is similar to the previously considered compression method using quantization, but it is not the brightness values ‚Äã‚Äãof the pixels that will be quantized, but the conversion factors (transformations) calculated in a special way.  Diagrams of transformational compression and image recovery are shown in Fig.  3, Fig.  four. <br><br><img src="https://habrastorage.org/files/032/e87/339/032e8733960d4b9098b34e46b1c8f3a1.PNG"><br><br>  Direct compression does not occur at the time of encoding, but at the time of quantizing the coefficients, since  for most real-life images, most of the coefficients can be roughly quantized. <br><br>  To obtain the coefficients, reversible linear transformations are used: for example, a Fourier transform, a discrete cosine transform, or a Walsh-Hadamard transform.  The choice of a particular transformation depends on the permissible level of distortion and the resources available. <br><br>  In the general case, an image with the dimensions N * N of pixels is considered as a discrete function of two arguments I (r, c), then the direct transformation can be expressed as follows: <br><br><img src="https://habrastorage.org/files/2f4/ec7/752/2f4ec7752ab840ba95a42274c8d76083.PNG"><br><br>  Set <img src="https://habrastorage.org/files/ef0/34d/a4a/ef034da4a293413f9b2ddda656d6bfa5.PNG">  just is the desired set of coefficients.  Based on this set, you can restore the original image using the inverse transform: <br><br><img src="https://habrastorage.org/files/72f/6de/5e1/72f6de5e1be7451397ea7cc987b80cfb.PNG"><br><br>  Functions <img src="https://habrastorage.org/files/f31/02b/dfd/f3102bdfda864dd5a88b780138e61231.PNG">  They are called transformation kernels, and the function g is the kernel of the direct transformation, and the function h is the kernel of the inverse transformation.  The choice of transformation core determines both the compression efficiency and the computational resources required to perform the conversion. <br><br><h2>  Widespread transformation cores </h2><br>  Most often, transformational coding uses the kernels listed below. <br><br><img src="https://habrastorage.org/files/457/945/829/4579458290544ece9d2afbaea0b2eb11.PNG"><br><br>  The use of the first two nuclei of these nuclei leads to a simplified version of the discrete Fourier transform.  The second pair of cores corresponds to the often used Walsh-Hadamard transform.  Finally, the most common transform is the discrete cosine transform: <br><br>  The prevalence of discrete cosine transform is due to its good ability to compact the image energy.  Strictly speaking, there are more efficient conversions.  For example, the Karhunen-Loeve transformation has the best efficiency in terms of energy compaction, but its complexity practically excludes the possibility of wide practical application. <br><br>  The combination of energy compression efficiency and comparative ease of implementation has made the discrete cosine transformation standard transformation coding. <br><br><h2>  Graphic explanation of transformational coding </h2><br>  In the context of functional analysis, the considered transformations can be considered as an expansion over a set of basis functions.  At the same time, in the context of image processing, these transformations can be perceived as decomposition into basis images.  To clarify this idea, consider a square image I of size n * n pixels.  The transformation cores do not depend on the transformation coefficients, nor on the pixel values ‚Äã‚Äãof the original image, so the inverse transformation can be written in matrix form: <br><br><img src="https://habrastorage.org/files/177/a7c/18c/177a7c18c87941588c46d42e3434d3c8.PNG"><br><br>  If these matrices are arranged in the form of a square, and the obtained values ‚Äã‚Äãare presented in the form of pixels of a certain color, you can get a graphical representation of the basis functions, i.e.  basic images.  The basic images of the Walsh-Hadamard transform and the discrete cosine transform for n = 4 are given in Table.  7 <br><br><img src="https://habrastorage.org/files/28d/1e7/a1a/28d1e7a1acad45c189f67c7f387b0c6d.PNG"><br><br><h2>  Features of the practical implementation of transformational coding </h2><br>  The general idea of ‚Äã‚Äãtransformational coding was described earlier, but, nevertheless, the practical implementation of transformational coding requires clarification of several issues. <br><br>  Before encoding the image is divided into square blocks, and then each block is subjected to conversion.  Both the computational complexity and the final compression efficiency depend on the choice of the block size.  As the block size increases, both compression efficiency and computational complexity increase, so in practice blocks of 8 * 8 or 16 * 16 pixels are most often used. <br><br>  After the transformation coefficients are calculated for each block, it is necessary to quantize and encode them.  The most common approaches are zonal and threshold coding. <br><br>  In the zonal approach, it is assumed that the most informative coefficients will be located near the zero indices of the resulting array.  This means that the corresponding coefficients must be most accurately quantized.  Other coefficients can be quantized much rougher or simply discarded.  The easiest way to understand the idea of ‚Äã‚Äãzonal coding is by looking at the appropriate mask (Table 8): <br><br><img src="https://habrastorage.org/files/fcb/bdc/925/fcbbdc925cab4a348de712d59a07abc3.png"><br><br>  The number in the cell indicates the number of bits reserved for encoding the corresponding coefficient. <br>  Unlike zone coding, where the same mask is used for every block, threshold coding implies the use of a unique mask for each block.  The threshold mask for the block is built on the basis of the following considerations: the most important factors for restoring the original image are the coefficients with the maximum value, therefore, keeping these coefficients and zeroing all the others, it is possible to provide both a high compression ratio and an acceptable quality of the reconstructed image.  Tab.  9 shows an exemplary mask view for a specific block. <br><br><img src="https://habrastorage.org/files/04e/110/7b4/04e1107b46fa461d9542f06e33cecb41.png"><br><br>  The units correspond to the coefficients that will be stored and quantized, and the zeros correspond to the coefficients that are thrown out. After applying the mask, the resulting matrix of coefficients must be converted into a one-dimensional array, and a snake bypass must be used for the conversion.  Then, in the resulting one-dimensional array, all significant coefficients will be concentrated in the first half, and the second half will consist almost entirely of zeros.  As was shown in earlier, such sequences are very efficiently compressed by run-length coding algorithms. <br><br><h1>  Wavelet compression </h1><br>  Wavelets - mathematical functions for analyzing the frequency components of data.  In problems of information compression, wavelets are used relatively recently, however, researchers managed to achieve impressive results. <br><br>  Unlike the transformations discussed above, wavelets do not require a preliminary splitting of the original image into blocks, but can be applied to the image as a whole.  In this section, wavelet compression will be explained using a fairly simple Haar wavelet as an example. <br><br>  First, consider the Haar transform for a one-dimensional signal.  Let there be a set S of n values; in the Haar transformation, each pair of elements is assigned two numbers: the half-sum of the elements and their half-difference.  It is important to note that this transformation is reversible: i.e.  from a pair of numbers you can easily restore the original pair.  In Fig.  5 shows an example of a one-dimensional Haar transform: <br><br><img src="https://habrastorage.org/files/ee9/398/6d6/ee93986d63f34482ad661a572e3fa7c0.PNG"><br><br>  It can be seen that the signal is divided into two components: an approximate value of the original (with a resolution reduced by two times) and clarifying information. <br><br>  The two-dimensional Haar transform is a simple composition of one-dimensional transforms.  If the source data is presented in the form of a matrix, then a transformation is first performed for each row, and then a transformation is performed for the resulting matrices for each column.  In Fig.  6 shows an example of a two-dimensional Haar transform. <br><br><img src="https://habrastorage.org/files/3e9/4f1/bd1/3e94f1bd1a404eb38004981cfe5a303c.PNG"><br><br>  The color is proportional to the value of the function at the point (the higher the value, the darker).  As a result of the transformation, four matrices are obtained: one contains an approximation of the original image (with a reduced sampling frequency), and the other three contain clarifying information. <br><br>  Compression is achieved by removing some of the coefficients from the refinement matrices.  In Fig.  7 shows the recovery process and the reconstructed image itself after deleting from the refinement matrixes of coefficients of small modulus: <br><br><img src="https://habrastorage.org/files/569/9d0/f92/5699d0f92d594ac896e3f7771f32e3c9.PNG"><br><br>  It is obvious that the representation of the image with the help of wavelets allows you to achieve effective compression, while maintaining the visual quality of the image.  Despite its simplicity, the Haar transform is relatively rarely used in practice, since  There are other wavelets that have several advantages (for example, Daubechay wavelets or biorthogonal wavelets). <br><br>  PS Other chapters of the monograph are freely available on <a href="http://gorkoff.ru/">gorkoff.ru</a> </div><p>Source: <a href="https://habr.com/ru/post/251417/">https://habr.com/ru/post/251417/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251407/index.html">How to create a subdomain in VestaCP</a></li>
<li><a href="../251409/index.html">How to set up a LibGDX project with Gradle, Google Play Services</a></li>
<li><a href="../251411/index.html">Why learn programming so damn hard?</a></li>
<li><a href="../251413/index.html">How to make a game for iOS from an empty plastic bottle, pictures and shader</a></li>
<li><a href="../251415/index.html">Flask. Fill the "flask" functionality</a></li>
<li><a href="../251419/index.html">Generating textures of planets using the Fault Formation algorithm</a></li>
<li><a href="../251421/index.html">We increase the stability of the front-end</a></li>
<li><a href="../251423/index.html">Intel IoT Meet-up in Nizhny Novgorod</a></li>
<li><a href="../251425/index.html">Patterns Don't Stop at Design - Be a Pattern-Driven Team</a></li>
<li><a href="../251427/index.html">Getting to know GStreamer: items and containers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>