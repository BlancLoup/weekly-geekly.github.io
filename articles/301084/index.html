<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The evolution of neural networks for image recognition in Google: GoogLeNet</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I have a VM synchronized here for a long time, so I have time to talk about what I recently read. 
 For example, about GoogLeNet. 
 GoogLeNet is the f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The evolution of neural networks for image recognition in Google: GoogLeNet</h1><div class="post__text post__text-html js-mediator-article"><p>  I have a VM synchronized here for a long time, so I have time to talk about what I recently read. <br>  For example, about GoogLeNet. <br>  GoogLeNet is the first incarnation of the so-called Inception architecture, which is clear to everyone that: </p><br><p><img src="http://jonfriesen.ca/content/images/2013/Oct/deeper.jpg" alt="image"><br>  (By the way, the link to it goes first in the list of references of the article, dude harness) </p><br><p>  She won the ImageNet recognition challenge in 2014 with a score of 6.67% top 5 error.  Let me remind you, top 5 error is a metric in which the algorithm can produce 5 variants of a picture class and an error is counted if among all these variants there is no correct one.  There are a total of 150K pictures and 1000 categories in the test dataset, that is, the task is extremely nontrivial. </p><br><p>  To understand why, how and why GoogLeNet is arranged, as usual, a bit of context. </p><a name="habracut"></a><br><p>  <em>Disclaimer: The post is written on the basis of the edited chat logs <a href="http://closedcircles.com/%3Finvite%3D24f9c0846734449fdafe18c58354f5fb4a8aa0da">closedcircles.com</a> , hence the style of presentation, and clarifying questions.</em> </p><br><h2>  In 2012, an epoch-making event takes place - ImageNet challenge wins deep convolutional network </h2><br><p>  And not only wins, but shows an error almost two times less than the second place (15% vs 26% top5 error) <br>  (to show the development of the region, the current top result is 3%) <br>  The grid is called AlexNet by the name of Alex Krizhevsky, a Hinton student.  It has only 8 levels (5 convolutional and 3 fully-connected), but in general it is thick and zyrnaya - as much as 60M parameters.  Her training does not fit into a single GPU with 3GB of memory and Alex already has to come up with a trick on how to train this on two GPUs. </p><br><h2>  And now people at Google are working to make it more practical. </h2><br><p>  For example, in order to be able to use it on smaller devices and in general. <br>  We love GoogLeNet not so much for accuracy as for efficiency in the size of the model and the required number of calculations. <br>  Actually paper - <a href="">http://arxiv.org/abs/1409.4842</a> . </p><br><p>  Their main ideas are: </p><br><ul><li>  The original AlexNet did large convolutions that require a lot of parameters, we will try to do smaller convolutions with a lot of handlers. </li><li>  And then we will aggressively reduce the number of measurements to compensate for thicker layers.  It is clever to do this with the help of 1x1 convolutions - in fact, a linear filter used throughout the picture to take the current number of measurements, and mix them linearly into a smaller one.  Since he also learns, it turns out very effectively. </li><li>  At each level, we will run several convolution kernels of different sizes in order to pull features of different scale.  If the scale is too large for the current level, it is recognized at the next. </li><li>  We do not make hidden FC layers at all, because they have <em>a</em> lot of parameters.  Instead, at the last level we do the global average pool and hook it to the output layer directly. </li></ul><br><p>  This is what one "inception" module looks like: <br><img src="https://habrastorage.org/files/449/171/7f8/4491717f88c34940b67947c1bc769bcd.png" alt="image"></p><br><p>  The very same kernels of different sizes are visible, 1x1 convolutions are visible to reduce the dimension. </p><br><p>  And here the network consists of 9 such blocks.  In this design, there are about 10 times fewer parameters than in AlexNet, and it is also calculated faster, because the dimensionality reduction works well. </p><br><p>  And then it turned out that it also actually classifies pictures better - as it was written above, 6.67% top5 error. </p><br><p><img src="https://habrastorage.org/files/e3a/dbb/092/e3adbb0922d04ed48283246d24955cce.png" alt="image"><br>  Here is a picture of the full network.  It looks scary, but when you realize that these are repetitive blocks, simpler. </p><br><h2>  What other details to tell ... </h2><br><p>  She has three training heads (yellow squares) - this was done to make it easier to train such a deep network.  In each additional training head there are some FC layers that predict the same class based on low levels, so that the signal reaches the lower levels more quickly (although in the following papers it turned out that they help rather because they are additional regularization). <br>  In the release, everything leading to the auxiliary training heads is discarded.  This technique is used elsewhere in the literature, but since then we have learned how to train deepnets better, which is why it is often necessary. </p><br><p>  Such an architecture, besides GoogLeNet itself, is called Inception-v1. </p><br><p>  Inception-BN is the same mesh, only trained using <a href="http://arxiv.org/abs/1502.03167">Batch Normalization</a> (this <a href="http://opendatascience.ru/%25D1%2587%25D1%2582%25D0%25BE-%25D1%2582%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25B5-batch-normalization-%25D0%25BD%25D0%25B0-%25D0%25BF%25D0%25B0%25D0%25BB%25D1%258C%25D1%2586%25D0%25B0%25D1%2585/">is a</a> good explanation on the fingers). <br>  And Inception-v2 and further are already more complex architectures, about which I will tell next time, otherwise they can start feeding soon. </p><br><p>  "Le" in GoogLeNet is a reference to LeNet 5, the first grid published by LeKun before deep learning was a thing. </p><br><blockquote>  <em>About the compression of networks, I also recently read something.</em>  <em>They take the net there, cut off the excess weight from it, the net is reduced by a factor of one hundred, and accuracy is almost not affected.</em>  <em>That is, it seems, right from gigabytes to megabytes, you can shove it into the memory of a mobile phone.</em>  <em>I feel another ten years and each camera will start to see for real.</em> <br>  <em>About compression paper, by the way, if you're interested - <a href="http://arxiv.org/abs/1510.00149">http://arxiv.org/abs/1510.00149</a> .</em> <br>  Yeah.  This game is a little different level. <br>  You can optimize at the level of architecture and training, and you can at a low level - working already with learned weights. <br>  Most likely, in practice, you need both. <br><br>  <em>By the way, the question of space.</em> <br>  <em>Is it possible to draw a global conclusion from this?</em> <br>  <em>Why does this all work?</em>  <em>Or at least - how best to design the network with this experience?</em> <br>  Great questions about this will be a lot of meat in the next part of the story.  Stay tuned! </blockquote></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/301084/">https://habr.com/ru/post/301084/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../301072/index.html">The skin of the unkilled unicorn: legal clashes with a billionaire startup Cruise</a></li>
<li><a href="../301076/index.html">How much is the fish?</a></li>
<li><a href="../301078/index.html">City hackathon 2GIS in Moscow</a></li>
<li><a href="../301080/index.html">Microsoft has released the second service pack for Windows 7</a></li>
<li><a href="../301082/index.html">Development of the Kinetic Novels: costs, income, statistics, tips, post factum conclusions</a></li>
<li><a href="../301086/index.html">Google turns its data centers into works of art.</a></li>
<li><a href="../301092/index.html">Urho3D: Post Effects</a></li>
<li><a href="../301094/index.html">Deterministic number factorization method based on mod 6 and mod 4</a></li>
<li><a href="../301096/index.html">Recognize faces in the photo using Python and OpenCV</a></li>
<li><a href="../301098/index.html">ACM ICPC Live Webcast: How it works</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>