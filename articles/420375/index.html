<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learn OpenGL. Lesson 5.8 - Bloom</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Bloom 
 Due to the limited range of brightness available to conventional monitors, the task of convincingly displaying bright light sources and bright...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learn OpenGL. Lesson 5.8 - Bloom</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/web/c9e/9b2/a3b/c9e9b2a3baf749ab8e2b385c6d93d966.png" alt="Ogl3" width="300"><h2>  Bloom </h2><br>  Due to the limited range of brightness available to conventional monitors, the task of convincingly displaying bright light sources and brightly lit surfaces is complex by definition.  One of the most common methods to highlight bright areas on the monitor is a technique that adds a halo of glow around bright objects, giving the impression that the light is spreading beyond the light source.  As a result, the observer has the impression of a high brightness of such illuminated areas or light sources. <br><br>  The described effect of the halo and the light coming out of the source is achieved by a post-processing technique called a <i>bloom</i> .  Applying an effect adds a characteristic halo of glow to all bright areas of the displayed scene, which can be seen in the example below: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oi/qw/mj/oiqwmjiua0ogznqfllr0q9v53fc.png"></div><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">Content</b> <div class="spoiler_text">  Part 1. Start 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  <a href="https://habrahabr.ru/post/310790/">Opengl</a> </li><li>  <a href="https://habrahabr.ru/post/311198/">Creating a window</a> </li><li>  <a href="https://habrahabr.ru/post/311234/">Hello window</a> </li><li>  <a href="https://habrahabr.ru/post/311808/">Hello triangle</a> </li><li>  <a href="https://habrahabr.ru/post/313380/">Shaders</a> </li><li>  <a href="https://habrahabr.ru/post/315294/">Textures</a> </li><li>  <a href="https://habrahabr.ru/post/319144/">Transformations</a> </li><li>  <a href="https://habrahabr.ru/post/324968/">Coordinate systems</a> </li><li>  <a href="https://habrahabr.ru/post/327604/">Camera</a> </li></ol><br>  Part 2. Basic lighting <br><br><ol><li>  <a href="https://habrahabr.ru/post/329592/">Colors</a> </li><li>  <a href="https://habrahabr.ru/post/333932/">Lighting Basics</a> </li><li>  <a href="https://habrahabr.ru/post/336166/">Materials</a> </li><li>  <a href="https://habrahabr.ru/post/337550/">Texture Cards</a> </li><li>  <a href="https://habrahabr.ru/post/337642/">Sources of light</a> </li><li>  <a href="https://habrahabr.ru/post/338254/">Multiple light sources</a> </li></ol><br>  Part 3. Loading 3D Models <br><br><ol><li>  <a href="https://habrahabr.ru/post/338436/">Assimp library</a> </li><li>  <a href="https://habrahabr.ru/post/338436/">Mesh mesh class</a> </li><li>  <a href="https://habrahabr.ru/post/338998/">3D model class</a> </li></ol><br>  Part 4. OpenGL advanced features <br><br><ol><li>  <a href="https://habrahabr.ru/post/342610/">Depth test</a> </li><li>  <a href="https://habrahabr.ru/post/344238/">Stencil test</a> </li><li>  <a href="https://habrahabr.ru/post/343096/">Mixing colors</a> </li><li>  <a href="https://habrahabr.ru/post/346964/">Face clipping</a> </li><li>  <a href="https://habrahabr.ru/post/347354/">Frame buffer</a> </li><li>  <a href="https://habrahabr.ru/post/347750/">Cubic cards</a> </li><li>  <a href="https://habrahabr.ru/post/350008/">Advanced data handling</a> </li><li>  <a href="https://habrahabr.ru/post/350156/">Advanced GLSL</a> </li><li>  <a href="https://habrahabr.ru/post/350782/">Geometric shader</a> </li><li>  <a href="https://habrahabr.ru/post/352962/">Instancing</a> </li><li>  <a href="https://habrahabr.ru/post/351706/">Smoothing</a> </li></ol><br>  Part 5. Advanced Lighting <br><br><ol><li>  <a href="https://habrahabr.ru/post/353054/">Advanced lighting.</a>  <a href="https://habrahabr.ru/post/353054/">Model Blinna-Phong.</a> </li><li>  <a href="https://habrahabr.ru/post/353632/">Gamma Correction</a> </li><li>  <a href="https://habrahabr.ru/post/353956/">Shadow maps</a> </li><li>  <a href="https://habr.com/post/354208/">Omnidirectional shadow maps</a> </li><li>  <a href="https://habr.com/post/415579/">Normal mapping</a> </li><li>  <a href="https://habr.com/post/416163/">Parallax mapping</a> </li><li>  <a href="https://habr.com/post/420409/">Hdr</a> </li><li>  <b>Bloom</b> </li></ol><br></div></div><br>  Bloom adds to the image a well-visible visual hint about the significant brightness of objects covered by the halo from the applied effect.  Being applied selectively and in a verified volume (with which many games, alas, do not cope), the effect can significantly improve the visual expressiveness of the lighting used in the scene, as well as add drama in certain situations. <br><br>  This technique works in conjunction with <a href="https://habr.com/post/420409/">HDR-</a> rendering almost as a matter of course.  Apparently, because of this, many people mistakenly mix these two terms up to complete interchangeability.  However, these techniques are completely independent and are used for different purposes.  It is quite possible to implement bloom using the default frame buffer with an 8bit color depth, just like applying HDR rendering without using bloom.  The only thing is that the HDR-rendering allows you to realize the effect in a more efficient way (we will see this later). <br><br>  To implement the bloom, the lit scene is first rendered in the usual way.  Next, HDR color buffers and a color buffer containing only bright areas of the scene are extracted.  This extracted image of bright areas is then blurred and superimposed over the original HDR image of the scene. <br><br>  To clarify the process step by step.  Render a scene containing 4 bright light sources rendered as color cubes.  All of them have a brightness value in the range from 1.5 to 15.0.  If you output to the HDR color buffer, the result is as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_2/_h/wn/_2_hwnque0owtvpdcyh_vo_p9pg.png"></div><br>  From this HDR color buffer, we extract all fragments whose brightness exceeds the specified limit.  It turns out an image containing only brightly lit areas: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/q7/jb/uz/q7jbuz_9apwuc9cb2jzpe4a-4si.png"></div><br>  Further this image of bright sites is blurred.  The severity of the effect is essentially determined by the strength and radius of the blur filter applied: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/5u/cv/b7/5ucvb73pzpcbvbrn2pcp5khu1_i.png"></div><br>  The resulting blurred image of bright areas is the basis of the final halo effect around bright objects.  This texture is simply mixed with the original HDR image of the scene.  As the bright areas were blurred, their sizes increased, which ultimately gives a visual effect of luminosity beyond the boundaries of the light sources: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wq/pt/lx/wqptlxwywvag8dzh0dck64yzrbg.png"></div><br>  As you can see, bloom is not the most sophisticated technique, however, to achieve its high visual quality and reliability is not always easy.  For the most part, the effect depends on the quality and type of blur filter applied.  Even small changes in the filter parameters can dramatically change the final quality of the equipment. <br><br>  So, the above steps give us a step-by-step algorithm for the post-processing effect for the bloom effect.  The image below summarizes the necessary actions: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/t7/kt/pz/t7ktpzzm8bo_ccpmh70uu5x0rye.png"></div><br>  First of all, we will need information about bright areas of the scene based on a predetermined threshold value.  And this will do. <br><br><h2>  Removing bright areas </h2><br>  So, first we need to get two images based on our scene.  It would be naive to render the render twice, but use the more advanced method of multiple render targets ( <i>Multiple Render Targets</i> , <i>MRT</i> ): we set more than one output in the final fragment shader and thanks to this, the extraction of two images can be done in one pass!  To specify which color buffer the shader will be output to, the <i>layout</i> specifier is used: <br><br><pre><code class="cpp hljs">layout (location = <span class="hljs-number"><span class="hljs-number">0</span></span>) out vec4 FragColor; layout (location = <span class="hljs-number"><span class="hljs-number">1</span></span>) out vec4 BrightColor;</code> </pre> <br>  Of course, the method will only work if we have prepared several buffers for writing.  In other words, in order to implement multiple output from the fragment shader, the frame buffer used at this point must contain a sufficient number of connected color buffers.  If we refer to the <a href="https://habrahabr.ru/post/347354/">frame buffer</a> lesson, it is recalled that, when binding a texture as a color buffer, we could specify a <i>color attachment number</i> .  Until now, we didn‚Äôt need to use an attachment different from <i>GL_COLOR_ATTACHMENT0</i> , but this time <i>GL_COLOR_ATTACHMENT1</i> is also <i>useful</i> , because we need two targets for recording at once: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//       unsigned int hdrFBO; glGenFramebuffers(1, &amp;hdrFBO); glBindFramebuffer(GL_FRAMEBUFFER, hdrFBO); unsigned int colorBuffers[2]; glGenTextures(2, colorBuffers); for (unsigned int i = 0; i &lt; 2; i++) { glBindTexture(GL_TEXTURE_2D, colorBuffers[i]); glTexImage2D( GL_TEXTURE_2D, 0, GL_RGB16F, SCR_WIDTH, SCR_HEIGHT, 0, GL_RGB, GL_FLOAT, NULL ); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); //     glFramebufferTexture2D( GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0 + i, GL_TEXTURE_2D, colorBuffers[i], 0 ); }</span></span></code> </pre> <br>  Also, by calling <i>glDrawBuffers</i> , you need to explicitly tell OpenGL that we are going to output to several buffers.  Otherwise, the library will still output only to the first attachment, ignoring write operations to other attachments.  As an argument of the function, an array of identifiers of the used attachments from the corresponding enum is passed: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> attachments[<span class="hljs-number"><span class="hljs-number">2</span></span>] = { GL_COLOR_ATTACHMENT0, GL_COLOR_ATTACHMENT1 }; glDrawBuffers(<span class="hljs-number"><span class="hljs-number">2</span></span>, attachments);</code> </pre> <br>  For this frame buffer, any fragment shader that specified the <i>location</i> specifier for its outputs will write to the corresponding color buffer.  And this is great news, because in this way we avoid an extra draw pass to extract data about bright parts of the scene - you can do everything at once in a single shader: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core layout (location = 0) out vec4 FragColor; layout (location = 1) out vec4 BrightColor; [...] void main() { [...] </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//      FragColor = vec4(lighting, 1.0); //         //   -    ,    float brightness = dot(FragColor.rgb, vec3(0.2126, 0.7152, 0.0722)); if(brightness &gt; 1.0) BrightColor = vec4(FragColor.rgb, 1.0); else BrightColor = vec4(0.0, 0.0, 0.0, 1.0); }</span></span></span></span></code> </pre> <br>  In this fragment, the part containing the typical code for the calculation of illumination is omitted.  The result is written to the first output of the shader - the variable <i>FragColor</i> .  Next, the resulting color of the fragment is used to calculate the brightness value.  To do this, weighted translation is performed in grayscale (by scalar multiplication, we multiply the corresponding components of the vectors and add them together, resulting in a single value).  Then, when exceeding the brightness of a fragment of a certain threshold, we record its color in the second output of the shader.  For cubes replacing light sources, this shader is also executed. <br><br>  Having dealt with the algorithm, we can understand why this technique fits so well with HDR rendering.  Rendering in HDR format allows color components to go beyond the upper limit of 1.0, which makes it possible to more flexibly adjust the brightness threshold beyond the standard interval [0., 1.], providing the ability to fine-tune which parts of the scene are considered bright.  Without using HDR, you will have to be content with a brightness threshold in the interval [0., 1.], which is quite acceptable, but leads to a more ‚Äúsharp‚Äù cutoff in brightness, which often makes the bloom too intrusive and flashy (imagine yourself on a snowy field high in the mountains) . <br><br>  After the shader is executed, the two target buffers will contain a normal image of the scene, as well as an image containing only bright areas. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hp/rf/pr/hprfprhsu9v4q6_gvhhg43leup4.png"></div><br>  The image of bright areas should now be processed using blur.  You can do this with a simple rectangular ( <i>box</i> ) filter, which was used in the post-processing section of the lesson for the <a href="https://habrahabr.ru/post/347354/">frame buffer</a> .  But a much better result is provided by <i>Gauss filtering</i> . <br><br><h2>  Gaussian blur </h2><br>  The postprocessing lesson gave us an idea of ‚Äã‚Äãblurring using simple color averaging of adjacent image fragments.  This blurring method is simple, but the resulting image may look more attractive.  Gaussian blur is based on a bell-shaped distribution curve of the same name: high values ‚Äã‚Äãof the function are located closer to the center of the curve and fall off to both sides of it.  Mathematically, the Gauss curve can be expressed with different parameters, but the general form of the curve remains as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0o/xq/eq/0oxqeqhhsip9d0iai3eit6cpooo.png"></div><br>  Blur with weights based on Gaussian curve values ‚Äã‚Äãlooks much better than a rectangular filter: due to the fact that the curve has a larger area in the vicinity of its center, which corresponds to greater weights for fragments near the center of the filter core.  Taking, for example, the 32x32 core, we will use the weighting factors the smaller the further the fragment is from the central one.  It is this filter characteristic that gives a visually more satisfying Gaussian blur result. <br><br>  The implementation of the filter will require a two-dimensional array of weights, which could be filled based on the same two-dimensional expression describing the Gauss curve.  However, we will immediately encounter a performance problem: even a relatively small blur core in a 32x32 fragment will require 1024 texture samples for each fragment of the image being processed! <br><br>  To our happiness, the expression of the Gaussian curve has a very convenient mathematical characteristic - separability, which will make it possible to make two one-dimensional expressions from a single two-dimensional expression, describing horizontal and vertical components.  This allows you to perform a blur in turn in two approaches: horizontally, and then vertically with sets of weights corresponding to each of the directions.  The resulting image will be the same as in the processing by a two-dimensional algorithm, but it will require much less computing power of the video processor: instead of 1024 texture samples, we will need only 32 + 32 = 64!  This is the essence of two-pass Gauss filtering. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3g/my/aq/3gmyaqmqfsy1rk3hegbx_4s5rpc.png"></div><br>  For us, all this means one thing: the blurring of one image will have to be done twice, and here by the way, we will have to use the frame buffer objects.  Apply the so-called ping-pong technique: there are a couple of frame buffer objects and the contents of the single framebuffer color buffer is rendered with some processing into the color buffer of the current framebuffer, then the source and subframes are reversed and the process is repeated a specified number of times.  In essence, the current frame buffer for displaying the image is simply switched, and with it is the current texture from which the selection is made for drawing.  The approach allows you to blur the original image by placing it in the first frame buffer, then blur the contents of the first frame buffer, placing it in the second, then blur the second one, placing it in the first one, and so on. <br><br>  Before moving on to the frame buffer configuration code, let's take a look at the Gaussian blur shader code: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core out vec4 FragColor; in vec2 TexCoords; uniform sampler2D image; uniform bool horizontal; uniform float weight[5] = float[] (0.227027, 0.1945946, 0.1216216, 0.054054, 0.016216); void main() { </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//     vec2 tex_offset = 1.0 / textureSize(image, 0); //    vec3 result = texture(image, TexCoords).rgb * weight[0]; if(horizontal) { for(int i = 1; i &lt; 5; ++i) { result += texture(image, TexCoords + vec2(tex_offset.x * i, 0.0)).rgb * weight[i]; result += texture(image, TexCoords - vec2(tex_offset.x * i, 0.0)).rgb * weight[i]; } } else { for(int i = 1; i &lt; 5; ++i) { result += texture(image, TexCoords + vec2(0.0, tex_offset.y * i)).rgb * weight[i]; result += texture(image, TexCoords - vec2(0.0, tex_offset.y * i)).rgb * weight[i]; } } FragColor = vec4(result, 1.0); }</span></span></span></span></code> </pre> <br>  As you can see, we use a rather small sample of Gaussian curve coefficients, which are used as weights for the samples horizontally or vertically relative to the current fragment.  The code has two main branches dividing the algorithm into a vertical and horizontal pass based on the value of the uniform <i>horizontal</i> .  The offset for each sample is set equal to the size of the texel, which is defined as the reciprocal of the size of the texture (a value of type <i>vec2</i> , returned by the function <i>textureSize</i> ()). <br><br>  Create two frame buffers containing one texture-based color buffer: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> pingpongFBO[<span class="hljs-number"><span class="hljs-number">2</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> pingpongBuffer[<span class="hljs-number"><span class="hljs-number">2</span></span>]; glGenFramebuffers(<span class="hljs-number"><span class="hljs-number">2</span></span>, pingpongFBO); glGenTextures(<span class="hljs-number"><span class="hljs-number">2</span></span>, pingpongBuffer); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; <span class="hljs-number"><span class="hljs-number">2</span></span>; i++) { glBindFramebuffer(GL_FRAMEBUFFER, pingpongFBO[i]); glBindTexture(GL_TEXTURE_2D, pingpongBuffer[i]); glTexImage2D( GL_TEXTURE_2D, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB16F, SCR_WIDTH, SCR_HEIGHT, <span class="hljs-number"><span class="hljs-number">0</span></span>, GL_RGB, GL_FLOAT, <span class="hljs-literal"><span class="hljs-literal">NULL</span></span> ); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE); glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE); glFramebufferTexture2D( GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, pingpongBuffer[i], <span class="hljs-number"><span class="hljs-number">0</span></span> ); }</code> </pre> <br>  After we get the HDR texture of the scene and extract the texture of the bright areas, we fill the color buffer of one of the pair of prepared framebuffers with the brightness texture and start the ping-pong process ten times (five times vertically, five horizontally): <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">bool</span></span> horizontal = <span class="hljs-literal"><span class="hljs-literal">true</span></span>, first_iteration = <span class="hljs-literal"><span class="hljs-literal">true</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> amount = <span class="hljs-number"><span class="hljs-number">10</span></span>; shaderBlur.use(); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">unsigned</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i &lt; amount; i++) { glBindFramebuffer(GL_FRAMEBUFFER, pingpongFBO[horizontal]); shaderBlur.setInt(<span class="hljs-string"><span class="hljs-string">"horizontal"</span></span>, horizontal); glBindTexture( GL_TEXTURE_2D, first_iteration ? colorBuffers[<span class="hljs-number"><span class="hljs-number">1</span></span>] : pingpongBuffers[!horizontal] ); RenderQuad(); horizontal = !horizontal; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (first_iteration) first_iteration = <span class="hljs-literal"><span class="hljs-literal">false</span></span>; } glBindFramebuffer(GL_FRAMEBUFFER, <span class="hljs-number"><span class="hljs-number">0</span></span>);</code> </pre> <br>  At each iteration, we select and bind one of the frame buffers based on whether this iteration blurs horizontally or vertically, and the color buffer of the other framebuffer is then used as the input texture for the blur shader.  At the first iteration, we have to explicitly use an image containing bright areas ( <i>brightnessTexture</i> ) - otherwise both ping-pong framebuffers will remain empty.  After ten passes, the original image takes on the appearance of a five-fold blurred Gaussian full filter.  The approach used allows us to easily change the degree of blurring: the more ping-pong iterations - the stronger the blur. <br><br>  In our case, the result of the blur looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/2j/du/ga/2jdugaud8hudvnsz8pkdjgaqvus.png"></div><br>  To complete the effect, it remains only to combine the blurred image with the original HDR image of the scene. <br><br><h2>  Texture blending </h2><br>  Having at hand an HDR texture of a rendered scene and a blurred texture of overexposed areas, all you need to realize the famous bloom effect or glow is to combine these two images.  The final fragmentary shader (quite similar to the one present in the <a href="https://habr.com/post/420409/">HDR</a> format lesson) does just that - additively blends two textures: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#version 330 core out vec4 FragColor; in vec2 TexCoords; uniform sampler2D scene; uniform sampler2D bloomBlur; uniform float exposure; void main() { const float gamma = 2.2; vec3 hdrColor = texture(scene, TexCoords).rgb; vec3 bloomColor = texture(bloomBlur, TexCoords).rgb; hdrColor += bloomColor; </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">// additive blending //   vec3 result = vec3(1.0) - exp(-hdrColor * exposure); //     - result = pow(result, vec3(1.0 / gamma)); FragColor = vec4(result, 1.0); }</span></span></span></span></code> </pre> <br>  What to look for: mixing is carried out before applying <i>tone mapping</i> .  This will allow you to correctly translate the additional brightness from the effect in the LDR ( <i>Low Dynamic Range</i> ) range, while maintaining the relative brightness distribution in the scene. <br><br>  The result of processing - all bright areas have a noticeable glow effect: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ye/ga/s7/yegas7stvrwhww7-a_rzdu3g_lk.png"></div><br>  Cubes replacing light sources now look much brighter and better convey the impression of a light source.  This scene is quite primitive, because the implementation of the effect of particular enthusiasm will not cause, but in complex scenes with well-thought-out lighting, a quality-realized bloom can be a decisive visual element that adds drama. <br><br>  The source code of the example is <a href="https://learnopengl.com/code_viewer_gh.php%3Fcode%3Dsrc/5.advanced_lighting/7.bloom/bloom.cpp">here</a> . <br><br>  I note that the lesson used a fairly simple filter with only five samples in each direction.  By making more samples in a larger radius or by performing several iterations of the filter, you can visually improve the effect.  Also, it should be said that visually the quality of the entire effect directly depends on the quality of the blur algorithm used.  By improving the filter you can achieve a significant improvement and the entire effect.  For example, more impressive results are shown by the combination of several filters with different core sizes or different Gauss curves.  Below are additional resources from Kalogirou and EpicGames that deal with improving the quality of the bloom by modifying the Gaussian blur. <br><br><h2>  Additional resources </h2><br><ul><li>  <a href="http://rastergrid.com/blog/2010/09/efficient-gaussian-blur-with-linear-sampling/">Efficient Gaussian Blur with linear sampling</a> : a qualitative description of the operation of the Gauss filter, coupled with the study of improving the performance of the method through the use of OpenGL bilinear filtering of texture samples. </li><li>  <a href="https://udk-legacy.unrealengine.com/udk/Three/Bloom.html">Bloom Post Process Effect</a> : an article from EpicGames on improving the quality of the effect by combining several Gauss curves. </li><li>  <a href="http://kalogirou.net/2006/05/20/how-to-do-good-bloom-for-hdr-rendering/">How to do a good bloom for HDR rendering</a> : An article by Kalogirou describing how to improve bloom by modifying the original Gauss filter algorithm. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/420375/">https://habr.com/ru/post/420375/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420363/index.html">HPE webinars in August-October: new topics (+ storage, AI practice, turnkey petabyte storage)</a></li>
<li><a href="../420367/index.html">Air-conditioned apocalypse: blackout scenario of the grid using smart climate instruments</a></li>
<li><a href="../420369/index.html">Extreme Extended Edge, or IEEE 802.1BR based switching</a></li>
<li><a href="../420371/index.html">To the issue of bicycle engineering in the field of electromail posting</a></li>
<li><a href="../420373/index.html">Almost OCR to get VPNBook password. PHP + Mikrotik</a></li>
<li><a href="../420377/index.html">How we ran video calls</a></li>
<li><a href="../420379/index.html">Chamber of Commerce of Russia proposed not to punish users of "spyware" devices</a></li>
<li><a href="../420381/index.html">Why stop counting neural networks as a black box?</a></li>
<li><a href="../420383/index.html">"Yandex.Money to enter your application is not interesting to make stonibut"</a></li>
<li><a href="../420385/index.html">Container-based integration testing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>