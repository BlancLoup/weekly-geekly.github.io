<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How did the Cossacks retro contest decide</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the spring of this year, a landmark Retro Contest from OpenAI was held, which was devoted to learning with reinforcements, meta learning and, of co...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How did the Cossacks retro contest decide</h1><div class="post__text post__text-html js-mediator-article"><p>  In the spring of this year, a landmark Retro Contest from OpenAI was held, which was devoted to learning with reinforcements, meta learning and, of course, Sonic.  Our team took the 4th place from 900+ teams.  The field of study with reinforcements is slightly different from standard machine learning, and this contest was different from the typical RL competition.  For details, I ask under the cat. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sx/pt/0r/sxpt0rdvj5g3xn6eohh6vmusj1o.jpeg" alt="image"></div><br><hr><a name="habracut"></a><br><h2 id="tldr">  TL; DR </h2><br><p>  Properly batted baseline doesn't need extra tricks ... practically. </p><br><h2 id="intro-v-obuchenie-s-podkrepleniem">  Intro training with reinforcement </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yx/nb/vn/yxnbvndwcscdelo4q_m7zxxhfsu.png" alt="image"></div><br><p>  Reinforced learning is an area that combines optimal control theory, game theory, psychology, and neuroscience.  In practice, reinforcement learning is applied to solving decision-making problems and finding optimal behavior strategies, or policies that are too complex for ‚Äúdirect‚Äù programming.  In this case, the agent is trained on the history of interactions with the environment.  The environment, in turn, evaluating the agent's actions, provides him with a reward (scalar) - the better the agent's behavior, the greater the reward.  As a result, the best policy is learned from the agent who learned to maximize the total reward for all the time of interaction with the environment. </p><br><p>  As a simple example, you can play BreakOut.  In this good old Atari series, a person / agent needs to control the lower horizontal platform, hit the ball and gradually break all the upper blocks.  The more knocked down - the greater the reward.  Accordingly, what the person / agent sees is an image from the screen and it is required to make a decision in which direction to move the lower platform. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wi/an/ke/wiankeye_8wb0cxtrhpsfdf5sdm.gif" alt="image"></div><br><p>  If you are interested in the topic of training with reinforcements, I recommend the steep <a href="https://www.coursera.org/learn/practical-rl/home/welcome">introductory course from HSE</a> , as well as its more detailed <a href="https://github.com/yandexdataschool/Practical_RL">open source counterpart</a> .  If you want something that you can read, but with examples - a <a href="https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On">book</a> inspired by these two courses.  I surveyed / passed / helped in creating these courses, and therefore I know from their own experience that they provide an excellent basis. </p><br><h2 id="pro-zadachu">  About the task </h2><br><p>  The main purpose of this competition was to get an agent who could play well in a series of SEGA games - Sonic The Hedgehog.  At that time, OpenAI was just starting to import games from SEGA to its training platform for RL agents, and thus decided to promote this moment a little.  Even <a href="https://arxiv.org/abs/1804.03720">an article was</a> published with a device of everything and a detailed description of the basic methods. </p><br><p>  All 3 Sonic games were supported, each with 9 levels, on which, having brushed away a tear, you could even play, recalling childhood (having previously bought them on Steam). </p><br><p>  The image from the simulator - RGB picture acted as the state of the environment (what the agent saw), and the agent was asked to select which button on the virtual joystick to press - jump / left / right and so on.  The agent received the award points in the same way as in the original game, i.e.  for collecting rings, as well as for the speed of passing the level.  In fact, we had an original sonic in front of us, only we had to go through it with the help of our agent. </p><br><p>  The competition was held from April 5 to June 5, i.e.  only 2 months, which seems pretty small.  Our team was able to get together and enter the competition only in May, which made us learn a lot on the go. </p><br><h2 id="baselines">  Baselines </h2><br><p>  As a baseline, full <a href="https://arxiv.org/abs/1710.02298">Rainbow</a> (DQN approach) and <a href="https://arxiv.org/abs/1707.06347">PPO</a> (Policy Gradient approach) training guides were given at one of the possible levels in Sonic and the resulting agent was released. </p><br><p>  The Rainbow version was based on the little-known project <a href="https://github.com/unixpickle/anyrl-py">anyrl</a> , but PPO used the good old <a href="https://github.com/openai/baselines">baselines</a> from OpenAI and it seemed much more preferable to us. </p><br><p>  From the approaches described in the article, the published baselines were distinguished by greater simplicity and smaller sets of ‚Äúhacks‚Äù to accelerate learning.  Thus, the organizers and ideas were thrown and the direction was set, but the decision on the use and implementation of these ideas remained with the competitor. </p><br><p>  Regarding ideas, I want to thank OpenAI for openness, and in particular John Schulman for the advice, ideas and suggestions he voiced at the very beginning of this competition.  We, like many participants (and even more so for newcomers in the RL world), have allowed us to better focus on the main goal of the competition - meta learning and improved agent generalization, which we will now discuss. </p><br><h2 id="osobennosti-ocenivaniya-resheniy">  Features of decision evaluation </h2><br><p>  The most interesting thing started at the moment of evaluating the agents.  In typical RL competitions / benchmarks, algorithms are tested in the same environment where they were trained, which contributes to algorithms that are good at remembering and have many hyper parameters.  In the same competition, the testing of the algorithm was carried out at the new levels of Sonic (which were never shown to anyone) developed by the OpenAI team specifically for this competition.  Cherry on the cake was the fact that in the process of testing the agent was also given a reward during the passage of the level, which made it possible to train directly in the process of testing.  However, in this case it was worth remembering that testing was limited both in time - 24 hours, and in game ticks - 1 million.  At the same time, OpenAI strongly supported the creation of such agents, which could quickly be trained to new levels.  As already mentioned, the receipt and study of such solutions was the main task of OpenAI in the course of this competition. </p><br><p>  In an academic environment, the direction of learning policies that can quickly adapt to new conditions is called meta learning, and has been actively developed in recent years. </p><br><p>  Additionally, in contrast to the usual kaggle competitions, where the entire submission comes down to sending your response file, in this competition (and indeed in RL competitions) the team was required to wrap its solution in a docker container with the specified API, build it and send docker image.  This increased the entry threshold for the competition, but made the decision process much more honest - resources and time for the docker image were limited, respectively, too heavy and / or slow algorithms simply did not pass the selection.  It seems to me that such an approach to assessing is much more preferable, as it allows researchers without the ‚Äúhome cluster of DGX and AWS‚Äù to compete on a par with fans of 100,500 models.  I hope to see more of this kind of competition in the future. </p><br><h2 id="komanda">  Team </h2><br><p>  Kolesnikov Sergey ( <a href="https://github.com/Scitator">scitator</a> ) <br>  Rl enthusiast.  At the time of the competition, an FIFT student at the Moscow Institute of Physics and Technology, wrote and defended a diploma from last year‚Äôs NIPS: Learning to Run competition (an article about which we should also write). <br>  Senior Data Scientist @ <a href="https://dbrain.io/">Dbrain</a> - we bring production-ready contests with docker and limited resources to the real world. </p><br><p>  Mikhail Pavlov ( <a href="https://github.com/fgvbrt">fgvbrt</a> ) <br>  Senior research developer <a href="https://reason8.ai/">Deephack Lab</a> .  Repeatedly participated and won prizes in hackathons and contests for training with reinforcements. </p><br><p>  Sergeev Ilya ( <a href="https://github.com/sergeevii123">sergeevii123</a> ) <br>  Rl enthusiast.  Got on one of the hackathons on RL from Deephack and everything started to happen.  Data Scientist @ <a href="https://www.avito.ru/">Avito.ru</a> - computer vision for various internal projects. </p><br><p>  Sorokin Ivan ( <a href="https://github.com/1ytic">1ytic</a> ) <br>  Engaged in speech recognition in <a href="https://www.speechpro.ru/">speechpro.ru</a> . </p><br><h2 id="podhody-i-reshenie">  Approaches and solutions </h2><br><p> After a quick test of the proposed baselines, our choice fell on the approach from OpenAI - PPO, as a more formed and interesting option for the development of our solution.  In addition, judging by their article for this competition, the PPO agent did a little better job.  From the same article, the first improvements were born that we used in our solution, but first things first: </p><br><ol><li><p>  PPO co-education at all available levels </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ia/oc/lx/iaoclxlyzkgmdgiqkkksiwzcoli.png" alt="image"></div><br><p>  Laid out baseline was trained only on one of the available 27 levels of Sonic.  However, with the help of small modifications, learning could be parallelized to all 27 levels at once.  Due to the greater diversity in training, the resulting agent had much greater generalization and a better understanding of the structure of the Sonic world, and therefore coped much better. </p><br></li><li><p>  Pre-training in the testing process <br>  Returning to the basic idea of ‚Äã‚Äãcompetition, meta learning, it was required to find an approach that would have maximum generalization and could easily adapt to new environments.  And for adaptation, it was necessary to train the existing agent under the test environment, which, in fact, was done (at each test level, the agent took 1 million steps, which was enough to tune to a specific level).  At the end of each of the test games, the agent evaluated the award received and optimized his policy using the history he had just received.  Here it is important to note that with this approach it is important not to forget all of your previous experience and not to degrade under specific conditions, which, in fact, is the main interest of meta learning, since such an agent immediately loses all of the existing ability to generalize. </p><br></li><li><p>  Exploration bonuses <br>  Delving into the conditions of remuneration for the level - the agent was given a reward for moving forward along x - coordinates, respectively, he could get stuck on some levels when you first had to go forward and then back.  It was decided to make an additive to the agent, the so-called <a href="https://arxiv.org/abs/1606.01868">count based exploration</a> , when the agent was given a small reward if he fell into a state in which he had not yet been.  Two types of exploitation bonus were implemented: based on the image and based on the x-coordinates of the agent.  The reward based on the picture was calculated as follows: for each pixel location in the picture, it was counted how many times each value occurred per episode, the reward was inversely proportional to the product across all pixel locations, how many times the values ‚Äã‚Äãmet in these locations per episode.  The award based on the x-coordinate was considered similarly: for each x-coordinate (with a certain accuracy), the number of times the agent was in this coordinate per episode, the reward is inversely proportional to this number for the current x-coordinate. </p><br></li><li><p>  <a href="https://arxiv.org/abs/1710.09412">Mixup experiments</a> <br>  In ‚Äúlearning with the teacher‚Äù, a simple, but effective, data augmentation method, the so-called, has recently been used.  mixup.  The idea is very simple: the addition of two arbitrary input images is done and a weighted sum of corresponding labels is assigned to this new image (for example, 0.7 <em>dog + 0.3</em> cat).  In such tasks as the classification of images and speech recognition, mixup shows good results.  Therefore, it was interesting to test this method for RL.  The augmentation was done in each large batche consisting of several episodes.  The input images were mixed in pixels, but with tags, it was not so simple.  The values ‚Äã‚Äãof returns, values ‚Äã‚Äãand neglogpacs were mixed by a weighted sum, but the action (actions) was chosen from the example with the maximum coefficient.  Such a decision did not show a tangible increase directly (although it would seem that there should have been an increase in generalization), but it did not worsen the baseline.  The graphs below compare the PPO algorithm with mixup (red) and without mixup (blue): at the top is the reward during training, at the bottom is the length of the episode. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/bk/h-/jb/bkh-jbhhzowcl3gpazkbn8mun2a.jpeg" alt="image"></div><br></li><li><p>  Selection of the best initial policy <br>  This improvement was one of the last and made a very significant contribution to the final result.  At the training level, several different policies were trained with different hyperparameters.  At the test level, the first few episodes were tested for each of them and the policy that gave the maximum test reward for their episode was chosen for further study. </p><br></li></ol><br><h2 id="bloopers">  Bloopers </h2><br><p>  And now on the question of what was tried, but did not fly.  In the end, this is not a new SOTA article to hide something. </p><br><ol><li>  Changing network architecture: <a href="https://arxiv.org/abs/1706.02515">SELU activation</a> , self-attention, <a href="https://arxiv.org/abs/1709.01507">SE blocks</a> </li><li>  <a href="https://arxiv.org/abs/1712.06567">Neuroevolution</a> </li><li>  Creating your own Sonic levels - everything was prepared, but it‚Äôs simply not enough time </li><li>  Meta-learning through <a href="https://arxiv.org/abs/1703.03400">MAML</a> and <a href="https://arxiv.org/abs/1803.02999">REPTILE</a> </li><li>  Ensemble of several models and additional training in the process of testing each of the models using importance sampling </li></ol><br><h2 id="itogi">  Results </h2><br><p>  After 3 weeks from the end of the competition OpenAI posted the <a href="https://blog.openai.com/first-retro-contest-retrospective/">results</a> .  On another 11 additionally created levels, our team received an honorable 4th place, jumping from 8th on a public test, and overtaking the dubbed baselines from OpenAI. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yv/om/iw/yvomiwj1jmffvmlicjmhe7vjp9y.png" alt="image"></div><br><p>  The main distinctive moments that ‚Äúflew‚Äù in the first 3ki: </p><br><ol><li>  Modified system of actions (they invented their buttons, removed the extra ones); </li><li>  Examine states through hash from input image; </li><li>  More learning levels; </li></ol><br><p>  Additionally, we would like to note that in this competition, in addition to the victory, the promotion of the description of their decisions, as well as materials that helped other participants, was actively encouraged - for this there was also a separate nomination.  Which, again, increased the bulbiness of the competition. </p><br><h2 id="posleslovie">  Afterword </h2><br><p>  Personally, I really enjoyed this competition, as well as the topic of meta learning.  During the time of participation, I got acquainted with a large list of articles (I did <a href="https://github.com/Scitator/papers">not</a> even <a href="https://github.com/Scitator/papers">forget</a> some of them) and learned a huge number of different approaches that I hope to use in the future. </p><br><p>  In the best tradition of participating in the competition, all code is available and uploaded to the <a href="https://github.com/fgvbrt/retro_contest">githab</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/421585/">https://habr.com/ru/post/421585/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421571/index.html">Kivy. Xamarin. React Native. Three frameworks - one experiment (part 3)</a></li>
<li><a href="../421573/index.html">Product Design Digest August 2018</a></li>
<li><a href="../421575/index.html">The confrontation between Yandex and Roskomnadzor is brewing, and in a day or two the search engine may fall under partial blocking</a></li>
<li><a href="../421577/index.html">An exploit has been published for an unpatched vulnerability in Windows Task Scheduler (translation)</a></li>
<li><a href="../421579/index.html">Organization of effective interaction microservices</a></li>
<li><a href="../421587/index.html">[Ekaterinburg, Announcement] Mitap on Java - JUG.EKB</a></li>
<li><a href="../421589/index.html">Metamorphosis: form programming at the molecular level</a></li>
<li><a href="../421591/index.html">Budget Wireless (Wi-Fi) Autonomous System (from the battery) video surveillance</a></li>
<li><a href="../421593/index.html">SandboxEscaper / PoC-LPE: What's Inside?</a></li>
<li><a href="../421595/index.html">How IT professionals find work in the US and the EU: 9 best resources</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>