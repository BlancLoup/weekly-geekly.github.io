<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to choose an algorithm for the address filter</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Quite often articles appear on Habr√© with new algorithms for automatically parsing addresses written in one line. In addition, address processing serv...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to choose an algorithm for the address filter</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/f90/3ea/e44/f903eae4417a40bc995d5d6d7f085e16.jpg" align="right"><br>  Quite often articles appear on Habr√© with new algorithms for automatically parsing addresses written in one line.  In addition, address processing services are provided by various IT companies.  In the article we will explain how to use your address base to select the algorithm for automatic address resolution, and what you should pay attention to when testing and developing address filter algorithms. <br><br>  This article is for everyone who stores customer data and wants to solve one of the following tasks: <br><ol><li>  make sure that the address exists, so as not to send the parcel or letter to nowhere; </li><li>  split the address into components to understand where sales are best; </li><li>  to add the address with the missing information in order to optimize the work plan of the couriers; </li><li>  standardize addresses to find duplicate records of the same customer; </li><li>  update and bring the addresses to the directory format in order to pass regulator checks. </li></ol><br>  The task of automatically parsing mailing addresses seems quite simple at first glance - take and match the address directory ( <a href="http://habrahabr.ru/company/hflabs/blog/230823/">for example, FIAS</a> ) words from the input string.  But all those who take it, are buried in a large number of address features ... <br><a name="habracut"></a><br><h2>  What we know about addresses </h2><br>  To begin, introduce ourselves.  We are engaged in the task of automated address analysis for more than 9 years.  During this time we have worked with both large companies and small firms.  We have accumulated a large sample of addresses describing the format of customer data in order to understand well how our ideas influence the quality of address processing in real systems. <br><br>  Over the past year, we have developed a new version of the algorithm (we call it the address filter) in order to put an end to the address resolution algorithm. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  We define the task </h2><br>  We know three ways to get the current correct addresses: <br><ol><li>  get a good address from the client immediately (for example, with the help of <a href="http://dadata.ru/suggestions/">prompts by addresses</a> ); </li><li>  hire operators to manually parse addresses; </li><li>  automatically parse the data. </li></ol><br>  The first option is the best, but not suitable for those who already have a large base of addresses of dubious quality. <br>  The second option has a large percentage of well-parsed addresses, but, as our practice shows, is expensive and time consuming. <br>  The third option alone will never provide such a percentage of well-parsed addresses, as in the second option, but it is cheaper and much faster. <br><br>  We recommend using the synthesized version of the 2nd and 3rd paragraphs: <br><ol><li>  parse addresses automatically with an indication of the quality of parsing the address; </li><li>  Addresses with a good quality indicator - send to business processes, and with bad ones - send to operators for analysis. </li></ol><br>  So you will get a large percentage of parsed addresses for a reasonable amount. <br><br>  If you decide to use this option or parse the addresses only automatically, you will need to choose the right algorithm for automatic data parsing.  How to do this, we will continue. <br><br><h2>  We prepare addresses </h2><br>  To select an algorithm, it is necessary to analyze the results of processing a certain volume of addresses by different algorithms.  It seems logical to take a part of addresses from real data and add them with cosmetic corrections to check what percentage of addresses with errors and typos will be recognized correctly. <br><br><h4>  Fallacy first: automatically correct any typos - good </h4><br>  Most of our customers, who first encountered the problem of automatic address analysis, and we ourselves at first thought that typos correction was the main thing that any self-respecting algorithm should be able to do. <br><br>  Subsequently, we realized that the correction of typos looks beautiful only at the stage of demonstrations, when instead of checking the algorithm at their addresses, customers invent unprecedented cases, admiring the transformations of the <i>‚Äúihonravov, Maskva, Yubileynaya, MK‚Äù type</i> in <i>‚ÄúMoscow Region, Yubileiny, Tikhonravov St.</i>  In combat conditions, this functionality is not only not used, but also hurts the work with the main address database. <br><br>  Our research shows that in the source addresses of corporate systems, rarely more than 2% of addresses are typographical errors - among all of our customers the percentage of such systems is less than 5%.  The majority of typos (about 95% of all typos) are systemic in nature, that is, it is either a common typo, for example, <i>Maskva</i> , or a correction of the type of <i>street.</i>  <i>3rd Mytishchinskaya &gt;&gt;&gt; st.</i>  <i>3rd Mytishchinskaya</i> or <i>st.</i>  <i>Tolstoy &gt;&gt;&gt; st.</i>  <i>Tolstoy</i> .  These typos can be described by a finite set of rules that will allow them to be corrected. <br><br>  What is bad typo correction in the general case?  Making a correction of all misprints on n-grams, Levenshtein distance, etc., the algorithm tries to attract the address to the directory with a great chance to get something completely different from what was meant in the original address.  In addition, the source address may contain additional information that is not in the address directory: company name, business center, how to get from the metro, etc.  In the algorithm for correcting typos, these additions are more likely to be perceived as a normal component of the address. <br><br>  For 9 years of work, we came to the conclusion that it is necessary to make a correction of typos only according to the rules, which guarantee that this typo can be brought only to the correct analyzed options. <br><br>  Thus, we advise checking the algorithms only on real data without artificial distortions.  For example, if you have a <i>Moscow</i> address of <i>Pushkin 13</i> in the database, then use it, and not <i>Mask Pushikino 13</i> . <br><br>  Algorithms with typo correction should be treated with caution.  The worst thing to which the use of the algorithm for correcting typos, described above, can lead to is getting incorrectly parsed addresses with a good quality code. <br><br><h4>  Fallacy second: the percentage of well-parsed addresses is the main criterion for choosing a filter (except for the cost, of course) </h4><br>  Any algorithm for automatically parsing addresses at the input accepts the address, and at the output - it gives it in a standardized form.  Usually he is able to return a sign indicating whether the algorithm is sure in address parsing or not.  Such a feature is usually called a quality code. <br><br>  Addresses of our customers with a good parsing quality code automatically go into business processes, and with a bad quality code they are sent for manual parsing.  The greater the percentage of addresses with a good quality code, the more the customer saves on the process of manual processing of addresses. <br><br>  Thus, the main criterion for choosing an algorithm becomes the percentage of addresses with a good quality code. <br><br>  One important point is often forgotten: it is much cheaper to give an address with a bad quality code to good manually than to correct the consequences in the system, which will result in incorrectly recognized addresses with a good quality code. <br><br>  For example, we are currently developing a system for estimating the value of real estate, where the cost per square meter is known for each house, which is used to assess the client‚Äôs solvency when granting a loan.  The system automatically analyzes the new ads for the sale of apartments in the network, standardizes the address and adjusts the average cost in the directory.  If among the standardized addresses there will be many addresses with incorrect parsing and a good quality code, we will have many errors in the directory where instead of the real average apartment price it will be several times higher or lower.  Such addresses are difficult to find, and they have a strong negative impact on business processes. <br><br>  It is precisely this that badly corrects all the misprints: the algorithm tries to attract a deliberately bad address to a directory with a good quality code, which increases the percentage of the reverse error, that is, the percentage of addresses with a good quality code, but incorrectly standardized. <br><br><h4>  What addresses to pay attention to </h4><br>  When comparing address filtering algorithms, look not only at the percentage of addresses with a good quality code, but also at the percentage of incorrectly parsed addresses with a good quality code.  It is best to prepare a sample of your addresses, including cases of writing addresses of increased danger, namely: <br><ul><li>  <b>Addresses with typos or incorrect indication of the address component</b> (for example, the <i>3rd Mytishchi</i> instead <i>of the 3rd Mytishchi</i> ). </li><li>  <b>Ambiguous addresses</b> , for which only the source data cannot be unambiguously determine what is at stake, including when analyzed by the operator.  For example, missing or incorrectly specified address components: <i>Moscow, Tverskaya</i> can mean both Tverskaya Square and the street. </li><li>  <b>Error in specifying the type of the address component.</b>  According to our data, about 5% of customers' addresses contain certain errors indicating the type of address component: instead of ‚Äúurban-type settlement‚Äù they write ‚Äúvillage‚Äù, instead of ‚Äúdeadlock‚Äù they write ‚Äúlane‚Äù and so on. </li><li>  <b>Error in specifying the component itself.</b>  Most often incorrectly indicate: <ul><li>  The area in which the settlement is located, if it is located on the border of two regions.  For example, in the address of the <i>Moscow region, Dmitrovsky district, town of Zaprudnya</i> , the area is incorrectly specified, correctly - Taldomsky. </li><li>  The region in which the object is located.  Especially often it meets with the addresses of Moscow and St. Petersburg, for example: <ul><li>  <i>Leningrad region, St. Petersburg, Fontanka</i> </li><li>  <i>Moscow region, Moscow, st.</i>  <i>Rastorgueva</i> </li><li>  <i>Moscow region, Zelenograd, to 3113</i> </li></ul></li></ul></li><li>  <b>A mistake in a name, rank, or other common misconception.</b>  For example, <i>Lev</i> <i>Tolstoy is</i> written instead of <i>Alexei Tolstoy</i> Street, or <i>General Zhukov is</i> written instead of <i>Marshal</i> <i>Zhukov</i> .  Sometimes they also give some outdated or local, known only to local residents, name. </li><li>  <b>Duplicate words in the original string.</b>  Sometimes after many transformations and transitions from system to system, addresses with duplicate components are formed.  For example, one conference had the address: <i>Moscow, Moscow, Moscow, Moscow, Moscow, Leningradsky Avenue, 39, p. 79</i> .  Obviously, the word Moscow here is written several times by mistake and the algorithm may not take into account duplicates from the original address.  But is it always possible to delete duplicates?  Another example: <i>Sakhalin, Yuzhno-Sakhalinsk, Sakhalin</i> is the address of <i>Sakhalin Oblast, Yuzhno-Sakhalinsk, Sakhalin Street</i> .  A good algorithm should find duplicates only if they are really duplicates and do not divert the address to the wrong parsing. </li><li>  <b>Garbage or additional information in the source data.</b>  This is usually either the name or additional information about the building itself and how to get to it.  For example: <i>Ivanov Ivan, Pirogov 20 to 1 total 8/1 room 313, Novosibirsk, NSO</i> or <i>Moscow, Turchaninov, BC Krymsky Bridge, building 6, page 2, 2 minutes from the metro and a <a href="http://hh.ru/vacancy/11938386">good salary</a></i> .  In such cases, all components of the input line that are not address components or frequently encountered additional address information (for example, metro or city districts) should be submitted to the operator for analysis, as they can affect the quality of address parsing. </li><li>  <b>Outdated addresses.</b>  These are the addresses that others now have: sometimes, the streets are renamed, it happens that they move to another settlement, merge, etc.  When there are two addresses: <i>Samara 13 passage</i> and <i>Samara Georgy Ratner</i> , then it would be nice to understand that this is the same address.  The algorithm should be able to update the address and set a good quality code for it only if it is updated. </li></ul><br><br><h2>  We compare algorithms </h2><br>  When we have prepared a test sample, then everything is simple.  We process addresses with different algorithms and compare them according to the criteria: <br><ol><li>  <b>The percentage of parsing good addresses</b> (that is, addresses without garbage, ambiguities, and typos).  The algorithm must be able to correctly parse good addresses with a good quality code. </li><li>  <b>Percentage parse bad addresses.</b>  The algorithm should be able to parse bad addresses as well as possible, that is, if the address is bad, but can be well parsed with a good quality code, the algorithm should be able to do it. </li><li>  <b>The percentage of addresses with an inverse error.</b>  The algorithm must contain a minimal reverse error, that is, do not put a good quality code into the addresses with incorrect parsing.  We think this is the most important point of all. </li><li>  <b>The presence of additional properties of the standardized address.</b>  The algorithm should provide convenient levers for analyzing and working with addresses with bad quality codes.  At the same time, working with tools should be simple and straightforward. </li></ol><br><br><h2>  findings </h2><br>  The task of automatic address analysis is not as simple as it seems at first glance.  If you decide to choose an algorithm for parsing addresses or write your own, then you need to approach this process correctly: analyze existing addresses, make a representative sample for tests.  We hope that this article will help you in this work and all your addresses will be disassembled automatically and correctly. <br><br>  PS: Within a month we will install a new version of the address filter, which was discussed at the beginning of the article, on <a href="http://dadata.ru/">dadata.ru</a> .  Register to be in the know and be among the first researchers of the new algorithm. <br><br>  Thank you <a href="http://habrahabr.ru/users/chipqa/" class="user_link">chipQA</a> for help in preparing the article. </div><p>Source: <a href="https://habr.com/ru/post/240633/">https://habr.com/ru/post/240633/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../240621/index.html">Have a good takeoff and soft landing. Landing page for end users</a></li>
<li><a href="../240623/index.html">Dive into Docker: Dockerfile and communication between containers</a></li>
<li><a href="../240625/index.html">The most geeky competition in the history of Habr: about prizes</a></li>
<li><a href="../240627/index.html">12 great examples of email lists for abandoned baskets</a></li>
<li><a href="../240629/index.html">Collection of outsourcing fallacies</a></li>
<li><a href="../240635/index.html">Convenient search of the user's computer in the Windows domain</a></li>
<li><a href="../240637/index.html">Another boxed CMS - familiarity with FFCMS</a></li>
<li><a href="../240639/index.html">All about Visual Studio 2013 Update 3 + webinar</a></li>
<li><a href="../240641/index.html">Cloud file website integrity monitoring service</a></li>
<li><a href="../240643/index.html">The new Wi-Fi 60 GHz technology from Samsung Electronics will increase data transfer speed by 5 times</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>