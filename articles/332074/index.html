<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autoencoders in Keras, Part 6: VAE + GAN</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 


- Part 1: Introduction 
- Part 2: Manifold learning and latent variables 
- Part 3: Variational autoencoders ( VAE ) 
- Part 4: Conditional...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autoencoders in Keras, Part 6: VAE + GAN</h1><div class="post__text post__text-html js-mediator-article"><h3>  Content </h3><br><ul><li>  Part 1: <a href="https://habrahabr.ru/post/331382/">Introduction</a> <br></li><li>  Part 2: <a href="https://habrahabr.ru/post/331500/"><em>Manifold learning</em> and <em>latent</em> variables</a> <br></li><li>  Part 3: <a href="https://habrahabr.ru/post/331552/">Variational autoencoders ( <em>VAE</em> )</a> <br></li><li>  Part 4: <a href="https://habrahabr.ru/post/331664/"><em>Conditional VAE</em></a> <br></li><li>  Part 5: <a href="https://habrahabr.ru/post/332000/"><em>GAN</em> (Generative Adversarial Networks) and tensorflow</a> <br></li><li>  <strong>Part 6: VAE + GAN</strong> <br></li></ul><br>  In the previous part, we created a <strong><em>CVAE</em></strong> autoencoder, whose decoder is able to generate a digit of a given label, we also tried to create pictures of numbers of other labels in the style of a given picture.  It turned out pretty good, but the numbers were generated blurry. <br><br>  In the last part, we studied how the <strong><em>GANs</em></strong> work, getting quite clear images of numbers, but the possibility of coding and transferring the style was lost. <br><br>  In this part we will try to take the best from both approaches by combining <em>variational autoencoders</em> ( <strong><em>VAE</em></strong> ) and <em>generative competing networks</em> ( <strong><em>GAN</em></strong> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The approach, which will be described later, is based on the article <strong>[Autoencoding beyond pixels using a learned similarity metric, Larsen et al, 2016]</strong> . <br><br><img src="https://habrastorage.org/web/7a1/8db/d39/7a18dbd3969048c2b085cc707e539f0c.png"><br><br>  Illustration of <strong><em>[1]</em></strong> <br><a name="habracut"></a><br><h4>  We will understand in more detail why the recovered images are blurry. </h4><br>  In the part about <em>VAE</em> , the process of image generation was considered. <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  from <em>latent</em> variables <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  . <br>  Since the dimension of hidden variables <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  significantly lower than the dimension of objects <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  (in the part about <em>VAE,</em> these dimensions were 2 and 784), and there is always some randomness, then the same <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  may correspond to multidimensional distribution <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  , i.e <img src="https://habrastorage.org/getpro/habr/post_images/7d4/1ae/4ed/7d41ae4ed44b2e23789bd2a370daea77.svg" alt="P (X | Z)">  .  This distribution can be represented as: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cda/735/6a9/cda7356a9b751f0478a5f5a97e160854.svg" alt="P (X | Z) = f (Z) + \ epsilon,"></div><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/bc1/12d/78f/bc112d78f43d281ebb9e846d5d4512f4.svg" alt="f (z)">  some average most likely object for a given <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  , but <img src="https://habrastorage.org/getpro/habr/post_images/272/6bf/a03/2726bfa03e335f630171c1eb09b04019.svg" alt="\ epsilon">  - the noise of a complex nature. <br><br>  When we train autoencoders, we compare the input from the sample <img src="https://habrastorage.org/getpro/habr/post_images/e58/d4d/2e2/e58d4d2e228243b9006317b2fac6833e.svg" alt="X_s">  and autoencoder exit <img src="https://habrastorage.org/getpro/habr/post_images/75f/653/c49/75f653c4987aaf1cefa41342aed2a70f.svg" alt="\ tilde X_s">  using some error functional <img src="https://habrastorage.org/getpro/habr/post_images/824/ed8/7f2/824ed87f2e22c5c92ae1df8b4186c5b0.svg" alt="L">  , <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/929/05b/862/92905b862f686017d7bf950fcc915454.svg" alt="L (X_s, \ tilde X_s), \\ \ tilde X_s = f_d (Z; \ theta_d), \\ Z \ sim Q (Z | X_s; \ theta_e),"></div><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/9d8/5dc/e2c/9d85dce2c9602f0a3b35169043a57178.svg" alt="Q, \ f_d">  - encoder and decoder. <br><br>  Asking <img src="https://habrastorage.org/getpro/habr/post_images/824/ed8/7f2/824ed87f2e22c5c92ae1df8b4186c5b0.svg" alt="L">  we define noise <img src="https://habrastorage.org/getpro/habr/post_images/cb7/b48/388/cb7b483887dc3689214803c9d70c7214.svg" alt="\ epsilon_L">  which bring real noise <img src="https://habrastorage.org/getpro/habr/post_images/272/6bf/a03/2726bfa03e335f630171c1eb09b04019.svg" alt="\ epsilon">  . <br>  Minimizing <img src="https://habrastorage.org/getpro/habr/post_images/824/ed8/7f2/824ed87f2e22c5c92ae1df8b4186c5b0.svg" alt="L">  , we teach autoencoder to adapt to noise <img src="https://habrastorage.org/getpro/habr/post_images/cb7/b48/388/cb7b483887dc3689214803c9d70c7214.svg" alt="\ epsilon_L">  , removing it, that is, to find the average value in a given metric (in the second part it was shown visually on a simple artificial example). <br><br>  If the noise <img src="https://habrastorage.org/getpro/habr/post_images/cb7/b48/388/cb7b483887dc3689214803c9d70c7214.svg" alt="\ epsilon_L">  which we define as functional <img src="https://habrastorage.org/getpro/habr/post_images/824/ed8/7f2/824ed87f2e22c5c92ae1df8b4186c5b0.svg" alt="L">  does not match the actual noise <img src="https://habrastorage.org/getpro/habr/post_images/272/6bf/a03/2726bfa03e335f630171c1eb09b04019.svg" alt="\ epsilon">  then <img src="https://habrastorage.org/getpro/habr/post_images/4b2/921/fa8/4b2921fa84d992090875a82a8f8c0d37.svg" alt="f_d (Z; \ theta_2)">  prove to be greatly biased from real <img src="https://habrastorage.org/getpro/habr/post_images/bc1/12d/78f/bc112d78f43d281ebb9e846d5d4512f4.svg" alt="f (z)">  (example: if the real noise in the regression is laplassovsky, and the difference of squares is minimized, then the predicted value will be shifted towards emissions). <br><br>  Returning to the pictures: let's see how the pixel metric is connected, which defines the loss in the previous parts, and the metric used by the person.  An example and illustration from <strong><em>[2]</em></strong> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/web/972/180/174/9721801740014b6da131dc811e2fed84.png"></div><br><br>  In the picture above: <br>  (a) - the original image of the digit, <br>  (b) - obtained from (a) by cutting a piece, <br>  (c) is the digit (a) shifted half a pixel to the right. <br><br>  In terms of per-pixel metric, (a) is much closer to (b) than to (c);  although from the point of view of human perception (b) is not even a figure, but the difference between (a) and (b) is almost imperceptible. <br><br>  Autoencoders with a pixel-by-pixel metric thus blurred the image, reflecting the fact that within the limits of close <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  : <br><ul><li>  the position of the figures slightly walks in the picture, <br></li><li>  figures are drawn slightly differently (although pixel-by-pixel can be far away). <br></li></ul><br>  According to the metrics of human perception, the fact that the figure has blurred already makes it be very different from the original.  Thus, if we know the person‚Äôs metric or close to it and optimize in it, the numbers will not be eroded, and the importance of the figure being full, not like from the picture (b), will increase dramatically. <br><br>  You can try to manually invent a metric that will be closer to the human.  But using the <strong><em>GAN</em></strong> approach, it is possible to train the neural network to look for a good metric itself. <br><br>  About <em>GAN'y</em> written in the last part. <br><br><h3>  Connecting <em>VAE</em> and <em>GAN</em> </h3><br>  The <em>GAN</em> generator performs a function similar to the decoder in <em>VAE</em> : both samples are from a prior distribution <img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  and translate it into <img src="https://habrastorage.org/getpro/habr/post_images/10e/85c/f7c/10e85cf7c9acd729f42a736b83e443bb.svg" alt="P_g (X)">  .  However, they have different roles: the decoder restores the object encoded by the encoder, while learning based on some comparison metric;  the generator, on the other hand, generates a random object that is not compared with anything, so long as the discriminator cannot distinguish which of the distributions <img src="https://habrastorage.org/getpro/habr/post_images/d3a/eb7/444/d3aeb74440becc6f0f689b98c40429cb.svg" alt="P">  or <img src="https://habrastorage.org/getpro/habr/post_images/e45/65c/97d/e4565c97d32af21cfc41c1d61d105a4a.svg" alt="P_g">  he belongs <br><br>  The idea is to add a third network to the <em>VAE</em> - the discriminator and feed the input and the restored object and the original to it, and train the discriminator to determine which one is which. <br><img src="https://habrastorage.org/web/7a1/8db/d39/7a18dbd3969048c2b085cc707e539f0c.png"><br><br>  Illustration of <strong><em>[1]</em></strong> <br><br>  Of course, we can no longer use the same comparison metric from <em>VAE</em> , because, while studying in it, the decoder generates images that are easily distinguishable from the original.  Do not use the metric at all - either, since we would like the recreated <img src="https://habrastorage.org/getpro/habr/post_images/4ca/30e/198/4ca30e19847019fca6a2d10e04823bb4.svg" alt="\ tilde X">  looked like the original and not just some random of <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  as in pure <em>gan</em> . <br><br>  Let us think, however, about this: the discriminator, learning to distinguish between a real object and a generated one, will isolate some characteristic features of one and the other.  These features of the object will be encoded in the layers of the discriminator, and based on their combination, it will already give the probability of the object to be real.  For example, if the image is blurred, then some kind of neuron in the discriminator will activate more strongly than if it is clear.  Moreover, the deeper the layer, the more abstract characteristics of the input object are encoded in it. <br><br>  Since each discriminator layer is a code-description of an object and at the same time encodes features that allow the discriminator to distinguish generated objects from real ones, it is possible to replace some simple metric (for example, pixel-by-pixel) with a metric over neuronal activations in one of the layers: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/773/3b4/edd/7733b4edd7f179d0bec876924757116e.svg" alt="L (X_s, \ tilde X_s) \ longrightarrow L_d (d_l (X_s), d_l (\ tilde X_s)) \\ \ tilde X_s = f_d (Z; \ theta_d), \\ Z \ sim Q (X_s; \ theta_e) ,"></div><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/3b1/1d8/ee7/3b11d8ee72187458f5475fb2ccd302f9.svg" alt="d_l">  - activation on <img src="https://habrastorage.org/getpro/habr/post_images/c45/533/ea2/c45533ea2036f068f1331712e9ac8cec.svg" alt="l">  th layer of the discriminator, and <img src="https://habrastorage.org/getpro/habr/post_images/9d8/5dc/e2c/9d85dce2c9602f0a3b35169043a57178.svg" alt="Q, \ f_d">  - encoder and decoder. <br><br>  At the same time, it is hoped that the new metric <img src="https://habrastorage.org/getpro/habr/post_images/f9f/aa7/35f/f9faa735f9456a6487c3241f604240bd.svg" alt="L_d">  will be better. <br><br>  Below is a diagram of the work of the resulting <em>VAE + GAN</em> network, proposed by the authors <strong><em>[1]</em></strong> . <br><br><img src="https://habrastorage.org/web/701/bbb/212/701bbb21273045fc9ed4aab7e0529764.png"><br><br>  Illustration of <strong><em>[1]</em></strong> <br><br>  Here: <br><br><ul><li><img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  - input object from <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  , <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/a9b/f9b/a6a/a9bf9ba6a756c7432a228d2be0956c47.svg" alt="Z_p">  - sampled <img src="https://habrastorage.org/getpro/habr/post_images/d2d/297/e80/d2d297e8073685ab6fb84a0fb938ba3c.svg" alt="Z">  of <img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  , <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/d86/66a/d9f/d8666ad9faf0084001cff22aee214db1.svg" alt="X_p">  - object generated by decoder from <img src="https://habrastorage.org/getpro/habr/post_images/a9b/f9b/a6a/a9bf9ba6a756c7432a228d2be0956c47.svg" alt="Z_p">  , <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/4ca/30e/198/4ca30e19847019fca6a2d10e04823bb4.svg" alt="\ tilde X">  - object recovered from <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  , <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/9e5/0fe/123/9e50fe123a7a8f84e366f8a93cc6856c.svg" alt="\ mathcal L_ {prior} = KL \ left [Q (Z | X) || P (Z) \ right]">  - loss forcing the encoder to translate <img src="https://habrastorage.org/getpro/habr/post_images/8e5/d52/dea/8e5d52dea71bd2983ce35b05f42587a7.svg" alt="P (X)">  in the right for us <img src="https://habrastorage.org/getpro/habr/post_images/a30/620/58e/a3062058ed53bbfca2cd5199c5a84843.svg" alt="P (Z)">  (just like in part 3 about <em>VAE</em> ), <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/b92/769/6cc/b927696cc317483b90663a1caa9a090b.svg" alt="\ mathcal L_ {llike} ^ {Dis_l} = L_d (d_l (X), d_l (\ tilde X))">  - metric between activations <img src="https://habrastorage.org/getpro/habr/post_images/c45/533/ea2/c45533ea2036f068f1331712e9ac8cec.svg" alt="l">  layer of discriminator <img src="https://habrastorage.org/getpro/habr/post_images/382/bca/d6d/382bcad6d8d09bcc3223582fd8b28a78.svg" alt="D">  on real <img src="https://habrastorage.org/getpro/habr/post_images/321/8c7/f74/3218c7f74f7f865cc525a03fdd9aed8f.svg" alt="X">  and restored <img src="https://habrastorage.org/getpro/habr/post_images/ef7/618/3ba/ef76183badde5edcd50999c1d94acf30.svg" alt="\ tilde X = f_d (Q (X))">  , <br></li><li><img src="https://habrastorage.org/getpro/habr/post_images/b1e/9ef/755/b1e9ef7551a2a889d03c5725a7d2d87c.svg" alt="\ mathcal L_ {GAN} = \ log (D (X)) + \ log (1 - D (f_d (Z))) + \ log (1 - D (f_d (Q (X))))">  - cross-entropy between the real distribution of labels of real / generated objects, and the probability distribution predicted by the discriminator. <br></li></ul><br>  As with the <em>GAN</em> , we cannot train all 3 parts of the network at the same time.  The discriminator should be trained separately, in particular, it is not necessary that the discriminator try to reduce <img src="https://habrastorage.org/getpro/habr/post_images/eaf/c6f/618/eafc6f61811bfed63ff57f700c74122d.svg" alt="\ mathcal L_ {llike} ^ {Dis_l}">  , since it will collapse the difference of activations in 0. Therefore, the training of all networks should be limited only to their relevant losses. <br><br>  The scheme proposed by the authors: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/07a/d0b/dc0/07ad0bdc0524f17cd4ae6c6e1be3c36d.svg" alt="\ theta_ {Enc} = \ theta_ {Enc} - \ Delta _ {\ theta_ {Enc}} (\ mathcal L_ {prior} + \ mathcal L ^ {Dis_l} _ {llike}), \\ \ theta_ {Dec} = \ theta_ {Dec} - \ Delta _ {\ theta_ {Dec}} (\ gamma \ mathcal L ^ {Dis_l} _ {llike} - \ mathcal L_ {GAN}), \\ \ theta_ {Dis} = \ theta_ {Dis } - \ Delta _ {\ theta_ {Dis}} (\ mathcal L_ {GAN})"></div><br>  Above it can be seen on which Loss what networks study.  Particular attention should be paid to the decoder: on the one hand, it tries to reduce the distance between the input and the output in the metric of the <em>lth</em> discriminator layer ( <img src="https://habrastorage.org/getpro/habr/post_images/eaf/c6f/618/eafc6f61811bfed63ff57f700c74122d.svg" alt="\ mathcal L ^ {Dis_l} _ {llike}">  ), on the other hand, trying to trick the discriminator (by increasing <img src="https://habrastorage.org/getpro/habr/post_images/b6b/3c5/273/b6b3c52735e453a77eca776f47813027.svg" alt="\ mathcal L_ {GAN}">  ).  In the article, the authors argue that by changing the coefficient <img src="https://habrastorage.org/getpro/habr/post_images/8e8/a20/60f/8e8a2060ff3876bd34e176345673b0e5.svg" alt="\ gamma">  , you can influence what is more important for the network: content ( <img src="https://habrastorage.org/getpro/habr/post_images/eaf/c6f/618/eafc6f61811bfed63ff57f700c74122d.svg" alt="\ mathcal L ^ {Dis_l} _ {llike}">  ) or style ( <img src="https://habrastorage.org/getpro/habr/post_images/b6b/3c5/273/b6b3c52735e453a77eca776f47813027.svg" alt="\ mathcal L_ {GAN}">  ).  I can not, however, say that I have observed this effect. <br><br><h2>  Code </h2><br>  The code largely repeats what was in the previous parts about pure <strong><em>VAE</em></strong> and <strong><em>GAN</em></strong> . <br><br>  Again, we will immediately write the <strong>conditional</strong> model. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Dropout, BatchNormalization, Reshape, Flatten, RepeatVector <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Lambda, Dense, Input, Conv2D, MaxPool2D, UpSampling2D, concatenate <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.advanced_activations <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LeakyReLU <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Activation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.models <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Model, load_model <span class="hljs-comment"><span class="hljs-comment">#    keras from keras import backend as K import tensorflow as tf sess = tf.Session() K.set_session(sess) #   from keras.datasets import mnist from keras.utils import to_categorical (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') / 255. x_test = x_test .astype('float32') / 255. x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) y_train_cat = to_categorical(y_train).astype(np.float32) y_test_cat = to_categorical(y_test).astype(np.float32) #   batch_size = 64 batch_shape = (batch_size, 28, 28, 1) latent_dim = 8 num_classes = 10 dropout_rate = 0.3 gamma = 1 #   #      def gen_batch(x, y): n_batches = x.shape[0] // batch_size while(True): idxs = np.random.permutation(y.shape[0]) x = x[idxs] y = y[idxs] for i in range(n_batches): yield x[batch_size*i: batch_size*(i+1)], y[batch_size*i: batch_size*(i+1)] train_batches_it = gen_batch(x_train, y_train_cat) test_batches_it = gen_batch(x_test, y_test_cat) #   x_ = tf.placeholder(tf.float32, shape=(None, 28, 28, 1), name='image') y_ = tf.placeholder(tf.float32, shape=(None, 10), name='labels') z_ = tf.placeholder(tf.float32, shape=(None, latent_dim), name='z') img = Input(tensor=x_) lbl = Input(tensor=y_) z = Input(tensor=z_)</span></span></code> </pre> <br>  The description of models from <em>GAN</em> differs almost only in the added encoder. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_units_to_conv2d</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(conv2, units)</span></span></span><span class="hljs-function">:</span></span> dim1 = int(conv2.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) dim2 = int(conv2.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]) dimc = int(units.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>]) repeat_n = dim1*dim2 units_repeat = RepeatVector(repeat_n)(lbl) units_repeat = Reshape((dim1, dim2, dimc))(units_repeat) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> concatenate([conv2, units_repeat]) <span class="hljs-comment"><span class="hljs-comment">#   ,  -        (,  - ,  P  P_g   ) def apply_bn_relu_and_dropout(x, bn=False, relu=True, dropout=True): if bn: x = BatchNormalization(momentum=0.99, scale=False)(x) if relu: x = LeakyReLU()(x) if dropout: x = Dropout(dropout_rate)(x) return x with tf.variable_scope('encoder'): x = Conv2D(32, kernel_size=(3, 3), strides=(2, 2), padding='same')(img) x = apply_bn_relu_and_dropout(x) x = MaxPool2D((2, 2), padding='same')(x) x = Conv2D(64, kernel_size=(3, 3), padding='same')(x) x = apply_bn_relu_and_dropout(x) x = Flatten()(x) x = concatenate([x, lbl]) h = Dense(64)(x) h = apply_bn_relu_and_dropout(h) z_mean = Dense(latent_dim)(h) z_log_var = Dense(latent_dim)(h) def sampling(args): z_mean, z_log_var = args epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.0) return z_mean + K.exp(K.clip(z_log_var/2, -2, 2)) * epsilon l = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var]) encoder = Model([img, lbl], [z_mean, z_log_var, l], name='Encoder') with tf.variable_scope('decoder'): x = concatenate([z, lbl]) x = Dense(7*7*128)(x) x = apply_bn_relu_and_dropout(x) x = Reshape((7, 7, 128))(x) x = UpSampling2D(size=(2, 2))(x) x = Conv2D(64, kernel_size=(5, 5), padding='same')(x) x = apply_bn_relu_and_dropout(x) x = Conv2D(32, kernel_size=(3, 3), padding='same')(x) x = UpSampling2D(size=(2, 2))(x) x = apply_bn_relu_and_dropout(x) decoded = Conv2D(1, kernel_size=(5, 5), activation='sigmoid', padding='same')(x) decoder = Model([z, lbl], decoded, name='Decoder') with tf.variable_scope('discrim'): x = Conv2D(128, kernel_size=(7, 7), strides=(2, 2), padding='same')(img) x = MaxPool2D((2, 2), padding='same')(x) x = apply_bn_relu_and_dropout(x) x = add_units_to_conv2d(x, lbl) x = Conv2D(64, kernel_size=(3, 3), padding='same')(x) x = MaxPool2D((2, 2), padding='same')(x) x = apply_bn_relu_and_dropout(x) # l-      l = Conv2D(16, kernel_size=(3, 3), padding='same')(x) x = apply_bn_relu_and_dropout(x) h = Flatten()(x) d = Dense(1, activation='sigmoid')(h) discrim = Model([img, lbl], [d, l], name='Discriminator')</span></span></code> </pre><br>  Building a graph of calculations based on models: <br><br><pre> <code class="python hljs">z_mean, z_log_var, encoded_img = encoder([img, lbl]) decoded_img = decoder([encoded_img, lbl]) decoded_z = decoder([z, lbl]) discr_img, discr_l_img = discrim([img, lbl]) discr_dec_img, discr_l_dec_img = discrim([decoded_img, lbl]) discr_dec_z, discr_l_dec_z = discrim([decoded_z, lbl]) cvae_model = Model([img, lbl], decoder([encoded_img, lbl]), name=<span class="hljs-string"><span class="hljs-string">'cvae'</span></span>) cvae = cvae_model([img, lbl])</code> </pre><br>  Loss Definition: <br><br>  Interestingly, the result was slightly better if the cross-entropy rather than <em>MSE was</em> taken as the metric for layer activations. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   L_prior = -0.5*tf.reduce_sum(1. + tf.clip_by_value(z_log_var, -2, 2) - tf.square(z_mean) - tf.exp(tf.clip_by_value(z_log_var, -2, 2)))/28/28 log_dis_img = tf.log(discr_img + 1e-10) log_dis_dec_z = tf.log(1. - discr_dec_z + 1e-10) log_dis_dec_img = tf.log(1. - discr_dec_img + 1e-10) L_GAN = -1/4*tf.reduce_sum(log_dis_img + 2*log_dis_dec_z + log_dis_dec_img)/28/28 # L_dis_llike = tf.reduce_sum(tf.square(discr_l_img - discr_l_dec_img))/28/28 L_dis_llike = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.sigmoid(discr_l_img), logits=discr_l_dec_img))/28/28 #  , ,  L_enc = L_dis_llike + L_prior L_dec = gamma * L_dis_llike - L_GAN L_dis = L_GAN #    optimizer_enc = tf.train.RMSPropOptimizer(0.001) optimizer_dec = tf.train.RMSPropOptimizer(0.0003) optimizer_dis = tf.train.RMSPropOptimizer(0.001) encoder_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "encoder") decoder_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "decoder") discrim_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, "discrim") step_enc = optimizer_enc.minimize(L_enc, var_list=encoder_vars) step_dec = optimizer_dec.minimize(L_dec, var_list=decoder_vars) step_dis = optimizer_dis.minimize(L_dis, var_list=discrim_vars) def step(image, label, zp): l_prior, dec_image, l_dis_llike, l_gan, _, _ = sess.run([L_prior, decoded_z, L_dis_llike, L_GAN, step_enc, step_dec], feed_dict={z:zp, img:image, lbl:label, K.learning_phase():1}) return l_prior, dec_image, l_dis_llike, l_gan def step_d(image, label, zp): l_gan, _ = sess.run([L_GAN, step_dis], feed_dict={z:zp, img:image, lbl:label, K.learning_phase():1}) return l_gan</span></span></code> </pre><br>  Functions of drawing pictures after and during the training: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">digit_size = <span class="hljs-number"><span class="hljs-number">28</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_digits</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(*args, invert_colors=False)</span></span></span><span class="hljs-function">:</span></span> args = [x.squeeze() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> args] n = min([x.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> args]) figure = np.zeros((digit_size * len(args), digit_size * n)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(args)): figure[j * digit_size: (j + <span class="hljs-number"><span class="hljs-number">1</span></span>) * digit_size, i * digit_size: (i + <span class="hljs-number"><span class="hljs-number">1</span></span>) * digit_size] = args[j][i].squeeze() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> invert_colors: figure = <span class="hljs-number"><span class="hljs-number">1</span></span>-figure plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">2</span></span>*n, <span class="hljs-number"><span class="hljs-number">2</span></span>*len(args))) plt.imshow(figure, cmap=<span class="hljs-string"><span class="hljs-string">'Greys_r'</span></span>) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) ax = plt.gca() ax.get_xaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) ax.get_yaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) plt.show() <span class="hljs-comment"><span class="hljs-comment"># ,     ,    figs = [[] for x in range(num_classes)] periods = [] save_periods = list(range(100)) + list(range(100, 1000, 10)) n = 15 #   15x15  from scipy.stats import norm #     N(0, I),   ,          grid_x = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) grid_y = norm.ppf(np.linspace(0.05, 0.95, n)) def draw_manifold(label, show=True): #     figure = np.zeros((digit_size * n, digit_size * n)) input_lbl = np.zeros((1, 10)) input_lbl[0, label] = 1 for i, yi in enumerate(grid_x): for j, xi in enumerate(grid_y): z_sample = np.zeros((1, latent_dim)) z_sample[:, :2] = np.array([[xi, yi]]) x_decoded = sess.run(decoded_z, feed_dict={z:z_sample, lbl:input_lbl, K.learning_phase():0}) digit = x_decoded[0].squeeze() figure[i * digit_size: (i + 1) * digit_size, j * digit_size: (j + 1) * digit_size] = digit if show: #  plt.figure(figsize=(15, 15)) plt.imshow(figure, cmap='Greys') plt.grid(False) ax = plt.gca() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) plt.show() return figure #   z def draw_z_distr(z_predicted): im = plt.scatter(z_predicted[:, 0], z_predicted[:, 1]) im.axes.set_xlim(-5, 5) im.axes.set_ylim(-5, 5) plt.show() def on_n_period(period): n_compare = 10 clear_output() #   output #      b = next(test_batches_it) decoded = sess.run(cvae, feed_dict={img:b[0], lbl:b[1], K.learning_phase():0}) plot_digits(b[0][:n_compare], decoded[:n_compare]) #     y draw_lbl = np.random.randint(0, num_classes) print(draw_lbl) for label in range(num_classes): figs[label].append(draw_manifold(label, show=label==draw_lbl)) xs = x_test[y_test == draw_lbl] ys = y_test_cat[y_test == draw_lbl] z_predicted = sess.run(z_mean, feed_dict={img:xs, lbl:ys, K.learning_phase():0}) draw_z_distr(z_predicted) periods.append(period)</span></span></code> </pre><br></div></div><br>  Learning process: <br><br><pre> <code class="python hljs">sess.run(tf.global_variables_initializer()) nb_step = <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-comment"><span class="hljs-comment">#      batches_per_period = 3 for i in range(48000): print('.', end='') #    for j in range(nb_step): b0, b1 = next(train_batches_it) zp = np.random.randn(batch_size, latent_dim) l_g = step_d(b0, b1, zp) if l_g &lt; 1.0: break #      for j in range(nb_step): l_p, zx, l_d, l_g = step(b0, b1, zp) if l_g &gt; 0.4: break b0, b1 = next(train_batches_it) zp = np.random.randn(batch_size, latent_dim) #    if not i % batches_per_period: period = i // batches_per_period if period in save_periods: on_n_period(period) print(i, l_p, l_d, l_g)</span></span></code> </pre><br>  Gif drawing function: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> FuncAnimation <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_2d_figs_gif</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(figs, periods, c, fname, fig, batches_per_period)</span></span></span><span class="hljs-function">:</span></span> norm = matplotlib.colors.Normalize(vmin=<span class="hljs-number"><span class="hljs-number">0</span></span>, vmax=<span class="hljs-number"><span class="hljs-number">1</span></span>, clip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im = plt.imshow(np.zeros((<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>)), cmap=<span class="hljs-string"><span class="hljs-string">'Greys'</span></span>, norm=norm) plt.grid(<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">"Label: {}\nBatch: {}"</span></span>.format(c, <span class="hljs-number"><span class="hljs-number">0</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i)</span></span></span><span class="hljs-function">:</span></span> im.set_array(figs[i]) im.axes.set_title(<span class="hljs-string"><span class="hljs-string">"Label: {}\nBatch: {}"</span></span>.format(c, periods[i]*batches_per_period)) im.axes.get_xaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) im.axes.get_yaxis().set_visible(<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> im anim = FuncAnimation(fig, update, frames=range(len(figs)), interval=<span class="hljs-number"><span class="hljs-number">100</span></span>) anim.save(fname, dpi=<span class="hljs-number"><span class="hljs-number">80</span></span>, writer=<span class="hljs-string"><span class="hljs-string">'ffmpeg'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(num_classes): make_2d_figs_gif(figs[label], periods, label, <span class="hljs-string"><span class="hljs-string">"./figs6/manifold_{}.mp4"</span></span>.format(label), plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)), batches_per_period)</code> </pre><br></div></div><br>  Since we again have a model based on autoencoder, we can apply the style transfer: <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   def style_transfer(X, lbl_in, lbl_out): rows = X.shape[0] if isinstance(lbl_in, int): label = lbl_in lbl_in = np.zeros((rows, 10)) lbl_in[:, label] = 1 if isinstance(lbl_out, int): label = lbl_out lbl_out = np.zeros((rows, 10)) lbl_out[:, label] = 1 #     zp = sess.run(z_mean, feed_dict={img:X, lbl:lbl_in, K.learning_phase():0}) #    ,   created = sess.run(decoded_z, feed_dict={z:zp, lbl:lbl_out, K.learning_phase():0}) return created #    def draw_random_style_transfer(label): n = 10 generated = [] idxs = np.random.permutation(y_test.shape[0]) x_test_permut = x_test[idxs] y_test_permut = y_test[idxs] prot = x_test_permut[y_test_permut == label][:batch_size] for i in range(num_classes): generated.append(style_transfer(prot, label, i)[:n]) generated[label] = prot plot_digits(*generated, invert_colors=True) draw_random_style_transfer(7)</span></span></code> </pre><br></div></div><br><h2>  results </h2><br><h3>  Comparison with a simple CVAE </h3><br>  Top of the original figures, recovered from the bottom. <br><br>  <em>CVAE</em> , hidden dimension - 2 <br><img src="https://habrastorage.org/web/ce0/46d/a33/ce046da333314c68930dede8c70fde5d.png"><br><br>  <em>CVAE + GAN</em> , hidden dimension - 2 <br><img src="https://habrastorage.org/web/683/e02/848/683e028486074212b85bbb8696cc05b7.png"><br><br>  <em>CVAE + GAN</em> , hidden dimension - 8 <br><img src="https://habrastorage.org/web/f6c/975/bbf/f6c975bbf34c425cb955f1ca2007029b.png"><br><br>  The generated digits of each label are sampled from <img src="https://habrastorage.org/getpro/habr/post_images/e3d/6b1/d29/e3d6b1d29a89a32d44eba93c5db11abf.svg" alt="N (0 | I)">  : <br><br> <a href=""><img src="https://habrastorage.org/web/d7a/468/2ef/d7a4682ef6ed41dd90bcddbeb9d1b2e4.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/50e/263/85e/50e26385e2e146a0a33bcc827bfade39.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/5ed/bc2/4f0/5edbc24f0f2b4ce2bd41ca5f21543d02.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/309/49b/061/30949b0613d2460f9a2fae7c19865c08.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/c23/ae0/c3e/c23ae0c3eaea43f29add817bc59b6201.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/944/78f/c98/94478fc985304338968036fbf5c90e67.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/26d/09c/21d/26d09c21d6a04545b7e51998e7f31aaa.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/d2a/d40/2aa/d2ad402aabc34b4a846ff79c0c8b6917.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/5d9/431/ce5/5d9431ce5815417b9a83c8a42f5db6da.png" width="350"></a> <a href=""><img src="https://habrastorage.org/web/6f6/47e/938/6f647e9388a84882964c77c7a8fbc6ff.png" width="350"></a> <br><br><h3>  Learning process </h3><br><div class="spoiler">  <b class="spoiler_title">Gifs</b> <div class="spoiler_text"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/053/0af/03d/0530af03dcf6f7ba483d05537350325c.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/1d3/f62/26d/1d3f6226dbcf41389913b482c8b9c212.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/dab/905/67e/dab90567ed464de19822102de1526752.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/71f/e07/b6a/71fe07b6a28c4c3b8c7a076e96d087f9.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/b51/010/5c4/b510105c4c4a4637b4f38558b1ca1f92.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/712/d61/555/712d6155577e4d39b1786da16391e8c6.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/50c/f6e/ccf/50cf6eccf2aa4ac58253105f5d176041.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/07c/6c3/bb0/07c6c3bb099e4ed7806a6d378b9f437d.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/602/102/111/602102111a374d7aae900f2b867d49e6.gif" width="350"></a> <a href=""><img src="https://habrastorage.org/web/bec/5ec/330/bec5ec33020b4951adae3171af205aaf.gif" width="350"></a> <br></div></div><br><h3>  Style transfer </h3><br>  The ‚Äú7‚Äù was taken as a basis, from the style of which the rest of the figures were already created <img src="https://habrastorage.org/getpro/habr/post_images/336/2f5/9f9/3362f59f97ac50b4f3ac574d0d806cc9.svg" alt="\ dim Z = 8">  ). <br><br>  So it was with a simple <em>CVAE</em> : <br><img src="https://habrastorage.org/web/cfd/3c4/d14/cfd3c4d142f04ce4a5929272c5bb8c6c.png" width="600"><br><br>  And so it became: <br><img src="https://habrastorage.org/web/a02/5f4/fcc/a025f4fcce9645e398df7e92f4ff6121.png" width="600"><br><br><h2>  Conclusion </h2><br>  In my opinion, it turned out very well.  Having gone from the simplest <em>autoencoders</em> , we reached generative models, namely, <em>VAE</em> , <em>GAN</em> , understood what <em>conditional</em> models are, and why the metric is important. <br>  We also learned how to use <em>keras</em> and combine it with the bare <em>tensorflow</em> . <br><br>  Thank you all for your attention, I hope it was interesting! <br><br>  <a href="https://github.com/urtrial/ae_vae_gan"><strong>Repository with all laptops</strong></a> <br><br><h2>  Useful links and literature </h2><br>  Original article: <br>  [1] Autoencoding beyond pixels using a learned similarity metric, Larsen et al, 2016, <a href="https://arxiv.org/abs/1512.09300">https://arxiv.org/abs/1512.09300</a> <br><br>  <em>VAE</em> Tutorial: <br>  [2] Tutorial on Variational Autoencoders, Carl Doersch, 2016, <a href="https://arxiv.org/abs/1606.05908">https://arxiv.org/abs/1606.05908</a> <br><br>  Tutorial on using <em>keras</em> with <em>tensorflow</em> : <br>  [3] <a href="https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html">https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html</a> </div><p>Source: <a href="https://habr.com/ru/post/332074/">https://habr.com/ru/post/332074/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332064/index.html">How to confuse the analyst. Part Two: What is domain modeling?</a></li>
<li><a href="../332066/index.html">Probabilistic and informational analysis of measurement results in Python</a></li>
<li><a href="../332068/index.html">Automation blocking Petya / NonPetya</a></li>
<li><a href="../332070/index.html">We answer readers' questions: what is the IBM Watson cognitive system, and how does it work?</a></li>
<li><a href="../332072/index.html">‚ÄúYou, thunderstorm, threaten, and we hold on to each other!‚Äù - tale about how I saved the ADSL modem</a></li>
<li><a href="../332076/index.html">Dynamic instrumentation is not easy, but trivial *: we write yet another instrumentation for American Fuzzy Lop</a></li>
<li><a href="../332078/index.html">Classifying Text with Java Neural Network</a></li>
<li><a href="../332080/index.html">Hackers and exchanges: how to attack the sphere of finance</a></li>
<li><a href="../332082/index.html">Integration of 1C with DLL using Python</a></li>
<li><a href="../332084/index.html">Work with heterogeneous containers with C ++ 17</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>