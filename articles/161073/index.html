<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A frequency analyzer of English words written in python 3 that can normalize words using WordNet and translate using StarDict</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to all! 
 I learn English and simplify this process in every possible way. Somehow I needed to get a list of words along with the translation an...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A frequency analyzer of English words written in python 3 that can normalize words using WordNet and translate using StarDict</h1><div class="post__text post__text-html js-mediator-article">  Hello to all! <br>  I learn English and simplify this process in every possible way.  Somehow I needed to get a list of words along with the translation and transcription for a specific text.  The task was not difficult, and I set to work.  A little later, a <b>python</b> script was written, which is all able, and even able a little more, since I also wanted to get a frequency dictionary from all the files with English text inside.  So a small set of scripts came out, which I would like to talk about. <br>  The work of the script consists in parsing files, extracting English words, normalizing them, counting and issuing the first countWord words from the entire resulting list of English words. <br>  In the final file, the word is written in the form: <br>  <b>[number of repetitions] [word itself] [translation of a word]</b> <br><br>  What will happen next: <br><ol><li>  We will start by getting a list of English words from a file (using <b>regular expressions</b> ); </li><li>  Then we begin to normalize the words, that is, to bring them from the natural form into the form in which they are stored in dictionaries (here we will study the <b>WordNet</b> format a little); </li><li>  Then we calculate the number of occurrences of all normalized words (this is quick and easy); </li><li>  Next, we delve into the <b>StarDict</b> format, because it is through it that we will get translations and transcriptions. </li><li>  Well, at the very end we will write down the result somewhere (I chose an <b>Excel</b> file). </li></ol><br><a name="habracut"></a><br>  I used <b>python 3.3</b> and I must say more than once regretted that I did not write in python 2.7, because the necessary modules were often not enough. <br><br><h4>  Frequency analyzer. </h4><br>  So, let's start with a simple one, get the files, parse them into words, count, sort, and give the result. <br>  To begin with we will make a regular expression for the search of English words in the text. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Regular expression to search for English words </h5><br>  A simple English word, for example ‚Äúover‚Äù, can be found using the expression <b>"([a-zA-Z] +)"</b> - here one or more letters of the English alphabet are searched for. <br>  A compound word, for example ‚Äúcommander-in-chief‚Äù, is somewhat more difficult to find, we need to look for sub-expressions of the form ‚Äúcommander-‚Äù, ‚Äúin-‚Äù that follow each other, followed by the word ‚Äúchief‚Äù.  The regular expression takes the form <b>"(([a-zA-Z] + -?) * [A-zA-Z] +)"</b> . <br>  If an intermediate subexpression is present in the expression, it is also included in the result.  So, our result includes not only the word ‚Äúcommander-in-chief‚Äù, but also all the found subexpressions. To exclude them, add at the beginning of the subexpression <b>'?:</b> ' To the rhinestone after the opening parenthesis.  Then the regular expression takes the form <b>"((?: [A-zA-Z] + -?) * [A-zA-Z] +)"</b> .  We still have to include words with an apostrophe like "did not" in expressions.  For this we replace in the first subexpression <b>"-?"</b>  <b>on "[-']?"</b>  . <br>  Everything, on it we will finish improvements of the regular expression, it could be improved and further, but we will stop on the such: <br>  <b>"((?: [a-zA-Z] + [- ']?) * [a-zA-Z] +)"</b> <br><br><h5>  The implementation of the frequency analyzer English words </h5><br><br><div class="spoiler">  <b class="spoiler_title">We will write a small class that can extract English words, count them and produce a result.</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- import re import os from collections import Counter class FrequencyDict: def __init__(): #        self.wordPattern = re.compile("((?:[a-zA-Z]+[-']?)*[a-zA-Z]+)") #  (  collections.Counter       ) self.frequencyDict = Counter() #   ,     def ParseBook(self, file): if file.endswith(".txt"): self.__ParseTxtFile(file, self.__FindWordsFromContent) else: print('Warning: The file format is not supported: "%s"' %file) #      txt def __ParseTxtFile(self, txtFile, contentHandler): try: with open(txtFile, 'rU') as file: for line in file: #    contentHandler(line) #       except Exception as e: print('Error parsing "%s"' % txtFile, e) #               def __FindWordsFromContent(self, content): result = self.wordPattern.findall(content) #       for word in result: word = word.lower() #      self.frequencyDict[word] += 1 #         #    countWord   ,      def FindMostCommonElements(self, countWord): dict = list(self.frequencyDict.items()) dict.sort(key=lambda t: t[0]) dict.sort(key=lambda t: t[1], reverse = True) return dict[0 : int(countWord)]</span></span></code> </pre> <br></div></div><br><br>  On this, in essence, work with a frequency dictionary could have been completed, but our work is just beginning.  The thing is that the words in the text are written taking into account the grammatical rules, which means that the text may contain words with the endings ed, ing, and so on.  In fact, even the forms of the verb to be (am, is, are) will be counted for different words. <br>  So before the word is added to the word counter, you need to bring it to the correct form. <br>  We turn to the second part - the writing of the <b>English word normalizer</b> . <br><br><h4>  English lemmatizer </h4><br>  There are two algorithms - <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B3">stemming</a> and <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">lemmatization</a> .  Stemming refers to heuristic analysis, it does not use any databases.  During lemmatization, various bases of words are used, and transformations are applied according to grammatical rules.  We will use lemmatization for our purposes, since the error of the result is much less than with the stemming. <br><br>  About lemmatization, there have already been several articles on Habr√©, for example, <a href="http://habrahabr.ru/post/123334/">here</a> and <a href="http://habrahabr.ru/post/49421/">here</a> .  They use <a href="http://aot.ru/">aot</a> bases.  I did not want to repeat myself, and it was also interesting to look for some other bases for lemmatization.  I would like to tell you about <a href="http://ru.wikipedia.org/wiki/WordNet">WordNet</a> , and we will build a lemmatizer on it.  To begin with, on the <a href="http://wordnet.princeton.edu/wordnet/download/">official WordNet website</a> you can download the source code of the program and the databases themselves.  WordNet can do a lot, but we need only a small part of its capabilities - the normalization of words. <br>  We need only databases.  The WordNet source code (in C) describes the normalization process itself, in essence, I took the algorithm from there, rewriting it in python.  Oh yes, of course for WordNet there is a library for python - <a href="http://nltk.org/">nltk</a> , but firstly, it only works on python 2.7, and secondly, how quickly I looked, during normalization, only requests are sent to the WordNet server. <br>  General class diagram for a lemmatizer: <br><br><img src="https://habrastorage.org/storage2/355/a91/971/355a919713a1e1d9bebbe54c943665d0.png"><br><br>  As can be seen from the diagram, only 4 parts of speech (nouns, verbs, adjectives and adverbs) are normalized. <br>  Briefly describe the normalization process, it is as follows: <br>  1. For each part of speech, 2 files are loaded from WordNet - an index dictionary (it has an index name and an extension according to a part of speech, for example, index.adv for adverbs) and an exclusion file (it has an exc extension and a name according to a part of speech, for example adv.exc for adverbs). <br>  2. During normalization, the array of exceptions is first checked, if the word is there, its normalized form is returned.  If the word is not an exception, then the word ghost begins according to grammatical rules, that is, the ending is cut off, a new ending is pasted, then the word is searched for in the indexed array, and if it is there, then the word is considered normalized.  Otherwise, the following rule applies, and so on, until the rules end or the word is normalized earlier. <br>  Classes for lemmalizator: <br><div class="spoiler">  <b class="spoiler_title">Base class for parts of speech BaseWordNetItem.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- import os class BaseWordNetItem: #  def __init__(self, pathWordNetDict, excFile, indexFile): self.rule=() #        . self.wordNetExcDict={} #   self.wordNetIndexDict=[] #   self.excFile = os.path.join(pathWordNetDict, excFile) #      self.indexFile = os.path.join(pathWordNetDict, indexFile) #      self.__ParseFile(self.excFile, self.__AppendExcDict) #    self.__ParseFile(self.indexFile, self.__AppendIndexDict) #    self.cacheWords={} #  .     ,  -  ,  -   #       . #     : [-][][] def __AppendExcDict(self, line): #     ,     2      (  - ,  - ).         group = [item.strip() for item in line.replace("\n","").split(" ")] self.wordNetExcDict[group[0]] = group[1] #       . def __AppendIndexDict(self, line): #        group = [item.strip() for item in line.split(" ")] self.wordNetIndexDict.append(group[0]) #     ,          ,    def __ParseFile(self, file, contentHandler): try: with open(file, 'r') as openFile: for line in openFile: contentHandler(line) #       except Exception as e: raise Exception('File does not load: "%s"' %file) #      .      ,   . #        def _GetDictValue(self, dict, key): try: return dict[key] except KeyError: return None #     ,    True,  False. #  ,  ,   ,   (       ). def _IsDefined(self, word): if word in self.wordNetIndexDict: return True return False #   (  ) def GetLemma(self, word): word = word.strip().lower() #     if word == None: return None #   ,           lemma = self._GetDictValue(self.cacheWords, word) if lemma != None: return lemma # ,      ,    if self._IsDefined(word): return word #   ,    ,     lemma = self._GetDictValue(self.wordNetExcDict, word) if lemma != None: return lemma #    ,         ,      . lemma = self._RuleNormalization(word) if lemma != None: self.cacheWords[word] = lemma #       return lemma return None #     (  ,     ) def _RuleNormalization(self, word): #    ,         ,  ,   . for replGroup in self.rule: endWord = replGroup[0] if word.endswith(endWord): lemma = word #     lemma = lemma.rstrip(endWord) #    lemma += replGroup[1] #    if self._IsDefined(lemma): # ,        ,    ,    return lemma return None</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class for the normalization of verbs WordNetVerb.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from WordNet.BaseWordNetItem import BaseWordNetItem #     #    BaseWordNetItem class WordNetVerb(BaseWordNetItem): def __init__(self, pathToWordNetDict): #   (BaseWordNetItem) BaseWordNetItem.__init__(self, pathToWordNetDict, 'verb.exc', 'index.verb') #        .  ,  "s"   "" , "ies"   "y" . self.rule = ( ["s" , "" ], ["ies" , "y" ], ["es" , "e" ], ["es" , "" ], ["ed" , "e" ], ["ed" , "" ], ["ing" , "e" ], ["ing" , "" ] ) #      GetLemma(word)     BaseWordNetItem</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class for the normalization of nouns WordNetNoun.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from WordNet.BaseWordNetItem import BaseWordNetItem #       #    BaseWordNetItem class WordNetNoun(BaseWordNetItem): def __init__(self, pathToWordNetDict): #   (BaseWordNetItem) BaseWordNetItem.__init__(self, pathToWordNetDict, 'noun.exc', 'index.noun') #        .  ,  "s"   "", "ses"   "s"  . self.rule = ( ["s" , "" ], ["'s" , "" ], ["'" , "" ], ["ses" , "s" ], ["xes" , "x" ], ["zes" , "z" ], ["ches" , "ch" ], ["shes" , "sh" ], ["men" , "man" ], ["ies" , "y" ] ) #    (  ) #       BaseWordNetItem,          , #      def GetLemma(self, word): word = word.strip().lower() #    ,         if len(word) &lt;= 2: return None #     "ss",         if word.endswith("ss"): return None #   ,           lemma = self._GetDictValue(self.cacheWords, word) if lemma != None: return lemma # ,      ,    if self._IsDefined(word): return word #   ,    ,     lemma = self._GetDictValue(self.wordNetExcDict, word) if (lemma != None): return lemma #     "ful",   "ful",   ,     . #  ,  ,   "spoonsful"    "spoonful" suff = "" if word.endswith("ful"): word = word[:-3] #   "ful" suff = "ful" #   "ful",     #    ,         ,      . lemma = self._RuleNormalization(word) if (lemma != None): lemma += suff #     "ful",    self.cacheWords[word] = lemma #       return lemma return None</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class for the normalization of adverbs WordNetAdverb.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from WordNet.BaseWordNetItem import BaseWordNetItem #     #    BaseWordNetItem class WordNetAdverb(BaseWordNetItem): def __init__(self, pathToWordNetDict): #   (BaseWordNetItem) BaseWordNetItem.__init__(self, pathToWordNetDict, 'adv.exc', 'index.adv') #      (adv.exc)    (index.adv). #           .</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class for the normalization of adjectives WordNetAdjective.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from WordNet.BaseWordNetItem import BaseWordNetItem #       #    BaseWordNetItem class WordNetAdjective(BaseWordNetItem): def __init__(self, pathToWordNetDict): #   (BaseWordNetItem) BaseWordNetItem.__init__(self, pathToWordNetDict, 'adj.exc', 'index.adj') #        .  ,  "er"   ""  "e"  . self.rule = ( ["er" , "" ], ["er" , "e"], ["est" , "" ], ["est" , "e"] ) #      GetLemma(word)     BaseWordNetItem</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Lemmatizer class Lemmatizer.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from WordNet.WordNetAdjective import WordNetAdjective from WordNet.WordNetAdverb import WordNetAdverb from WordNet.WordNetNoun import WordNetNoun from WordNet.WordNetVerb import WordNetVerb class Lemmatizer: def __init__(self, pathToWordNetDict): #    self.splitter = "-" #      adj = WordNetAdjective(pathToWordNetDict) #  noun = WordNetNoun(pathToWordNetDict) #  adverb = WordNetAdverb(pathToWordNetDict) #  verb = WordNetVerb(pathToWordNetDict) #  self.wordNet = [verb, noun, adj, adverb] #     (, ) def GetLemma(self, word): #     ,    ,   ( )  ,    wordArr = word.split(self.splitter) resultWord = [] for word in wordArr: lemma = self.__GetLemmaWord(word) if (lemma != None): resultWord.append(lemma) if (resultWord != None): return self.splitter.join(resultWord) return None #   (  ) def __GetLemmaWord(self, word): for item in self.wordNet: lemma = item.GetLemma(word) if (lemma != None): return lemma return None</span></span></code> </pre><br></div></div><br><br>  Well, with the normalization finished.  Now the frequency analyzer is able to normalize words.  We proceed to the last part of our task - receiving translations and transcriptions for English words. <br><br><h4>  Translator of foreign words using StarDict dictionaries </h4><br>  You can write about <a href="http://ru.wikipedia.org/wiki/StarDict">StarDict for a</a> long time, but the main advantage of this format is that there are a lot of vocabulary databases for it, in almost all languages.  On Habr√© there were no articles on StarDict yet and it is time to fill this gap.  The file that describes the StarDict format is usually located next to the sources themselves. <br>  If we discard all the additions, then the most minimal set of knowledge on this format will be as follows: <br>  Each dictionary must contain 3 required files: <br><br>  1. File with <b>ifo</b> extension - contains a consistent description of the dictionary itself; <br>  2. File with <b>idx</b> extension.  Each entry inside the idx file consists of 3 fields, one after the other: <br><ul><li>  <b>word_str</b> - A string in the utf-8 format, ending with '\ 0'; </li><li>  <b>word_data_offset ‚Äî</b> Offset before entry in the .dict file (32 or 64 bits); </li><li>  <b>word_data_size</b> - The size of the entire entry in the .dict file. </li></ul><br>  3. File with the <b>dict</b> extension - contains the translations themselves, which can be reached by knowing the offset to the translation (the offset is recorded in the idx file). <br><br>  Without long thinking about which classes should end up, I created one class for each of the files, and one general StarDict class that unites them. <br>  The resulting class diagram is: <br><br><img src="https://habrastorage.org/storage2/226/5cd/b47/2265cdb471d0b995a847da0c4fadb746.png"><br><br>  Classes for StarDict translator: <br><div class="spoiler">  <b class="spoiler_title">Base class for BaseStarDictItem.py dictionary items</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- import os class BaseStarDictItem: def __init__(self, pathToDict, exp): #     self.encoding = "utf-8" #      self.dictionaryFile = self.__PathToFileInDirByExp(pathToDict, exp) #    self.realFileSize = os.path.getsize(self.dictionaryFile) #     path      exp def __PathToFileInDirByExp(self, path, exp): if not os.path.exists(path): raise Exception('Path "%s" does not exists' % path) end = '.%s'%(exp) list = [f for f in os.listdir(path) if f.endswith(end)] if list: return os.path.join(path, list[0]) #    else: raise Exception('File does not exist: "*.%s"' % exp)</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class ifo.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from StarDict.BaseStarDictItem import BaseStarDictItem from Frequency.IniParser import IniParser class Ifo(BaseStarDictItem): def __init__(self, pathToDict): #   (BaseStarDictItem) BaseStarDictItem.__init__(self, pathToDict, 'ifo') #     self.iniParser = IniParser(self.dictionaryFile) #   ifo   #        ,        self.bookName = self.__getParameterValue("bookname", None) #   [ ] self.wordCount = self.__getParameterValue("wordcount", None) #    ".idx"  [ ] self.synWordCount = self.__getParameterValue("synwordcount", "") #    ".syn"   [ ,    ".syn"] self.idxFileSize = self.__getParameterValue("idxfilesize", None) #  ( ) ".idx" .    ,        [ ] self.idxOffsetBits = self.__getParameterValue("idxoffsetbits", 32) #    (32  64),         .dict.      3.0.0,      32 [ ] self.author = self.__getParameterValue("author", "") #   [ ] self.email = self.__getParameterValue("email", "") #  [ ] self.description = self.__getParameterValue("description", "") #   [ ] self.date = self.__getParameterValue("date", "") #    [ ] self.sameTypeSequence = self.__getParameterValue("sametypesequence", None) # ,    [ ] self.dictType = self.__getParameterValue("dicttype", "") #     ,  WordNet[ ] def __getParameterValue(self, key, defaultValue): try: return self.iniParser.GetValue(key) except: if defaultValue != None: return defaultValue raise Exception('\n"%s" has invalid format (missing parameter: "%s")' % (self.dictionaryFile, key))</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class Idx.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from struct import unpack from StarDict.BaseStarDictItem import BaseStarDictItem class Idx(BaseStarDictItem): #  def __init__(self, pathToDict, wordCount, idxFileSize, idxOffsetBits): #   (BaseStarDictItem) BaseStarDictItem.__init__(self, pathToDict, 'idx') self.idxDict ={} # , self.idxDict = {'.': [_____dict, _____dict], ...} self.idxFileSize = int(idxFileSize) #   .idx,   .ifo  self.idxOffsetBytes = int(idxOffsetBits/8) #  ,         .dict.        self.wordCount = int(wordCount) #    ".idx"  #    (  .ifo    .idx  [idxfilesize]      ) self.__CheckRealFileSize() #   self.idxDict    .idx self.__FillIdxDict() #    (  .ifo     [wordcount]        .idx ) self.__CheckRealWordCount() #    ,   .ifo ,           def __CheckRealFileSize(self): if self.realFileSize != self.idxFileSize: raise Exception('size of the "%s" is incorrect' %self.dictionaryFile) #    ,   .ifo ,       .idx       def __CheckRealWordCount(self): realWordCount = len(self.idxDict) if realWordCount != self.wordCount: raise Exception('word count of the "%s" is incorrect' %self.dictionaryFile) #         ,      def __getIntFromByteArray(self, sizeInt, stream): byteArray = stream.read(sizeInt) #   ,    #       formatCharacter = 'L' #   "unsigned long" ( sizeInt = 4) if sizeInt == 8: formatCharacter = 'Q' #   "unsigned long long" ( sizeInt = 8) format = '&gt;' + formatCharacter #     : "  " + " " #  '&gt;' - ,       int( formatCharacter)     . integer = (unpack(format, byteArray))[0] #       return int(integer) #    .idx    (   3- )       self.idxDict def __FillIdxDict(self): languageWord = "" with open(self.dictionaryFile, 'rb') as stream: while True: byte = stream.read(1) #    if not byte: break #    ,     if byte != b'\0': #        '\0',      languageWord += byte.decode("utf-8") else: #    '\0',  ,         ("     dict"  "     dict") wordDataOffset = self.__getIntFromByteArray(self.idxOffsetBytes, stream) #    "     dict" wordDataSize = self.__getIntFromByteArray(4, stream) #    "     dict" self.idxDict[languageWord] = [wordDataOffset, wordDataSize] #    self.idxDict :   +  +   languageWord = "" #  ,     #       .dict ("     dict"  "     dict"). #      ,   None def GetLocationWord(self, word): try: return self.idxDict[word] except KeyError: return [None, None]</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Class Dict.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- from StarDict.BaseStarDictItem import BaseStarDictItem #     ( , sametypesequence = tm). #  -x    (       utf-8,  '\0'): # 'm' -     utf-8,  '\0' # 'l' -       utf-8,  '\0' # 'g' -        Pango # 't' -    utf-8,  '\0' # 'x' -    utf-8,    xdxf # 'y' -    utf-8,  (YinBiao)   (KANA)  # 'k' -    utf-8,    KingSoft PowerWord XML # 'w' -     MediaWiki # 'h' -     Html # 'n' -     WordNet # 'r' -    .      (jpg),  (wav),  (avi), (bin)   . # 'W' - wav  # 'P' -  # 'X' -       class Dict(BaseStarDictItem): def __init__(self, pathToDict, sameTypeSequence): #   (BaseStarDictItem) BaseStarDictItem.__init__(self, pathToDict, 'dict') # ,     self.sameTypeSequence = sameTypeSequence def GetTranslation(self, wordDataOffset, wordDataSize): try: #              .dict self.__CheckValidArguments(wordDataOffset, wordDataSize) #   .dict   with open(self.dictionaryFile, 'rb') as file: #   file.seek(wordDataOffset) #      ,     byteArray = file.read(wordDataSize) #   ,     return byteArray.decode(self.encoding) #       o (self.encoding     BaseDictionaryItem) except Exception: return None def __CheckValidArguments(self, wordDataOffset, wordDataSize): if wordDataOffset is None: pass if wordDataOffset &lt; 0: pass endDataSize = wordDataOffset + wordDataSize if wordDataOffset &lt; 0 or wordDataSize &lt; 0 or endDataSize &gt; self.realFileSize: raise Exception</span></span></code> </pre><br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, the translator is ready. </font><font style="vertical-align: inherit;">Now we just have to combine the frequency analyzer, the word normalizer and the translator. </font><font style="vertical-align: inherit;">Let's create the main main.py file and the Settings.ini file.</font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Main file main.py</font></font></b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># -*- coding: utf-8 -*- import os import xlwt3 as xlwt from Frequency.IniParser import IniParser from Frequency.FrequencyDict import FrequencyDict from StarDict.StarDict import StarDict ConfigFileName="Settings.ini" class Main: def __init__(self): self.listLanguageDict = [] #      StarDict self.result = [] #      ( , ,  ) try: #    - config = IniParser(ConfigFileName) self.pathToBooks = config.GetValue("PathToBooks") #   ini   PathToBooks,     (,   ),      self.pathResult = config.GetValue("PathToResult") #   ini   PathToResult,       self.countWord = config.GetValue("CountWord") #   ini   CountWord,       ,    self.pathToWordNetDict = config.GetValue("PathToWordNetDict") #   ini   PathToWordNetDict,      WordNet self.pathToStarDict = config.GetValue("PathToStarDict") #   ini   PathToStarDict,        StarDict #    StarDict           .      listPathToStarDict listPathToStarDict = [item.strip() for item in self.pathToStarDict.split(";")] #       StarDict     for path in listPathToStarDict: languageDict = StarDict(path) self.listLanguageDict.append(languageDict) #   ,      self.listBooks = self.__GetAllFiles(self.pathToBooks) #    self.frequencyDict = FrequencyDict(self.pathToWordNetDict) #  ,   StarDict  WordNet.    ,      ,     self.__Run() except Exception as e: print('Error: "%s"' %e) #    ,    path def __GetAllFiles(self, path): try: return [os.path.join(path, file) for file in os.listdir(path)] except Exception: raise Exception('Path "%s" does not exists' % path) #     ,      .        ,    def __GetTranslate(self, word): valueWord = "" for dict in self.listLanguageDict: valueWord = dict.Translate(word) if valueWord != "": return valueWord return valueWord #   ( , ,  )   countWord     Excel def __SaveResultToExcel(self): try: if not os.path.exists(self.pathResult): raise Exception('No such directory: "%s"' %self.pathResult) if self.result: description = 'Frequency Dictionary' style = xlwt.easyxf('font: name Times New Roman') wb = xlwt.Workbook() ws = wb.add_sheet(description + ' ' + self.countWord) nRow = 0 for item in self.result: ws.write(nRow, 0, item[0], style) ws.write(nRow, 1, item[1], style) ws.write(nRow, 2, item[2], style) nRow +=1 wb.save(os.path.join(self.pathResult, description +'.xls')) except Exception as e: print(e) #      def __Run(self): #       for book in self.listBooks: self.frequencyDict.ParseBook(book) #   countWord        mostCommonElements = self.frequencyDict.FindMostCommonElements(self.countWord) #      for item in mostCommonElements: word = item[0] counterWord = item[1] valueWord = self.__GetTranslate(word) self.result.append([counterWord, word, valueWord]) #      Excel self.__SaveResultToExcel() if __name__ == "__main__": main = Main()</span></span></code> </pre><br></div></div><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Settings.ini settings file</font></font></b> <div class="spoiler_text"><pre> <code class="python hljs">;   (,   ),      PathToBooks = e:\Bienne\Frequency\Books ;    WordNet(    ) PathToWordNetDict = e:\Bienne\Frequency\WordNet\wn3<span class="hljs-number"><span class="hljs-number">.1</span></span>.dict\ ;      StarDict(   ) PathToStarDict = e:\Bienne\Frequency\Dict\stardict-comn_dictd04_korolew ;     ,        Excel CountWord = <span class="hljs-number"><span class="hljs-number">100</span></span> ; ,    (   Excel     -  , ,  ) PathToResult = e:\Bienne\Frequency\Books</code> </pre><br></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The only third-party library that you need to download and deliver additionally is </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">xlwt</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , it will be required to create an Excel file (the result is written there). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the Settings.ini settings file for the PathToStarDict variable, you can write several dictionaries with a ";". </font><font style="vertical-align: inherit;">In this case, the words will be searched in the order of priority of the dictionaries - if the word is found in the first dictionary, the search ends, otherwise all the other StarDict dictionaries are searched.</font></font><br><br><h4>  Afterword </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All source codes described in this article can be downloaded on </font></font><a href="https://github.com/Bienne/Frequency"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reminder:</font></font><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The scripts were written under </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">windows</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ;</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Used </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">for python 3.3</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ;</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Additionally, you will need to put the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">xlwt</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> library </font><font style="vertical-align: inherit;">to work with Excel;</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Separately, you need to download dictionary databases for </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WordNet</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">StarDict</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (for StarDict dictionaries, you will need to additionally unpack files packed into the archive with the dict extension);</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Settings.ini</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> file </font><font style="vertical-align: inherit;">you need to register paths for dictionaries and where to save the result.</font></font></li><li>      ,        StarDict,             (      ). </li></ol></div><p>Source: <a href="https://habr.com/ru/post/161073/">https://habr.com/ru/post/161073/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../161057/index.html">Do you type blindly?</a></li>
<li><a href="../161061/index.html">8 principles to make the product that people want - squeeze out Mike Krieger's speech on 500 Startups' Warm Gun</a></li>
<li><a href="../161063/index.html">An algorithm for annotating illustrations, or why not a programmer to be a bit of a designer?</a></li>
<li><a href="../161067/index.html">Stop twisting - 2. About ways to fix the cable</a></li>
<li><a href="../161071/index.html">In SearchMan made a rating of "detectability" of applications on the App Store</a></li>
<li><a href="../161075/index.html">Windows Store numbers for the first month of Windows 8</a></li>
<li><a href="../161077/index.html">Design Camp - the first Microsoft design conference in Russia + Winter Design School for Windows 8</a></li>
<li><a href="../161079/index.html">Sandy victims will be able to find their lost photos online</a></li>
<li><a href="../161081/index.html">Android in every TV: ASUS and Netgear prepare Google TV devices, according to FCC</a></li>
<li><a href="../161083/index.html">TouchDevelop - creating applications on touch devices directly from the browser</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>