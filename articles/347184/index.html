<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Generative Modeling and AI</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the previous chapter, we talked about classical discriminative models in machine learning and dismantled the simplest examples of such models. Let'...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Generative Modeling and AI</h1><div class="post__text post__text-html js-mediator-article">  In the <a href="https://habrahabr.ru/post/343800/">previous chapter,</a> we talked about classical discriminative models in machine learning and dismantled the simplest examples of such models.  Let's now look at the bigger picture. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/05/sx/9a/05sx9a4nprmgcf26aulbzfi9qh4.gif"></div><br><a name="habracut"></a><br><h2>  Artificial Intelligence Task </h2><br>  Artificial Intelligence (AI) is an algorithm that can solve problems that are typically solved by people, at a level comparable or superior to human.  This can be the recognition of visual images, understanding and analysis of texts, control mechanisms and the construction of logical chains.  An artificial intelligence system is a system that can effectively solve all such tasks at once.  The solution to this problem still does not exist, but there are various approaches that can be called the first steps towards solving the problem of AI. <br><br>  In the twentieth century, the rule-based approach was the most popular.  His idea is that the world obeys laws that, for example, physics and other natural sciences are studying.  And even if they cannot be effectively programmed in AI directly, it is reasonable to assume that with a sufficient number of programmed rules, an AI system based on computers can effectively exist in a world based on these rules and solve arbitrary tasks.  This approach does not take into account the natural stochasticity of some events.  To take it into account, it is necessary to construct a probabilistic model on this list of rules for making stochastic decisions, provided random input data are available. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      A full description of the world would require such a large number of rules that it is impossible to set and maintain them manually.  Hence the idea to conduct enough observations of events in the real world and derive the necessary rules automatically from these observations.  This idea immediately supports both the actual stochasticity of some natural processes, as well as the imaginary stochasticity, arising from the fact that it is not fully known what factors influence a certain deterministic process.  An automatic derivation of the rules from observations is done by a section of mathematics called machine learning, and it is this that is currently the most promising foundation for AI. <br><br><h2>  AI machine trained </h2><br>  In the previous chapter, we looked at the classical machine learning problems (classification and regression) and the classical linear methods for solving them (logistic and linear regression).  In reality, for complex problems, nonlinear models are used, usually based either on decision trees or on deep artificial neural networks, which are better shown for processing dense data with a high level of redundancy: images, sound and text. <br><br>  These methods are capable of automatically deriving rules based on observations, and they are very successfully applied in commercial applications.  However, they have a disadvantage, because of which they are insufficient for use as an AI system - they are specifically designed to solve a specific, highly specialized task, for example, to distinguish cats and dogs in an image.  Obviously, a model that summarizes an image into a single number loses a lot of data.  To determine the cat on the image does not necessarily understand the essence of the cat, you just need to learn how to look for the main features of this cat.  The task of classifying an image does not imply a complete understanding of the scene, but only a search for specific objects in it.  It is not possible to determine the classifier for all possible combinations of connections for all possible combinations of objects, if only because of the exponentially large amount of observations that need to be collected and marked.  Therefore, the ideas of classical problems of machine learning with reinforcement, in which a specific task is solved, are not suitable.  For AI, a fundamentally different approach to the formulation of the problem for machine learning is needed. <br><br><h2>  Probabilistic formulation of the problem of understanding the world </h2><br>  So, we need, having a set of observations, in a sense, to understand the process that generates them.  We reformulate this problem in a probabilistic language.  Let observation be the realization of a random variable. <img src="https://habrastorage.org/getpro/habr/post_images/6da/1d6/4f3/6da1d64f302a295b65cba84d4078eada.gif" title="x: \ Omega \ to X% 0">  and there is a set of observed events <img src="https://habrastorage.org/getpro/habr/post_images/2d3/d05/cab/2d3d05cab4a4f1e57731c5657c74b243.gif" title="D = \ {x_i \ sim P (x), i = \ overline {1, N} \}% 0">  .  Then the ‚Äúunderstanding‚Äù of the whole variety of events can be formulated as the task of restoring the distribution <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  . <br><br>  There are several approaches to solving this problem.  One of the most common methods is the introduction of a latent variable.  Let's say there is some kind of representation <img src="https://habrastorage.org/getpro/habr/post_images/44f/399/470/44f399470b605ab36a7b4e6bf9c3c661.gif" title="z \ sim Q (z)% 0">  observations <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x% 0">  .  This presentation describes the ‚Äúunderstanding‚Äù of the observation model.  For example, for the frame of a computer game such an understanding can be the relevant state of the game world and the position of the camera.  In this case <img src="https://habrastorage.org/getpro/habr/post_images/3c1/55f/792/3c155f7927351f2043c3597a462d47a2.gif" title="P (x) = \ int_Z P (x | z) P (z) dz% 0">  .  Fixing <img src="https://habrastorage.org/getpro/habr/post_images/ca3/557/073/ca35570735f832b9f0f49ac5ea35fbec.gif" title="P (z)% 0">  so that it is easy to sample the distribution, we get a model in which <img src="https://habrastorage.org/getpro/habr/post_images/836/113/550/8361135504256f60decbf2f084de141b.gif" title="P (x | z)% 0">  and <img src="https://habrastorage.org/getpro/habr/post_images/4b0/717/f0e/4b0717f0ea04213a65d5a6f42ddb8386.gif" title="P (z | x)% 0">  you can bring neural networks.  Teaching this model with standard deep learning methods can be obtained <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  according to the formula above, after which we can use it in probabilistic inference.  More precise formulations of such models will be in the subsequent parts, but here it should be noted that complex versions of such models require approximation of complex noncomputable integrals, for which approximate methods are usually used, such as <a href="https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a> or <a href="https://en.wikipedia.org/wiki/Variational_Bayesian_methods">Variational Inference</a> .  Building <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  to extract samples from it is called the task of generative modeling. <br><br>  You can approach the question differently.  In fact, explicitly count <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  , approximating complex integrals is not necessary.  One idea is that if a model can ‚Äúimagine‚Äù the world for itself, then it understands how it works.  For example, if I can draw a person in different positions and angles, then I understand how his anatomy and perspective as a whole are arranged.  If people cannot distinguish objects generated by a model from real ones, then the model has been able to honestly ‚Äúunderstand‚Äù how this or that phenomenon works as well as these people understand it.  This idea inspires the development of generative modeling with implicit <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  - development of models, which, having a finite number of observations, are able to generalize them and generate new observations that are indistinguishable from real ones.  Let's pretend that <img src="https://habrastorage.org/getpro/habr/post_images/7f4/615/ad4/7f4615ad4e764c0dda1b500623cb6e9b.gif" title="z \ sim N (0, 1)% 0">  , or any other simple sampling distribution.  Then, under fairly general conditions, there is a function <img src="https://habrastorage.org/getpro/habr/post_images/201/c43/efc/201c43efc4ec0b6139801bc8b66df669.gif" title="f: Z \ to X% 0">  such that <img src="https://habrastorage.org/getpro/habr/post_images/fcd/9d7/66b/fcd9d766bd1120934aed1a6e3049fada.gif" title="f (z) \ sim P (x)% 0">  .  In this case, we do not get <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  explicitly, but the model still contains information about it implicitly.  Instead of recovering <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  can recover <img src="https://habrastorage.org/getpro/habr/post_images/a7a/de4/2fb/a7ade42fbdb03eddb77600e3f9ae2bfe.gif" title="f (z)% 0">  followed by samples from <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)% 0">  can be obtained as <img src="https://habrastorage.org/getpro/habr/post_images/3f7/15d/2c3/3f715d2c3abbe583d6f29a1822af9710.gif" title="f (z), z \ sim N (0,1)% 0">  . <img src="https://habrastorage.org/getpro/habr/post_images/a7a/de4/2fb/a7ade42fbdb03eddb77600e3f9ae2bfe.gif" title="f (z)% 0">  cannot be used in probabilistic output directly, but, firstly, it is not always necessary, and secondly, when it is needed, you can often use Monte Carlo methods, for which you just need to get samples.  The methods of this type include the Generative Adversarial Networks model, which is explored in the next section. <br><br><h2>  Principal component analysis </h2><br>  Let's look at a simple generative model.  Let there be some observable value <img src="https://habrastorage.org/getpro/habr/post_images/ae2/aae/fc6/ae2aaefc6f763bd300279266de63ecb4.gif" title="x \ sim P (x)% 0">  .  For example, it can be a person's height or a pixel image.  Suppose that this value is fully explained by some hidden (latent) value <img src="https://habrastorage.org/getpro/habr/post_images/44f/399/470/44f399470b605ab36a7b4e6bf9c3c661.gif" title="z \ sim P (z)% 0">  .  In our analogy, this may be the weight of a person or an object and its orientation in an image.  Suppose now that for a hidden quantity <img src="https://habrastorage.org/getpro/habr/post_images/8e9/814/2d6/8e98142d609bd2e16e35679151ea55db.gif" title="P (z) = N (z; 0, 1)% 0">  , and the observed value depends linearly with normal noise, i.e. <img src="https://habrastorage.org/getpro/habr/post_images/de5/5b6/956/de55b69567ac6599a60e4a26e433b480.gif" title="P (x | z) = N (x; Wz + b, \ sigma ^ 2 I)% 0">  .  This model is called Probabilistic Principal Component Analysis (PPCA) and, in essence, it is a probabilistic reformulation of the classical <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">Principal Component Analysis</a> (PCA) model, in which the visible variable <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x% 0">  linearly dependent on latent variable <img src="https://habrastorage.org/getpro/habr/post_images/f5c/c9e/478/f5cc9e47883d3f86d5795d82e36323c4.gif" title="z% 0">  no noise. <br><br><h2>  Expectation Maximization </h2><br>  <a href="https://en.wikipedia.org/wiki/Expectation%25E2%2580%2593maximization_algorithm">Expectation Maximization (EM)</a> - an algorithm for training models with latent variables.  Details can be found in the specialized literature, but the general essence of the algorithm is quite simple: <br><br><ol><li>  Initialize the model with initial parameter values. </li><li>  E-step.  Fill in the latent variables with their expected values ‚Äã‚Äãin the current model. </li><li>  M-pitch.  Maximize the likelihood of a model with fixed values ‚Äã‚Äãof latent variables.  For example, gradient descent on the parameters. </li><li>  Repeat (2, 3) until the expected values ‚Äã‚Äãof the latent variables stop changing. </li></ol><br>  If the M-step does not maximize the likelihood to the end, but only take a step in the direction of the maximum, it is called Generalized EM (GEM). <br><br><h2>  PCA Solution with EM </h2><br>  Let us apply to our PCA EM model an algorithm and a maximum likelihood method for finding optimal parameters. <img src="https://habrastorage.org/getpro/habr/post_images/73c/8e4/3cf/73c8e43cf4fc8ec0fe30f50441e7c1b1.gif" title="\ theta = (W, b, \ sigma)% 0">  models.  The joint likelihood of the observed and latent parameters can be written as: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8a0/879/de6/8a0879de6fc0f26e8f43cfe4c2afec55.gif" title="L (x | \ theta) = \ log P (x | \ theta) = \ log P (x | \ theta) \ int_z q (z) = \ int_z q (z) \ log P (x | \ theta)% 0"></div><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/623/c53/913/623c539132e71fa79a9088c940c4bf33.gif" title="q (z)% 0">  - this is an arbitrary distribution.  Next, we will omit the conditional distribution of the parameters in order to facilitate the formulas. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/471/fde/cc1/471fdecc1edf92fc4b59314c8d233724.gif" title="\ int_z q (z) \ log P (x | \ theta) = \ int_z q (z) \ log \ frac {P (x, z)} {P (z | x)} = \ int_z q (z) \ log \ frac {P (x, z) q (z)} {q (z) P (z | x)} ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bba/b4d/779/bbab4d779c205d76a65fc7078ab45b22.gif" title="= \ int_zq (z) \ logP (x, z) - \ int_zq (z) \ logq (z) + \ int_zq (z) \ log \ frac {q (z)} {P (z | x)} ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d6d/2d4/63c/d6d2d463ccdf4891d4195a7a8a165879.gif" title="= \ int_z q (z) \ log P (x, z) + H \ left (q \ left (z \ right) \ right) + KL (q (z) || P (z | x))% 0"></div><br>  Where is the value <img src="https://habrastorage.org/getpro/habr/post_images/eef/017/38f/eef01738fb8b22cb3ceabb2126bdd51b.gif" title="KL (q (z) || P (z | x)) = \ int_z q (z) \ log \ frac {q (z)} {P (z | x)}">  called <img src="https://habrastorage.org/getpro/habr/post_images/dcf/936/eab/dcf936eab85d6d0539498d793c847eaa.gif" title="KL">  divergence between distributions <img src="https://habrastorage.org/getpro/habr/post_images/623/c53/913/623c539132e71fa79a9088c940c4bf33.gif" title="q (z)">  and <img src="https://habrastorage.org/getpro/habr/post_images/4b0/717/f0e/4b0717f0ea04213a65d5a6f42ddb8386.gif" title="P (z | x)">  .  Magnitude <img src="https://habrastorage.org/getpro/habr/post_images/b5a/67a/0fa/b5a67a0fa3b56e982b1ac0dcc013b892.gif" title="H \ left (q \ left (z \ right) \ right) = - \ int_z q (z) \ log q (z)">  called entropy <img src="https://habrastorage.org/getpro/habr/post_images/623/c53/913/623c539132e71fa79a9088c940c4bf33.gif" title="q (z)">  . <img src="https://habrastorage.org/getpro/habr/post_images/66e/ae4/657/66eae465795b50b44d1af7a3f15abc0c.gif" title="H \ left (q \ left (z \ right) \ right)">  independent of parameters <img src="https://habrastorage.org/getpro/habr/post_images/d74/eda/af2/d74edaaf2305d2d981f9c13219e34f36.gif" title="\ theta">  , so we can ignore this addend when optimizing: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/710/14b/63a/71014b63aedba03d5b154a45dafacc98.gif" title="L (x | \ theta) \ propto \ int_z q (z) \ log P (x, z) + KL (q (z) || P (z | x)) ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/46c/6e0/5a4/46c6e05a4976e20fe646665343cd6f98.gif" title="= \ int_z q (z) \ log \ left (P (x | z) P (z) \ right) + KL (q (z) || P (z | x)) ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d78/071/a3e/d78071a3efe1643c98fc5d97990f5f14.gif" title="= \ int_z q (z) \ log P (x | z) + \ int_z q (z) \ log P (z) + KL (q (z) || P (z | x)) \ propto"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/141/d42/011/141d42011206d4237c7ad23456027b66.gif" title="\ propto \ int_z q (z) \ log P (x | z) + KL (q (z) || P (z | x))"></div><br>  Magnitude <img src="https://habrastorage.org/getpro/habr/post_images/dae/d3a/af1/daed3aaf1aea81782be4d3f8f38a1878.gif" title="KL (q (z) || P (z | x))">  non-negative and zero when <img src="https://habrastorage.org/getpro/habr/post_images/c45/d9d/43f/c45d9d43f8eefb054db07da4844e5bf9.gif" title="q (z) = P (z | x)">  .  In this regard, let's define the following EM algorithm: <br><br><ol><li>  E: <img src="https://habrastorage.org/getpro/habr/post_images/121/bae/fa6/121baefa6432a90d29b8b85f198b935d.gif" title="q (z): = P (z | x)">  .  This will nullify the second term. <img src="https://habrastorage.org/getpro/habr/post_images/dae/d3a/af1/daed3aaf1aea81782be4d3f8f38a1878.gif" title="KL (q (z) || P (z | x))">  . </li><li>  M: Maximize the first term <img src="https://habrastorage.org/getpro/habr/post_images/a57/4f3/590/a574f35907709b2aa9a32b55ec5c9066.gif" title="L (x | \ theta) \ propto \ int_z q (z) \ log P (x | z)">  . </li></ol><br>  PPCA is a linear model, because it can be solved analytically.  But instead, we will try to solve it with the help of a generalized EM-algorithm, when maximization is performed not up to the end, but only by one step of the gradient rise in the direction of the optimum.  Moreover, we will use a stochastic gradient lift, i.e.  his step will be a step towards optimum only on average.  Since our data is iid, <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/488/cc5/188/488cc5188714316b339d1c82b233649b.gif" title="L (x | \ theta) \ propto \ int_z q (z) \ log P (x | z) = \ int_z q (z) \ log \ prod_ {i = 1} ^ {N} P (x_i | z_i) = \ int_z q (z) \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i)"></div><br>  Note that the expression of the form <img src="https://habrastorage.org/getpro/habr/post_images/635/523/68a/63552368adfd483194dfb7984809eaec.gif" title="\ int_z q (z) f (z)">  is a mathematical expectation <img src="https://habrastorage.org/getpro/habr/post_images/0db/00d/abb/0db00dabb27cf9b363a9f519fae3914f.gif" title="E_ {z \ sim q (z)} f ‚Äã‚Äã(z)">  .  Then <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f79/ac5/586/f79ac55865207217729606a40cef2a30.gif" title="\ int_z q (z) \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i) = E_ {z \ sim q (z)} \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i) \ propto E_ {z \ sim q (z)} \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i)"></div><br>  Since a single sample is an unbiased estimate of the expectation, the following equality can be written approximately: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/201/fdb/966/201fdb9667b3fb0569ed5f95489fb185.gif" title="E_ {z \ sim q (z)} \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i) = \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i)"></div><br>  Total by substituting <img src="https://habrastorage.org/getpro/habr/post_images/de5/5b6/956/de55b69567ac6599a60e4a26e433b480.gif" title="P (x | z) = N (x; Wz + b, \ sigma ^ 2 I)">  , we get: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb1/e4c/d91/bb1e4cd911a8de5ab6adb338da78db38.gif" title="L (x | \ theta) \ propto \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log P (x_i | z_i) ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b2/d43/fc1/0b2d43fc19680963c1ca56edfd716920.gif" title="= \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log \ left (\ frac {1} {\ sqrt {\ left (2 \ pi \ right) ^ d \ left | \ sigma ^ 2 I \ right |}} \ exp \ left (- \ frac {{|| x_i - \ left (W z_i + b \ right) ||} ^ 2} {2 \ sigma ^ 2} \ right) \ right ) ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/84c/cc2/e8f/84ccc2e8fff30f3082ba362ca191be45.gif" title="= \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ log \ left (\ frac {1} {\ sqrt {\ left (2 \ pi \ sigma ^ 2 \ right) ^ d} } \ exp \ left (- \ frac {{|| x_i - \ left (W z_i + b \ right) ||} ^ 2} {2 \ sigma ^ 2} \ right) \ right) ="></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/734/952/5fb/7349525fb2ecd57b55c3e533d8c403e5.gif" title="= \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ left (- \ log \ sqrt {\ left (2 \ pi \ sigma ^ 2 \ right) ^ d} - \ frac {1 } {2 \ sigma ^ 2} {|| x_i - b - W z_i ||} ^ 2 \ right)"></div><br>  or <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d02/1cc/6d4/d021cc6d45dc02c7f18b80ba26d1ddde.gif" title="L (x | \ theta) \ propto L ^ {*} (x | \ theta) = - \ frac {1} {N} \ sum_ {i = 1} ^ {N} \ left (d \ log \ left ( \ sigma ^ 2 \ right) + \ frac {1} {\ sigma ^ 2} {|| x_i - b - W z_i ||} ^ 2 \ right)"></div><br>  Formula 1. Loss function proportional to the likelihood of data in the PPCA model. <br><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/dab/8f2/01b/dab8f201ba10fb5a14e991ab157a9c7c.gif" title="d">  - dimension of the observed variable <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" title="x">  .  Now let's write out the GEM-algorithm for PPCA. <img src="https://habrastorage.org/getpro/habr/post_images/de5/5b6/956/de55b69567ac6599a60e4a26e433b480.gif" title="P (x | z) = N (x; Wz + b, \ sigma ^ 2 I)">  , but <img src="https://habrastorage.org/getpro/habr/post_images/775/448/15e/77544815ecc8152dfb82a7332bfbaf86.gif" title="P (z | x) = N \ left (z; \ left (W ^ TW + \ sigma ^ 2 I \ right) ^ {- 1} W ^ T \ left (x - b \ right), \ sigma ^ 2 \ left (W ^ TW + \ sigma ^ 2 I \ right) ^ {- 1} \ right)">  .  Then the GEM algorithm will look like this: <br><br><ol><li>  Initialize the parameters <img src="https://habrastorage.org/getpro/habr/post_images/a09/bfc/d75/a09bfcd75045bc4ff04418f04cb289c2.gif" title="W, b, \ sigma">  reasonable random initial approximations. </li><li>  Sampling <img src="https://habrastorage.org/getpro/habr/post_images/a00/af1/0bc/a00af10bc510d574f9ddfb2a6298ae4c.gif" title="{x_i} \ sim P (x)">  - selection of the mini-batches from the data. </li><li>  Calculate the expected values ‚Äã‚Äãof latent variables. <img src="https://habrastorage.org/getpro/habr/post_images/e57/1c5/e73/e571c5e73152d7a3b37f5e0162c981a7.gif" title="z_i \ sim P (z | x_i)">  or <img src="https://habrastorage.org/getpro/habr/post_images/eae/c23/e96/eaec23e96173a89c36c897fbfde1461f.gif" title="z_i = \ left (W ^ TW + \ sigma ^ 2 I \ right) ^ {- 1} W ^ T \ left (x_i - b \ right) + \ varepsilon, \ varepsilon \ sim N (0, \ sigma ^ 2 \ left (W ^ TW + \ sigma ^ 2 I \ right) ^ {- 1})">  . </li><li>  Substitute <img src="https://habrastorage.org/getpro/habr/post_images/7de/633/f3f/7de633f3f2eff57c39c70e342761c825.gif" title="x_i, z_i">  in formula (1) for <img src="https://habrastorage.org/getpro/habr/post_images/7e0/754/933/7e075493316fca3e9542548c38116492.gif" title="L ^ {*} (x | \ theta)">  and take a step gradient rise in the parameters.  It is important to remember that <img src="https://habrastorage.org/getpro/habr/post_images/b6c/074/598/b6c074598e5cedd50d95fe57d5bce90b.gif" title="z_i">  must be taken as inputs and not allowed to propagate the error back inside it. </li><li>  If the likelihood of the data and the expected values ‚Äã‚Äãof the latent variables for the control visible variables do not change much, stop learning.  Otherwise, go to step (2). </li></ol><br>  After the model is trained, samples can be generated from it: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc1/810/83b/dc181083be65d41ddad5e18d0897fed5.gif" title="P (x) = N (x; b, W W ^ T + \ sigma ^ 2 I)"></div><br><br><h2>  Numerical solution of the PCA problem </h2><br>  Let's train the PPCA model using standard SGD.  We will again study the work of the model on a toy example in order to understand all the details.  The full model code can be found <a href="https://github.com/Monnoroch/generative/tree/master/ppca">here</a> , and in this article only key points will be covered. <br><br>  Set <img src="https://habrastorage.org/getpro/habr/post_images/f93/988/7b4/f939887b4f4ea46f31f77bbb830f4e3f.gif" title="P (x) = N (x; \ left (\ begin {matrix} 5 \\ 10 \ end {matrix} \ right) \ left (\ begin {matrix} 1.2 ^ 2 &amp; 0 \\ 0 &amp; 2.4 ^ 2 \ end {matrix} \ right))">  - two-dimensional normal distribution with a diagonal covariance matrix. <img src="https://habrastorage.org/getpro/habr/post_images/8e9/814/2d6/8e98142d609bd2e16e35679151ea55db.gif" title="P (z) = N (z; 0, 1)">  - one-dimensional normal latent view. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kb/aa/sc/kbaascwgzdhtpctkeiamtvrisis.png"></div><br>  Fig.  1. Ellipse around the middle, which falls 95% of the points from the distribution <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  . <br><br>  So the first thing to do is generate the data.  We generate samples from <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  : <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">normal_samples</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(batch_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">example</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.contrib.distributions.MultivariateNormalDiag( [<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>], [<span class="hljs-number"><span class="hljs-number">1.2</span></span>, <span class="hljs-number"><span class="hljs-number">2.4</span></span>]).sample(sample_shape=[<span class="hljs-number"><span class="hljs-number">1</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> tf.contrib.data.Dataset.from_tensors([<span class="hljs-number"><span class="hljs-number">0.</span></span>]) .repeat() .map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: example()) .batch(batch_size)</code> </pre> <br>  Now you need to determine the parameters of the model: <br><br><pre> <code class="python hljs">input_size = <span class="hljs-number"><span class="hljs-number">2</span></span> latent_space_size = <span class="hljs-number"><span class="hljs-number">1</span></span> stddev = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"stddev"</span></span>, initializer=tf.constant(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, shape=[<span class="hljs-number"><span class="hljs-number">1</span></span>])) biases = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"biases"</span></span>, initializer=tf.constant(<span class="hljs-number"><span class="hljs-number">0.1</span></span>, shape=[input_size])) weights = tf.get_variable( <span class="hljs-string"><span class="hljs-string">"Weights"</span></span>, initializer=tf.truncated_normal( [input_size, latent_space_size], stddev=<span class="hljs-number"><span class="hljs-number">0.1</span></span>))</code> </pre><br>  After that you can get their latent views for the sample of visible variables: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_latent</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(visible, latent_space_size, batch_size)</span></span></span><span class="hljs-function">:</span></span> matrix = tf.matrix_inverse( tf.matmul(weights, weights, transpose_a=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) + stddev**<span class="hljs-number"><span class="hljs-number">2</span></span> * tf.eye(latent_space_size)) mean_matrix = tf.matmul(matrix, weights, transpose_b=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Multiply each vector in a batch by a matrix. expected_latent = batch_matmul( mean_matrix, visible - biases, batch_size) stddev_matrix = stddev**2 * matrix noise = tf.contrib.distributions.MultivariateNormalFullCovariance( tf.zeros(latent_space_size), stddev_matrix) .sample(sample_shape=[batch_size]) return tf.stop_gradient(expected_latent + noise)</span></span></code> </pre><br>  Here you need to pay attention to tf.stop_gradient (...).  This function does not allow the parameter values ‚Äã‚Äãwithin the input subgraph to affect the gradient by these parameters.  It is necessary that <img src="https://habrastorage.org/getpro/habr/post_images/121/bae/fa6/121baefa6432a90d29b8b85f198b935d.gif" title="q (z): = P (z | x)">  remained fixed during the M-step, which is necessary for the correct operation of the EM algorithm. <br><br>  Let's write down the loss function now. <img src="https://habrastorage.org/getpro/habr/post_images/7e0/754/933/7e075493316fca3e9542548c38116492.gif" title="L ^ {*} (x | \ theta)">  for optimization at M-step: <br><br><pre> <code class="python hljs">sample = dataset.get_next() latent_sample = get_latent(sample, latent_space_size, batch_size) norm_squared = tf.reduce_sum((sample - biases - batch_matmul(weights, latent_sample, batch_size))**<span class="hljs-number"><span class="hljs-number">2</span></span>, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) loss = tf.reduce_mean( input_size * tf.log(stddev**<span class="hljs-number"><span class="hljs-number">2</span></span>) + <span class="hljs-number"><span class="hljs-number">1</span></span>/stddev**<span class="hljs-number"><span class="hljs-number">2</span></span> * norm_squared) train = tf.train.AdamOptimizer(learning_rate) .minimize(loss, var_list=[bias, weights, stddev], name=<span class="hljs-string"><span class="hljs-string">"train"</span></span>)</code> </pre><br><br>  So, the model is ready for training.  Let's take a look at the learning curve of the model: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vb/lc/gg/vblcggfdlrodyjqrsbf68elbmxy.png"></div><br>  Fig.  2. The PPCA learning curve. <br><br>  It can be seen that the model converges fairly regularly and quickly, which well reflects the simplicity of the model.  Let's look at the learned distribution parameters. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lw/u6/yd/lwu6yde05yixhwky3zsvjuluyz0.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/x2/y2/6ax2y24xzzhbip5g8cbtmvcglog.png"></div><br>  Fig.  3. Displacement plots from the origin (parameter <img src="https://habrastorage.org/getpro/habr/post_images/a07/675/eb4/a07675eb420c5a094218ccaf1bb3763f.gif" title="b">  ). <br><br>  It's clear that <img src="https://habrastorage.org/getpro/habr/post_images/e0d/dd6/41d/e0ddd641d93ee0cb5bd7d1bb86f1a672.gif" title="b_i">  quickly converged to analytical values <img src="https://habrastorage.org/getpro/habr/post_images/134/b46/c4e/134b46c4e9511d2584e1aac895f801e9.gif" title="five">  and <img src="https://habrastorage.org/getpro/habr/post_images/37e/eaa/e1f/37eeaae1f44c27ea16c331c7cf54ee4a.gif" title="ten">  .  Let's now look at the parameters. <img src="https://habrastorage.org/getpro/habr/post_images/e41/9ae/265/e419ae26568307428c20e5949e3c9341.gif" title="W, \ sigma">  : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/xu/dt/hn/xudthn7f8i6uaevdaxkddlebfes.png"></div><br>  Fig.  4. Graph of parameter change <img src="https://habrastorage.org/getpro/habr/post_images/5a4/4d0/8a2/5a44d08a2c46ced5dd1a8786e2d30d12.gif" title="\ sigma">  . <br><br>  See that the value <img src="https://habrastorage.org/getpro/habr/post_images/5a4/4d0/8a2/5a44d08a2c46ced5dd1a8786e2d30d12.gif" title="\ sigma">  converged to 1.2, i.e.  to the smaller axis of deviation of the input distribution, as expected. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nr/6r/jq/nr6rjqsz6cxka0zvzgcvb8xfu6u.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yg/v_/4d/ygv_4dohosewrh5kvhp2mdfvfre.png"></div><br>  Fig.  5. Graphs of parameters change <img src="https://habrastorage.org/getpro/habr/post_images/c7d/708/483/c7d708483c844dcac146d4ccb8b1455f.gif" title="W_ {i0}">  . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/982/ba7/03f/982ba703f70cd3d04aa174ca0d31e9ac.gif" title="W">  , in turn, came to about the value at which <img src="https://habrastorage.org/getpro/habr/post_images/c4d/2c7/66c/c4d2c766c934af8c03c8e9cd6afdca59.gif">  .  Substituting these values ‚Äã‚Äãinto the model, we get <img src="https://habrastorage.org/getpro/habr/post_images/bc3/2cc/c5b/bc32ccc5bebfb35a0f8f146c32560095.gif">  that means we restored the distribution of the data. <br><br>  Let's look at the distribution of data.  The latent variable is one-dimensional, because it is displayed as a one-dimensional distribution.  The visible variable is two-dimensional, but its given covariance matrix is ‚Äã‚Äãdiagonal, which means that its ellipsoid is aligned with the axes of coordinates.  Therefore, we will display it as two projections of its distribution on the coordinate axes.  This is the set and learned distribution. <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  in projection on the first axis of coordinates: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mc/ky/w8/mckyw8nolc-a7gr3e5ye1ouurdi.png"></div><br>  Fig.  6. The projection of a given distribution <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  on the first axis of coordinates. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/f7/rv/nq/f7rvnqsuavrezx3hat5klm3maoi.png"></div><br>  Fig.  7. Projection of the learned distribution <img src="https://habrastorage.org/getpro/habr/post_images/ea2/1e5/771/ea21e57713a2421d18493c52b4cc5945.gif" title="P (x | \ theta)">  on the first axis of coordinates. <br><br>  And this is the set and learned distribution. <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  projected on the second axis of coordinates: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cw/q7/1k/cwq71k1uhhvjdzm33iyf1hpt1vg.png"></div><br>  Fig.  8. The projection of a given distribution <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  on the second axis of coordinates. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/is/wa/pw/iswapwivuoovhust5p0ccqtzfnq.png"></div><br>  Fig.  9. Projection of the learned distribution <img src="https://habrastorage.org/getpro/habr/post_images/ea2/1e5/771/ea21e57713a2421d18493c52b4cc5945.gif" title="P (x | \ theta)">  on the second axis of coordinates. <br><br>  And so look analytic and learned distribution <img src="https://habrastorage.org/getpro/habr/post_images/ca3/557/073/ca35570735f832b9f0f49ac5ea35fbec.gif" title="P (z)">  : <br><img src="https://habrastorage.org/webt/lb/tg/ti/lbtgtiowdedumc_25cff0acshyk.png"><br>  Fig.  10. Specified distribution <img src="https://habrastorage.org/getpro/habr/post_images/8e9/814/2d6/8e98142d609bd2e16e35679151ea55db.gif" title="P (z) = N (z; 0, 1)">  . <br><br><img src="https://habrastorage.org/webt/8l/3h/k3/8l3hk3mj2-tldqe4j4pex3yinyw.png"><br>  Fig.  11. Learned distribution <img src="https://habrastorage.org/getpro/habr/post_images/829/65b/7e8/82965b7e8f7bfad6e3aa6ae3945271f8.gif" title="P (z | \ theta)">  . <br><br>  It can be seen that all the distributions learned converge to distributions very similar to those given by the task.  Let's look at the dynamics of the training model, to be sure of this finally: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/05/sx/9a/05sx9a4nprmgcf26aulbzfi9qh4.gif"></div><br>  Fig.  12. The training process of the PPCA model, in which the learned distribution <img src="https://habrastorage.org/getpro/habr/post_images/829/65b/7e8/82965b7e8f7bfad6e3aa6ae3945271f8.gif" title="P (z | \ theta)">  converges to data distribution <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  . <br><br><h2>  Conclusion </h2><br>  The presented model is a probabilistic interpretation of the classical PCA model, which is a linear model.  We used the math from the <a href="http://www.robots.ox.ac.uk/~cvrg/hilary2006/ppca.pdf">original article</a> , built the GEM algorithm on top of it and showed that the resulting model converges to an analytical solution using a simple example.  Of course, if in the problem <img src="https://habrastorage.org/getpro/habr/post_images/3ac/7d4/8a7/3ac7d48a793626d97a397da494839421.gif" title="P (x)">  it was not normal, the model would not have done so well.  In the same way as PCA does not cope perfectly with data that does not lie in a certain hyperplane.  To solve more complex problems of approximating distributions, we need more complex and nonlinear models.  One of these models, Generative Adversarial Networks, will be discussed in the next article. <br><br><h2>  Thanks </h2><br>  Thank you <a href="https://www.linkedin.com/in/olga-talanova-b319b761/">Olga Talanova</a> for <a href="https://www.linkedin.com/in/olga-talanova-b319b761/">reviewing</a> the text.  Thanks to <a href="https://github.com/andrewtar">Andrei Tarashkevich</a> for help with the layout of this article. </div><p>Source: <a href="https://habr.com/ru/post/347184/">https://habr.com/ru/post/347184/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347168/index.html">Routing in socks. Another way</a></li>
<li><a href="../347170/index.html">Making games in Python 3 and Pygame: Part 2</a></li>
<li><a href="../347174/index.html">Not a webpack</a></li>
<li><a href="../347180/index.html">Convert from Sketch to PSD</a></li>
<li><a href="../347182/index.html">Digital events in Moscow from January 22 to 28</a></li>
<li><a href="../347186/index.html">Introduction to ConcourseCI</a></li>
<li><a href="../347188/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ298 (January 15 - 21, 2018)</a></li>
<li><a href="../347190/index.html">Pros of the "correct" virtual number</a></li>
<li><a href="../347192/index.html">News from the world of OpenStreetMap ‚Ññ391 (09.01.2018-15.01.2018)</a></li>
<li><a href="../347194/index.html">CSS naming conventions and time saving</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>