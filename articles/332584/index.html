<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Making an image recognition service using TensorFlow Serving</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There always comes a time when the trained model needs to be released in production. To do this, you often have to write bicycles in the form of machi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Making an image recognition service using TensorFlow Serving</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/881/492/468/88149246877e4b3490af383a11988be9.jpg" alt="image"><br><br>  There always comes a time when the trained model needs to be released in production.  To do this, you often have to write bicycles in the form of machine learning library wrappers.  But if your model is implemented on Tensorflow, then I have good news for you - you won't have to write a bicycle, because  you can use Tensorflow Serving. </p><br><p>  In this article we will look at how to use Tensorflow Serving to quickly create a productive service for image recognition. </p><a name="habracut"></a><br><p>  <strong>Tensorflow Serving</strong> - a system for deploying Tensorflow-models with features such as: </p><br><ul><li>  automatic batching; </li><li>  hot swapping of models and versioning; </li><li>  the ability to process parallel requests. </li></ul><br><p>  An additional advantage is the ability to overtake the model from Keras to the Tensorflow model and secure it through Serving (if of course the Keens uses the Tensorflow backend). </p><br><h2 id="kak-rabotaet-tensorflow-serving">  How Tensorflow Serving Works </h2><br><p><img src="https://habrastorage.org/web/e83/4ff/81b/e834ff81becb444681ed4a7a321d93af.png"></p><br><p>  The main part of Tensorflow Serving is the Model Server. </p><br><p>  Consider the operation of the server models.  After launch, the model server loads the model from the path specified at startup and starts listening to the specified port.  The server communicates with clients via remote procedure calls using the gRPC library.  This allows you to create a client application in any language that supports gRPC. </p><br><p>  If the model server receives a request, it can perform the following actions: </p><br><ul><li>  Run model execution for this query. </li><li> Combine several requests into a batch and perform a calculation for the entire batch if the corresponding option (the <code>--enable_batching</code> flag) is activated at startup.  Processing by batchy is more efficient (especially on GPU), so this feature allows you to increase the number of requests processed per unit of time. </li><li>  Put the request in the queue, if at the moment the computing resources are busy. </li></ul><br><p>  As previously mentioned, Tensorflow Serving supports hot swapping of models.  The model server constantly scans the path specified at launch for new models and when a new version is found, it automatically downloads this version.  This allows you to upload new versions of models without having to stop the server models. </p><br><p>  Thus, Tensorflow Serving has enough functionality to complete production in production.  Therefore, the use of such approaches as the creation of your own wrapper over the model seems unjustified, since  Tensorflow Serving offers the same features and even more without the need to write and maintain self-written solutions. </p><br><h2 id="ustanovka">  Installation </h2><br><p>  Building Tensorflow Serving is probably the hardest part of using this tool.  In principle, there is nothing difficult, but there are several underwater rakes.  I will tell about them in this section. </p><br><p>  For assembly the <a href="https://bazel.build/">bazel</a> assembly system is <a href="https://bazel.build/">used</a> . </p><br><p>  The installation of Tensorflow Serving is described on the official website. <a href="https://tensorflow.github.io/serving/setup"></a>  <a href="https://tensorflow.github.io/serving/setup">https://tensorflow.github.io/serving/setup</a> .  I will not describe in detail each step, but will talk about the problems that may arise during the installation. </p><br><p>  With all the steps to configuring Tensorflow ( <code>./configure</code> ) there should be no problems. </p><br><p>  When configuring Tensorflow for almost all parameters, you can leave default values.  But if you choose to install with CUDA, the configurator will ask for the version of cuDNN.  It is necessary to introduce the full version of cuDNN (in my case 5.1.5). </p><br><p>  We reach the assembly ( <code>bazel build tensorflow_serving/...</code> ). </p><br><p>  First you need to determine what optimizations are available to your processor and specify them during assembly, since  bazel cannot recognize them automatically. <br>  Thus, the build command is complicated to the following: </p><br><p> <code>bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 tensorflow_serving/...</code> </p> <br><p>  <em>Check that all these optimizations are available to your processor.</em>  My processor does not support AVX2 and FMA so I compiled with the following command: </p><br><p> <code>bazel build -c opt --copt=-mavx --copt=-mfpmath=both --copt=-msse4.2 tensorflow_serving/...</code> </p> <br><p>  By default, the Tensorflow build consumes a lot of memory, so if you don‚Äôt have much of it, then you need to limit the resource consumption.  You can do this with the following flag <code>--local_resources availableRAM,availableCPU,availableIO</code> (RAM in MB, CPU in cores, available I / O (1.0 being average workstation), for example, <code>--local_resources 2048,.5,1.0</code> ). </p><br><p>  If you want to build Tensorflow Serving with GPU support, then you need to add the flag <code>--config=cuda</code> .  Get about this team. </p><br><p> <code>bazel build -c opt --copt=-mavx --copt=-mfpmath=both --copt=-msse4.2 --config=cuda tensorflow_serving/...</code> </p> <br><p>  The following error may occur during assembly. </p><br><div class="spoiler">  <b class="spoiler_title">Error text</b> <div class="spoiler_text"><p> <code>ERROR: no such target '@org_tensorflow//third_party/gpus/crosstool:crosstool': target 'crosstool' not declared in package 'third_party/gpus/crosstool' defined by /home/movchan/.cache/bazel/_bazel_movchan/835a50f8a234772a7d7dac38871b88e9/external/org_tensorflow/third_party/gpus/crosstool/BUILD.</code> </p> </div></div><br><p>  To correct this error, in the <code>tools/bazel.rc</code> replace <code>@org_tensorflow//third_party/gpus/crosstool</code> with <code>@local_config_cuda//crosstool:toolchain</code> </p><br><p>  Another error may appear. </p><br><div class="spoiler">  <b class="spoiler_title">Error text</b> <div class="spoiler_text"><p> <code>ERROR: /home/movchan/.cache/bazel/_bazel_movchan/835a50f8a234772a7d7dac38871b88e9/external/org_tensorflow/tensorflow/contrib/nccl/BUILD:23:1: C++ compilation of rule '@org_tensorflow//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 80 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1. In file included from external/org_tensorflow/tensorflow/conERROR: /home/movchan/.cache/bazel/_bazel_movchan/835a50f8a234772a7d7dac38871b88e9/external/org_tensorflow/tensorflow/contrib/nccl/BUILD:23:1: C++ compilation of rule '@org_tensorflow//tensorflow/contrib/nccl:python/ops/_nccl_ops.so' failed: crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter ... (remaining 80 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1. In file included from external/org_tensorflow/tensorflow/contrib/nccl/kernels/nccl_manager.cc:15:0: external/org_tensorflow/tensorflow/contrib/nccl/kernels/nccl_manager.h:23:44: fatal error: external/nccl_archive/src/nccl.h: No such file or directory compilation terminated.</code> </p> </div></div><br><p>  To fix it, remove the prefix <code>/external/nccl_archive</code> in the line <code>#include "external/nccl_archive/src/nccl.h"</code> in the following files: <br> <code>tensorflow/tensorflow/contrib/nccl/kernels/nccl_ops.cc tensorflow/tensorflow/contrib/nccl/kernels/nccl_manager.h</code> </p> <br><p>  Hooray!  Collected finally! </p><br><h2 id="eksport-modeli">  Export Model </h2><br><p>  Exporting a model from Tensorflow is described in detail in <a href="https://tensorflow.github.io/serving/serving_basic"></a>  <a href="https://tensorflow.github.io/serving/serving_basic">https://tensorflow.github.io/serving/serving_basic</a> in the section "Train And Export TensorFlow Model". </p><br><p>  For export, the class <code>SavedModelBuilder</code> .  I use Keras to train Tensorflow-models, t.ch.  I will describe the process of exporting a model from Keras to Serving using this module. </p><br><p>  Export code ResNet-50, trained on ImageNet. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input, decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.contrib.session_bundle <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> exporter <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> keras.backend <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> K <span class="hljs-comment"><span class="hljs-comment">#    test time. K.set_learning_phase(0) #      model = ResNet50(weights='imagenet') sess = K.get_session() #        export_path_base = './model' export_version = 1 export_path = os.path.join( tf.compat.as_bytes(export_path_base), tf.compat.as_bytes(str(export_version))) print('Exporting trained model to', export_path) builder = tf.saved_model.builder.SavedModelBuilder(export_path) #       model_input = tf.saved_model.utils.build_tensor_info(model.input) model_output = tf.saved_model.utils.build_tensor_info(model.output) #    ,        prediction_signature = ( tf.saved_model.signature_def_utils.build_signature_def( inputs={'images': model_input}, outputs={'scores': model_output}, method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)) #    SavedModelBuilder legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op') builder.add_meta_graph_and_variables( sess, [tf.saved_model.tag_constants.SERVING], signature_def_map={ 'predict': prediction_signature, }, legacy_init_op=legacy_init_op) builder.save()</span></span></code> </pre> <br><p>  Instead of <code>'images'</code> and <code>'scores'</code> , you can specify any names when setting inputs and outputs.  These names will be used further. </p><br><p>  If the model has several inputs and / or outputs, then you need to specify this in <code>tf.saved_model.signature_def_utils.build_signature_def</code> .  For this you need to use <code>model.inputs</code> and <code>model.outputs</code> .  Then the installation code for inputs and outputs will look like this: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#       model_input = tf.saved_model.utils.build_tensor_info(model.inputs[0]) model_output = tf.saved_model.utils.build_tensor_info(model.outputs[0]) model_aux_input = tf.saved_model.utils.build_tensor_info(model.inputs[1]) model_aux_output = tf.saved_model.utils.build_tensor_info(model.outputs[1]) #     prediction_signature = ( tf.saved_model.signature_def_utils.build_signature_def( inputs={'images': model_input, 'aux_input': model_aux_input}, outputs={'scores': model_output, 'aux_output': model_aux_output}, method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))</span></span></code> </pre> <br><p>  It is also worth noting that <code>signature_def_map</code> indicates all available methods (signatures), of which there can be more than 1. In the example above, only one method has been added - <code>predict</code> .  The name of the method will be used later. </p><br><h2 id="zapusk-servera-modeley">  Running the model server </h2><br><p>  Starting the model server is performed with the following command: </p><br><p> <code>./bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --enable_batching --port=9001 --model_name=resnet50 --model_base_path=/home/movchan/ml/serving_post/model</code> </p> <br><p>  Consider what the flags mean in this command. </p><br><ul><li>  <code>enable_batching</code> - the flag of activating automatic batching, allows Tensorflow Serving to combine requests into batch files for more efficient processing. </li><li>  <code>port</code> - the port that the model will listen on. </li><li>  <code>model_name</code> is the name of the model (will be used later). </li><li>  <code>model_base_path</code> - the path to the model (where you saved it in the previous step). </li></ul><br><h2 id="ispolzovanie-tensorflow-serving-iz-python">  Using Tensorflow Serving from python </h2><br><p>  To begin with, we will install the grpcio package via pip. </p><br><p> <code>sudo pip3 install grpcio</code> </p> <br><p>  In general, according to the tutorial on the official website, it is proposed to collect python-scripts via bazel.  But I do not like this idea, t.ch.  I found another way. </p><br><p>  To use the python API, you can copy (softlink) the <code>bazel-bin/tensorflow_serving/example/inception_client.runfiles/tf_serving/tensorflow_serving</code> .  It contains everything you need to work python API.  I usually just copy to the directory where the script is located that uses this API. </p><br><p>  Consider an example of using the python API. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> grpc.beta <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> implementations <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> predict_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> prediction_service_pb2 <span class="hljs-comment"><span class="hljs-comment">#        Serving host = '127.0.0.1' port = 9001 channel = implementations.insecure_channel(host, port) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) #   request = predict_pb2.PredictRequest() #   ,       ( model_name) request.model_spec.name = 'resnet50' #   ,       (. signature_def_map). request.model_spec.signature_name = 'predict' #   .        . request.inputs['images'].CopyFrom( tf.contrib.util.make_tensor_proto(image, shape=image.shape)) #  .   - timeout. result = stub.Predict(request, 10.0) #  .        . prediction = np.array(result.outputs['scores'].float_val)</span></span></code> </pre> <br><div class="spoiler">  <b class="spoiler_title">Full code of the python API usage example</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> grpc.beta <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> implementations <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> predict_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> prediction_service_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input, decode_predictions <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img_path)</span></span></span><span class="hljs-function">:</span></span> img = image.load_img(img_path, target_size=(<span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>)) x = image.img_to_array(img) x = np.expand_dims(x, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) x = preprocess_input(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(host, port, img_path)</span></span></span><span class="hljs-function">:</span></span> image = preprocess_image(img_path) start_time = time.time() channel = implementations.insecure_channel(host, port) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) request = predict_pb2.PredictRequest() request.model_spec.name = <span class="hljs-string"><span class="hljs-string">'resnet50'</span></span> request.model_spec.signature_name = <span class="hljs-string"><span class="hljs-string">'predict'</span></span> request.inputs[<span class="hljs-string"><span class="hljs-string">'images'</span></span>].CopyFrom( tf.contrib.util.make_tensor_proto(image, shape=image.shape)) result = stub.Predict(request, <span class="hljs-number"><span class="hljs-number">10.0</span></span>) prediction = np.array(result.outputs[<span class="hljs-string"><span class="hljs-string">'scores'</span></span>].float_val) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> prediction, (time.time()-start_time)*<span class="hljs-number"><span class="hljs-number">1000.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">"__main__"</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(sys.argv) != <span class="hljs-number"><span class="hljs-number">4</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> (<span class="hljs-string"><span class="hljs-string">'usage: serving_test.py &lt;host&gt; &lt;port&gt; &lt;img_path&gt;'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> (<span class="hljs-string"><span class="hljs-string">'example: serving_test.py 127.0.0.1 9001 ~/elephant.jpg'</span></span>) exit() host = sys.argv[<span class="hljs-number"><span class="hljs-number">1</span></span>] port = int(sys.argv[<span class="hljs-number"><span class="hljs-number">2</span></span>]) img_path = sys.argv[<span class="hljs-number"><span class="hljs-number">3</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): prediction, elapsed_time = get_prediction(host, port, img_path) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">'Predicted:'</span></span>, decode_predictions(np.atleast_2d(prediction), top=<span class="hljs-number"><span class="hljs-number">3</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(<span class="hljs-string"><span class="hljs-string">'Elapsed time:'</span></span>, elapsed_time, <span class="hljs-string"><span class="hljs-string">'ms'</span></span>)</code> </pre> </div></div><br><p>  Compare the speed of Tensorflow Serving with Keras-version. </p><br><div class="spoiler">  <b class="spoiler_title">Keras code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ResNet50 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input, decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img_path)</span></span></span><span class="hljs-function">:</span></span> img = image.load_img(img_path, target_size=(<span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>)) x = image.img_to_array(img) x = np.expand_dims(x, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) x = preprocess_input(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model, img_path)</span></span></span><span class="hljs-function">:</span></span> image = preprocess_image(img_path) start_time = time.time() prediction = model.predict(image) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> prediction, (time.time()-start_time)*<span class="hljs-number"><span class="hljs-number">1000.</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">"__main__"</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(sys.argv) != <span class="hljs-number"><span class="hljs-number">2</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> (<span class="hljs-string"><span class="hljs-string">'usage: keras_test.py &lt;img_path&gt;'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> (<span class="hljs-string"><span class="hljs-string">'example: keras_test.py ~/elephant.jpg'</span></span>) exit() img_path = sys.argv[<span class="hljs-number"><span class="hljs-number">1</span></span>] model = ResNet50(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">10</span></span>): prediction, elapsed_time = get_prediction(model, img_path) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> i == <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">'Predicted:'</span></span>, decode_predictions(np.atleast_2d(prediction), top=<span class="hljs-number"><span class="hljs-number">3</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>]) print(<span class="hljs-string"><span class="hljs-string">'Elapsed time:'</span></span>, elapsed_time, <span class="hljs-string"><span class="hljs-string">'ms'</span></span>)</code> </pre> </div></div><br><p>  All measurements were made on the CPU. </p><br><p>  For testing, take this photo of a cat from <a href="https://www.pexels.com/photo/animal-asian-cat-vintage-225932/">Pexels.com</a> , which I found through <a href="https://everypixel.com/"></a>  <a href="https://everypixel.com/">https://everypixel.com</a> . </p><br><p><img src="https://habrastorage.org/web/398/483/bb2/398483bb2a11455c971e5ccb227fcbac.jpeg"></p><br><p>  Keras </p><br><pre> <code class="hljs pgsql">Predicted: [(<span class="hljs-string"><span class="hljs-string">'n02127052'</span></span>, <span class="hljs-string"><span class="hljs-string">'lynx'</span></span>, <span class="hljs-number"><span class="hljs-number">0.59509182</span></span>), (<span class="hljs-string"><span class="hljs-string">'n02128385'</span></span>, <span class="hljs-string"><span class="hljs-string">'leopard'</span></span>, <span class="hljs-number"><span class="hljs-number">0.050437182</span></span>), (<span class="hljs-string"><span class="hljs-string">'n02123159'</span></span>, <span class="hljs-string"><span class="hljs-string">'tiger_cat'</span></span>, <span class="hljs-number"><span class="hljs-number">0.049577814</span></span>)] Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">419.47126388549805</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">125.33354759216309</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">122.70569801330566</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">122.8172779083252</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">122.3604679107666</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">116.24360084533691</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">116.51420593261719</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">113.5416030883789</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">112.34736442565918</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">110.09907722473145</span></span> ms</code> </pre> <br><p>  Serving </p><br><pre> <code class="hljs pgsql">Predicted: [(<span class="hljs-string"><span class="hljs-string">'n02127052'</span></span>, <span class="hljs-string"><span class="hljs-string">'lynx'</span></span>, <span class="hljs-number"><span class="hljs-number">0.59509176015853882</span></span>), (<span class="hljs-string"><span class="hljs-string">'n02128385'</span></span>, <span class="hljs-string"><span class="hljs-string">'leopard'</span></span>, <span class="hljs-number"><span class="hljs-number">0.050437178462743759</span></span>), (<span class="hljs-string"><span class="hljs-string">'n02123159'</span></span>, <span class="hljs-string"><span class="hljs-string">'tiger_cat'</span></span>, <span class="hljs-number"><span class="hljs-number">0.049577809870243073</span></span>)] Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">117.71702766418457</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">75.67715644836426</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">72.94225692749023</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">71.62714004516602</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">71.4271068572998</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">74.54872131347656</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">70.8014965057373</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">70.94025611877441</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">70.58024406433105</span></span> ms Elapsed <span class="hljs-type"><span class="hljs-type">time</span></span>: <span class="hljs-number"><span class="hljs-number">68.82333755493164</span></span> ms</code> </pre> <br><p>  As you can see, Serving is even faster than the Keras version.  This will be even more noticeable with a large number of requests. </p><br><h2 id="realizaciya-rest-api-k-tensorflow-serving-cherez-flask">  Implementing REST API to Tensorflow Serving via Flask </h2><br><p>  First install Flask. </p><br><p> <code>sudo pip3 install flask</code> </p> <br><div class="spoiler">  <b class="spoiler_title">Full REST service code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Flask <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> request <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> flask <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> jsonify <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> grpc.beta <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> implementations <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> predict_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow_serving.apis <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> prediction_service_pb2 <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> image <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.applications.resnet50 <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> preprocess_input, decode_predictions <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np application = Flask(__name__) host = <span class="hljs-string"><span class="hljs-string">'127.0.0.1'</span></span> port = <span class="hljs-number"><span class="hljs-number">9001</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">preprocess_image</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> img = image.load_img(img, target_size=(<span class="hljs-number"><span class="hljs-number">224</span></span>, <span class="hljs-number"><span class="hljs-number">224</span></span>)) x = image.img_to_array(img) x = np.expand_dims(x, axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) x = preprocess_input(x) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_prediction</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(img)</span></span></span><span class="hljs-function">:</span></span> image = preprocess_image(img) channel = implementations.insecure_channel(host, port) stub = prediction_service_pb2.beta_create_PredictionService_stub(channel) request = predict_pb2.PredictRequest() request.model_spec.name = <span class="hljs-string"><span class="hljs-string">'resnet50'</span></span> request.model_spec.signature_name = <span class="hljs-string"><span class="hljs-string">'predict'</span></span> request.inputs[<span class="hljs-string"><span class="hljs-string">'images'</span></span>].CopyFrom( tf.contrib.util.make_tensor_proto(image, shape=image.shape)) result = stub.Predict(request, <span class="hljs-number"><span class="hljs-number">10.0</span></span>) prediction = np.array(result.outputs[<span class="hljs-string"><span class="hljs-string">'scores'</span></span>].float_val) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> decode_predictions(np.atleast_2d(prediction), top=<span class="hljs-number"><span class="hljs-number">3</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>] @application.route(<span class="hljs-string"><span class="hljs-string">'/predict'</span></span>, methods=[<span class="hljs-string"><span class="hljs-string">'POST'</span></span>]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> request.files.get(<span class="hljs-string"><span class="hljs-string">'data'</span></span>): img = request.files[<span class="hljs-string"><span class="hljs-string">'data'</span></span>] resp = get_prediction(img) response = jsonify(resp) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> response <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> jsonify({<span class="hljs-string"><span class="hljs-string">'status'</span></span>: <span class="hljs-string"><span class="hljs-string">'error'</span></span>}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">"__main__"</span></span>: application.run()</code> </pre> </div></div><br><p>  Run the service. </p><br><p> <code>python3 serving_service.py</code> </p> <br><p>  Test the service.  Send a request through curl. </p><br><p> <code>curl '127.0.0.1:5000/predict' -X POST -F "data=@./cat.jpeg"</code> </p> <br><p>  We get the following answer. </p><br><p> <code>[ [ "n02127052", "lynx", 0.5950918197631836 ], [ "n02128385", "leopard", 0.05043718218803406 ], [ "n02123159", "tiger_cat", 0.04957781359553337 ] ]</code> </p> <br><p>  Wonderful!  It is working! </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  In this article, we looked at how Tensorflow Serving can be used to deploy models in production.  We also looked at how you can implement a simple REST service on Flask that accesses the model server. </p><br><h2 id="ssylki">  Links </h2><br><p>  <a href="https://tensorflow.github.io/serving/">Tensorflow Serving official website</a> <br>  <a href="https://github.com/movchan74/tensorflow_serving_examples">Code of all article scripts</a> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/332584/">https://habr.com/ru/post/332584/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332572/index.html">Is it time to use CSS Grid Layout?</a></li>
<li><a href="../332574/index.html">AliveScript - a programming language for children 12+</a></li>
<li><a href="../332578/index.html">Vs interface</a></li>
<li><a href="../332580/index.html">Protecting your site with ZIP bombs</a></li>
<li><a href="../332582/index.html">Yet another tutorial: launch dotnet core docker application on Linux</a></li>
<li><a href="../332586/index.html">Fix bugs in 1988 style</a></li>
<li><a href="../332588/index.html">How to create your own metro</a></li>
<li><a href="../332594/index.html">Let's Encrypt will start issuing wildcard certificates in January 2018</a></li>
<li><a href="../332596/index.html">We remove and deposit cash at an ATM using a smartphone. World's first</a></li>
<li><a href="../332598/index.html">What I learned by converting a project to Kotlin using Android Studio</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>