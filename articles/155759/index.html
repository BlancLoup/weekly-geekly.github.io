<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Active appearance models</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Active Models of Appearance (Active Appearance Models, AAM) are statistical models of images that can be adapted to a real image by various kinds of d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Active appearance models</h1><div class="post__text post__text-html js-mediator-article">  Active Models of Appearance (Active Appearance Models, AAM) are statistical models of images that can be adapted to a real image by various kinds of deformations.  This type of model in a two-dimensional version was proposed by Tim Kutes and Chris Taylor in 1998 [1].  Initially, active models of appearance were used to estimate the parameters of facial images, but then they were actively used in other areas, in particular, in medicine in the analysis of X-ray images and images obtained using magnetic resonance imaging. <br><br><img src="https://habrastorage.org/storage2/f13/7f8/1b4/f137f81b402767bb38bcc99f7e9c2707.png"><br><div class="spoiler">  <b class="spoiler_title">Description of the illustration</b> <div class="spoiler_text">  The figure shows the result of the adaptation of the active model of appearance to the image of the face.  The blue grid shows the initial state of the model, and the red one shows what happened. </div></div><br><br>  This article discusses a brief description of how active models of appearance and the associated mathematical apparatus function, as well as an example of their implementation. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><h4>  Understanding Active Appearance Models </h4><br>  Over the past years, the mathematical apparatus of active models of appearance has been actively developed and at the moment there are 2 approaches to constructing such models: the classical one (the one that was originally proposed by Kutes) and based on the so-called reverse composition (proposed by Matthews and Baker in 2003 [2]). <br><br>  We first consider the common parts of the two approaches.  In active appearance models, two types of parameters are modeled: parameters associated with the <b>form</b> (shape parameters), and parameters related to the statistical model of the image or <b>texture</b> (appearance parameters).  Before using the model should be trained on a set of pre-marked images.  The marking of images is done manually or in semi-automatic mode, when using some algorithm are the approximate location of the tags, and then they are specified by an expert.  Each label has its own number and determines the characteristic point that the model will have to find when adapting to a new image.  An example of such markup (XM2VTS face database) is shown in the figure below. <br><img src="https://habrastorage.org/storage2/2bf/495/323/2bf4953232b3a8761465f3abfc7f55bd.png"><br><br>  In the presented example, the image shows 68 marks that form the shape of the model of the active appearance.  This form indicates the outer contour of the face, the contours of the mouth, eyes, nose, eyebrows.  This character of the markup allows us to further determine the various parameters of the face from its image, which can be used for further processing by other algorithms.  For example, these can be personality identification algorithms, audio-visual speech recognition, and the determination of the emotional state of a subject. <br><br>  The procedure for learning the appearance of active models begins with the normalization of the position of all forms in order to compensate for differences in scale, tilt and offset.  For this, the so-called generalized Procrustes analysis is used.  Here we will not give a detailed description of it, and an interested reader can read the <a href="http://en.wikipedia.org/wiki/Procrustes_analysis">corresponding article</a> in Wikipedia.  Here is how many labels look like before and after normalization (according to [3]). <br><img src="https://habrastorage.org/storage2/451/118/c67/451118c671fe020c1e20eb4f82226a6e.png"><br><br>  After all forms are normalized, a matrix is ‚Äã‚Äãformed from the components of their points. <img src="https://habrastorage.org/storage2/175/4a5/48a/1754a548a5683b1541fca41d92e58628.png">  where <img src="https://habrastorage.org/storage2/e06/5ea/8d6/e065ea8d65bbb4d288fae4c0d7144754.png">  .  After selecting the main components of the specified matrix, we obtain the following expression for the synthesized form: <br><img src="https://habrastorage.org/storage2/6fc/fed/46f/6fcfed46f2cba66d820e9155b26cee62.png">  . <br>  Here <img src="https://habrastorage.org/storage2/fc3/608/abe/fc3608abe6efa782d1d439796671ff2a.png">  - the form averaged over all implementations of the training sample (basic form), <img src="https://habrastorage.org/storage2/8a1/387/e6f/8a1387e6f81edac8f1e0c5c29bedd9c1.png">  - matrix of main vectors, <img src="https://habrastorage.org/storage2/a37/8e5/7ec/a378e57ec96718e80f38111ad8b20551.png">  - form parameters.  The above expression means that the form <img src="https://habrastorage.org/storage2/269/779/b8c/269779b8cbcb8755937fac6e52990770.png">  can be expressed as the sum of the base form <img src="https://habrastorage.org/storage2/fc3/608/abe/fc3608abe6efa782d1d439796671ff2a.png">  and linear combinations of proper forms contained in a matrix <img src="https://habrastorage.org/storage2/8a1/387/e6f/8a1387e6f81edac8f1e0c5c29bedd9c1.png">  .  By changing the parameter vector <img src="https://habrastorage.org/storage2/a37/8e5/7ec/a378e57ec96718e80f38111ad8b20551.png">  we can get various kinds of deformation of the form to fit it to the real image.  Below is an example of such a form [7].  The blue and red arrows show the directions of the main components. <br><img src="https://habrastorage.org/storage2/94c/f38/ef3/94cf38ef334ab4ff4d0c843126ed8601.png"><br><br>  It should be noted that there are models of active appearance with a hard and not hard deformation.  Models with rigid deformation can undergo only affine transformations (rotation, shift, scaling), while models with non-rigid deformation can be subjected to other types of deformations.  In practice, a combination of both types of deformations is used.  In this case, the layout parameters (rotation angle, scale, offset or affine transform coefficients) are also added to the form parameters. <br><br>  The learning procedure for the components of the appearance is performed after the components of the form (the basic form and the matrix of the main components) are calculated.  The learning process here consists of three steps.  The first step is to extract textures from the training images that best fit the basic form.  For this, a triangulation of the marks of the basic form and the form consisting of the marks of the training image is performed.  Then, using piecewise interpolation, the mapping of the training image regions resulting from the triangulation into the corresponding regions of the generated texture is performed.  As an example, the figure below shows the result of such a conversion for one of the images of the IMM database. <br><img src="https://habrastorage.org/storage2/521/e67/18e/521e6718e44c736926e25692aef4f2a2.png"><br><br>  After all the textures have been formed, in the second step, their photometric normalization is performed in order to compensate for the different lighting conditions.  Currently, a large number of methods have been developed to allow this.  The simplest of them is the subtraction of the average value and the normalization of the dispersion of pixel brightness. <br><br>  Finally, in the third step, a matrix is ‚Äã‚Äãformed from the textures, such that each column of it contains the pixel values ‚Äã‚Äãof the corresponding texture (similar to the matrix <img src="https://habrastorage.org/storage2/8e0/3b2/676/8e03b267621409ab1063869293048b4e.png">  ).  It is worth noting that the textures used for learning can be either single-channel (grayscale) or multi-channel (for example, RGB color space or another).  In the case of multichannel textures, the vectors of pixels are formed separately for each of the channels, and then they are concatenated.  After finding the main components of the texture matrix, we obtain the expression for the synthesized texture: <br><img src="https://habrastorage.org/storage2/eeb/94b/3d8/eeb94b3d85f47eb54077dc07b7a46821.png">  . <br>  Here <img src="https://habrastorage.org/storage2/6e2/b1c/5f6/6e2b1c5f6bde2194c8b19756c3bff5e3.png">  - basic texture obtained by averaging over all textures of the training set, <img src="https://habrastorage.org/storage2/f8e/b96/8f9/f8eb968f972a97a7017aff637379a25c.png">  - Matrix own textures <img src="https://habrastorage.org/storage2/8f7/fee/381/8f7fee381e43cbc2516246e629f109c7.png">  - vector of active appearance parameters.  Below is an example of a synthesized texture [7]. <br><img src="https://habrastorage.org/storage2/793/d82/ca6/793d82ca6b1238f6a3a50b29d0da53f9.png"><br><br>  In practice, to reduce the overtraining effect of the model, only 95-98% of the most significant vectors are left in the matrices of the main components.  Moreover, this number may be different for the main components of the form and the main components of the appearance.  Refined figures can be selected already in the process of experimental studies or when testing a model using the cross-validation procedure. <br><br>  This is where the common part of different types of active models of appearance ends and now we consider the differences between the two approaches. <br><br><h4>  Classic Active Appearance Model </h4><br>  In a model of this type, we also need to calculate the vector of combined parameters, which is given by the following formula: <br><img src="https://habrastorage.org/storage2/0e8/8e8/e8c/0e88e8e8cb000a86e6ff75787a1328ee.png">  . <br>  Here <img src="https://habrastorage.org/storage2/98f/462/251/98f462251706a84ecb69c0f1586ac9e3.png">  - a diagonal matrix of weight values, which allows to balance the contribution of the distances between pixels and pixel intensities.  For each element of the training set (texture-form pair), its own vector is calculated <img src="https://habrastorage.org/storage2/b1b/88a/275/b1b88a275c7600e760ec10445bf96b9a.png">  .  Then the resulting set of vectors is combined into a matrix and its main components are found.  In this case, the synthesized vector of the combined shape and texture parameters is defined by the following expression: <br><img src="https://habrastorage.org/storage2/b44/c48/c24/b44c48c243eaee5cc074eb7e4a2d1e6d.png">  . <br>  Here <img src="https://habrastorage.org/storage2/9ac/6c4/5c5/9ac6c45c505e5ae407a61e13b0aa83be.png">  - the matrix of the main components of the combined parameters <img src="https://habrastorage.org/storage2/ec7/40b/e23/ec740be2346366f997fbe5837a267485.png">  - vector of combined appearance parameters.  From here we can get new expressions for the synthesized form and texture: <br><img src="https://habrastorage.org/storage2/c65/8fc/371/c658fc371ad0caaf2f3e932c60f66950.png">  . <br><br>  In practice, the matrix <img src="https://habrastorage.org/storage2/9ac/6c4/5c5/9ac6c45c505e5ae407a61e13b0aa83be.png">  It also removes noise components to reduce the effect of retraining and reduce the amount of computation. <br><br>  After the shape, appearance and combined parameters have been calculated, we need to find the so-called prediction matrix. <img src="https://habrastorage.org/storage2/ea4/c70/fbc/ea4c70fbca86b4f7dc0f54ae58fa01ac.png">  which, in the sense of minimum rms error, would satisfy the following linear equation: <br><img src="https://habrastorage.org/storage2/c7f/ca7/76b/c7fca776b45a640f39c4566b2cf04866.png">  . <br>  Here <img src="https://habrastorage.org/storage2/05a/0d6/677/05a0d6677216b3bd4a0fcdea3632a273.png">  , but <img src="https://habrastorage.org/storage2/377/234/a97/377234a973f3f10f85ee39d4eed22cba.png">  - perturbation of the position vector and combined appearance parameters.  Various methods have been developed to solve the above equation.  A detailed review of them was carried out in [3 - 6]. <br><br>  Adaptation of the considered active model of appearance to the analyzed image occurs, in the general case, as follows. <br><ol><li>  Based on the initial approximation, all model parameters and affine transformations of the form are calculated; </li><li>  Calculates the error vector <img src="https://habrastorage.org/storage2/5fd/dea/fa3/5fddeafa3865c164a4bc8dda7f31b965.png">  .  The texture is extracted from the analyzed image using its piecewise deformation; </li><li>  Calculate the perturbation vector <img src="https://habrastorage.org/storage2/c7f/ca7/76b/c7fca776b45a640f39c4566b2cf04866.png">  ; </li><li>  The vector of combined parameters and affine transformations is updated by summing their current values ‚Äã‚Äãwith the corresponding components of the perturbation vector; </li><li>  The shape and texture are updated; </li><li>  We proceed to the implementation of paragraph 2 until, until we reach convergence. </li></ol><br><br>  Various modifications and improvements have been proposed to this algorithm, but its overall structure and essence remains the same. <br><br>  The above algorithm is quite effective, but it has a rather serious drawback, which limits its use in real-time applications: it slowly converges and requires a large amount of computation.  To overcome these shortcomings, a new type of active appearance models was proposed in [2, 7], which will be discussed in the next section. <br><br><h4>  Active model of the appearance of the reverse composition </h4><br>  Matthews and Baker proposed a computationally efficient algorithm for adapting the active model of appearance, which depends only on the shape parameters (the so-called ‚Äúproject-out‚Äù model).  Due to this, it was possible to significantly increase its speed.  The adaptation algorithm, which was based on the Lucas-Canada approach, uses the Newton method to find the minimum of the error function. <br><br>  The Lucas-Canada algorithm is trying to find a locally best fit in the sense of the minimum of the root-mean-square error between the template and the real image.  In this case, the pattern is subjected to deformation (affine and / or piecewise) specified by the parameter vector <img src="https://habrastorage.org/storage2/d9a/a08/b74/d9aa08b74a9255be00aae3be08704598.png">  which maps its pixels to the pixels of the real image. <br><br>  Direct finding of parameters <img src="https://habrastorage.org/storage2/d9a/a08/b74/d9aa08b74a9255be00aae3be08704598.png">  is a nonlinear optimization problem.  To solve it using linear methods, the Lucas-Canada algorithm assumes that the initial value of the deformation parameters is known and then iteratively finds the increments of the parameters <img src="https://habrastorage.org/storage2/377/234/a97/377234a973f3f10f85ee39d4eed22cba.png">  by updating the vector on each iteration <img src="https://habrastorage.org/storage2/d9a/a08/b74/d9aa08b74a9255be00aae3be08704598.png">  . <br><br>  The active model of the appearance of the reverse composition uses a similar approach to update its own parameters during the adaptation process, except that the non-basic texture undergoes deformation <img src="https://habrastorage.org/storage2/6e2/b1c/5f6/6e2b1c5f6bde2194c8b19756c3bff5e3.png">  , and the analyzed image. <br><br>  At the stage of learning the active model of the appearance of the reverse composition, so-called images of the fastest descent and their Hessian are calculated.  Adaptation of the model takes place in a manner similar to the classical model of appearance, except that in this case only the update of the form parameters and (optionally) the location parameters takes place. <br><br>  It is worth noting that Matthews and Baker offered a large number of possible variations with different properties of the models they developed.  An interested reader can refer to the works [2, 7 - 9] for more information. <br><br><h4>  Software implementation </h4><br>  For the practical implementation and research of the above learning algorithms and adaptation of active models of appearance, the author has developed a specialized software library called AAMToolbox.  The library is distributed under the GPLv3 license and is intended for use only for non-commercial and research purposes.  Source codes are available at this <a href="https://github.com/phoenix367/AAMToolbox">link</a> . <br><br>  AAMToolbox requires the library OpenCV 2.4, boost 1.42 or higher, IDE NetBeans 6.9.  Ubuntu Linux OS 10.04 and 10.10 are currently supported.  Performance and collection on other platforms has not been verified. <br><br>  AAMToolbox implements algorithms for working with both the classic active appearance model and the active appearance model of the reverse composition.  Access to both types of algorithms is carried out through a single interface that provides training of the model on a given training sample, saving and restoring the trained model from the file, and adapting the model to the real image.  Both color images (in three-channel color) and images in grayscale are supported. <br><br>  In order to train a model, you must first prepare a training set.  The sample should consist of two types of files.  The first type is the actual images that the model will be trained on.  Files of the second type are text markup files and contain form marks indicated on the corresponding images of the training sample.  Below is a fragment of such a file. <br><br><pre><code class="hljs">1 228 307 2 232 327 3 239 350 5 270 392 6 294 406 7 314 410 8 343 403 9 361 388 10 372 370 11 382 349 12 388 331 13 393 312 14 374 243</code> </pre> <br><br>  Here, the first column is the label number, the second column is the X coordinate of the label, the third column is the Y coordinate of the label.  Each image must have its own markup file. <br><br>  The learning code for the active appearance model is fairly simple. <br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"aam/AAMEstimator.h"</span></span></span><span class="hljs-meta"> void trainAAM() { </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//      aam::AAMEstimator estimator; //     ,  //    // aam::ModelPathType     // std::pair&lt;std::string, std::string&gt;,   //       ,  //  -    . std::vector&lt;aam::ModelPathType&gt; modelPaths; //  -     ...................................................... // //       aam::TrainOptions options; //     .    0  1. options.setPCACutThreshold(0.95); // ,     : // true -  , false -   options.setGrayScale(true); //       . //       //  . options.setMultithreading(true); //    : // aam::algorithm::conventional -  , // aam::algorithm::inverseComposition -    options.setAAMAlgorithm(aam::algorithm::conventional); //   .     . //   ,       //  .     triangles  //   std::vector&lt;cv::Vec3i&gt;      //  (    0. options.setTriangles(triangles); //      . //        , //        . options.setScales(4); estimator.setTrainOptions(options); //    estimator.train(modelPaths); //      estimator.save("data/aam_test.xml"); }</span></span></span></span></code> </pre><br><br>  As a result of executing the presented code fragment, it allows you to train the active model of the appearance of a given type and save it to a file.  It is worth noting that during training, all data, including images, are in RAM, so when loading a large number of images (several hundred), you should take care to ensure that there are enough of it available (2 - 3 GB).  As an example of code that conducts the training procedure for different types of active models of appearance, you can look at the ‚ÄúAAM Estimator test‚Äù unit test of the library project.  If it is launched for execution, it will train and save each of the supported types in the appropriate files in the variant for color images and grayscale (total 4 different models). <br><br>  The adaptation code of the active model of appearance to the image will look as follows: <br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"aam/AAMEstimator.h"</span></span></span><span class="hljs-meta"> void aplyAAM() { </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">//  . //         aam::AAMEstimator estimator; estimator.load("&lt;___&gt;"); //   cv::Mat im = cv::imread("&lt;___&gt;"); //      std::vector&lt;cv::Rect&gt; faces; cv::cvtColor(im, im, CV_BGR2GRAY); cascadeFace.detectMultiScale(im, faces, 1.1, 2, 0 |CV_HAAR_FIND_BIGGEST_OBJECT //|CV_HAAR_DO_ROUGH_SEARCH |CV_HAAR_SCALE_IMAGE , cv::Size(30, 30) ); if (faces.empty()) { return; } cv::Rect r = faces[0]; aam::Point2D startPoint(rx + r.width * 0.5 + 20, ry + r.height * 0.5 + 40); // ,        aam::Vertices2DList foundPoints; //   .   verbose  //       . estimator.estimateAAM(im, startPoint, foundPoints, true); }</span></span></span></span></code> </pre><br><br>  In order to see a demonstration of the operation of adaptation algorithms for active appearance models, it is necessary to run the ‚ÄúAply model test‚Äù and ‚ÄúAply model IC test‚Äù unit tests, which adapt the models of supported types to the image.  The figure below shows an example of one of the results obtained. <br><img src="https://habrastorage.org/storage2/634/679/19c/63467919c2d97103e22a4193e79742cb.png"><br><br>  These tests clearly demonstrate the difference in the rate of convergence of the classical active model of appearance and the active model of the appearance of the reverse composition.  However, the lack of the latter can be attributed to the divergence of the algorithm of its adaptation in some cases.  Several approaches have been proposed for its elimination, but they are not implemented in the AAMToolbox library under review (at least for the time being). <br><br><h4>  Conclusion </h4><br>  The article briefly reviewed the active models of appearance and the basic concepts and mathematical apparatus associated with them.  Also considered is the author-developed AAMToolbox software library that implements the algorithms outlined in the article.  Examples of its use are given. <br><br>  Behind a frame there were three-dimensional models of active appearance and the algorithms connected with them.  Perhaps they will be discussed in the following articles. <br><br><h4>  Bibliography </h4><br><ol><li>  T. Cootes, G. Edwards, and C. Taylor.  Active appearance models.  In Proceedings of the European Conference on Computer Vision, volume 2, pages 484‚Äì498, 1998. </li><li>  S. Baker, R. Gross, and I. Matthews.  Lucas-Kanade 20 years on: A unifying framework: Part 3. Technical Report CMU-RI-TR-03-35, Carnegie Mellon University Robotics Institute, 2003. </li><li>  Stegmann Analysis and Segmentation of MB Images using Linear Subspace Techniques.  Technical report IMM-REP-2002-22, Informatics and Mathematical Modeling, Technical University of Denmark, 2002 </li><li>  TF Cootes, GJ Edwards, and CJ Taylor.  Active appearance models.  IEEE Trans.  on Pattern Recognition and Machine Intelligence, 23 (6): 681‚Äì685, 2001. </li><li>  TF Cootes and CJ Taylor.  Statistical analysis and computer vision.  In Proc.  SPIE Medical Imaging 2001, volume 1, pages 236‚Äì248.  SPIE, 2001. </li><li>  TF Cootes and CJ Taylor.  Constrained active appearance models.  Computer Vision, 2001. ICCV 2001. Proceedings.  Eighth IEEE International Conference on, 1: 748‚Äì754 vol.1, 2001. </li><li>  Iain Matthews and Simon Baker Active Appearance Models Revisited.  International Journal of Computer Vision, Vol.  60, No.  2, November, 2004, pp.  135 - 164. </li><li>  S. Baker, R. Gross, and I. Matthews.  Lucas-Kanade 20 years on: A unifying framework: Part 1. Technical Report CMU-RI-TR-02-16, Carnegie Mellon University Robotics Institute, 2002. </li><li>  S. Baker, R. Gross, and I. Matthews.  Lucas-Kanade 20 years on: A unifying framework: Part 2. Technical Report CMU-RI-TR-03-01, Carnegie Mellon University Robotics Institute, 2003. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/155759/">https://habr.com/ru/post/155759/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../155743/index.html">Habrahabra icon disappeared</a></li>
<li><a href="../155745/index.html">GTLD administrators will provide registration services directly</a></li>
<li><a href="../155747/index.html">Chromebook for 249 cu already available on Google Play Store</a></li>
<li><a href="../155749/index.html">You can run Ubuntu on Samsung's Chromebook</a></li>
<li><a href="../155751/index.html">Installing / Configuring Exchange 2010 in a multi-domain / hosting implementation</a></li>
<li><a href="../155761/index.html">Bot watchers - the freedom to comment</a></li>
<li><a href="../155763/index.html">Integrating SAS and Greenplum</a></li>
<li><a href="../155769/index.html">Adding SSD to laptop</a></li>
<li><a href="../155771/index.html">Innovations node-webkit version 0.3.0</a></li>
<li><a href="../155773/index.html">Vertica on HighLoad ++</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>