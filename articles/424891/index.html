<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Identify fraud using Enron dataset. Part 1, data preparation and selection of marks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Enron Corporation is one of the most famous figures in the American business of the 2000s. This was facilitated not by their sphere of activity (elect...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Identify fraud using Enron dataset. Part 1, data preparation and selection of marks</h1><div class="post__text post__text-html js-mediator-article"><p>  Enron Corporation is one of the most famous figures in the American business of the 2000s.  This was facilitated not by their sphere of activity (electricity and contracts for its supply), but by the response due to fraud in it.  For 15 years, corporate earnings grew rapidly, and work in it promised good wages.  But it ended just as quickly: in the period 2000-2001.  the share price fell from $ 90 / piece to almost zero due to the discovery of declared income fraud.  Since then, the word "Enron" has become a household name and acts as a label for companies that operate in a similar way. </p><br><p>  During the trial, 18 people (including the biggest defendants in this case: Andrew Fastov, Jeff Skilling and Kenneth Lay) were convicted. </p><br><p><img src="https://habrastorage.org/webt/te/rh/1l/terh1lsenbtg26n8nhjbhv3opfi.jpeg" alt="image! [image] (http: // https: //habrastorage.org/webt/te/rh/1l/terh1lsenbtg26n8nhjbhv3opfi.jpeg)"></p><br><p>  However, an archive of electronic correspondence between employees of the company, better known as Enron Email Dataset, and insider information about the income of employees of this company were published. </p><br><p>  The article will consider the sources of these data and build a model based on them, which allows to determine whether a person is suspected of fraud.  Sounds interesting?  Then, welcome under habrakat. <a name="habracut"></a></p><br><h1 id="opisanie-dataseta">  Dataset description </h1><br><p>  Enron dataset (dataset) is a consolidated set of open data that contains records of people working in a memorable corporation with a corresponding name. <br>  It can be divided into 3 parts: </p><br><ul><li>  payments_features - a group characterizing financial movements; </li><li>  stock_features - group, reflecting the attributes associated with the shares; </li><li>  email_features is a group that reflects information about the email of a particular person in an aggregated form. </li></ul><br><p>  Of course, there is also a target variable that indicates whether the person is suspected of fraud (the <abbr title="Person of interest">'poi'</abbr> sign <abbr title="Person of interest"><abbr>).</abbr></abbr> <abbr title="Person of interest"><br></abbr> </p><p>  Let's load our data and start with working with them: </p><br><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pickle <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>("final_project/enron_dataset.pkl", "rb") <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> data_file: data_dict = pickle.<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>(data_file)</code> </pre> <br><p>  Then turn the <strong>data_dict</strong> set into the Pandas dataframe for more convenient work with the data: </p><br><pre> <code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> warnings warnings.filterwarnings('<span class="hljs-title"><span class="hljs-title">ignore'</span></span>) source_df = pd.DataFrame.from_dict(<span class="hljs-title"><span class="hljs-title">data_dict</span></span>, <span class="hljs-title"><span class="hljs-title">orient</span></span> = '<span class="hljs-title"><span class="hljs-title">index'</span></span>) source_df.drop('<span class="hljs-type"><span class="hljs-type">TOTAL</span></span>',<span class="hljs-title"><span class="hljs-title">inplace</span></span>=<span class="hljs-type"><span class="hljs-type">True</span></span>)</code> </pre> <br><p>  Group the signs in accordance with the previously specified types.  This should facilitate the work with the data afterwards: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">payments_features</span></span> = [<span class="hljs-string"><span class="hljs-string">'salary'</span></span>, <span class="hljs-string"><span class="hljs-string">'bonus'</span></span>, <span class="hljs-string"><span class="hljs-string">'long_term_incentive'</span></span>, <span class="hljs-string"><span class="hljs-string">'deferred_income'</span></span>, <span class="hljs-string"><span class="hljs-string">'deferral_payments'</span></span>, <span class="hljs-string"><span class="hljs-string">'loan_advances'</span></span>, <span class="hljs-string"><span class="hljs-string">'other'</span></span>, <span class="hljs-string"><span class="hljs-string">'expenses'</span></span>, <span class="hljs-string"><span class="hljs-string">'director_fees'</span></span>, <span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>] stock_features = [<span class="hljs-string"><span class="hljs-string">'exercised_stock_options'</span></span>, <span class="hljs-string"><span class="hljs-string">'restricted_stock'</span></span>, <span class="hljs-string"><span class="hljs-string">'restricted_stock_deferred'</span></span>,<span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>] email_features = [<span class="hljs-string"><span class="hljs-string">'to_messages'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_poi_to_this_person'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_messages'</span></span>, <span class="hljs-string"><span class="hljs-string">'from_this_person_to_poi'</span></span>, <span class="hljs-string"><span class="hljs-string">'shared_receipt_with_poi'</span></span>] target_field = <span class="hljs-string"><span class="hljs-string">'poi'</span></span></code> </pre> <br><h2 id="finansovye-dannye">  Financial data </h2><br><p>  In this data there is a well-known NaN, and it expresses a familiar gap in the data.  In other words, the author of the dataset could not find any information on a particular attribute related to a specific line in the data frame.  As a consequence, we can assume that NaN is 0, since there is no information about a specific trait. </p><br><pre> <code class="hljs pgsql">payments = source_df[payments_features] payments = payments.replace(<span class="hljs-string"><span class="hljs-string">'NaN'</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><h3 id="proverka-dannyh">  Data checking </h3><br><p>  When comparing with the <a href="https://github.com/udacity/ud120-projects/raw/master/final_project/enron61702insiderpay.pdf">original PDF</a> underlying the dataset, it turned out that the data is slightly distorted, since the total_payments field is not for all the lines in the data frame of <strong>payments</strong> .  You can check this as follows: </p><br><pre> <code class="hljs pgsql">errors = payments[payments[payments_features[:<span class="hljs-number"><span class="hljs-number">-1</span></span>]].sum(axis=<span class="hljs-string"><span class="hljs-string">'columns'</span></span>) != payments[<span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>]] errors.head()</code> </pre> <br><p><img src="https://habrastorage.org/webt/eo/79/ye/eo79ye27iirsiwcuickrxz4on30.png" alt="2 incorrect lines"><br>  We see that BELFER ROBERT and BHATNAGAR SANJAY have incorrect payment amounts. </p><br><p>  This error can be corrected by shifting the data in the erroneous lines to the left or right and counting the sum of all payments again: </p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np shifted_values = payments.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, payments_features[<span class="hljs-number"><span class="hljs-number">1</span></span>:]].<span class="hljs-keyword"><span class="hljs-keyword">values</span></span> expected_payments = shifted_values.sum() shifted_values = np.append(shifted_values, expected_payments) payments.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, payments_features] = shifted_values shifted_values = payments.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, payments_features[:<span class="hljs-number"><span class="hljs-number">-1</span></span>]].<span class="hljs-keyword"><span class="hljs-keyword">values</span></span> payments.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, payments_features] = np.<span class="hljs-keyword"><span class="hljs-keyword">insert</span></span>(shifted_values, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><h2 id="dannye-po-akciyam">  Share data </h2><br><pre> <code class="hljs pgsql">stocks = source_df[stock_features] stocks = stocks.replace(<span class="hljs-string"><span class="hljs-string">'NaN'</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>)</code> </pre> <br><p>  Perform a validation check in this case as well: </p><br><pre> <code class="hljs pgsql">errors = stocks[stocks[stock_features[:<span class="hljs-number"><span class="hljs-number">-1</span></span>]].sum(axis=<span class="hljs-string"><span class="hljs-string">'columns'</span></span>) != stocks[<span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>]] errors.head()</code> </pre> <br><p><img src="https://habrastorage.org/webt/sd/28/mz/sd28mzikmhurh_b0mqwibnje8fa.png" alt="image"></p><br><p>  Correct the same error in promotions: </p><br><pre> <code class="hljs pgsql">shifted_values = stocks.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, stock_features[<span class="hljs-number"><span class="hljs-number">1</span></span>:]].<span class="hljs-keyword"><span class="hljs-keyword">values</span></span> expected_payments = shifted_values.sum() shifted_values = np.append(shifted_values, expected_payments) stocks.loc[<span class="hljs-string"><span class="hljs-string">'BELFER ROBERT'</span></span>, stock_features] = shifted_values shifted_values = stocks.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, stock_features[:<span class="hljs-number"><span class="hljs-number">-1</span></span>]].<span class="hljs-keyword"><span class="hljs-keyword">values</span></span> stocks.loc[<span class="hljs-string"><span class="hljs-string">'BHATNAGAR SANJAY'</span></span>, stock_features] = np.<span class="hljs-keyword"><span class="hljs-keyword">insert</span></span>(shifted_values, <span class="hljs-number"><span class="hljs-number">0</span></span>, shifted_values[<span class="hljs-number"><span class="hljs-number">-1</span></span>])</code> </pre> <br><h2 id="svodnye-dannye-po-elektronnoy-perepiske">  Email Summary </h2><br><p>  If NaN was equivalent to 0 for these finances or shares, and this fit into the final result for each of these groups, in the case of email, NaN is more reasonable to replace with some default value.  To do this, you can use the Imputer: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">from</span></span> sklearn.impute <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> SimpleImputer imp = SimpleImputer()</code> </pre> <br><p>  However, we will consider the default value for each category (if a person is suspected of fraud) separately: </p><br><pre> <code class="hljs markdown">target = source<span class="hljs-emphasis"><span class="hljs-emphasis">_df[target_</span></span>field] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data = source_</span></span>df[<span class="hljs-string"><span class="hljs-string">email_features</span></span>] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data = pd.concat([email_</span></span>data, target], axis=1) email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>poi = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data[email_</span></span>data[<span class="hljs-string"><span class="hljs-string">target_field</span></span>]][<span class="hljs-string"><span class="hljs-string">email_features</span></span>] email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>nonpoi = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data[email_</span></span>data[<span class="hljs-string"><span class="hljs-string">target_field</span></span>] == False][email<span class="hljs-emphasis"><span class="hljs-emphasis">_features] email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_poi[email_</span></span>features] = imp.fit<span class="hljs-emphasis"><span class="hljs-emphasis">_transform(email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_poi) email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_nonpoi[email_</span></span>features] = imp.fit<span class="hljs-emphasis"><span class="hljs-emphasis">_transform(email_</span></span>data<span class="hljs-emphasis"><span class="hljs-emphasis">_nonpoi) email_</span></span>data = email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>poi.append(email<span class="hljs-emphasis"><span class="hljs-emphasis">_data_</span></span>nonpoi)</code> </pre> <br><p>  Final dataset after correction: </p><br><pre> <code class="hljs cs">df = payments.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(stocks) df = df.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(email_data) df = df.astype(<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)</code> </pre> <br><h2 id="vybrosy">  Emissions </h2><br><p>  At the final step of this stage, we‚Äôll remove all outliers that can distort the training.  At the same time, there is always the question: how much data can we remove from the sample and not lose as a learning model?  I followed the advice of one of the lecturers leading the course on ML (machine learning) at Udacity - ‚ÄúRemove 10 pieces and check for emissions again‚Äù. </p><br><pre> <code class="hljs xml">first_quartile = df.quantile(q=0.25) third_quartile = df.quantile(q=0.75) IQR = third_quartile - first_quartile outliers = df[(df &gt; (third_quartile + 1.5 * IQR)) | (df <span class="hljs-tag"><span class="hljs-tag">&lt; (</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">first_quartile</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">-</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">1.5</span></span></span><span class="hljs-tag"> * </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">IQR</span></span></span><span class="hljs-tag">))]</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">.count</span></span></span><span class="hljs-tag">(</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axis</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">1)</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">outliers.sort_values</span></span></span><span class="hljs-tag">(</span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axis</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">0,</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">ascending</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">False,</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">inplace</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">True)</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">outliers</span></span></span><span class="hljs-tag"> = </span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">outliers.head(10)</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">outliers</span></span></span></span></code> </pre> <br><p>  At the same time, we will not delete records that are outliers and are suspected of fraud.  The reason is that there are only 18 lines with such data, and we cannot sacrifice them, as this may lead to a lack of examples for learning.  As a result, we remove only those who are not suspected of fraud, but at the same time have a large number of signs, according to which emissions are observed: </p><br><pre> <code class="hljs pgsql">target_for_outliers = target.loc[outliers.<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>] outliers = pd.concat([outliers, target_for_outliers], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) non_poi_outliers = outliers[np.logical_not(outliers.poi)] df.<span class="hljs-keyword"><span class="hljs-keyword">drop</span></span>(non_poi_outliers.<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>, inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><h2 id="privedenie-k-itogovom-vidu">  Reduction to the final form </h2><br><p>  Normalize our data: </p><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scale df[df.<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>] = scale(df)</code> </pre> <br><p>  Let's bring the target variable target to a compatible view: </p><br><pre> <code class="hljs cmake"><span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.drop(non_poi_outliers.index, inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">target</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.map({<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>}) <span class="hljs-keyword"><span class="hljs-keyword">target</span></span>.value_counts()</code> </pre> <br><p><img src="https://habrastorage.org/webt/nr/mn/fi/nrmnfi0bdldw36fg9uy-tcxinsa.png" alt="image"><br>  As a result, 18 suspects and 121 those who did not fall under suspicion. </p><br><h1 id="otbor-priznakov">  Feature selection </h1><br><p>  Perhaps one of the most key points before learning any model is the selection of the most important features. </p><br><h2 id="proverka-na-multikollinearnost">  Multicollinearity test </h2><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns %matplotlib <span class="hljs-keyword"><span class="hljs-keyword">inline</span></span> sns.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(style="whitegrid") corr = df.corr() * <span class="hljs-number"><span class="hljs-number">100</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">Select</span></span> upper triangle <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> correlation matrix mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span> up the matplotlib figure f, ax = plt.subplots(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>, <span class="hljs-number"><span class="hljs-number">11</span></span>)) # Generate a custom diverging colormap cmap = sns.diverging_palette(<span class="hljs-number"><span class="hljs-number">220</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>) # Draw the heatmap <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> the mask <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> correct aspect ratio sns.heatmap(corr, mask=mask, cmap=cmap, center=<span class="hljs-number"><span class="hljs-number">0</span></span>, linewidths=<span class="hljs-number"><span class="hljs-number">1</span></span>, cbar_kws={"shrink": <span class="hljs-number"><span class="hljs-number">.7</span></span>}, annot=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, fmt=".2f")</code> </pre> <br><p><img src="https://habrastorage.org/webt/kw/66/ta/kw66tahpopi7zjc6tvaj1uzq9qe.png" alt="image"><br>  As can be seen from the image, we have a pronounced relationship between 'loan_advanced' and 'total_payments', as well as between 'total_stock_value' and 'restricted_stock'.  As mentioned earlier, 'total_payments' and 'total_stock_value' are just the result of adding all the indicators in a particular group.  Therefore, they can be removed: </p><br><pre> <code class="hljs pgsql">df.<span class="hljs-keyword"><span class="hljs-keyword">drop</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>=[<span class="hljs-string"><span class="hljs-string">'total_payments'</span></span>, <span class="hljs-string"><span class="hljs-string">'total_stock_value'</span></span>], inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><h2 id="sozdanie-novyh-priznakov">  Creating new features </h2><br><p>  There is also an assumption that suspects wrote to accomplices more often than employees who were not involved in this.  And as a result, the share of such messages should be greater than the share of messages to ordinary employees.  Based on this statement, you can create new signs reflecting the percentage of incoming / outgoing related to the suspects: </p><br><pre> <code class="hljs cs">df[<span class="hljs-string"><span class="hljs-string">'ratio_of_poi_mail'</span></span>] = df[<span class="hljs-string"><span class="hljs-string">'from_poi_to_this_person'</span></span>]/df[<span class="hljs-string"><span class="hljs-string">'to_messages'</span></span>] df[<span class="hljs-string"><span class="hljs-string">'ratio_of_mail_to_poi'</span></span>] = df[<span class="hljs-string"><span class="hljs-string">'from_this_person_to_poi'</span></span>]/df[<span class="hljs-string"><span class="hljs-string">'from_messages'</span></span>]</code> </pre> <br><h2 id="otsev-lishnih-priznakov">  Eliminating unnecessary signs </h2><br><p>  In the toolkit of people associated with ML, there are many excellent tools for selecting the most significant attributes (SelectKBest, SelectPercentile, VarianceThreshold, etc.).  In this case, RFECV will be used because it includes cross-validation, which allows you to calculate the most important features and check them on all subsets of the sample: </p><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split X_train, X_test, y_train, y_test = train_test_split(<span class="hljs-title"><span class="hljs-title">df</span></span>, <span class="hljs-title"><span class="hljs-title">target</span></span>, <span class="hljs-title"><span class="hljs-title">test_size</span></span>=0.2, <span class="hljs-title"><span class="hljs-title">random_state</span></span>=42)</code> </pre> <br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RFECV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier forest = RandomForestClassifier(random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>) rfecv = RFECV(estimator=forest, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, scoring=<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>) rfecv = rfecv.fit(X_train, y_train) plt.figure() plt.xlabel("Number of features selected") plt.ylabel("Cross validation score of number of selected features") plt.plot(range(<span class="hljs-number"><span class="hljs-number">1</span></span>, len(rfecv.grid_scores_) + <span class="hljs-number"><span class="hljs-number">1</span></span>), rfecv.grid_scores_, <span class="hljs-string"><span class="hljs-string">'--o'</span></span>) indices = rfecv.get_support() <span class="hljs-keyword"><span class="hljs-keyword">columns</span></span> = X_train.<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>[indices] print(<span class="hljs-string"><span class="hljs-string">'The most important columns are {}'</span></span>.format(<span class="hljs-string"><span class="hljs-string">','</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">join</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>)))</code> </pre> <br><p><img src="https://habrastorage.org/webt/fm/-7/oo/fm-7oo9gnbxhmpiw_vwldkgpwpi.png" alt="image"><br>  As you can see, RandomForestClassifier considered that only 7 signs out of 18 are important.  Using the rest leads to a decrease in the accuracy of the model. </p><br><pre> <code class="hljs pgsql">The most important <span class="hljs-keyword"><span class="hljs-keyword">columns</span></span> are bonus, deferred_income, other, exercised_stock_options, shared_receipt_with_poi, ratio_of_poi_mail, ratio_of_mail_to_poi</code> </pre> <br><p>  These 7 features will be used further in order to simplify the model and reduce the risk of retraining: </p><br><ul><li>  bonus </li><li>  deferred_income </li><li>  other </li><li>  exercised_stock_options </li><li>  shared_receipt_with_poi </li><li>  ratio_of_poi_mail </li><li>  ratio_of_mail_to_poi </li></ul><br><p>  Let's change the structure of the training and test samples for the future training of the model: </p><br><pre> <code class="hljs pgsql">X_train = X_train[<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>] X_test = X_test[<span class="hljs-keyword"><span class="hljs-keyword">columns</span></span>]</code> </pre> <br><p>  This is the end of the first part, describing the use of Enron Dataset as an example of a classification task in ML.  Based on materials from the course Introduction to Machine Learning at Udacity.  There is also a <a href="https://github.com/VeeSot/Enron-Fraud-Identify/blob/master/part1.ipynb">python notebook</a> , reflecting the entire sequence of actions. </p><br><blockquote>  The second part is <a href="https://habr.com/post/425607/">here</a> <br></blockquote><p></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/424891/">https://habr.com/ru/post/424891/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424877/index.html">Liquid brake cooling system</a></li>
<li><a href="../424879/index.html">Interface availability. Yandex lecture</a></li>
<li><a href="../424881/index.html">Newtoo - development of a full browser engine from scratch in 2018?</a></li>
<li><a href="../424887/index.html">What Lida is silent about: the beginning of a developer's career. Principles or how to become Middl'om</a></li>
<li><a href="../424889/index.html">Looking inside the Intel 8087 coprocessor</a></li>
<li><a href="../424893/index.html">The craftsman created a WiFi module for Macintosh SE / 30, 1989 models</a></li>
<li><a href="../424895/index.html">Designing Types: How to make invalid states ineffable</a></li>
<li><a href="../424897/index.html">Unexpected meeting. Chapter 18</a></li>
<li><a href="../424899/index.html">What to listen about audio equipment: 15 podcasts</a></li>
<li><a href="../424901/index.html">The digest of interesting materials for the mobile developer # 272 (September 24 - September 30)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>