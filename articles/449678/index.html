<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why software tasks always take longer than you think</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Everyone in the IT industry knows how difficult it is to assess the duration of a project. It is difficult to objectively estimate how long it will ta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why software tasks always take longer than you think</h1><div class="post__text post__text-html js-mediator-article">  Everyone in the IT industry knows how <i>difficult it</i> is <i>to</i> assess the duration of a project.  It is difficult to objectively estimate how long it will take <i>to solve a</i> difficult problem.  One of my favorite theories is that it‚Äôs just a statistical artifact. <br><br>  Suppose you rate a project at 1 week.  Suppose there are three equally likely results: either it takes 1/2 week, or 1 week, or 2 weeks.  The median result is actually the same as the assessment: 1 week, but the average value (aka average, aka expected value) is 7/6 = 1.17 weeks.  The estimate is actually calibrated (impartial) for the median (which is equal to 1), but not for the average. <br><a name="habracut"></a><br>  A reasonable model for the ‚Äúinflation factor‚Äù (actual time divided by estimated time) will be something like a <a href="https://en.wikipedia.org/wiki/Log-normal_distribution">lognormal distribution</a> .  If the estimate is equal to one week, then we will model the real result as a random variable distributed in accordance with a lognormal distribution of about one week.  In this situation, the median of the distribution is exactly one week, but the average is much larger: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/756/143/4a6/7561434a6cc5fb6c832d986b878a0eb9.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      If we take the logarithm of the inflation rate, we get a simple normal distribution with center about 0. This implies a median inflation rate of 1x, and, as you hopefully remember, log (1) = 0. However, in various problems there can be different uncertainties around 0. We can model them by changing the parameter œÉ, which corresponds to the standard deviation of the normal distribution: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/883/51f/1de/88351f1de5c5fe433423c35c7d934c17.png"><br><br>  Just to show real numbers: when log (actual / estimated) = 1, then the inflation coefficient exp (1) = e = 2.72.  It is equally likely that the project will stretch in exp (2) = 7.4 times, and that it will end in exp (-2) = 0.14, that is, in 14% of the estimated time.  Intuitively, the reason why the average is so high is that tasks that are performed faster than expected cannot compensate for tasks that take much longer than expected.  We are limited to 0, but not limited in the other direction. <br><br>  Is it just a model?  Still would!  But soon I will get to the real data and on some empirical data I will show that in fact it corresponds well enough to reality. <br><br><h1>  Estimation of software development terms </h1><br>  So far so good, but let's really try to understand what this means in terms of estimating the timing of software development.  Suppose we look at a plan of 20 different software projects and try to estimate how long it will take to complete <i>them all</i> . <br><br>  This is where the middle becomes decisive.  The averages are added, but there are no medians.  Therefore, if we want to get an idea of ‚Äã‚Äãhow long it will take to complete the sum of N projects, we need to look at the average value.  Suppose we have three different projects with the same œÉ = 1: <br><br><div class="scrollable-table"><table><thead><tr><th></th><th>  Median </th><th>  The average </th><th>  99% </th></tr></thead><tbody><tr><td>  Task A </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Task B </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Task C </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr></tbody><tbody><tr><td>  SUM </td><td>  3.98 </td><td>  4.95 </td><td>  18.85 </td></tr></tbody></table></div><br>  Note that the averages add up and 4.95 = 1.65 * 3, but other columns do not. <br><br>  Now we add three projects with different sigmas: <br><br><div class="scrollable-table"><table><thead><tr><th></th><th>  Median </th><th>  The average </th><th>  99% </th></tr></thead><tbody><tr><td>  Problem A (œÉ = 0.5) </td><td>  1.00 </td><td>  1.13 </td><td>  3.20 </td></tr><tr><td>  Problem B (œÉ = 1) </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Problem C (œÉ = 2) </td><td>  1.00 </td><td>  7.39 </td><td>  104.87 </td></tr></tbody><tbody><tr><td>  SUM </td><td>  4.00 </td><td>  10.18 </td><td>  107.99 </td></tr></tbody></table></div><br>  The averages still add up, but the reality is not even close to the naive estimate of 3 weeks, which you could have imagined.  Note that a highly uncertain project with œÉ = 2 <i>dominates</i> over the others on average completion time.  And for the 99th percentile, it not only dominates, but literally absorbs all the others.  We can give an example more .: <br><br><div class="scrollable-table"><table><thead><tr><th></th><th>  Median </th><th>  The average </th><th>  99% </th></tr></thead><tbody><tr><td>  Problem A (œÉ = 0.5) </td><td>  1.00 </td><td>  1.13 </td><td>  3.20 </td></tr><tr><td>  Problem B (œÉ = 0.5) </td><td>  1.00 </td><td>  1.13 </td><td>  3.20 </td></tr><tr><td>  Problem C (œÉ = 0.5) </td><td>  1.00 </td><td>  1.13 </td><td>  3.20 </td></tr><tr><td>  Problem D (œÉ = 1) </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Problem E (œÉ = 1) </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Problem F (œÉ = 1) </td><td>  1.00 </td><td>  1.65 </td><td>  10.24 </td></tr><tr><td>  Problem G (œÉ = 2) </td><td>  1.00 </td><td>  7.39 </td><td>  104.87 </td></tr></tbody><tbody><tr><td>  SUM </td><td>  9.74 </td><td>  15.71 </td><td>  112.65 </td></tr></tbody></table></div><br>  Again, the only unpleasant task mainly dominates the estimation calculations, at least in 99% of cases.  Even in average time, one crazy project ultimately takes about half the time spent on all tasks, although the median has similar values.  For simplicity, I assumed that all tasks have the same time estimate, but different uncertainties.  Mathematics is saved when you change the timing. <br><br>  It's funny, but I have a feeling for a long time.  Adding ratings rarely works when you have a lot of tasks.  Instead, find out which tasks have the highest uncertainty: these tasks will usually dominate on average the execution time. <br><br>  The diagram shows the mean and 99th percentile as a function of uncertainty (œÉ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/228/bab/611/228bab611a1cf9710cab10d93635cf1e.png"><br><br>  Now math explained my feelings!  I started to take this into account when planning projects.  I really think that adding estimates to the deadlines for completing tasks is very misleading and creates a false picture of how long the project will take entirely, because you have these crazy skewed tasks that ultimately take up all the time. <br><br><h1>  Where is the empirical evidence? </h1><br>  For a long time I kept it in the brain in the section ‚Äúcurious toy models‚Äù, sometimes thinking that this is a neat illustration of the phenomenon of the real world.  But one day, wandering around the network, I came across an interesting set of data on estimating the timing of projects and the actual time they were completed.  Fantasy! <br><br>  Let's make a quick scatter plot of estimated and actual time: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/576/182/116/576182116876075931f0ea4fed9de0ad.png"><br><br>  The median inflation rate for this dataset is 1X, while the average coefficient is 1.81x.  Again, this confirms the conjecture that developers are well aware of the median, but the average is much higher. <br><br>  Let's look at the distribution of the coefficient of inflation (logarithm): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/66c/c12/283/66cc12283d4f964ff2ef0e420e7c31ca.png"><br><br>  As you can see, it is fairly well centered around 0, where the inflation coefficient is exp (0) = 1. <br><br><h1>  Take statistical tools </h1><br>  Now I'm going to fantasize a bit with statistics - feel free to skip this part if you are not interested.  What can we conclude from this empirical distribution?  You can expect that the logarithms of the inflation rate will be distributed according to the normal distribution, but this is not exactly the case.  Note that the œÉ themselves are random and vary for each project. <br><br>  One of the convenient ways to simulate œÉ is that they are selected from the <a href="https://en.wikipedia.org/wiki/Inverse-gamma_distribution">inverse gamma distribution</a> .  If we assume (as before) that the logarithm of inflation coefficients is distributed in accordance with the normal distribution, then the ‚Äúglobal‚Äù distribution of the logarithms of inflation rates ends with <a href="https://en.wikipedia.org/wiki/Student%2527s_t-distribution">the Student‚Äôs distribution</a> . <br><br>  Apply the distribution of student on the previous: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ee/233/134/6ee233134993e53d4c8bb06643188ec6.png"><br><br>  Decently converges, in my opinion!  Student‚Äôs distribution parameters also determine the inverse gamma distribution of œÉ values: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0be/9af/507/0be9af507445a67752dcee4a3d5fe1f1.png"><br><br>  Note that the values ‚Äã‚Äãof œÉ&gt; 4 are very unlikely, but when they occur, they cause an average explosion several thousand times. <br><br><h1>  Why software tasks always take longer than you think </h1><br>  Assuming that this data set is representative of software development (it is doubtful!), We can draw a few more conclusions.  We have parameters for Student‚Äôs distribution, so we can calculate the average time it takes to complete a task without knowing œÉ for this task. <br><br>  While the median inflation rate of this fit is 1x (as before), the 99% inflation rate is 32x, but if you go to the 99.99th percentile, that's a whopping 55 <i>million</i> !  One (free) interpretation is that some tasks are almost impossible in the end.  In fact, these very extreme cases have such a huge impact on the <i>average</i> that the average inflation rate of <i>any task</i> becomes <i>infinite</i> .  This is pretty bad news for those who are trying to meet deadlines! <br><br><h1>  Summary </h1><br>  If my model is correct (great if), then this is what we can find out: <br><br><ul><li>  People appreciate the <i>median</i> task time, but not the average. <br></li><li>  The average time is much longer than the median because the distribution is distorted (lognormal distribution). <br></li><li>  When you add estimates for n problems, things get worse. <br></li><li>  The tasks of greatest uncertainty (rather, the largest size) can often dominate the average time required to complete all tasks. <br></li><li>  The average time to complete a task that we know nothing about is actually <i>infinite</i> . </li></ul><br><h1>  Notes </h1><br><ul><li>  Obviously, the conclusions are based on only one data set, which I found on the Internet.  Other data sets may give different results. <br></li><li>  My model, of course, is also very subjective, like any statistical model. <br></li><li> I would be happy to apply the model to a much larger set of data to see how stable it is. <br></li><li>  I assumed that all tasks are independent.  In fact, they may have a correlation, which will make the analysis much more annoying, but (I think) end up with similar conclusions. <br></li><li>  The sum of lognormally distributed values ‚Äã‚Äãis not another lognormally distributed value.  This is a weakness of this distribution, since you can argue that most tasks are simply the sum of subtasks.  It would be nice if our distribution was <a href="https://en.wikipedia.org/wiki/Stable_distribution">sustainable</a> . <br></li><li>  I deleted small tasks from the histogram (the estimated time is less than or equal to 7 hours), since they distort the analysis and there was a strange surge at exactly 7. <br></li><li>  <a href="https://github.com/erikbern/software-estimation">The code is on Github</a> , as usual. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/449678/">https://habr.com/ru/post/449678/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../449668/index.html">Notes IoT provider. Pitfalls survey of housing and communal services counters</a></li>
<li><a href="../44967/index.html">Future social networking opportunities Parasite Eliminator</a></li>
<li><a href="../449670/index.html">On the muse support: how donations for streamers are arranged</a></li>
<li><a href="../449672/index.html">Noerden Life2 - analog watch with ‚Äúgesture‚Äù control and ‚Äútouch‚Äù glass</a></li>
<li><a href="../449676/index.html">Competent management - myth or reality? Kolesa Group, Evil Martians and Wrike answer</a></li>
<li><a href="../449680/index.html">How 5G changes gaming technology</a></li>
<li><a href="../449686/index.html">We talk about one of the key speakers OFFZONE 2019</a></li>
<li><a href="../449688/index.html">Testing the sendBeacon setting to send data</a></li>
<li><a href="../44969/index.html">Married to a programmer</a></li>
<li><a href="../449690/index.html">Inside the secret lab Ledger</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>