<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pitfalls of A / B testing or why 99% of your split tests are conducted incorrectly?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The ‚Äúhot‚Äù and frequently discussed topic of conversion optimization led to unconditional popularization of A / B testing, as the only objective way to...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pitfalls of A / B testing or why 99% of your split tests are conducted incorrectly?</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/dd7/885/69c/dd788569c84645d581292c8550c63c3c.jpg" alt="image"></div><br>  The ‚Äúhot‚Äù and frequently discussed topic of conversion optimization led to unconditional popularization of A / B testing, as the only objective way to learn the truth about the performance of various technologies / solutions related to the increase in economic efficiency for online business. <br><br>  Behind <a href="http://www.ngdata.com/top-ab-testing-blogs/">this popularity</a> lies the almost complete absence of culture in the organization, conduct, and analysis of the results of experiments.  In <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-ab-tests">Retail Rocket,</a> we have accumulated a great deal of expertise in assessing the cost-effectiveness of personalization systems in e-commerce.  For two years, the ideal process of conducting A / B tests was rebuilt, which we want to share in this article. <a name="habracut"></a><br><br><h2>  Two words about the principles of A / B testing </h2><br>  In theory, everything is incredibly simple: <br><ul><li>  We hypothesize that some kind of change (for example, <a href="http://retailrocket.ru/blog/keys-personalizatsii-glavnoy-stranitsyi-internet-magazina-www-dostavka-ru/">personalization of the main page</a> ) will increase the conversion of the online store. </li><li>  Create an alternative version of the site "B" - a copy of the original version "A" with the changes from which we are waiting for the growth of the effectiveness of the site. </li><li>  We randomly divide all site visitors into two equal groups: we show the original version to one group, the second alternative - to the second one. </li><li>  At the same time we measure the conversion for both versions of the site. </li><li>  We determine the statistically significant winning option. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/files/f93/400/fba/f93400fbacc24a21833d3606b8619908.png" alt="image"></div><br>  The beauty of this approach is that any hypothesis can be tested using numbers.  There is no need to argue or rely on the opinion of pseudo-experts.  We launched the test, measured the result, and proceeded to the next test. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Example in numbers </h2><br>  For example, imagine that we made a change to the site, launched the A / B test and obtained the following data: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b2e/f53/d1f/b2ef53d1f2f24ec296662ad2221eee26.jpg" alt="image"></div><br>  Conversion is not a static value, depending on the number of ‚Äútrials‚Äù and ‚Äúsuccess‚Äù (in the case of an online store, site visits and completed orders, respectively), the conversion is distributed in a certain interval with a calculated probability. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/210/20f/8a9/21020f8a9eba474eac92a30038490788.jpg" alt="image"></div><br>  For the table above, this means that if we list another 1000 users for the site version ‚ÄúA‚Äù under unchanged external conditions, then with a probability of 99% these users will make from 45 to 75 orders (that is, converted into buyers with a ratio from 4.53% to 7.47 %). <br><br>  By itself, this information is not very valuable, however, during the A / B test, we can get 2 intervals of distribution of conversion.  Comparing the intersection of the so-called ‚Äúconfidence intervals‚Äù of conversions received from two user segments interacting with different versions of the site, allows you to decide and argue that one of the tested site variants is statistically significantly superior to the other.  Graphically, this can be represented as: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/913/b74/643/913b74643d5641e7b97c05b47ad712b5.png" alt="image"></div><br><h2>  Why are 99% of your A / B tests conducted incorrectly? </h2><br>  So, the majority of the above described concept of conducting experiments is already familiar, they tell about it at industry events and write articles.  <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-ab-tests">Retail Rocket</a> simultaneously passes 10-20 A / B tests, over the past 3 years we have been faced with a huge amount of nuances that are often ignored. <br><br>  There is a huge risk in this: if the A / B test is conducted with an error, then the business is guaranteed to make the wrong decision and take hidden losses.  Moreover, if you previously performed A / B tests, then most likely they were conducted incorrectly. <br><br>  Why?  Let us examine the most frequent mistakes that we encountered in the process of conducting a variety of post-test analyzes of the results of experiments conducted when implementing <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-ab-tests">Retail Rocket</a> into our customers' online stores. <br><br><h3>  Audience share in the test </h3><br>  Perhaps the most common mistake - when testing starts, it is not checked that the entire audience of the site participates in it.  A fairly frequent example of life (screenshot from Google Analytics): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/530/ec7/825/530ec78250e741008973344c020496d9.png" alt="image"></div><br>  The screenshot shows that in total, a little less than 6% of the audience took part in the test.  It is extremely important that the entire audience of the site belongs to one of the test segments, otherwise it is impossible to assess the impact of the change on the business as a whole. <br><br><h3>  The uniform distribution of the audience between the tested variations </h3><br>  It is not enough to distribute the entire audience of the site into test segments.  It is also important to do this evenly across all cuts.  Consider the example of one of our clients: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b73/6ad/3be/b736ad3be0e048a1b871093697a12f5e.png" alt="image"></div><br>  Before us is a situation in which the audience of the site is divided unevenly between the segments of the test.  In this case, a 50/50 traffic division was set up in the test tool settings.  This picture is a clear sign that the traffic distribution tool does not work as expected. <br><br>  In addition, pay attention to the last column: it is clear that the second segment gets more repeated, and therefore more loyal audience.  Such people will make more orders and distort the test results.  And this is another sign of incorrect testing tool. <br><br>  To exclude such errors a few days after testing starts, always check the uniformity of traffic division across all available slices (at least by city, browser and platform). <br><br><h3>  Filtering staff online store </h3><br>  The next common problem is related to employees of online stores, who, once in one of the test segments, place orders received by telephone.  Thus, employees form additional sales in one segment of the test, while the callers are in all.  Of course, such anomalous behavior will ultimately distort the final results. <br><br>  Call center operators can be identified using the report on networks in Google Analytics: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/42a/8fe/3ce/42a8fe3ce8ec4995828fc4387a9e8421.png" alt="image"></div><br><br><img src="https://habrastorage.org/files/fc2/772/2e7/fc27722e7cb541f19c689946ccc89e37.png" align="right" alt="image">  In the screenshot, an example from our experience: a visitor visited the site 14 times under the name ‚ÄúElectronics Shopping Center on Presnya‚Äù and placed an order 35 times - this is a clear behavior of a store employee who, for some reason, ordered orders through a basket on the site, and not through the store admin panel. <br><br>  In any case, you can always unload orders from Google Analytics and assign them the property ‚Äúdecorated by the operator‚Äù or ‚Äúdecorated not by the operator‚Äù.  Then build a pivot table as in the screenshot, reflecting another situation that we face quite often: if you take the revenues of the RR and Not RR segments (‚Äúsite with Retail Rocket‚Äù and ‚Äúwithout‚Äù, respectively), then ‚Äúsite with Retail Rocket‚Äù brings less money than ‚Äúwithout‚Äù.  But if you highlight the orders issued by the operators of the call-center, it turns out that Retail Rocket gives a revenue increase of 10%. <br><br><h3>  What indicators should pay attention to the final assessment of the results? </h3><br>  Last year, an A / B test was conducted, the results of which were as follows: <br><ul><li>  + 8% for conversion in the segment ‚Äúsite with Retail Rocket‚Äù. </li><li>  The average check practically did not change (+ 0.4% - at the level of error). </li><li>  Growth in revenue + 9% in the segment ‚ÄúWebsite with Retail Rocket‚Äù. </li></ul><br>  After reporting the results, we received a letter from the client: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8f0/64b/798/8f064b798948400e9c4062c92cdeca98.png" alt="image"></div><br>  The manager of the online store insisted that if the average check did not change, then there was no effect from the service.  At the same time, the fact of total increase in revenue due to the recommendation system is completely ignored. <br><br>  So which indicator should you focus on?  Of course, the most important thing for business is money.  If, as part of an A / B test, traffic is divided evenly between the segments of visitors, then the desired indicator for comparison is the revenue for each segment. <br><br>  In life, no tool for randomly dividing traffic gives absolutely equal segments, there is always a difference in a fraction of a percent, so you need to rationalize the revenue by the number of sessions and use the ‚Äúrevenue per visit‚Äù metric. <br><br>  This is recognized in the world of KPI, which we recommend to focus on when conducting A / B tests. <br><br>  It is important to remember that revenue from orders placed on the site and ‚Äúexecuted‚Äù revenue (revenue from actually paid orders) are completely different things. <br><br>  Here is an example of an A / B test, in which the <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-ab-tests">Retail Rocket</a> system was compared with another recommender system: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/155/7d7/8b4/1557d78b43b04c97b79703bdeb622f36.jpg" alt="image"></div><br>  The ‚Äúnon Retail Rocket‚Äù segment wins in all respects.  However, within the next stage of the post-test analysis, the call-center orders, as well as canceled orders were excluded.  Results: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/7fd/191/c6b/7fd191c6b60d4251aca36c03a6a45180.jpg" alt="image"></div><br>  Post-test results analysis - a mandatory item when conducting A / B-testing! <br><br><h3>  Data slices </h3><br>  Working with different data slices is an extremely important component in the post-test analysis. <br>  Here is another test case for Retail Rocket at one of the largest online stores in Russia: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/60c/0f3/ad7/60c0f3ad78114c6fb3348909e0caba19.png" alt="image"></div><br>  At first glance, we got an excellent result - revenue growth + 16.7%.  But if you add an additional slice of data <i>‚ÄúDevice Type‚Äù</i> to the report <i>,</i> you can see the following picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0b5/773/996/0b57739962ab44b186fb2c785fca612b.png" alt="image"></div><br><ol><li>  Desktop traffic revenue growth of almost 72%! </li><li>  On tablets in the Retail Rocket segment drawdown. </li></ol><br>  As it turned out after testing, blocks of Recommendations Retail Rocket were incorrectly displayed on the tablets. <br><br>  It is very important in the post-test analysis to build reports at least in the context of the city, browser and user platform, so as not to miss such problems and maximize the results. <br><br><h3>  Statistical confidence </h3><br>  The next topic that needs to be addressed is statistical confidence.  It is possible to make a decision on the introduction of changes to the site only after the statistical certainty of excellence has been achieved. <br><br>  To calculate the statistical accuracy of the conversion, there are many online tools, for example, <a href="http://htraffic.ru/calc/">htraffic.ru/calc/</a> : <br><div style="text-align:center;"><img src="https://habrastorage.org/files/572/388/0dc/5723880dc01746c8aaca6950a7aaf325.png" alt="image"></div><br>  But conversion is not the only indicator that determines the economic efficiency of the site.  The problem with most A / B tests today is that only the statistical confidence of the conversion is checked, which is insufficient. <br><br><h3>  Average check </h3><br>  The revenue of the online store is based on the conversion (the share of people who buy) and from the average check (purchase size).  The statistical reliability of a change in the average check is more difficult to calculate, but without this in any way, otherwise incorrect conclusions are inevitable. <br><br>  The screenshot shows another example of the Retail Rocket A / B test, in which an order worth more than a million rubles got into one of the segments: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d11/414/d86/d11414d861f046be9de75c514b7388b0.png" alt="image"></div><br>  This order constitutes almost 10% of the total revenue for the test period.  In this case, when achieving statistical confidence in conversion, can the results in revenue be considered reliable?  Of course not. <br><br>  Such huge orders significantly distort the results, we have two approaches to post-test analysis from the point of view of the average check: <br><ol><li>  Complicated.  " <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25BE%25D0%25B2%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">Bayesian statistics</a> ", which we will discuss in the following articles.  In <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-ab-tests">Retail Rocket,</a> we use it to assess the reliability of the average receipt of internal tests for optimizing recommendation algorithms. </li><li>  Plain.  Cutting off several percentiles of orders at the top and bottom (usually 3-5%) of the list, sorted in descending order amounts. </li></ol><br><h3>  Test time </h3><br>  And finally, always pay attention to when you run the test and how long it takes.  Try not to run the test a few days before major gender holidays and on holidays / weekends.  Seasonality is also traced to the level of salaries: as a rule, it stimulates the sale of expensive goods, in particular, electronics. <br><br>  In addition, there is a proven relationship between the average check in a store and the time it takes a decision to purchase.  Simply put, the more expensive the goods - the longer they are chosen.  In the screenshot is an example of a store in which 7% of users think about 1 to 2 weeks before buying: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/72b/691/a72/72b691a724e74e689b6c12b2a5641299.png" alt="image"></div><br>  If an A / B test is conducted for less than a week on such a store, then about 10% of the audience will not get into it and the impact of the change on the website on the business cannot be unambiguously assessed. <br><br><h2>  Instead of output.  How to conduct a perfect A / B test? </h2><br>  So, to eliminate all the problems described above and to conduct a proper A / B test, you need to perform 3 steps: <br><br>  <strong>1. Split traffic 50/50</strong> <br>  <i>Difficult:</i> using a traffic balancer. <br>  <i>Simple:</i> use the open source library <a href="https://github.com/RetailRocket/RetailRocket.Segmentator">Retail Rocket Segmentator</a> , which is supported by the Retail Rocket team.  For several years of testing, we were unable to solve the problems described above in tools like Optimizely or Visual Website Optimizer. <br><br>  <strong>Goal on the first step:</strong> <br><ul><li>  Get a uniform distribution of the audience across all available sections (browsers, cities, traffic sources, etc.). </li><li>  100% of the audience should get into the test. </li></ul><br>  <strong>2. Conduct A / A test</strong> <br>  Without changing anything on the site, transfer to Google Analytics (or another web analytics system that you like) different user segment identifiers (in the case of Google Analytics - Custom var / Custom dimension). <br><br>  <strong>The goal in the second step:</strong> not to get a winner, i.e.  in two segments with the same versions of the site there should be no difference in key indicators. <br><br>  <strong>3. Conduct post-test analysis</strong> <br><ul><li>  Exclude company employees. </li><li>  Cut out extreme values. </li><li>  Check the significance of the conversion value, use performance data and cancellation of orders, i.e.  take into account all the cases mentioned above. </li></ul><br>  <strong>The goal in the last step:</strong> make the right decision. <br><br>  Share your case studies of A / B tests in the comments! </div><p>Source: <a href="https://habr.com/ru/post/261593/">https://habr.com/ru/post/261593/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../261579/index.html">Yandex.Money does not require the cardholder‚Äôs full name when paying</a></li>
<li><a href="../261581/index.html">IBM Science and Technology Center announces additional recruitment for the internship program for 2015/2016</a></li>
<li><a href="../261583/index.html">Question to the community: a shared private message box for HB, GT and MM, how are you?</a></li>
<li><a href="../261589/index.html">Analysis of the tone of statements on Twitter: implementation with an example of R</a></li>
<li><a href="../261591/index.html">LVEE 2015 Conference. Results</a></li>
<li><a href="../261595/index.html">Competently select and test the storage of their backups</a></li>
<li><a href="../261597/index.html">Video from the next meeting of the PUG: Phalcon and Zephir</a></li>
<li><a href="../261599/index.html">Modulo 2</a></li>
<li><a href="../261603/index.html">Writing Custom MSBuild Task for Deploy (WMI included)</a></li>
<li><a href="../261605/index.html">Meet TurboBytes Pulse - a service for finding problems in a CDN</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>