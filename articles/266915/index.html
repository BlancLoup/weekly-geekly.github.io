<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Technical website audit with Screaming Frog SEO Spider</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="For most people, a general site audit is a rather complicated and time-consuming task, but with tools such as Screaming Frog SEO Spider (SEO Spider) t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">ğŸ”</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">ğŸ“œ</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">â¬†ï¸</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">â¬‡ï¸</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Technical website audit with Screaming Frog SEO Spider</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/700/a5b/b91/700a5bb9114f9f05813a00f8cd1f7dfa.png" alt="image"><br><br>  For most people, a general site audit is a rather complicated and time-consuming task, but with tools such as Screaming Frog SEO Spider (SEO Spider) the task can become much simpler for both professionals and beginners.  Convenient interface Screaming Frog provides an easy and fast operation, but the variety of configuration options and functionality can make it difficult to get acquainted with the program, the first steps in communicating with it. <br><br>  The following instructions are intended to demonstrate the various ways that Screaming Frog can be used primarily for auditing sites, but also for other tasks. <br><a name="habracut"></a><br><h2>  <strong>Basic principles of site scanning</strong> </h2><br><ul><li><h3>  How to scan the entire site. </h3><br></li></ul><br>  By default, Screaming Frog scans only the subdomain you are visiting.  Any additional subdomain Spider encounters is considered an external link.  In order to scan additional subdomains, you need to make adjustments in the Spider configuration menu.  By selecting the "Crawl All Subdomains" option, you can be sure that Spider will analyze any links that are found on the subdomains of your site. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/afa/1a6/0f6/afa1a60f63c01f3ff32855c24ea5d791.jpg" alt="image"><br><br>  To speed up scanning do not use images, CSS, JavaScript, SWF or external links. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c49/d22/66c/c49d2266cc1f3ebb58f2fb5dc61aa727.jpg" alt="image"><br><br><ul><li><h3>  How to scan a single directory. </h3><br></li></ul><br>  If you want to limit scanning to a specific folder, simply enter the URL and click on the â€œstartâ€ button without changing the default settings.  If you made changes to the preset settings, you can reset them using the File menu. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6a7/636/ffa/6a7636ffae0b847206c6922716acd880.jpg" alt="image"><br><br>  If you want to start scanning from a specific folder, and then go on to analyze the remaining part of the subdomain, then before you start working with the URL you need, first go to the Spider section called â€œConfigurationâ€ and select the option â€œCrawl Outside Of Start Folder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d29/ac5/536/d29ac5536548078fbcbb0a7729ababf6.jpg" alt="image"><br><br><ul><li><h3>  How to scan a set of defined subdomains or subdirectories. </h3><br></li></ul><br>  To take into account a specific list of subdomains or sub-pathologists, you can use RegEx to set the rules for inclusion (Include settings) or exclusions (Settings) for certain items in the "Configuration" menu. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/35c/4a6/610/35c4a66102278dfe75702efce96ed8a1.jpg" alt="image"><br><br>  In the example below, all pages of havaianas.com were selected for scanning, except for the â€œAboutâ€ pages in each individual subdomain (exception).  The following example shows how to scan the English-language pages of subdomains of the same site (inclusion). <br><br><ul><li><h3>  If you want to scan the list of all pages of my site. </h3><br></li></ul><br>  By default, Screaming Frog scans all JavaScript images, CSS, and flash files that Spider encounters.  To analyze only HTML, you need to uncheck the options â€œCheck Imagesâ€, â€œCheck CSSâ€, â€œCheck JavaScriptâ€ and â€œCheck SWFâ€ in the menu â€œConfigurationâ€ Spider.  Spider will be launched without taking into account the specified positions, which will allow you to get a list of all pages on the site that are internally linked.  After the scan is completed, go to the â€œInternalâ€ tab and filter the results using the HTML standard.  Click on the â€œExportâ€ button to get a complete list in CSV format. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0c3/a58/27b/0c3a5827b74e87c9603b5e5d5db4e5fb.jpg" alt="image"><br><br>  Tip: If you intend to use the specified settings for subsequent scans, then Screaming Frog will give you the opportunity to save the specified options. <br><br><ul><li><h3>  If you want to scan the list of all pages in a specific subdirectory. </h3><br></li></ul><br>  In addition to â€œCheck Imagesâ€, â€œCheck CSSâ€, â€œCheck JavaScriptâ€ and â€œCheck SWFâ€ in the â€œConfigurationâ€ Spider menu you will need to select â€œCheck Links Outside Folderâ€.  That is, you exclude these options from Spider, which will provide you with a list of all pages of the selected folder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5dd/932/adc/5dd932adc176841d7772519b86bb2afb.jpg" alt="image"><br><br><ul><li><h3>  If you want to scan the list of domains that your client has just redirected to your commercial site. </h3><br></li></ul><br>  In ReverseInter.net, add the URL of this site, then click the link in the upper table to find sites that use the same IP address, DNS server, or GA code. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/083/b88/6d1/083b886d19fbcd0d3e30b6bfc3c9b2e2.jpg" alt="image"><br><br>  Further using the Google Chrome extension called Scraper, you can find a list of all links with an anchor to â€œvisit the site.â€  If Scraper is already installed, then you can launch it by clicking the mouse anywhere on the page and selecting the â€œScrape similarâ€ item.  In the pop-up window, you will need to change the XPath request to the following: <br>  <em>// a [text () = 'visit site'] / @ href.</em> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c8a/c92/163/c8ac92163f818aecdf07282b8cf2dde1.jpg" alt="image"><br><br>  Then click â€œScrapeâ€ and after â€œExport to Google Docsâ€.  From the Word document, you can later save the list as a .csv file. <br><br>  Further this list you can load in Spider and start scanning.  When the Spider finishes working, you will see the corresponding status in the â€œInternalâ€ tab.  Or, you can go to â€œResponse Codesâ€ and use the â€œRedirectionâ€ position to filter the results to see all domains that have been redirected to a commercial site or anywhere else. <br><br>  Please note that when uploading .csv files to Screaming Frog, you must select the appropriate CSV format type, otherwise the program will crash. <br><br>  Tip: You can also use this method to identify domains that refer to competitors and identify how they were used. <br><br><ul><li><h3>  How to find all the subdomains of the site and check the internal links. </h3><br></li></ul><br><br>  Fill in ReverseInternet with the root URL of the domain, then click on the â€œSubdomainsâ€ tab to see the list of subdomains. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d1/c22/efa/1d1c22efa61a927703d1b4207a90be7c.jpg" alt="image"><br><br>  After that, enable Scrape Similar to build a list of URLs using an XPath query: <br><br>  <em>// a [text () = 'visit site'] / @ href.</em> <br><br>  Export the results in .csv format, then upload the CSV file to Screaming Frog using the â€œListâ€ mode.  When the Spider is finished, you can view status codes, as well as any links on subdomain pages, anchor entries, and even duplicate page headers. <br><br><ul><li><h3>  How to scan a commercial or any other large site. </h3><br></li></ul><br>  Screaming Frog is not designed to crawl hundreds of thousands of pages, but there are several measures to prevent program crashes when scanning large sites.  First, you can increase the amount of memory used by the Spider.  Secondly, you can disable the scanning of a subdirectory and work only with certain fragments of the site, using the inclusion and exclusion tools.  Thirdly, you can turn off the scanning of images, javascript, css and flash files by focusing on html.  This saves memory resources. <br><br>  Tip: If earlier when scanning large sites you had to wait a very long time for the operation to complete, Screaming Frog allows you to pause the procedure for using large amounts of memory.  This valuable option allows you to save the results already obtained until the moment when the program is supposedly ready to fail, and to increase the memory size. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9ee/1c1/3ad/9ee1c13ad64ee09e7e2d77b5e6f2b4f8.jpg" alt="image"><br><br>  At the moment, this option is enabled by default, but if you plan to scan a large site, it is better to make sure that in the Spider configuration menu, in the Advanced tab, there is a tick in the Pause On High Memory Usage field. <br><br><ul><li><h3>  How to scan a site hosted on an old server. </h3><br></li></ul><br>  In some cases, older servers may not be able to process a given number of URL requests per second.  To change the scan speed in the â€œConfigurationâ€ menu, open the â€œSpeedâ€ section and in the pop-up window, select the maximum number of threads that should be enabled at the same time.  In this menu you can also select the maximum number of URLs requested per second. <br><br>  Tip: If you find a large number of server errors in the scan results, go to the Advanced tab in the Spider configuration menu and increase the Response Timeout value and the number of new request attempts (5xx Response Retries).  This will allow you to get the best results. <br><br><ul><li><h3>  How to scan a site that requires cookies. </h3><br></li></ul><br>  Although search robots do not accept cookies, if you need to allow cookies while scanning the site, simply select â€œAllow Cookiesâ€ in the â€œAdvancedâ€ tab of the â€œConfigurationâ€ menu. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bb8/47b/e73/bb847be73786c2c12c76fb5db2d9b89d.jpg" alt="image"><br><br><ul><li><h3>  How to scan a site using a proxy or other user agent. </h3><br></li></ul><br>  In the configuration menu, select "Proxy" and enter the appropriate information.  To scan using another agent, select â€œUser Agentâ€ in the configuration menu, then select a search bot from the drop-down menu or enter its name. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/859/ab3/c5a/859ab3c5a1c244d66cd8110136a3f5bc.jpg" alt="image"><br><br><ul><li><h3>  How to scan sites that require authorization. </h3><br></li></ul><br>  When the Screaming Frog Spider enters the page requesting identification, a window pops up in which you need to enter your login and password. <br><br>  In order to continue without this procedure, in the configuration menu, in the Advanced tab, uncheck the â€œRequest Authenticationâ€ option. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/09c/596/c31/09c596c313a7d5e327db8c966715bfa8.jpg" alt="image"><br><br><h2>  <strong>Internal links</strong> </h2><br><ul><li><h3>  What to do when you need to get information about external and internal links of the site (anchors, directives, relink, etc.). </h3><br></li></ul><br>  If you do not need to check the site image, JavaScript, Flash or CSS, then exclude these options from the scan mode to save memory resources. <br><br>  After the Spider scan completes, use the Advanced Export menu to export CSV from the All Links database.  This will provide you with all the reference locations and their corresponding anchor entries, directives, etc. <br><br>  To quickly count the number of links on each page, go to the â€œInternalâ€ tab and sort the information through the â€œOutlinksâ€ option.  Everything to be above the 100th position may require additional attention. <br><br><ul><li><h3>  How to find broken internal links to a page or site. </h3><br></li></ul><br>  As always, do not forget to exclude images, JavaScript, Flash or CSS from scanned objects in order to optimize the process. <br><br>  After the end of Spider scanning, filter the results of the Internal panel using the Status Code function.  Every 404th, 301st and other status codes will be well viewed. <br><br>  When you click on each individual URL in the scan results in the bottom of the program window you will see the information.  By clicking on â€œIn Linksâ€ in the bottom window, you will find a list of pages that link to the selected URL, as well as anchor entries and directives used on these pages.  Use this feature to identify internal links that require updating. <br><br>  To export in CSV format a list of pages containing broken links or redirections, use the â€œAdvanced Exportâ€ menu option â€œRedirection (3xx) In Linksâ€ or â€œClient Error (4xx) In Linksâ€ or â€œServer Error (5xx) In Links ". <br><br><ul><li><h3>  How to identify broken outgoing links on a page or site (or all broken links at the same time). </h3><br></li></ul><br>  Similarly, we first focus on scanning HTML-content, while not forgetting to leave a tick in the "Check External Links". <br><br>  When the scan is complete, select the â€œExternalâ€ tab in the top window and use the â€œStatus Codeâ€ to filter the content to identify URLs with status codes other than 200. Click on any particular URL in the scan results and then select the â€œIn Linksâ€ tab in the bottom window - you will find a list of pages that point to the selected URL.  Use this information to identify links that require updating. <br><br>  To export a complete list of outbound links, click on â€œExportâ€ in the â€œInternalâ€ tab.  You can also set a filter to export links to external images, javascript, css, flash and pdf.  To restrict exporting to pages only, sort through the â€œHTMLâ€ option. <br><br>  To get a complete list of all the locations and anchor entries of outgoing links, select the â€œAll Out Linksâ€ item in the â€œAdvanced Exportâ€ menu and then filter the â€œDestinationâ€ column in the exported CSV to exclude your domain. <br><br><ul><li><h3>  How to find redirect links. </h3><br></li></ul><br>  After the scan is completed, select the "Response Codes" panel in the top window and then sort the results using the "Redirection (3xx)" option.  This will get a list of all internal and outgoing links that will redirect.  Applying the filter "Status Code", you can split the results by type.  When you click "In Links" in the bottom window, you can see all the pages that use the redirect links. <br><br>  If you export information directly from this tab, you will see only the data that is displayed in the upper window (the original URL, the status code and the place where the redirection takes place). <br><br>  To export a complete list of pages containing redirect links, you should select "Redirection (3xx) In Links" in the "Advanced Export" menu.  This will return a CSV file that includes the location of all the redirect links.  To show only internal redirects, filter the contents in a CSV file with data about your domain using the â€œDestinationâ€ column. <br><br>  Tip: On top of two exported files, use VLOOKUP to match the â€œSourceâ€ and â€œDestinationâ€ columns with the location of the final URL. <br><br>  An example of a formula is as follows: <br>  = VLOOKUP ([@ Destination], 'response_codes_redirection_ (3xx) .csv'! $ A $ 3: $ F $ 50.6, FALSE).  Where â€œresponse_codes_redirection_ (3xx) .csvâ€ is a CSV file containing redirect URLs and â€œ50â€ is the number of lines in this file. <br><br><h2>  <strong>Site content</strong> </h2><br><ul><li><h3>  How to identify pages with non-informative content (the so-called "thin content" - "current content"). </h3><br></li></ul><br>  After Spider completes its work, go to the â€œInternalâ€ panel, set filtering by HTML, and then scroll to the right to the â€œWord Countâ€ column.  Sort the content of the pages according to the word count principle to identify those with the least amount of text.  You can drag the â€œWord Countâ€ column to the left, placing it next to the corresponding URLs, making the information more revealing.  Click on the â€œExportâ€ button in the â€œInternalâ€ tab if you prefer to work with data in CSV format. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f21/379/be5/f21379be594affde117b0178c4e7a577.jpg" alt="image"><br><br>  Remember that Word Count allows you to estimate the volume of the placed text, but it does not really give any information about whether this text is just the names of goods / services or a block optimized for keywords. <br><br><ul><li><h3>  If you want to select a list of links to images from specific pages. </h3><br></li></ul><br>  If you have already scanned the entire site or a separate folder, simply select the page in the upper window, then click â€œImage Infoâ€ in the lower window to view the images that were found on this page.  Images will be listed in the â€œToâ€ column. <br>  Tip: Right-click on any entry in the bottom window to copy or open the URL. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/322/862/510/32286251025bbc2a508728a5fa103dea.jpg" alt="image"><br><br>  You can view images on a single page by scanning this particular URL.  Make sure that the depth of the scan in the Spider's scan configuration settings is "1".  After the page is scanned, go to the Images tab, and you will see all the images that Spider was able to detect. <br><br>  Finally, if you prefer CSV, use the â€œAdvanced Exportâ€ menu, the â€œAll Image Alt Textâ€ option to see a list of all the images, their location and any associated replacement text. <br><br><ul><li><h3>  How to find images that have no replacement text or images that have a long Alt text. </h3><br></li></ul><br>  First of all, you need to make sure that the â€œCheck Imagesâ€ item is selected in the â€œConfigurationâ€ menu of the Spider.  When the scan is complete, go to the Images tab and filter the content using the Missing Alt Text or Alt Text Over 100 Characters options.  By clicking on the â€œImage Infoâ€ tab in the bottom window, you will find all the pages on which at least some images are placed.  Pages will be listed in the "From" column. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/266/a15/37c/266a1537c1a73cde800177f72d48a319.jpg" alt="image"><br><br>  However, in the Advanced Export menu, you can save time and export All Image Alt Text (All images, or Images Missing Alt Text) to CSV format. <br><br><ul><li><h3>  How to find on the site every CSS file. </h3><br></li></ul><br>  In the Spider configuration menu before scanning, select â€œCheck CSSâ€.  At the end of the process, filter the analysis results in the Internal panel using the CSS option. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ac/874/c44/6ac874c44f30e91faced54b45567fb01.jpg" alt="image"><br><br><ul><li><h3>  How to find javascript files. </h3><br></li></ul><br>  In the Spider configuration menu before scanning, select â€œCheck JavaScriptâ€.  At the end of the process, filter the analysis results in the Internal panel using the JavaScript option. <br><br><ul><li><h3>  How to identify all jQuery plugins used on the site and their location. </h3><br></li></ul><br>  First of all, make sure that â€œCheck JavaScriptâ€ is selected in the configuration menu.  After the scan is completed, apply the â€œJavaScriptâ€ filter in the â€œInternalâ€ panel, and then search for â€œjQueryâ€.  This will allow you to get a list of files with plugins.  Sort the list by the â€œAddressâ€ option for more convenient viewing.  Then browse the â€œInLinksâ€ in the bottom window or export the information to CSV.  To find pages that use files, work with the â€œFromâ€ column. <br><br>  At the same time, you can use the â€œAdvanced Exportâ€ menu to export â€œAll Linksâ€ to CSV and filter the â€œDestinationâ€ column to view only URLs from â€œjqueryâ€. <br><br>  Tip: Not all jQuery plugins are bad for SEO.  If you see a site that uses jQuery, then it will be reasonable to make sure that the content you intend to index is included in the source code of the page and is displayed when the page loads, and not after that.  If you are not sure about this aspect, then read about the plugin on the Internet to learn more about how it works. <br><br><br><ul><li><h3>  How to determine where the site is located flash. </h3><br></li></ul><br><br>  Before scanning, select â€œCheck SWFâ€ in the configuration menu.  And when Spider completes its work, filter the results in the â€œInternalâ€ panel by the value â€œFlashâ€. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/598/80f/803/59880f803f83597c0749c026b6ece68e.jpg" alt="image"><br><br><br>  Remember that this method only allows you to find .SWF files located on the page.  If the plugin is pulled out via javascript, you will have to use a custom filter. <br><br><br><ul><li><h3>  How to find on the site internal PDF-documents. </h3><br></li></ul><br><br>  After scanning is completed, filter the Spider results using the â€œPDFâ€ option in the â€œInternalâ€ panel. <br><br><br><ul><li><h3>  How to identify content segmentation within a site or group of pages. </h3><br></li></ul><br><br>  If you want to find pages on the site that contain unusual content, install a custom filter that detects HTML print not relevant to this page.  This should be done before the launch of the Spider. <br><br><br><ul><li><h3>  How to find pages that have social sharing buttons. </h3><br></li></ul><br><br>  To do this, before running Spider, you will need to install a custom filter.  To install it, go to the configuration menu and click â€œCustomâ€.  After that enter any code fragment from the source code of the page. <br><br>  In the given example, the task was to find pages containing the â€œLikeâ€ button of the social network Facebook, respectively, a filter of the format â€œhttp://www.facebook.com/plugins/like.phpâ€ was created for them. <br><br><br><ul><li><h3>  How to find pages using iframe. </h3><br></li></ul><br><br>  To do this, you must set the &lt;iframe tag to the corresponding custom filter. <br><br><br><ul><li><h3>  How to find pages containing embedded video or audio content. </h3><br></li></ul><br><br>  To do this, set a custom filter for the embed code snippet under Youtube or any other media player used on the site. <br><br><br><h2>  <strong>Meta data and directives</strong> </h2><br><br><ul><li><h3>  How to find pages with long, short or missing headings, meta description or meta keywords </h3><br></li></ul><br><br>  When the scan is complete, go to the â€œPage Titlesâ€ tab and filter the content through â€œOver 70 Charactersâ€ to see excessively long page headers.  The same can be done in the â€œMeta Descriptionâ€ and â€œURLâ€ panels.  Exactly the same algorithm can be used to define pages with missing or short headers and meta data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f8a/c22/67b/f8ac2267b657ec2690a2ff4cf8c50b4f.jpg" alt="image"><br><br><br><br><ul><li><h3>  How to find pages with duplicate headings, meta description or meta keywords </h3><br></li></ul><br><br>  When the scan is complete, go to the Page Titles tab and filter the contents through Duplicate to see duplicate page titles.  The same can be done in the â€œMeta Descriptionâ€ and â€œURLâ€ panels. <br><br><br><ul><li><h3>  How to find duplicate content and / or URLs that should be redirected / rewritten / canonized. </h3><br></li></ul><br><br>  When Spider completes its work, go to the URL tab and filter the results using Underscores, Uppercase or Non ASCII Characters, revealing URLs that could potentially be rewritten into a more standard structure.  Filter through the â€œDuplicateâ€ tool to see pages that have multiple URL versions.  Use the â€œDynamicâ€ filter to recognize URLs that include parameters. <br><br>  In addition, if you go to the â€œInternalâ€ panel through the â€œHTMLâ€ filter and scroll further to the right to the â€œHashâ€ column, you will see a unique sequence of letters and numbers on each page.  If you click â€œExportâ€, you can use conditional formatting in Excel to highlight duplicate values â€‹â€‹in this column, eventually showing that the pages are identical and should be considered. <br><br><br><ul><li><h3>  How to identify pages that contain meta-directives. </h3><br></li></ul><br><br>  After scanning, go to the "Directives" panel.  To see the type of directive, just scroll to the right and see which columns are filled.  Alternatively, use a filter to find any of the following tags: <br><br><ol><li>  Index; </li><li>  Noindex; </li><li>  Follow; </li><li>  Nofollow; </li><li>  Noarchive; </li><li>  Nosnippet; </li><li>  Noodp; </li><li>  Noydir; </li><li>  Noimageindex; </li><li>  Notranslate; </li><li>  Unavailable_after; </li><li>  Refresh; </li><li>  Canonical. </li></ol><br><br><br><ul><li><h3>  How to determine that the robots.txt file works as expected. </h3><br></li></ul><br><br>  By default, Screaming Frog will correspond to robots.txt.  As a priority, the program will follow directives made specifically for the user agent.  If there are none, then the Spider will follow any directives for the Google bot.  If there are no special directives for Googlebot, Spider will follow global directives adopted for all user agents.  In this case, Spider will select only one set of directives, without affecting all subsequent ones. <br><br>  If you need to block some parts of the site from Spider, then use the syntax of the usual robots.txt with the user agent Screaming Frog SEO Spider for this purpose.  If you want to ignore robots.txt, then simply select the appropriate option in the program's configuration menu. <br><br><br><ul><li><h3>  How to find and check Schema or other microdata. </h3><br></li></ul><br><br>  To find each page containing Schema markup or other microdata, you need to use custom filters.  In the â€œConfigurationâ€ menu, click on â€œCustomâ€ and enter the marker you are looking for. <br><br>  To find each page that contains Schema markup, simply add the following code snippet to the custom filter: itemtype = http: //schema.org. <br><br>  To find a specific type of markup you will have to be more specific.  For example, using the custom filter â€¹span itemprop =â€ ratingValue â€â€º allows you to find all the pages that contain the Schema markup for building ratings. <br><br>  You can use 5 different filters for scanning.  After that, you just have to click "OK" and enlighten the site or the list of pages with a software scanner. <br><br>  When the Spider is finished, select the â€œCustomâ€ tab in the upper window to see all the pages with the marker you are looking for.  If you specify more than one custom filter, you can view them alternately, switching between the filter pages in the scan results. <br><br><br><br><h2>  <strong>Site Map</strong> </h2><br><br><ul><li><h3>  How to create a sitemap (sitemap) in XML. </h3><br></li></ul><br><br>  After the spider has finished scanning your site, click on â€œAdvanced Exportâ€ and select â€œXML Sitemapâ€. <br><br>  Save your sitemap, and then open it in Excel.  Select "Read Only" and open the file "As an XML table".  In this case, a message may come out that some schemes cannot be integrated.  Just click on the â€œYesâ€ button. <br><br>  After the site map appears in a tabular form, you can easily change the frequency, priority and other settings.  Be sure to ensure that the Sitemap contains only one preferred (canonical) version of each URL, with no parameters and other duplicate factors. <br><br>  After making any changes, save the file in XML mode. <br><strong>&nbsp;</strong><br><br><ul><li><h3>  How to find out your existing XML Sitemap file. </h3><br></li></ul><br><br>  First of all, you will need to create a copy of the Sitemap on your PC.  You can save any live sitemap by going to the URL and saving the file or importing it into Excel. <br><br>  After that, go to the Screaming Frog menu called â€œModeâ€ and select â€œListâ€.  After at the top of the page, click on â€œSelect Fileâ€, select your file and start scanning.  When Spider completes its work, you can see any redirections, 404 errors, duplicate URLs, etc. in the â€œInternalâ€ tab, in the â€œSitemap dirtâ€ section. <br><br><br><br><h2>  <strong>General troubleshooting tips</strong> </h2><br><br><ul><li><h3>  How to determine why some sections of my site are not indexed or ranked. </h3><br></li></ul><br><br>  I wonder why some pages are not indexed?  First, make sure that they do not fall into robots.txt and are not marked as noindex.  Secondly, you need to make sure that the spiders can get to the pages of the site to check the internal links.  After the spider scans your site, export the list of internal links as a CSV file using the HTML filter in the Internal tab. <br><br>  Open a CSV document and in the second sheet copy the list of URLs that are not indexed or ranked.  Use VLOOKUP to see if similar problem URLs are in the scan results. <br><br><br><ul><li><h3>  How to check if the site has been successfully redesigned / redesigned. </h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can use the screaming frog to find out if the old URLs were redirected. </font><font style="vertical-align: inherit;">The â€œListâ€ mode will help in this, through which you can check the status codes. </font><font style="vertical-align: inherit;">If the old URLs give a 404 error, then you will know exactly which of them require redirection.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find slow loading site pages. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> After the scanning process is completed, go to the tab â€œResponse Codesâ€ and sort the column â€œResponse Timeâ€ on a large-to-small basis to find pages that may suffer from a slow download speed. </font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find malware or spam on the site. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First of all, you need to identify traces left by malware or spam. </font><font style="vertical-align: inherit;">Next in the configuration menu, click on â€œCustomâ€ and enter the name of the marker you are looking for. </font><font style="vertical-align: inherit;">For one scan, you can analyze up to 5 such markers. </font><font style="vertical-align: inherit;">Fill in all the necessary and then click on "OK" to explore the entire site or a list of pages on it. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When the process is complete, go to the â€œCustomâ€ tab, located in the top window, to view all the pages on which the â€œtracesâ€ of the fraudulent and virus programs that you specified were found. </font><font style="vertical-align: inherit;">If you specify more than one custom filter, then the results for each will be displayed in a separate window, and you will be able to familiarize yourself with them by moving from one filter to another.</font></font><br><br><br><br><h2> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PPC and analytics</font></font></strong> </h2><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to simultaneously check the list of all URLs used for contextual advertising. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Save the address list in .txt or .csv format and change the mode settings from â€œModeâ€ to â€œListâ€. </font><font style="vertical-align: inherit;">After select your file to download and click on "Start". </font><font style="vertical-align: inherit;">View in the â€œInternalâ€ tab the status code for each page.</font></font><br><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Scraping </font></font></h2><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to collect meta data from a series of pages. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do you have a bunch of URLs where itâ€™s important to get as much information as possible? </font><font style="vertical-align: inherit;">Turn on â€œModeâ€ mode, then load the list of addresses in .txt or .csv format. </font><font style="vertical-align: inherit;">After Spider completes the work, you can see the status codes, outgoing links, the number of words and, of course, the meta data for each page in your list.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to make a scraping site for all pages containing a specific marker. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First of all, you will need to deal with the marker itself - to determine what exactly you need. After that, in the â€œConfigurationâ€ menu, click on â€œCustomâ€ and enter the name of the marker you are looking for. Remember that you can enter up to 5 different markers. Then click on â€œOKâ€ to start the scanning process and filter the site pages by the presence of specified â€œtracesâ€ on them. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6d/48c/02c/b6d48c02c7d9fbe60e64dbf1e5e41c14.jpg" alt="image"><br><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The example shows the situation when you need to find all the pages that contain the words â€œPlease Callâ€ in the sections concerning the cost of goods. For this, the HTML code from the source code of the page was found and copied.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After scanning, you need to go to the â€œCustomâ€ section in the top window to view a list of all pages containing the specified marker. </font><font style="vertical-align: inherit;">If more than one marker was entered, the information on each of them will be submitted in a separate window. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tip: This method is good if you do not have direct access to the site. </font><font style="vertical-align: inherit;">If you need to obtain data from the client's site, then it will be much easier and faster to ask him to take the necessary information directly from the database and transfer it to you.</font></font><br><br><br><h2> <strong><font style="vertical-align: inherit;"></font></strong> <font style="vertical-align: inherit;"><strong><font style="vertical-align: inherit;">URL </font></strong><strong><font style="vertical-align: inherit;">rewriting</font></strong></font><strong><font style="vertical-align: inherit;"></font></strong> </h2><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find and delete session IDs or other parameters from crawled URLs. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To identify URLs with ID sessions or other parameters, simply scan the site with the default settings. </font><font style="vertical-align: inherit;">When Spider completes its work, go to the â€œURLâ€ tab and apply the â€œDynamicâ€ filter to see all URLs containing the required parameters. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To remove the parameters from the display on the scanned pages, select â€œURL Rewritingâ€ in the configuration menu. </font><font style="vertical-align: inherit;">Then in the â€œRemove Parametersâ€ panel, click â€œAddâ€ to add the parameters that you want to remove from the URL and click â€œOKâ€. </font><font style="vertical-align: inherit;">To activate the changes you need to run Spider again.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to rewrite scanned URLs (for example, change .com to .co.uk or write down all URLs in lower case). </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To rewrite any of the addresses worked by the spider, select â€œURL Rewritingâ€ in the configuration menu, then in the â€œRegex Replaceâ€ panel, click on â€œAddâ€ and add RegEx to what you need to replace. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After you have made all the required corrections, you can check them in the â€œTestâ€ panel by entering test URLs in the â€œURL before rewritingâ€ window. </font><font style="vertical-align: inherit;">The string â€œURL after rewritingâ€ will be updated automatically, following the parameters you specified. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you need to rewrite all URLs in lower case, then simply select â€œLowercase discovered URLsâ€ in the Options panel. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not forget to re-launch Spider after making changes, so that they enter into their rights.</font></font><br><br><br><br><h2> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Keyword Analysis</font></font></strong> </h2><br><h2></h2><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find out which pages of competitor sites have the most value. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, competitors will try to expand link popularity and drive traffic to their most valuable pages by linking them internally. Any SEO competitor who pays attention will also build a strong link between the corporate blog and the most important pages of the site. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Find the most significant pages of a competitor's site by scanning, and then go to the â€œInternalâ€ panel and sort the results in the â€œInlinksâ€ column according to the principle â€œfrom large to smallâ€ to see which pages have the most internal links.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To view pages related to a competitorâ€™s corporate blog, uncheck the â€œCheck links outside folderâ€ checkbox in the Spider configuration menu and scan the blog folder / subdomain. </font><font style="vertical-align: inherit;">Then in the â€œExternalâ€ panel, filter the results using the search by the URL of the main domain. </font><font style="vertical-align: inherit;">Scroll to the end of the page to the right and sort the list in the "Inlinks" column to see the pages that are linked most often. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tip: For the convenience of working with the program table, move the columns left and right using the Drag and Drop method.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find out what anchor competitors use for internal linking. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the Advanced Export menu, select All Anchor Text to export CSV containing anchor site entries and find out their location and bindings. </font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find out which meta keywords competitors use on their site. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After the spider has finished scanning, look in the â€œMeta Keywordsâ€ panel to view the list of meta keywords found on each individual page. </font><font style="vertical-align: inherit;">Sort the â€œMeta Keyword 1â€ column alphabetically to make the information more indicative.</font></font><br><br><br><h2> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reference build</font></font></strong> </h2><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to analyze potential link locations. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After collecting a list of URLs, you can scan them in â€œListâ€ mode to collect as much information about the pages as possible. After the scan is complete, check the status codes in the Response Codes panel and examine the outgoing links, link types, anchor entries, and directives in the Out Links panel. This will give you an idea of â€‹â€‹which sites link to these pages and how. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To view the Out Links panel, make sure that the URL of interest is selected in the upper window. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You will probably want to use custom filters to determine if there are already links in these places.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can also export a full list of links by clicking on the â€œAll Out Linksâ€ option in the Advanced Export Menu panel. </font><font style="vertical-align: inherit;">This will allow you to get not only links leading to third-party sites, but also to show internal links to individual pages of your list.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to find broken links for external advertising. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, there is a site from which you would like to receive links to your own resource. </font><font style="vertical-align: inherit;">Using Screaming Frog, you can find broken links to the siteâ€™s pages (or to the entire site) and then, contacting the owner of the resource you like, suggest that he replace broken links with links to your resource, where possible.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to check backlinks and view anchors. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Upload your list of backlinks and launch Spider in â€œListâ€ mode. </font><font style="vertical-align: inherit;">After that, export the full list of external links by clicking on â€œAll Out Linksâ€ in the â€œAdvanced Export Menuâ€ menu. </font><font style="vertical-align: inherit;">This will provide you with the URL and anchor text / alt text for all links on these pages. </font><font style="vertical-align: inherit;">After that, you can filter the â€œDestinationâ€ column in the CSV file to determine if your site is relinked and which anchor text / Alt text it includes.</font></font><br><br><br><ul><li><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> How to make sure that backlinks have been successfully removed. </font></font></h3><br></li></ul><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To do this, you need to set a custom filter that contains the root domain of the URL, then load your list of backlinks and run Spider in â€œListâ€ mode. </font><font style="vertical-align: inherit;">After the scan is complete, go to the â€œCustomâ€ panel to view a list of pages that continue to link to you. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Tip: Remember that by right-clicking on any URL in the top scan results window, you can, in particular:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Copy or open URL. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Run rescan address or remove it from the list. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Export information about the URL or image available on this page, inbound and outbound links. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Check the indexing of the page in Google, Bing and Yahoo. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Check backlinks pages in Majestic, OSE, Ahrefs and Blekko. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> View cached version. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> View old versions of the page. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Open robots.txt for the domain in which the page is located. </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Run a search for other domains on the same IP. </font></font></li></ul><br><br><br><h3>  Conclusion </h3><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, we have examined in detail all aspects of using the program Screaming Frog. </font><font style="vertical-align: inherit;">We hope that our detailed instructions will help you to make the site audit more simple and at the same time quite effective, while providing an opportunity to save a lot of time.</font></font><br><br><br></div><p>Source: <a href="https://habr.com/ru/post/266915/">https://habr.com/ru/post/266915/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266903/index.html">PSR-2, the analysis of one item of the standard. Spaces or tabs</a></li>
<li><a href="../266905/index.html">FP on Scala: What is a functor?</a></li>
<li><a href="../266907/index.html">HP TippingPoint ATA Network and HP TippingPoint ATA Mail</a></li>
<li><a href="../266909/index.html">PHP and realpath_cache</a></li>
<li><a href="../266911/index.html">Crisis Junior System Administrator</a></li>
<li><a href="../266917/index.html">Search for periodic security elements of the RF Passport using the Fourier transform: part two</a></li>
<li><a href="../266919/index.html">VI Hi-Tech Tour Inoventica Cloud Center: â€œIndian Summer in the Cloudsâ€</a></li>
<li><a href="../266921/index.html">Sipuni: development path from a virtual PBX to a SaaS-service for receiving phone calls</a></li>
<li><a href="../266923/index.html">HostGator vs. GoDaddy - Hosting Review</a></li>
<li><a href="../266925/index.html">A collection of practical PHP tasks to prepare for an interview</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>