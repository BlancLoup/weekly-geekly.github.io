<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to handle terabytes of data in 1000 streams in PHP - Hadoop / MapReduce</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 

 Already heard about Bigdata ? Well, yes, the web is growing, data is becoming more and they need to be kept under control and periodically a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to handle terabytes of data in 1000 streams in PHP - Hadoop / MapReduce</h1><div class="post__text post__text-html js-mediator-article">  Hello! <br><br>  Already heard about <a href="http://en.wikipedia.org/wiki/Big_data">Bigdata</a> ?  Well, yes, the web is growing, data is becoming more and they need to be kept under control and periodically analyzed.  Databases - burst under load, relational theory does not quite cope with the problem, you need a solution.  Marketing actively presses from above, and iron with sharp corners - from below and smacks of suicide. <br><br>  In this post I will try to give specific working recipes and pieces of code with brief theoretical conclusions, how to handle&gt; = terabytes in&gt; = 1000 threads in PHP.  To be able to take and solve the problem, without losing time and not scoring a head theory. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      However, if suddenly it became nauseated and dizzy, you can no longer read - and admire the beautiful birds and forget about the above.  But be on the alert, Bigdata may take a knock on the door tomorrow ;-) <br><img src="https://habrastorage.org/getpro/habr/post_images/338/623/363/3386233633315e443f8a35137124d2b4.jpg"><br><a name="habracut"></a><br><br><h4>  As usual done </h4><br><br>  As usually happens on the web.  Add the data to the database until it crashes.  If it bursts, conversations about MySQL sharding, <a href="https://dev.mysql.com/doc/refman/5.6/en/partitioning.html">partitioning</a> begin, remember a <a href="http://www.mysql.com/products/cluster/">multi-master cluster</a> in RAM. <br><br>  If it does not help, begin the search and implementation of NoSQL solutions like <a href="http://redis.io/">redis</a> or cloud services like <a href="http://aws.amazon.com/dynamodb/">DynamoDB</a> .  Not bad proved itself as an effective search engine for volumetric data <a href="http://www.1c-bitrix.ru/products/cms/new/new140.php%3Fsphrase_id%3D3012622">Sphinx</a> . <br><br>  Subconsciously, the calculation is going to be saved in the database and then we will analyze the information.  And it often works.  But not always ... and this "not always" becomes more frequent. <br><br><h4>  More data, online analytics required </h4><br><br>  It is not always possible to answer the business - wait for a day, analyze logs / data and give tsiferki.  It is often important for business to have tsiferki online, to manage the situation with instruments with live arrows. <br><img src="https://habrastorage.org/getpro/habr/post_images/f69/66c/721/f6966c7211e7cb0b554c1ef155b52899.jpg"><br><br>  It is terrible to imagine the control of the aircraft by analyzing the information recorded in black boxes once a day in a hotel for pilots :-) <br><img src="https://habrastorage.org/getpro/habr/post_images/7b1/ac9/327/7b1ac9327af16213adb14b9074e42801.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/cc0/75e/d15/cc075ed1517c67e46d6469505970c57f.jpg"><br><br>  When the data flow becomes even more intense or business logic requires the availability of current information on data that has not yet been processed ... Then we are helped by the tools of "stream analysis" of the type: <br>  1) <a href="http://pinba.org/">pinba</a> <br>  2) <a href="http://aws.amazon.com/kinesis/%3Fnc1%3Dh_l2_al">Amazon Kinesis</a> <br>  3) Stream parsers based on <a href="http://www.evanmiller.org/nginx-modules-guide-advanced.html">nginx</a> / <a href="http://en.wikipedia.org/wiki/Ragel">ragel</a> <br><br>  It is useful at least once to understand each of these invaluable tools with a piece of paper and a pencil, even more useful to ‚Äúsleep‚Äù with the manual and prototype at least overnight. <br><img src="https://habrastorage.org/getpro/habr/post_images/48a/e8b/2f1/48ae8b2f14997acce0969093255d43af.gif"><br><br>  I especially want to highlight here <a href="http://pinba.org/">pinba</a> for ease of setup and ease of operation and the minimum load created.  To organize the collection of statistics on the performance of a web application in the browser of its clients based on js <a href="http://www.w3.org/TR/navigation-timing/">Navigation Timing API</a> - done in 2 PHP files on 30 lines. <br><br>  When it is not possible to analyze the data online, the search for a solution to the parallel analysis of the accumulated data and the associated algorithms begins. <br><br><h4>  Parallel array processing </h4><br><br>  There is a list of objects, let's say these are files in the s3 cloud, of which you have tens of millions.  No matter how much we trust the cloud, you need to periodically upload these files to another cloud / server.  Each file is encrypted, compressed, other operations occur and copied. <br><br>  There are many similar problems in nature: <br><ul><li>  image processing </li><li>  processing XML documents via XSLT filter </li><li>  log processing </li><li>  sorting </li></ul><br><br>  These tasks fall under the general divide-and-conquer algorithm: <br>  - distribute puzzles to pieces <br>  - each part is processed separately and in parallel with other parts <br>  - combine the results through aggregation <br><img src="https://habrastorage.org/getpro/habr/post_images/ccc/de8/95b/cccde895be377e49bbc5b6b194b3b342.jpg"><br><br>  For PHP, you can try to solve this problem using a queue such as <a href="https://www.rabbitmq.com/">RabbitMQ</a> and / or <a href="http://gearman.org/">Gearman</a> - but you have to tinker a lot to solve exceptions, sharding the shared file system, clustering on 20 servers, etc. <br><br>  Therefore, if your task can be solved in 30 PHP threads on a single server, the listed tools are usually sufficient.  However, if you are ‚Äúunlucky‚Äù and you need to process several terabytes in an hour and give how much iron you will carry, there is a way out :-) <br><br>  Yes, yes, of course this is <a href="http://ru.wikipedia.org/wiki/Hadoop">Hadoop</a> , which implements the MapReduce paradigm that correlates with the photos of girls above ;-) <br><br>  Who is too lazy to read further and want to know the recipe, here is an example of the original problem and its solutions on Hadoop: <br><br><blockquote>  It is necessary to compress, encrypt and transfer 10 million files from baket1 s3 to baket2 s3. <br>  If you do it using PHP on the server, then you can fork a maximum of up to 20-30 PHP threads, which each will be executed in its process.  And it will take several weeks.  And the amount of data is growing and you need a system solution. <br>  If the same thing is done using Hadoop, then the task can be completed in an hour, but on a large number of hardware.  If you choose a reasonable number of glands with 15 threads each - then you can keep within 2 days. <br>  Those.  if in six months the number of files for processing grows from 10 million to 50 million, you will need to change only one dial in the Hadoop cluster launch configuration, increasing the number of pieces of hardware only. <br>  Is not it beautiful and systemic?  :-) </blockquote><br><br><h4>  Hadoop </h4><br><br>  In general, this is a rather large product, and there probably won't be enough weeks for reading manuals 24/7 - but this is not required.  We will learn to use this technology efficiently and quickly, saving you and our time. <br><br><h5>  Installation </h5><br><br>  In addition to installing java-software, you will need to configure the cluster file system.  Why - and how will the cluster nodes exchange shared files?  But we‚Äôll get smarter - we‚Äôll launch the <a href="http://aws.amazon.com/elasticmapreduce/">Hadoop</a> cluster <a href="http://aws.amazon.com/elasticmapreduce/">in Amazon</a> .  There everything is already configured and installed. <br><br><h5>  Preparing map and reduce scripts </h5><br><br>  Here is the most interesting post.  Hadoop allows you to use scripts in any language - and to sort the file in bash or processing in PHP / Python / Perl. <br><br>  Scriptists read from standard input and write to standard output.  Well, what could be easier? <br><br>  Skriptik should be 2: mapper, reducer. <br><br>  If you just need to parallelize the task on N servers, just write one mapper. <br><br><h6>  Mapper example </h6><br><br><pre><code class="php hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/php &lt;?php error_reporting(-1); set_time_limit(0); ini_set('memory_limit', '2048M'); gc_enable(); require '/usr/share/php/aws.phar'; $fp=fopen("php://stdin","r"); while (true) { $line=stream_get_line($fp,65535,"\n"); //    : , , ,  ... } echo "s3 copied direct\t".$copy_count."\n"; echo "s3 copied precond\t".$copy_precond_count ."\n"; echo "s3 src not found\t".$s3_src_not_found ."\n";</span></span></code> </pre> <br><br>  If aggregated statistics is not needed, the second script is not needed.  If needed, write a reducer: <br><br><h6>  Example reducer </h6><br><br><pre> <code class="php hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/php &lt;?php error_reporting(-1); ini_set('memory_limit', '1024M'); set_time_limit(0); gc_enable(); $ar_reduce = array(); while (($line = fgets(STDIN)) !== false) { $line = str_replace("\n","",$line); $ar_line = explode("\t", $line); if ( !isset($ar_reduce[$ar_line[0]]) ) $ar_reduce[$ar_line[0]] = 0; $ar_reduce[$ar_line[0]] += intval($ar_line[1]); } foreach ($ar_reduce as $key=&gt;$value) { echo $key."\t".$value."\n"; } ?&gt;</span></span></code> </pre><br><br><h6>  Initializing Cluster Servers </h6><br><br>  Since  our PHP scripts, you need to prepare an initialization script that runs on each server in the cluster: <br><pre> <code class="bash hljs">sudo apt-get -y update sudo apt-get -y install libssh2-php sudo apt-get -y install php5-curl sudo rm -f /etc/php5/cli/conf.d/suhosin.ini sudo mkdir -p /usr/share/php <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /usr/share/php sudo wget https://github.com/aws/aws-sdk-php/releases/download/2.5.0/aws.phar ...</code> </pre><br><br><h6>  Uploading scripts in PHP and bash to the cloud (s3) </h6><br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> FILE <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> bkp_s3_folder_hadoop_bootstrap.sh bkp_s3_folder_hadoop_mapper.php bkp_s3_folder_hadoop_reducer.php; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> s3cmd -c /root/.s3cfg-key put /home/project/cron_jobs/<span class="hljs-variable"><span class="hljs-variable">$FILE</span></span> s3://<span class="hljs-comment"><span class="hljs-comment"># #/code/ done</span></span></code> </pre><br><br><h6>  Data upload for processing in s3 </h6><br><br>  Simply, for example with the help of s3cmd, we unload the initial data for processing into a folder in s3.  This data will then spread over the cluster automatically.  You can upload as many data as you like and let the cluster suffer with them. <br><br><h6>  Starting data processing in a cluster </h6><br><br>  And finally, such a tasty treat - we launch a cluster to process our data. <br><br><pre> <code class="bash hljs">D=$(date +<span class="hljs-string"><span class="hljs-string">"%Y-%m-%d_%H-%M-%S"</span></span>) /opt/aws/emr/elastic-mapreduce --create --stream \ --name myproject_<span class="hljs-variable"><span class="hljs-variable">$D</span></span> \ --step-name step_<span class="hljs-variable"><span class="hljs-variable">$D</span></span> \ --with-termination-protection \ --step-action CANCEL_AND_WAIT \ --ami-version <span class="hljs-string"><span class="hljs-string">'2.4.2'</span></span> \ --bootstrap-action <span class="hljs-string"><span class="hljs-string">'#    , . #'</span></span> \ --bootstrap-action <span class="hljs-string"><span class="hljs-string">'s3://elasticmapreduce/bootstrap-actions/configure-hadoop'</span></span> \ --args <span class="hljs-string"><span class="hljs-string">"-m,mapred.map.max.attempts=20,-m,mapred.tasktracker.map.tasks.maximum=15,-m,mapred.task.timeout=600000"</span></span> \ --input <span class="hljs-string"><span class="hljs-string">'s3://#     #/input/'</span></span> \ --mapper <span class="hljs-string"><span class="hljs-string">'s3://# #/code/# mapper#.php'</span></span> \ --reducer <span class="hljs-string"><span class="hljs-string">'s3://# #/code/# reducer#.php'</span></span> \ --output <span class="hljs-string"><span class="hljs-string">'s3://#  #/output_'</span></span><span class="hljs-variable"><span class="hljs-variable">$D</span></span> \ --<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-uri <span class="hljs-string"><span class="hljs-string">'s3://#  #/logs/'</span></span> \ --num-instances 5 \ --master-instance-type m1.small \ --slave-instance-type m1.xlarge \ --key-pair <span class="hljs-string"><span class="hljs-string">'myproject_mapreduce'</span></span></code> </pre><br><img src="https://habrastorage.org/files/0bb/cce/9c1/0bbcce9c1264438e90f812b2d5887689.jpg"><br>  Here it is important to choose the correct number of pieces of iron for the reproduction of a cluster - the more, the faster of course.  In this example, we install no more than 15 processes on one server.  It can be more, it depends on the amount of RAM, but carefully - we monitor its consumption. <br><br>  After the cluster has been worked out, it will be possible to see the aggregated statistics in the logs, the logs will also be uploaded to s3. <br><br>  Usually, the processing speed that has been done for weeks - amazes, inspires and takes a new level of awareness of IT-continuum better than the last part of ‚Äú300 Spartans‚Äù :-) <br><img src="https://habrastorage.org/files/769/f2e/263/769f2e2637ec42de92dd66fe834a2043.jpg"><br><br><h4>  Results </h4><br><br>  As a result, you have a business tool that is managed by 2 scripts in PHP.  The number of servers (--num-instances 5) directly affects the processing speed of the loaded data array.  In principle, no one forbids starting up 100 servers with 10 threads on each and processing data much faster than it could be done on a single server using a queue of tasks. <br><br>  Using this technology in a simple and understandable way, we have reduced the processing time of tens of millions of objects in s3 from weeks to 2 days on one <a href="https://www.bitrix24.ru/">of our projects</a> . <br><br>  Colleagues, if you have questions, please ask in the comments and attend our <a href="http://www.failoverconf.ru/conf2014/">conferences</a> - we will be happy to share our experience.  And good luck to all in the implementation of web projects and victories over Bigdata! </div><p>Source: <a href="https://habr.com/ru/post/218003/">https://habr.com/ru/post/218003/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../217989/index.html">We write client for Yandex. Metrics for iPhone</a></li>
<li><a href="../217993/index.html">We are waiting for the summer webinars from Alexey Kibkalo</a></li>
<li><a href="../217995/index.html">Science fiction technology: light engines and systems (Star Trek)</a></li>
<li><a href="../217997/index.html">Learn BlackBerry 10 App Development is available for free download.</a></li>
<li><a href="../218001/index.html">Part 4.1 We return the sight. From points to the excimer laser</a></li>
<li><a href="../218007/index.html">Features of mobile applications on the iOS platform, using the login and password of clients to access paid content</a></li>
<li><a href="../218009/index.html">New Nimbus Clipper, as well as passwords for public notes</a></li>
<li><a href="../218011/index.html">Impressions of the Human-Computer Interaction course on Coursera</a></li>
<li><a href="../218013/index.html">What can be done with ITEAD IBOX. Part 1</a></li>
<li><a href="../218015/index.html">Space exit 23: conquering the new frontier</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>