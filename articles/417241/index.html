<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>S3 metadata in PostgreSQL. Yandex lecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is the second lecture from J.Subbotnik on databases - we published the first one a couple of weeks ago. 

 Dmitry Sarafannikov, the head of the g...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>S3 metadata in PostgreSQL. Yandex lecture</h1><div class="post__text post__text-html js-mediator-article">  This is the second lecture from J.Subbotnik on databases - we published the <a href="https://habr.com/company/yandex/blog/415817/">first</a> one a couple of weeks ago. <br><br>  Dmitry Sarafannikov, the head of the general purpose DBMS group, spoke about the evolution of data storage in Yandex: how we decided to make an S3-compatible interface, why we chose PostgreSQL, what rakes we attacked and how we coped with them. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/HqPYXZDt3VA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  - Hello!  My name is Dima, in Yandex I deal with databases. <a name="habracut"></a>  I'll tell you how we did S3, how we came to the point of doing exactly S3, and what kind of vaults we had before.  The first one is Elliptics, it is posted on the open source, available on GitHub.  Many may have come across it. <br><img src="https://habrastorage.org/webt/ji/kg/05/jikg05jprfdg32gcrus0iunyha4.jpeg"><br>  This is, in fact, a distributed hash table with a 512-bit key, the result of SHA-512.  It forms a key ring that is randomly divided between machines.  If you want to add machines there, the keys are redistributed, rebalancing takes place.  This repository has its own problems associated, in particular, with rebalancing.  If you have a sufficiently large number of keys, then with ever-increasing volumes you need to constantly drop cars there, and rebalancing may simply not converge on a very large number of keys.  It was a big enough problem. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      But at the same time, this storage is great for more or less static data, when you fill up a large amount of one-time data, and then chase them read-only-load.  For such solutions it fits perfectly. <br><br>  We go further.  The problems with rebalancing were quite serious, so the next storage appeared. <br><img src="https://habrastorage.org/webt/s6/p-/0q/s6p-0qu2vq5zqtguekkjpyvljf4.jpeg"><br>  What is its essence?  This is not a key-value-storage, it is a value-storage.  When you upload an object or file there, it answers you with a key, which can then be used to pick up this file.  What does this give?  Theoretically, one hundred percent availability for recording, if you have free space in the storage.  If you have some typewriters, you simply write to others that do not lie, on which there is free space, get other keys and calmly collect your data. <br><br>  This storage is very easy to scale, you can throw it with iron, it will work.  It is very simple, reliable.  Its only drawback: the client does not control the key and all clients must keep keys somewhere, store the mapping of their keys.  All this is uncomfortable.  In fact, this is a very similar task for all clients, and everyone solves it in their own way in their metabase, etc. This is inconvenient.  But at the same time, I don‚Äôt want to lose the reliability and simplicity of this storage, in fact it works at network speed. <br><br>  Then we started looking at S3.  This is key-value storage, the client controls the key, all storage is divided into so-called buckets.  In each bucket, the key space is from minus infinity to plus infinity.  The key is some kind of text string.  And we stayed on it, on this option.  Why S3? <br><br>  Everything is quite simple.  By this time, many ready-made clients have already been written for various programming languages, many ready-made tools have already been written for storing something in S3, say, database backups.  Andrei <a href="https://habr.com/company/yandex/blog/415817/">talked</a> about one of the examples.  There is already quite thoughtful API, which for years has been tested on clients, and there is nothing to invent.  The API has many convenient features such as listings, multipart-apploads, and so on.  Therefore, we decided to stay on it. <br><br>  How to make S3 from our storage?  What comes to mind?  Since the clients themselves keep the key mapping, we just take, put a number of databases, and store the key mapping in it.  When reading, we will simply find the keys and storage in our database, and give the client what he wants.  If you sketch this out, how does the fill work? <br><img src="https://habrastorage.org/webt/99/kr/9t/99kr9tmubs8skxzhbl-yrxmq4iq.jpeg"><br>  There is a certain entity, here it is called Proxy, the so-called backend.  It takes the file, uploads it to storage, receives the key from there and saves it to the database. Everything is quite simple. <br><img src="https://habrastorage.org/webt/jx/m3/q8/jxm3q8hbv-qzn7qohrpaqw8cgv8.jpeg"><br>  How is the receipt?  The proxy finds the necessary key in the database, comes with the key to storage, downloads the object from there, gives it to the client.  Everything is also simple. <br><img src="https://habrastorage.org/webt/tb/w3/hw/tbw3hwknk1vjapsvw9af7pvquig.jpeg"><br>  How is the removal?  The proxy does not work directly with the storage, since it is difficult to coordinate the base and storage, so it simply goes to the database, tells it that this object is deleted, the object is moved to the queue for deletion, and then in the background a specially trained professional the robot takes these keys, removes them from storage and from the database.  It's all quite simple too. <br><br>  We have selected PostgreSQL as the database for this metabase. <br><br>  You already know that we love him very much.  With the relocation of Yandex.Mail, we have gained sufficient expertise on PostgreSQL, and when various mail services were moving, we developed several so-called sharding patterns.  One of them went well with the S3, with a few changes, but it went well there. <br><br>  What are the sharding options?  This is a large repository, on the scale of Yandex, one must immediately think that there will be many objects, one must immediately think through how to shuffle all this.  You can shard by hash on behalf of the object, this is the most reliable way, but it will not work here, because S3 has, for example, listings that should show a list of keys in a sorted order, when you cache, all your sortings will be lost, you need to remove all objects so that the output conforms to the API specification. <br><br>  The next option, can be shaded by hash on behalf of or id of the bake.  One bucket can live inside one DB shard. <br><br>  Another option is to shard across key ranges.  Inside the baketa, the space is from minus infinity to plus infinity, we can divide it into as many ranges as we like, we call this range a chunk, it can live only in one shard. <br><img src="https://habrastorage.org/webt/yw/d3/vh/ywd3vh23mont7zoqjcw6xmcmrbi.jpeg"><br>  We chose the third option, sharding by chunks, because theoretically in one baget there can be an infinite number of objects, and it stupidly does not fit into one piece of metal.  There will be big problems, so we will cut and decompose into shards as we like.  Everybody is here. <br><img src="https://habrastorage.org/webt/yu/l-/cu/yul-cuz5u4eb5w1hrtzfguio9ro.jpeg"><br>  What happened?  The entire database consists of three components.  S3 Proxy - a group of hosts, there is also a database.  PL / Proxy stand under the balancer, requests from that backend go there.  Further S3Meta, such a group of bass, which stores information about buckets and chunks.  And S3DB, shards, where objects are stored, turn to delete.  If you sketch, it looks like this. <br><img src="https://habrastorage.org/webt/3y/0n/y9/3y0ny9npaewsndqfusttbvlqczk.jpeg"><br>  A request comes to S3Proxy, it goes to S3Meta and to S3DB and provides information to the top. <br><img src="https://habrastorage.org/webt/uw/1s/j1/uw1sj1zph_sjghozg404mmdzhnw.jpeg"><br>  Consider more.  S3Proxy, functions created in it in the procedural language PLProxy, is a language that allows you to execute remotely stored procedures or queries.  This is how the code of the ObjectInfo function looks, in essence, a Get request. <br><br>  The cluster on LProxy has a Cluster operator, in this case it is db_ro.  What does it mean? <br><img src="https://habrastorage.org/webt/59/f8/sp/59f8spslkiczmbu6arwngghstoq.jpeg"><br>  If the typical configuration of the database shard is a master and two replicas.  The master is included in the db_rw cluster, all three hosts are included in the db-ro, this is where you can send a read only request, and a write request is sent to db_rw.  The db_rw cluster includes all masters of all shards. <br><br>  The following RUN ON statement, it takes either the value all, which means it must be executed on all shards, or an array, or some kind of shard.  In this case, it accepts the result of the function get_object_shard as input, this is the number of the shard on which this object lies. <br><br>  And target - what function to call on the remote shard.  He will call this function and substitute the arguments that came to this function. <br><img src="https://habrastorage.org/webt/kw/y3/ib/kwy3ib0cemkqipnlkceiaf_vny8.jpeg"><br>  The get_object_shard function is also written in PLProxy, already a meta_ro cluster, the request will fly to the S3Meta shard, which will return this function get_bucket_meta_shard. <br><br>  S3Meta can also be shaded, we also laid it, while it is irrelevant, but there is a possibility.  And it will call the get_object_shard function on S3Meta. <br><img src="https://habrastorage.org/webt/cx/t2/am/cxt2amhwudlblfjx2l8vrrzsxce.jpeg"><br>  get_bucket_meta_shard is just a hash of the text in the name of the batch, we just shard S3Meta with the hash in the name of the bake. <br><img src="https://habrastorage.org/webt/sd/r9/1x/sdr91x352qh8xpccft-yay_ozqq.jpeg"><br>  Consider S3Meta, what happens in it.  The most important information that is there is a table with chunks.  I cut out some unnecessary information, the most important thing left is the bucket_id, the initial key, the final key and the shard in which this chunk lies. <br><img src="https://habrastorage.org/webt/-n/74/zv/-n74zv7ncle8hcvuj7-_utprjta.jpeg"><br>  How could a query look like such a table that returns us a chunk in which, for example, a test object lies?  Something like this.  Minus infinity in text form, we presented it as a null value, there are such subtle moments that you need to check start_key and end_key for is Null. <br><img src="https://habrastorage.org/webt/ay/1f/cl/ay1fclvy9w-_0xillystohcq8v4.jpeg"><br>  The request does not look very good, and the plan looks even worse.  As one of the variants of the plan of such a request, BitmapOr.  And 6,000 costa is such a plan. <br><img src="https://habrastorage.org/webt/qn/m6/ze/qnm6zelrf6qh2q1gpf4-eaxgzle.jpeg"><br>  How can it be different?  There is such a great thing in PostgreSQL as gist index, which can index range type, range in essence, what we need.  We have made this type; the s3.to_keyrange function returns us, in fact, a range.  We can verify with the contains operator, find the chunk that contains our key.  And for this, exclude constrain is built here, which provides for non-intersection of these chunks.  We need to allow, preferably at the DB level, to make some constraint so that the chunks cannot intersect with each other, so that only one row is returned in response to the query.  Otherwise, it will not be what we wanted.  This is the plan for such a query, the usual index_scan.  This condition is entirely in index condition, and such a plan has only 700 costa, 10 times less. <br><img src="https://habrastorage.org/webt/fz/ii/el/fziielc9cnipflupcktsoowvmle.jpeg"><br>  What is Exclude Constraint? <br><img src="https://habrastorage.org/webt/f8/zc/ai/f8zcaicub3p9ob_7n2fkcefww3k.jpeg"><br>  Let's create a test table with two columns, and put two constraint on it, one unique one that everyone knows, and one exclude constraint, which has parameters equal to, such operators.  Let's set with two operators equally, built such a table. <br><img src="https://habrastorage.org/webt/2o/gb/jx/2ogbjxuanf1sfqxvsaz2nm81a9o.jpeg"><br>  Next we try to insert two identical lines, we get an error of violation of the key uniqueness on the first constraint.  If we drop it, we have already violated the exclusion constraint.  This is a common case of unique constraint. <br><img src="https://habrastorage.org/webt/rf/jo/5c/rfjo5cabxypgt7ox6nfrptpccgc.jpeg"><br>  In fact, a unique constraint is the same exclude constraint with operators equal, but in the case of an exclude constraint you can build some more general cases. <br><img src="https://habrastorage.org/webt/t4/r5/pw/t4r5pw1rosp_z4hdf72djmkddfy.jpeg"><br>  We have such indexes.  If you look closely, you will see that these are both gist index, and in general they are the same.  You probably ask why duplicate this thing at all.  I'll tell you. <br><img src="https://habrastorage.org/webt/lj/rr/tp/ljrrtplgolj6ipalh4quhrpbqq8.jpeg"><br>  Indexes are such a thing, especially the gist index, that the table lives its own life, there are updates, divides, and so on, the index deteriorates there, ceases to be optimal.  And there is such a practice, in particular the pg repack extension, the indices are rebuilt periodically, once in a while they are rebuilt. <br><br>  How to rebuild an index under unique constraint?  Create a create index currently, create the same index quietly next to it without blocking, and then use the alter table expression to constraint user_index so and so.  And that's it, everything is clear and good, it works. <br><br>  In the case of exclude constraint, you can rebuild it only through reindex blocking, or rather your index will be blocked exclusively, and in fact you will have all the requests.  This is unacceptable, the gist index can be built long enough.  Therefore, we keep close to the second index, which is smaller in volume, takes up less space, the glider uses it, and we can rebuild that index competitively without blocking. <br><img src="https://habrastorage.org/webt/4t/5i/j_/4t5ij__ami8qcaik9hwlvyku4sk.jpeg"><br>  Here is a graph of CPU consumption.  Green line - CPU consumption in user_space, it jumps from 50% to 60%.  At this point, consumption drops sharply, this is the moment when the index is rebuilt.  We rebuilt the index, the old one was deleted, our processor consumption has plummeted.  This is a gist index problem, it is, and this is a clear example of how this can be. <br><br>  When we did all this, we started on version 9.5 S3DB, according to the plan we planned to lay 10 billion objects in each shard.  As you know, more than 1 billion and even earlier problems begin, when there are many lines in the table, everything becomes much worse.  There is a practice of parting.  At that time there were two options, either standard through inheritance, but this does not work very well, since there is a linear speed for selecting the partition.  And judging by the number of objects, we need a lot of partitions.  The guys from Postgres Pro then actively sawed the pg_pathman extension. <br><img src="https://habrastorage.org/webt/hs/vp/ex/hsvpexzv7ox6fs7ajmyctotqlcw.jpeg"><br>  We chose pg_pathman, we had no other choice.  Even version 1.4.  And as you can see, we use 256 partitions.  We split the entire table of objects into 256 partitions. <br><br>  What does pg_pathman do?  With this expression, you can create 256 partitions that are partitioned by hash from the bid column. <br><img src="https://habrastorage.org/webt/5k/5h/x7/5k5hx738e1mzf4f2jcimr7snb9g.jpeg"><br>  How does pg_pathman work? <br><img src="https://habrastorage.org/webt/e7/_b/c2/e7_bc24xsopwt5pvme1xw27lhj4.jpeg"><br>  It registers its hooks in the glider, and further on the requests it replaces, in fact, a plan.  We see that he didn‚Äôt search for 256 partitions on the usual search request for the object named test, but immediately determined that he needed to go into the objects_54 table, but this wasn‚Äôt all smooth, pg_pathman has its own problems.  Firstly, there were a lot of bugs in the beginning, while it was sawed, but thanks to the guys from Postgres Pro, they promptly repaired and fixed them. <br><br>  The first problem is the complexity of updating it.  The second problem is prepared statements. <br><br>  Consider more.  In particular, the update.  What does pg_pathman consist of? <br><img src="https://habrastorage.org/webt/zj/fp/on/zjfpond4kxp6jbx23zuuls_xxmc.jpeg"><br>  It consists, in essence, of the C-code, which is packaged in the library.  And it consists of a SQL part, all the functions of creating partitions and so on.  Plus, interfaces to the functions that are in the library.  These two parts cannot be updated at the same time. <br><br>  This leads to difficulties, something like this algorithm for updating the pg_pathman version, we first roll a new package with a new version, but the old versions of PostgreSQL are still in memory, it uses it.  This is immediately in any case, the base must be restarted. <br><br>  Next we call the set_enable_parent function, it turns on the function in the parent table, which is turned off by default.  Next, turn off the pathman, restart the database, say ALTER EXTENSION UPDATE, at this time we all fall into the parent table. <br><br>  Next, turn on the pathman, and run the function, which is in the extension, which shifts the objects from the parent table, which were attacked there during this short period of time, shifts it back to the tables where they should be.  And then turn off the use of the parent table, search in it. <br><img src="https://habrastorage.org/webt/i0/vi/wp/i0viwpliiv3stsq2vhn2xoid9su.jpeg"><br>  The next problem is prepared statements. <br><img src="https://habrastorage.org/webt/7a/kn/3v/7akn3vmg3-owux9ywqyqocpn1e0.jpeg"><br>  If we prescribe the same usual request, search by bid and key, we will try to execute it.  We do it five times - all is well.  We carry out the sixth - we see such a plan.  And in this plan we see all 256 partitions.  If you look closely at these conditions, we see here 1 dollar, 2 dollar, this is the so-called generic plan, the general plan.  The first five queries were built individually, individual plans were used for these parameters, pg_pathman could immediately determine, because the parameter is known in advance, could immediately determine the table where to go.  In this case, he cannot do this.  Accordingly, the plan should have all 256 partitions, and when the executor goes to do this, he will go and take shared lock to all 256 partitions, and the performance of such a solution is no good at once.  It just loses all its advantages, and any request is insanely long. <br><img src="https://habrastorage.org/webt/pg/ny/vr/pgnyvrsmxkaizxhi1ifdazcith8.jpeg"><br>  How did we get out of this situation?  I had to wrap everything inside the stored procedures in execute, in dynamic SQL, so that the prepared statements were not used and the plan was built each time.  So it works. <br><br>  The downside is that you have to push all the code into constructions that touch these tables.  Here it is more difficult to read. <br><img src="https://habrastorage.org/webt/rh/6i/p9/rh6ip927nxvaryx5lq7uw80kc_w.jpeg"><br>  How is the distribution of objects?  In each S3DB shard, chunk counters are stored, there is also information about which chunks in this shard lie, and counters are stored for them.  For each mutable operation on an object ‚Äî add, delete, modify, overwrite ‚Äî these counters for the chunk are changed.  In order not to update the same line when there is an active fill in this chunk, we use a fairly standard method, when we insert a delta counter into a separate table, and once a minute a special robot passes and aggregates all of this, updates the chunk counters . <br><img src="https://habrastorage.org/webt/xe/rn/hv/xernhvrnmssloj0y6mngbtwi9zg.jpeg"><br>  Further, these counters are delivered to S3Meta with some delay, there is already a complete picture of how many counters are in which chunk, then you can look at the distribution by shards, how many shards of objects, and on the basis of this a decision is made about where the new chunk falls.  When you create a bucket, for the bucket, by default, a single chunk from minus infinity to plus infinity is created, depending on the current distribution of objects that S3Meta knows, it falls into a shard. <br><br>  When you fill in this baket data, all this data is poured into this chunk; when a certain size is reached, a special robot comes in and divides this chunk. <br><img src="https://habrastorage.org/webt/6g/9x/jx/6g9xjxmouaqfyz9-lt-d-qnl-fy.jpeg"><br>  We make them so small.  We do this so that, in which case, this small chunk can be dragged to another shard.  How is split chunk?  Here is a regular robot, it goes and this chunk in S3DB will split the two-phase commit and update the information in S3Meta. <br><img src="https://habrastorage.org/webt/9p/ec/pg/9pecpg4ivr6x4fbtp4dwgv6rhie.jpeg"><br>  Transferring a chunk is a slightly more complicated operation; it is a two-phase commit over three bases, S3Meta and two shards, S3DB, from one pitch, stacked in the other. <br><img src="https://habrastorage.org/webt/re/1o/so/re1osogppbljsj7camywvynnjg4.jpeg"><br>  In S3, there is such a feature as listings, this is the most difficult thing, and there are problems with it too.  Essentially, listings, this is you saying S3 - show me the objects that I have.  Red is the parameter that now has the value Null.  This parameter, delimeter, delimiter, you can specify listings with which delimiter you want. <br><img src="https://habrastorage.org/webt/ws/pg/ld/wspgldab_p7mtvwl1p-2rahx48s.jpeg"><br>  What does it mean?  If the delimeter is not specified, we see that we are simply given a list of files.  If we set a delimeter, in fact, S3 should show us the folders.  It must be realized that there are such folders, and in fact, shows all the folders and files in the current folder.  The current folder is set by the prefix, this parameter is here Null.  We see that there are 10 folders there. <br><br>  All keys are not stored in a hierarchical tree structure, as in a file system.  Each object is stored in a string, and they have a simple common prefix.  S3 should understand that this ass. <br><img src="https://habrastorage.org/webt/pi/3p/dq/pi3pdqgonztctekxsrwxg_go5ra.jpeg"><br>  Such logic badly enough lays down on declarative SQL, it is easy enough to describe it the imperative code.  The first option was made that way, just stored procedures on PL / pgSQL.  He imperatively processed this logic in a loop, required a level of repeatable read.  We need to see only one snapshot, execute all requests with one snapshot. ,  -     - ,    . <br><br>        Recursive CTE,       ,   -  ,       execute  PL/pgSQL.   ,      .  , ,  ,    list objects. ,     . <br><img src="https://habrastorage.org/webt/bn/ng/t5/bnngt5vw2so41bmailmlzscffom.jpeg"><br>   ,    . <br><br>      .       ,         . <br><img src="https://habrastorage.org/webt/df/am/2x/dfam2xo8k6ohr6gkv9mo1d5pmsa.jpeg"><br>     Docker,  <a href="https://github.com/behave/behave">Behave</a>   Behave   <a href="https://events.yandex.ru/lib/talks/4057/"></a>  .  ,   , ,    . <br><br>      .   ,    ,   CPU  S3Meta. Gist index    CPU,         , . CPU  S3Meta   .   ,      .       PLProxy  ,        S3Meta  S3DB.  ,      .          S3Meta   .  ,    . <br><br>      ,   ,     .   ‚Äî    ,     range  btree.    ,  btree     .   ,       ,      btree.     ,  .    PL/pgSQL-.    ,     . </div><p>Source: <a href="https://habr.com/ru/post/417241/">https://habr.com/ru/post/417241/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417231/index.html">Corporate merchandise with human UI</a></li>
<li><a href="../417233/index.html">Google Code-in 2017</a></li>
<li><a href="../417235/index.html">How online stores lose money because of the address on the order form</a></li>
<li><a href="../417237/index.html">What the developers are listening to: from classics to game soundtracks - we discuss the most interesting</a></li>
<li><a href="../417239/index.html">The digest of interesting materials for the mobile developer # 261 (July 9 - July 15)</a></li>
<li><a href="../417243/index.html">Install the 3CX SBC Session Border Controller on Windows, Raspberry Pi or Debian 9</a></li>
<li><a href="../417245/index.html">Erlang for IoT</a></li>
<li><a href="../417247/index.html">VSCE # 1: a podcast about media entrepreneurs</a></li>
<li><a href="../417249/index.html">The US Accounts Chamber warns: SpaceX and Boeing are waiting for new delays, it is possible that the United States will have a break in flights to the ISS</a></li>
<li><a href="../417251/index.html">Using the Fish eye camera on a Raspberry Pi 3 with ROS - part 1</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>