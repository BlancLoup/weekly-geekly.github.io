<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Open machine learning course. Topic 3. Classification, decision trees and the method of nearest neighbors</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to everyone who takes a course of machine learning in Habr√©! 


 In the first two parts ( 1 , 2 ) we practiced in the primary analysis of data w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Open machine learning course. Topic 3. Classification, decision trees and the method of nearest neighbors</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/30f/e14/6d4/30fe146d40dc414dbd61ba87a83585bb.jpeg" align="right" width="320"><br><p>  Hello to everyone who takes a course of machine learning in Habr√©! </p><br><p>  In the first two parts ( <a href="https://habrahabr.ru/company/ods/blog/322626/">1</a> , <a href="https://habrahabr.ru/company/ods/blog/323210/">2</a> ) we practiced in the primary analysis of data with Pandas and in the construction of pictures, allowing to draw conclusions from the data.  Today we finally move on to machine learning.  Let's talk about machine learning tasks and consider 2 simple approaches - decision trees and the method of nearest neighbors.  We also discuss how to choose a model for specific data using cross-validation. </p><br><p>  <strong>UPD:</strong> now the course is in English under the brand <a href="http://mlcourse.ai/">mlcourse.ai</a> with <a href="https://medium.com/open-machine-learning-course">articles</a> on Medium, and materials on Kaggle ( <a href="https://www.kaggle.com/kashnitsky/mlcourse">Dataset</a> ) and on <a href="">GitHub</a> . </p><br><p>  <a href="https://www.youtube.com/watch%3Fv%3Dp9Hny3Cs6rk">Video recording of the</a> lecture based on this article in the second run of the open course (September-November 2017). </p><a name="habracut"></a><br><div class="spoiler">  <b class="spoiler_title">List of articles series</b> <div class="spoiler_text"><ol><li>  <a href="https://habrahabr.ru/company/ods/blog/322626/">Primary data analysis with Pandas</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/323210/">Visual data analysis with Python</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Classification, decision trees and the method of nearest neighbors</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/323890/">Linear classification and regression models</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/324402/">Compositions: bagging, random forest</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/325422/">Construction and selection of signs</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/325654/">Teaching without a teacher: PCA, clustering</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/326418/">Training in gigabytes with Vowpal Wabbit</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/327242/">Time Series Analysis with Python</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/327250/">Gradient boosting</a> </li></ol></div></div><br><p>  Plan for this article: </p><br><ol><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Introduction</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Decision tree</a> <br><ul><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">How the decision tree is built</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">How a decision tree works with quantitative attributes</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">The main parameters of the tree</a> </li><li> <a href="https://habrahabr.ru/company/ods/blog/322534/">DecisionTreeClassifier class in Scikit-learn</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Decision tree in the regression problem</a> </li></ul></li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Nearest Neighbor Method</a> <br><ul><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">The method of nearest neighbors in real problems</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">KNeighborsClassifier class in Scikit-learn</a> </li></ul></li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Selection of model parameters and cross-validation</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Application examples</a> <br><ul><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Decision trees and the nearest neighbors method in the problem of predicting customer outflow by a telecom operator</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Difficult case for decision trees</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Decision trees and the nearest neighbors method in the MNIST handwriting recognition problem</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Difficult case for the nearest neighbors method</a> </li></ul></li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Pros and cons of decision trees and the method of nearest neighbors</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Homework number 3</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Useful resources</a> </li></ol><br><h1 id="vvedenie">  Introduction </h1><br><p>  Probably, I want to immediately rush into battle, but first we will talk about exactly what task we will solve and what is its place in the field of machine learning. <br>  The classic, general (and not painfully strict) definition of machine learning sounds like (T. Mitchell "Machine learning", 1997): </p><br><blockquote>  they say that a computer program is <em>trained</em> in solving some problem from class <em>T</em> , if its performance, according to metric <em>P</em> , improves with accumulation of experience <em>E.</em> </blockquote><p>  Further, in different scenarios, <em>T, P</em> , and <em>E</em> imply completely different things.  Among the most popular <strong>tasks of <em>T</em> in machine learning</strong> : </p><br><ul><li>  classification - assignment of an object to one of the categories based on its characteristics </li><li>  regression - forecasting the quantitative characteristic of an object based on its other characteristics </li><li>  clustering - splitting a set of objects into groups based on the characteristics of these objects so that within groups the objects are similar to each other, and outside of one group they are less similar </li><li>  detection of anomalies - search for objects "strongly unlike" all others in the sample or some group of objects </li><li>  and many others more specific.  A good overview is given in the chapter "Machine Learning basics" of the book <a href="http://www.deeplearningbook.org/">"Deep Learning"</a> (Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016) </li></ul><br><p>  <strong>Experience <em>E</em></strong> means data (without them nowhere), and depending on this, machine learning algorithms can be divided into those that are trained <em>with a teacher</em> and <em>without a teacher</em> (supervised and unsupervised learning).  In the tasks of learning without a teacher, there is a <em>sample</em> consisting of <em>objects</em> described by a set of <em>features</em> .  In addition to this, for each object of a certain sample, called a <em>training one</em> , the <em>target attribute</em> is known in the tasks of training with a teacher - in fact, this is what I would like to predict for other objects that are not from a training sample. </p><br><h4 id="primer">  Example </h4><br><p>  The tasks of classification and regression are the problems of learning with a teacher.  As an example, we will present the task of credit scoring: on the basis of data accumulated by a credit institution about its customers, we would like to predict loan default.  Here, for the algorithm, experience <em>E</em> is the available training sample: a set of <em>objects</em> (people), each of which is characterized by a set of <em>attributes</em> (such as age, salary, type of credit, defaults in the past, etc.), as well as a <em>target attribute</em> .  If this target sign is simply a fact of non-repayment of a loan (1 or 0, that is, the bank knows about its customers, who repaid the loan and who does not), then this is the (binary) classification task.  If it is known <em>how</em> long the client has delayed the return of the loan and would like to predict the same for new clients, then this will be a regression task. </p><br><p>  Finally, the third abstraction in the definition of machine learning is the <strong>metrics for evaluating the performance of the algorithm <em>P.</em></strong>  Such metrics differ for different tasks and algorithms, and we will talk about them as we study the algorithms.  So far we say that the simplest metric of the quality of the algorithm that solves the classification problem is the proportion of correct answers ( <em>accuracy</em> , do not call it <em>accuracy</em> , this translation is reserved for another metric, <em>precision</em> ) - that is, simply the proportion of correct predictions of the algorithm on the test sample. </p><br><p>  Further we will talk about two tasks of teaching with a teacher: about classification and regression. </p><br><h1 id="derevo-resheniy">  Decision tree </h1><br><p>  Let us begin the review of the methods of classification and regression with one of the most popular - the decision tree.  Decision trees are used in everyday life in the most diverse areas of human activity, sometimes very far from machine learning.  Decision tree can be called visual instruction what to do in what situation.  Let us give an example from the field of consulting research institutes.  The Higher School of Economics publishes info-schemes that make life easier for its employees.  Here is a fragment of the instruction for publishing a scientific article on the institute portal. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/401/8cd/bea/4018cdbea7a64306be94ed784fce4a06.png"></div><br><br><p>  In terms of machine learning, we can say that this is an elementary classifier that determines the form of publication on the portal (book, article, chapter of the book, preprint, publication in the HSE and the Media) on several grounds: type of publication (monograph, brochure, article and etc.), the type of publication in which the article was published (a scientific journal, a collection of papers, etc.) and the rest. </p><br><p>  Often, a decision tree serves as a synthesis of expert experience, a means of transferring knowledge to future employees or a model of a company's business process.  For example, prior to the introduction of scalable machine learning algorithms in the banking sector, the task of credit scoring was solved by experts.  The decision to issue a loan to the borrower was made on the basis of some intuitively (or by experience) derived rules that can be represented as a decision tree. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/194/9b6/ae9/1949b6ae97ab4fc9b1a37fbf182eda8f.gif"></div><br><br><p>  In this case, we can say that the binary classification problem is being solved (the target class has two meanings: ‚ÄúIssue a loan‚Äù and ‚ÄúRefuse‚Äù) according to the signs ‚ÄúAge‚Äù, ‚ÄúHome‚Äù, ‚ÄúIncome‚Äù and ‚ÄúEducation‚Äù. </p><br><p>  A decision tree as an algorithm for machine learning is essentially the same: a combination of logical rules of the form "Meaning of a feature <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-1"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-2">a</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.23ex" height="1.455ex" viewBox="0 -520.7 529.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-1"> a </script>  less <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-3"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-4">x</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.33ex" height="1.455ex" viewBox="0 -520.7 572.5 626.5" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-78" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-2"> x </script>  And Sign Value <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-5"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-6">b</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0.998ex" height="2.057ex" viewBox="0 -780.1 429.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-62" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-3"> b </script>  less <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-7"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-8">y</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.155ex" height="1.817ex" viewBox="0 -520.7 497.5 782.1" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-79" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-4"> y </script>  ... =&gt; Class 1 "in the data structure" Tree ". The tremendous advantage of decision trees is that they are easily interpretable, understandable to man. For example, according to the diagram in the figure above, you can explain to the borrower why he was denied a loan. For example, that he does not have a home and an income of less than 5,000. As we will see later, many other, albeit more accurate, models do not have this property and can rather be viewed as a ‚Äúblack box‚Äù into which they downloaded data and received an answer. "clear" decision trees and their similarity to the model adopted  solutions by man (you can easily explain your model to the boss), decision trees have gained immense popularity, and one of the representatives of this group of classification methods, C4.5, is considered first in the list of the top 10 data mining algorithms ("Top 10 algorithms in data mining", Knowledge and Information Systems, 2008. <a href="http://www.cs.uvm.edu/~icdm/algorithms/10Algorithms-08.pdf">PDF</a> ). </p><br><h3 id="kak-stroitsya-derevo-resheniy">  How the decision tree is built </h3><br><p>  In the example of credit scoring, we saw that the decision to issue a loan was made on the basis of age, availability of real estate, income, and others.  But which sign to choose first?  To do this, consider an example more simply, where all the signs are binary. </p><br><p>  Here you can recall the game "20 questions", which is often mentioned in the introduction to decision trees.  Surely everyone was playing it.  One person makes a celebrity, and the second one tries to guess by asking only questions that can be answered with ‚ÄúYes‚Äù or ‚ÄúNo‚Äù (omitting the options ‚ÄúI don‚Äôt know‚Äù and ‚ÄúI can‚Äôt say‚Äù).  What question guesses ask the first thing?  Of course, one that will reduce the number of remaining options the most.  For example, the question "Is this Angelina Jolie?"  in the case of a negative answer, it will leave more than 7 billion options for further search (of course, smaller, not every person is a celebrity, but still a lot), but the question "Is this a woman?"  already cut off about half of the celebrities.  That is, the sign "gender" is much better shared by a sample of people than the sign "this is Angelina Jolie", "Spanish nationality" or "likes football".  This is intuitively consistent with the concept of information gain based on entropy. </p><br><h4 id="entropiya">  Entropy </h4><br><p>  Shannon entropy is determined for a system with <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-9"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-10">N</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.064ex" height="2.057ex" viewBox="0 -780.1 888.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-4E" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-5"> N </script>  possible states as follows: </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-11"><span class="MJXp-mtext" id="MJXp-Span-12">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-13">L</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-14">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-15">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-16">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-17">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-18">S</span><span class="MJXp-mo" id="MJXp-Span-19" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-20" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-21">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-22">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-23">u</span><span class="MJXp-msubsup" id="MJXp-Span-24"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-25" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-30"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-31">N</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-26"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-27">i</span><span class="MJXp-mo" id="MJXp-Span-28">=</span><span class="MJXp-mn" id="MJXp-Span-29">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-32"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-33" style="margin-right: 0.05em;">p</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-34" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mtext" id="MJXp-Span-35">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-36">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-37">o</span><span class="MJXp-msubsup" id="MJXp-Span-38"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-39" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-40" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-41"><span class="MJXp-msubsup" id="MJXp-Span-42"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-43" style="margin-right: 0.05em;">p</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-44" style="vertical-align: -0.4em;">i</span></span></span><span class="MJXp-mo" id="MJXp-Span-45" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processed" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="30.139ex" height="3.021ex" viewBox="0 -883.9 12976.3 1300.8" role="img" focusable="false" style="vertical-align: -0.969ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-4C" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="931" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="1461" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="1912" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-65" x="2393" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-53" x="2859" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="3782" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2212" x="4839" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-73" x="5867" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-75" x="6337" y="0"></use><g transform="translate(6909,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6D" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-4E" x="1242" y="488"></use><g transform="translate(878,-308)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-69" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="345" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="1124" y="0"></use></g></g><g transform="translate(9036,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-69" x="712" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6C" x="10134" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="10433" y="0"></use><g transform="translate(10918,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="675" y="-213"></use></g><g transform="translate(11849,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-69" x="712" y="-213"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2C" x="12697" y="0"></use></g></svg></span></div><script type="math/tex;mode=display" id="MathJax-Element-6"> \ Large S = - \ sum_ {i = 1} ^ {N} p_i \ log_2 {p_i}, </script></p><br><p>  Where <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-46"><span class="MJXp-msubsup" id="MJXp-Span-47"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-48" style="margin-right: 0.05em;">p</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-49" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.059ex" height="1.817ex" viewBox="-38.5 -520.7 886.3 782.1" role="img" focusable="false" style="vertical-align: -0.607ex; margin-left: -0.089ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-69" x="712" y="-213"></use></g></svg></span><script type="math/tex" id="MathJax-Element-7"> p_i </script>  - probability of finding the system in <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-50"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-51">i</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="0.802ex" height="1.937ex" viewBox="0 -728.2 345.5 834" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-69" x="0" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-8"> i </script>  -th state.  This is a very important concept used in physics, information theory and other fields.  Omitting the premises of the introduction (combinatorial and information-theoretic) of this concept, we note that, intuitively, entropy corresponds to the degree of chaos in the system.  The higher the entropy, the less ordered the system and vice versa.  This will help us to formalize the "effective sampling division," which we talked about in the context of the 20 questions game. </p><br><h4 id="primer-1">  Example </h4><br><p>  To illustrate how entropy will help identify good signs for building a tree, we‚Äôll give the same toy example as in the article <a href="https://habrahabr.ru/post/171759/">"Entropy and Decision Trees"</a> .  We will predict the color of the ball according to its coordinate.  Of course, it has nothing to do with life, but it does allow showing how entropy is used to build a decision tree. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/c96/80a/a4b/c9680aa4babc40f4bbc8b3595e203979.png"></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>  There are 9 blue balls and 11 yellow balls.  If we pulled a ball at random, then with probability <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-52"><span class="MJXp-msubsup" id="MJXp-Span-53"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-54" style="margin-right: 0.05em;">p</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-55" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-56" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-57">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-58">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-59">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-60">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-61">c</span><span class="MJXp-mrow" id="MJXp-Span-62"><span class="MJXp-mn" id="MJXp-Span-63">9</span></span><span class="MJXp-mrow" id="MJXp-Span-64"><span class="MJXp-mn" id="MJXp-Span-65">20</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="14.043ex" height="2.419ex" viewBox="-38.5 -780.1 6046.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex; margin-left: -0.089ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="712" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="1235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="2541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="3091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="3543" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="4072" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-39" x="4506" y="0"></use><g transform="translate(5006,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-9"> p_1 = \ frac {9} {20} </script>  will be blue and with probability <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-66"><span class="MJXp-msubsup" id="MJXp-Span-67"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-68" style="margin-right: 0.05em;">p</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-69" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-70" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-71">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-72">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-73">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-74">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-75">c</span><span class="MJXp-mrow" id="MJXp-Span-76"><span class="MJXp-mn" id="MJXp-Span-77">11</span></span><span class="MJXp-mrow" id="MJXp-Span-78"><span class="MJXp-mn" id="MJXp-Span-79">20</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="15.206ex" height="2.419ex" viewBox="-38.5 -780.1 6547 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex; margin-left: -0.089ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="712" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="1235" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="2541" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="3091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="3543" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="4072" y="0"></use><g transform="translate(4506,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="500" y="0"></use></g><g transform="translate(5507,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-10"> p_2 = \ frac {11} {20} </script>  - yellow.  So the state entropy <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-80"><span class="MJXp-msubsup" id="MJXp-Span-81"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-82" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-83" style="vertical-align: -0.4em;">0</span></span><span class="MJXp-mo" id="MJXp-Span-84" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-85" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-86">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-87">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-88">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-89">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-90">c</span><span class="MJXp-mrow" id="MJXp-Span-91"><span class="MJXp-mn" id="MJXp-Span-92">9</span></span><span class="MJXp-mrow" id="MJXp-Span-93"><span class="MJXp-mn" id="MJXp-Span-94">20</span></span><span class="MJXp-mtext" id="MJXp-Span-95">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-96">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-97">o</span><span class="MJXp-msubsup" id="MJXp-Span-98"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-99" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-100" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-101"><span class="MJXp-mtext" id="MJXp-Span-102">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-103">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-104">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-105">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-106">c</span><span class="MJXp-mrow" id="MJXp-Span-107"><span class="MJXp-mn" id="MJXp-Span-108">9</span></span><span class="MJXp-mrow" id="MJXp-Span-109"><span class="MJXp-mn" id="MJXp-Span-110">20</span></span></span><span class="MJXp-mo" id="MJXp-Span-111" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-112">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-113">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-114">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-115">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-116">c</span><span class="MJXp-mrow" id="MJXp-Span-117"><span class="MJXp-mn" id="MJXp-Span-118">11</span></span><span class="MJXp-mrow" id="MJXp-Span-119"><span class="MJXp-mn" id="MJXp-Span-120">20</span></span><span class="MJXp-mtext" id="MJXp-Span-121">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-122">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-123">o</span><span class="MJXp-msubsup" id="MJXp-Span-124"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-125" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-126" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-127"><span class="MJXp-mtext" id="MJXp-Span-128">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-129">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-130">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-131">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-132">c</span><span class="MJXp-mrow" id="MJXp-Span-133"><span class="MJXp-mn" id="MJXp-Span-134">11</span></span><span class="MJXp-mrow" id="MJXp-Span-135"><span class="MJXp-mn" id="MJXp-Span-136">20</span></span></span><span class="MJXp-mtext" id="MJXp-Span-137">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-138">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-139">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-140">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-141">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-142">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-143">x</span><span class="MJXp-mn" id="MJXp-Span-144">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="65.026ex" height="2.419ex" viewBox="0 -780.1 27997.2 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-53" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="867" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="1345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2212" x="2401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="3429" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="3980" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="4431" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="4961" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-39" x="5394" y="0"></use><g transform="translate(5895,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6C" x="7146" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="7444" y="0"></use><g transform="translate(7930,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="675" y="-213"></use></g><g transform="translate(8861,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="1781" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-39" x="2215" y="0"></use><g transform="translate(2715,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2212" x="12800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="14051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="14601" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="15053" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="15582" y="0"></use><g transform="translate(16016,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="500" y="0"></use></g><g transform="translate(17017,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6C" x="18268" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="18566" y="0"></use><g transform="translate(19052,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="675" y="-213"></use></g><g transform="translate(19983,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="500" y="0"></use></g><g transform="translate(3216,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30" x="500" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="24450" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="24980" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="25483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="25987" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="26438" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-78" x="26924" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="27496" y="0"></use></g></svg></span><script type="math/tex" id="MathJax-Element-11"> S_0 = - \ frac {9} {20} \ log_2 {\ frac {9} {20}} - \ frac {11} {20} \ log_2 {\ frac {11} {20}} \ approx 1 </script>  .  This value itself does not tell us anything yet.  Now let's see how the entropy changes, if the balls are divided into two groups - with a coordinate less than or equal to 12 and more than 12. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/186/444/a8b/186444a8bd0e451c8324ca8529f8d4f4.png"></div><br><br><p>  In the left group were 13 balls, of which 8 are blue and 5 are yellow.  The entropy of this group is equal to <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-145"><span class="MJXp-msubsup" id="MJXp-Span-146"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-147" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-148" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-149" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-150" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-151">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-152">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-153">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-154">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-155">c</span><span class="MJXp-mrow" id="MJXp-Span-156"><span class="MJXp-mn" id="MJXp-Span-157">5</span></span><span class="MJXp-mrow" id="MJXp-Span-158"><span class="MJXp-mn" id="MJXp-Span-159">13</span></span><span class="MJXp-mtext" id="MJXp-Span-160">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-161">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-162">o</span><span class="MJXp-msubsup" id="MJXp-Span-163"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-164" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-165" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-166"><span class="MJXp-mtext" id="MJXp-Span-167">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-168">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-169">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-170">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-171">c</span><span class="MJXp-mrow" id="MJXp-Span-172"><span class="MJXp-mn" id="MJXp-Span-173">5</span></span><span class="MJXp-mrow" id="MJXp-Span-174"><span class="MJXp-mn" id="MJXp-Span-175">13</span></span></span><span class="MJXp-mo" id="MJXp-Span-176" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-177">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-178">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-179">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-180">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-181">c</span><span class="MJXp-mrow" id="MJXp-Span-182"><span class="MJXp-mn" id="MJXp-Span-183">8</span></span><span class="MJXp-mrow" id="MJXp-Span-184"><span class="MJXp-mn" id="MJXp-Span-185">13</span></span><span class="MJXp-mtext" id="MJXp-Span-186">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-187">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-188">o</span><span class="MJXp-msubsup" id="MJXp-Span-189"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-190" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-191" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-192"><span class="MJXp-mtext" id="MJXp-Span-193">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-194">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-195">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-196">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-197">c</span><span class="MJXp-mrow" id="MJXp-Span-198"><span class="MJXp-mn" id="MJXp-Span-199">8</span></span><span class="MJXp-mrow" id="MJXp-Span-200"><span class="MJXp-mn" id="MJXp-Span-201">13</span></span></span><span class="MJXp-mtext" id="MJXp-Span-202">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-203">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-204">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-205">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-206">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-207">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-208">x</span><span class="MJXp-mrow" id="MJXp-Span-209"><span class="MJXp-mo" id="MJXp-Span-210" style="margin-left: 0em; margin-right: 0em;">$</span></span><span class="MJXp-mn" id="MJXp-Span-211">0.9</span></span></span><span class="MathJax_SVG MathJax_SVG_Processed" id="MathJax-Element-12-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="65.673ex" height="2.539ex" viewBox="0 -832 28275.7 1093.4" role="img" focusable="false" style="vertical-align: -0.607ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-53" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31" x="867" y="-213"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-3D" x="1345" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2212" x="2401" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="3429" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="3980" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="4431" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="4961" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-35" x="5394" y="0"></use><g transform="translate(5895,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-33" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6C" x="7146" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="7444" y="0"></use><g transform="translate(7930,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="675" y="-213"></use></g><g transform="translate(8861,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="1781" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-35" x="2215" y="0"></use><g transform="translate(2715,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-33" x="500" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2212" x="12800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="14051" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="14601" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="15053" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="15582" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-38" x="16016" y="0"></use><g transform="translate(16516,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-33" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6C" x="17767" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="18066" y="0"></use><g transform="translate(18551,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-67" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-32" x="675" y="-213"></use></g><g transform="translate(19483,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-63" x="1781" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-38" x="2215" y="0"></use><g transform="translate(2715,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-33" x="500" y="0"></use></g></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-61" x="23449" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="23979" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-70" x="24482" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-72" x="24986" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-6F" x="25437" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMATHI-78" x="25923" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-24" x="26495" y="0"></use><g transform="translate(26996,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/ods/blog/322534/&amp;xid=17259,15700019,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgPtyM8iMy2HZIB_6V4zkbmE-tcEg#MJMAIN-39" x="779" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-12"> S_1 = - \ frac {5} {13} \ log_2 {\ frac {5} {13}} - \ frac {8} {13} \ log_2 {\ frac {8} {13}} \ approx $ 0.9</script>  .  In the right group were 7 balls, of which 1 is blue and 6 are yellow.  The entropy of the right group is <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-212"><span class="MJXp-msubsup" id="MJXp-Span-213"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-214" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-215" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-216" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-217" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-218">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-219">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-220">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-221">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-222">c</span><span class="MJXp-mrow" id="MJXp-Span-223"><span class="MJXp-mn" id="MJXp-Span-224">1</span></span><span class="MJXp-mrow" id="MJXp-Span-225"><span class="MJXp-mn" id="MJXp-Span-226">7</span></span><span class="MJXp-mtext" id="MJXp-Span-227">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-228">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-229">o</span><span class="MJXp-msubsup" id="MJXp-Span-230"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-231" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-232" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-233"><span class="MJXp-mtext" id="MJXp-Span-234">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-235">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-236">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-237">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-238">c</span><span class="MJXp-mrow" id="MJXp-Span-239"><span class="MJXp-mn" id="MJXp-Span-240">1</span></span><span class="MJXp-mrow" id="MJXp-Span-241"><span class="MJXp-mn" id="MJXp-Span-242">7</span></span></span><span class="MJXp-mo" id="MJXp-Span-243" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-244">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-245">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-246">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-247">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-248">c</span><span class="MJXp-mrow" id="MJXp-Span-249"><span class="MJXp-mn" id="MJXp-Span-250">6</span></span><span class="MJXp-mrow" id="MJXp-Span-251"><span class="MJXp-mn" id="MJXp-Span-252">7</span></span><span class="MJXp-mtext" id="MJXp-Span-253">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-254">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-255">o</span><span class="MJXp-msubsup" id="MJXp-Span-256"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-257" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-258" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-259"><span class="MJXp-mtext" id="MJXp-Span-260">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-261">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-262">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-263">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-264">c</span><span class="MJXp-mrow" id="MJXp-Span-265"><span class="MJXp-mn" id="MJXp-Span-266">6</span></span><span class="MJXp-mrow" id="MJXp-Span-267"><span class="MJXp-mn" id="MJXp-Span-268">7</span></span></span><span class="MJXp-mtext" id="MJXp-Span-269">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-270">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-271">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-272">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-273">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-274">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-275">x</span><span class="MJXp-mn" id="MJXp-Span-276">0.6</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-13-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-13"> S_2 = - \ frac {1} {7} \ log_2 {\ frac {1} {7}} - \ frac {6} {7} \ log_2 {\ frac {6} {7}} \ approx 0.6 </script>  .  As you can see, the entropy decreased in both groups as compared with the initial state, although not much in the left state.  Since entropy is essentially a degree of chaos (or uncertainty) in a system, a decrease in entropy is called an increase in information.  Formally, the increase in information (information gain, IG) when dividing the sample on the basis of <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-277"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-278">Q</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-14-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-14"> Q </script>  (in our example, this is a sign " <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-279"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-280">x</span><span class="MJXp-mtext" id="MJXp-Span-281">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-282">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-283">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-284">q</span><span class="MJXp-mn" id="MJXp-Span-285">12</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-15-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-15"> x \ leq 12 </script>  ") is defined as </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-286"><span class="MJXp-mtext" id="MJXp-Span-287">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-288">L</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-289">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-290">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-291">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-292">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-293">I</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-294">G</span><span class="MJXp-mo" id="MJXp-Span-295" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-296">Q</span><span class="MJXp-mo" id="MJXp-Span-297" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-298" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-299"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-300" style="margin-right: 0.05em;">S</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-301" style="vertical-align: -0.4em;">O</span></span><span class="MJXp-mo" id="MJXp-Span-302" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-303">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-304">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-305">u</span><span class="MJXp-msubsup" id="MJXp-Span-306"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-307" style="margin-right: 0.05em;">m</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-312"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-313">q</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-308"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-309">i</span><span class="MJXp-mo" id="MJXp-Span-310">=</span><span class="MJXp-mn" id="MJXp-Span-311">1</span></span></span></span></span></span></span><span class="MJXp-mtext" id="MJXp-Span-314">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-315">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-316">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-317">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-318">c</span><span class="MJXp-mrow" id="MJXp-Span-319"><span class="MJXp-msubsup" id="MJXp-Span-320"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-321" style="margin-right: 0.05em;">N</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-322" style="vertical-align: -0.4em;">i</span></span></span><span class="MJXp-mrow" id="MJXp-Span-323"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-324">N</span></span><span class="MJXp-msubsup" id="MJXp-Span-325"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-326" style="margin-right: 0.05em;">S</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-327" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-328" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-16-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-16"> \ Large IG (Q) = S_O - \ sum_ {i = 1} ^ {q} \ frac {N_i} {N} S_i, </script></p><br><p>  Where <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-329"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-330">q</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-17-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-17"> q </script>  - the number of groups after splitting, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-331"><span class="MJXp-msubsup" id="MJXp-Span-332"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-333" style="margin-right: 0.05em;">N</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-334" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-18-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-18"> N_i </script>  - the number of sample elements in which the sign <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-335"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-336">Q</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-19-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-19"> Q </script>  It has <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-337"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-338">i</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-20-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-20"> i </script>  th value.  In our case, after the separation, we got two groups ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-339"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-340">q</span><span class="MJXp-mo" id="MJXp-Span-341" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-342">2</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-21-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-21"> q = 2 </script>  ) - one of 13 elements ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-343"><span class="MJXp-msubsup" id="MJXp-Span-344"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-345" style="margin-right: 0.05em;">N</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-346" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-347" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-348">13</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-22-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-22"> N_1 = 13 </script>  ), the second - from 7 ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-349"><span class="MJXp-msubsup" id="MJXp-Span-350"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-351" style="margin-right: 0.05em;">N</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-352" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-353" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-354">7</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-23-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-23"> N_2 = 7 </script>  ).  The increase in information turned out </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-355"><span class="MJXp-mtext" id="MJXp-Span-356">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-357">L</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-358">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-359">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-360">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-361">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-362">I</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-363">G</span><span class="MJXp-mo" id="MJXp-Span-364" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-365">x</span><span class="MJXp-mtext" id="MJXp-Span-366">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-367">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-368">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-369">q</span><span class="MJXp-mn" id="MJXp-Span-370">12</span><span class="MJXp-mo" id="MJXp-Span-371" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-372" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-373"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-374" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-375" style="vertical-align: -0.4em;">0</span></span><span class="MJXp-mo" id="MJXp-Span-376" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-377">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-378">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-379">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-380">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-381">c</span><span class="MJXp-mrow" id="MJXp-Span-382"><span class="MJXp-mn" id="MJXp-Span-383">13</span></span><span class="MJXp-mrow" id="MJXp-Span-384"><span class="MJXp-mn" id="MJXp-Span-385">20</span></span><span class="MJXp-msubsup" id="MJXp-Span-386"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-387" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-388" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-389" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-390">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-391">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-392">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-393">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-394">c</span><span class="MJXp-mrow" id="MJXp-Span-395"><span class="MJXp-mn" id="MJXp-Span-396">7</span></span><span class="MJXp-mrow" id="MJXp-Span-397"><span class="MJXp-mn" id="MJXp-Span-398">20</span></span><span class="MJXp-msubsup" id="MJXp-Span-399"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-400" style="margin-right: 0.05em;">S</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-401" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mtext" id="MJXp-Span-402">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-403">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-404">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-405">p</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-406">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-407">o</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-408">x</span><span class="MJXp-mn" id="MJXp-Span-409">0.16.</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-24-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-24"> \ Large IG (x \ leq 12) = S_0 - \ frac {13} {20} S_1 - \ frac {7} {20} S_2 \ approx 0.16. </script></p><br><p>  It turns out that by dividing the balls into two groups on the basis of "the coordinate is less or equal to 12", we have already received a more ordered system than at the beginning.  We continue the division of the balls into groups until the balls in each group are of the same color. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/dae/a88/2b0/daea882b0a8e4ef4b23325c88f0353a1.png"></div><br><br><p>  For the right group, it took only one additional splitting on the basis of "the coordinate is less or equal to 18", for the left group - three more.  Obviously, the entropy of a group with balls of the same color is 0 ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-410"><span class="MJXp-mtext" id="MJXp-Span-411">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-412">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-413">o</span><span class="MJXp-msubsup" id="MJXp-Span-414"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-415" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-416" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-417"><span class="MJXp-mn" id="MJXp-Span-418">1</span></span><span class="MJXp-mo" id="MJXp-Span-419" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-420">0</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-25-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-25"> \ log_2 {1} = 0 </script>  ), which corresponds to the notion that a group of balls of the same color is ordered. <br>  As a result, we built a decision tree that predicts the color of the ball according to its coordinate.  Note that such a decision tree can work poorly for new objects (determining the color of new balls), since it ideally adjusted to the training sample (the original 20 balls).  To classify new balls, a tree with a smaller number of ‚Äúquestions‚Äù or divisions will be better suited, even if it does not ideally divide the training set by color.  This problem, retraining, we will look further. </p><br><h3 id="algoritm-postroeniya-dereva">  Tree building algorithm </h3><br><p>  One can be convinced that the tree constructed in the previous example is in a certain sense optimal - only 5 "questions" were required (conditions for the sign <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-421"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-422">x</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-26-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-26"> x </script>  ) to "fit" the decision tree under the training sample, that is, so that the tree correctly classifies any training object.  Under other conditions of sampling separation, the tree will turn out deeper. </p><br><p>  The basis of popular decision-making algorithms, such as ID3 and C4.5, is the principle of greedily maximizing the growth of information - at each step, the feature is chosen, when divided into which information growth is greatest.  Then the procedure is repeated recursively until the entropy is zero or some small value (unless the tree is ideally fitted to the training set in order to avoid retraining). <br>  Different algorithms use different heuristics for "early stopping" or "clipping" to avoid building a retrained tree. </p><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">build</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(L)</span></span></span><span class="hljs-function">:</span></span> create node t <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> the stopping criterion <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>: assign a predictive model to t <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: Find the best binary split L = L_left + L_right t.left = build(L_left) t.right = build(L_right) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> t</code> </pre> <br><h3 id="drugie-kriterii-kachestva-razbieniya-v-zadache-klassifikacii">  Other breakdown quality criteria in the classification task </h3><br><p>  We understand how the concept of entropy allows us to formalize the idea of ‚Äã‚Äãthe quality of a partition in a tree.  But this is just a heuristic, there are others: </p><br><ul><li>  Gini impurity: <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-423"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-424">G</span><span class="MJXp-mo" id="MJXp-Span-425" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-426">1</span><span class="MJXp-mo" id="MJXp-Span-427" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-428">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-429">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-430">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-431">m</span><span class="MJXp-mtext" id="MJXp-Span-432">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-433">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-434">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-435">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-436">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-437">t</span><span class="MJXp-msubsup" id="MJXp-Span-438"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-439" style="margin-right: 0.05em;">s</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-440" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-mo" id="MJXp-Span-441" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-442"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-443" style="margin-right: 0.05em;">p</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-444" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-msubsup" id="MJXp-Span-445"><span class="MJXp-mo" id="MJXp-Span-446" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-447" style="vertical-align: 0.5em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-27-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-27"> G = 1 - \ sum \ limits_k (p_k) ^ 2 </script>  .  The maximization of this criterion can be interpreted as maximizing the number of pairs of objects of the same class that are in the same subtree.  You can learn more about this (as well as about many other things) from the <a href="https://github.com/esokolov/ml-course-hse">repository of</a> Evgeny Sokolov.  Not to be confused with the Gini index!  Read more about this confusion - the <a href="https://alexanderdyakonov.wordpress.com/2015/12/15/%25D0%25B7%25D0%25BD%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25BC%25D1%258C%25D1%2582%25D0%25B5%25D1%2581%25D1%258C-%25D0%25B4%25D0%25B6%25D0%25B8%25D0%25BD%25D0%25B8/">blog post</a> Alexander Dyakonov </li><li>  Misclassification error: <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-448"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-449">E</span><span class="MJXp-mo" id="MJXp-Span-450" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-451">1</span><span class="MJXp-mo" id="MJXp-Span-452" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-453">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-454">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-455">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-456">x</span><span class="MJXp-mtext" id="MJXp-Span-457">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-458">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-459">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-460">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-461">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-462">t</span><span class="MJXp-msubsup" id="MJXp-Span-463"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-464" style="margin-right: 0.05em;">s</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-465" style="vertical-align: -0.4em;">k</span></span><span class="MJXp-msubsup" id="MJXp-Span-466"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-467" style="margin-right: 0.05em;">p</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-468" style="vertical-align: -0.4em;">k</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-28-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-28"> E = 1 - \ max \ limits_k p_k </script></li></ul><br><p>  In practice, the classification error is almost not used, and the Gini uncertainty and the increase in information work almost equally. </p><br><p>  In the case of a binary classification problem ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-469"><span class="MJXp-msubsup" id="MJXp-Span-470"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-471" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-472" style="vertical-align: -0.4em;">+</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-29-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-29"> p _ + </script>  - the probability of an object to have a label +) entropy and Gini uncertainty will take the following form: <br><br></p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-473"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-474">S</span><span class="MJXp-mo" id="MJXp-Span-475" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-476" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-477"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-478" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-479" style="vertical-align: -0.4em;">+</span></span><span class="MJXp-mtext" id="MJXp-Span-480">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-481">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-482">o</span><span class="MJXp-msubsup" id="MJXp-Span-483"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-484" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-485" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-486"><span class="MJXp-msubsup" id="MJXp-Span-487"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-488" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-489" style="vertical-align: -0.4em;">+</span></span></span><span class="MJXp-mo" id="MJXp-Span-490" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-491"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-492" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-493" style="vertical-align: -0.4em;">‚àí</span></span><span class="MJXp-mtext" id="MJXp-Span-494">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-495">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-496">o</span><span class="MJXp-msubsup" id="MJXp-Span-497"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-498" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-499" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-500"><span class="MJXp-msubsup" id="MJXp-Span-501"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-502" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-503" style="vertical-align: -0.4em;">‚àí</span></span></span><span class="MJXp-mo" id="MJXp-Span-504" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mo" id="MJXp-Span-505" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-506"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-507" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-508" style="vertical-align: -0.4em;">+</span></span><span class="MJXp-mtext" id="MJXp-Span-509">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-510">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-511">o</span><span class="MJXp-msubsup" id="MJXp-Span-512"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-513" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-514" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-515"><span class="MJXp-msubsup" id="MJXp-Span-516"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-517" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-518" style="vertical-align: -0.4em;">+</span></span></span><span class="MJXp-mo" id="MJXp-Span-519" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mo" id="MJXp-Span-520" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-521">1</span><span class="MJXp-mo" id="MJXp-Span-522" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-523"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-524" style="margin-right: 0.05em;">p</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-525" style="vertical-align: -0.4em;"><span class="MJXp-mo" id="MJXp-Span-526">+</span></span></span><span class="MJXp-mo" id="MJXp-Span-527" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mtext" id="MJXp-Span-528">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-529">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-530">o</span><span class="MJXp-msubsup" id="MJXp-Span-531"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-532" style="margin-right: 0.05em;">g</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-533" style="vertical-align: -0.4em;">2</span></span><span class="MJXp-mrow" id="MJXp-Span-534"><span class="MJXp-mo" id="MJXp-Span-535" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-536">1</span><span class="MJXp-mo" id="MJXp-Span-537" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-538"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-539" style="margin-right: 0.05em;">p</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-540" style="vertical-align: -0.4em;"><span class="MJXp-mo" id="MJXp-Span-541">+</span></span></span><span class="MJXp-mo" id="MJXp-Span-542" style="margin-left: 0em; margin-right: 0em;">)</span></span><span class="MJXp-mo" id="MJXp-Span-543" style="margin-left: 0em; margin-right: 0.222em;">;</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-30-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-30"> S = -p_ + \ log_2 {p_ +} -p_- \ log_2 {p_-} = -p_ + \ log_2 {p_ +} - (1 - p _ {+}) \ log_2 {(1 - p _ {+} )}; </script></p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math MJXp-display" id="MJXp-Span-544"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-545">G</span><span class="MJXp-mo" id="MJXp-Span-546" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-547">1</span><span class="MJXp-mo" id="MJXp-Span-548" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-549"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-550" style="margin-right: 0.05em;">p</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-552">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mo" id="MJXp-Span-551">+</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-553" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-554"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-555" style="margin-right: 0.05em;">p</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-557">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mo" id="MJXp-Span-556">‚àí</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-558" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-559">1</span><span class="MJXp-mo" id="MJXp-Span-560" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-561"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-562" style="margin-right: 0.05em;">p</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mn" id="MJXp-Span-564">2</span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mo" id="MJXp-Span-563">+</span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-565" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mo" id="MJXp-Span-566" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-567">1</span><span class="MJXp-mo" id="MJXp-Span-568" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-569"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-570" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-571" style="vertical-align: -0.4em;">+</span></span><span class="MJXp-msubsup" id="MJXp-Span-572"><span class="MJXp-mo" id="MJXp-Span-573" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-574" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-575" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mn" id="MJXp-Span-576">2</span><span class="MJXp-msubsup" id="MJXp-Span-577"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-578" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-579" style="vertical-align: -0.4em;">+</span></span><span class="MJXp-mo" id="MJXp-Span-580" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mn" id="MJXp-Span-581">1</span><span class="MJXp-mo" id="MJXp-Span-582" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-583"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-584" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-585" style="vertical-align: -0.4em;">+</span></span><span class="MJXp-mo" id="MJXp-Span-586" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-587" style="margin-left: 0em; margin-right: 0.222em;">.</span></span></span><div class="MathJax_SVG_Display MathJax_SVG_Processing"><span class="MathJax_SVG" id="MathJax-Element-31-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span></div><script type="math/tex;mode=display" id="MathJax-Element-31"> G = 1 - p _ + ^ 2 - p _- ^ 2 = 1 - p _ + ^ 2 - (1 - p _ +) ^ 2 = 2p _ + (1-p _ +). </script></p><br><p>  When we build graphs of two functions of the argument <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-588"><span class="MJXp-msubsup" id="MJXp-Span-589"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-590" style="margin-right: 0.05em;">p</span><span class="MJXp-mo MJXp-script" id="MJXp-Span-591" style="vertical-align: -0.4em;">+</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-32-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-32"> p _ + </script>  then we will see that the entropy graph is very close to the Gini double uncertainty graph, and therefore in practice these two criteria ‚Äúwork‚Äù almost equally. </p><br><div class="spoiler">  <b class="spoiler_title">Import Libraries</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> __future__ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> division, print_function <span class="hljs-comment"><span class="hljs-comment">#    Anaconda import warnings warnings.filterwarnings('ignore') import numpy as np import pandas as pd %matplotlib inline import seaborn as sns from matplotlib import pyplot as plt</span></span></code> </pre> </div></div><br><div class="spoiler">  <b class="spoiler_title">Picture drawing</b> <div class="spoiler_text"><pre> <code class="python hljs">plt.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>) xx = np.linspace(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">50</span></span>) plt.plot(xx, [<span class="hljs-number"><span class="hljs-number">2</span></span> * x * (<span class="hljs-number"><span class="hljs-number">1</span></span>-x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xx], label=<span class="hljs-string"><span class="hljs-string">'gini'</span></span>) plt.plot(xx, [<span class="hljs-number"><span class="hljs-number">4</span></span> * x * (<span class="hljs-number"><span class="hljs-number">1</span></span>-x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xx], label=<span class="hljs-string"><span class="hljs-string">'2*gini'</span></span>) plt.plot(xx, [-x * np.log2(x) - (<span class="hljs-number"><span class="hljs-number">1</span></span>-x) * np.log2(<span class="hljs-number"><span class="hljs-number">1</span></span> - x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xx], label=<span class="hljs-string"><span class="hljs-string">'entropy'</span></span>) plt.plot(xx, [<span class="hljs-number"><span class="hljs-number">1</span></span> - max(x, <span class="hljs-number"><span class="hljs-number">1</span></span>-x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xx], label=<span class="hljs-string"><span class="hljs-string">'missclass'</span></span>) plt.plot(xx, [<span class="hljs-number"><span class="hljs-number">2</span></span> - <span class="hljs-number"><span class="hljs-number">2</span></span> * max(x, <span class="hljs-number"><span class="hljs-number">1</span></span>-x) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> x <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xx], label=<span class="hljs-string"><span class="hljs-string">'2*missclass'</span></span>) plt.xlabel(<span class="hljs-string"><span class="hljs-string">'p+'</span></span>) plt.ylabel(<span class="hljs-string"><span class="hljs-string">'criterion'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'     p+ ( )'</span></span>) plt.legend();</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/a88/bc3/e18/a88bc3e185b246e088a4382e212e4473.png"></div><br><br><h4 id="primer-2">  Example </h4><br><p>  Consider an example of using the decision tree from the Scikit-learn library for synthetic data.  Two classes will be generated from two normal distributions with different means. </p><br><div class="spoiler">  <b class="spoiler_title">Code to generate data</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   np.seed = 7 train_data = np.random.normal(size=(100, 2)) train_labels = np.zeros(100) #    train_data = np.r_[train_data, np.random.normal(size=(100, 2), loc=2)] train_labels = np.r_[train_labels, np.ones(100)]</span></span></code> </pre> </div></div><br><p>  Display the data.  Informally, the task of classification in this case is to build some kind of "good" border separating 2 classes (red points from yellow ones).  If to exaggerate, machine learning in this case comes down to how to choose a good dividing border.  Perhaps the straight line will be too simple a boundary, and some complex curve that goes around every red dot will be too complicated and we will make a lot of mistakes on new examples from the same distribution from which the training sample came.  Intuition tells you that some kind of <em>smooth</em> border separating 2 classes, or at least just a straight line (in <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-592"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-593">n</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-33-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-33"> n </script>  -dimensional case - hyperplane). </p><br><div class="spoiler">  <b class="spoiler_title">Picture drawing</b> <div class="spoiler_text"><pre> <code class="python hljs">plt.rcParams[<span class="hljs-string"><span class="hljs-string">'figure.figsize'</span></span>] = (<span class="hljs-number"><span class="hljs-number">10</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>) plt.scatter(train_data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], train_data[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=train_labels, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>, edgecolors=<span class="hljs-string"><span class="hljs-string">'black'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">1.5</span></span>); plt.plot(range(<span class="hljs-number"><span class="hljs-number">-2</span></span>,<span class="hljs-number"><span class="hljs-number">5</span></span>), range(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">-3</span></span>,<span class="hljs-number"><span class="hljs-number">-1</span></span>));</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/987/707/6e8/9877076e87ac410b8e40eedc77a17a99.png"></div><br><br><p>  Let's try to separate these two classes, having trained a decision tree.  In the tree, we will use the <code>max_depth</code> parameter, which limits the depth of the tree.  Visualize the resulting class separation boundary. </p><br><div class="spoiler">  <b class="spoiler_title">Code for learning the tree and drawing its dividing border</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTreeClassifier <span class="hljs-comment"><span class="hljs-comment">#   ,       . def get_grid(data): x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1 y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1 return np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01)) #  min_samples_leaf ,     #        clf_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=17) #   clf_tree.fit(train_data, train_labels) #       xx, yy = get_grid(train_data) predicted = clf_tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) plt.pcolormesh(xx, yy, predicted, cmap='autumn') plt.scatter(train_data[:, 0], train_data[:, 1], c=train_labels, s=100, cmap='autumn', edgecolors='black', linewidth=1.5);</span></span></code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/560/d97/0ca/560d970caaf749fda34bd8417160ed7e.png"></div><br><br><p>  And what does a self-made tree look like?  We see that the tree "cuts" the space into 7 rectangles (there are 7 leaves in the tree).  In each such rectangle, the tree forecast will be constant, according to the prevalence of objects of a particular class. </p><br><div class="spoiler">  <b class="spoiler_title">Code to display the tree</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  .dot     from sklearn.tree import export_graphviz export_graphviz(clf_tree, feature_names=['x1', 'x2'], out_file='../../img/small_tree.dot', filled=True) #     pydot (pip install pydot) !dot -Tpng '../../img/small_tree.dot' -o '../../img/small_tree.png'</span></span></code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/bf1/1fe/490/bf11fe49088f428996a27b0d2d2a6592.png"></div><br><br><p>  How does a tree ‚Äúread‚Äù? </p><br><p>  In the beginning there were 200 objects, 100 - of one class and 100 - of another.  The entropy of the initial state was maximum - 1. Then, the objects were divided into 2 groups depending on the comparison of the sign <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-594"><span class="MJXp-msubsup" id="MJXp-Span-595"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-596" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-597" style="vertical-align: -0.4em;">1</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-34-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-34"> x_1 </script>  with value <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-598"><span class="MJXp-mn" id="MJXp-Span-599">0.3631</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-35-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-35"> 0.3631 </script>  (find this section of the border in the figure above, before the tree).  At the same time, the entropy in the left and in the right group of objects decreased.  And so on, the tree is built to a depth of 3. With such visualization, the more objects of one class, the vertex color is closer to dark orange and, conversely, the more objects of the second class, the closer the color to dark blue.  At the beginning of the objects of one class equally, therefore the root of the tree is white. </p><br><h3 id="kak-derevo-resheniy-rabotaet-s-kolichestvennymi-priznakami">  How a decision tree works with quantitative attributes </h3><br><p>  Suppose there is a quantitative attribute "Age" in the sample, which has many unique values.  The decision tree will look for the best (according to the criterion of the type of information growth) sampling, checking binary signs such as "Age &lt;17", "Age &lt;22.87", etc.  But what if there are too many of these "cuts" of age?  And what if there is still a quantitative sign "Salary", and the salary can also be "cut" in a large number of ways?  It turns out too many binary features to select the best at each step of building a tree.  To solve this problem, heuristics are used to limit the number of thresholds with which we compare a quantitative trait. </p><br><p>  Consider this on a toy example.  Let there be the following sample: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/5e0/213/081/5e0213081b034e63aa76e2086e521519.png" height="80%"></div><br><br><p>  Sort it by increasing age. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/10b/d4a/dbf/10bd4adbf3804c3bbd9443cbd2ac7539.png" height="80%"></div><br><br><p>  Let's teach a decision tree on this data (without limiting the depth) and look at it. </p><br><div class="spoiler">  <b class="spoiler_title">Code for learning and drawing a tree</b> <div class="spoiler_text"><pre> <code class="python hljs">age_tree = DecisionTreeClassifier(random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) age_tree.fit(data[<span class="hljs-string"><span class="hljs-string">''</span></span>].values.reshape(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>), data[<span class="hljs-string"><span class="hljs-string">' '</span></span>].values) export_graphviz(age_tree, feature_names=[<span class="hljs-string"><span class="hljs-string">''</span></span>], out_file=<span class="hljs-string"><span class="hljs-string">'../../img/age_tree.dot'</span></span>, filled=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) !dot -Tpng <span class="hljs-string"><span class="hljs-string">'../../img/age_tree.dot'</span></span> -o <span class="hljs-string"><span class="hljs-string">'../../img/age_tree.png'</span></span></code> </pre> </div></div><br><p>  In the picture below we see that the tree used 5 values ‚Äã‚Äãwith which the age is compared: 43.5, 19, 22.5, 30 and 32 years.  If you look closely, then this is exactly the average values ‚Äã‚Äãbetween the ages at which the target class "changes" from 1 to 0 or vice versa.  Difficult phrase, so an example: 43.5 is the average between 38 and 49 years old, the client who was not returned a loan for 38 years, and the one to whom 49 returned.  Likewise, 19 years is an average between 18 and 20 years.  That is, as thresholds for ‚Äúcutting‚Äù a quantitative trait, the tree ‚Äúlooks‚Äù at those values ‚Äã‚Äãat which the target class changes its value. </p><br><p>  Think about why it does not make sense in this case to consider the sign "Age &lt;17.5". </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/10b/d4a/dbf/10bd4adbf3804c3bbd9443cbd2ac7539.png" height="80%"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/files/1dc/56d/fce/1dc56dfcee144e0db7043f6752d40360.png"></div><br><br><p>  Consider an example more complicated: add the sign "Salary" (thousand rubles / month). </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/dc6/073/7bd/dc60737bd1c0488f8c8b4df02d8c621b.png" height="80%"></div><br><br><p>  If you sort by age, the target class ("Loan non-repayment") changes (from 1 to 0 or vice versa) 5 times.  And if you sort by salary - then 7 times.  How will the tree now select signs?  We'll see. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/67f/77a/2fa/67f77a2fa24441f198c2deccb1d8c9c3.png" height="80%"></div><br><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/cee/a9c/d76/ceea9cd76cee4fe2a8fde36be1af3e2d.png" height="80%"></div><br><br><div class="spoiler">  <b class="spoiler_title">Code for learning and drawing a tree</b> <div class="spoiler_text"><pre> <code class="python hljs">age_sal_tree = DecisionTreeClassifier(random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) age_sal_tree.fit(data2[[<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>]].values, data2[<span class="hljs-string"><span class="hljs-string">' '</span></span>].values); export_graphviz(age_sal_tree, feature_names=[<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">''</span></span>], out_file=<span class="hljs-string"><span class="hljs-string">'../../img/age_sal_tree.dot'</span></span>, filled=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) !dot -Tpng <span class="hljs-string"><span class="hljs-string">'../../img/age_sal_tree.dot'</span></span> -o <span class="hljs-string"><span class="hljs-string">'../../img/age_sal_tree.png'</span></span></code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/4a6/c17/1e0/4a6c171e06324bb2afee3c76eb6bb226.png" height="80%"></div><br><br><p>  We see that the tree involved as a breakdown of age and salary.  And the thresholds with which the signs are compared: 43.5 and 22.5 years - for the age and 95 and 30.5 thousand rubles / month - for the salary.  And again it can be noted that 95 thousand is the average between 88 and 102, while the person with a salary of 88 turned out to be ‚Äúbad‚Äù, and from 102 - ‚Äúgood‚Äù.  The same is true for 30.5 thousand. That is, the comparison of wages and age did not go with all possible values, but only with a few.  Why did these signs appear in the tree?  Because the breaks for them turned out to be better (according to the Gini criterion of uncertainty). </p><br><p>  <strong>Conclusion:</strong> the simplest heuristics for processing quantitative attributes in the decision tree: the quantitative attribute is sorted in ascending order, and only those thresholds are checked in the tree at which the target characteristic changes value.  It does not sound very strict, but I hope I brought the meaning with the help of toy examples. </p><br><p>  In addition, when there are many quantitative features in the data, and each has many unique values, not all the thresholds described above can be selected, but only the top-N, which give the maximum gain of the same criterion.  That is, in fact, for each threshold, a tree of depth 1 is built, it is considered how much the entropy (or Gini uncertainty) has decreased and only the best thresholds are selected with which to compare the quantitative trait. </p><br><p>  For illustration: when splitting on the basis of "Salary <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-600"><span class="MJXp-mtext" id="MJXp-Span-601">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-602">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-603">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-604">q</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-36-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-36"> \ leq </script>  34.5 "entropy 0 in the left subgroup (all clients are" bad "), and in the right subgroup - 0.954 (3" bad "and 5" good ", you can check, 1 part of the homework will be just to sort out the construction of trees ). The increase in information is obtained approximately 0.3. <br>  And when splitting on the basis of "Salary <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-605"><span class="MJXp-mtext" id="MJXp-Span-606">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-607">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-608">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-609">q</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-37-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-37"> \ leq </script>  95 "in the left subgroup is entropy 0.97 (6" bad "and 4" good "), and in the right - 0 (only one object). The increase in information is approximately 0.11. <br>  Considering the increase in information for each partition in this way, it is possible to select, before building a large tree (for all attributes), thresholds with which each quantitative attribute will be compared. </p><br><p>  More examples of quantifying quantitative traits can be viewed in posts similar to <a href="http://kevinmeurer.com/a-simple-guide-to-entropy-based-discretization/">this</a> or <a href="http://clear-lines.com/blog/post/Discretizing-a-continuous-variable-using-Entropy.aspx">that</a> .  One of the most famous scientific articles on this topic is "On the handling of continuous-valued attributes in decision tree generation" (UM Fayyad. KB Irani, "Machine Learning", 1992). </p><br><h3 id="osnovnye-parametry-dereva">  The main parameters of the tree </h3><br><p>  In principle, a decision tree can be built to such a depth that there is exactly one object in each sheet.  But in practice this is not done (if only one tree is built) due to the fact that such a tree will be <em>retrained</em> - it will be too tuned in to the training set and will not work well on the forecast on new data.  Somewhere at the bottom of the tree, at a great depth, breaks will appear on less important grounds (for example, whether a client came from Saratov or Kostroma).  If you exaggerate, it may turn out that of all 4 clients who came to the bank for a loan in green pants, no one returned the loan.  But we do not want our classification model to generate such specific rules. </p><br><p>  There are two exceptions, situations where trees are built to maximum depth: </p><br><ul><li>  Random forest (the composition of many trees) averages the answers of trees built to the maximum depth (why you should do this, we will understand later) </li><li>  Tree <em>trimming</em> ( <em>pruning</em> ).  With this approach, the tree is first built to the maximum depth, then gradually, from bottom to top, some tree tops are removed by comparing the quality of the tree with this partition and without it (the comparison is carried out using <em>cross-validation</em> , about which just below).  You can read more in the materials of the <a href="https://github.com/esokolov/ml-course-hse">repository of</a> Evgeny Sokolov. </li></ul><br><p>  The image below is an example of a dividing boundary built by a retrained tree. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/f9f/3b5/133/f9f3b5133bae460ba96ab7e546155b1d.png"></div><br><br><p>  The main ways to combat retraining in the case of decision trees are: </p><br><ul><li>  artificial limitation of the depth or the minimum number of objects in the sheet: the tree is simply stopped at some point; </li><li>  tree trimming </li></ul><br><h3 id="klass-decisiontreeclassifier-v-scikit-learn">  DecisionTreeClassifier class in Scikit-learn </h3><br><p>  The main parameters of the class <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"><code>sklearn.tree.DecisionTreeClassifier</code></a> : </p><br><ul><li>  <code>max_depth</code> - maximum tree depth </li><li>  <code>max_features</code> - the maximum number of features by which the best partitioning in the tree is sought (this is necessary because with a large number of attributes it will be ‚Äúexpensive‚Äù to search for the best (by the criterion of the type of information growth) the partitioning among <em>all</em> features) </li><li>  <code>min_samples_leaf</code> - the minimum number of objects in the sheet.  This parameter has a clear interpretation: for example, if it is 5, then the tree will generate only those classifying rules that are valid for at least 5 objects </li></ul><br><p>  Tree parameters must be adjusted depending on the input data, and this is usually done using <em>cross-validation</em> , about it just below. </p><br><h3 id="derevo-resheniy-v-zadache-regressii">  Decision tree in the regression problem </h3><br><p>  When predicting a quantitative trait, the idea of ‚Äã‚Äãbuilding a tree remains the same, but the quality criterion changes: </p><br><ul><li>  Dispersion around the middle: <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-610"><span class="MJXp-mtext" id="MJXp-Span-611">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-612">L</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-613">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-614">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-615">g</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-616">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-617">D</span><span class="MJXp-mo" id="MJXp-Span-618" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-mtext" id="MJXp-Span-619">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-620">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-621">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-622">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-623">c</span><span class="MJXp-mrow" id="MJXp-Span-624"><span class="MJXp-mn" id="MJXp-Span-625">1</span></span><span class="MJXp-mrow" id="MJXp-Span-626"><span class="MJXp-mtext" id="MJXp-Span-627">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-628">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-629">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-630">l</span></span><span class="MJXp-mtext" id="MJXp-Span-631">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-632">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-633">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-634">m</span><span class="MJXp-mtext" id="MJXp-Span-635">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-636">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-637">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-638">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-639">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-640">t</span><span class="MJXp-msubsup" id="MJXp-Span-641"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-642" style="margin-right: 0.05em;">s</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-647"><span class="MJXp-mtext" id="MJXp-Span-648">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-649">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-650">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-651">l</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-643"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-644">i</span><span class="MJXp-mo" id="MJXp-Span-645">=</span><span class="MJXp-mn" id="MJXp-Span-646">1</span></span></span></span></span></span></span><span class="MJXp-mo" id="MJXp-Span-652" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-msubsup" id="MJXp-Span-653"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-654" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-655" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-mo" id="MJXp-Span-656" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mtext" id="MJXp-Span-657">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-658">f</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-659">r</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-660">a</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-661">c</span><span class="MJXp-mrow" id="MJXp-Span-662"><span class="MJXp-mn" id="MJXp-Span-663">1</span></span><span class="MJXp-mrow" id="MJXp-Span-664"><span class="MJXp-mtext" id="MJXp-Span-665">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-666">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-667">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-668">l</span></span><span class="MJXp-mtext" id="MJXp-Span-669">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-670">s</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-671">u</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-672">m</span><span class="MJXp-mtext" id="MJXp-Span-673">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-674">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-675">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-676">m</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-677">i</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-678">t</span><span class="MJXp-msubsup" id="MJXp-Span-679"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-680" style="margin-right: 0.05em;">s</span><span class="MJXp-script-box" style="height: 1.86em; vertical-align: -0.64em;"><span class=" MJXp-script"><span><span style="margin-bottom: -0.25em;"><span class="MJXp-mrow" id="MJXp-Span-685"><span class="MJXp-mtext" id="MJXp-Span-686">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-687">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-688">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-689">l</span></span></span></span></span><span class=" MJXp-script"><span><span style="margin-top: -0.85em;"><span class="MJXp-mrow" id="MJXp-Span-681"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-682">i</span><span class="MJXp-mo" id="MJXp-Span-683">=</span><span class="MJXp-mn" id="MJXp-Span-684">1</span></span></span></span></span></span></span><span class="MJXp-msubsup" id="MJXp-Span-690"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-691" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-692" style="vertical-align: -0.4em;">i</span></span><span class="MJXp-msubsup" id="MJXp-Span-693"><span class="MJXp-mo" id="MJXp-Span-694" style="margin-left: 0em; margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-695" style="vertical-align: 0.5em;">2</span></span><span class="MJXp-mo" id="MJXp-Span-696" style="margin-left: 0em; margin-right: 0.222em;">,</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-38-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-38"> \ Large D = \ frac {1} {\ ell} \ sum \ limits_ {i = 1} ^ {\ ell} (y_i - \ frac {1} {\ ell} \ sum \ limits_ {i = 1} ^ {\ ell} y_i) ^ 2, </script><br>  Where <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-697"><span class="MJXp-mtext" id="MJXp-Span-698">&nbsp;</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-699">e</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-700">l</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-701">l</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-39-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-39"> \ ell </script>  - the number of objects in the sheet, <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-702"><span class="MJXp-msubsup" id="MJXp-Span-703"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-704" style="margin-right: 0.05em;">y</span><span class="MJXp-mi MJXp-italic MJXp-script" id="MJXp-Span-705" style="vertical-align: -0.4em;">i</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-40-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-40"> y_i </script>  - values ‚Äã‚Äãof the target feature.  Simply put, by minimizing the variance around the mean, we look for signs that break the sample in such a way that the values ‚Äã‚Äãof the target characteristic in each sheet are approximately equal. </li></ul><br><h4 id="primer-3">  Example </h4><br><p>  Generate data distributed around the function. <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-706"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-707">f</span><span class="MJXp-mo" id="MJXp-Span-708" style="margin-left: 0em; margin-right: 0em;">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-709">x</span><span class="MJXp-mo" id="MJXp-Span-710" style="margin-left: 0em; margin-right: 0em;">)</span><span class="MJXp-mo" id="MJXp-Span-711" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-712"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-713" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-714" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-715">‚àí</span><span class="MJXp-msubsup" id="MJXp-Span-716"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-717" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-718" style="vertical-align: 0.5em;">2</span></span></span></span><span class="MJXp-mo" id="MJXp-Span-719" style="margin-left: 0.267em; margin-right: 0.267em;">+</span><span class="MJXp-mn" id="MJXp-Span-720">1.5</span><span class="MJXp-mo" id="MJXp-Span-721" style="margin-left: 0.267em; margin-right: 0.267em;">‚àó</span><span class="MJXp-msubsup" id="MJXp-Span-722"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-723" style="margin-right: 0.05em;">e</span><span class="MJXp-mrow MJXp-script" id="MJXp-Span-724" style="vertical-align: 0.5em;"><span class="MJXp-mo" id="MJXp-Span-725">‚àí</span><span class="MJXp-mo" id="MJXp-Span-726">(</span><span class="MJXp-mi MJXp-italic" id="MJXp-Span-727">x</span><span class="MJXp-mo" id="MJXp-Span-728">‚àí</span><span class="MJXp-mn" id="MJXp-Span-729">2</span><span class="MJXp-msubsup" id="MJXp-Span-730"><span class="MJXp-mo" id="MJXp-Span-731" style="margin-right: 0.05em;">)</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-732" style="vertical-align: 0.5em;">2</span></span></span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-41-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-41"> f (x) = e ^ {- x ^ 2} + 1.5 * e ^ {- (x - 2) ^ 2} </script>  with some noise, we will teach them a decision tree and we will depict what forecasts the tree makes. </p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs">n_train = <span class="hljs-number"><span class="hljs-number">150</span></span> n_test = <span class="hljs-number"><span class="hljs-number">1000</span></span> noise = <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">f</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> x = x.ravel() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.exp(-x ** <span class="hljs-number"><span class="hljs-number">2</span></span>) + <span class="hljs-number"><span class="hljs-number">1.5</span></span> * np.exp(-(x - <span class="hljs-number"><span class="hljs-number">2</span></span>) ** <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n_samples, noise)</span></span></span><span class="hljs-function">:</span></span> X = np.random.rand(n_samples) * <span class="hljs-number"><span class="hljs-number">10</span></span> - <span class="hljs-number"><span class="hljs-number">5</span></span> X = np.sort(X).ravel() y = np.exp(-X ** <span class="hljs-number"><span class="hljs-number">2</span></span>) + <span class="hljs-number"><span class="hljs-number">1.5</span></span> * np.exp(-(X - <span class="hljs-number"><span class="hljs-number">2</span></span>) ** <span class="hljs-number"><span class="hljs-number">2</span></span>) + \ np.random.normal(<span class="hljs-number"><span class="hljs-number">0.0</span></span>, noise, n_samples) X = X.reshape((n_samples, <span class="hljs-number"><span class="hljs-number">1</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> X, y X_train, y_train = generate(n_samples=n_train, noise=noise) X_test, y_test = generate(n_samples=n_test, noise=noise) <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTreeRegressor reg_tree = DecisionTreeRegressor(max_depth=<span class="hljs-number"><span class="hljs-number">5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) reg_tree.fit(X_train, y_train) reg_tree_pred = reg_tree.predict(X_test) plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">6</span></span>)) plt.plot(X_test, f(X_test), <span class="hljs-string"><span class="hljs-string">"b"</span></span>) plt.scatter(X_train, y_train, c=<span class="hljs-string"><span class="hljs-string">"b"</span></span>, s=<span class="hljs-number"><span class="hljs-number">20</span></span>) plt.plot(X_test, reg_tree_pred, <span class="hljs-string"><span class="hljs-string">"g"</span></span>, lw=<span class="hljs-number"><span class="hljs-number">2</span></span>) plt.xlim([<span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>]) plt.title(<span class="hljs-string"><span class="hljs-string">"Decision tree regressor, MSE = %.2f"</span></span> % np.sum((y_test - reg_tree_pred) ** <span class="hljs-number"><span class="hljs-number">2</span></span>)) plt.show()</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/856/c8b/9ad/856c8b9ad9094250a9d23e91e6f74e97.png"></div><br><br><p>  We see that the decision tree approximates the dependence in the data by a piecewise constant function. </p><br><h1 id="metod-blizhayshih-sosedey">  Nearest Neighbor Method </h1><br><p>  The Neighbor Neighbor Method (kNN) is also a very popular classification method, also sometimes used in regression tasks.  This, along with the decision tree, is one of the most understandable approaches to classification.  At the level of intuition, the essence of the method is as follows: look at the neighbors that prevail, and so do you.  Formally, the basis of the method is the compactness hypothesis: if the metric of the distance between the examples is introduced quite successfully, then similar examples are more often in the same class than in the other. </p><br><p>  According to the method of the nearest neighbors, the test example (green ball) will be classified as ‚Äúblue‚Äù, not ‚Äúred‚Äù. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/4b8/000/4ab/4b80004ab2414944802677e2e1cb1b76.png"></div><br><br><p>  For example, if you don‚Äôt know what type of product to specify in the ad for a Bluetooth headset, you can find 5 similar headsets, and if 4 of them are categorized as ‚ÄúAccessories‚Äù, and only one - into the category ‚ÄúTechnique‚Äù, common sense will tell your ad also indicate the category "Accessories". </p><br><p>  To classify each of the test sample objects, you must perform the following operations sequentially: </p><br><ul><li>  Calculate the distance to each of the objects of the training sample </li><li>  Take away <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-733"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-734">k</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-42-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-42"> k </script>  objects of the training sample, the distance to which the minimum </li><li>  The class of the object being classified is the class most often found among <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-735"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-736">k</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-43-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-43"> k </script>  closest neighbors </li></ul><br><p>  The method adapts to the regression task quite easily - at step 3, it is not the label that returns, but the number - the average (or median) value of the target attribute among the neighbors. </p><br><p>  A notable feature of this approach is its laziness.  This means that the calculations begin only at the moment of the classification of the test case, and in advance, only if there are training examples, no model is built.  This is different, for example, from the previously considered decision tree, where first a tree is built on the basis of a training sample, and then the test case is classified relatively quickly. </p><br><p>  It is worth noting that the method of nearest neighbors is a well-studied approach (in machine learning, econometrics and statistics, more is known, probably, only about linear regression).  For the method of nearest neighbors, there are quite a few important theorems that state that on "infinite" samples this is the optimal classification method.  The authors of the classic book "The Elements of Statistical Learning" consider kNN to be a theoretically ideal algorithm, the applicability of which is simply limited by computational capabilities and the curse of dimensions. </p><br><h3 id="metod-blizhayshih-sosedey-v-realnyh-zadachah">  The method of nearest neighbors in real problems </h3><br><ul><li>  In its pure form, kNN can serve as a good start (baseline) in solving a problem; </li><li>  In Kaggle competitions, kNN is often used to build meta-attributes (the kNN prediction is input to other models) or in stacking / blending; </li><li>  The idea of ‚Äã‚Äãthe nearest neighbor extends to other tasks, for example, in recommender systems, a simple initial solution may be to recommend some product (or service) popular among the <em>nearest neighbors of the</em> person to whom we want to make a recommendation; </li><li>  In practice, for large samples often use <em>approximate</em> methods of finding the nearest neighbors.  <a href="https://www.youtube.com/watch%3Fv%3DUUm4MOyVTnE">Here is a</a> lecture by Artyom Babenko about efficient algorithms for finding the nearest neighbors among billions of objects in high-dimensional spaces (search by pictures).  Also known are open libraries in which these algorithms are implemented, thanks to Spotify for its <a href="https://github.com/spotify/annoy">Annoy</a> library. </li></ul><br><p>  /       : </p><br><ul><li>   </li><li>     (   ,  ,     ). ,         .  ,   ""     100       ,  ""    100. </li><li>   (       , ,   ,       "") </li></ul><br><h3 id="klass-kneighborsclassifier-v-scikit-learn">  KNeighborsClassifier  Scikit-learn </h3><br><p>    sklearn.neighbors.KNeighborsClassifier: </p><br><ul><li> weights: "uniform" (  ), "distance" (      )      </li><li> algorithm (): "brute", "ball_tree", "KD_tree",  "auto".             .     ‚Äî      ,     .     "auto"           . </li><li> leaf_size ():         BallTree  KDTree    </li><li> metric: "minkowski", "manhattan", "euclidean", "chebyshev"   </li></ul><br><h1 id="vybor-parametrov-modeli-i-kross-validaciya">     - </h1><br><p>     ‚Äì   <em></em> ,       .             (      ,           ),      ,      . </p><br><p>       2 : </p><br><ul><li>   ( <em>held-out/hold-out set</em> ).      -    (   20%  40%),      (60-80%  )       (,   ‚Äì      )   . </li><li> - ( <em>cross-validation</em> ,         ).     ‚Äì K-fold - </li></ul><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/b1d/706/e6c/b1d706e6c9df49c297b6152878a2d03f.png"></div><br><br><p>    <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-737"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-738">K</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-44-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-44"> K </script>    ( <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-739"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-740">K</span><span class="MJXp-mo" id="MJXp-Span-741" style="margin-left: 0.267em; margin-right: 0.267em;">‚àí</span><span class="MJXp-mn" id="MJXp-Span-742">1</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-45-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-45">K-1</script> )    ( ),      (   ,  ). <br>  <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-743"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-744">K</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-46-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-46"> K </script>   ,   ,     /  -. </p><br><p> -             .  -  ,   . </p><br><p> - ‚Äì       (     ),      ,    ,        ..    , , <a href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html"></a>  Sebastian Raschka        ()  </p><br><h1 id="primery-primeneniya">   </h1><br><h3 id="derevya-resheniy-i-metod-blizhayshih-sosedey-v-zadache-prognozirovaniya-ottoka-klientov-telekom-operatora">            - </h3><br><p>    DataFrame   .       Series,    .      ,  ,   . </p><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><pre> <code class="python hljs">df = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'../../data/telecom_churn.csv'</span></span>) df[<span class="hljs-string"><span class="hljs-string">'International plan'</span></span>] = pd.factorize(df[<span class="hljs-string"><span class="hljs-string">'International plan'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>] df[<span class="hljs-string"><span class="hljs-string">'Voice mail plan'</span></span>] = pd.factorize(df[<span class="hljs-string"><span class="hljs-string">'Voice mail plan'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>] df[<span class="hljs-string"><span class="hljs-string">'Churn'</span></span>] = df[<span class="hljs-string"><span class="hljs-string">'Churn'</span></span>].astype(<span class="hljs-string"><span class="hljs-string">'int'</span></span>) states = df[<span class="hljs-string"><span class="hljs-string">'State'</span></span>] y = df[<span class="hljs-string"><span class="hljs-string">'Churn'</span></span>] df.drop([<span class="hljs-string"><span class="hljs-string">'State'</span></span>, <span class="hljs-string"><span class="hljs-string">'Churn'</span></span>], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, inplace=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/978/022/6a8/9780226a800b4a1da342daaa966b4a0e.png"></div><br><br><p>  70%  (X_train, y_train)    30%    (X_holdout, y_holdout).          ,     ,   ,    .  2  ‚Äì    kNN,   ,   ,  :    5,    ‚Äì 10. </p><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split, StratifiedKFold <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KNeighborsClassifier X_train, X_holdout, y_train, y_holdout = train_test_split(df.values, y, test_size=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) tree = DecisionTreeClassifier(max_depth=<span class="hljs-number"><span class="hljs-number">5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number"><span class="hljs-number">10</span></span>) tree.fit(X_train, y_train) knn.fit(X_train, y_train)</code> </pre> </div></div><br><p>         ‚Äì   .     .    :     94%  88%  kNN.       . </p><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score tree_pred = tree.predict(X_holdout) accuracy_score(y_holdout, tree_pred) <span class="hljs-comment"><span class="hljs-comment"># 0.94</span></span></code> </pre> <br><pre> <code class="python hljs">knn_pred = knn.predict(X_holdout) accuracy_score(y_holdout, knn_pred) <span class="hljs-comment"><span class="hljs-comment"># 0.88</span></span></code> </pre> </div></div><br><p>      -.            .  ,   GridSearchCV:       <code>max_depth</code>  <code>max_features</code>   5- -     . </p><br><div class="spoiler"> <b class="spoiler_title">  </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV, cross_val_score</code> </pre> <br><pre> <code class="python hljs">tree_params = {<span class="hljs-string"><span class="hljs-string">'max_depth'</span></span>: range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>), <span class="hljs-string"><span class="hljs-string">'max_features'</span></span>: range(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">19</span></span>)}</code> </pre> <br><pre> <code class="python hljs">tree_grid = GridSearchCV(tree, tree_params, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, verbose=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><pre> <code class="python hljs">tree_grid.fit(X_train, y_train)</code> </pre> <br><p>           -: </p><br><pre> <code class="python hljs">tree_grid.best_params_</code> </pre> <br><p> {'max_depth': 6, 'max_features': 17} </p><br><pre> <code class="python hljs">tree_grid.best_score_</code> </pre> <br><p> 0.94256322331761677 </p><br><pre> <code class="python hljs">accuracy_score(y_holdout, tree_grid.predict(X_holdout))</code> </pre> <br><p> 0.94599999999999995 </p><br><p>        kNN. </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.pipeline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Pipeline <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.preprocessing <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StandardScaler</code> </pre> <br><pre> <code class="python hljs">knn_pipe = Pipeline([(<span class="hljs-string"><span class="hljs-string">'scaler'</span></span>, StandardScaler()), (<span class="hljs-string"><span class="hljs-string">'knn'</span></span>, KNeighborsClassifier(n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>))])</code> </pre> <br><pre> <code class="python hljs">knn_params = {<span class="hljs-string"><span class="hljs-string">'knn__n_neighbors'</span></span>: range(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)}</code> </pre> <br><pre> <code class="python hljs">knn_grid = GridSearchCV(knn_pipe, knn_params, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, verbose=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><pre> <code class="python hljs">knn_grid.fit(X_train, y_train)</code> </pre> <br><pre> <code class="python hljs">knn_grid.best_params_, knn_grid.best_score_</code> </pre> <br><p> ({'knn__n_neighbors': 7}, 0.88598371195885128) </p><br><pre> <code class="python hljs">accuracy_score(y_holdout, knn_grid.predict(X_holdout))</code> </pre> <br><p> 0.89000000000000001 </p></div></div><br><p>       ,    : 94.2%    -  94.6%     88.6% / 89%  kNN.  ,        ,     (      ,    -  ,   )           (95.1%  -  95.3% ‚Äì  ),    . </p><br><div class="spoiler"> <b class="spoiler_title">      </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForestClassifier forest = RandomForestClassifier(n_estimators=<span class="hljs-number"><span class="hljs-number">100</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) print(np.mean(cross_val_score(forest, X_train, y_train, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>))) <span class="hljs-comment"><span class="hljs-comment"># 0.949</span></span></code> </pre> <br><pre> <code class="python hljs">forest_params = {<span class="hljs-string"><span class="hljs-string">'max_depth'</span></span>: range(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">11</span></span>), <span class="hljs-string"><span class="hljs-string">'max_features'</span></span>: range(<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">19</span></span>)}</code> </pre> <br><pre> <code class="python hljs">forest_grid = GridSearchCV(forest, forest_params, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, verbose=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre> <br><pre> <code class="python hljs">forest_grid.fit(X_train, y_train)</code> </pre> <br><pre> <code class="python hljs">forest_grid.best_params_, forest_grid.best_score_ <span class="hljs-comment"><span class="hljs-comment"># ({'max_depth': 9, 'max_features': 6}, 0.951)</span></span></code> </pre> <br><pre> <code class="python hljs">accuracy_score(y_holdout, forest_grid.predict(X_holdout)) <span class="hljs-comment"><span class="hljs-comment"># 0.953</span></span></code> </pre> </div></div><br><p>   . - ,      (  ‚Äì 6),     ,     "",    . </p><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><pre> <code class="python hljs">export_graphviz(tree_grid.best_estimator_, feature_names=df.columns, out_file=<span class="hljs-string"><span class="hljs-string">'../../img/churn_tree.dot'</span></span>, filled=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) !dot -Tpng <span class="hljs-string"><span class="hljs-string">'../../img/churn_tree.dot'</span></span> -o <span class="hljs-string"><span class="hljs-string">'../../img/churn_tree.png'</span></span></code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/b52/84b/4db/b5284b4dbf994192af92808a628b4685.png"></div><br><br><h3 id="slozhnyy-sluchay-dlya-derevev-resheniy">      </h3><br><p>              ,    ,    - "",   .      (2 ),         (+1, ,  -1 ‚Äì ).        ,      ‚Äì   . </p><br><div class="spoiler"> <b class="spoiler_title">     </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">form_linearly_separable_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">500</span></span></span></span><span class="hljs-function"><span class="hljs-params">, x1_min=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, x1_max=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">30</span></span></span></span><span class="hljs-function"><span class="hljs-params">, x2_min=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, x2_max=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">30</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> data, target = [], [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(n): x1, x2 = np.random.randint(x1_min, x1_max), np.random.randint(x2_min, x2_max) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> np.abs(x1 - x2) &gt; <span class="hljs-number"><span class="hljs-number">0.5</span></span>: data.append([x1, x2]) target.append(np.sign(x1 - x2)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.array(data), np.array(target) X, y = form_linearly_separable_data() plt.scatter(X[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=y, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>, edgecolors=<span class="hljs-string"><span class="hljs-string">'black'</span></span>);</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/efe/630/812/efe6308122d24681a635fdf8a6d361d9.png"></div><br><br><p>              .  , ,           <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-745"><span class="MJXp-mn" id="MJXp-Span-746">30</span><span class="MJXp-mo" id="MJXp-Span-747" style="margin-left: 0.267em; margin-right: 0.267em;">√ó</span><span class="MJXp-mn" id="MJXp-Span-748">30</span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-47-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-47">30 \times 30</script> ,   . </p><br><div class="spoiler"> <b class="spoiler_title">    ,   </b> <div class="spoiler_text"><pre> <code class="python hljs">tree = DecisionTreeClassifier(random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>).fit(X, y) xx, yy = get_grid(X, eps=<span class="hljs-number"><span class="hljs-number">.05</span></span>) predicted = tree.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) plt.pcolormesh(xx, yy, predicted, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>) plt.scatter(X[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=y, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>, edgecolors=<span class="hljs-string"><span class="hljs-string">'black'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">1.5</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Easy task. Decision tree compexifies everything'</span></span>);</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/004/5a7/9a2/0045a79a2b1c4c378cdb3fb1e80b1de8.png"></div><br><br><p>    ,   (  ) ‚Äì     <math></math><span class="MathJax_Preview" style="color: inherit;"><span class="MJXp-math" id="MJXp-Span-749"><span class="MJXp-msubsup" id="MJXp-Span-750"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-751" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-752" style="vertical-align: -0.4em;">1</span></span><span class="MJXp-mo" id="MJXp-Span-753" style="margin-left: 0.333em; margin-right: 0.333em;">=</span><span class="MJXp-msubsup" id="MJXp-Span-754"><span class="MJXp-mi MJXp-italic" id="MJXp-Span-755" style="margin-right: 0.05em;">x</span><span class="MJXp-mn MJXp-script" id="MJXp-Span-756" style="vertical-align: -0.4em;">2</span></span></span></span><span class="MathJax_SVG MathJax_SVG_Processing" id="MathJax-Element-48-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"></span><script type="math/tex" id="MathJax-Element-48">x_1 = x_2</script>  . </p><br><div class="spoiler"> <b class="spoiler_title">   </b> <div class="spoiler_text"><pre> <code class="python hljs">export_graphviz(tree, feature_names=[<span class="hljs-string"><span class="hljs-string">'x1'</span></span>, <span class="hljs-string"><span class="hljs-string">'x2'</span></span>], out_file=<span class="hljs-string"><span class="hljs-string">'../../img/deep_toy_tree.dot'</span></span>, filled=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) !dot -Tpng <span class="hljs-string"><span class="hljs-string">'../../img/deep_toy_tree.dot'</span></span> -o <span class="hljs-string"><span class="hljs-string">'../../img/deep_toy_tree.png'</span></span></code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/077/436/c6b/077436c6b10044b8a7d673c6400798f0.png"></div><br><br><p>         ,      ,    (  ). </p><br><div class="spoiler"> <b class="spoiler_title">    ,   kNN</b> <div class="spoiler_text"><pre> <code class="python hljs">knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number"><span class="hljs-number">1</span></span>).fit(X, y) xx, yy = get_grid(X, eps=<span class="hljs-number"><span class="hljs-number">.05</span></span>) predicted = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape) plt.pcolormesh(xx, yy, predicted, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>) plt.scatter(X[:, <span class="hljs-number"><span class="hljs-number">0</span></span>], X[:, <span class="hljs-number"><span class="hljs-number">1</span></span>], c=y, s=<span class="hljs-number"><span class="hljs-number">100</span></span>, cmap=<span class="hljs-string"><span class="hljs-string">'autumn'</span></span>, edgecolors=<span class="hljs-string"><span class="hljs-string">'black'</span></span>, linewidth=<span class="hljs-number"><span class="hljs-number">1.5</span></span>); plt.title(<span class="hljs-string"><span class="hljs-string">'Easy task, kNN. Not bad'</span></span>);</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/998/ad6/80c/998ad680c23e482d90193066059ec94c.png"></div><br><br><h3 id="derevya-resheniy-i-metod-blizhayshih-sosedey-v-zadache-raspoznavaniya-rukopisnyh-cifr-mnist">            MNIST </h3><br><p>     2    .  ""  <code>sklearn</code>    .    ,        . </p><br><p>     8 x 8 (     ).    ""    64,    . </p><br><p>    , ,   . </p><br><div class="spoiler"> <b class="spoiler_title">     </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> load_digits data = load_digits() X, y = data.data, data.target X[<span class="hljs-number"><span class="hljs-number">0</span></span>,:].reshape([<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>])</code> </pre> <br><p> array([[ 0., 0., 5., 13., 9., 1., 0., 0.], <br> [ 0., 0., 13., 15., 10., 15., 5., 0.], <br> [ 0., 3., 15., 2., 0., 11., 8., 0.], <br> [ 0., 4., 12., 0., 0., 8., 8., 0.], <br> [ 0., 5., 8., 0., 0., 9., 8., 0.], <br> [ 0., 4., 11., 0., 1., 12., 7., 0.], <br> [ 0., 2., 14., 5., 10., 12., 0., 0.], <br> [ 0., 0., 6., 13., 10., 0., 0., 0.]]) </p><br><pre> <code class="python hljs">f, axes = plt.subplots(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">4</span></span>, sharey=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, figsize=(<span class="hljs-number"><span class="hljs-number">16</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">4</span></span>): axes[i].imshow(X[i,:].reshape([<span class="hljs-number"><span class="hljs-number">8</span></span>,<span class="hljs-number"><span class="hljs-number">8</span></span>]));</code> </pre></div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/856/119/828/8561198285a542fcace86e5939905ff5.png"></div><br><br><p>      ,     ,        . </p><br><div class="spoiler"> <b class="spoiler_title"> DT  kNN   MNIST</b> <div class="spoiler_text"><p>  70%  (X_train, y_train)    30%    (X_holdout, y_holdout).          ,     ,   ,    . </p><br><pre> <code class="python hljs">X_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=<span class="hljs-number"><span class="hljs-number">0.3</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>)</code> </pre> <br><p>     kNN,     . </p><br><pre> <code class="python hljs">tree = DecisionTreeClassifier(max_depth=<span class="hljs-number"><span class="hljs-number">5</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>) knn = KNeighborsClassifier(n_neighbors=<span class="hljs-number"><span class="hljs-number">10</span></span>) tree.fit(X_train, y_train) knn.fit(X_train, y_train)</code> </pre> <br><p>     . ,       .       . </p><br><pre> <code class="python hljs">tree_pred = tree.predict(X_holdout) knn_pred = knn.predict(X_holdout) accuracy_score(y_holdout, knn_pred), accuracy_score(y_holdout, tree_pred) <span class="hljs-comment"><span class="hljs-comment"># (0.97, 0.666)</span></span></code> </pre> <br><p>   ,       -,  ,    ,     ‚Äî 64. </p><br><pre> <code class="python hljs">tree_params = {<span class="hljs-string"><span class="hljs-string">'max_depth'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">25</span></span>, <span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">40</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>], <span class="hljs-string"><span class="hljs-string">'max_features'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">20</span></span> ,<span class="hljs-number"><span class="hljs-number">30</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>]} tree_grid = GridSearchCV(tree, tree_params, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>, n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>, verbose=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) tree_grid.fit(X_train, y_train)</code> </pre> <br><p>           -: </p><br><pre> <code class="python hljs">tree_grid.best_params_, tree_grid.best_score_ <span class="hljs-comment"><span class="hljs-comment"># ({'max_depth': 20, 'max_features': 64}, 0.844)</span></span></code> </pre> <br><p>    66%,    97%.         .       -   99% . </p><br><pre> <code class="python hljs">np.mean(cross_val_score(KNeighborsClassifier(n_neighbors=<span class="hljs-number"><span class="hljs-number">1</span></span>), X_train, y_train, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 0.987</span></span></code> </pre> <br><p>       ,      ,    .     . </p><br><pre> <code class="python hljs">np.mean(cross_val_score(RandomForestClassifier(random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>), X_train, y_train, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># 0.935</span></span></code> </pre> <br><p>   ,  ,       RandomForestClassifier,          98%,       . </p></div></div><br><p>   <br> <em>(: CV  Holdout‚Äì       -    -. DT ‚Äì  , kNN ‚Äì   , RF ‚Äì  )</em> </p><br><table><thead><tr><th></th><th> CV </th><th> Holdout </th></tr></thead><tbody><tr><td> <strong>DT</strong> </td><td> 0.844 </td><td> 0.838 </td></tr><tr><td> <strong>kNN</strong> </td><td> 0.987 </td><td> 0.983 </td></tr><tr><td> <strong>RF</strong> </td><td> 0.935 </td><td> 0.941 </td></tr></tbody></table><br><p> <strong></strong>    (  ):        ‚Äì       (       ),  ,      . </p><br><h3 id="slozhnyy-sluchay-dlya-metoda-blizhayshih-sosedey">       </h3><br><p>      .           ,       . </p><br><div class="spoiler"> <b class="spoiler_title">      </b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">form_noisy_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n_obj=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1000</span></span></span></span><span class="hljs-function"><span class="hljs-params">, n_feat=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">100</span></span></span></span><span class="hljs-function"><span class="hljs-params">, random_seed=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">17</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> np.seed = random_seed y = np.random.choice([<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], size=n_obj) <span class="hljs-comment"><span class="hljs-comment">#     x1 = 0.3 * y #   ‚Äì  x_other = np.random.random(size=[n_obj, n_feat - 1]) return np.hstack([x1.reshape([n_obj, 1]), x_other]), y X, y = form_noisy_data()</span></span></code> </pre> </div></div><br><p>  ,        -    .  ,       <code>n_neighbors</code>    .     . </p><br><p> ,           ,         . ,    ""          . </p><br><div class="spoiler"> <b class="spoiler_title">    kNN</b> <div class="spoiler_text"><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.model_selection <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cross_val_score cv_scores, holdout_scores = [], [] n_neighb = [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>] + list(range(<span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-number"><span class="hljs-number">550</span></span>, <span class="hljs-number"><span class="hljs-number">50</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> n_neighb: knn = KNeighborsClassifier(n_neighbors=k) cv_scores.append(np.mean(cross_val_score(knn, X_train, y_train, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>))) knn.fit(X_train, y_train) holdout_scores.append(accuracy_score(y_holdout, knn.predict(X_holdout))) plt.plot(n_neighb, cv_scores, label=<span class="hljs-string"><span class="hljs-string">'CV'</span></span>) plt.plot(n_neighb, holdout_scores, label=<span class="hljs-string"><span class="hljs-string">'holdout'</span></span>) plt.title(<span class="hljs-string"><span class="hljs-string">'Easy task. kNN fails'</span></span>) plt.legend();</code> </pre> </div></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/files/920/8d1/4ce/9208d14ced8e4ce49a27ffb64838b252.png"></div><br><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="python hljs">tree = DecisionTreeClassifier(random_state=<span class="hljs-number"><span class="hljs-number">17</span></span>, max_depth=<span class="hljs-number"><span class="hljs-number">1</span></span>) tree_cv_score = np.mean(cross_val_score(tree, X_train, y_train, cv=<span class="hljs-number"><span class="hljs-number">5</span></span>)) tree.fit(X_train, y_train) tree_holdout_score = accuracy_score(y_holdout, tree.predict(X_holdout)) print(<span class="hljs-string"><span class="hljs-string">'Decision tree. CV: {}, holdout: {}'</span></span>.format(tree_cv_score, tree_holdout_score))</code> </pre> <br><p> Decision tree. CV: 1.0, holdout: 1.0 </p></div></div><br><p> ,        ,      . ,     ,    :       ,      . </p><br><h2 id="plyusy-i-minusy-derevev-resheniy-i-metoda-blizhayshih-sosedey">          </h2><br><h4 id="plyusy-i-minusy-derevev-resheniy">      </h4><br><p>  Pros: </p><br><ul><li>    ,  , , "  &lt; 25    ,    ".     ; </li><li>     ,    "" (    )    (),         (  ); </li><li>     ; </li><li>    ; </li><li>   ,   . </li></ul><br><p>  Minuses: </p><br><ul><li>         :        ,     ,      (,         ),       ,    ; </li><li>  ,   ,    (  ,  -   ),            ; </li><li>     (pruning)                . ,  ‚Äî     ; </li><li> .          .          ( ); </li><li>      (        ) NP-,             ,       ; </li><li>     . Friedman ,         50%  CART (       ‚Äì Classification And Regression Trees,  <code>sklearn</code>      ); </li><li>    ,    (         ).         ,      ,     .          ,           &gt; 19  &lt; 0. </li></ul><br><h4 id="plyusy-i-minusy-metoda-blizhayshih-sosedey">       </h4><br><p>  Pros: </p><br><ul><li>  ; </li><li>   ; </li><li>  ,      ,      ,  , , ; </li><li>          (  :          ,    kNN   ). ,                kNN,      .       ( , "VideoLectures.Net Recommender System Challenge")   <a href="http://alexanderdyakonov.narod.ru/contests.htm"></a> ; </li><li>  ,  ,       .     :    ,    (: "    ,      350 ,   70 ‚Äì ,   12% ,     "). </li></ul><br><p>  Minuses: </p><br><ul><li>     , ,   ,    ,  ,  ,   ,   (100-150),          ,   ; </li><li>      ,       ,      /; </li><li>       .          .      ,         ; </li><li>        ‚Äî   (,         ).         ,    ; </li><li>  ,  ,   , - " ".       ML-  Pedro Domingos ‚Äì <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"></a>    "A Few Useful Things to Know about Machine Learning",  "the curse of dimensionality"    Deep Learning  <a href="http://www.deeplearningbook.org/contents/ml.html"></a> "Machine Learning basics". </li></ul><br><p>      , ,     .   ,     . </p><br><h1 id="domashnee-zadanie--3">   ‚Ññ 3 </h1><br><p>         ,    <a href="https://vk.com/mlcourse"> </a>   <a href=""></a> . </p><br><p>       <a href="http://nbviewer.jupyter.org/github/Yorko/mlcourse.ai/blob/master/jupyter_russian/assignments_demo/assignment03_decision_trees.ipynb"> </a> ‚Äì   ,    ,   ,          Adult  UCI.       <a href="https://docs.google.com/forms/d/1bC3jNPH7XZUty_DaIvt0fPrsiS8YFkcpeBKHPSG0hw0">-</a> (    ). </p><br><h1 id="poleznye-resursy">  Useful resources </h1><br><ul><li> <a href="https://medium.com/open-machine-learning-course/open-machine-learning-course-topic-3-classification-decision-trees-and-k-nearest-neighbors-8613c6b6d2cd">Open Machine Learning Course. Topic 3. Classification, Decision Trees and k Nearest Neighbors</a> (    ) </li><li> <a href="https://www.youtube.com/watch%3Fv%3Dp9Hny3Cs6rk"></a>      </li><li>        ‚Äì <a href="https://github.com/rushter/MLAlgorithms"></a> <a href="https://habr.com/users/rushter/" class="user_link">rushter</a> . , , ,        ; </li><li> <a href="https://github.com/esokolov/ml-course-hse"></a>      (  GitHub).  ,    ; </li><li> <a href="https://github.com/diefimov"></a>    GitHub (.).    ; </li><li> <a href="https://habrahabr.ru/post/171759/"></a> "    "  ; </li><li> <a href="https://habrahabr.ru/company/yandex/blog/206058/"></a> "    .     "  . </li></ul></div><p>Source: <a href="https://habr.com/ru/post/322534/">https://habr.com/ru/post/322534/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../322524/index.html">Unscientific revolution: how the lack of real research is ruining technological projects</a></li>
<li><a href="../322526/index.html">Another node.js library ...</a></li>
<li><a href="../322528/index.html">Functional patterns in domain modeling - anemic patterns and behavioral layout</a></li>
<li><a href="../322530/index.html">How IT professionals work. Mamikon Vartapetyan, Head of the Lester IT Database Development Team</a></li>
<li><a href="../322532/index.html">MySQL and MongoDB - when and what is better to use</a></li>
<li><a href="../322536/index.html">Security Week 08: SHA-1 exactly everything, vulnerabilities in TP-Link routers, cross-platform botnet with Mirai code</a></li>
<li><a href="../322538/index.html">ASP.NET Core: your first Linux application using Visual Studio Code</a></li>
<li><a href="../322540/index.html">The first experience of developing games for Apple Watch</a></li>
<li><a href="../322542/index.html">Backups as a way to avoid extra costs during infection with cryptographer</a></li>
<li><a href="../322544/index.html">2fa implementation experience on linux with duosecurity</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>