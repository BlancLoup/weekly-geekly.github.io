<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognition of DGA domains. And what if neural networks?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! 


 Today we will talk about the recognition of domains generated using domain name generation algorithms. Let's look at the existing methods, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognition of DGA domains. And what if neural networks?</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/8b7/123/ae2/8b7123ae2a80471bae178607c05d60ba.jpg" align="left" width="250" height="250"><br><p>  Hello! </p><br><p>  Today we will talk about the recognition of domains generated using domain name generation algorithms.  Let's look at the existing methods, and also we will offer our own, based on recurrent neural networks.  Interesting?  Welcome under cat. </p><a name="habracut"></a><br><p><br><br><br><br>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Domain Name Generation Algorithms (DGA) are algorithms used by malware (malware) to generate a large number of pseudo-random domain names that will allow you to establish a connection with the control command center.  Thus, they provide a powerful layer of infrastructure protection for malware.  At first glance, the concept of creating a large number of domain names for establishing communications does not seem complicated, but the methods used to create arbitrary strings are often hidden behind different layers of obfuscation.  This is done to complicate the process of reverse engineering and to obtain a model of the functioning of a particular family of algorithms. </p><br><p>  For example, the Conficker computer worm in 2008 was one of the first cases.  Today, there are dozens of similar malware, each of which is a serious threat.  In addition, the algorithms are improved, their detection becomes more difficult. </p><br><h1>  General principle of work </h1><br><div class="spoiler">  <b class="spoiler_title">Drawing work</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/85b/810/025/85b810025c944195938b38be8badf27c.png"></div></div><br><p>  In general, a malicious file needs a seed to initialize a pseudo-random number generator (PRNG).  The seed can be any parameter that will be known to the malicious file and the owner of the botnet.  In our case, this is the value of the current date and time taken from the cnn.com resource.  Using the same initialization vectors, the malicious file and the botnet owner get identical domain name tables.  After that, the botnet owner only needs to register one domain in order for the malicious file, recursively sending requests to the DNS server, to get the IP address of the management server for further connection with it and receiving commands. </p><br><h1>  Recognition </h1><br><p>  Currently, there are many works related to the analysis of domain name generation algorithms.  Some of them use Machine Learning methods.  In general, the well-known models that can be found in the processing and analysis environment of natural languages ‚Äã‚Äãare used ‚Äî the models n-gramm, TF-IDF, etc. </p><br><p>  However, the question arises of the training set.  Our sample will consist of 2 classes.  The first, Legit, was taken from the Alexa Top Million list.  The second, DGA, was compiled by reverse engineering algorithms for generating malicious domain names taken from malware instances on the Internet, and is available in the repository ( <a href="https://github.com/andrewaeva/DGA">https://github.com/andrewaeva/DGA</a> ). </p><br><p>  To begin with, we tried the approach described by the guys from licksecurity.  They propose using the following list of parameters: length, entropy, TF-IDF model with N-gram selection.  The first parameter is the length of the domain name.  The second parameter is entropy.  Further, the N-gram model was considered.  Each n-gram (from 3 to 5) was represented as a vector in n-dimensional space, and the distance between them was calculated using the scalar product of these vectors.  This is quite simply implemented using the Scikit Learn library. </p><br><div class="spoiler">  <b class="spoiler_title">A slice of Python code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> CountVectorizer alexa_vc = CountVectorizer(analyzer=<span class="hljs-string"><span class="hljs-string">'char'</span></span>, ngram_range=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), min_df=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>, max_df=<span class="hljs-number"><span class="hljs-number">1.0</span></span>) counts_matrix = alexa_vc.fit_transform(dataframe_dict[<span class="hljs-string"><span class="hljs-string">'alexa'</span></span>][<span class="hljs-string"><span class="hljs-string">'domain'</span></span>]) alexa_counts = np.log10(counts_matrix.sum(axis=<span class="hljs-number"><span class="hljs-number">0</span></span>).getA1()) dict_vc = CountVectorizer(analyzer=<span class="hljs-string"><span class="hljs-string">'char'</span></span>, ngram_range=(<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), min_df=<span class="hljs-number"><span class="hljs-number">1e-5</span></span>, max_df=<span class="hljs-number"><span class="hljs-number">1.0</span></span>) counts_matrix = dict_vc.fit_transform(word_dataframe[<span class="hljs-string"><span class="hljs-string">'word'</span></span>]) dict_counts = np.log10(counts_matrix.sum(axis=<span class="hljs-number"><span class="hljs-number">0</span></span>).getA1()) all_domains[<span class="hljs-string"><span class="hljs-string">'alexa_grams'</span></span>] = alexa_counts * alexa_vc.transform(all_domains[<span class="hljs-string"><span class="hljs-string">'domain'</span></span>]).T all_domains[<span class="hljs-string"><span class="hljs-string">'word_grams'</span></span>] = dict_counts * dict_vc.transform(all_domains[<span class="hljs-string"><span class="hljs-string">'domain'</span></span>]).T all_domains[<span class="hljs-string"><span class="hljs-string">'diff'</span></span>] = all_domains[<span class="hljs-string"><span class="hljs-string">'alexa_grams'</span></span>] - all_domains[<span class="hljs-string"><span class="hljs-string">'word_grams'</span></span>]</code> </pre> </div></div><br><p>  As a result, we received an additional 3 parameters: Alexa gram ‚Äî cosine distance to a dictionary consisting of the Alexa Top Million domains; Word gram ‚Äî cosine distance to a specially composed dictionary consisting of the most commonly used words and phrases; and diff = alexa gram - word gram . </p><br><p>  For each of the parameters we have built visual charts.  Do not forget to look :) </p><br><div class="spoiler">  <b class="spoiler_title">Charts under the spoiler</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/04c/43d/218/04c43d2184d54a0eae61e7b019f0ecba.png" width="650" height="450"><br><img src="https://habrastorage.org/files/b64/c68/0a7/b64c680a72824e779ce16b54dda6625a.png" width="650" height="450"><br><img src="https://habrastorage.org/files/f8d/6da/cdf/f8d6dacdfb8742e5992b2e4f8caa3e66.png" width="650" height="450"><br><img src="https://habrastorage.org/files/f15/457/40a/f1545740a08c4d3fb85f6e063e2d0978.png" width="650" height="450"><br><img src="https://habrastorage.org/files/07b/6fc/847/07b6fc8477be46d48214baa5fa25a047.png" width="650" height="450"></div></div><br><p>  The classification itself was carried out on the 80/20 principle, i.e.  training took place on 80% of the initial data, and the testing of the algorithm on the remaining 20%.  After checking the quality of the classification, the following results were obtained: </p><br><table><thead><tr><th>  Algorithm </th><th>  Classification accuracy </th></tr></thead><tbody><tr><td>  Logistic regression </td><td>  87% </td></tr><tr><td>  Random forest </td><td>  95% </td></tr><tr><td>  Naive bayes </td><td>  75% </td></tr><tr><td>  Extra tree forest </td><td>  94.6% </td></tr><tr><td>  Voting Classification </td><td>  94.7% </td></tr></tbody></table><br><p>  We thought, why so far no one has tried to use neural networks?  Need to try! </p><br><h1>  Neural networks </h1><br><p>  To solve our problem, we used a recurrent neural network.  Recurrent neural networks are mainly characterized by the presence of a cycle.  They allow you to save and use the information obtained from the previous steps of the neural network.  In our case, each domain is considered as a sequence of characters from a fixed dictionary, which is fed to the input of a recurrent neural network.  Training of such a neural network is carried out by the method of back propagation of an error in such a way as to maximize the probability of the correct choice of the corresponding class. </p><br><img src="https://habrastorage.org/files/83f/6c6/7dc/83f6c67dcd57417886959fe70a681b7a.png"><br><p>  Recurrent neural network and Yandex.ru </p><br><p>  Such a neural network architecture can analyze information that was previously submitted for data analysis at the current time.  However, in practice it turns out that if the gap between the past information and the present is large enough, then this connection is lost, and such a network is unable to process it.  A solution to this problem was found in 1997 by Hochreiter &amp; Schmidhuber scientists.  In their work, they proposed a new model of the recurrent neural network, namely the Long short-therm memory.  Currently, this model is widely used to solve various classes of tasks, such as: speech recognition, processing of natural languages, etc. LSTM consists of a number of constantly connected subnets known as memory blocks.  Instead of a single layer of the neural network, this model uses 4 layers that interact in a special way.  In our model, we will use a variation of the LSTM model, namely the Gated Recurrent Unit (GRU).  You can read more about LSTM and GRU in a wonderful article ( <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> ), and we will move on. </p><br><p>  To implement our model, we will use Python and all the favorite libraries of Theano ( <a href="https://pypi.python.org/pypi/Theano">https://pypi.python.org/pypi/Theano</a> ) and Lasagne ( <a href="">https://pypi.python.org/pypi/Lasagne/0.1</a> ). </p><br><p>  Let's load our data into memory (yes, we are lazy) and do the preprocessing: </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> theano.tensor <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> T <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> lasagne dataset = pd.read_csv(<span class="hljs-string"><span class="hljs-string">'/home/andrw/dataset_all_2class.csv'</span></span>, sep = <span class="hljs-string"><span class="hljs-string">','</span></span>) dataset.head() chars = dataset[<span class="hljs-string"><span class="hljs-string">'domain'</span></span>].tolist() chars = <span class="hljs-string"><span class="hljs-string">''</span></span>.join(chars) chars = list(set(chars)) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> chars <span class="hljs-comment"><span class="hljs-comment"># ['-', '.', '1', '0', '3', '2', '5', '4', '7', '6', '9', '8', '_', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'j', 'm', 'l', 'o', 'n', 'q', 'p', 's', 'r', 'u', 't', 'w', 'v', 'y', 'x', 'z'] classes = dataset['class'].tolist() classes = list(set(classes)) print classes #['dga', 'legit']</span></span></code> </pre> </div></div><br><p>  Now we will encode our domain into a sequence and form arrays X, y, mask M. Why do we need a mask?  It's simple, Carl!  Domains of varying length. </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="python hljs">char_to_ix = { ch:i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,ch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(chars) } ix_to_char = { i:ch <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,ch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(chars) } class_to_y = { cl:i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i,cl <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(classes) } NUM_VOCAB = len(chars) NUM_CLASS = len(classes) NUM_CHARS = <span class="hljs-number"><span class="hljs-number">75</span></span> N = len(dataset.index) X = np.zeros((N, NUM_CHARS)).astype(<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) M = np.zeros((N, NUM_CHARS)).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) Y = np.zeros(N).astype(<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, r <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataset.iterrows(): inputs = [char_to_ix[ch] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> ch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> r[<span class="hljs-string"><span class="hljs-string">'domain'</span></span>]] length = len(inputs) X[i,:length] = np.array(inputs) M[i,:length] = np.ones(length) Y[i] = class_to_y[r[<span class="hljs-string"><span class="hljs-string">'class'</span></span>]]</code> </pre> </div></div><br><p>  Form a training and test sample: </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="python hljs">rand_indx = np.random.randint(N, size=N) X = X[rand_indx,:] M = M[rand_indx,:] Y = Y[rand_indx] Ntrain = int(N * <span class="hljs-number"><span class="hljs-number">0.75</span></span>) Ntest = N - Ntrain Xtrain = X[:Ntrain,:] Mtrain = M[:Ntrain,:] Ytrain = Y[:Ntrain] Xtest = X[Ntrain:,:] Mtest = M[Ntrain:,:] Ytest = Y[Ntrain:]</code> </pre> </div></div><br><p>  Now everything is ready to describe the architecture of our network, which looks like the one shown in the figure.  For classification, we will transfer the state of the last hidden layer to the Softmax layer, the output of which we can interpret as the probabilities of a domain belonging to one of the classes (malicious or legitimate). </p><br><img src="https://habrastorage.org/files/b0f/042/eaf/b0f042eaf352448ea5a08e7da05b0d83.png"><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><pre> <code class="python hljs">BATCH_SIZE = <span class="hljs-number"><span class="hljs-number">100</span></span> NUM_UNITS_ENC = <span class="hljs-number"><span class="hljs-number">128</span></span> x_sym = T.imatrix() y_sym = T.ivector() xmask_sym = T.matrix() Tdata = np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>,size=(BATCH_SIZE, NUM_CHARS)).astype(<span class="hljs-string"><span class="hljs-string">'int32'</span></span>) Tmask = np.ones((BATCH_SIZE, NUM_CHARS)).astype(<span class="hljs-string"><span class="hljs-string">'float32'</span></span>) l_in = lasagne.layers.InputLayer((<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)) l_emb = lasagne.layers.EmbeddingLayer(l_in, NUM_VOCAB, NUM_VOCAB, name=<span class="hljs-string"><span class="hljs-string">'Embedding'</span></span>) l_mask_enc = lasagne.layers.InputLayer((<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>)) l_enc = lasagne.layers.GRULayer(l_emb, num_units=NUM_UNITS_ENC, name=<span class="hljs-string"><span class="hljs-string">'GRUEncoder'</span></span>, mask_input=l_mask_enc) l_last_hid = lasagne.layers.SliceLayer(l_enc, indices=<span class="hljs-number"><span class="hljs-number">-1</span></span>, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, name=<span class="hljs-string"><span class="hljs-string">'LastState'</span></span>) l_softmax = lasagne.layers.DenseLayer(l_last_hid, num_units=NUM_CLASS, nonlinearity=lasagne.nonlinearities.softmax, name=<span class="hljs-string"><span class="hljs-string">'SoftmaxOutput'</span></span>) output_train = lasagne.layers.get_output(l_softmax, inputs={l_in: x_sym, l_mask_enc: xmask_sym}, deterministic=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) total_cost = T.nnet.categorical_crossentropy(output_train, y_sym.flatten()) mean_cost = T.mean(total_cost) <span class="hljs-comment"><span class="hljs-comment">#accuracy function argmax = T.argmax(output_train, axis=-1) eq = T.eq(argmax,y_sym) acc = T.mean(eq) all_parameters = lasagne.layers.get_all_params([l_softmax], trainable=True) all_grads = T.grad(mean_cost, all_parameters) all_grads_clip = [T.clip(g,-1,1) for g in all_grads] all_grads_norm = lasagne.updates.total_norm_constraint(all_grads_clip, 1) updates = lasagne.updates.adam(all_grads_norm, all_parameters, learning_rate=0.005) train_func_a = theano.function([x_sym, y_sym, xmask_sym], mean_cost, updates=updates) test_func_a = theano.function([x_sym, y_sym, xmask_sym], acc</span></span></code> </pre> </div></div><br><p>  We train our model by breaking the sample into batches of 100 domain names.  As a result, we get the following graph: </p><br><img src="https://habrastorage.org/files/025/926/74c/02592674c0964ae19d8b0643f9dd943d.png" width="650" height="450"><br><p>  In conclusion, we can say that the resulting model is not at all inferior to the Random Forest algorithm, and even surpasses it.  In addition, our model can be further improved, for example, by adding a reverse pass through the domain name to it or by including an attention mechanism (Attention LSTM) in the model.  Well, for the topic of machine learning in information security, everything is just beginning :) </p><br><h4>  References </h4><br><ul><li>  <a href="https://github.com/ClickSecurity/data_hacking/blob/master/dga_detection/DGA_Domain_Detection.ipynb">https://github.com/ClickSecurity/data_hacking/blob/master/dga_detection/DGA_Domain_Detection.ipynb</a> </li><li>  <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a> </li><li>  <a href="https://github.com/andrewaeva/DGA/">https://github.com/andrewaeva/DGA/</a> </li><li>  <a href="http://openbooks.ifmo.ru/ru/collections_article/997/raspoznavanie_i_klassifikaciya_vredonosnyh_domennyh_imen.html">http://openbooks.ifmo.ru/ru/collections_article/997/raspoznavanie_i_klassifikaciya_vredonosnyh_domennyh_imen.html</a> </li><li>  <a href="http://openbooks.ifmo.ru/ru/collections_article/4053/analiz_algoritmov_generacii_vredonosnyh_domennyh_imen_i_metody_ih_raspoznavaniya_s_ispolzovaniem_rekurrentnyh_neyronnyh_setey.htm">http://openbooks.ifmo.ru/ru/collections_article/4053/analiz_algoritmov_generacii_vredonosnyh_domennyh_imen_i_metody_ih_raspoznavaniya_s_ispolzovaniem_rekurrentnyh_neyronnyh_setey.hht</a> </li></ul><br><p>  Abakumov Andrey, Digital Security </p><br></div><p>Source: <a href="https://habr.com/ru/post/282433/">https://habr.com/ru/post/282433/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../282423/index.html">Extending Splunk functionality is just</a></li>
<li><a href="../282425/index.html">Frontend Dev Conf 2016: the heroes, events and surprises of the conference</a></li>
<li><a href="../282427/index.html">Native advertising returns: Native Admob, RecyclerView and briefly about the rules</a></li>
<li><a href="../282429/index.html">We are looking for a replacement for Digital Ocean among domestic hosting companies</a></li>
<li><a href="../282431/index.html">Two languages, one Cup. Reflections on the RCC 2016 Rules</a></li>
<li><a href="../282435/index.html">"And how well everything began ...", or the benefits of O-notation, not only for the analysis of algorithms</a></li>
<li><a href="../282441/index.html">Mill Fighting - 1: Interpolation Splines</a></li>
<li><a href="../282443/index.html">MNP and Appearance API 1.0 - Voluntary Eating Cacti</a></li>
<li><a href="../282449/index.html">Silent revolution: the introduction of x86-architecture instead of RISC-machines for bank processing</a></li>
<li><a href="../282455/index.html">Why security experts prefer outdated email clients</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>