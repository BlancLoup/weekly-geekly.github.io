<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Why do we need the Ho-Kashyap algorithm?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, a publication appeared on Habr√© about the Ho-Kashyap algorithm (the Ho-Kashyap procedure, which is also the NSCO algorithm, the smallest roo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Why do we need the Ho-Kashyap algorithm?</h1><div class="post__text post__text-html js-mediator-article">  Recently, a publication appeared on Habr√© about <a href="https://habrahabr.ru/post/312600/">the Ho-Kashyap algorithm</a> (the Ho-Kashyap procedure, which is also the NSCO algorithm, the smallest root-mean-square error).  It seemed to me not very clear and I decided to sort out the topic myself.  It turned out that in the Russian-speaking Internet the topic was not well disassembled, so I decided to issue an article based on the search results. <br><br>  Despite the boom of neural networks in machine learning, linear classification algorithms remain much simpler to use and interpret.  But at the same time, sometimes you don‚Äôt want to use any advanced methods, such as <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2580%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B2%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B2">the support vector</a> or <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">logistic regression</a> <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25BF%25D0%25BE%25D1%2580%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B2%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B2">method,</a> and the temptation is to drive all the data into one large <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2580%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D1%258F">linear least-squares regression</a> , even more so it knows how to build MS Excel. <br><br>  The problem with this approach is that even if the input data is linearly separable, the resulting classifier may not separate them.  For example, for a set of points <img src="https://tex.s2cms.ru/svg/%20X%20%3D%20%5B(6%2C%209)%2C%20(5%2C%207)%2C%20(5%2C%209)%2C%20(10%2C%201)%5D%20" alt="X = [(6, 9), (5, 7), (5, 9), (10, 1)]">  , <img src="https://tex.s2cms.ru/svg/%20y%20%3D%20%5B1%2C%201%2C%20-1%2C%20-1%5D%20" alt="y = [1, 1, -1, -1]">  get the dividing line <img src="https://tex.s2cms.ru/svg/(0.15x_1%20-%200.43x_2%20%2B%203.21)%20%3D%200%20" alt="(0.15x_1 - 0.43x_2 + 3.21) = 0">  (example borrowed from (1)): 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/files/6f5/a40/b54/6f5a40b543d245898e13e6d5da843da2.png" alt="Latex"></div><br><br>  The question is - is it possible to somehow get rid of this particular behavior? <br><a name="habracut"></a><br><h3>  Linear classification task </h3><br>  To begin, we formalize the subject of the article. <br><br>  Dana matrix <img src="https://tex.s2cms.ru/svg/%20X%20" alt="X">  each line <img src="https://tex.s2cms.ru/svg/%20X_i%20" alt="X_i">  which corresponds to the characteristic description of the object <img src="https://tex.s2cms.ru/svg/%20i%20" alt="i">  (including constant <img src="https://tex.s2cms.ru/svg/%201%20" alt="one">  ) and class label vectors <img src="https://tex.s2cms.ru/svg/y" alt="y">  where <img src="https://tex.s2cms.ru/svg/%20y_i%20%5Cin%20%5C%7B%2B1%2C%20-1%5C%7D%20" alt="y_i \ in \ {+ 1, -1 \}">  - object label <img src="https://tex.s2cms.ru/svg/%20i%20" alt="i">  .  We want to build a linear classifier of the form. <img src="https://tex.s2cms.ru/svg/%20sign(Xw)%20%3D%20y%20" alt="sign (Xw) = y">  . <br><br><pre><code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np &gt;&gt;&gt; X = np.array([[<span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]]) &gt;&gt;&gt; y = np.array([[<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>], [<span class="hljs-number"><span class="hljs-number">-1</span></span>]])</code> </pre> <br>  The easiest way to do this is to construct a least-squares regression for <img src="https://tex.s2cms.ru/svg/%20Xw%20%3D%20y%20" alt="Xw = y">  that is, minimize the sum of squared deviations <img src="https://tex.s2cms.ru/svg/%20%5Cmin%20%5Csum%20(w%20X_i%20-%20y_i)%5E2%20" alt="\ min \ sum (w X_i - y_i) ^ 2">  .  The optimal weight can be found by the formula <img src="https://tex.s2cms.ru/svg/%20w%20%3D%20X%5E%7B%2B%7D%20y%20" alt="w = x ^ {+} y">  where <img src="https://tex.s2cms.ru/svg/%20X%5E%7B%2B%7D%20" alt="X ^ {+}">  - pseudoinverse matrix.  Thus, a picture from the beginning of the article. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>w = np.dot(np.linalg.pinv(X), y) &gt;&gt;&gt; w array([[ <span class="hljs-number"><span class="hljs-number">0.15328467</span></span>], [<span class="hljs-number"><span class="hljs-number">-0.4379562</span></span> ], [ <span class="hljs-number"><span class="hljs-number">3.2189781</span></span> ]]) &gt;&gt;&gt; np.dot(X, w) array([[ <span class="hljs-number"><span class="hljs-number">0.19708029</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.91970803</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.04379562</span></span>], [<span class="hljs-number"><span class="hljs-number">-1.16058394</span></span>]])</code> </pre><br><h3>  Linear separability </h3><br>  For convenience of writing, we multiply each line of inequality element by element. <img src="https://tex.s2cms.ru/svg/%20Xw%20%3D%20y%20" alt="Xw = y">  on <img src="https://tex.s2cms.ru/svg/%20y%20" alt="y">  and call the resulting matrix on the left <img src="https://tex.s2cms.ru/svg/%20Y%20%3D%20y%20%5Ctimes%20X%20" alt="Y = y \ times X">  (here <img src="https://tex.s2cms.ru/svg/%20%5Ctimes%20" alt="\ times">  means line by line multiplication).  Then the condition for OLS-regression is reduced to the form <img src="https://tex.s2cms.ru/svg/%20Yw%20%3D%201%20" alt="Yw = 1">  , and the minimization problem - to minimize <img src="https://tex.s2cms.ru/svg/%20%5Cmin%20%5Csum%20(w%20Y_i%20-%201)%5E2%20" alt="\ min \ sum (w Y_i - 1) ^ 2">  . <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>Y = y * X &gt;&gt;&gt; Y array([[ <span class="hljs-number"><span class="hljs-number">6</span></span>, <span class="hljs-number"><span class="hljs-number">9</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [ <span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">7</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>], [ <span class="hljs-number"><span class="hljs-number">-5</span></span>, <span class="hljs-number"><span class="hljs-number">-9</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>], [ <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>]])</code> </pre><br>  At this point, you can recall that the condition for the separation of classes is <img src="https://tex.s2cms.ru/svg/%20sign(Xw)%20%3D%20y%20" alt="sign (Xw) = y">  or <img src="https://tex.s2cms.ru/svg/%20sign(Yw)%20%3D%201%20" alt="sign (Yw) = 1">  and since we want to separate classes, we need to solve this problem. <br><br>  We introduce a vector <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  , wherein <img src="https://tex.s2cms.ru/svg/%20b_i%20" alt="b_i">  responsible for the distance from the element <img src="https://tex.s2cms.ru/svg/%20i%20" alt="i">  to the dividing line ( <img src="https://tex.s2cms.ru/svg/%20Yw%20%3D%20b%20" alt="Yw = b">  ).  Since we want all elements to be classified correctly, we introduce the condition <img src="https://tex.s2cms.ru/svg/%20b%20%3E%200%20" alt="b &amp; gt; 0">  .  Then the task will be reduced to <img src="https://tex.s2cms.ru/svg/%20%5Cmin%20%5Csum%20(w%20Y_i%20-%20b_i)%5E2%20" alt="\ min \ sum (w Y_i - b_i) ^ 2">  and will decide how <img src="https://tex.s2cms.ru/svg/%20w%20%3D%20Y%5E%7B%2B%7D%20y%20" alt="w = Y ^ {+} y">  .  You can manually pick up such values <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  that the resulting plane will divide our sample: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>b = np.ones([<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) &gt;&gt;&gt; b[<span class="hljs-number"><span class="hljs-number">3</span></span>] = <span class="hljs-number"><span class="hljs-number">10</span></span> &gt;&gt;&gt; w = np.dot(np.linalg.pinv(Y), b) &gt;&gt;&gt; np.dot(Y, w) array([[ <span class="hljs-number"><span class="hljs-number">0.8540146</span></span> ], [ <span class="hljs-number"><span class="hljs-number">0.98540146</span></span>], [ <span class="hljs-number"><span class="hljs-number">0.81021898</span></span>], [ <span class="hljs-number"><span class="hljs-number">10.02919708</span></span>]])</code> </pre><br><h3>  Ho-Kashyap Algorithm </h3><br>  The Ho-Kashyap algorithm is designed to pick <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  automatically.  Algorithm diagram ( <img src="https://tex.s2cms.ru/svg/%20k%20" alt="k">  - step number <img src="https://tex.s2cms.ru/svg/%20b%5E%7B(0)%7D%20" alt="b ^ {(0)}">  usually taken equal <img src="https://tex.s2cms.ru/svg/%201%20" alt="one">  ): <br><br><ol><li>  Calculate the OLS-regression coefficients ( <img src="https://tex.s2cms.ru/svg/%20w%5E%7B(k)%7D%20%3D%20Y%5E%7B%2B%7D%20b%5E%7B(k)%7D%20" alt="w ^ {(k)} = Y ^ {+} b ^ {(k)}">  ). </li><li>  Calculate the indentation vector <img src="https://tex.s2cms.ru/svg/%20b%5E%7B(k%2B1)%7D%20" alt="b ^ {(k + 1)}">  . </li><li>  If the decision is not agreed ( <img src="https://tex.s2cms.ru/svg/%20b%5E%7B(k)%7D%20%5Cneq%20b%5E%7B(k%2B1)%7D%20" alt="b ^ {(k)} \ neq b ^ {(k + 1)}">  ), then repeat step 1. </li></ol><br>  I want to calculate the indentation vector in some way, like <img src="https://tex.s2cms.ru/svg/%20b%5E%7B(k%2B1)%7D%20%3D%20%5Cmax(Yw%5E%7B(k)%7D%2C%200)%20" alt="b ^ {(k + 1)} = \ max (Yw ^ {(k)}, 0)">  because it minimizes the loss function <img src="https://tex.s2cms.ru/svg/%20%5Cmin%20%5Csum%20(w%20Y_i%20-%20b_i)%5E2%20" alt="\ min \ sum (w Y_i - b_i) ^ 2">  .  Unfortunately, the condition <img src="https://tex.s2cms.ru/svg/%20b%20%3E%200%20" alt="b &amp; gt; 0">  does not allow us to do this, and instead it is proposed to take a step on the positive part of the loss function gradient <img src="https://tex.s2cms.ru/svg/%20e%5E%7B(k)%7D%20%3D%20b%5E%7Bk%7D%20-%20Yw%5E%7B(k)%7D%20" alt="e ^ {(k)} = b ^ {k} - Yw ^ {(k)}">  : <img src="https://tex.s2cms.ru/svg/%20b%5E%7B(k%2B1)%7D%20%3D%20b%5E%7B(k)%7D%20-%20%5Cmu%20(e%5E%7B(k)%7D%20-%20%7Ce%5E%7Bk%7D%7C)%20" alt="b ^ {(k + 1)} = b ^ {(k)} - \ mu (e ^ {(k)} - | e ^ {k} |)">  where <img src="https://tex.s2cms.ru/svg/%20%5Cmu%20" alt="\ mu">  - step gradient descent, decreasing in the course of solving the problem. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>e = -np.inf * np.ones([<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) &gt;&gt;&gt; b = np.ones([<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>]) &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> np.any(e &lt; <span class="hljs-number"><span class="hljs-number">0</span></span>): ... w = np.dot(np.linalg.pinv(Y), b) ... e = b - np.dot(Y, w) ... b = b - e * (e &lt; <span class="hljs-number"><span class="hljs-number">0</span></span>) ... &gt;&gt;&gt; b array([[ <span class="hljs-number"><span class="hljs-number">1.</span></span>], [ <span class="hljs-number"><span class="hljs-number">1.</span></span>], [ <span class="hljs-number"><span class="hljs-number">1.</span></span>], [ <span class="hljs-number"><span class="hljs-number">12.</span></span>]]) &gt;&gt;&gt; w array([[ <span class="hljs-number"><span class="hljs-number">2.</span></span>], [<span class="hljs-number"><span class="hljs-number">-1.</span></span>], [<span class="hljs-number"><span class="hljs-number">-2.</span></span>]])</code> </pre><br><div style="text-align:center;"><img src="https://habrastorage.org/files/d83/d93/904/d83d9390457447edae015fadf98aee5d.png"></div><br><br>  In the case of a linearly separable sample, the algorithm always converges and converges to the separating plane (if all the elements of the gradient are <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  non-negative, they are all zero). <br><br>  In the case of a linearly inseparable sample, the loss function can be arbitrarily small, since it suffices to multiply <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  and <img src="https://tex.s2cms.ru/svg/%20w%20" alt="w">  on a constant to change its value and, in principle, the algorithm may not converge.  The search does not give any specific recommendations on this topic. <br><br><h3>  Connection of the Ho-Kashyap algorithm and linear SVM </h3><br>  It can be noted that if the object is classified correctly, then the error in the stated optimization problem ( <img src="https://tex.s2cms.ru/svg/%20%5Cmin%20%5Csum_%7Bi%7D%20(w%20Y_i%20-%20b_i)%5E2%20" alt="\ min \ sum_ {i} (w Y_i - b_i) ^ 2">  ) the error on it can be reduced to zero.  If the object is classified incorrectly, then the minimum error on it is equal to the square of its indent from the dividing line ( <img src="https://tex.s2cms.ru/svg/%20(w%20Y_i)%5E2%20" alt="(w Y_i) ^ 2">  ).  Then the loss function can be rewritten in the form: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20%5Cmin_%7Bw%7D%20%5Csum_%7Bi%7D%20%5Cmax(0%2C%201%20-%20w%20Y_i)%5E2%20-%202%5Cmax(0%2C%201%20-%20w%20Y_i)%20%20" alt="\ min_ {w} \ sum_ {i} \ max (0, 1 - w Y_i) ^ 2 - 2 \ max (0, 1 - w Y_i)"></div><br>  In turn, the loss function of a linear SVM is: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%20%5Cmin_%7Bw%7D%20%5Clambda%20%7C%7Cw%7C%7C%5E2%20%2B%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%7D%20%5Cmax(0%2C%201%20-%20w%20Y_i)%20" alt="\ min_ {w} \ lambda || w || ^ 2 + \ frac {1} {N} \ sum_ {i} \ max (0, 1 - w Y_i)"></div><br>  Thus, the problem solved by the Ho-Kashyap algorithm is an analogue of SVM with a quadratic loss function (it penalizes farther for emissions far from the dividing plane) and ignores the width of the dividing strip (that is, it‚Äôs not looking for a plane as far as possible from the nearest properly classified elements, and any dividing plane). <br><br><h3>  Multidimensional case </h3><br>  It may be recalled that the OLS-regression is an analogue of <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B4%25D0%25B8%25D1%2581%25D0%25BA%25D1%2580%25D0%25B8%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25BD%25D1%2582_%25D0%25A4%25D0%25B8%25D1%2588%25D0%25B5%25D1%2580%25D0%25B0">Fisher‚Äôs</a> two-class <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B4%25D0%25B8%25D1%2581%25D0%25BA%25D1%2580%25D0%25B8%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25BD%25D1%2582_%25D0%25A4%25D0%25B8%25D1%2588%25D0%25B5%25D1%2580%25D0%25B0">linear discriminant</a> (their solutions coincide up to a constant).  The Ho-Kashpyap algorithm can also be applied to the case <img src="https://tex.s2cms.ru/svg/%20K%20" alt="K">  classes - in this <img src="https://tex.s2cms.ru/svg/%20w%20" alt="w">  and <img src="https://tex.s2cms.ru/svg/%20b%20" alt="b">  become matrices <img src="https://tex.s2cms.ru/svg/%20W%20" alt="W">  and <img src="https://tex.s2cms.ru/svg/%20B%20" alt="B">  size <img src="https://tex.s2cms.ru/svg/%20D%20%5Ctimes%20K%20" alt="D \ times K">  and <img src="https://tex.s2cms.ru/svg/%20N%20%5Ctimes%20K%20" alt="N \ times K">  accordingly where <img src="https://tex.s2cms.ru/svg/%20D%20" alt="D">  - the dimension of the problem, and <img src="https://tex.s2cms.ru/svg/%20N%20" alt="N">  - number of objects.  In this case, the columns corresponding to the wrong classes should have negative values. <br><br><h3>  Thanks </h3><br>  <a href="https://habrahabr.ru/users/parpalak/" class="user_link">parpalak</a> for a convenient editor. <br>  <a href="https://habrahabr.ru/users/rocket3/" class="user_link">rocket3</a> for the original article. <br><br><h3>  Links </h3><br><blockquote>  (1) <a href="http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture10.pdf">http://www.csd.uwo.ca/~olga/Courses/CS434a_541a/Lecture10.pdf</a> <br>  (2) <a href="http://research.cs.tamu.edu/prism/lectures/pr/pr_l17.pdf">http://research.cs.tamu.edu/prism/lectures/pr/pr_l17.pdf</a> <br>  (3) <a href="http://web.khu.ac.kr/~tskim/PatternClass%2520Lec%2520Note%252007-1.pdf">http://web.khu.ac.kr/~tskim/PatternClass Lec Note 07-1.pdf</a> <br>  (4) A.E.  Lepsky, A.G.  Bronevich <em>Mathematical methods of pattern recognition.</em>  <em>Lecture course</em> <br>  (5) Tu J., Gonzalez R. <em>Principles of Pattern Recognition</em> <br>  (6) R.Duda, P.Hart <em>Pattern Recognition and Scene Analysis</em> </blockquote></div><p>Source: <a href="https://habr.com/ru/post/312774/">https://habr.com/ru/post/312774/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../312760/index.html">How the company abandoned WordPress and replaced it with a self-made CMS</a></li>
<li><a href="../312762/index.html">Everything collapses</a></li>
<li><a href="../312766/index.html">Xcode: probably the best way to work with storyboards</a></li>
<li><a href="../312770/index.html">Happy birthday, Aaron Schwartz</a></li>
<li><a href="../312772/index.html">On October 21, at 19.00 in Moscow, the founder Andy Tryba presents: How we created a non profit competitor Uber in 4 weeks</a></li>
<li><a href="../312776/index.html">DSL for regular expressions on Kotlin</a></li>
<li><a href="../312778/index.html">OpenShift v 3 III. OpenShift Origin 1.3</a></li>
<li><a href="../312780/index.html">Joanna Hoffman - Steve Jobs's ‚ÄúGuardian Angel‚Äù</a></li>
<li><a href="../312782/index.html">Basics of Auto Layout - Concept, structure, application</a></li>
<li><a href="../312784/index.html">NPM module support in backend as a service Scorocode</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>