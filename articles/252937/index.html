<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Sound prints: radio advertising recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="From this article, you will learn that recognizing even short sound fragments in a noisy recording is a completely solvable problem, and the prototype...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Sound prints: radio advertising recognition</h1><div class="post__text post__text-html js-mediator-article">  From this article, you will learn that recognizing even short sound fragments in a noisy recording is a completely solvable problem, and the prototype is generally implemented in 30 lines of Python code.  We will see how the Fourier transform helps here, and we‚Äôll see how the search and matching algorithm works.  The article will be useful if you yourself want to write a similar system, or you wonder how it can be arranged. <br><a name="habracut"></a><br>  To begin with, let us ask ourselves the question: who generally needs to recognize advertising on the radio?  This is useful for advertisers who can track the real outputs of their commercials, catch cases of cropping or interruption;  radio stations can monitor the output of network advertising in the regions, etc.  The same problem of recognition arises if we want to track the playback of a piece of music (which the copyright holders are very fond of), or learn a song from a small fragment (as Shazam and other similar services do). <br><br>  More strictly, the task is formulated as follows: we have a certain set of reference audio fragments (songs or commercials), and there is an audio recording of the air in which some of these fragments presumably sound.  The task is to find all the fragments sounded, to determine the moments of the beginning and the duration of playback.  If we analyze the recordings of the ether, then we need the system as a whole to work faster than real time. <br><br><h3>  How it works </h3><br>  Everyone knows that sound (in the narrow sense) is a wave of compression and rarefaction, spreading through the air.  Sound recording, for example in a wav file, is a sequence of amplitude values ‚Äã‚Äã(physically, it corresponds to the degree of compression, or pressure).  If you opened the audio editor, you probably saw the visualization of this data - a graph of the amplitude versus time (the fragment duration is 0.025 s): 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/6c8/06c/c64/6c806cc64b904e21bdc19b7aabd2c31b.png" alt="image"><br><br>  But we do not perceive these fluctuations in frequency directly, but hear sounds of different frequencies and timbres.  Therefore, another method of sound visualization is often used - the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BF%25D0%25B5%25D0%25BA%25D1%2582%25D1%2580%25D0%25BE%25D0%25B3%25D1%2580%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0">spectrogram</a> , where time is represented on the horizontal axis, frequency is on the vertical axis, and the color of the point denotes amplitude.  For example, here is a spectrogram of the sound of a violin, spanning several seconds in time: <br><br><img src="https://habrastorage.org/files/7e6/44e/b4e/7e644eb4ef1c44b49a37b1d287475735.png"><br><br>  On it are visible individual notes and their harmonics, as well as noise - vertical stripes, covering the entire range of frequencies. <br><br><div class="spoiler">  <b class="spoiler_title">To build such a spectrogram using Python, you can:</b> <div class="spoiler_text">  You can use the <a href="http://www.scipy.org/">SciPy</a> library to load data from a wav file, and use <a href="http://matplotlib.org/">matplotlib</a> to build a spectrogram.  All examples are given for Python version 2.7, but probably should work for version 3 as well.  Suppose that file.wav contains a sound recording with a sampling frequency of 8000 Hz: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyplot, mlab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.io.wavfile <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> collections <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> defaultdict SAMPLE_RATE = <span class="hljs-number"><span class="hljs-number">8000</span></span> <span class="hljs-comment"><span class="hljs-comment"># Hz WINDOW_SIZE = 2048 #  ,    fft WINDOW_STEP = 512 #   def get_wave_data(wave_filename): sample_rate, wave_data = scipy.io.wavfile.read(wave_filename) assert sample_rate == SAMPLE_RATE, sample_rate if isinstance(wave_data[0], numpy.ndarray): #  wave_data = wave_data.mean(1) return wave_data def show_specgram(wave_data): fig = pyplot.figure() ax = fig.add_axes((0.1, 0.1, 0.8, 0.8)) ax.specgram(wave_data, NFFT=WINDOW_SIZE, noverlap=WINDOW_SIZE - WINDOW_STEP, Fs=SAMPLE_RATE) pyplot.show() wave_data = get_wave_data('file.wav') show_specgram(wave_data)</span></span></code> </pre> <br></div></div><br>  The task of searching for a fragment on the air can be divided into two parts: first, candidates are found among a large number of reference fragments, and then to check whether the candidate really sounds in this fragment of the ether, and if so, at what point the sound begins and ends.  Both operations use for their work the ‚Äúimprint‚Äù of a fragment of the sound.  It must be noise resistant and compact enough.  This imprint is constructed as follows: we divide the spectrogram into short segments of time, and in each such segment we search for a frequency with a maximum amplitude (in fact, it is better to look for several maxima in different ranges, but for simplicity we take one maximum in the most meaningful range).  A set of such frequencies (or frequency indices) is an imprint.  Very roughly, we can say that these are ‚Äúnotes‚Äù that sound at each moment in time. <br><br><div class="spoiler">  <b class="spoiler_title">Here's how to get a sound track print.</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_fingerprint</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(wave_data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># pxx[freq_idx][t] -   pxx, _, _ = mlab.specgram(wave_data, NFFT=WINDOW_SIZE, noverlap=WINDOW_OVERLAP, Fs=SAMPLE_RATE) band = pxx[15:250] #     60  1000 Hz return numpy.argmax(band.transpose(), 1) # max     print get_fingerprint(wave_data)</span></span></code> </pre><br></div></div><br>  We can get an imprint of a fragment of the ether and all the reference fragments, and we will only have to learn how to quickly search for candidates and compare the fragments.  First, look at the comparison task.  It is clear that prints will never match exactly due to noise and distortion.  But it turns out that the frequencies thus coarsened in such a way quite well survive all the distortions (the frequencies almost never ‚Äúfloat‚Äù), and a sufficiently large percentage of frequencies coincide exactly - thus, we can only find a shift in which there are many coincidences among the two sequences of frequencies.  A simple way to visualize this is to first find all the pairs of points that match in frequency, and then build a histogram of the time differences between the matching points.  If two fragments have a common area, then there will be a pronounced peak on the histogram (and the peak position indicates the start time of the matched fragment): <br><br><img src="https://habrastorage.org/files/540/1a5/076/5401a5076a32472eb0f272ede53846d4.png"><br><br>  If the two fragments are not connected in any way, then there will be no peak: <br><br><img src="https://habrastorage.org/files/c01/888/325/c01888325302462686524bfb7773e4b5.png"><br><br><div class="spoiler">  <b class="spoiler_title">You can build such a wonderful histogram like this:</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">compare_fingerprints</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(base_fp, fp)</span></span></span><span class="hljs-function">:</span></span> base_fp_hash = defaultdict(list) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> time_index, freq_index <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(base_fp): base_fp_hash[freq_index].append(time_index) matches = [t - time_index <span class="hljs-comment"><span class="hljs-comment">#     for time_index, freq_index in enumerate(fp) for t in base_fp_hash[freq_index]] pyplot.clf() pyplot.hist(matches, 1000) pyplot.show()</span></span></code> </pre><br>  The files on which you can practice your recognition are <a href="https://bitbucket.org/kostialopuhin/radio-ads/">here</a> . <br></div></div><br>  The problem of finding candidates is usually solved with the use of hashing - a larger number of hashes is constructed from the fragment fingerprint, as a rule, these are several values ‚Äã‚Äãfrom the fingerprint, reaching in a row or at some distance.  Various approaches can be viewed at the links at the end of the article.  In our case, the number of reference fragments was about a hundred, and it was possible to do without the candidate selection stage altogether. <br><br><h3>  results </h3><br>  On those records that we had, the <a href="http://en.wikipedia.org/wiki/F1_score">F-score</a> was 98.5%, and the accuracy of determining the beginning was about 1 s.  As expected, most of the errors were on short (4-5 s) videos.  But the main conclusion for me personally is that in such tasks a solution written independently often works better than a ready-made one (for example, from EchoPrint, about which they already <a href="http://habrahabr.ru/post/122969/">wrote</a> on Habr√©, it turned out to squeeze no more than 50-70% because of short clips) simply because that all tasks and data have their own specifics, and when there are many variations in the algorithm and a great deal of arbitrariness in the choice of parameters, understanding of all stages of work and visualization on real data greatly contributes to a good result. <br><br>  <b>Fun facts:</b> <br><ul><li>  On the spectrogram of one of the tracks of the Aphex Twin group there is a <a href="http://www.bastwood.com/%3Fpage_id%3D10">human face</a> </li><li>  The author of the article, which reproduced the Shazam algorithm in Java, received <a href="http://www.royvanrijn.com/blog/2010/07/patent-infringement/">threats</a> from their lawyers </li></ul><br>  <b>Literature:</b> <br><ul><li>  <a href="https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf">An Industrial-Strength Audio Search Algorithm</a> (pdf) </li><li>  <a href="https://laplacian.wordpress.com/2009/01/10/how-shazam-works/">Brief description of the Shazam algorithm</a> and <a href="http://drakulavich.blogspot.ru/2010/10/shazam.html">translation</a> </li><li>  <a href="http://www.royvanrijn.com/blog/2010/06/creating-shazam-in-java/">Shazam Java algorithm reproduction</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/252937/">https://habr.com/ru/post/252937/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../252925/index.html">Error in Delone condition test formula</a></li>
<li><a href="../252927/index.html">Device driver and what it eats</a></li>
<li><a href="../252929/index.html">Practical aspects of using DHCP relay + option82</a></li>
<li><a href="../252931/index.html">Network Management Basics in Microsoft Azure</a></li>
<li><a href="../252933/index.html">Blocking of forbidden resources of RKN by url including https</a></li>
<li><a href="../252939/index.html">Link censorship by Skype (continued)</a></li>
<li><a href="../252941/index.html">Introduction to fetch</a></li>
<li><a href="../252943/index.html">Demo on Alcatel-Lucent booth, OpenTouch platform overview</a></li>
<li><a href="../252947/index.html">3/14/15 9:26:53 Century Celebration of the "Day of the Pi Number", as well as a story about how to get your very personal piece of pi</a></li>
<li><a href="../252949/index.html">Google Code closes and invites everyone to go to GitHub</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>