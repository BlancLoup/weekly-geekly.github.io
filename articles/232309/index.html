<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. Hitachi HUS VM with FMD modules</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Russian office of Hitachi Data Systems kindly provided us with access to its virtual laboratory, in which, specifically for our tests, a stand was...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. Hitachi HUS VM with FMD modules</h1><div class="post__text post__text-html js-mediator-article">  The Russian office of Hitachi Data Systems kindly provided us with access to its virtual laboratory, in which, specifically for our tests, a stand was prepared that included an entry-level storage system Hitachi <a href="http://www.hds.com/assets/pdf/brochure-hus-vm-ru.pdf">Unified Storage VM (HUS VM)</a> .  The main distinctive feature of the architecture of this solution is the specially designed Hitachi flash drive modules - <a href="http://www.hds.com/assets/pdf/brochure-fmd-modules-ru.pdf">Flash Module Drive (FMD)</a> .  In addition to the flash drive, each such module contains its own controller for buffering, data compression, and other additional service operations. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/40f/a50/388/40fa50388b8b5f044c741a91464fcfa2.jpg" height="265" width="400"></div><br><h3>  <b>Testing method</b> </h3><br>  During testing, the following tasks were solved: <br><ul><li>  studies of the process of storage degradation during long-term write load (Write Cliff); </li><li>  performance study of HUS VM storage systems under various load profiles; </li><li>  study of the influence of the number of servers generating load on the performance of the storage system. </li></ul><a name="habracut"></a><br><h4>  Testbed Configuration </h4><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c52/b1f/576/c52b1f576ca1386ae3d1f65848f61a27.jpg" height="448" width="640"></div></td></tr><tr><td>  Figure 1. The block diagram of the test bench. </td></tr></tbody></table><br>  The test bench consists of four servers, each connected by four 8Gb connections to two FC switches, each of which has four 8Gb FC connections to the HUS VM storage system.  By setting zones on the FC switches, 4 independent access paths from each server to the HUS VM storage system are installed. <br>  <abbr title="Manufacturer: Hitachi Data Systems Model: CB540A Server Blade Processor: Intel E5-4650 2.7GHz 8C 20M QPI8.0GT / s - 4 pcs RAM: 512 GB DDR3 1333FC adapter: Emulex 8Gb 2p FC mezzanine - 2 pcs: RHEL 6.3 x86-64">Server</abbr> ;  <abbr title="Manufacturer: Hitachi Data Systems Model: HUS VM Raw Capacity: 46.4 TB Formatted Capacity: 22 TB Volume of Cache Memory: 256 GB Volume of Cache Memory Used for Testing: 32 GB (partition) (the minimum cache size recommended by the system is selected, because preliminary testing showed that the cache size does not affect the performance of FMD modules) Connection Interfaces: 16x FC 8GbNumber of flash modules (data + HS): 28 + 1SVP firmware version: 73-03-04 / 00">Storage system</abbr> <br>  As additional software, Symantec Storage Foundation 6.1 is installed on the test server, which implements: <br><ul><li>  Functional logical volume manager (Veritas Volume Manager); </li><li>  Functional fault-tolerant connection to disk arrays (Dynamic Multi Pathing) for tests of groups 1 and 2. For tests of group 3 uses native Linux DMP) </li></ul><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  On the test server, the following settings were made to reduce disk I / O latency: <br><ul><li> Changed the I / O scheduler from ‚Äúcfq‚Äù to ‚Äúnoop‚Äù by assigning the value to the noop parameter; <code>/sys/&lt;___Symantec_VxVM&gt;/queue/scheduler</code> </li><li>  The following parameter has been added to /etc/sysctl.conf that minimizes the queue size at the level of the Symantec logical volume manager: <code>¬´vxvm.vxio.vol_use_rq = 0¬ª</code> ; </li><li>  The limit of simultaneous I / O requests to the device is increased to 1024 by setting the value of 1024 to the parameter <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nr_requests</code> </li><li>  Disabling the check of the possibility of merging I / O operations (iomerge) by setting the value 1 to <code>/sys/&lt;___Symantec_VxVM&gt;/queue/nomerges</code> </li><li>  read-ahead is disabled by setting the value 0 to <code>/sys/&lt;___Symantec_VxVM&gt;/queue/read_ahead_kb</code> </li><li>  The queue size on FC adapters has been increased by adding the <code>/etc/modprobe.d/lpfc.conf  lpfc lpfc_lun_queue_depth=64 (options lpfc lpfc_lun_queue_depth=64)</code> configuration file <code>/etc/modprobe.d/lpfc.conf  lpfc lpfc_lun_queue_depth=64 (options lpfc lpfc_lun_queue_depth=64)</code> ; </li></ul><br>  On the storage system, the following configuration settings are performed for partitioning disk space: <br><ul><li>  On the HUS VM storage, out of 28 FMD modules, 7 RAID groups RAID1 2D + 2D are created, which are included in one DP pool with a total capacity of 22.4 TB. </li><li>  In this pool, 32 DP-VOL volumes of the same volume are created with a total volume covering the entire capacity of the disk array.  Each of the four test servers presents eight volumes created, accessible through all four paths each. </li></ul><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (performance of synthetic tests) on the storage system, the Flexible IO Tester (fio) version 2.1.4 utility is used.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li>  thread = 0 </li><li>  direct = 1 </li><li>  group_reporting = 1 </li><li>  norandommap = 1 </li><li>  time_based = 1 </li><li>  randrepeat = 0 </li><li>  ramp_time = 10 </li></ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  iostat, part of the sysstat version 9.0.4 package with txk keys; </li><li>  vxstat, which is part of Symantec Storage Foundation 6.1 with svd keys; </li><li>  vxdmpadm, part of Symantec Storage Foundation 6.1 with the -q iostat keys; </li><li>  fio version 2.1.4, to generate a summary report for each load profile. </li></ul><br>  The removal of performance indicators during the test with the utilities iostat, vxstat, vxdmpstat is performed at intervals of 5 seconds. <br></div></div><br><h3>  Testing program. </h3><br>  Testing consisted of 3 groups of tests: <br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h5>  <b>Group 1: Tests that implement long-term random write load generated by a single server.</b> </h5><br>  The tests were performed by creating a synthetic load on the Block Device using fio, which is a <code>stripe, 16 column, stripe unit size=1MiB</code> logical volume <code>stripe, 16 column, stripe unit size=1MiB</code> , created using Veritas Volume Manager from 16 LUNs on the system under test and presented to a single test device. server. <br>  When creating a test load, the following additional parameters of the fio program are used: <br><ul><li>  rw = randwrite </li><li>  blocksize = 4K and 64K </li><li>  numjobs = 64 </li><li>  iodepth = 64 </li></ul><br>  A test group consists of two tests that differ in the size of a block of I / O operations: write tests in 4K and 64K blocks, performed on a fully-marked storage system.  The total volume of the presented LUN is equal to the effective storage capacity.  The test duration is 5 hours. <br><br>  According to the test results, based on the data output by the vxstat command, graphs are generated that combine the test results: <br><ul><li>  IOPS as a function of time; </li><li>  Bandwidth as a function of time; </li><li>  Latency as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  The presence of performance degradation during long-term load on the record and read; </li><li>  The performance of the service processes storage (Garbage Collection) limiting the performance of the disk array to write during a long peak load; </li><li>  The degree of influence of the size of a block of I / O operations on the performance of storage service processes; </li><li>  The amount of space reserved for storage for leveling storage service processes. </li></ul><br><h5>  <b>Group 2: Disk array performance tests under different types of load generated by 4 servers per block device.</b> </h5><br>  The tests were performed by creating a synthetic load on the block device using fio, which is a <code>stripe, 8 column, stripe unit size=1MiB</code> logical volume <code>stripe, 8 column, stripe unit size=1MiB</code> , created using Veritas Volume Manager from 8 LUNs on the system under test and presented to test servers . <br><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters fio: randomrw, rwmixedread): </li></ul><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter fio: blocksize); </li><li>  methods of processing I / O operations: synchronous, asynchronous (variable software parameter fio: ioengine); </li><li>  the number of load generating processes: 1, 2, 4, 8, 16, 32, 64, 128, 160, 192 (changeable software parameter fio: numjobs); </li><li>  queue depth (for asynchronous I / O operations): 32, 64 (changeable software parameter fio: iodepth). </li></ul><br>  A test group consists of a set of tests representing all possible combinations of the above types of load.  To level the impact of the service processes of the storage system (Garbage Collection) on the test results, a test pause is realized equal to the amount of information recorded during the test divided by the performance of the storage service processes (determined by the results of the first group of tests). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Based on the test results, the following graphs are generated for each combination of the following load types based on the data output by the fio software after each of the tests: load profile, method of processing I / O operations, queue depth, which combine tests with different I / O block values : <br><ul><li>  IOPS as a function of the number of load generating processes; </li><li>  Bandwidth as a function of the number of processes that generate the load; </li><li>  Latitude (clat) as a function of the number of processes that generate the load; </li></ul><br>  The analysis of the obtained results is carried out, conclusions are drawn on the load characteristics of the disk array at jobs = 1, at latency &lt;1ms, as well as on maximum performance indicators. <br><br><h5>  <b>Group 3: Performance tests of a disk array with different types of load generated by one and two servers on a block device.</b> </h5><br>  Tests are conducted similarly to tests of group 2, except that the load is generated from one and then from two servers and the maximum performance indicators are investigated.  At the end of the tests, the maximum performance indicators are given, the conclusion is made about the impact of the number of servers from which the load is generated on the storage performance <br></div></div><br><h2>  <b>Test results</b> </h2><br><h5>  <b>Group 1: Tests that implement a continuous load of random write type with a change in the size of the block I / O operations (I / O).</b> <br><br>  With long-term load, regardless of the size of the unit, the performance drop over time is not fixed.  The phenomenon of "Write Cliff" is missing.  Therefore, any maximum performance can be considered average. <br><div class="spoiler">  <b class="spoiler_title">Charts</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b94/5cb/6c4/b945cb6c4de38acf0152d549d48f2dd6.jpg" height="640" width="452"></div></td></tr><tr><td>  Changing the speed of I / O operations (iops), bandwidth and delay (Latency) during long-term recording with 4K and 64K blocks </td></tr></tbody></table><br></div></div><br></h5><h5>  <b>Group 2: Disk array performance tests under different types of load generated by 4 servers per block device.</b> </h5><br><div class="spoiler">  <b class="spoiler_title">Block device performance tables.</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a62/637/cec/a62637cec9a41ec0e33bbe64c7016153.png" height="603" width="640"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f52/0ae/fbb/f520aefbb26da03fbd379cc4ce54a0d7.png" height="609" width="640"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bbe/b51/3a8/bbeb513a81be79753a6b03568f9c75b5.png" height="604" width="640"></div><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Block device performance graphs.</b> <div class="spoiler_text">  (All pictures are clickable) <br><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/728/21f/4f7/72821f4f76b96837cde1391910d866a1.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3c6/dd1/163/3c6dd116331816e0c05148b09c6d50e8.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0ab/a9b/69f/0aba9b69f11ba057cc3c65d08318b2c1.jpg" height="320" width="226"></div><br></td></tr><tr><td>  With random recording </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/521/e70/3bf/521e703bf6f0711e96262e7e85c7971d.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/487/b99/a03/487b99a036e1e499b0ad43ef7993ddb8.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/857/891/a9a/857891a9a00b1292feb45f2260ff7733.jpg" height="320" width="226"></div><br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f5d/cc0/662/f5dcc06629fcc2090442d43d632a1b47.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/417/f76/124/417f761247e954c40c6fdcaaed4d1483.jpg" height="320" width="226"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b1/0a7/2b1/1b10a72b15238b5e68e56e97a26b15ca.jpg" height="320" width="226"></div><br></td></tr></tbody></table><br></div></div><br><h6>  <b>Maximum recorded performance parameters:</b> </h6><br>  Record: <br><ul><li>  147000 IOPS with latency 1.7ms (block 4 / 8KB sync and async qd32) </li><li>  Bandwidth: 3568MB / c for large blocks </li></ul><br>  Reading: <br><ul><li>  298000 IOPS with latency 0,5ms (8KB sync block); </li><li>  390000 IOPS with latency 1,3ms (8KB async qd 32 block); </li><li>  419,000 IOPS with latency 2ms (4 / 8k async qd64); </li><li>  Bandwidth: 5637MB / s for medium blocks (32K). </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  224000 IOPS with latency 0.9ms (4 / 8KB sync block); </li><li>  267,000 IOPS with latency 3,8ms (block 4 / 8KB async qd 64); </li><li>  Bandwidth 6307MB / s for medium blocks (32K) </li></ul><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.144ms for 4K jobs = 1 block </li><li>  When reading 0,36ms for 4K jobs block = 1 </li></ul><br>  1. The performance (iops) of the array is almost the same for 4K and 8K units, which allows us to conclude that the array operates on 8K units. <br>  2. The array works better with medium blocks (16K-64K), and shows good bandwidth for any I / O direction on middle blocks. <br>  3. When reading and mixed I / O by the middle blocks, the system reaches the maximum throughput during operations by the middle blocks already at 4-16 jobs.  With the increase in the number of jobs productivity drops significantly (almost 2 times) <br>  4. Monitoring the performance of the storage system from its interface, as well as the performance of each server individually, showed that the load during the tests was very well balanced between all components of the stand.  (Between servers, between fc server adapters and storage systems, between shelves, between BE SAS interfaces, between moons, raid groups, flash cards). <br><br><h5>  <b>Group 3: Disk array performance tests with different types of load generated by one or two servers on a block device.</b> </h5><br><h6>  <b>Maximum recorded performance parameters for tests from one server:</b> </h6><br>  Record: <br><ul><li>  90000 IOPS (4 / 8K async jobs = 128 qd = 32/64) </li><li>  Bandwidth: 2377MB / c large blocks (64K sync jobs = 64) </li></ul><br>  Reading: <br><ul><li>  145,300 IOPS (4K async jobs = 32 qd64); </li><li>  Bandwidth: 3090MB / s for medium blocks (64K async jobs = 32 qd = 32). </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  97,000 IOPS (4 / 8KB sync4K async jobs = 32 qd = 64); </li><li>  Bandwidth MB4274MB / s medium blocks (16K) (64K async jobs = 128 qd = 32) </li></ul><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.25ms for 4K jobs = 1 block </li><li>  With reading 0.4ms for block 4 / 8K jobs = 1 </li></ul><br><h6>  <b>Maximum recorded performance parameters for storage at tests from two servers:</b> </h6><br>  Record: <br><ul><li>  137000 IOPS (8KB async and sync (with 1ms delay)) </li><li>  Bandwidth: 2388MB / c (1M large async and sync blocks) </li></ul><br>  Reading: <br><ul><li>  232,000 IOPS (4 / 8KB async); </li><li>  Bandwidth: 5410MB / s (64K async block, jobs = 2) </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  214,000 IOPS (block 4 / 8KB async); </li><li>  Bandwidth of 5200 MB / s for medium blocks (16K) </li></ul><br>  Minimal latency fixed: <br><ul><li>  When writing - 0.138 ms for 4K jobs = 1 </li><li>  When reading 0,357ms for 4K jobs = 1 block </li></ul><br>  For clarity, we have compiled a table of maximum performance indicators of HUS VM depending on the number of servers. <br>  generating load: <br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/84b/ccb/895/84bccb895df1b225023094a97b1859cc.png" height="200" width="640"></a> </div><br>  Obviously, the performance of this storage system is directly dependent on the number of servers connected to it.  Most likely, this is caused by the internal data flow processing algorithms and their limitations.  Based on the indicators taken, both from the server and from the storage system, it can be said with confidence that not a single component along the way of the data was overloaded.  Potentially, 1 server used configuration can generate a larger data stream. <br><br><h2>  <b>findings</b> </h2><br>  Summarizing, we can say that the tests revealed several characteristic features of the work of HUS VM with FMD modules: <br><ol><li>  There is no performance degradation on write operations (write cliff); </li><li>  The size of the storage cache does not affect the performance of FMD.  It is used by internal data processing algorithms.  Does not disable or put into Write-Through mode; </li><li>  Maximum system performance directly depends on the number of servers generating load.  4 servers give 2-4 times more performance than one, although not fully loaded.  The reason probably lies in the internal logic of the system. </li><li>  The capacity of 6.3GB / s, obtained on 32K blocks, looks very worthy.  Specialized flash solutions give a large number of IOPS on small blocks, but on large ones, apparently, they rest against the throughput of internal buses.  While HUS VM does not have this problem. </li></ol><br>  Overall, HUS VM impressed as very stable and resilient. <br>  Hi-End systems with all the reliability, functionality and scalability.  The individual features and advantages of the architecture built on "smart" modules (FMD). <br><br>  A separate advantage of HUS VM is its cronyism.  Using specialized flash systems, not <br>  eliminates the need for conventional disk storage, which leads to the complication of the general storage subsystem.  An array with Enterprise functionality (replication, snapshot, etc.) supporting a wide selection of drives from mechanical disks to specialized flash modules allows you to choose the optimal combination for any specific task.  Fully covering all issues of accessibility, storage and data protection. <br><br><br>  <font color="green">PS The author expresses cordial thanks to Pavel Katasonov, Yuri Rakitin, Dmitry Vlasov and all other company employees who participated in the preparation of this material.</font> </div><p>Source: <a href="https://habr.com/ru/post/232309/">https://habr.com/ru/post/232309/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../232293/index.html">Tizen and Ubuntu - a long way to HelloWorld</a></li>
<li><a href="../232295/index.html">A team of scientists from the University of Luxembourg managed to implant "artificial" neurons in the brain of mice</a></li>
<li><a href="../232297/index.html">HLS vs. RTMP - dry statistics</a></li>
<li><a href="../232303/index.html">How to program in Visual C # 2012. 5th ed</a></li>
<li><a href="../232305/index.html">Sessions in PHP - an underwater pebble with asynchronous requests</a></li>
<li><a href="../232313/index.html">WGDC Competition: First Findings and List of Top Questions</a></li>
<li><a href="../232315/index.html">Unlimited 1Gbs Channels in Holland</a></li>
<li><a href="../232317/index.html">4-4-2 turns into 0101: Robocup in Brazil</a></li>
<li><a href="../232323/index.html">Debugging Qt Style Sheet</a></li>
<li><a href="../232327/index.html">Sequel "Intel Inside" in Russian with 3G: tablet review iconBIT NETTAB THOR IZ 3G</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>