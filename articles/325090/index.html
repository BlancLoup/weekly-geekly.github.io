<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We start VMWare ESXi 6.5 under QEMU hypervisor</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There is a wonderful VMWare ESXi hypervisor in the world, and everything is fine in it, but the requirements for the hardware, on which it can work, a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We start VMWare ESXi 6.5 under QEMU hypervisor</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/aa8/cf0/947/aa8cf094742d49198edcbd6e74c8dda5.png"><br><br>  There is a wonderful VMWare ESXi hypervisor in the world, and everything is fine in it, but the requirements for the hardware, on which it can work, are very immodest.  ESXi fundamentally does not support software RAIDs, 100-megabit and cheap gigabit network cards, so you can try what it is like to work by purchasing the appropriate hardware. <br>  However, ESXi is the most ‚Äúdelicious‚Äù feature of ESXi when we have more than one ESXi host, but clustering, live migration, VSAN distributed storage, a distributed network switch, and so on.  In this case, the cost of the test equipment can already amount to a decent amount.  Fortunately, ESXi supports Nested Virtualization - that is, the ability to run from under an already running hypervisor.  At the same time, the external hypervisor understands that his guest needs access to hardware virtualization, and ESXi knows that it does not work on bare metal.  As a rule, ESXi is also used as the main hypervisor - this configuration has been supported by VMWare for quite some time.  We will try to run ESXi using the QEMU hypervisor.  The network has instructions on this subject, but, as we will see below, they are slightly out of date. <br><a name="habracut"></a><br>  To begin with, we denote the version of QEMU, on which we will conduct experiments: <br><br><pre><code class="bash hljs">user@debian-pc:~$ QEMU emulator version 2.8.0(Debian 1:2.8+dfsg-2) Copyright (c) 2003-2016 Fabrice Bellard and the QEMU Project developers</code> </pre> <br>  The latest version at the moment, but my focus even turned out to be 2.4.0. <br>  Then, turn off the impolite behavior of the KVM module at times when the guest tries to read machine-specific registers, which are not really present.  By default, KVM in response to this, generates inside the guest an exception to the <a href="https://en.wikipedia.org/wiki/General_protection_fault">General protection fault</a> , which causes the guest to go to the blue (in our case, pink) death screen.  Let's do it under the root: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> <code class="bash hljs">root@debian-pc:/&gt; <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> 1 &gt; /sys/module/kvm/parameters/ignore_msrs</code> </pre> <br>  In some distributions, the kvm module is loaded by default with the necessary parameters, in some - not.  In any case, you need to check dmesg for the presence of lines. <br><br><pre> <code class="bash hljs">user@debian-pc:~$ dmesg | grep kvm [ 6.266942] kvm: Nested Virtualization enabled [ 6.266947] kvm: Nested Paging enabled</code> </pre> <br>  If these lines are not present, add the line to /etc/modprobe.d/kvm.conf <br><br><pre> <code class="bash hljs">options kvm-amd npt=1 nested=1</code> </pre> <br>  and reboot.  For an Intel processor, the line will look like this: <br><br><pre> <code class="bash hljs">options kvm-intel ept=1 nested=1</code> </pre> <br>  What is interesting is that only kvm-amd delivers messages about enabled Nested Paging / Nested Virtualization to dmesg, but kvm-intel does not do this. <br><br>  Let's try to solve the problem ‚Äúin the forehead‚Äù - let's go to the <a href="https://my.vmware.com/en/web/vmware/evalcenter%3Fp%3Dfree-esxi6">VMWare</a> website, register there and download the latest VMware-VMvisor-Installer-201701001-4887370.x86_64.iso image. <br><br>  We will not be blunt, we will create an analog ‚Äúflash drive‚Äù on 16Gb, take for sure the supported e1000 network card, put RAM in 4 Gb (with less memory, ESXi is not guaranteed to come up) and assume that at least ESXi will not see the IDE in this configuration -disk: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-img create -f qcow2 -o nocow=on /media/storage/VMs/esx_6.5-1.qcow2 16G Formatting <span class="hljs-string"><span class="hljs-string">'/media/storage/VMs/esx_6.5-1.qcow2'</span></span>, fmt=qcow2 size=17179869184 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16 nocow=on</code> </pre> <br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -hda /media/storage/VMs/esxi_6.5-1.qcow2 -cdrom /media/storage/iso/VMware-VMvisor-Installer-201701001-4887370.x86_64.iso -netdev user,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  And here we are waited by the first surprise - ESXi not only detects our IDE disk, but also successfully installs on it, although, for five minutes, it hangs on 27% of the installation: <br><br><img src="https://habrastorage.org/files/70c/eb1/4e8/70ceb14e82314194b3e9b1126e5b695b.png"><br><br><img src="https://habrastorage.org/files/c41/312/e3a/c41312e3aaf54a4aa924b1a3fef0cf86.png"><br><br>  By the way, before starting the installation, I get this message: <br><br><img src="https://habrastorage.org/files/69f/843/432/69f84343278746d4bef5533b70db3144.png"><br><br>  Well, with the processor it is clear - I used the option -cpu host, which copies the CPUID of the host processor into a guest, and my host processor is the AMD A8-3850 APU for the deceased socket FM1.  It is strange that ESXi is generally placed on such an iron. <br><br>  But 8086: 100e is the identifier of the chip ‚ÄúIntel 82540EM Gigabit Ethernet Controller‚Äù, which for some time was <a href="https://kb.vmware.com/selfservice/microsites/search.do%3Flanguage%3Den_US%26cmd%3DdisplayKC%26externalId%3D2087970">declared</a> unsupported, i.e.  He works, but technical support does not work with him. <br><br>  In general, QEMU supports emulation of various network cards: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 -device <span class="hljs-built_in"><span class="hljs-built_in">help</span></span> &lt; &gt; Network devices: name <span class="hljs-string"><span class="hljs-string">"e1000"</span></span>, bus PCI, <span class="hljs-built_in"><span class="hljs-built_in">alias</span></span> <span class="hljs-string"><span class="hljs-string">"e1000-82540em"</span></span>, desc <span class="hljs-string"><span class="hljs-string">"Intel Gigabit Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"e1000-82544gc"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel Gigabit Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"e1000-82545em"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel Gigabit Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"e1000e"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel 82574L GbE Controller"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82550"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82550 Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82551"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82551 Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82557a"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82557A Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82557b"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82557B Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82557c"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82557C Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82558a"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82558A Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82558b"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82558B Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82559a"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82559A Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82559b"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82559B Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82559c"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82559C Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82559er"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82559ER Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82562"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82562 Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"i82801"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Intel i82801 Ethernet"</span></span> name <span class="hljs-string"><span class="hljs-string">"ne2k_isa"</span></span>, bus ISA name <span class="hljs-string"><span class="hljs-string">"ne2k_pci"</span></span>, bus PCI name <span class="hljs-string"><span class="hljs-string">"pcnet"</span></span>, bus PCI name <span class="hljs-string"><span class="hljs-string">"rocker"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"Rocker Switch"</span></span> name <span class="hljs-string"><span class="hljs-string">"rtl8139"</span></span>, bus PCI name <span class="hljs-string"><span class="hljs-string">"usb-bt-dongle"</span></span>, bus usb-bus name <span class="hljs-string"><span class="hljs-string">"usb-net"</span></span>, bus usb-bus name <span class="hljs-string"><span class="hljs-string">"virtio-net-device"</span></span>, bus virtio-bus name <span class="hljs-string"><span class="hljs-string">"virtio-net-pci"</span></span>, bus PCI, <span class="hljs-built_in"><span class="hljs-built_in">alias</span></span> <span class="hljs-string"><span class="hljs-string">"virtio-net"</span></span> name <span class="hljs-string"><span class="hljs-string">"vmxnet3"</span></span>, bus PCI, desc <span class="hljs-string"><span class="hljs-string">"VMWare Paravirtualized Ethernet v3"</span></span> &lt;   &gt;</code> </pre> <br>  but not all of them work equally well in ESXi, for example, with the formally supported e1000e, port forwarding does not work in the user-mode network, and half of the packets in vmxnet3 are missing.  So let's stop at e1000. <br><br>  Reboot the VM and see that the hypervisor started successfully.  Actually, that's all - QEMU patch for ESXi, as recommended by some manuals, is not necessary. <br><br>  It should be noted that I use the nocow = on parameter when creating a disk, since the VM disk will lie on btrfs, which itself is a file system with the concept of copy-on-write.  If we add to this the fact that a thin-provisioned qcow2 disk also implements this principle, then we get a multiple increase in the number of records per disk.  The nocow = on parameter causes qemu-img to create a file with the nocow attribute and thus block the copy-on-write mechanism in btrfs for a specific file. <br><br>  In the user network mode, a lightweight DHCP server is running inside the VM, so you do not need to assign an address, but you have to forward ports.  Go to the QEMU console by pressing Ctrl + Alt + 1, enter the command there <br><br><pre> <code class="bash hljs">&gt; hostfwd_add tcp::4443-:443</code> </pre> <br>  and forward port 443 from the network interface of the virtual machine to port 4443 of the host.  Then in the browser we type <br><br>  <a href="https://localhost:4443/ui">https: // localhost: 4443 / ui</a> <br><br>  confirm the security exception (ESXi, of course, while using a self-signed certificate for https) and see the hypervisor web interface: <br><br><img src="https://habrastorage.org/files/b34/8d3/90b/b348d390bab84a62be24d16596b651ca.png"><br><br>  Surprisingly, the ESXi installer even created ‚Äústorage‚Äù in the free disk space as large as 8Gb.  First of all, we will install a package with an updated Web-interface, because the development of this most useful component is faster than new versions of ESXi are released.  Go to the QEMU console via Ctrl + Alt + 1 and forward port 22 there: <br><br><pre> <code class="bash hljs">&gt; hostfwd_add tcp::2222-:22</code> </pre> <br>  then we switch to the hypervisor console via Ctrl + Alt + 2, press F2 - Troubleshooting Options - Enable SSH and connect with the SSH client: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ ssh root@127.0.0.1 -p 2222 Password: The time and date of this login have been sent to the system logs. VMware offers supported, powerful system administration tools. Please see www.vmware.com/go/sysadmintools <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> details. The ESXi Shell can be disabled by an administrative user. See the vSphere Security documentation <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> more information.</code> </pre> <br>  Go to the temporary directory <br><br><pre> <code class="bash hljs">[root@localhost:~] <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /tmp</code> </pre> <br>  Download the update <br><br><pre> <code class="bash hljs">[root@localhost:/tmp] wget http://download3.vmware.com/software/vmw-tools/esxui/esxui-offline-bundle-6.x-5214684.zip Connecting to download3.vmware.com (172.227.88.162:80) esxui-offline-bundle 100% |************************************************************************************| 3398k 0:00:00 ETA</code> </pre> <br>  and put it <br><br><pre> <code class="bash hljs">[root@localhost:/tmp] esxcli software vib install -d /tmp/esxui-offline-bundle-6.x-5214684.zip Installation Result Message: Operation finished successfully. Reboot Required: <span class="hljs-literal"><span class="hljs-literal">false</span></span> VIBs Installed: VMware_bootbank_esx-ui_1.17.0-5214684 VIBs Removed: VMware_bootbank_esx-ui_1.8.0-4516221 VIBs Skipped:</code> </pre> <br>  As you can see, the size of the web interface is just over three megabytes. <br><br>  Now we will try to improve our virtual machine.  First of all, we will change the disk controller from IDE to AHCI, because the implementation of the PIIX3 controller of 1996 release in QEMU, as it were, is slightly slow.  And the AHCI controller (Intel ICH9 chipset is emulated), firstly, faster, and, secondly, it supports <a href="https://ru.wikipedia.org/wiki/NCQ">NCQ</a> command queues. <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-1.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device ide-drive,drive=drive0,bus=ahci.0 -netdev user,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  Even by reducing the load time of the components of the hypervisor, we can see that the gain in speed we received.  To celebrate, go to the Web interface and ... how is it no disks?  There is an AHCI controller on the ‚ÄúAdapters‚Äù tab, but disks are not detected on it.  So how did the hypervisor boot up?  Very simple - at the initial stage, the loader reads the disk data using the BIOS and it does not need to see the disks directly.  After the components are loaded into memory, the loader transfers control to them, and the hypervisor initializes without a disk access. <br><br><img src="https://habrastorage.org/files/bb4/6e4/f2e/bb46e4f2eaff4b5ea9050ec72d9487f8.png"><br><br>  Anyway, ESXi 6.5 does not see disks on the AHCI controller, but ESXi 6.0 saw these disks - I give a tooth.  With the help of Google and such a mother, we find out the <a href="http://www.nxhut.com/2016/11/fix-slow-disk-performance-vmwahci.html">reason</a> : in ESXi 6.5, the old ahci driver is replaced with the completely rewritten vmw_ahci driver, which is why SSDs are slowing down a lot of people, and we don‚Äôt have disks.  According to the advice from the article doing on the hypervisor <br><br><pre> <code class="bash hljs">[root@localhost:~] esxcli system module <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> --enabled=<span class="hljs-literal"><span class="hljs-literal">false</span></span> --module=vmw_ahci</code> </pre> <br>  reboot and ... nothing happens.  What did we want?  There are no disks, there is nowhere to write the configuration, therefore, our changes have not been preserved.  It is necessary to return to the IDE-disk, execute this command and only then boot from AHCI - then the disks will be detected. <br><br>  By the way, if we go to the web interface, we will see that the 8-gigabyte ‚Äústorage‚Äù created by the installer is now unavailable.  So for different types of controllers, VMWare has different storage definition policies.  Now let's try to depict the actual system configuration, when the ESXi is installed on a USB flash drive, and the storage is located on hard drives.  We use USB 3.0 emulation: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-1.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0 -netdev user,id=hostnet0, -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  USB 3.0 drives are also not detected.  Apparently, the driver is rewritten here too.  Well, we already know what to do.  We go to the console hypervisor, we write there <br><br><pre> <code class="bash hljs"> esxcli system module <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> -m=vmkusb -e=FALSE</code> </pre> <br>  When the system boots, go to Storage - Devices and see our USB flash drive there.  By the way, with the USB 3.0 controller nec-usb-xhci, the system boots much faster than with ich9-usb-ehci2. <br><br><img src="https://habrastorage.org/files/542/e6f/bf0/542e6fbf09594fd6975521db8245d32c.png"><br><br>  So, at least two disk controller drivers in ESXi 6.5 are rewritten compared to ESXi 6.0.  And it would seem - only the figure after the point in the version number has changed, one can say, a minor release. <br><br>  If we add a 1 Tb disk to the virtual machine configuration, we can create a full-fledged storage in addition to the disk with the hypervisor.  In order for the system to boot from a usb disk, and not from ahci, we use the bootindex parameter.  Usually, the -boot parameter is used to control the boot order, but in our case it will not help, because the disks are ‚Äúhanging‚Äù on different controllers.  At the same time, we will replace the platform from the old 440fx chipset to the new Q35 / ICH9. <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-img create -f qcow2 -o nocow=on /media/storage/VMs/esxi_6.5-1.qcow2 1T user@debian-pc:~$ qemu-system-x86_64 -machine q35 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-1.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0,bootindex=1 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-1-1T.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive1 -device ide-drive,drive=drive1,bus=ahci.0,bootindex=2 -netdev user,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  We go to the console - here they are, our disks. <br><br><img src="https://habrastorage.org/files/e4f/766/78b/e4f76678b10a4b7d89021dcda0a4cdab.png"><br><br>  Let's continue the experiments: now we need to combine several hypervisors into a network.  Some libvirt independently creates a virtual switch and connects machines to it, and we will try to carry out these operations manually. <br><br>  Let us have two virtual machines, which means we need two virtual adapters <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo ip tuntap add mode tap tap0 user@debian-pc:~$ sudo ip tuntap add mode tap tap1</code> </pre> <br>  Now we need a virtual switch.  For a long time, it was customary for these purposes to use the virtual switch brctl managed by the Linux kernel.  Now it is decided to solve the problem through Open vSwitch - a switch implementation designed specifically for virtual environments.  Open vSwitch has built-in support for VLANs, tunneling protocols (GRE, etc.) for combining multiple switches and, most interestingly, OpenFlow technology.  In other words, the L2 / L3 filtering rules can be downloaded to the switch in a readable format.  Previously, filtering required using iptables / ebtables, but, as they say, the good thing ‚Äúebtables‚Äù will not be called. <br><br>  Install Open vSwitch if it is not yet worth it: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo aptitude install openvswitch-switch</code> </pre><br>  Create a virtual switch: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo ovs-vsctl add-br ovs-bridge</code> </pre> <br>  Add interfaces to it: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo sudo ovs-vsctl add-port ovs-bridge tap0 user@debian-pc:~$ sudo sudo ovs-vsctl add-port ovs-bridge tap1</code> </pre> <br>  Let's see what happened: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo ovs-vsctl show e4397bbd-0a73-4c0b-8007-12872cf132d9 Bridge ovs-bridge Port <span class="hljs-string"><span class="hljs-string">"tap1"</span></span> Interface <span class="hljs-string"><span class="hljs-string">"tap1"</span></span> Port ovs-bridge Interface ovs-bridge <span class="hljs-built_in"><span class="hljs-built_in">type</span></span>: internal Port <span class="hljs-string"><span class="hljs-string">"tap0"</span></span> Interface <span class="hljs-string"><span class="hljs-string">"tap0"</span></span> ovs_version: <span class="hljs-string"><span class="hljs-string">"2.6.2"</span></span></code> </pre> <br>  Run the interfaces: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo ip link <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> tap0 up user@debian-pc:~$ sudo ip link <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> tap1 up user@debian-pc:~$ sudo ip link <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> ovs-bridge up</code> </pre> <br>  Now assign the address to the switch interface: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ sudo ip addr add 192.168.101.1 dev ovs-bridge</code> </pre> <br>  It would seem that simply changing the type of network in the QEMU command line from user to tap, like this: <br><br><pre> <code class="bash hljs">-netdev tap,ifname=tap,script=no,downscripot=no,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  and everything will work. <br><br>  Let's try: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 -machine q35 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-1.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0,bootindex=1 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-1-1T.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive1 -device ide-drive,drive=drive1,bus=ahci.0,bootindex=2 -netdev tap,ifname=tap0,script=no,downscript=no,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  Go to the ESXi console and assign it the address - 192.168.101.2, and then check the connection: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ ping 192.168.101.2 PING 192.168.101.2 (192.168.101.2) 56(84) bytes of data. 64 bytes from 192.168.101.2: icmp_seq=1 ttl=64 time=0.582 ms 64 bytes from 192.168.101.2: icmp_seq=2 ttl=64 time=0.611 ms</code> </pre> <br>  ... <br>  and from the ESXi console - F2- Test Network <br><br>  Everything works, pings go. <br><br>  Make a copy of the disk esxi_6.5-1.qcow2 and run the second instance of ESXi: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-img create -f qcow2 -o nocow=on /media/storage/VMs/esxi_6.5-2.qcow2 16G Formatting <span class="hljs-string"><span class="hljs-string">'/media/storage/VMs/esxi_6.5-2.qcow2'</span></span>, fmt=qcow2 size=17179869184 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16 nocow=on user@debian-pc:~$ dd <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=/media/storage/VMs/esxi_6.5-1.qcow2 of=/media/storage/VMs/esxi_6.5-2.qcow2 bs=16M 31+1 records <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> 31+1 records out 531759104 bytes (532 MB, 507 MiB) copied, 10.6647 s, 49.9 MB/s user@debian-pc:~$ qemu-img create -f qcow2 -o nocow=on /media/storage/VMs/esxi_6.5-2-1T.qcow2 1T Formatting <span class="hljs-string"><span class="hljs-string">'/media/storage/VMs/esxi_6.5-2-1T.qcow2'</span></span>, fmt=qcow2 size=1099511627776 encryption=off cluster_size=65536 lazy_refcounts=off refcount_bits=16 nocow=on user@debian-pc:~$ qemu-system-x86_64 -machine q35 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-2.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0,bootindex=1 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-2-1T.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive1 -device ide-drive,drive=drive1,bus=ahci.0,bootindex=2 -netdev tap,ifname=tap0,script=no,downscript=no,id=hostnet0 -device e1000,netdev=hostnet0,id=net0</code> </pre> <br>  Surprises await us here: ping from the host to the first guest goes only until we start ping to the second guest.  After the interruption of the second ping command, the packets to the first guest start walking in 10 seconds. Pings between guests do not go at all. <br><br>  It is clear that we screwed up with mac-addresses, and indeed, QEMU assigns the same mac-address to all tap adapters, unless otherwise specified. <br><br>  Turn off both ESXi'a, show them the unique mac'i and run again. <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 -machine q35 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-1.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0,bootindex=1 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-1-1T.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive1 -device ide-drive,drive=drive1,bus=ahci.0,bootindex=2 -netdev tap,ifname=tap0,script=no,downscript=no,id=hostnet0 -device e1000,netdev=hostnet0,id=net0,mac=DE:AD:BE:EF:16:B6</code> </pre> <br><br>  And in another console: <br><br><pre> <code class="bash hljs">user@debian-pc:~$ qemu-system-x86_64 -machine q35 --<span class="hljs-built_in"><span class="hljs-built_in">enable</span></span>-kvm -cpu host -smp 2 -m 4096 -device nec-usb-xhci,id=xhci -drive file=/media/storage/VMs/esxi_6.5-2.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive0 -device usb-storage,drive=drive0,bus=xhci.0,bootindex=1 -device ich9-ahci,id=ahci -drive file=/media/storage/VMs/esxi_6.5-2-1T.qcow2,<span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=none,id=drive1 -device ide-drive,drive=drive1,bus=ahci.0,bootindex=2 -netdev tap,ifname=tap0,script=no,downscript=no,id=hostnet0 -device e1000,netdev=hostnet0,id=net0,mac=DE:AD:BE:EF:C3:FD</code> </pre> <br>  To our great surprise, the problem with pings has not gone away, moreover, the arp command shows that the MAC addresses of the hypervisors have not changed.  Now is the time to remember how the network works in ESXi: the physical network card is transferred to ‚Äúillegible mode‚Äù and connected as one of the ports to the virtual switchboard.  Another port is connected to this switch vmkernel interface, which is a network card from the point of view of the hypervisor.  At the time of installing ESXi, the hardware address of the physical network card is cloned into vmkernel, so as not to confuse the system administrator.  After that, it can be <a href="https://kb.vmware.com/selfservice/microsites/search.do%3Flanguage%3Den_US%26cmd%3DdisplayKC%26externalId%3D1031111">changed</a> only by deleting the interface and re-creating it or by specifying the hypervisor that the vmkernel should be reconfigured due to a change in the address of the physical map. <br><br>  The first way: <br><br>  Delete: <br><br><pre> <code class="bash hljs">esxcfg-vmknic -d -p pgName</code> </pre> <br>  Create: <br><br><pre> <code class="bash hljs">esxcfg-vmknic -a -i DHCP -p pgName</code> </pre> <br>  or <br><br><pre> <code class="bash hljs">esxcfg-vmknic -a -i xxxx -n 255.255.255.0 pgName</code> </pre> <br>  The second way: <br><br><pre> <code class="bash hljs">esxcfg-advcfg -s 1 /Net/FollowHardwareMac</code> </pre> <br>  The essential difference between these methods is that the former does not require a reboot of the hypervisor, and the latter does. <br><br>  Having executed these simple operations, we will receive two hypervisors in one network. <br><br>  Now you can install vCenter and check ‚Äúlive migration‚Äù.  For some time, vCenter is available as an image of a Linux virtual machine and the corresponding services on board.  This is the option we will try to install.  We take the image of VMware-VCSA-all-6.5.0-5178943.iso, mount it in the host OS, run the installer from the vcsc-ui-installer \ lin64 directory and deploy the image, following the wizard‚Äôs instructions.  The virtual machine will require 10 Gb of RAM, so on the host system it would be nice to have at least 16 Gb.  However, my image turned around and on 12 Gb RAM, having eaten all the available memory and drove the system into a swap. <br><br><img src="https://habrastorage.org/files/ad2/e69/40e/ad2e6940e63646e1a7247124b7838255.png"><br><br>  After installing VCSA, go to the Web-based interface with credentials of the form administrator@mydomain.tlb and password, which we specified when configuring SSO.  After that we add both hosts to vCenter and we get the simplest cluster in which live migration is running.  Configure vMotion on the network cards of both hosts, create a TestVM virtual machine and make sure that it can move from one host to another, changing both the host and the repository. <br><br><img src="https://habrastorage.org/files/8ff/98b/a92/8ff98ba92d8c4b09b19327c0a45297c6.png"><br><br>  By the way, on previous versions of ESXi up to and including 6.0, the virtual machine could not be started in nested virtualization mode without adding a line <br><br><pre> <code class="bash hljs">vmx.allowNested = TRUE</code> </pre> <br>  in its configuration file.  In ESXi 6.5, this is not required, and the virtual machine starts up without issues. <br><br>  In conclusion, a small life hack.  Suppose you need to copy a VM disk file from an ESXi storage somewhere to a backup server, and you cannot use FastSCP from Veeam Backup and Replication.  The good old rsync will come to your rescue, you just need to find a binary file that will run on ESXi.  Unfortunately, there is a <a href="https://bugzilla.samba.org/show_bug.cgi%3Fid%3D8177">bug</a> in rsync up to version 3.0.9 inclusively, because of which large files on VMFS volumes are incorrectly processed, so you should use rsync version 3.1.0.  and higher.  You can take it <a href="https://damiendebin.net/blog/2013/12/06/esxi-5-dot-1-and-rsync/">here</a> . </div><p>Source: <a href="https://habr.com/ru/post/325090/">https://habr.com/ru/post/325090/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../325078/index.html">MEPhI organizes information security competition for students</a></li>
<li><a href="../325080/index.html">How to let cryptographers sink a company</a></li>
<li><a href="../325082/index.html">Terminal graphics</a></li>
<li><a href="../325084/index.html">Test items. Opinions and speculations</a></li>
<li><a href="../325088/index.html">Blog a la Habr, the choice of platform</a></li>
<li><a href="../325092/index.html">Several arguments against Dependency Injection and Inversion of Control</a></li>
<li><a href="../325094/index.html">‚ÄúPresent documents‚Äù or what will help to recognize a passport</a></li>
<li><a href="../325096/index.html">Kaggle: British satellite imagery. How we took the third place</a></li>
<li><a href="../325098/index.html">A new record in speech recognition: the error rate of the algorithm is reduced to 5.5%</a></li>
<li><a href="../325102/index.html">GTK3 applications in a browser with https and basic auth</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>