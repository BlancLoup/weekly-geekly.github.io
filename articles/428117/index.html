<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Google's free tensor processors in the cloud laboratory</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, Google provided free access to its tensor processors (tensor processing unit, TPU) on a cloud-based machine learning lab . The tensor proces...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Google's free tensor processors in the cloud laboratory</h1><div class="post__text post__text-html js-mediator-article">  Recently, Google provided free access to its <a href="https://cloud.google.com/tpu/">tensor processors</a> (tensor processing unit, TPU) on a cloud-based machine learning <a href="https://colab.research.google.com/">lab</a> .  The tensor processor is a specialized integrated circuit (ASIC) developed by Google for machine learning tasks using the TensorFlow library.  I decided to try teaching a Keras convolutional network on TPU, which recognizes objects in CIFAR-10 images.  The complete solution code can be viewed and run in a <a href="https://colab.research.google.com/drive/1FWI2XEovgVptqD7GpNx9pb6a7JQcA2cI">laptop</a> . <br><br><img src="https://habrastorage.org/webt/sl/ut/ho/slutho5dsyeduk2biql9rcsblnu.jpeg"><br>  <i>Photos <a href="https://cloud.google.com/tpu/">cloud.google.com</a></i> <br><a name="habracut"></a><br><h2>  Tensor processors </h2><br>  On Habr√©, they already wrote about how TPUs are arranged ( <a href="https://habr.com/post/402955/">here</a> , <a href="https://habr.com/post/350042/">here</a> and <a href="https://habr.com/company/squadra-group/blog/326308/">here</a> ), and also <a href="https://habr.com/post/422317/">why TPUs are well suited for learning neural networks</a> .  Therefore, I will not go into the details of the TPU architecture, but consider only those features that need to be considered when training neon networks. <br><br>  Now there are three generations of tensor processors, the performance of TPU is the last, the third generation is 420 TFlops (trillions of floating point operations per second), it contains 128 GB of High Bandwidth Memory.  However, only the second generation TPUs with 180 TFlops of performance and 64 GB of memory are available at the Colaboratory.  In the future, I will consider these TPUs. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The tensor processor consists of four chips, each of which contains two cores, a total of eight cores in the TPU.  Training on TPU is conducted in parallel on all cores using replication: each core runs a copy of the TensorFlow graph with one eighth of the data. <br><br>  The basis of the tensor processor is the matrix unit (MXU).  It uses a clever data structure, a 128x128 <a href="https://en.wikipedia.org/wiki/Systolic_array">systolic array</a> for efficient implementation of matrix operations.  Therefore, in order to make the best use of the resources of the TPU equipment, the dimension of the mini-sample or attributes must be a multiple of 128 ( <a href="https://cloud.google.com/tpu/docs/tpus">source</a> ).  Also, due to the peculiarities of the TPU memory system, it is desirable that the dimension of the mini-sample and features be a multiple of 8. <br><br><h2>  Collaboration Platform </h2><br>  <a href="https://colab.research.google.com/">Collaboration</a> is Google‚Äôs cloud platform for promoting machine learning technologies.  On it you can get a free virtual machine with installed popular libraries TensorFlow, Keras, sklearn, pandas, etc.  The most convenient thing is that you can run laptops like Jupyter on Colaboratory.  Laptops are saved on Google Drive, you can distribute them and even organize collaboration.  This is what a laptop at Colaboratory looks like (the <i>image is clickable</i> ): <br><br> <a href=""><img src="https://habrastorage.org/webt/4b/gp/sn/4bgpsnbpkhwyqrid6fixnaurcbw.png"></a> <br><br>  You write code in a browser in a laptop, it runs in a virtual machine in the Google cloud.  The car is given to you for 12 hours, after that it stops.  However, nothing prevents you from running another virtual machine and working for another 12 hours.  Just keep in mind that after stopping a virtual machine, all data from it is deleted.  Therefore, do not forget to save the necessary data to your computer or Google Drive, and after restarting the virtual machine, download again. <br><br>  Detailed instructions for working on the Colaboratory platform are <a href="https://habr.com/post/358146/">here</a> , <a href="https://habr.com/post/348058/">here</a> and <a href="https://www.asozykin.ru/deep_learning/2018/04/04/Google-Colaboratory-for-Deep-Learning.html">here</a> . <br><br><h2>  We connect the tensor processor at Colaboratory </h2><br>  By default, Collaboration does not use GPU or TPU computing accelerators.  You can connect them in the menu Runtime -&gt; Change runtime type -&gt; Hardware accelerator.  In the list that appears, select "TPU": <br><img src="https://habrastorage.org/webt/1f/7r/vt/1f7rvtfjvdowdjwrz0ctgyyly7s.png" alt="image"><br><br>  After choosing the type of accelerator, the virtual machine to which the laptop connects Colaboratory will restart and TPU will become available. <br><br>  If you upload any data to the virtual machine, it will be deleted during the restart process.  Have to download data again. <br><br><h2>  Keras neural network for recognition of CIFAR-10 </h2><br>  As an example, let's try to teach a neural network on Keras on TPU, which recognizes images from the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 data set</a> .  This is a popular data set containing small images of 10 class objects: an airplane, a car, a bird, a cat, a deer, a dog, a frog, a horse, a ship, and a truck.  Classes do not overlap, each object in the picture belongs to only one class. <br><br>  Load the CIFAR-10 dataset with Keras: <br><br><pre><code class="python hljs">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</code> </pre> <br>  To create a neural network, I started a separate function.  We will create the same model two times: the first version of the model for TPU, on which we will train, and the second for the CPU, where we will recognize objects. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">create_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> input_layer = Input(shape=(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), dtype=tf.float32, name=<span class="hljs-string"><span class="hljs-string">'Input'</span></span>) x = BatchNormalization()(input_layer) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = BatchNormalization()(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.25</span></span>)(x) x = Flatten()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">512</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)(x) output_layer = Dense(<span class="hljs-number"><span class="hljs-number">10</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>)(x) model = Model(inputs=[input_layer], outputs=[output_layer]) model.compile( optimizer=tf.train.AdamOptimizer(<span class="hljs-number"><span class="hljs-number">0.001</span></span>), loss=tf.keras.losses.sparse_categorical_crossentropy, metrics=[<span class="hljs-string"><span class="hljs-string">'sparse_categorical_accuracy'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre> <br>  For now, the Keras optimizers cannot be used on the TPU, so when compiling the model, the optimizer from TensorFlow is specified. <br><br>  Create a Keras model for the CPU, which in the next step will be converted into a model for TPU: <br><br><pre> <code class="python hljs">cpu_model = create_model()</code> </pre> <br><h2>  We convert the neural network on Keras into a model for TPU </h2><br>  Keras and TensorFlow models can be trained on the GPU without any changes.  For now, it is impossible to do this on TPU, so we will have to convert the model we have created into a model for TPU. <br><br>  First you need to know where the available TPU is.  On the Colaboratory platform, this can be done with the following command: <br><br><pre> <code class="python hljs">TPU_WORKER = <span class="hljs-string"><span class="hljs-string">'grpc://'</span></span> + os.environ[<span class="hljs-string"><span class="hljs-string">'COLAB_TPU_ADDR'</span></span>]</code> </pre> <br>  In my case, the TPU address turned out to be this - <code>grpc://10.102.233.146:8470</code> .  Addresses were different for different launches. <br><br>  Now you can get a model for the TPU using the <code>keras_to_tpu_model</code> function: <br><br><pre> <code class="python hljs">tf.logging.set_verbosity(tf.logging.INFO) tpu_model = tf.contrib.tpu.keras_to_tpu_model( cpu_model, strategy=tf.contrib.tpu.TPUDistributionStrategy( tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))</code> </pre> <br>  The first line includes logging at the Info level.  Here's what's in the model conversion log: <br><br> <code>INFO:tensorflow:Querying Tensorflow master (b'grpc://10.102.233.146:8470') for TPU system metadata. <br> INFO:tensorflow:Found TPU system: <br> INFO:tensorflow:*** Num TPU Cores: 8 <br> INFO:tensorflow:*** Num TPU Workers: 1 <br> INFO:tensorflow:*** Num TPU Cores Per Worker: 8 <br> ... <br> WARNING:tensorflow:tpu_model (from tensorflow.contrib.tpu.python.tpu.keras_support) is experimental and may change or be removed at any time, and without warning.</code> <br> <br>  You can see that the TPU was found at the address we specified earlier, it has 8 cores.  We also see a warning that <code>tpu_model</code> is experimental and can be changed or deleted at any time.  I hope that in time it will be possible to train Keras models directly on TPU without any transformation. <br><br><h2>  We teach the model on TPU </h2><br>  The model for TPU can be trained in the usual way for Keras by calling the <code>fit</code> method: <br><br><pre> <code class="python hljs">history = tpu_model.fit(x_train, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">128</span></span>*<span class="hljs-number"><span class="hljs-number">8</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">50</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>)</code> </pre> <br>  What are the features here.  We remember that in order to effectively use the TPU, the size of the mini-sample must be a multiple of 128. In addition, one-eighth of all data in the mini-sample is performed on each TPU core.  Therefore, when training, the size of the mini-sample is set to 128 * 8, 128 images for each TPU core are obtained.  You can use a larger size, for example, 256 or 512, then the performance will be higher. <br><br>  In my case, an average of 6 seconds is required for the training of one epoch. <br><br>  The quality of education in the 50 era: <br> <code>Epoch 50/50 <br> - 6s - loss: 0.2727 - sparse_categorical_accuracy: 0.9006</code> <br> <br>  The share of correct answers to the data for training was 90.06%.  Check the quality of the test data using TPU: <br><br><pre> <code class="python hljs">scores = tpu_model.evaluate(x_test, y_test, verbose=<span class="hljs-number"><span class="hljs-number">0</span></span>, batch_size=batch_size * <span class="hljs-number"><span class="hljs-number">8</span></span>) print(<span class="hljs-string"><span class="hljs-string">"     : %.2f%%"</span></span> % (scores[<span class="hljs-number"><span class="hljs-number">1</span></span>]*<span class="hljs-number"><span class="hljs-number">100</span></span>))</code> </pre> <br> <code>     : 80.79%</code> <br> <br>  Now save the weight of the trained model: <br><br><pre> <code class="python hljs">tpu_model.save_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  TensorFlow will give us a message that the weights are transferred from the TPU to the CPU: <br> <code>INFO:tensorflow:Copying TPU weights to the CPU</code> <br> <br>  It should be noted that the weights of the trained network are preserved on the disk of the Colaboratory virtual machine.  When the virtual machine is stopped, all data from it will be erased.  If you do not want to lose the trained weights, then save them to your computer: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> google.colab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> files files.download(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br><h2>  Recognize objects on the CPU </h2><br>  Now let's try to use a TPU-trained model to recognize objects in images using the CPU.  To do this, we create a new model and load TPU-trained weights into it: <br><br><pre> <code class="python hljs">model = create_model() model.load_weights(<span class="hljs-string"><span class="hljs-string">"cifar10_model.h5"</span></span>)</code> </pre> <br>  The model is ready for use on the central processor.  Let's try to recognize with its help one of the images of the test set CIFAR-10: <br><br><pre> <code class="python hljs">index=<span class="hljs-number"><span class="hljs-number">111</span></span> plt.imshow(toimage(x_test[index])) plt.show()</code> </pre> <br><img src="https://habrastorage.org/webt/za/z3/f-/zaz3f-jatj-5crlgsih84gsakg0.png"><br><br>  The picture is small, but you can understand that this is an airplane.  Run the recognition: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#      CIFAR-10 classes=['', '', '', '', '', '', '', '', '', ''] x = x_test[index] #  , .. Keras    x = np.expand_dims(x, axis=0) #   prediction = model.predict(x) #       print(prediction) #     prediction = np.argmax(prediction) print(classes[prediction])</span></span></code> </pre> <br>  We get a list of output values ‚Äã‚Äãof neurons, almost all of them are close to zero, except for the first value, which corresponds to the plane. <br><br> <code>[[9.81738389e-01 2.91262069e-07 1.82225723e-02 9.78524668e-07 <br> 5.89265142e-07 6.76223244e-10 1.03252004e-10 9.23009047e-09 <br> 3.71878523e-05 3.16599618e-08]] <br> </code> <br> <br>  Recognition was successful! <br><br><h2>  Results </h2><br>  It was possible to demonstrate the performance of TPU on the platform of Laboratory, it can be used to train neural networks on Keras.  However, the CIFAR-10 data set is too small, it is not enough to fully load TPU resources.  The acceleration in comparison with the GPU turned out to be small (you can check for yourself by choosing the GPU instead of the TPU as the accelerator and retraining the model again). <br><br>  On Habr√© there is an article in which <a href="https://habr.com/post/354602/">measured the performance of TPU and GPU V100 on the training network ResNet-50</a> .  On this task, the TPU showed the same performance as the four V100 GPUs.  It's nice that Google provides such a powerful accelerator for learning neural networks for free! <br><br>  Video demonstration of teaching neural network Keras on TPU. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/60xbDEpA49M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  useful links </h2><br><ol><li>  <a href="https://colab.research.google.com/drive/1FWI2XEovgVptqD7GpNx9pb6a7JQcA2cI">Colaboratory's laptop with full Keras TPU model learning code</a> . </li><li>  <a href="https://colab.research.google.com/gist/ceshine/f196d6b030adb1ec3a8d0b50642709dc/keras-fashion-mnist-tpu.ipynb">The Colaboratory laptop with an example of Keras‚Äôs TPU training for recognizing clothes and shoes from Fashion MNIST</a> . </li><li>  <a href="https://cloud.google.com/tpu/">Tensor processors in the Google cloud</a> . </li><li>  <a href="https://cloud.google.com/tpu/docs/tpus">Features of the architecture and use of tensor processors</a> . </li></ol></div><p>Source: <a href="https://habr.com/ru/post/428117/">https://habr.com/ru/post/428117/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428107/index.html">Containerizing Angular 6 SPA Template ASP .NET Core 2.1 Applications</a></li>
<li><a href="../428109/index.html">Corporate wall</a></li>
<li><a href="../428111/index.html">Arithmetic of arbitrary precision in Erlang</a></li>
<li><a href="../428113/index.html">On the question of Bezier curves, speed Arduino and one interesting site, or how I spent the weekend</a></li>
<li><a href="../428115/index.html">Web development for ecommerce: 5 technology trends for 2019</a></li>
<li><a href="../428119/index.html">Class-fields-proposal or What went wrong on the tc39 committer</a></li>
<li><a href="../428121/index.html">Stan Drapkin. High-level cryptography hooks in .NET</a></li>
<li><a href="../428123/index.html">Security Week 41: Good News</a></li>
<li><a href="../428125/index.html">Who are the product analysts and why are they needed in the team?</a></li>
<li><a href="../428127/index.html">Nginx cache: all new - well forgotten old</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>