<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Building client routing / semantic search at Profi.ru</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Building client routing / semantic search and clustering arbitrary external corpuses at Profi.ru 
 Tldr 


 2 department of the DS Department for a bi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Building client routing / semantic search at Profi.ru</h1><div class="post__text post__text-html js-mediator-article"><h1 id="building-client-routing--semantic-search-and-clustering-arbitrary-external-corpuses-at-profiru">  Building client routing / semantic search and clustering arbitrary external corpuses at Profi.ru </h1><br><h2 id="tldr">  <strong>Tldr</strong> </h2><br><p>  2 department of the DS Department for a bit more than a few months. done at first). </p><a name="habracut"></a><br><h2 id="projected-goals">  Projected goals </h2><br><ol><li>  If you‚Äôre looking at what you‚Äôre looking for, then you can‚Äôt understand how to do it. </li><li>  Find totally new services and synonyms for the existing services; </li><li>  As a sub-goal of (2) - learn to build proper clusters on arbitrary external corpuses; </li></ol><br><h2 id="achieved-goals">  Achieved goals </h2><br><p>  It was clear that I‚Äôm not able to get it. enough proxies + probably some experience with selenium). </p><br><p>  <strong>Business goals:</strong> </p><br><ol><li> ~ <code>88+%</code> (vs ~ <code>60%</code> with elastic search) accuracy for client routing / intent classification (~ <code>5k</code> classes); </li><li>  Search is agnostic to input quality (misprints / partial input); </li><li>  Classifier generalizes the morphologic structure of the language is exploited; </li><li>  Classifier severely beats elastic on various benchmarks (see below); </li><li>  New services were found + at least <code>15,000</code> synonyms (vs. the current state of <code>5,000</code> + ~ <code>30,000</code> ).  I expect this figure to double; </li></ol><br><p>  The last bullet is a ballpark estimate, but a conservative one. <br>  Also AB tests will follow.  But I am confident in these results. </p><br><p>  <strong>"Scientific" goals:</strong> </p><br><ol><li>  We have been thoroughly compared with the database of service synonyms; </li><li>  We are able to beat weakly supervised (see their bag-size-a-bag-of-ngrams) elastic pattern on this benchmark (see details below) using <strong>UNSUPERVISED</strong> methods; </li><li>  We‚Äôve been developing a model for the RLP, which is an RRNNGNG case-style bag; </li><li>  We demonstrated that our final embedding technique was combined with state-of-the-art unsupervised algorithms (UMAP + HDBSCAN) can produce stellar clusters; </li><li>  We demonstrated the possibility of feasibility and usability of: <br><ul><li>  Knowledge distillation; </li><li>  Augmentations for text data (sic!); </li></ul></li><li>  Training text-based classifiers with dynamic augmentations reduced convergence of time drastically (10x) compared to generating static datasets (ie, CNN); </li></ol><br><h2 id="overall-project-structure">  Overall project structure </h2><br><p>  This does not include the final classifier. <br>  We also have redeemed the classifier bottleneck. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c2/449/157/1c24491576ed703ebc571dfd4d7d8da3.png"></p><br><h2 id="what-works-in-nlp-now">  What works in NLP now? </h2><br><p>  A birds' eye view: <br><img src="https://habrastorage.org/getpro/habr/post_images/5a1/8f5/df1/5a18f5df1e133bef082edf9315011da7.png"></p><br><p>  Also you may know that NLP may be experiencing the <a href="https://thegradient.pub/nlp-imagenet/">moment now</a> . </p><br><h2 id="large-scale-umap-hack">  Large scale UMAP hack </h2><br><p>  UMAP to 100m + point (or maybe even 1bn) sized datasets.  Essentially build a KNN graph with <a href="https://github.com/facebookresearch/faiss">FAIS</a> and then just rewrite the main UMAP loop into PyTorch using your GPU.  We‚Äôve only got 10-15m points after all, but please follow this <a href="https://github.com/lmcinnes/umap/issues/125">thread</a> for details. </p><br><h2 id="what-works-best">  What works best </h2><br><ul><li>  For supervised fast-text classification meets the RNN (bi-LSTM) + carefully chosen set of n-grams; </li><li>  Implementation - plain python for <a href="https://t.me/snakers4/2137">n-grams</a> + PyTorch Embedding bag layer; </li><li>  For clustering - this model + UMAP + HDBSCAN; </li></ul><br><h2 id="best-classifier-benchmarks">  <strong>Best classifier benchmarks</strong> </h2><br><p>  <strong>Manually annotated dev set</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/04b/7cc/e7e/04b7cce7e8cee9cee4b066b6a353bed9.jpg"></p><br><p>  <strong>Left to right:</strong> <br>  (Top1 accuracy) </p><br><ul><li>  Current algorithm (elastic search); </li><li>  First RNN; </li><li>  New annotation; </li><li>  Tuning; </li><li>  Fast-text embedding bag layer; </li><li>  Adding typos and partial input; </li><li>  Dynamic generation of errors and partial input ( <strong>training time reduced by 10x</strong> ); </li><li>  Final score; </li></ul><br><p>  <strong>Manually annotated dev set + 1-3 errors per query</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ae2/a31/040/ae2a31040dbd77402d6b6dfee9eeba28.jpg"></p><br><p>  <strong>Left to right:</strong> <br>  (Top1 accuracy) </p><br><ul><li>  Current algorithm (elastic search); </li><li>  Fast-text embedding bag layer; </li><li>  Adding typos and partial input; </li><li>  Dynamic generation of errors and partial input; </li><li>  Final score; </li></ul><br><p>  <strong>Manually annotated dev set + partial input</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/680/681/c3c680681dd3166b95246930f1f1b1a8.jpg"></p><br><p>  <strong>Left to right:</strong> <br>  (Top1 accuracy) </p><br><ul><li>  Current algorithm (elastic search); </li><li>  Fast-text embedding bag layer; </li><li>  Adding typos and partial input; </li><li>  Dynamic generation of errors and partial input; </li><li>  Final score; </li></ul><br><h2 id="large-scale-corpuses--n-gram-selection">  Large scale corpuses / n-gram selection </h2><br><ul><li>  We collected the largest corpuses for the Russian language: <br><ul><li>  <a href="https://t.me/snakers4/2127">Areneum</a> - a processed version is available here - dataset authors did not reply; </li><li>  <a href="https://t.me/snakers4/2131">Taiga</a> ; </li><li>  <a href="https://spark-in.me/post/parsing-common-crawl-in-four-simple-commands">Common crawl</a> and <a href="https://t.me/snakers4/2136">wiki</a> - please follow these articles; </li></ul></li><li>  We collected a <code>100m</code> word dictionary using <a href="https://t.me/snakers4/2147">1TB crawl</a> ; </li><li>  Also use this <a href="https://t.me/snakers4/2148">hack</a> to download such files faster (overnight); </li><li>  (I.e. 500kg class) </li></ul><br><p>  <strong>Stress test of our 1M n-grams on 100M vocabulary:</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/198/1fe/38b/1981fe38b03b4cdf76022f4ff6ef0074.png" alt="image"></p><br><h2 id="text-augmentations">  Text augmentations </h2><br><p>  In a nutshell: </p><br><ul><li>  Take a large dictionary with errors (eg 10-100m unique words); </li><li>  Generate an error (drop a letter, swap a letter using calculated probabilities, insert a random letter, maybe use keyboard layout, etc); </li><li>  Check that new word is in dictionary; </li></ul><br><p>  If you‚Äôre trying to get the best of your money, you‚Äôll be able to do this.  <strong>30-50% of words we had on some corpuses</strong> . </p><br><p>  <strong>Our approach is far superior, if you have access to a large domain vocabulary</strong> . </p><br><h2 id="best-unsupervised--semi-supervised-results">  Best unsupervised / semi-supervised results </h2><br><p>  KNN is used to compare different embedding methods. </p><br><p>  (vector size) List of models tested: </p><br><ul><li>  (512) Large scale fighter sentence trained on 200 GB of common crawl data; </li><li>  (300) sentence a sentence sentence from a service; </li><li>  (300) Fast-text obtained from here, pre-trained on araneum corpus; </li><li>  (200) Fast-text trained for our domain data; </li><li>  (300) Fast-text trained on 200GB of Common Crawl data; </li><li>  (300) A Siamese network trained with services / synonyms / random sentences from Wikipedia; </li><li>  (200) First iteration of embedding bag embossing bag; </li><li>  (200) Same, the word is taken; </li><li>  (300); </li><li>  (300); </li><li>  (250) Bottleneck layer (250 neurons); </li><li>  Weakly supervised elastic search baseline; </li></ul><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ca1/e0b/e9c/ca1e0be9c152d092d4149f9986b87289.png" alt="default"></p><br><p>  To avoid leaks, all sampled.  It was compared with services / synonyms.  It was taken by the women to get vocabularies (it was not embraced by the words of the Wikipedia sentence). </p><br><h2 id="cluster-visualization">  Cluster visualization </h2><br><p>  <strong>3D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/4b7/f10/d19/4b7f10d19a785b5f690a28f2e2a039e6.gif"></p><br><p>  <strong>2D</strong> <br><img src="https://habrastorage.org/getpro/habr/post_images/ad7/0ad/441/ad70ad441ecae6f396c8bb76826484df.png"></p><br><h2 id="cluster-exploration-interface">  Cluster exploration "interface" </h2><br><p>  Green - new word / synonym. <br>  Gray background - likely new word. <br>  Gray text - existing synonym. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/cda/d17/00f/cdad1700ff0701ff6643a4aa14041d31.jpg"></p><br><h2 id="ablation-tests-and-what-works-what-we-tried-and-what-we-did-not">  We didn‚Äôt </h2><br><ol><li>  See the above charts; </li><li>  Plain average / tf-idf average of fast-text embeddings - a <strong>VERY formidable baseline</strong> ; </li><li>  Fast-text&gt; Word2Vec for Russian; </li><li>  Sentence embedding by fake; </li><li>  BPE (sentencepiece) showed no improvement on our domain; </li><li>  Char level models struggled to generalize, despite the recant paper from google; </li><li>  We tried a multi-head transformer (LSTM-based models.)  When we migrated, we‚Äôdn‚Äôt be able to follow the embracing bag; </li><li>  <strong>BERT</strong> - it seems to be overkill </li><li>  <strong>ELMO</strong> - <strong>Seems</strong> like I think it‚Äôs not worth it; </li></ol><br><h2 id="deploy">  Deploy </h2><br><p>  Done using: </p><br><ul><li>  Docker container with a simple web-service; </li><li>  CPU-only for inference is enough; </li><li>  ~ <code>2.5 ms</code> per query on CPU, batching not really necessary; </li><li>  ~ <code>1GB</code> RAM memory footprint; </li><li>  Almost no dependencies, apart from <code>PyTorch</code> , <code>numpy</code> and <code>pandas</code> (and web server ofc). </li><li>  Mimic fast-text n-gram generation like <a href="https://t.me/snakers4/2137">this</a> ; </li><li>  Embedding bag layer + indexes as a directory; </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/428674/">https://habr.com/ru/post/428674/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428662/index.html">Retail Programming Challenge</a></li>
<li><a href="../428664/index.html">Linux kernel boot. Part 1</a></li>
<li><a href="../428666/index.html">How I created a mood-changing animation using CSS masks</a></li>
<li><a href="../428668/index.html">Blizzard announced the release of a reissue of WarCraft III in 2019. Pre-order open</a></li>
<li><a href="../428672/index.html">QuietOn Active Squelch Review</a></li>
<li><a href="../428676/index.html">Breaking down the fundamentals of C #: allocating memory for a reference type on the stack</a></li>
<li><a href="../428680/index.html">Creating and integrating a VK bot into a group via VkBotLongPoll [Python]</a></li>
<li><a href="../428682/index.html">Fallout 76 beta self-destruct</a></li>
<li><a href="../428688/index.html">Setting up the Docker desktop environment for the yii-framework application</a></li>
<li><a href="../428690/index.html">How to teach your girlfriend to program if you are not a teacher, but she believes in you</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>