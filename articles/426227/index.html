<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to farm Kaggle</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="* farm - (from the English. Farming) - long and boring repetition of certain game actions with a specific purpose (gaining experience, resource extrac...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to farm Kaggle</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/du/bk/pz/dubkpzvie8fhp3rdunxfjnkoqki.jpeg" alt="image"><br>  * <strong>farm</strong> - (from the English. Farming) - long and boring repetition of certain game actions with a specific purpose (gaining experience, resource extraction, etc.). </p><br><h1 id="vvedenie">  Introduction </h1><br><p>  Recently (October 1) a new session of an <a href="https://mlcourse.ai/">excellent course on DS / ML</a> started (I highly recommend as an initial course to anyone who wants, as it is now called, ‚Äúenter‚Äù DS).  And, as usual, after the end of any course, graduates have a question - and where now to get practical experience in order to consolidate the still raw theoretical knowledge.  If you ask this question on any profile forum, the answer is likely to be the same - go decide Kaggle.  Kaggle is yes, but where to start and how to most effectively use this platform for leveling up practical skills?  In this article, the author will try to give his own answers to these questions, as well as describe the location of the main rake on the field of competitive DS, in order to speed up the process of pumping and receive from this fan. </p><a name="habracut"></a><br><p>  A few words about the <a href="https://mlcourse.ai/">course</a> from its creators: </p><br><p>  <em>The mlcourse.ai course is one of the largest activities of the OpenDataScience community.</em>  <em><a href="https://habr.com/users/yorko/">@yorko</a> and company (~ 60 people) demonstrate that you can get cool skills outside the university and even absolutely free.</em>  <em>The main idea of ‚Äã‚Äãthe course is the optimal combination of theory and practice.</em>  <em>On the one hand, the presentation of the basic concepts is not without mathematics, on the other hand - a lot of homework, Kaggle Inclass competitions and projects will give, with a certain investment of strength on your part, excellent machine learning skills.</em>  <em>It is necessary to note the competitive nature of the course - there is a general rating of students, which strongly motivates.</em>  <em>The course is also different in that it takes place in a truly lively community.</em> </p><br><p>  <em>The course includes two Kaggle Inclass competitions.</em>  <em>Both are very interesting, the construction of features works well in them.</em>  <em>The first is <a href="https://www.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2">user identification by the sequence of visited sites</a> .</em>  <em>The second is the <a href="https://www.kaggle.com/c/how-good-is-your-medium-article">prediction of the popularity of the article on Medium</a> .</em>  <em>The main benefit is from two homework assignments, where you have to be smart and beat the baselines in these competitions.</em> </p><br><p>  Having paid tribute to the course and its creators, we continue our history ... </p><br><p>  I remember myself a year and a half ago, a <a href="https://www.coursera.org/learn/machine-learning">course (still the first version) from Andrew Ng</a> was completed, <a href="https://www.coursera.org/specializations/machine-learning-data-analysis">specialization from MIPT was completed</a> , a pile of books was read - a little bit of theoretical knowledge is full, but when you try to solve any basic combat task, a stupor arises.  No, how to solve the problem - it is clear which algorithms to use - is also understandable, but the code is written very hard, with a minute call on the sklearn / pandas help, etc.  Why is it so - there are no accrued pipelines and the feeling of the code "at your fingertips". </p><br><p>  So it will not work, the author thought, and went to Kaggle.  It was scary to start from the combat competition right away, and the first sign was Getting started the " <strong><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></strong> " competition, in which that approach to efficient pumping described in this article took shape. </p><br><p>  In what will be described further, there is no know-how, all the techniques, methods and techniques are obvious and predictable, but this does not detract from their effectiveness.  At least, following them, the author managed to take the <strong><a href="https://www.kaggle.com/progression">Kaggle Competition Master die</a></strong> for six months and three competitions in solo mode and, at the time of writing this article, enter the <a href="https://www.kaggle.com/kruegger/competitions">top-200 Kaggle world ranking</a> .  By the way, this answers the question of why the author at all allowed himself the courage to write an article of this kind. </p><br><h1 id="v-dvuh-slovah-chto-voobsche-takoe-kaggle">  In a nutshell, what is Kaggle? </h1><br><p><img src="https://habrastorage.org/webt/wd/h-/ab/wdh-abr6mg1cvivfe3bexrihn0y.png" alt="image"><br>  <a href="https://www.kaggle.com/">Kaggle</a> is one of the most well-known platforms for holding Data Science competitions. In each competition, the organizers post a description of the problem, the data for solving this task, a metric by which the decision will be assessed - and set time limits and prizes.  Participants are given from 3 to 5 attempts (by the will of the organizers) per day for ‚Äúsubmit‚Äù (sending their own solution). </p><br><p>  The data is divided into a training sample (train) and test (test).  For the training part, the value of the target variable (target) is known, for the test one - no.  The task of the participants is to create a model that, being trained in the training part of the data, will give the maximum result on the test. </p><br><p>  Each participant makes predictions for the test sample - and sends the result to Kaggle, then the robot (who knows the target variable for the test) evaluates the result sent, which is displayed on the leaderboard. </p><br><p> But not everything is so simple - the test data, in turn, is divided in a certain proportion into the public (public) and private (private) part.  During the competition, the submitted decision is evaluated, according to the metrics set by the organizers, on the public part of the data and is laid out on the leaderboard (the so-called public leaderboard) - according to which participants can evaluate the quality of their models.  The final decision (usually two - at the participant's choice) is evaluated on the private part of the test data - and the result falls on the private leaderboard, which is available only after the end of the competition and according to which, in fact, the final results are evaluated, prizes, buns, and medals are distributed. </p><br><p>  Thus, during the competition only information is available to participants as their model behaved (what result - or soon it showed) in the public part of the test data.  If, in the case of a spherical horse in a vacuum, the private part of the data is the same in distribution and the statisticians are in the public - everything is fine, but if not, then a model that showed itself well in public may not work on the private part, that is, trust (retrain).  And it is here that what is called a ‚Äúflight‚Äù in the jargon, when people from 10 places in public fly down to 1000-2000 places on the private part due to the fact that their chosen model has retrained and failed to produce the necessary accuracy for new data. </p><br><p><img src="https://habrastorage.org/webt/qy/yl/8h/qyyl8hszncuhi3uzzxqnirrbpjw.png" alt="image"></p><br><p>  How to avoid it?  To do this, first of all, you need to build the correct validation scheme, what is taught in the first lessons in almost all DS courses.  Since  if your model cannot give the correct prediction for data that it has never seen - then you wouldn‚Äôt use whatever sophisticated technique, no matter how complex neural networks would be built - you cannot produce such a model, because  its results are worth nothing. </p><br><p>  For each competition on Kaggle, a separate page is created on which there is a section with data, with a description of the metric - and the most interesting for us is the forum and the kernels. </p><br><p>  Forum he and Kaggle forum, people write, discuss and share ideas.  But kernels are more interesting.  In essence, this is the ability to run your code, which has direct access to competition data in the Kaggle cloud (similar to Amazon AWS, Google GCE, etc.) Limited resources are allocated for each kernel, so if there is not very much data, then work with You can use them directly from the browser on the Kaggle website - write the code, launch it, submit the result.  Two years ago, Kaggle was acquired by Google, so it‚Äôs no wonder that this functionality is used under the hood by the Google Cloud Engine. </p><br><p>  Moreover, there were several competitions (recent - <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge">Mercari</a> ), where data could be worked on only through Kernels.  A very interesting format, leveling the difference in the gland of the participants and forcing the brain to turn on for optimizing the code and approaches, since, naturally, the kernels had a hard resource limit, at that time - 4 cores / 16 GB RAM / 60 minutes run-time / 1 GB scratch and output disk space.  While working on this competition, the author learned more about the optimization of neural networks than from any theoretical course.  It was not enough for gold, finished solo 23rd, but got experience and pleasure fairly ... </p><br><p>  Taking this opportunity, I want to say once again Thanks to colleagues from <strong>ods.ai</strong> - <a href="https://www.kaggle.com/kingarthur7"><strong>Arthur Stepanenko (arthur)</strong></a> , <a href="https://www.kaggle.com/lopuhin"><strong>Konstantin Lopukhin (kostia)</strong></a> , <a href="https://www.kaggle.com/sergeifironov"><strong>Sergey Fironov (sergeif)</strong></a> for advice and support in this competition.  In general, there were many interesting moments, <a href="https://www.kaggle.com/lopuhin"><strong>Konstantin Lopukhin (kostia)</strong></a> , who took the <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussion/50256">first place</a> together with <a href="https://www.kaggle.com/paweljankiewicz"><strong>Pawe≈Ç Jankiewicz</strong></a> , then <a href="https://www.kaggle.com/lopuhin/mercari-golf-0-3875-cv-in-75-loc-1900-s">laid out what</a> they called ‚Äú <strong>reference humiliation in 75 lines</strong> ‚Äù in chatika - a kernel of 75 lines of code, which gives the result to the gold leaderboard zone.  This is, of course, a must see :) </p><br><p>  Well, they were distracted, so - the people write the code and lay out the kernels with solutions, interesting ideas and other things.  Usually, in each competition in a couple of weeks, one or two excellent EDA (exploratory data analysis) of the kernel appears, with a detailed description of the dataset, statistics, characteristics, etc.  And a couple of baselines (basic solutions), which, of course, do not show the best result on the leaderboard, but they can be used as a starting point for creating your own solution. </p><br><h1 id="pochemu-kaggle">  Why Kaggle? </h1><br><p><img src="https://habrastorage.org/webt/wm/dn/nz/wmdnnzz6mg4tmp-lmr1eh7rwgt8.png" alt="image"></p><br><p>  In fact, no matter what platform you play, just Kaggle is one of the first and most promoted, with an excellent community and a fairly comfortable environment (I hope they will finalize the kernels for stability and performance, but many people remember the hell that was going on <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge">Mercari</a> ) But, in general, the platform is very convenient and self-sufficient, and its dies are still appreciated. </p><br><p>  A small digression in general on the topic of competitive DS.  Very often, in articles, conversations and other conversations, the thought sounds that this is all bullshit, the experience in competitions has no relation to real problems, and the people there are engaged in tyunit 5th decimal point, which is insanity and divorced from reality.  Let's look at this issue in more detail: </p><br><p>  As practicing DS-specialists, unlike academy and science, we, in our work, should and will solve business problems.  That is (here is a reference to <a href="https://ru.wikipedia.org/wiki/CRISP-DM">CRISP-DM</a> ) to solve the problem you need: </p><br><ul><li>  understand the business problem </li><li>  evaluate the data on the subject of whether they can hide the answer to this business problem </li><li>  collect additional data if there is not enough existing to receive a response </li><li>  select the <a href="https://habr.com/company/ods/blog/328372/">metric</a> that most accurately approximates the business goal </li><li>  and only after that choose the model, convert the data under the selected model and "drain the boots".  (WITH) </li></ul><br><p>  The first four items from this list are not taught anywhere (correct me if such courses have appeared - I will sign up without hesitation), then just learn from the experience of colleagues working in this industry.  But the last point - starting with the choice of the model and further, it is possible and necessary to pump in competitions. </p><br><p>  In any competition, most of the work for us was done by the organizers.  We have a described business goal, an approximating metric is selected, data is collected - and our task is to build a working pipeline from all of this Lego.  And it is here that the skills are being pumped - how to work with passes, how to prepare data for neural networks and trees (and why neural networks require a special approach), how to correctly build validation, how not to retrain, how to choose hyper parameters, how ... ... moreover a dozen and two ‚Äúhows‚Äù, the competent execution of which distinguishes a good specialist from people in our profession. </p><br><h1 id="chto-mozhno-farmit-na-kaggle">  What can farm on Kaggle </h1><br><p><img src="https://habrastorage.org/webt/un/0g/lv/un0glv3uxuhotixueeazkz8vr3y.png" alt="image"></p><br><p>  Basically, and this is reasonable, all newcomers come to Kaggle to gain and pump practical experience, but one should not forget that besides this there are at least two goals: </p><br><ul><li>  Farm medals and dies </li><li>  Farm reputation in the Kaggle community </li></ul><br><p>  The main thing to remember is <strong><em>that these three goals are completely different, they require different approaches to achieve them, and you should not mix them up especially at the initial stage!</em></strong> </p><br><p>  Not in vain it is emphasized <strong>‚Äúat the initial stage‚Äù</strong> , when you pump through - these three goals will merge into one and will be solved in parallel, but while you are just starting out - <strong>do not mix them</strong> !  This way you will avoid pain, disappointment and resentment of this unjust world. </p><br><p>  Let's go briefly on the objectives from the bottom up: </p><br><ul><li>  <strong>Reputation</strong> - is pumped by writing good posts (and comments) on the forum and creating useful kernels.  For example, EDA kernels (see above), posts describing non-standard techniques, etc. </li><li>  <strong>Medals</strong> are a very controversial and haute topic, but oh well.  Bleeding public Kernels (*) is pumped through, participation in a team with a bias in the experience, and the creation of its own top-line. </li><li>  <strong>Experience</strong> - pumped through the analysis of decisions and work on the bugs. </li></ul><br><p>  (*) <em>blending of public kernels</em> is a <em>technique of farm medals, at which the laid out kernels are selected with the maximum soon on the public leaderboard, their predictions are averaged (blends), the result is submitted.</em>  <em>As a rule, such a method leads to a hard overfit (retraining on the train) and flying to a private flight, but sometimes it allows you to get a submission almost into silver.</em>  <em>The author, at the initial stage, does not recommend such an approach (read below about the belt and pants).</em> </p><br><p>  I recommend the first goal to choose "experience" and stick to it until the moment when you feel ready to work on two / three goals at the same time. </p><br><p>  There are two more points worth mentioning <a href="https://www.linkedin.com/in/iglovikov/"><strong>(Vladimir Iglovikov (ternaus)</strong></a> - thanks for the reminder). </p><br><p>  <strong>The first</strong> is the conversion of efforts invested in Kaggle into a new, more interesting and / or highly paid job.  No matter how now the Kaggle dies are leveled, but for understanding people the line in the summary of the Kaggle Competition Master, and other achievements, are worth something. </p><br><p>  As an illustration of this point, two interviews ( <a href="https://dev.by/news/angarsk-minsk">one</a> , <a href="https://dev.by/news/zachem-pobeditel-kaggle-i-topcoder-pereehal-v-minsk">two</a> ) with our colleagues <a href="https://habr.com/users/cepera_ang/"><strong>Sergey Mushinsky (cepera_ang)</strong></a> and <a href="https://habr.com/users/albu/"><strong>Alexander Buslaev (albu)</strong></a> </p><br><p>  And also the opinion of <a href="https://www.linkedin.com/in/venheads/"><strong>Valery Babushkina</strong> ( <strong>venheads)</strong></a> : </p><br><p>  <strong>Valery Babushkin - Head of Data Science at X5 Retail Group (the current number of units is 30 people + 20 vacancies from 2019)</strong> </p><br><p>  <strong>Team Leader, Analytics, Yandex Advisor</strong> </p><br><p>  <em>Kaggle Competition Master is an excellent proxy metric for evaluating a future team member.</em>  <em>Of course, in connection with the latest events in the form of teams of 30 people each and undisguised locomotives, a little more thorough study of the profile is required than before, but this is still a matter of a few minutes.</em>  <em>A person who has achieved the title of master, is very likely to be able to write at least medium quality code, reasonably understands machine learning, knows how to clean the data and build stable solutions.</em>  <em>If the master can still not boast with a master, then the fact of participation is also a plus, at least the candidate knows about the existence of Kagla and was not lazy and spent time learning it.</em>  <em>And if something other than the public kernel was launched and the resulting solution surpassed its results (which is pretty easy to check), then this is a reason for a detailed conversation about the technical details, which is much better and more interesting than the classical questions on the theory, the answers to which are given less understanding of how a person will cope in the future with work.</em>  <em>The only thing to be afraid of and what I have come across is that some people think that the work of DS is about as Cagle, which is fundamentally wrong.</em>  <em>Many more think that DS = ML, which is also a mistake</em> </p><br><p>  <strong>The second point</strong> is that many problems can be solved in the form of pre-prints or articles, which on the one hand allows the knowledge that the collective mind gave birth to during the competition not to die in the wilds of the forum, and on the other adds one more line to the authors portfolio. and +1 visibility, which in any case has a positive effect on career and on the citation index. </p><br><div class="spoiler">  <b class="spoiler_title">For example, a list of works of our colleagues following several competitions</b> <div class="spoiler_text"><p>  Authors (in alphabetical order): </p><br><p>  <strong>Andrei O., Ilya, albu, aleksart, alex.radionov, almln, alxndrkalinin, cepera_ang, dautovri, davydov, fartuk, golovanov, ikibardin, kes, mpavlov, mvakhrushev, n01z3, rakhlin, rauf, ututut, seselovator, mvakhrushev, n01z3, rakhlin, rautlin, ututv, itlov m m, mvakhrushev, n01z3, rakhlin, rautlin, ututrov, itkvard, mvakhrushev snikolenko, ternaus, twoleggedeye, versus, vicident, zfturbo</strong> </p><br><table><tbody><tr><td>  <strong>Competition Name</strong> <br></td><td>  <strong>Article title</strong> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection">Dstl Satellite Imagery Feature Detection</a> <br></td><td>  <a href="https://arxiv.org/abs/1706.06169">Satellite imagery feature detection using deep convolutional neural network: A Kaggle competition</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/carvana-image-masking-challenge">Carvana Image Masking Challenge</a> <br></td><td>  <a href="https://arxiv.org/abs/1801.05746">TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation</a> <br></td></tr><tr><td>  <a href="http://endovissub2017-giana.grand-challenge.org/">MICCAI2017: Gastrointestinal Image ANAlysis (GIANA)</a> <br></td><td>  <a href="https://arxiv.org/abs/1804.08024">Angioysplasia Detection and Localization Using Deep Convolutional Neural Networks</a> <br></td></tr><tr><td>  <a href="https://endovissub2017-roboticinstrumentsegmentation.grand-challenge.org/">MICCAI2017: Robotic Instrument Segmentation</a> <br></td><td>  <a href="https://arxiv.org/abs/1803.01207">Automatic Instrument Segmentation in Robot-Assisted Surgery Using Deep Learning</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Road Extraction</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Buslaev_Fully_Convolutional_Network_CVPR_2018_paper.pdf">Fully convolutional network for automatic terrain extraction from satellite imagery</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Building Detection</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Iglovikov_TernausNetV2_Fully_Convolutional_CVPR_2018_paper.pdf">Ternausnetv2: Fully convolutional network for instance segmentation</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Land Cover Classification</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Seferbekov_Feature_Pyramid_Network_CVPR_2018_paper.pdf">Feature pyramid network for multi-class land segmentation</a> <br></td></tr><tr><td>  <a href="http://rsnachallenges.cloudapp.net/competitions/4">Pediatric Bone Age Challenge</a> <br></td><td>  <a href="https://doi.org/10.1007/978-3-030-00889-5_34">Pediatric Bone Age Assessment Using Deep Convolution Neural Networks</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/sp-society-camera-model-identification">IEEE's Signal Processing Society - Camera Model Identification</a> <br></td><td>  <a href="https://arxiv.org/abs/1810.02981">Camera Model Identification Using Solution Neural Networks</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/tensorflow-speech-recognition-challenge">TensorFlow Speech Recognition Challenge</a> <br></td><td>  <a href="https://arxiv.org/abs/1810.02364">Deep Learning Approaches for Understanding Simple Speech Commands</a> <br></td></tr><tr><td>  <a href="https://iciar2018-challenge.grand-challenge.org/">ICIAR2018-Challenge</a> <br></td><td>  <a href="https://doi.org/10.1007/978-3-319-93000-8_83">Deep Convolutional Neural Networks for Breast Cancer Histology Image Analysis</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/diabetic-retinopathy-detection">Diabetic Retinopathy Detection</a> <br></td><td>  <a href="https://doi.org/10.1101/225508">Deep Learning classification framework</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Land Cover Classification</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Rakhlin_Land_Cover_Classification_CVPR_2018_paper.pdf">Land Cover Classification from Satellite Imagery With U-Net and Lovasz-Softmax Loss</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Land Cover Classification</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Davydow_Land_Cover_Classification_CVPR_2018_paper.pdf">Land Cover Classification With Superpixels and Jaccard Index Post-Optimization</a> <br></td></tr><tr><td>  <a href="http://deepglobe.org/challenge.html">DEEPGLOBE - CVPR18: Building Detection</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w4/Golovanov_Building_Detection_From_CVPR_2018_paper.pdf">Building Detection from Satellite Composite Loss Function</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/whale-detection-challenge">The Marinexplore and Cornell University Whale Detection Challenge</a> <br></td><td>  <a href="http://lsis.univ-tln.fr/~glotin/icml4b_material_bck/ICML4B_short_Smirnov_CNN_RightWhale.pdf">North Atlantic Right Whale Call Detection with Convolutional Neural Networks</a> <br></td></tr><tr><td>  <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/leaderboards">NIPS 2017: Learning to Run</a> <br></td><td>  <a href="https://arxiv.org/abs/1711.06922">Run, skeleton, run: physics-based simulation</a> <br></td></tr><tr><td>  <a href="https://www.crowdai.org/challenges/nips-2017-learning-to-run/leaderboards">NIPS 2017: Learning to Run</a> <br></td><td>  <a href="https://arxiv.org/abs/1804.00361">Learning to Run Challenges: Adapting reinforcement learning methods for neuromusculoskeletal environments</a> <br></td></tr><tr><td>  <a href="http://image-net.org/challenges/LSVRC/2013/index">ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013)</a> <br></td><td>  <a href="https://www.sciencedirect.com/science/article/pii/S2212671614000146">Comparison of Regularization Methods for ImageNet Classification with Deep Convolutional Neural Networks</a> <br></td></tr><tr><td>  <a href="https://www.msceleb.org/">MS-Celeb-1M (2017)</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/w27/html/Smirnov_Doppelganger_Mining_for_ICCV_2017_paper.html">Doppelganger Mining for Face Representation Learning</a> <br></td></tr><tr><td>  <a href="http://iab-rubric.org/DFW/2018.html">Disguised Faces in the Wild (DFW) 2018</a> <br></td><td>  <a href="http://openaccess.thecvf.com/content_cvpr_2018_workshops/w1/html/Smirnov_Hard_Example_Mining_CVPR_2018_paper.html">Hard Example Mining with Auxiliary Embeddings</a> <br></td></tr><tr><td>  <a href="https://www.kaggle.com/c/favorita-grocery-sales-forecasting">Favorita grocery sales forecasting</a> <br></td><td>  Sales forecasting using WaveNet within the framework of the Kaggle competition <br></td></tr></tbody></table></div></div><br><h1 id="kak-izbezhat-boli-ot-poteri-medali">  How to avoid pain from losing medals </h1><br><p><img src="https://habrastorage.org/webt/hu/s6/mu/hus6mujcgwbgcvo9rqi1dzpplh0.jpeg" alt="image"></p><br><p>  <strong>Score!</strong> </p><br><p>  I will explain.  In almost every competition, closer to its end, a kernel is laid out on the public with a solution that shifts the entire leaderboard up, well, and you, with your decision, are accordingly down.  And every time on the forum begins the <strong>pain!</strong>  As so, here I had a decision on silver, and now I don‚Äôt even draw on bronze.  What are you doing, get everything back. </p><br><p>  Remember - Kaggle is a competitive DS.  Where you are on the leaderboard is up to you.  Not from the guy who laid out the kernel, not from the stars came together or not, but from how much effort you put into the decision and whether you used all possible ways to improve it. </p><br><blockquote>  <strong>If Public Kernel knocks you from your place on the leaderboard - this is not your place.</strong> </blockquote><p>  Instead of pouring out the pain of the injustice of the world - thank this guy.  Seriously, Public Kernel with a better solution than yours means that you have missed something in your pipelines.  Find what exactly, improve your pipeline - and go around the whole crowd of hamsters with the same soon.  Remember, to return to your place you just need to be a little bit better than this public. </p><br><p>  How I was upset by this moment in the first competition, my hands were already falling, here you are in silver - and here you are in the bottom of the leaderboard.  Nothing, you just have to get together, to understand where and what you missed - to alter your decision - and to return to the place. </p><br><p>  Moreover, this moment will be present only at an early stage of your competitive process.  The more experienced you become, the less the lined kernels and stars will influence you.  In one of the last competitions ( <a href="https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection">Talking Data</a> , in which our team <a href="https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/leaderboard">took 8th place</a> ) they also laid out such a kernel, but he was honored with just one line in our team chat from <a href="https://www.kaggle.com/ppleskov"><strong>Pavel Pleskov (ppleskov)</strong></a> : " <em>Guys, I messed it up with our decision it just got worse - we throw it away</em> . "  That is, all the useful signal that this kernel was pulling from the data was already pulled out by our models. </p><br><p>  And about the medals - remember: </p><br><blockquote>  <strong>"a belt without equipment is needed only to maintain pants" (C)</strong> </blockquote><br><h1 id="gde-na-chem-i-kak-pisat-kod">  Where, on what and how to write code. </h1><br><p><img src="https://habrastorage.org/webt/co/6e/as/co6easi6qrnfr5hvvp5mrqv0lyg.jpeg" alt="image"></p><br><p>  Here is my recommendation - <strong>python 3.6</strong> on <strong>jupyter notebook</strong> under <strong>ubuntu</strong> .  <strong>Python</strong> has long become the de facto standard in DS, given the huge number of libraries and the community, <strong>jupyter</strong> , especially with the presence of <strong>jupyter_contrib_nbextensions</strong> very convenient for rapid prototyping, analysis and data processing, <strong>ubuntu</strong> is convenient in itself, plus some of the data processing is sometimes easier to do in <strong>bash</strong> :) </p><br><p>  After installing <strong>jupyter_contrib_nbextensions, I</strong> immediately recommend that you include: </p><br><ul><li>  <em>Collapsible headings (helps a lot in organizing blocks of code)</em> </li><li>  <em>Code folding (same)</em> </li><li>  <em>Split cells (rarely, but useful if you need to debug something in parallel)</em> </li></ul><br><p>  And your life will be much easier and more pleasant. </p><br><p>  As soon as your pipelines become more or less stable, I recommend to immediately take the code into separate modules.  Believe me - you will rewrite it more than once and not two or even five.  But it normal. </p><br><p>  There is just the opposite approach when participants try to use <strong>jupyter notebook</strong> as rarely as possible and only when necessary, preferring to write the pipelines immediately with scripts.  (An adherent of such an option is, for example, <a href="https://www.linkedin.com/in/iglovikov/"><strong>(Vladimir Iglovikov (ternaus)</strong></a> ) </p><br><p>  And there are those who are trying to combine <strong>jupyter</strong> with an IDE, for example <strong>pycharm</strong> . </p><br><p>  Each approach has the right to life, each has its pros and cons, as they say "the taste and color of all markers are different."  Choose what you want to work. </p><br><p>  But in any case, make it a rule. </p><br><blockquote>  <strong>save the code for each submission / OOF made (see below)</strong> . </blockquote><p>  <em>(*) <a href="">OOF - out of folds</a> , a technique for obtaining model predictions for the training part of datasets using cross-validation.</em>  <em>Indispensable for the further assembly of several solutions in the ensemble.</em>  <em>It is taught again on courses or easily googled.</em> </p><br><p>  How?  Well, there are at least three options: </p><br><ul><li>  For each competition, a separate repository is created on the <a href="https://github.com/">githaba</a> or <a href="https://bitbucket.org/">bitback,</a> and the code for each submission is committed to the repository with a commentary containing the resulting speed, model parameters, etc. </li><li>  The code of each submission is collected in a separate archive, with the name of the file in which all the meta information of the submission is specified (the same speed, parameters, etc.) </li><li>  It uses a version control system sharpened specifically for DS / ML.  For example <a href="https://dvc.org/">https://dvc.org</a> . </li></ul><br><p>  In general, in the community there is a tendency of a gradual transition to the third option, since  both the first and second have their flaws, but they are simple, reliable and, frankly, for Kaggle they are quite enough. </p><br><p><img src="https://habrastorage.org/webt/vf/4w/hj/vf4whjome3djjzy-mhoaez8v-s8.jpeg" alt="image"></p><br><p>  Yes, more about <strong>python</strong> for those who are not programmers - do not be afraid of it.  Your task is to understand the basic structure of the code and the basic essence of the language, to understand other people's kernels and write your own libraries.  There are many good beginner courses on the web, perhaps in the comments they will tell you exactly where.  Unfortunately (or fortunately) I cannot assess the quality of such courses, so I don‚Äôt provide references in the article. </p><br><h1 id="itak-perehodim-k-freymvorku">  So, go to the framework </h1><br><p><img src="https://habrastorage.org/webt/yg/ec/ix/ygecixpiac-samlo54tzu1mldc8.jpeg" alt="image"></p><br><h2 id="primechanie">  Note </h2><br><p>  All further description will be based on working with tabular and textual data. ,      Kaggle ‚Äî      .       ,    ,    -  ResNet/VGG   ,         ‚Äî      ,      . </p><br><p>   ,      .         <a href="https://www.kaggle.com/c/sp-society-camera-model-identification">Camera Identification</a> ,  , ,     [ <strong>ods.ai</strong> ]   <a href="https://www.kaggle.com/c/sp-society-camera-model-identification/leaderboard"></a>   ,   Kaggle        , ,      ‚Äî   .  ,         46- ,         ,  ,       ‚Äî         ,  300 ,   . </p><br><p>        ‚Äî        . </p><br><h2 id="osnovnaya-cel">   </h2><br><p>     (   jupyter notebooks + )   : </p><br><ul><li> <strong>EDA (exploratory data analysis)</strong> .     ‚Äî  Kaggle     :),       EDA .     - ,  ,      - , ..         .   ,   . </li><li> <strong>Data Cleaning</strong> ‚Äî ,    . , ,  .. </li><li> <strong>Data preparation</strong> ‚Äî ,      .  : <br><ul><li>  Overall </li><li>  /  </li><li>   </li><li>  ( , , <a href="https://www.analyticsvidhya.com/blog/2018/01/factorization-machines/">FM/FFM</a> ) </li><li>  ( <a href="https://medium.com/%40iamHarin/feature-extraction-from-text-e5f5c1b36fe9">Vectorizers, TF-IDF</a> , <a href="https://machinelearningmastery.com/what-are-word-embeddings/">Embeddings</a> ) </li></ul></li><li> <strong>Models</strong> <br><ul><li> Linear models </li><li> Tree models </li><li> Neural Networks </li><li> Exotic (FM/FFM) </li></ul></li><li> <strong>Feature selection</strong> </li><li> <strong>Hyperparameters search</strong> </li><li> <strong>Ensemble</strong> </li></ul><br><p>          ,   ,               ( ).     . </p><br><p>    ‚Äî        ,       -      . </p><br><p>        <strong>CSV,</strong>  <strong>feather/pickle/hdf</strong> ‚Äî           . </p><br><p>          ,  TalkingData, , <a href="https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56105">   memmap</a> ,         lgb. </p><br><p>    ‚Äî     <strong>hdf/feather,</strong> -  (   ) ‚Äî  <strong>CSV</strong> .  ‚Äî  ,    ,    . </p><br><h2 id="nachalnyy-etap">   </h2><br><p><img src="https://habrastorage.org/webt/jp/s8/rp/jps8rp9ee-v4hvobowhm6lq7vfa.jpeg" alt="image"></p><br><p>    Getting started  (  ,    <strong><a href="https://www.kaggle.com/c/house-prices-advanced-regression-techniques">House Prices: Advanced Regression Techniques</a></strong> ),     .   ,   , , ,  ..  etc.    ,  ‚Äî   ,     . </p><br><blockquote> <strong>    ‚Äî    <em>  </em> ‚Äî        .</strong> </blockquote><p>   ,         100%     : </p><br><ul><li> <strong>EDA</strong> . (  , ,  , ...) </li><li> <strong>Data Cleaning.</strong> (  fillna,  ,  ) </li><li> <strong>Data preparation</strong> <br><ul><li>  (  ‚Äî label/ohe/frequency,    ,  , ) </li><li>   ( ) </li></ul></li><li> <strong>Models</strong> <br><ul><li> Linear models (  ‚Äî ridge/logistic) </li><li> Tree models (lgb) </li></ul></li><li> <strong>Feature selection</strong> <br><ul><li> grid/random search </li></ul></li><li> <strong>Ensemble</strong> <br><ul><li> Regression / lgb </li></ul></li></ul><br><h2 id="idem-v-boy">    </h2><br><p><img src="https://habrastorage.org/webt/6j/h2/il/6jh2ilj65upy2jhi1oyjp759cxw.jpeg" alt="image"></p><br><p>      ‚Ä¶  :) </p><br><ul><li><p>   ,     <strong>  </strong> .     ‚Äî    ,     Mercedes, Santander  .   <a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard"> Mercedes</a> ,  (               ,  ‚Äî    ): </p><br><p>    <a href="https://habr.com/company/ods/blog/336168/"></a>  <a href="https://www.youtube.com/watch%3Fv%3DHT3QpRp2ewA"></a> <a href="https://www.kaggle.com/daniel89"><strong>  (danila_savenkov)</strong></a> : </p><br><p>        <strong><a href="https://www.coursera.org/learn/competitive-data-science">How to Win a Data Science Competition: Learn from Top Kagglers"</a></strong> </p><br></li></ul><br><blockquote> <strong>     ‚Äî   !!!</strong> </blockquote><br><ul><li>          </li><li>   , , ‚Ä¶  ‚Ä¶ </li><li>          </li><li>      </li><li> /    </li><li>   . 1 </li></ul><br><p>  ‚Äî      ‚Äî <strong> </strong> !       ,     .     ‚Äî  , ,        ,    ‚Äî  .       ,      . </p><br><p>   ,  - ,    ‚Äî   ? </p><br><p> <strong>!</strong> </p><br><p> <strong>   :</strong> </p><br><ul><li>   .   ,   Kaggle   .      . </li><li>   .               ‚Äî    ,   ,    . </li></ul><br><p> <strong>     !</strong> </p><br><ul><li>      4,         (EDA/ Preparation/ Model/ Ensemble/ Feature selection/ Hyperparameters search/ ...) </li><li>    ,        , , . </li></ul><br><p> <strong>  :</strong> </p><br><ul><li>      ()     ,     . </li><li>   -         ,                 . </li></ul><br><p> <strong>        .</strong> </p><br><p> ,   . ,   .  . </p><br><p>   5 ,    ,      ?    (  )        , ,      ‚Äî  -  ,       ,     ) </p><br><p>      ? ,   ‚Äî      ,   .   .        ,    ‚Äî ,    <strong><a href="https://www.coursera.org/learn/competitive-data-science/lecture/b5Gxv/concept-of-mean-encoding">mean target encoding</a></strong> ,  ,            .  !      <strong>scipy.optimize</strong> ,        . </p><br><p> - ... </p><br><h2 id="vyhodim-na-rabochiy-rezhim">     </h2><br><p><img src="https://habrastorage.org/webt/0a/s8/i5/0as8i5cmgzdywdnaraqck1zdgis.png" alt="image"></p><br><p> In this mode, we solve several competitions.  Each time we notice that there are less and less records on the leaflets, and more and more code in the modules.  Gradually, the task of analysis is reduced to the fact that you just read the description of the solution, say aha, oh, oh, that's how it is!  And you add one or two new spells or approaches in your piggy bank. </p><br><p>  After this mode is changed to the mode of work on the errors.  You have the base ready, now you just need to apply it correctly.  After each competition, reading the description of the decisions, look - what you did not do, what could be done better, what you missed, well, or where you specifically lazhanuli, as I happened in <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic</a> .  He walked quite well, into the underbelly of gold, and flew down to private positions by 1500 positions.  It's a shame to tears ... but he calmed down, found a mistake, wrote a post in a slaka - and learned a lesson. </p><br><p>  The fact that one of the descriptions of the top solution will be written from your nickname can serve as a sign of the final exit to the working mode. </p><br><p>  What should be <strong>approximately</strong> in pipelines by the end of this stage: </p><br><ul><li>  All sorts of options for pre-processing and creating numeric features - projections, relationships, </li><li>  Different methods of working with categories - Mean target encoding in the correct version, frequencies, label / ohe, </li><li>  Various schemes of embeddings over text (Glove, Word2Vec, Fasttext) </li><li>  Various text vectoring schemes (Count, TF-IDF, Hash) </li><li>  Several validation schemes (N * M for standard cross-validation, time-based, by group) </li><li>  Bayesian optimization / hyperopt / something else for selecting hyper parameters </li><li>  Shuffle / Target permutation / Boruta / RFE - for feature selection </li><li>  Linear models - in the same style over a single data set. </li><li>  LGB / XGB / Catboost - in the same style over a single data set </li></ul><br><p>  <em>The author made metaclasses separately for linear and tree-based models, with a single external interface, in order to level the differences in API between different models.</em>  <em>But now it is possible to run in a single key with one line, for example, LGB or XGB over one processed data set.</em> </p><br><ul><li>  Several neural networks for all occasions (do not take pictures yet) - embeddings / CNN / RNN for text, RNN for sequences, Feed-Forward for everything else.  It is good to understand and be able to <a href="https://habr.com/post/331382/">auto-encoders</a> . </li><li>  Lgb / regression / scipy ensemble for regression and classification tasks </li><li>  It‚Äôs good to already be able to <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BD%25D0%25B5%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">Genetic Algorithms</a> , sometimes they come in well </li></ul><br><h2 id="podytozhim">  Summarize </h2><br><p>  Any sport, and competitive DS is also a sport, it is a lot of sweat and a lot of work.  This is neither good nor bad, it is a fact.  Participation in competitions (if you approach the process correctly) pumps technical skills very well, plus more or less shakes the sporting spirit, when you really don't want to do something, breaks everything directly - but you get up to the laptop, rework the model, run it on still gnaw this unfortunate 5th decimal place. </p><br><p>  So decide Kaggle - farm experience, medals and fan! </p><br><h1 id="para-slov-pro-payplayny-avtora">  A couple of words about the author's pipelines </h1><br><p><img src="https://habrastorage.org/webt/oj/tg/nh/ojtgnhnbgznuvywklyrwy8qdjai.jpeg" alt="image"></p><br><p>  In this section I will try to describe the main idea of ‚Äã‚Äãthe pipelines and modules assembled in a year and a half.  Again - this approach does not pretend to be universal or unique, but suddenly someone will help. </p><br><ul><li>  All code for feature-engineering, except for mean target encoding, is placed in a separate module as functions.  Tried to collect through objects, it turned out cumbersome, and in this case it is not necessary. </li><li>  All functions of feature-engineering are executed in the same style and have a single signature of call and return: </li></ul><br><pre><code class="hljs kotlin">def do_cat_dummy(<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>, attrs, prefix_sep=<span class="hljs-string"><span class="hljs-string">'_ohe_'</span></span>, params=None): # <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> something <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> _data, new_attrs</code> </pre> <br><p>  At the input we pass dataset, attributes for work, prefix for new attributes and additional parameters.  At the output we get a new dataset with new attributes and a list of these attributes.  Next, this new dataset is stored in a separate pickle / feather. </p><br><p>  What it gives is that we get the opportunity to quickly assemble datasets for learning from pre-generated cubes.  For example, for categories we do three processing at once - Label Encoding / OHE / Frequency, save to three separate feathers, and then at the modeling stage we just play with these blocks, creating elegant datasets for training with one elegant movement. </p><br><pre> <code class="hljs kotlin"> pickle_list = [ <span class="hljs-string"><span class="hljs-string">'attrs_base'</span></span>, <span class="hljs-string"><span class="hljs-string">'cat67_ohe'</span></span>, # <span class="hljs-string"><span class="hljs-string">'cat67_freq'</span></span>, ] short_prefix = <span class="hljs-string"><span class="hljs-string">'base_ohe'</span></span> _attrs, use_columns, <span class="hljs-keyword"><span class="hljs-keyword">data</span></span> = load_attrs_from_pickle(pickle_list) cat_columns = []</code> </pre> <br><p>  If you need to build another dataset, change the <code>pickle_list</code> , reboot, and work with the new dataset. </p><br><p>  The main set of functions over tabular data (real and categorical) includes various categories coding, projection of numeric attributes onto categorical, as well as various transformations. </p><br><pre> <code class="hljs ruby"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_cat_le</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'le_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_cat_dummy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs, prefix_sep=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'_ohe_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, params=None)</span></span></span></span>: <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_cat_cnt</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'cnt_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_cat_fact</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'bin_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_cat_comb</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs_op, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'cat_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>: <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_proj_num_2cat</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs_op, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'prj_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span><span class="hljs-symbol"><span class="hljs-symbol">:</span></span></code> </pre> <br><p>  A universal Swiss knife for combining attributes, to which we transfer the list of initial attributes and the list of conversion functions, as a result we get, as usual, a list of new attributes. </p><br><pre> <code class="hljs ruby"> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">do_iter_num</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, attrs_op, params=None, prefix=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'comb_'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span><span class="hljs-symbol"><span class="hljs-symbol">:</span></span></code> </pre> <br><p>  Plus various additional specific converters. </p><br><p>  For processing text data, a separate module is used, which includes various methods of preprocessing, tokenization, lemmatization / stemming, translation into a frequency table, and so on.  etc.  Everything is standard using <strong>sklearn</strong> , <strong>nltk</strong> and <strong>keras</strong> . </p><br><p>  Time series are also processed by a separate module, with the transformation functions of the original dataset for both normal tasks (regression / classification) and sequence-to-sequence.  Thank you to <strong>Fran√ßois Chollet</strong> for finishing keras so that <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">building seq-2-seq models</a> doesn‚Äôt look like a voodoo ritual of summoning demons. </p><br><p>  In the same module, by the way, are the functions of the usual statistical analysis of the series - a test for stationarity, STL decomposition, etc ... It helps a lot at the initial stage of the analysis in order to ‚Äútouch‚Äù the series and see what it is. </p><br><ul><li><p>  Functions that cannot be applied immediately to the whole dataset, but need to be used inside the folds during cross-validation are moved to a separate module: </p><br><ul><li>  Mean target encoding </li><li>  Upsampling / Downsampling </li></ul><br><p>  They are passed into the class of the model (read about the model below) at the training stage. </p><br></li></ul><br><pre> <code class="hljs pgsql"> _fpreproc = fpr_target_enc _fpreproc_params = fpr_target_enc_params _fpreproc_params.<span class="hljs-keyword"><span class="hljs-keyword">update</span></span>(**{ <span class="hljs-string"><span class="hljs-string">'use_columns'</span></span> : cat_columns, })</code> </pre><br><ul><li>  A metaclass was created for modeling, which generalizes the concept of a model, with abstract methods: fit / predict / set_params /, etc.  For each specific library (LGB, XGB, Catboost, SKLearn, RGF, ...), an implementation of this metaclass was created. </li></ul><br><p>  That is, to work with LGB, we create a model </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">model_to_use</span></span> = <span class="hljs-string"><span class="hljs-string">'lgb'</span></span> model = KudsonLGB(task=<span class="hljs-string"><span class="hljs-string">'classification'</span></span>)</code> </pre> <br><p>  For XGB: </p><br><pre> <code class="hljs nginx"> <span class="hljs-attribute"><span class="hljs-attribute">model_to_use</span></span> = <span class="hljs-string"><span class="hljs-string">'xgb'</span></span> metric_name= <span class="hljs-string"><span class="hljs-string">'auc'</span></span> task=<span class="hljs-string"><span class="hljs-string">'classification'</span></span> model = KudsonXGB(task=task, metric_name=metric_name)</code> </pre> <br><p>  And all functions further operate with <code>model</code> . </p><br><ul><li><p>  For validation, several functions were created, which immediately considered both prediction and OOF for several seeders during cross-validation, as well as a separate function for ordinary validation via train_test_split.  All validation functions operate on meta-model methods, which provides model-independent code and makes it easy for any other library to connect to the pipeline. </p><br><pre> <code class="hljs pgsql">res = cv_make_oof( model, model_params, fit_params, dataset_params, XX_train[use_columns], yy_train, XX_Kaggle[use_columns], folds, scorer=scorer, metric_name=metric_name, fpreproc=_fpreproc, fpreproc_params=_fpreproc_params, model_seed=model_seed, silence=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span> ) score = res[<span class="hljs-string"><span class="hljs-string">'score'</span></span>]</code> </pre> <br></li><li><p>  For feature selection, nothing interesting, standard RFE, and my favorite shuffle permutation in all possible options. </p><br></li><li><p>  Bayesian optimization is mainly used to search for hyperparameters, again in a unified form so that you can run a search for any model (via the cross-validation module).  This unit lives in the same laptop as the simulation. </p><br></li><li><p>  For the ensembles, several functions have been done, unified for regression and classification problems based on the Ridge / Logreg, LGB, Neural network and my favorite scipy.optimize. </p><br><p>  A small explanation is that each model from the pipeline gives two files as a result: <strong>sub_xxx</strong> and <strong>oof_xxx</strong> , which are the predictions for the test and OOF prediction for the train.  Next, in the ensemble module from the specified directory, we load pairs of predictions from all models into two data frames - <strong>df_sub</strong> / <strong>df_oof</strong> .  Well, after that we look at the correlations, select the best ones, then we build models of the 2nd level over <strong>df_oof</strong> and apply them to <strong>df_sub</strong> . </p><br><p>  Sometimes, searching for the best subset of models, the search for <a href="https://towardsdatascience.com/genetic-algorithm-implementation-in-python-5ab67bb124a6">genetic algorithms</a> goes well (the author uses <a href="https://github.com/DEAP/deap">this library</a> ), sometimes the <a href="https://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf">method from Caruana</a> .  In the simplest cases, standard regressions and scipy.optimize work fine. </p><br></li><li><p>  Neural networks live in a separate module, the author uses <a href="https://keras.io/">keras</a> in a <a href="https://keras.io/getting-started/functional-api-guide/">functional style</a> , yes, not as flexible as <a href="https://pytorch.org/">pytorch</a> , but still enough.  Again, written universal functions for training, invariant to the type of network. </p><br></li></ul><br><p>  This pipeline was once again tested in a recent competition from <a href="https://www.kaggle.com/c/home-credit-default-risk">Home Credit</a> , attentive and careful use of all blocks and modules brought 94th place and silver. </p><br><p>  The author is generally ready to express a seditious idea that for tabular data and a normally made pipeline, the final submit at any competition should fly into the top 100 leaderboard.  Naturally there are exceptions, but in general, this statement seems to be true. </p><br><h1 id="pro-komandnuyu-rabotu">  About teamwork </h1><br><p><img src="https://habrastorage.org/webt/w8/3j/fd/w83jfd9i-hv761dxmp1dxkq3djq.jpeg" alt="image"></p><br><p>  It‚Äôs not all that simple, deciding Kaggle in a team or solo depends a lot on the person (and the team), but my advice for those who are just starting out is to try a solo.  Why?  I will try to explain my point of view: </p><br><ul><li>  First, you will understand your strengths, see weaknesses and, in general, be able to assess your potential as a DS practice. </li><li>  Secondly, even working in a team (unless this is not an established team with division of roles), you will still have to wait for a complete finished solution - that is, you should already have working pipelines.  (" <em>Submission or not</em> ") (C) </li><li>  And thirdly, optimally, when the level of players in a team is about the same (and quite high), then you can learn something really high-level) In weak teams (there is nothing derogatory, I‚Äôm talking about the level of training and experience at Kaggle) imho it is very difficult to learn something, it is better to nibble the forum and the kernels.  Yes, you can farm medals, but see Above about goals and a belt for maintaining pants) </li></ul><br><h1 id="poleznye-sovety-ot-kapitana-ochevidnost-i-obeschannaya-karta-grabel-">  Useful tips from the captain evidence and promised card rake :) </h1><br><p><img src="https://habrastorage.org/webt/8t/ox/qu/8toxqudxt4giw_aezjqga2gi3dk.jpeg" alt="image"></p><br><p>  <strong><em>These tips reflect the experience of the author, are not dogma, and can (and should) be verified by their own experiments.</em></strong> </p><br><ul><li><p>  <strong>Always start with the construction of valid validation</strong> - it will not be her, all other efforts will fly into the furnace.  <a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard">Take</a> another look at the <a href="https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/leaderboard">Mercedes leaderboard</a> . </p><br><p>  <em>The author is really pleased with the fact that in this competition he built a stable cross-validation scheme (3x10 folds), which retained speed and brought the legal 42nd place)</em> </p><br></li><li><p>  If a competent validation is built - <strong>always trust the results of your validation</strong> .  If your models' fastness improves on validation, but deteriorates in public, it is wiser to trust validation.  When analyzing, just count that piece of data on which the public leaderboard is considered to be another fold.  You do not want to make your model a single fold? </p><br></li><li><p>  If the model and the scheme allows - <strong>always do OOF predictions</strong> and save near the model.  At the stage of the ensemble you never know what will fire. </p><br></li><li><p>  <strong>Always keep near the result / OOF code to get them</strong> .  No matter on githab, locally, anywhere.  Twice I managed that in the ensemble the best model is the one that was made two weeks ago "out of the box" and for which no code was preserved.  Pain. </p><br></li><li><p>  Hammer on the selection of the <strong>"right" sid for cross-validation</strong> , he himself sinned with it first.  Better choose any three and do 3xN cross-validation.  The result will be more stable and easier. </p><br></li><li><p>  <strong>Do not chase after the number of models in the ensemble</strong> - better is less, but more diverse - more varied by models, by preprocessing, by datasets.  In the worst case, in terms of parameters, for example, one deep tree with rigid regularization, one shallow tree. </p><br></li><li><p>  <strong>To select a</strong> feature, <strong>use shuffle / boruta / RFE</strong> , remember that the feature importance in various tree-based models is the metrics in parrots on the sawdust bag. </p><br></li><li><p>  Personal opinion of the author (may not coincide with the opinion of the reader) <a href="">Bayesian optimization</a> for the selection of hyper <a href="https://github.com/hyperopt/hyperopt">parameters</a> .  <strong>("&gt;" == better)</strong> </p><br></li><li><p>  <strong>It is</strong> better to handle a <strong>leading kernel kernel</strong> posted on public as follows: </p><br><ul><li>  There is a time - we look at what's new there and build in ourselves </li><li>  Less time - we remake it on our validation, we do OOF - and we fasten it to the ensemble </li><li>  There is no time at all - stupidly blend with our best solution and we look soon. </li></ul><br></li><li><p>  <strong>How to choose two final submissions</strong> - by intuition, of course.  But seriously, everyone usually practices the following approaches: </p><br><ul><li>  Conservative submit (on sustainable models) / risky submit. </li><li>  With the best soon on OOF / public leaderboard </li></ul><br></li><li><p>  Remember - everything is a figure and the possibilities of its processing depend only on your imagination.  Use classification instead of regression, treat sequences as a picture, etc. </p><br></li></ul><br><p>  And finally: </p><br><ul><li>  Join <strong><a href="http://ods.ai/">ods.ai</a></strong> :) <strong><a href="http://ods.ai/">chat</a></strong> and get fan of <strong>DS</strong> and life!  ) </li></ul><br><h1 id="poleznye-ssylki">  useful links </h1><br><p><img src="https://habrastorage.org/webt/bt/rf/zo/btrfzook4i9ucb0j_itewbi8r5a.jpeg" alt="image"></p><br><h3 id="obschie">  Are common </h3><br><p>  <a href="http://ods.ai/">http://ods.ai/</a> - for those who want to join the best DS community :) <br>  <a href="https://mlcourse.ai/">https://mlcourse.ai/</a> - course site ods.ai <br>  <a href="https://www.kaggle.com/general/68205">https://www.Kaggle.com/general/68205</a> - post about a course on Kaggle </p><br><p>  In general, I highly recommend in the same mode as described in the article, view <a href="https://www.youtube.com/channel/UCeq6ZIlvC9SVsfhfKnSvM9w/videos">the video cycle mltrainings</a> - many interesting approaches and techniques. </p><br><h4 id="video">  Video </h4><br><ul><li>  <a href="https://www.youtube.com/watch%3Fv%3DfXnzjJMbujc">very good video about how to become a grandmaster :)</a> by <a href="https://www.kaggle.com/ppleskov"><strong>Pavel Pleskov (ppleskov)</strong></a> </li><li>  <a href="https://www.youtube.com/watch%3Fv%3Dg335THJxkto">video about hacking, non-standard approach and mean target encoding on the example of the BNP Paribas competition</a> from <a href="https://www.kaggle.com/stasg7"><strong>Stanislav Semenov (stasg7)</strong></a> </li><li>  <a href="https://www.youtube.com/watch%3Fv%3DGT4G7Vawt0Q">Another video with <strong>Stanislav</strong> "What does Kaggle teach"</a> </li></ul><br><h4 id="kursy">  Courses </h4><br><p>  In more detail the methods and approaches to solving problems on Kaggle can be found in the second course of <a href="https://www.coursera.org/specializations/aml">specialization</a> , " <strong><a href="https://www.coursera.org/learn/competitive-data-science">How to Win a Data Science Competition: Learn from Top Kagglers"</a></strong> </p><br><h4 id="vneklassnoe-chtenie">  Extracurricular reading: </h4><br><ul><li>  <a href="https://sites.google.com/view/lauraepp/parameters">Laurae ++, XGBoost / LightGBP parameters</a> </li><li>  <a href="https://github.com/facebookresearch/fastText">FastText - embedding for text from Facebook</a> </li><li>  <a href="https://github.com/anttttti/Wordbatch">WordBatch / FTRL / FM-FTRL - a set of libraries</a> from <a href="https://www.kaggle.com/anttip"><strong>@anttip</strong></a> </li><li>  <a href="https://github.com/alexeygrigorev/libftrl-python">Another FTRL implementation</a> </li><li>  <a href="">Bayesian Optimization - a library for the selection of hyper parameters</a> </li><li>  <a href="https://github.com/RGF-team/rgf">Regularized Greedy Forest (RGF) library - another tree method</a> </li><li>  <a href="http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/">A Kaggler's Guide to Model Stacking in Practice</a> </li><li>  <a href="https://github.com/TeamHG-Memex/eli5">ELI5 is an excellent library for visualizing model weights</a> from <a href="https://www.kaggle.com/lopuhin"><strong>Konstantin Lopukhin (kostia)</strong></a> </li><li>  <a href="https://www.kaggle.com/ogrellier/feature-selection-target-permutations">Feature selection: target permutations - and follow the links inside</a> </li><li>  <a href="https://medium.com/the-artificial-impostor/feature-importance-measures-for-tree-models-part-i-47f187c1a2c3">Feature Importance Measures Measures for Tree Models</a> </li><li>  <a href="https://www.kaggle.com/ogrellier/feature-selection-with-null-importances">Feature selection with null importance</a> </li><li>  <a href="https://www.kaggle.com/shivamb/how-autoencoders-work-intro-and-usecases">Briefly about autoencoders</a> </li><li>  <a href="https://www.slideshare.net/markpeng/general-tips-for-participating-kaggle-competitions">Slideshare Kaggle Presentation</a> </li><li>  <a href="https://www.slideshare.net/HJvanVeen/kaggle-presentation%3Fqid%3D9945759e-a06f-447d-bcfb-2a15592f30b6%26v%3D%26b%3D%26from_search%3D11">And another one</a> </li><li>  <a href="https://www.slideshare.net/search/slideshow%3Fsearchfrom%3Dheader%26q%3Dkaggle">And in general there are a lot of interesting things.</a> </li><li>  <a href="https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions">Winning solutions of kaggle competitions</a> </li><li>  <a href="https://www.kaggle.com/shivamb/data-science-glossary-on-kaggle-updated/">Data Science Glossary on Kaggle</a> </li></ul><br><h1 id="zaklyuchenie">  Conclusion </h1><br><p><img src="https://habrastorage.org/webt/2o/di/tx/2oditx8xp4e3oj_tkzwm0lglxio.jpeg" alt="image"></p><br><p>  The subject of Data Science in general and competitive Data Science in particular is as inexhaustible as atom (C).  In this article, the author only slightly opened the topic of pumping practical skills using competitive platforms.  If it became interesting - connect, look around, save up experience - and write your articles.  The more good content, the better for all of us! </p><br><p>  Anticipating questions ‚Äî no, the pipelines and author‚Äôs libraries have not yet been freely available. </p><br><p>  Many thanks to the colleagues from <strong>ods.ai:</strong> <a href="https://www.linkedin.com/in/iglovikov/"><strong>Vladimir Iglovikov (ternaus)</strong></a> , <a href="https://habr.com/users/yorko/"><strong>Yuri Kashnitsky (yorko)</strong></a> , <a href="https://www.linkedin.com/in/venheads/"><strong>Valery Babushkin</strong> ( <strong>venheads)</strong></a> , <a href="https://www.linkedin.com/in/avpronkin/"><strong>Alexey Pronkin (pronkin_alexey)</strong></a> , <a href="https://habr.com/users/dpetrov_ml/"><strong>Dmitry Petrov (dmitry_petrov)</strong></a> , <a href="https://www.kaggle.com/drn01z3"><strong>Artur Kuzin (i01z3)</strong></a> . article before publication, for edits and reviews. </p><br><p>  <a href="https://www.linkedin.com/in/nikita-zavgorodnii/"><strong>Special</strong></a> thanks to <a href="https://www.linkedin.com/in/nikita-zavgorodnii/"><strong>Nikita Zavgorodnoye (njz)</strong></a> - for the final proofreading. </p><br><p>  Thank you for your attention, I hope this article will be useful to someone. </p><br><p>  My nickname in <em>Kaggle</em> / <em>ods.ai</em> : <strong>kruegger</strong> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/426227/">https://habr.com/ru/post/426227/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../426211/index.html">Go vs Javascript. What to write IoT projects</a></li>
<li><a href="../426213/index.html">A group of petrol station employees who, using software and electronics, corrected and organized 5% under-filling at petrol stations were detained</a></li>
<li><a href="../426215/index.html">Nasty questions about the life cycle</a></li>
<li><a href="../426217/index.html">How is the security of your Windows operating system organized?</a></li>
<li><a href="../426223/index.html">Android development and problem solving related to development</a></li>
<li><a href="../426229/index.html">Code of Conduct: why Linux kernel developers threatened to delete their code - we understand the conflict</a></li>
<li><a href="../426231/index.html">Britain wants to regulate the Internet - what will affect the new laws</a></li>
<li><a href="../426233/index.html">Study: large wind power stations can heat up the soil, which affects climate</a></li>
<li><a href="../426235/index.html">Fiasco. The story of one homemade IoT</a></li>
<li><a href="../426237/index.html">How to determine the minimum size required for a DFSR replication staging folder</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>