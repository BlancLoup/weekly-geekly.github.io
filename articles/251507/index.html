<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apache Spark: What's under the hood?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Recently, the Apache Spark project has attracted immense attention, a large number of small practical articles have been written about ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apache Spark: What's under the hood?</h1><div class="post__text post__text-html js-mediator-article"><h4>  Introduction </h4><br>  Recently, the Apache Spark project has attracted immense attention, a large number of small practical articles have been written about it, it has become part of Hadoop 2.0.  Plus, it quickly gained additional frameworks, such as Spark Streaming, SparkML, Spark SQL, GraphX, and besides these ‚Äúofficial‚Äù frameworks, a sea of ‚Äã‚Äãprojects appeared - various connectors, algorithms, libraries, and so on.  Quickly and confidently enough to understand this zoo with a lack of serious documentation, especially considering the fact that Spark contains all sorts of basic pieces of other Berkeley projects (for example BlinkDB) is not easy.  Therefore, I decided to write this article in order to make life easier for busy people. <br><a name="habracut"></a><br><h4>  A little background: </h4><br>  Spark is a UC Berkeley lab project that began around 2009.  The founders of Spark are well-known database scientists, and according to their Spark's philosophy, they are in some way a response to MapReduce.  Now Spark is under the Apache ‚Äúroof‚Äù, but the ideologues and main developers are the same people. <br><br><h4>  Spoiler: Spark in 2 words </h4><br>  Spark can be described in one phrase like this - these are the insides of a massively parallel database engine.  That is, Spark does not promote its storage, but lives beyond the others (HDFS is the distributed file system Hadoop File System, HBase, JDBC, Cassandra, ...).  The truth is to immediately mention the project IndexedRDD - key / value repository for Spark, which probably will soon be integrated into the project. Spark does not care about transactions, but otherwise it is just MPP DBMS engine. <br><br><h4>  RDD - the basic concept of Spark </h4><br>  The key to understanding Spark is RDD: Resilient Distributed Dataset.  In essence, this is a reliable distributed table (in fact, RDD contains an arbitrary collection, but it is most convenient to work with tuples, as in a relational table).  RDD can be completely virtual and just know how it was born, so that, for example, in the event of a node failure, recover.  And maybe materialized - distributed, in memory or on disk (or in memory with crowding out to disk).  Also, inside, the RDD is divided into partitions - this is the minimum amount of RDD that each work node will process. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <habracut></habracut><br><br>  Everything interesting that happens in Spark happens through RDD operations.  That is, usually applications for Spark look like this - we create RDD (for example, we get data from HDFS), we patch it (map, reduce, join, groupBy, aggregate, reduce, ...), do something with the result - for example, we throw back HDFS. <br><br>  Well, based on this understanding, Spark should be considered as a parallel environment for complex analytical tasks, where there is a master who coordinates the task, and a bunch of working nodes that participate in the execution. <br><br>  Let's look at such a simple application in detail (we will write it on Scala - this is the reason to learn this trendy language): <br><br><h4>  Spark application example (not all included, for example include) </h4><br>  We will separately analyze what happens at each step. <br><br><pre><code class="scala hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span></span>(args: <span class="hljs-type"><span class="hljs-type">Array</span></span>[<span class="hljs-type"><span class="hljs-type">String</span></span>]){ <span class="hljs-comment"><span class="hljs-comment">// ,    val conf = new SparkConf().setAppName(appName).setMaster(master) val sc = new SparkContext(conf) //    HDFS,   RDD val myRDD = sc.textFile("hdfs://mydata.txt") //      .    . //      ,    (   // ) -  ""  val afterSplitRDD = myRDD.map( x =&gt; ( x.split(" ")( 0 ), x ) ) //    :  -    val groupByRDD = afterSplitRDD.groupByKey( x=&gt;x._1 ) //  -     val resultRDD = groupByRDD.map( x =&gt; ( x._1, x._2.length )) //       HDFS resultRDD.saveAsTextFile("hdfs://myoutput.txt") }</span></span></code> </pre> <br><br><h4>  And what happens there? </h4><br>  Now let's run through this program and see what happens. <br><br>  Well, first of all, the program is launched on the cluster master, and before any parallel processing goes, there is an opportunity to do something quietly in one thread.  Then, as probably already noticeable, each operation on RDD creates another RDD (except saveAsTextFile).  In this case, RDDs are all created lazily, only when we ask either to write to a file, or, for example, unload it into memory on a master ‚Äî execution begins.  That is, the implementation takes place both in terms of a request, by a conveyor, where the conveyor element is a partition. <br><br>  What happens to the very first RDD we made from the HDFS file?  Spark is well integrated with Hadoop, so each working node will download its own subset of data, and will be downloaded by partitions (which in the case of HDFS coincide with the blocks).  That is, all the nodes downloaded the first block, and the execution went further as planned. <br><br>  After reading from the disk, we have a map ‚Äî it runs trivially at each working node. <br><br>  Next comes groupBy.  This is no longer a simple conveyor operation, but a real distributed grouping.  For good, it is better to avoid this operator, because as long as it is not implemented too cleverly, it doesn‚Äôt keep track of the data locality and will be comparable to distributed sorting in performance.  Well, this is information for consideration. <br><br>  Let's think about the state of affairs at the time of groupBy.  All RDDs were conveyer before that, that is, they did not save anything anywhere.  In the event of a failure, they would again pull the missing data from the HDFS and pass through the pipeline.  But groupBy violates the conveyor and as a result we get cached RDD.  In case of loss, now we will have to redo all RDDs to groupBy completely. <br><br>  To avoid a situation where due to failures in a complex application for Spark, you have to recalculate the entire pipeline, Spark allows the user to control caching with the persist operator.  It can cache in memory (in this case, recalculation occurs when data is lost in memory - it can happen when the cache is full), to disk (not always fast enough), or to memory with flushing to disk in case of cache overflow. <br><br>  After, we again map and write to HDFS. <br><br>  Well, now it‚Äôs more or less clear what is happening inside Spark on a simple level. <br><br><h4>  But what about the details? </h4><br>  For example, you want to know exactly how the groupBy operation works.  Or the reduceByKey operation, and why it is much more efficient than groupBy.  Or how join and leftOuterJoin work.  Unfortunately, most of the details are the easiest to find out only from the Spark sources or by asking a question on their mailing list (by the way, I recommend subscribing to it if you do something serious or non-standard on Spark). <br><br>  Even worse, we understand what's going on in the various connectors to Spark.  And how much they can be used at all.  For example, we had to abandon the idea of ‚Äã‚Äãintegrating with Cassandra for a while because of their incomprehensible support for the Spark connector.  But there is hope that quality documentation will appear in the near future. <br><br><h4>  And what interesting things do we have on top of Spark? </h4><br><ul><li>  SparkSQL: SQL engine on top of Spark.  As we have already seen, Sparke already has almost everything for this, except for the storage, indexes and its statistics.  This seriously complicates optimization, but the SparkSQL team claims that they are sawing their new optimization framework, as well as AMP LAB (the laboratory from which Spark grew up) is not going to refuse from the Shark project - full replacement for Apache HIVE </li><li>  Spark MLib: This is essentially an Apache Mahaout replacement, only much more serious.  In addition to efficient parallel machine learning (not only using RDD tools, but also additional primitives), SparkML works much better with local data using the Breeze package of native linear algebra, which will attract Fortran code to you into the cluster.  Well, very well thought out API.  A simple example: in parallel, we are learning on a cluster with cross-validation. <br></li><li>  BlinkDB: A very interesting project - inaccurate SQL queries on top of large amounts of data.  We want to calculate the average for some field, but I want to do it no longer than 5 seconds (losing accuracy) - please.  We want the result with an error not more than the specified one - also suitable.  By the way, pieces of this BlinkDB can be found inside Spark (this can be viewed as a separate quest). <br></li><li>  Well, a lot of things are now being written on top of Spark, I just listed the most interesting projects from my point of view. <br></li></ul></div><p>Source: <a href="https://habr.com/ru/post/251507/">https://habr.com/ru/post/251507/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251491/index.html">MS Dynamics CRM and PostSharp</a></li>
<li><a href="../251493/index.html">DevCon Digest # 3. Immerse yourself in Visual Studio</a></li>
<li><a href="../251497/index.html">OpenCage is the most powerful geocoding tool.</a></li>
<li><a href="../251501/index.html">What exactly happens when a user types google.com in the address bar? Part 2</a></li>
<li><a href="../251503/index.html">What should be able to cool call center for IT-parts and what options are there at all</a></li>
<li><a href="../251525/index.html">How we "married" cloud PBX, GSM and realtors (part 2)</a></li>
<li><a href="../251529/index.html">Vagrant for kids, or as on Windows it is easy to get a customized server for developing web applications</a></li>
<li><a href="../251531/index.html">Formatting Python Code</a></li>
<li><a href="../251533/index.html">Market growth, games from The Sun, Angry Birds in Chinese - and other news of the week for a mobile developer</a></li>
<li><a href="../251535/index.html">How to create 40% more sales in the online store using cashback services</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>