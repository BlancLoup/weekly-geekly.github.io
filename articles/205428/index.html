<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Linear regression on fingers in recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the task of recognition, the key role is played by the selection of significant parameters of objects and the evaluation of their numerical values....">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Linear regression on fingers in recognition</h1><div class="post__text post__text-html js-mediator-article"><img align="right" alt="   " title="Finger linear regression" src="https://habrastorage.org/getpro/habr/post_images/397/c30/f7f/397c30f7f15c5c68834c1471b1f64206.jpg">  In the task of recognition, the key role is played by the selection of significant parameters of objects and the evaluation of their numerical values.  Nevertheless, even having received good numerical data, it is necessary to manage to use them correctly.  Sometimes it seems that the further solution of the problem is trivial, and one would like ‚Äúfrom general considerations‚Äù to obtain a recognition result from numerical data.  But the result in this case is far from optimal.  In this article I want to show, using the example of the recognition problem, how you can easily apply the simplest mathematical models and thereby significantly improve the results. <br><a name="habracut"></a><br><h4>  About the pattern recognition task </h4><br>  The task of <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D0%25BE%25D0%25B7%25D0%25BD%25D0%25B0%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2">pattern recognition</a> is the task of classifying an object to a certain class.  To determine belonging to a class, a certain set of object parameters is used.  For example, you can recognize people by some of their parameters: the distance between the eyes, skin color, nose length, iris pattern, characteristic voice frequency, number of fingers, etc. <img align="right" title="Classification task" src="https://habrastorage.org/getpro/habr/post_images/774/33f/22b/77433f22bf4f8273fc986380098ac829.jpg">  These parameters can be represented as a numerical vector of a fixed dimension (for numerical parameters this is trivial, for the color / pattern of the iris / face image, etc., we must also figure out how to do this).  That is, we have a certain number of classes X <sub>1</sub> , X <sub>2</sub> , ... X <sub>N</sub> , there is a prototype, described by a set of parameter values <img src="https://habrastorage.org/getpro/habr/post_images/8f5/994/04f/8f599404f573f8c2e9d69dee1e3e9c6c.gif">  , and we want to say about him if he belongs to one of these classes.  Usually classes are presented in the form of several samples of this class: for example, visual images of an object from different angles, various measurements of parameters, and various copies of photographs of the iris.  Then the simplest method of recognition is to compare the incoming object with each sample of each class, and by the distance from the nearest object, decide whether it is he or not. <br><br>  At first glance, the task sounds like a classification task with an indefinitely large number of classes: each type of object is a separate class, and a new object must be compared with each class. <img title="Multi-class classification problem" align="right" src="https://habrastorage.org/getpro/habr/post_images/dc2/122/36a/dc212236a5a83e8a19a0c5793642765a.jpg">  This is quite a difficult task, especially when we have an unfixed number of classes: for example, when recognizing people, a person who does not exist in the database can pass - that is, a person of another class.  But in fact, we calculate the difference dx between the object under study and the sample class and make a decision, is it the same or different object.  For example, if we want to recognize a person by fingerprints, then we compare the resulting fingerprint with all fingerprints from a certain database, calculate the measure of similarity between each pair of fingerprints, and according to this measure of similarity for the most similar fingerprint, decide that this is the same Man, if the prints are similar enough, or that this person is not in our database, if the most similar imprint is still not similar.  That is, our classifier takes as input not the parameters of the object x, but the difference between the parameters of two objects dx = x - x <sup>i</sup> (it is better to even take the difference module of each component so that the order of subtraction is not important), and by these parameters it gives a characteristic (measure of similarity ), allowing to make a decision: are they the same objects or different. <img title="Two-class classification task" align="right" src="https://habrastorage.org/getpro/habr/post_images/ddf/fdf/862/ddffdf862c5e9142219fd0a63ce04d83.jpg">  This can be viewed as a classification task with two classes: ‚Äúidentical pairs of objects‚Äù and ‚Äúdifferent pairs of objects‚Äù.  And the class of "identical pairs" will be heaped closer to zero, and different - on the contrary, they will be somewhere aside.  Traditionally, the classifier should produce a numerical measure of similarity (for example, from 0 to 1), which is greater, the greater the likelihood that these are different objects;  Further, depending on the requirements of the system, a threshold is determined for making a decision, on which <a href="http://ru.wikipedia.org/wiki/%25D0%259E%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8_%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B2%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25B8_%25D0%25B2%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25BE%25D0%25B4%25D0%25B0">errors of the first and second kind</a> depend. <br><br><h4>  How not to do </h4><br>  Initially, looking at these parameters, I just want to find the norm of the difference vector - the root of the sum of the squares of the components: <img src="https://habrastorage.org/getpro/habr/post_images/510/f7d/9f5/510f7d9f5f5085128a186fed8b626218.gif">  .  That is, as the Euclidean distance between the samples, if the parameters are considered as coordinates.  The smaller it is, the more ‚Äúsimilar‚Äù these two objects are, that is, there is a greater chance that they are the same.  This is correct, but with a number of limitations: all parameters must be uncorrelated (mutually independent), they must have the same distribution (they must be equivalent).  This is usually not the case (for example, height and weight should not be considered independent, and the shape of the nose is a much more reliable characteristic than the shape of the mouth or the length of the hair).  Then let the different components choose different weights to strengthen the more important parameters and weaken the less important ones: <img src="https://habrastorage.org/getpro/habr/post_images/7fc/a33/ebf/7fca33ebf9be4964be2d94a091b76b54.gif">  . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Here we come to a very correct idea: we must take all sorts of factors not from our head, but from practice, that is, we need to choose them so that they work well on some data set.  In machine learning, this is called <a href="http://ru.wikipedia.org/wiki/%25D0%259E%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581_%25D1%2583%25D1%2587%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BC">‚Äúlearning with a teacher‚Äù</a> : a set of data is taken (objects of different and identical classes, about which we initially know which class they belong to), called ‚Äútraining set‚Äù, and the coefficients are chosen so that our recognizer worked as best as possible (this is called "learning").  Then you can check how well the model works by applying it on another data set, called ‚Äútest sample‚Äù, and conclude how well we did it. <br><br>  So we want to determine the weights.  To do this, we walk through all pairs of objects from the test sample and for each component we calculate its average value and variance for the same pairs and for different pairs.  But what to do next with this is not very clear.  The weights should be greater, the greater the distance between the average values ‚Äã‚Äã( <img src="https://habrastorage.org/getpro/habr/post_images/91c/607/695/91c607695a7ea1bdf5ac08bc8e8a765e.gif">  ), while still they should be less if the variance is very large, that is, the components are more noise than informative.  All my attempts at optimization at this stage led to the need to take one most important component, and not to consider the rest, which obviously will not give a good result.  In the end, I scored on the dispersion and just counted for each component <img src="https://habrastorage.org/getpro/habr/post_images/ce6/03c/1e2/ce603c1e28b7a945bc582ff63636822a.gif">  , the exponent k was chosen experimentally, and for different sets of parameters the best indicator could turn out to be equal to both 0.01 and 4. <br><br>  Yes, this method gave much better results than just calculating the norm of the vector.  But he has at least two significant drawbacks.  The first thing that catches your eye is a formula sucked out of your fingers for weights, which means that we are not talking about any optimality.  Well, and secondly, all components are considered separately and independently, that is, they are meant to be uncorrelated, which is generally wrong. <br><br><h4>  Linear regression </h4><br>  The correct approach is to mathematically formalize the problem and find the optimal solution in some sense.  As is known, the simplest function is a linear function, so we will look for a function of the form <img src="https://habrastorage.org/getpro/habr/post_images/cc6/7eb/2cf/cc67eb2cf7c3fea925bce60c735f8521.gif">  (yes, it looks very similar to those weights). <img align="right" title="The direction of the greatest difference and the dividing line" src="https://habrastorage.org/getpro/habr/post_images/108/bd6/aba/108bd6abae7394fd2771d985ceb228e2.jpg">  Finding such a function is called linear regression.  The coefficients b <sub>0</sub> , b <sub>1</sub> , ... b <sub>n</sub> will be chosen in such a way that this OLS function best suits the correct values ‚Äã‚Äãfor the test sample.  And the correct values ‚Äã‚Äãare Y = 0 for identical objects and Y = 1 for different.  Intuitively, it seems that the value of b <sub>0</sub> must be equal to 0, because on the zero vector of difference we have guaranteed identical objects.  But we will not play a guessing game and calculate b <sub>0</sub> to be honest.  Thus, we essentially define the direction in the n-dimensional space in which the same and different pairs will differ most strongly. <br><br>  <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2580%25D0%25B5%25D1%2581%25D1%2581%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7">(source)</a> The sum of the squares of the errors will be defined as <img src="https://habrastorage.org/getpro/habr/post_images/81b/502/6e7/81b5026e714158e396441411c471188f.gif">  .  Hereinafter, M is the number of test sample objects, the superscript denotes the number of the object in the sample, the superscript is the number of the object's components.  This error is minimized at the point at which the derivative for each component is zero.  Elementary mathematics leads us to the system Ab = f, where A is the matrix (n + 1) * (n + 1), <img src="https://habrastorage.org/getpro/habr/post_images/3e2/4bc/a58/3e24bca580667aa8b5d301c500d2bd09.gif">  , <img src="https://habrastorage.org/getpro/habr/post_images/f09/e5e/467/f09e5e4674288ef49fd007e905030bb9.gif">  , <img src="https://habrastorage.org/getpro/habr/post_images/732/c9f/0b2/732c9f0b26efe4871d0d667f2c42bc9e.gif">  , i, k = 1..n. <img src="https://habrastorage.org/getpro/habr/post_images/6da/504/3be/6da5043be5184e00d420f65f244d6d6a.gif">  , <img src="https://habrastorage.org/getpro/habr/post_images/fde/2cd/e47/fde2cde477cc75f924ab853018e214cf.gif">  .  We calculate the elements of the matrix by the test sample, we solve the system, we get the coefficients, we build the function. <br><br>  It may seem strange that some coefficients b <sub>i</sub> turn out to be less than zero, that is, as if the more strongly the objects differ in this parameter, the more they are similar.  In fact, there is nothing wrong with that, it just says that we have learned the correlation between different parameters (for example, roughly speaking, this parameter only grows with another parameter and compensates for the third one). <br><br>  My experience shows that such a function distinguishes between identical and different pairs of objects, much better than randomly chosen weights.  This is not surprising, because this function is optimal in a certain sense. <br><br><h4>  Problems? </h4><br>  What problems may arise? <br><br>  First, to determine the coefficients of the function of n parameters, it is necessary to calculate a matrix of size (n + 1) * (n + 1), that is, all pixels of the image 256x256 cannot be represented as separate parameters, there is not enough memory.  Here you can resort to the traditional methods of lowering the dimension, in particular <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25B3%25D0%25BB%25D0%25B0%25D0%25B2%25D0%25BD%25D1%258B%25D1%2585_%25D0%25BA%25D0%25BE%25D0%25BC%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D1%2582">PCA</a> . <img align="right" title="Dimension increase in SVM" src="https://habrastorage.org/getpro/habr/post_images/312/473/878/312473878a42890bcf1bf679d6545d4b.jpg">  Or you can choose a certain number of the most significant parameters (for example, weed out those whose average value for different does not exceed the average value for the same, or the dispersion is too large).  And since no one reads the entire article, I propose to select parameters that have the most beautiful numbers, for example, that end in a seven. <br><br>  Another problem is if linear approximation is not enough, and the function separates classes too poorly.  Then you can use more complex classifiers - <a href="http://ru.wikipedia.org/wiki/AdaBoost">adaboost</a> , <a href="http://ru.wikipedia.org/wiki/SVM">SVM</a> + kernel trick ( <a href="http://habrahabr.ru/post/105220/">article on the topic</a> ), etc. <br><br>  Another important problem of all ‚Äúteacher-to-teacher‚Äù systems is <a href="http://ru.wikipedia.org/wiki/%25CF%25E5%25F0%25E5%25EE%25E1%25F3%25F7%25E5%25ED%25E8%25E5">retraining</a> .  Roughly speaking, if we have a lot of different parameters, then when learning we can detect some regularities that we don‚Äôt actually have (random fluctuations).  Then, when checking outside the training sample, this pseudo-consistency pattern will no longer exist, which will negatively affect the recognition result.  Wikipedia has mentioned methods for dealing with this, such as regularization, cross-validation, etc. <img align="right" title="Linear approximation with and without emissions" src="https://habrastorage.org/getpro/habr/post_images/c7d/ca5/63c/c7dca563c708c3a9b1de7c54b6319024.jpg"><br><br>  It can also worsen the result of training on a sample with errors.  If there are a lot of erroneous values ‚Äã‚Äãin the training sample, they can significantly affect the final result.  Traditionally, this is solved in the following way: training takes place, after which those objects that have produced a big error on this model are thrown out of the training sample.  Under the new, ‚Äúthinned out‚Äù learning sample, training is again being conducted.  If desired, the procedure can be repeated several times. <br><br><h4>  findings </h4><br>  Even if you come up with a clever system of matching contours, detecting tails or counting fingers, you will still most likely have a vector of parameters according to which you will need to build an evaluation function and make a decision, and you will still need machine learning.  And even the simplest mathematical model gives much better results than ‚Äúgeneral considerations‚Äù, because it is in some way optimal. </div><p>Source: <a href="https://habr.com/ru/post/205428/">https://habr.com/ru/post/205428/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../205414/index.html">Computer mouse turned 45 years old</a></li>
<li><a href="../205418/index.html">Speaking bot on php + prolog</a></li>
<li><a href="../205420/index.html">The first Russian changed his operator without losing numbers</a></li>
<li><a href="../205424/index.html">Promotion of non-pop app on Google Play</a></li>
<li><a href="../205426/index.html">Profiling and debugging Python, debugging</a></li>
<li><a href="../205432/index.html">Viks Bluetooth Vibro-Bracelet VI-T2: accessory for unresponsive ladies</a></li>
<li><a href="../205434/index.html">Unsealed - to jail</a></li>
<li><a href="../205436/index.html">Freelance engineering freelance or through the eyes of a design engineer</a></li>
<li><a href="../205440/index.html">Curiosity 2013: lake, geology, radiation</a></li>
<li><a href="../205442/index.html">Distribution of servers: sum up</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>