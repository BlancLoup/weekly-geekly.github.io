<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Magic Tester. How an untrained person notices mistakes, and how to teach the same robot</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! My name is Ilya Katsev, and I represent a small research unit in the testing department at Yandex. You could already read about our experimenta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Magic Tester. How an untrained person notices mistakes, and how to teach the same robot</h1><div class="post__text post__text-html js-mediator-article">  Hello!  My name is Ilya Katsev, and I represent a small research unit in the testing department at Yandex.  You could already read about our experimental <a href="http://habrahabr.ru/search/%3Fq%3D%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BE%25D1%2582%25D0%25B5%25D1%2581%25D1%2582%25D0%25B5%25D1%2580">project Roboster</a> - a robot that can make a significant part of the routine work as a tester. <br><br>  Our main goal is to invent radically new approaches for finding errors in Yandex.  I really like to speculate on how much of the work in the future will fall on the shoulders of robots.  They <a href="http://lenta.ru/news/2011/04/19/machine/">write sports reports</a> very well and <a href="http://ru.wikipedia.org/wiki/BigDog">carry loads for soldiers</a> .  And in general, it seems to me that human progress directly depends on the amount of work that we can transfer to robots - in this case, people have free time and they come up with new cool things. <br> <a href="http://habrahabr.ru/company/yandex/blog/221379/"><img src="https://habrastorage.org/getpro/habr/post_images/109/036/43e/10903643ece424c3374757e9123a41c6.jpg"></a> <br><br>  There is a thesis, which I personally support, that any robot that the unprepared user can notice can be detected.  That is what we tried to check.  But then the question arises: what exactly does a person think is a mistake? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Nobody invented artificial intelligence yet (although Hawking, for example, <a href="http://www.independent.co.uk/news/science/stephen-hawking-transcendence-looks-at-the-implications-of-artificial-intelligence--but-are-we-taking-ai-seriously-enough-9313474.html">thinks</a> it is for the best), but we have managed to achieve some successes.  In this post, I will talk about how we formulated the experience that testers-people have and tried to teach him our robots, and what came of it. <br><a name="habracut"></a><br><h4>  Robotster.  First approach </h4><br>  When we created Robotester, we were faced with the task of automating the routine work that takes the time of the tester.  We taught the algorithm to open a web page, understand what actions can be performed with elements, fill in text fields, press buttons and try to guess when an error occurred.  But we had a problem - it turned out that with high confidence the robot can detect only the most trivial errors. <br><br>  The fact is that the principle of operation of the robot - to check as many pages as possible - led to a large number of tests, and this meant that the accuracy of determining the error was becoming a key factor.  We calculated that our robot was doing 800,000 different checks per day to check Yandex.Market.  Accordingly, if the false positive rate checks an average of 0.001 (which seems to be a small number), then every day there will be a report on the allegedly found 800 errors, which actually are not.  Naturally, this does not suit anyone. <br><br>  It should be noted that "fake" error messages always happen.  Especially when it comes to web tests, as they use auxiliary tools - there may be problems with the browser that the robot uses, problems with the Internet, and so on. <br><br>  There are very ‚Äúclean‚Äù checks.  For example, the response code of the link 404 with great probability indicates that there is a problem.  An example of a test with low accuracy is the presence of the letter "," (space and comma) in the text.  Indeed, when no data is loaded between the space and the comma, such a combination of letters indicates errors.  But sometimes it's just a typo.  Especially when the text of user comments or content from external sources is in the scan area.  Therefore, to find these errors, it is necessary to carry out work manually, separating the ‚Äúreal‚Äù problems from the ‚Äúfake‚Äù ones. <br><br>  As a result, the robot really completely took over some of the work of the testers (for example, found all the broken links), but not at all.  And we thought about how to "add robot intelligence." <br><br><h4>  How does the robot guess about the error? </h4><br>  Traditionally, in testing, the program under test is run with some given input data and compares the results with some kind of ‚Äúcorrect‚Äù result also in advance.  It can be set manually, or, say, saved when the previous version of the program is started (then we can find all the differences between this version and the previous one), but the general principle remains the same.  In essence, the approach as a whole does not change here for decades, only the tools for the implementation of these tests change. <br>  However, it turns out that fundamentally different approaches are possible to determine what is an error. <br><br>  We initially said that the robot must learn to identify errors that anyone can, not necessarily a professional tester of this service. <br><br>  So where is the error on this page? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/11a/a3c/6b8/11aa3c6b83fef2c9a5e87c969ae4bd91.png"><br><br>  Of course, the correct answer - among the five sentences from the bottom there are recurring.  To understand that this is a mistake, you do not need to be an expert in the Market - you just have a minimum of common sense. <br><br>  Another example. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/953/d34/982/953d349824ec52c22d98f167b7b9a925.png"><br><br>  Here it says ‚Äúnot available‚Äù in one place, and prices in the other.  Again, absolutely anyone understands that this is an incorrect situation. <br><br>  One more example. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/640/589/ae8/640589ae898c78f44361155ac41c4491.png"><br><br>  This is just a blank page.  That is, it has a standard ‚Äúcap‚Äù and ‚Äúfooter‚Äù, but there is no content in the middle. <br><br>  This is where the key question arises - how does a person understand that this is a mistake?  What mechanism is it based on? <br><br>  We thought for a long time, and it seems that the key word here is experience.  That is, a person has already seen many web pages in his life, some of which were of exactly the same type.  And it was from them that he formed some kind of pattern.  This template can be either very general (‚Äúthe page should not be empty‚Äù, ‚Äúall search results should be different‚Äù), or, conversely, linked to these pages (‚Äútwo selected blocks contain or do not contain digits simultaneously‚Äù).  That is, these rules have already been formed in his head and, as we can see from these examples, they look quite simple. <br><br>  Thus, the further task here became clear - we need to learn how to automatically extract these rules by analyzing a lot of similar or not very similar web pages. <br><br>  It can be seen that the formulated rules are quite simple, and if we learned how to automatically extract them, it would be possible to form autotests without human intervention.  For example, a robot is given many pages of Yandex services for analysis, and he understands that there is a link ‚ÄúAbout the company‚Äù on all of them from below.  Accordingly, this fact is formulated as a rule, and the robot will check its implementation for the new version of the service or even for another service. <br><br>  The simplest application of this idea is this: we analyze all the pages of the service in production, isolate such ‚Äúrules‚Äù, and then check whether they are executed for the new version of the same service. <br><br>  This approach has long been known to all.  It is called back-to-back testing and there are those who use it.  However, this method is well applicable only when the work of the service should not significantly change from the version. <br><br>  Let's look at the search results yesterday and today - the search results and advertising are changing nearby.  Onii depend on the region of the user, his preferences, time of day, finally.  Accordingly, with direct comparison there will be too much ‚Äúgarbage‚Äù (note that in some situations such a direct method is admissible). <br><br>  Here we develop our idea and carry out a comparison, ‚Äúcleared‚Äù from garbage - we compare not the pages themselves, but some ‚Äúmodels‚Äù of the pages. <br><br><h4>  Blocks </h4><br>  Having formulated a general idea, let's move on to how to implement it.  Our task is to automatically retrieve the rules.  For human understanding, they are very simple.  We formulate them in the language of ‚Äúblocks‚Äù - we are talking about whether a certain part of the page contains an element or not, whether there is a specific block on the page, and so on.  For a person, the page is divided into them in a natural way - this is the authorization form, this is the search form (and in it the sub-blocks are the search string, the ‚Äúfind‚Äù button).  However, the robot ‚Äúsees‚Äù the html-code in front of it, and it is completely incomprehensible to him which part of this code forms a block. <br><br>  We decided to articulate the concept of a block.  That is, at first we decided to understand for ourselves what we, the people, call a block.  Let's see what blocks are. <br><br>  1. Permanent.  The whole block is fully present on each page). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/156/8e3/b21/1568e3b21020b75acb2465ff9dd7e2c4.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/01a/edf/da9/01aedfda95cce5846284e3f499f297e7.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/4e8/7c1/226/4e87c12260e317e5e36d4b598528ee37.png"><br><br>  2. Changing.  Some tag is present on every page, but inside it, everything changes every time. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d5a/3d9/ddf/d5a3d9ddf4d0718963ff54eec9c5d052.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/943/2c7/740/9432c77405db79a0fbcab6a9e4ca3a85.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/694/9f2/0ca/6949f20ca7341f894e03b6e9452fc4a8.png"><br><br>  3. Blocks of the same type on one page. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c9c/3c5/704/c9c3c57040cc2f2d0f6f175ca38ebf9a.png"><br><br>  4. One-type, but essentially different blocks on one page. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/abe/9b6/7b1/abe9b67b10e74a7a75ac512ebe57fbc6.png"><br><br>  At the same time on different pages the same block can stand in different places. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c1c/aa0/0af/c1caa00af083257abe01dc61d0d09286.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f24/7e3/a5b/f247e3a5b384fe2d0a7c5d0921cba836.png"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/11a/9cf/6ec/11a9cf6ec56950057fd6a722f1da3ba9.png"><br><br>  That is, for us, the ‚Äúblock‚Äù is a certain part of the page, which when it changes remains more or less constant.  For example, a person easily understands that the forms from paragraph 2 (see above) are special cases of the ‚Äúsame orange form‚Äù, but the content of the form changes. <br><br>  A separate problem is represented by several similar blocks going in a row.  Sometimes these are just ‚Äúseveral blocks of the same type‚Äù, and sometimes they are a set of objects of different types, and then you want to understand what type of objects are there. <br><br>  First of all, we need some way to mark an already selected block on each page.  Traditional xpath (for example, <code>/html/body/div[1]/table[2]/tbody/tr/td[2]/div[2]/ul/li[3]/span/a</code> ) is bad because the numbers in it can change - the same block can be the third or fifth in a row.  Depending on what other blocks there are (see the last two examples). <br><br>  Therefore, we first remove the numbers from the xpath (by getting <code>/html/body/div/table/tbody/tr/td/div/ul/li/span/a</code> instead of <code>/html/body/div[1]/table[2]/tbody/tr/td[2]/div[2]/ul/li[3]/span/a</code> ) and we get a much more stable match option (that is, this block will fall under xpath on any page), but less accurate - that is, the same xpath now, of course, can correspond to several elements.  To reduce this inaccuracy, we add the values ‚Äã‚Äãof some of its attributes to the description of each tag.  We simply divided the attributes into more or less stable ones.  For example, the id attribute is often re-generated when the page is loaded, that is, it is completely uninformative for us.  For example, for the block discussed above, the following description was obtained: another ipath <br><br><pre> <code class="hljs cs">&lt;body&gt; &lt;div <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-max-width"</span></span>&gt; &lt;table <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"l-head"</span></span>&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"l-headc"</span></span>&gt; &lt;div <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-head-search"</span></span> onclick=<span class="hljs-string"><span class="hljs-string">"return {name:'b-head-search'}"</span></span>&gt; &lt;div <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-head-searchwrap b-head-searcharrow"</span></span>&gt; &lt;form <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-search"</span></span>&gt; &lt;table <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-searchtable"</span></span>&gt; &lt;tbody&gt; &lt;tr&gt; &lt;td <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-searchinput"</span></span>&gt; &lt;span <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-form-input b-form-input_is-bem_yes b-form-input_size_16 i-bem"</span></span> onclick=<span class="hljs-string"><span class="hljs-string">"return {'b-form-input':{name:'b-form-input'}}"</span></span>&gt; &lt;span <span class="hljs-keyword"><span class="hljs-keyword">class</span></span>=<span class="hljs-string"><span class="hljs-string">"b-form-inputbox"</span></span>&gt; &lt;text src=<span class="hljs-string"><span class="hljs-string">"  Turion II"</span></span>&gt;</code> </pre><br><br>  Now let's look for the blocks.  If we represent the html-code as a <a href="http://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B5%25D1%2580%25D0%25B5%25D0%25B2%25D0%25BE_(%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B8%25D1%258F_%25D0%25B3%25D1%2580%25D0%25B0%25D1%2584%25D0%25BE%25D0%25B2)">tree</a> , then our task is to search for stable paths from the root vertex (stability here means that the path changes little when the page is changed).  The algorithm for constructing a set of such stable paths is the main part of our tool.  We built it "by trial and error."  That is, we modified the algorithm and looked at the result.  I will describe the algorithm below, but for now I'll tell you about a completely unexpected problem - we suddenly realized that we did not know how to portray which blocks stood out on the page. <br><br>  Indeed - the block looks different on different pages.  We wanted to get a report in the form of something that looks like a page divided into blocks.  But which of the block options to take?  Over and over again, we came across instances when an empty or uninformative version came into our report.  It was necessary to somehow compactly show all the options on the same page.  One of the developers came up with a good idea - to do something like, for example, made <strike>on porn sites</strike> in the <a href="http://coub.com/">Coub</a> service: when you hover the mouse on the video, there are shots from there.  We did exactly the same thing - if you hover your mouse over a block, this block starts to ‚Äúblink‚Äù, showing how it looks on different pages.  It turned out to be very convenient. <br><br>  As a result, the ‚Äúpage model‚Äù looks like this (see below).  You can look at it and immediately understand which blocks stand out. <br><br><img src="//habrastorage.org/files/a52/63a/9e6/a5263a9e6d574893af072fd2cf28ba7d.png"><br><br><h4>  Block Algorithm </h4><br>  Now about the algorithm.  The basic algorithm is as follows: <br><br><ol><li>  Choose one page from the set. </li><li>  From the first page we collect all the maximal paths from the root vertex to the others. </li><li>  Then for each selected path we try to lay it on all pages. <br>  If you manage to lay a part of the path that is different in length by at least 4, add this part to the set of paths. <br>  If it almost turned out to pave the entire path (did not reach 1-3 tags), then we replace the path in the set with this chopped off variant. </li><li>  As a result, the resulting paths and correspond to the blocks.  Then it is easy to choose from them those that are on all pages, or, say, 95% of the pages. </li></ol><br>  The great advantage of the algorithm is that it depends linearly on both the number of elements of the first page and the number of pages. <br><br>  However, as we soon learned, there are places where the algorithm works poorly.  For example, for these blocks (isbn, author, and so on): <br><br><img src="http://1450828717945492434276"><br><br>  As html-elements they are exactly the same, they differ only in the sequence number.  But it is immediately clear that this number does not mean anything - in the example above ISBN, then the second, then the first.  Therefore, it became clear to us that the only way to distinguish such elements is by the text inside.  Therefore, we changed the algorithm as follows: at the very beginning we are looking for places where the element has several descendants that differ only in text.  Then we collect similar elements of descendants from all pages, and then we cluster them in the text so that each page contains no more than one representative of each cluster. <br><br>  There were other improvements to the algorithm, but they may not be so interesting to you, since they are associated with Yandex specificity.  Even the algorithm described now will work quite well on most page sets. <br><br><h4>  Rule generation </h4><br>  So, after analyzing the structure of the pages from the training sample, selecting the blocks and understanding which blocks on how many pages are present, we can formulate the rules of the following types: <br><br><ul><li>  Block A must be present on the page. </li><li>  Blocks A and B must be present at the same time. </li><li>  Blocks A and B cannot be present at the same time. </li></ul><br>  In principle, it is possible to extract more complex laws, but we have not done it yet. <br><br>  For which blocks do we form a condition of the first type?  For those that are found on a fairly large proportion of pages.  This proportion is determined by a special parameter, which we call the confidence level.  It is wrong to set this parameter equal to one - the way we wanted to do initially, for two reasons: <br><br><ol><li>  One of the 100 pages of the training sample may not open (by timeout, for example), and as a result, no conditions on the mandatory presence of the block will not stand out. </li><li>  Among the pages of the training sample, pages with errors can happen by chance. </li></ol><br>  We vary this level of significance depending on the stability of the environment in which we train our instrument.  By reducing the value of the parameter, you can "remove" the effect of environmental instability.  However, if this parameter is greatly reduced, the tool will begin to highlight additional irregular patterns.  For example, if 96% of goods have a price, and 4% says ‚Äúnot available,‚Äù when setting a parameter to 0.95, pages with ‚Äúnot available‚Äù will be marked as erroneous.  By default, confidence level is 0.997. <br><br>  In addition to the conditions for the presence / absence of blocks, we automatically generate conditions for the text inside each block.  Conditions are formed in the form of regular expressions.  For example, on the block with the year of publication on this page <br><br><img src="http://1450828717945492434276"><br><br>  will be the following regular season: <br>  Year of publication: [0-9] {4} <br>  Or even this: <br>  Year of publication: (19 | 20) [0-9] {2} <br><br>  At the same time, this block itself is marked as optional.  That is, the following is checked - that if there is such a block, then after the words ‚Äúyear of publication‚Äù 4 numbers are written in it, or even 4 numbers are written, the first two of which are 19 or 20. A man would write about such a test. <br><br><h4>  Magic in practice </h4><br>  To summarize, our tool is able to do the following: for a set of pages, we extract rules of a certain type, which are executed for a given percentage of these pages.  How to find bugs with this?  There are two ways. <br><br><ol><li>  Considering that behavior that occurs infrequently is an error.  This is a controversial thesis, but many times it was possible to find functional bugs in this way.  Then it is enough to analyze a large number of pages on the service and mark those that are not like the others.  In any case, this path will not work if, relatively speaking, ‚Äúeverything is broken.‚Äù  Therefore, we usually use a different approach. </li><li>  Collect pages from the N version of the service (usually just from production), build rules based on them.  Then take a lot of pages from the N + 1 version of the service (usually it's just testing) and check on them the implementation of these rules.  It turns out a clear list of all the differences between the N and N + 1 versions. </li></ol><br>  In fact, in the second case, we get the same thing as in the presence of regression autotests.  However, in our case there is a big, even a huge advantage, - the code of these tests should not be written and maintained!  As soon as they posted a new version of the service, I press the ‚Äúrebuild all‚Äù button and in a few minutes a new version of ‚Äúautotests‚Äù is ready. <br><br>  As an implementation experiment, we used this approach when launching the <a href="http://market.yandex.com.tr/">Turkish version of Market</a> .  Testing experts wrote a list of required cases and what checks to do on which pages.  We crawler selected many pages of the service and divided them into groups - search results pages, category pages and so on.  Further, for each group of pages, the rules described above were automatically constructed and manually checked which of the indicated checks were actually carried out.  It turned out that about 90% of the checks were generated automatically.  In this case, the expected effort to write autotests were about three weeks, and our tool was able to do everything and set it up in one day. <br><br>  Of course, this does not mean that now any tests can be formed many times faster.  For example, our tool is still able to automatically create only static tests (without dynamic operations with page elements). <br><br>  Now we are busy implementing MT on various Yandex services in order to get an exact answer to the question of what part of human work this tool does.  But it is already obvious that quite considerable. <br><br>  As I said, the projects Roboster and Magic Tester are experimental and are being done in the research department of the testing department.  If you are an undergraduate and you are interested in trying yourself in our tasks, <a href="">come</a> to us for an internship. </div><p>Source: <a href="https://habr.com/ru/post/221379/">https://habr.com/ru/post/221379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../221365/index.html">Comparative review: Nokia X versus typical Android state employee</a></li>
<li><a href="../221367/index.html">Porting Netsukuku to Vala Completed</a></li>
<li><a href="../221373/index.html">"Performance" and "Live" - ‚Äã‚Äãinnovations in the statistics of Openstat</a></li>
<li><a href="../221375/index.html">User is drunk</a></li>
<li><a href="../221377/index.html">[Translation] Investigation of manipulations with the control panel. Part 1</a></li>
<li><a href="../221385/index.html">One day at the office of Veeam Software in St. Petersburg</a></li>
<li><a href="../221391/index.html">Lessons learned from a buried project</a></li>
<li><a href="../221395/index.html">All MIT students will hand out the equivalent of $ 100 in bitcoins</a></li>
<li><a href="../221413/index.html">Interview with the founder of the project 1WorldOnline, previously director of product development at Motorola, Alexey Fedoseyev</a></li>
<li><a href="../221415/index.html">The thinnest, most flexible and transparent transistor in the world</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>