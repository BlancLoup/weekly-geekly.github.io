<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How Amazon Go Perhaps Implements the ‚ÄúJust Get Out‚Äù Shopping Scheme</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In our time, the press releases of techno-companies do little to surprise us. The details of innovation either flow away a few months earlier or are n...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How Amazon Go Perhaps Implements the ‚ÄúJust Get Out‚Äù Shopping Scheme</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/geektimes/post_images/58c/7b7/06a/58c7b706a6dda65d68b1552b8620c392.jpg" alt="image"><br><br>  In our time, the press releases of techno-companies do little to surprise us.  The details of innovation either flow away a few months earlier or are not very impressive.  But recently we encountered several real surprises.  A few months before the Switch was released, Nintendo decided that the future of consoles was their past, and announced the <a href="https://arstechnica.com/gaming/2016/07/surprise-nintendos-next-console-is-the-nes/">NES Classic</a> .  And the <a href="https://arstechnica.com/information-technology/2016/03/google-ai-begins-battle-with-humanitys-best-go-player-tonight/">victory of</a> Google AlphaGo over a champion among people discouraged experts, who believed that such results could be obtained not earlier than ten years later. <br><br>  The December <a href="https://arstechnica.com/business/2016/12/trying-and-failing-to-sneak-into-amazons-skynet-take-on-grocery-shopping/">announcement of the</a> retail store Amazon Go, in which you can simply pick up products from the shelves and exit, can be compared to the shock of news about AlphaGo.  The ‚Äúpick up and go‚Äù method has been known for some time as ‚Äúthe future of retail sales‚Äù and has been ‚Äújust a few years away‚Äù from our time.  I worked for more than ten years in the research department of robotics in Caltech, Stanford and Berkeley, and now I head a <a href="http://www.getkuna.com/">startup</a> that manufactures security cameras for outdoor use.  Computer vision was a big part of my work.  But just a few months before the announcement, I confidently told someone that it would take several more years to implement the ‚Äútook and gone‚Äù system.  And I was not the only one who thought so - just two months before <a href="http://www.npr.org/sections/money/2016/10/19/498571623/episode-730-self-checkout">Planet Money</a> had an episode on this topic. <br><a name="habracut"></a><br>  So when Amazon suddenly surprised us all by creating such a thing, the first question was obvious: how will this work?  In the promotional video rush loud words such as computer vision, in-depth training and the synthesis of sensors.  But what does all this mean and how to really unite all these things? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I will start with the disclosure of intrigue: in fact, I do not know.  I did not take part in the development of the project, and the company did not talk about how it works.  But, given my experience and work in the field of computer vision, I can do a few guesses backed up with knowledge.  At its core, Amazon Go looks like the same product of the development of AI, computer vision and automatic decision-making, like AlphaGo, and sudden breakthroughs in the field of robo mobiles.  The breakthroughs in statistics and parallel computing over the past five years have created a new milestone in the field of machine intelligence. <br><br>  That is why advanced developments happen in waves, and therefore, by allowing the mobile phone to take you to the store to buy a bag of milk, you destroy the interaction between people much earlier than anyone could have imagined. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/NrmMk1Myrxc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Shopping basket </h2><br>  To better understand how Amazon Go‚Äôs ecosystem works, you need to outline the problem.  In the case of a grocery store, Amazon has to answer one question: what does a visitor take away when leaving the store?  In other words, what's in his shopping cart? <br><br>  In fact, there are only two ways to answer the question.  Amazon needs to either look into the cart when the user leaves, or keep track of what exactly goes into this cart.  The first way we call the queue at the cashier, and this is how most modern stores work (check everything that the user takes with them).  Another approach I call a bar account.  As a bartender tracks all customer orders, so a business can find out what is in the shopping cart, keeping track of what exactly goes into the basket or leaves it.  Ideally, you will know exactly what is there, and you will not have to force users to demonstrate their purchases. <br><br>  Of course, Amazon Go is no ordinary grocery store.  He should not only find out what is in each specific basket, but also understand who to write off for her money.  To charge in the world without cashiers, you need to identify the user. <br><br>  How does Amazon deal with this?  How will the company keep track of people in the store, and what they take from the shelves or return, while avoiding errors?  It all starts with the cameras.  They are unobtrusive and cheap, and they can be put everywhere.  Amazon talked about this by mentioning computer vision in a video.  But how to handle what the cameras see, and use it to track customers and their actions?  Then comes the second loud term, deep learning. <br><br><h2>  Neurons </h2><br>  The idea of ‚Äã‚Äãusing cameras in the process of charging was born a long time ago, but until recently it was just an idea. <br><br>  So far, the algorithms of view have worked through finding noticeable properties of the image and collecting them into objects.  From the image it was possible to extract lines, angles and edges.  Four lines and four corners in a certain combination give you a square (or a rectangle).  The same principles can be used to identify and track more complex objects using more complex properties and sets.  The complication of visual algorithms depends on the complexity of the properties and techniques used to recognize certain sets of properties of objects. <br><br>  For a long time, the most interesting progress in computer vision and machine learning depended on the invention of more complex features by researchers.  Instead of lines and corners came wavelets and Gaussian blur, and properties with esoteric names like SIFT and SURF.  For a time, the best property for identifying a person in an image was called HOG.  But pretty quickly it became clear that the meticulous creation of properties manually quickly rests on the ceiling of its capabilities. <br><br>  Algorithms based on the recognition of certain properties, surprisingly well worked on the recognition of what they have already seen.  Show the algorithm a picture of a pack of six cans of cola, and he will become a world expert in recognizing packs of six cans of coke.  But the generalization of these algorithms was not given;  it was much harder for them to recognize soda in general, or the wider world of drinks. <br><br>  To make matters worse, these systems were unreliable, and it was very difficult to improve them.  Correction of errors required diligent manual adjustment of the logic of work, and only doctors of science who understood how the algorithm worked could do this.  In the case of a store, you might not care if the algorithm mixed up a bottle of cola with a bottle of pepsi, but you would be worried if the algorithm took a bottle of wine worth $ 20 per bottle of soda to cost $ 2. <br><br>  Today's deep learning opportunities are intentionally designed to get rid of the manual search and adjustment of image features.  Instead of trying to manually find the characteristic properties, you use huge amounts of data to train a neural network.  For examples of what it needs to recognize, the neural network finds its own features.  Low-level neurons learn to recognize simple things such as lines, and their output is transmitted upward, to neurons that combine these primitives into more complex things, such as forms, into a hierarchical architecture. <br><br>  It is not necessary to specify which features should be recognized by neurons; in the process of training, they simply appear independently.  Neurons determine which laws make better sensitivity.  If you are trying to create a soda discernment system, you show it tens of thousands of soda images, and it will go from lines and curves to forms, and then to boxes and bottles. <br><br>  Our brain works in much the same way, so error correction takes place according to human schemes.  On examples.  If your neural network confuses wine and soda, you need to fix it, finding a few thousand more or other examples, and train it on them.  She herself will figure out how to distinguish objects. <br><br>  Software for simulating the work of neurons has existed for several decades, but its use for computer vision has long remained in the theoretical field.  To simulate the view of animals requires from tens to hundreds of layers of neurons, each of which contains tens of thousands of neurons.  And with each new layer the number of connections between the layers grows exponentially.  Such networks require huge computer capacities, and for training large data arrays. <br><br>  To create a neural network operating in a reasonable time, it is necessary to fine-tune its structure to minimize the number of internal connections.  But even then too much horsepower is required. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/32d/a6d/a59/32da6da59dc034f9be7a4faf29cfce71.jpg"><br><br>
<h2>  Computational Cooperation </h2><br>  The next breakthrough was related to the use of GPUs as desktop supercomputers.  The simulation of a neural network requires the collection of input data and the calculation of the output data for a variety of neurons - and this process is easy to parallelize.  The hours of computing on the most powerful CPUs started running in minutes on the average GPU hand. <br><br>  Parallel GPU computing finally allowed researchers to take advantage of the old discovery ‚Äî structuring a neural network to simulate vision.  Recall that even a simple network of several hundred thousand neurons can have billions of connections.  All of them need to simulate, unless there is some shortcut for the operation of these compounds. <br><br>  Fortunately, to create seeing networks, you can cheat a little - we have amazing examples of neural networks that are optimized for vision right in our heads.  Neurobiology has been marking the visual cortex of mammals for decades, which served as inspiration.  Thus was born the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">convolutional neural network</a> (SNS).  Over the past few years, it has become one of the most popular and powerful tools in the field of computer vision. <br><br>  Convolution is an amazing mathematical concept, a simple explanation of which goes beyond my capabilities.  One of the most colorful, but from a technical point of view, absolutely wrong ways to imagine it is to take one mathematical function and move it along another, watching the result. <br><br>  In the SNA, as in the visual cortex, there are neurons that are sensitive to certain properties (say, noses), and they are distributed throughout the field of view.  The output of these neurons is connected so as if we took the only neuron sensitive to the noses and led them around the field of view.  The result is an output containing location information on the nose image.  This, of course, is not limited to noses - the effect is used to create spatial layouts of where certain features are on the images.  These spatial relationships are fed to the higher layers of the network, and are combined in them to recognize patterns and objects. <br><br>  SNA became a revelation in the field of computer vision.  They are extremely useful for generalized object recognition: you train the SNS to recognize not a particular car or person, but cars or people in general.  They even made irrelevant one of the famous comics XKCD. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/78e/0dd/88a/78e0dd88ac714084d4158ddc38d4a15d.png" alt="image"><br><br>  And because of the spatial nature of their structure, they very well lend themselves to parallelization on the GPU.  Different neurons that monitor different parts of the image can be simulated completely independently.  Suddenly, it became possible to quickly and inexpensively recognize people, places and objects with impressive accuracy. <br><br>  The simultaneous explosion of the popularity of mobile phones and networks meant that hundreds of millions of people went online and uploaded billions of images to services like <a href="https://arstechnica.com/information-technology/2017/01/the-origins-and-future-of-artificial-intelligence-at-facebook/">Facebook</a> and Google, unwittingly creating huge sets for training algorithms. <br><br>  Recent advanced developments go even further.  The researchers have created a <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2580%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">recurrent neural network</a> (RNS), which has built-in memory.  Instead of simply transferring connections to the next layer, it uses internal connections to create persistent memory.  If you are familiar with digital logic, then as an analogy, you can imagine the triggers.  So, you can train the network with a single visual layer, "looking" at the image, and transmitting everything he saw into memory, so that the network can recognize actions on the video. <br><br>  And after these developments, you suddenly have algorithms that can recognize people, objects, and actions with extremely high accuracy.  In other words, you can train the algorithms to recognize a person, know where the product is from the store when it is moved, and recognize when a person puts or takes it from the shelf.  You only need a little GPU.  And how convenient that one of the largest collections of GPUs available on request is owned by Amazon - this is their extremely powerful and profitable AWS cloud service. <br><br>  Have we cracked Amazon Go's secret by combining cheap cameras with brain algorithms and an army of computers?  Not really, because you need to solve another problem.  The camera angle is limited - so how can a business cover the entire store space with them?  What if the client is between the camera and the shelf? <br><br>  To do this, you need to make sure that any area is viewed on multiple cameras.  But this raises another question - how to combine the input data of several cameras into a coherent picture of what is happening? <br><br><h2>  Food synthesis </h2><br>  To do this, back in the 1960s.  Then NASA engineers faced a big problem - they had a lot of different navigation tools, from gyros to tracking stars, and they needed to reduce all the measurements into one best estimate of the spacecraft location. <br><br>  Amazon Go had a similar problem.  In order for this whole undertaking to work, it is necessary to combine the observations from several different cameras for different periods of time into one coherent information about the shopping cart.  The catch is that the world is essentially an indefinite place, so the decision was in accepting this uncertainty.  Instead of trying to determine everything with maximum precision, successful models use a probabilistic approach. <br><br>  At NASA, it was an algorithm called the <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580_%25D0%259A%25D0%25B0%25D0%25BB%25D0%25BC%25D0%25B0%25D0%25BD%25D0%25B0">Kalman filter</a> , which they used to take into account the errors of each instrument and the combination of measurements into the best possible estimate.  The Kalman filter is based on the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2581%25D0%25B0">Bayes formula</a> . <br><br>  In essence, the Bayesian formula is a mathematical relationship that connects the observation of an event and the probability of its occurrence, and gives you the probability that an event actually occurred.  The result is the following: our belief that one of the probable states is true (a posteriori probability) is equal to the strength of our belief in this state prior to observation (a priori probability) multiplied by the data obtained from the sensors supporting this state. <br><br>  Returning to the example of wine and soda: let's say the neural network reports that the client took the wine.  The Bayesian formula tells us that the probability that he actually took it is equal to the probability that he will take the wine, multiplied by the probability that the camera correctly reports the fact of taking the wine. <br><br>  Amazon has two big advantages when using a probabilistic scheme based on the Bayes formula.  The first is that a company may consider a priori probabilities, since it knows the history of previous purchases of many customers.  This means that if an Amazon Go customer buys coffee and a cupcake every Tuesday, even before he goes to the respective shelves, the store can already increase the likelihood of these purchases.  This is a natural way to use a huge amount of data on users, which the company already has. <br><br>  The second big advantage is that translating everything into a probability language allows you to add multiple dimensions from multiple sensors over multiple periods of time.  Assuming the independence of the observations, you can simply multiply the probabilities.  Also, the posterior probability of one event can be used as a priori for another. <br><br><img src="https://habrastorage.org/getpro/geektimes/post_images/04c/543/7cf/04c5437cfbef8dc2b8b5df4691b4bcc0.jpg"><br><br>  For example, let several cameras see one regiment.  Some stand closer, some farther.  Several cameras believe that the client took an inexpensive soda from the shelf, one believes that he took an expensive product, one did not see anything, and the last one believes that he was just picking his nose.  And now what? <br><br>  Amazon could come up with a complex logic for this case, from which it would follow which camera you can believe.  Was it located closer and was there a better overview of the camera, which considers that the client took expensive soda?  Has the buyer blocked the camera, which saw picking his nose?  But all you need is probability.  Based on the number of errors of each camera, depending on its location and overview, the Bayes formula tells us how to combine all the input data in order to understand what the probability was that the user took cheap soda, expensive, or did not take anything. <br><br>  In fact, since you moved into the wonderful world of probabilities, the Bayes formula allows you to combine input data from completely different types of sensors. <br><br>  Therefore, Amazon has sent <a href="http://www.recode.net/2015/3/30/11560904/we-may-have-just-uncovered-amazons-vision-for-a-new-kind-of-retail">requests for patenting</a> methods for using RFID sensors to automatically pay for purchases.  Passive RFID sensors are placed on products, and then read by scanners located in the store.  This technology is an excellent candidate for the creation of an auto shop, since it is cheap and widespread today.  And since it allows remote scanning, it can be used instead of a cashier.  Place the scanner where customers go, and you will see what they have in the basket, without the need to get the goods and present them to the cashier.  When watching the promotional video, I noticed that all the goods were pre-packaged - canned food, packages of chips and plastic containers with food.  These products are not only more profit, they also allow you to place a label on each item. <br><br>  But using RFID alone has disadvantages.  It is impossible to distinguish one customer from another.  You see that the store is leaving a set of soda, chips and sandwiches, and you understand that this is a purchase, but who bought it?  In addition, RFID may give errors.  If two buyers pass by the scanner, you can scan the purchases of both, and not find out who ordered what. <br><br>  Probability estimates based on the Bayesian formula help to cope with such problems.  Amazon may give out probabilities by location and possible purchase combinations for hundreds of shoppers.  The situation is similar to the many-worlds interpretation of quantum mechanics: every time after a certain customer action, the store creates a new ‚Äúworld‚Äù with this action and tracks it (updating the probability of this world according to Bayes). <br><br>  Let's go back to cameras and a soda example: based on RFID, Amazon can use scans to confirm or deny cameras, without having to develop any special logic. <br><br>  And the cherry on the cake.  As in the case of machine learning of neural networks, probabilistic estimates are improved with the involvement of a larger amount of data.  As in the case of statistics, the more measurements you make, the better you get.  Each new data set improves the accuracy of the system and its perception by the user. <br><br><h2>  And Amazon proudly presents ... your dinner </h2><br>  The description may not be accurate, and for sure we will not know this until Amazon reveals its cards, but the Bayes formula helps to complete a fairly realistic picture of how this new-fangled system can work. <br><br>  Entering the store, you spend on the scanner smartphone.  Cameras running algorithms with image recognition and in-depth training track you while you go shopping.  Each time you take an item or return it, the cameras recognize this action.  Observations from several cameras are combined using the Bayes formula, and give information about what you have taken.  The system keeps track of all possible combinations of goods taken by you.  Each time you walk through a door or frame, you are scanned with RFID tags, which allows the system to reduce the list of combinations.  When you leave the store, the system looks at a list of what you think it has, chooses a guess with the highest probability, and deducts the required amount from your account. <br><br>  All this has become possible with the development of deep learning, cloud computing and probabilistic assessments.  Amazon Go could not be made even five years ago, but today all the components are already available.  And the same combination is currently at the core of the development of romo mobiles, AI, text translation systems and much more.  Today it is very interesting to work in the field of computer training.  And although it is very interesting for me to find out what else awaits us, I hope to enjoy shopping soon, where you can just take the goods and leave. </div><p>Source: <a href="https://habr.com/ru/post/403797/">https://habr.com/ru/post/403797/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../403787/index.html">Advisory love: advisors from Ernst & Young have joined the Polybius team</a></li>
<li><a href="../403789/index.html">The fragile beauty of the Samsung Galaxy S8 pleases owners of repair services</a></li>
<li><a href="../403791/index.html">Ezviz S5 and S5 +: high-definition action cameras</a></li>
<li><a href="../403793/index.html">Android under attack: 350 new malware appear every hour</a></li>
<li><a href="../403795/index.html">"The choice is simple": a subjective project on the choice of electronic equipment</a></li>
<li><a href="../403799/index.html">Thule Paramount TFDP-115: a great backpack for the city and short trips</a></li>
<li><a href="../403801/index.html">Garmin appeared on Aliexpress?</a></li>
<li><a href="../403803/index.html">The search for the causes of our existence turned into a study of strange atomic decay</a></li>
<li><a href="../403805/index.html">FCS refuses to let Xiaomi phones bought in online stores abroad to Russia</a></li>
<li><a href="../403807/index.html">Information Philosophy, Chapter 6. Creatures</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>