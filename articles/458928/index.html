<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>XLNet vs. BERT</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the end of June, a team from Carnegie Mellon University showed us XLNet, immediately laying out the publication , the code and the finished model (...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>XLNet vs. BERT</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/py/g0/es/pyg0es7u25w7xb0cc8z49aczcls.png"><br><br>  At the end of June, a team from Carnegie Mellon University showed us XLNet, immediately laying out the <a href="https://arxiv.org/abs/1906.08237">publication</a> , the <a href="https://github.com/zihangdai/xlnet/">code</a> and the finished model ( <a href="">XLNet-Large</a> , Cased: 24-layer, 1024-hidden, 16-heads).  This is a pre-trained model for solving various problems of natural language processing. <br><br>  In the publication, they immediately identified a comparison of their model with Google <a href="https://habr.com/ru/post/436878/">BERT</a> .  They write that XLNet surpasses BERT in a large number of tasks.  And it shows in 18 tasks state-of-the-art results. <br><a name="habracut"></a><br><h2>  BERT, XLNet and Transformers </h2><br>  One of the latest trends in deep learning is Transfer Learning.  We train models to solve simple problems on a huge amount of data, and then we use these pre-trained models, but already to solve other, more specific tasks.  BERT and XLNet are just such pre-trained networks that can be used to solve natural language processing tasks. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      These models develop the idea of <a href="https://habr.com/ru/post/341240/">transformers</a> - the currently dominant approach to building models for working with sequences.  Very detailed and with examples of the code on transformers and the attentional mechanism (Attention mechanism) is written in the article <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a> . <br><br>  If you look at the <a href="https://gluebenchmark.com/leaderboard/">General Language Understanding Evaluation (GLUE) benchmark Leaderboard</a> , from the top you can see many models based on transformers.  Including both models, which show the result better than man.  We can say that with the transformers we see a mini-revolution in the processing of natural language. <br><br><h2>  BERT disadvantages </h2><br>  BERT is an auto encoder (autoencoder, AE).  He hides and corrupts some words in a sequence and tries to restore the original sequence of words from the context. <br><br>  This leads to shortcomings of the model: <br><br><ul><li>  Each hidden word is predicted separately.  We lose information about possible links between masked words.  The article gives an example with the name "New York".  If we try to independently predict these words by context, we will not take into account the connection between them. </li><li>  The mismatch between the phases of training the BERT model and the use of the pre-trained BERT model.  When we train a model, we have hidden words ([MASK] tokens), when we use the pre-trained model, we don‚Äôt submit such tokens to it. </li></ul><br>  And yet, despite these problems, BERT showed state-of-the-art results on many natural language processing tasks. <br><br><h2>  XLNet Features </h2><br>  XLNet is an autoregressive model (AR LM).  She tries to predict the next token by the sequence of the previous ones.  In classical autoregressive models, this context sequence is taken independently from the two directions of the source line. <br><br>  XLNet summarizes this method and forms the context from different places in the original sequence.  How he does it.  It takes all (in theory) possible permutations of the original sequence and predicts each token in the sequence according to the previous ones. <br><br>  Here is an example from the article how the token x3 is predicted from various permutations of the original sequence. <br><br><img src="https://habrastorage.org/webt/yq/mb/fa/yqmbfas9mcnfkciq6pmew_-4hh8.png"><br><br>  The context is not a bag of words.  Information about the initial order of tokens is also supplied to the model. <br><br>  If we draw analogies with BERT, it turns out that we do not mask tokens in advance, but rather use different sets of hidden tokens for different permutations.  At the same time, the second problem of BERT also disappears - the absence of hidden tokens when using a pre-trained model.  In the case of XLNet, the entire sequence arrives at the input, without masks. <br><br>  Where XL in the title.  XL - because XLNet uses the Attention mechanism and ideas from the Transformer-XL model.  Although evil tongues claim that XL hints at the amount of resources needed to train a network. <br><br><img src="https://habrastorage.org/webt/hs/fb/u-/hsfbu-ufj-9e-me1agkauoa389c.png"><br><br>  And about the resources.  On Twitter, they posted a <a href="https://twitter.com/eturner303/status/1143174828804857856">calculation of</a> how much it would cost to train a network with parameters from the article.  It turned out 245,000 dollars.  True, an engineer came from Google and <a href="https://twitter.com/jekbradbury/status/1143397614093651969">corrected</a> that the article mentions 512 TPU chips, which are four on the device.  That is, the cost is already 62440 dollars, or even 32720 dollars, if we take into account the 512 cores, which are also mentioned in the article. <br><br><h2>  XLNet vs. BERT </h2><br>  So far, only one pre-trained model for the English language has been laid out for the article (XLNet-Large, Cased).  But the article mentions experiments with smaller models.  And in many tasks, XLNet models show better results compared to similar BERT models. <br><br><img src="https://habrastorage.org/webt/ac/p_/th/acp_thyxqwgcuyhvfvkkixhwj_y.png"><br><br>  The emergence of BERT and especially pre-trained models attracted a lot of attention from researchers and led to a huge number of related works.  Now here's the XLNet.  It is interesting to see whether it will for some time become the de facto standard in NLP, or vice versa, will spur researchers in search of new architectures and approaches for processing natural language. </div><p>Source: <a href="https://habr.com/ru/post/458928/">https://habr.com/ru/post/458928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458916/index.html">Under the hood of React. We write our implementation from scratch</a></li>
<li><a href="../458918/index.html">What can you learn from the design of giperkazualnyh games</a></li>
<li><a href="../45892/index.html">Stages of development of a promotional site. Request</a></li>
<li><a href="../458920/index.html">DevOps Fan Conference</a></li>
<li><a href="../458922/index.html">How to move from ESXi to KVM / LXD and not go crazy</a></li>
<li><a href="../458930/index.html">How do students from Perm get to the final of the international data analysis championship Data Mining Cup 2019</a></li>
<li><a href="../458932/index.html">Yota - or how you can find out everything</a></li>
<li><a href="../458934/index.html">Deploying Applications on Multiple Kubernetes Clusters with Helm</a></li>
<li><a href="../458936/index.html">‚ÄúIt's easier to answer than to remain silent‚Äù - a great interview with the father of transactional memory, Maurice Herlihy</a></li>
<li><a href="../458940/index.html">How we implemented Agile-testing</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>