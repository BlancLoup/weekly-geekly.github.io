<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>CUDA: Working with memory. Part I.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the process of working with CUDA, I almost didn‚Äôt touch on the use of graphics card memory. It is time to remove this gap. 

 Since the topic is ve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>CUDA: Working with memory. Part I.</h1><div class="post__text post__text-html js-mediator-article">  In the process of working with CUDA, I almost didn‚Äôt touch on the use of graphics card memory.  It is time to remove this gap. <a name="habracut"></a><br><br>  Since the topic is very voluminous, I decided to divide it into several parts.  In this part, I will talk about the main types of memory available on the video card and give an example of how the choice of the type of memory affects the performance of calculations on the GPU. <br><br><h2>  Video card and memory types </h2><br>  When using a GPU, the developer has several types of memory available: registers, local, global, shared, constant, and texture memory.  Each of these types of memory has a specific purpose, which is determined by its technical parameters (speed, level of access to read and write).  The hierarchy of memory types is shown in Fig.  one. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img title="GPU Memory Types" src="http://www.picamatic.com/show/2009/03/25/11/21/3017283_725x512.png"><br>  Fig.  1. Types of video card memory <br><ol><li>  <b>Register memory</b> (register) is the fastest of all kinds.  You can determine the number of registers of available GPUs using the already well-known cudaGetDeviceProperties function.  It is also easy to calculate the number of registers available to a single GPU thread, for this it is necessary to divide the total number of registers by the product of the number of threads in the block and the number of blocks in the grid.  All GPU registers are 32 bit.  There are no explicit ways to use register memory in CUDA; the compiler takes all the work of placing data in registers. </li><li>  <b>Local memory</b> can be used by the compiler with a large number of local variables in a function.  According to the speed characteristics, the local memory is much slower than the register memory.  The documentation from nVidia recommends using local memory only in the most necessary cases.  Explicit means of blocking the use of local memory is not provided, so when the performance drops, it is worth carefully analyzing the code and eliminating unnecessary local variables. </li><li>  <b>Global memory</b> (global memory) - the slowest type of memory available from the GPU.  Global variables can be selected using the __global__ specifier, as well as dynamically, using functions from the cudMallocXXX family.  Global memory is mainly used to store large amounts of data received on the device from the host, this movement is performed using the functions cudaMemcpyXXX.  In algorithms that require high performance, the number of operations with global memory must be minimized. </li><li>  <b>Shared memory</b> refers to a fast type of memory.  Shared memory is recommended to minimize access to global memory, as well as to store local variables of functions.  Addressing shared memory between thread threads is the same within one block, which can be used to exchange data between threads within one block.  To allocate data in shared memory, the __shared__ specifier is used. </li><li>  <b>Constant memory</b> (constant memory) is quite fast of the available GPU.  A distinctive feature of constant memory is the ability to write data from the host, but within the GPU only reading from this memory is possible, which causes its name.  To place data in the constant memory, the specifier __constant__ is provided.  If it is necessary to use an array in constant memory, then its size must be specified in advance, since dynamic allocation, in contrast to global memory, is not supported in constant memory.  To write from host to constant memory, use the cudaMemcpyToSymbol function, and to copy from device to host cudaMemcpyFromSymbol, as you can see, this approach is slightly different from the approach when working with global memory. </li><li>  <b>Texture memory</b> (texture memory), as the name implies, is designed primarily for working with textures.  Texture memory has specific features in addressing, reading and writing data.  In more detail about the texture memory, I will discuss when considering issues of image processing on the GPU. </li></ol><br><h2>  Shared Memory Example </h2><br>  Just above, I briefly talked about the different types of memory that are available when programming a GPU.  Now I want to give an example of using shared memory during matrix transposition. <br><br>  Before you start writing the main code, I‚Äôll give a small way to debug.  As you know, functions from the CUDA runtime API can return various error codes, but the previous time I did not take it into account.  To simplify your life you can use the following macro to catch errors: <br><blockquote><code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">#define</font> CUDA_DEBUG <br> <br> <font color="#0000ff">#ifdef</font> CUDA_DEBUG <br> <br> <font color="#0000ff">#define</font> CUDA_CHECK_ERROR(err)           \ <br> <font color="#0000ff">if</font> (err != cudaSuccess) {          \ <br> printf( <font color="#A31515">"Cuda error: %s\n"</font> , cudaGetErrorString(err));    \ <br> printf( <font color="#A31515">"Error in file: %s, line: %i\n"</font> , __FILE__, __LINE__);  \ <br> }                 \ <br> <br> <font color="#0000ff">#else</font> <br> <br> <font color="#0000ff">#define</font> CUDA_CHECK_ERROR(err) <br> <br> <font color="#0000ff">#endif</font> <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br>  As you can see, if the environment variable CUDA_DEBUG is defined, the error code is checked and information about the file and the line where it occurred is displayed.  This variable can be enabled when compiling for debugging and disabled when compiling for release. <br><br>  Getting down to the main task. <br><br>  In order to see how the use of shared memory affects the speed of calculations, you should also write a function that will use only global memory. <br>  We write this function: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#008000">//       </font> <br> <font color="#008000">//</font> <br> <font color="#008000">// inputMatrix -    </font> <br> <font color="#008000">// outputMatrix -    </font> <br> <font color="#008000">// width -    (   -)</font> <br> <font color="#008000">// height -    (   -)</font> <br> <font color="#008000">//</font> <br> <font color="#0000ff">__global__ void</font> transposeMatrixSlow( <font color="#0000ff">float</font> * inputMatrix, <font color="#0000ff">float</font> * outputMatrix, <font color="#0000ff">int</font> width, <font color="#0000ff">int</font> height) <br> { <br> <font color="#0000ff">int</font> xIndex = blockDim.x * blockIdx.x + threadIdx.x; <br> <font color="#0000ff">int</font> yIndex = blockDim.y * blockIdx.y + threadIdx.y; <br> <br> <font color="#0000ff">if</font> ((xIndex &lt; width) &amp;&amp; (yIndex &lt; height)) <br> { <br> <font color="#008000">//     </font> <br> <font color="#0000ff">int</font> inputIdx = xIndex + width * yIndex; <br> <br> <font color="#008000">//    -</font> <br> <font color="#0000ff">int</font> outputIdx = yIndex + height * xIndex; <br> <br> outputMatrix[outputIdx] = inputMatrix[inputIdx]; <br> } <br> } <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  This function simply copies the rows of the original matrix into the columns of the result matrix.  The only difficult point is to define the indexes of matrix elements; here it is necessary to remember that when calling the kernel, different dimensions of blocks and grid can be used, for this the built-in variables blockDim, blockIdx are used. <br><br>  We write a transpose function that uses shared memory: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">#define</font> BLOCK_DIM 16 <br> <br> <font color="#008000">//    c   </font> <br> <font color="#008000">//</font> <br> <font color="#008000">// inputMatrix -    </font> <br> <font color="#008000">// outputMatrix -    </font> <br> <font color="#008000">// width -    (   -)</font> <br> <font color="#008000">// height -    (   -)</font> <br> <font color="#008000">//</font> <br> <font color="#0000ff">__global__ void</font> transposeMatrixFast( <font color="#0000ff">float</font> * inputMatrix, <font color="#0000ff">float</font> * outputMatrix, <font color="#0000ff">int</font> width, <font color="#0000ff">int</font> height) <br> { <br> __shared__ <font color="#0000ff">float</font> temp[BLOCK_DIM][BLOCK_DIM]; <br> <br> <font color="#0000ff">int</font> xIndex = blockIdx.x * blockDim.x + threadIdx.x; <br> <font color="#0000ff">int</font> yIndex = blockIdx.y * blockDim.y + threadIdx.y; <br> <br> <font color="#0000ff">if</font> ((xIndex &lt; width) &amp;&amp; (yIndex &lt; height)) <br> { <br> <font color="#008000">//      </font> <br> <font color="#0000ff">int</font> idx = yIndex * width + xIndex; <br> <br> <font color="#008000">//   </font> <br> temp[threadIdx.y][threadIdx.x] = inputMatrix[idx]; <br> } <br> <br> <font color="#008000">//    </font> <br> __syncthreads(); <br> <br> xIndex = blockIdx.y * blockDim.y + threadIdx.x; <br> yIndex = blockIdx.x * blockDim.x + threadIdx.y; <br> <br> <font color="#0000ff">if</font> ((xIndex &lt; height) &amp;&amp; (yIndex &lt; width)) <br> { <br> <font color="#008000">//      </font> <br> <font color="#0000ff">int</font> idx = yIndex * height + xIndex; <br> <br> <font color="#008000">//   </font> <br> outputMatrix[idx] = temp[threadIdx.x][threadIdx.y]; <br> } <br> } <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  In this function, I use shared memory as a two-dimensional array. <br>  As already mentioned, the addressing of shared memory within the same block is the same for all threads, therefore, to avoid collisions when accessing and writing, each element in the array corresponds to one thread in the block. <br>  After copying the elements of the original matrix into the temp buffer, the function __syncthreads is called.  This function synchronizes threads within a block.  Its difference from other ways of synchronization lies in the fact that it runs only on the GPU. <br>  At the end, the saved elements of the original matrix are copied into the result matrix, in accordance with the transposition rule. <br>  It may seem that this function should be performed more slowly than its version without shared memory, where there are no intermediaries.  But in fact, copying from global to global memory is much slower than a bunch of global memory - shared memory - global memory. <br>  I want to note that it is worth checking manually the boundaries of the arrays of the matrices, the GPU does not have the hardware to monitor the boundaries of the arrays. <br><br>  And finally, let's write the transpose function, which is executed only on the CPU: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#008000">//   ,   CPU</font> <br> <font color="#0000ff">__host__ void</font> transposeMatrixCPU( <font color="#0000ff">float</font> * inputMatrix, <font color="#0000ff">float</font> * outputMatrix, <font color="#0000ff">int</font> width, <font color="#0000ff">int</font> height) <br> { <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> y = 0; y &lt; height; y++) <br> { <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> x = 0; x &lt; width; x++) <br> { <br> outputMatrix[x * height + y] = inputMatrix[y * width + x]; <br> } <br> } <br> } <br></font> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  Now you need to generate data for the calculations, copy them from the host to the device, in case of using the GPU, make performance measurements and clear resources. <br>  Since these stages are about the same as I described last time, I quote this fragment right away: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">#define</font> GPU_SLOW 1 <br> <font color="#0000ff">#define</font> GPU_FAST 2 <br> <font color="#0000ff">#define</font> CPU 3 <br> <br> <font color="#0000ff">#define</font> ITERATIONS 20 <font color="#008000">//  </font> <br> <br> <font color="#0000ff">__host__ int</font> main() <br> { <br> <font color="#0000ff">int</font> width = 2048; <font color="#008000">// </font> <br> <font color="#0000ff">int</font> height = 1536; <font color="#008000">// </font> <br> <br> <font color="#0000ff">int</font> matrixSize = width * height; <br> <font color="#0000ff">int</font> byteSize = matrixSize * <font color="#0000ff">sizeof</font> ( <font color="#0000ff">float</font> ); <br> <br> <font color="#008000">//     </font> <br> <font color="#0000ff">float</font> * inputMatrix = <font color="#0000ff">new</font> <font color="#0000ff">float</font> [matrixSize]; <br> <font color="#0000ff">float</font> * outputMatrix = <font color="#0000ff">new</font> <font color="#0000ff">float</font> [matrixSize]; <br> <br> <font color="#008000">//   </font> <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; matrixSize; i++) <br> { <br> inputMatrix[i] = i; <br> } <br> <br> <font color="#008000">//    </font> <br> printf( <font color="#A31515">"Select compute mode: 1 - Slow GPU, 2 - Fast GPU, 3 - CPU\n"</font> ); <br> <font color="#0000ff">int</font> mode; <br> scanf( <font color="#A31515">"%i"</font> , &amp;mode); <br> <br> <font color="#008000">//    </font> <br> printMatrixToFile( <font color="#A31515">"before.txt"</font> , inputMatrix, width, height); <br> <br> <font color="#0000ff">if</font> (mode == CPU) <font color="#008000">//   CPU</font> <br> { <br> <font color="#0000ff">int</font> start = GetTickCount(); <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; ITERATIONS; i++) <br> { <br> transposeMatrixCPU(inputMatrix, outputMatrix, width, height); <br> } <br> <font color="#008000">//     CPU ( )</font> <br> printf ( <font color="#A31515">"CPU compute time: %i\n"</font> , GetTickCount() - start); <br> } <br> <font color="#0000ff">else</font> <font color="#008000">//    GPU</font> <br> { <br> <font color="#0000ff">float</font> * devInputMatrix; <br> <font color="#0000ff">float</font> * devOutputMatrix; <br> <br> <font color="#008000">//       </font> <br> CUDA_CHECK_ERROR(cudaMalloc(( <font color="#0000ff">void</font> **)&amp;devInputMatrix, byteSize)); <br> CUDA_CHECK_ERROR(cudaMalloc(( <font color="#0000ff">void</font> **)&amp;devOutputMatrix, byteSize)); <br> <br> <font color="#008000">//      </font> <br> CUDA_CHECK_ERROR(cudaMemcpy(devInputMatrix, inputMatrix, byteSize, cudaMemcpyHostToDevice)); <br> <br> <font color="#008000">//  </font> <br> dim3 gridSize = dim3(width / BLOCK_DIM, height / BLOCK_DIM, 1); <br> dim3 blockSize = dim3(BLOCK_DIM, BLOCK_DIM, 1); <br> <br> cudaEvent_t start; <br> cudaEvent_t stop; <br> <br> <font color="#008000">// event'       GPU</font> <br> CUDA_CHECK_ERROR(cudaEventCreate(&amp;start)); <br> CUDA_CHECK_ERROR(cudaEventCreate(&amp;stop)); <br> <br> <font color="#008000">//    GPU</font> <br> cudaEventRecord(start, 0); <br> <br> <font color="#0000ff">if</font> (mode == GPU_SLOW) <font color="#008000">//    </font> <br> { <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; ITERATIONS; i++) <br> { <br> <br> transposeMatrixSlow&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(devInputMatrix, devOutputMatrix, width, height); <br> } <br> } <br> <font color="#0000ff">else</font> <font color="#0000ff">if</font> (mode == GPU_FAST) <font color="#008000">//    </font> <br> { <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> i = 0; i &lt; ITERATIONS; i++) <br> { <br> <br> transposeMatrixFast&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(devInputMatrix, devOutputMatrix, width, height); <br> } <br> } <br> <br> <font color="#008000">//  </font> <br> cudaEventRecord(stop, 0); <br> <br> <font color="#0000ff">float</font> time = 0; <br> <font color="#008000">//    </font> <br> cudaEventSynchronize(stop); <br> <font color="#008000">//   GPU</font> <br> cudaEventElapsedTime(&amp;time, start, stop); <br> <br> <font color="#008000">//    </font> <br> printf( <font color="#A31515">"GPU compute time: %.0f\n"</font> , time); <br> <br> <font color="#008000">//     </font> <br> CUDA_CHECK_ERROR(cudaMemcpy(outputMatrix, devOutputMatrix, byteSize, cudaMemcpyDeviceToHost)); <br> <br> <font color="#008000">//</font> <br> <font color="#008000">//   </font> <br> <font color="#008000">//</font> <br> <br> CUDA_CHECK_ERROR(cudaFree(devInputMatrix)); <br> CUDA_CHECK_ERROR(cudaFree(devOutputMatrix)); <br> <br> CUDA_CHECK_ERROR(cudaEventDestroy(start)); <br> CUDA_CHECK_ERROR(cudaEventDestroy(stop)); <br> } <br> <br> <font color="#008000">// -  </font> <br> printMatrixToFile( <font color="#A31515">"after.txt"</font> , outputMatrix, height, width); <br> <br> <font color="#008000">//   </font> <br> delete[] inputMatrix; <br> delete[] outputMatrix; <br> <br> <font color="#0000ff">return</font> 0; <br> }</font> <br> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  If calculations are performed only on the CPU, then the GetTickCount () function is used to measure the calculation time, which is connected from windows.h.  To measure the time of calculations on the GPU, use the cudaEventElapsedTime function, the prototype of which is as follows: <br><br>  cudaError_t cudaEventElapsedTime (float * time, cudaEvent_t start, cudaEvent_t end), where <br><ol><li>  <b>time</b> - pointer to float, to record the time between start and end events (in milliseconds), </li><li>  <b>start</b> - handle of the first event, </li><li>  <b>end</b> - handle of the second event. </li></ol><br>  Returns: <br><ol><li>  cudaSuccess - if successful </li><li>  cudaErrorInvalidValue - incorrect value </li><li>  cudaErrorInitializationError - initialization error </li><li>  cudaErrorPriorLaunchFailure - error during previous asynchronous function launch </li><li>  cudaErrorInvalidResourceHandle - invalid event handle </li></ol><br><br>  I also write the original matrix and the result in the files through the function printMatrixToFile.  To make sure the results are correct.  The code for this function is as follows: <br><br><blockquote> <code><a href="http://virtser.net/blog/post/source-code-highlighter.aspx"></a> <font color="black"><font color="#0000ff">__host__ void</font> printMatrixToFile( <font color="#0000ff">char</font> * fileName, <font color="#0000ff">float</font> * matrix, <font color="#0000ff">int</font> width, <font color="#0000ff">int</font> height) <br> { <br> FILE* file = fopen(fileName, <font color="#A31515">"wt"</font> ); <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> y = 0; y &lt; height; y++) <br> { <br> <font color="#0000ff">for</font> ( <font color="#0000ff">int</font> x = 0; x &lt; width; x++) <br> { <br> fprintf(file, <font color="#A31515">"%.0f\t"</font> , matrix[y * width + x]); <br> } <br> fprintf(file, <font color="#A31515">"\n"</font> ); <br> } <br> fclose(file); <br> }</font> <br> <br> <font color="gray">* This source code was highlighted with <font color="gray">Source Code Highlighter</font> .</font></code> </blockquote> <br><br>  If the matrices are very large, then outputting data to files can slow down the execution of the program. <br><br><h2>  Conclusion </h2><br><br>  In the process of testing, I used matrices of 2048 * 1536 = 3145728 elements and 20 iterations in load cycles.  After the measurement results, I obtained the following results (Fig. 2). <br><br><img title="Performance comparison" src="http://www.picamatic.com/show/2009/03/25/10/56/3017052_501x472.png"><br>  Fig.  2. Calculation time.  (less is better). <br><br>  As you can see, the GPU version with shared memory runs almost 20 times faster than the version on the CPU.  It is also worth noting that when using shared memory, the calculation is performed about 4 times faster than without it. <br>  In my example, I do not take into account the time of copying data from the host to the device and back, but in real applications it is also necessary to take them into account.  The number of data movement between the CPU and GPU, if possible, should be minimized. <br><br>  PS I hope you enjoyed the performance boost you can get with the GPU. </div><p>Source: <a href="https://habr.com/ru/post/55461/">https://habr.com/ru/post/55461/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../55450/index.html">Captcha. 0 and O.</a></li>
<li><a href="../55452/index.html">Powerful web client technology ... is it good?</a></li>
<li><a href="../55454/index.html">New service MixMarket.BIZ - designer of partner programs Mix-Uni</a></li>
<li><a href="../55456/index.html">Do I need a blog on Habr√© about finding freelancers for a one-time job?</a></li>
<li><a href="../55459/index.html">Matte painting lesson</a></li>
<li><a href="../55462/index.html">Go Russia!</a></li>
<li><a href="../55465/index.html">Easy way to recover deleted files</a></li>
<li><a href="../55468/index.html">New Opera mobile report: Russia, Ukraine and CIS countries</a></li>
<li><a href="../55470/index.html">Old friends are upside-down, or how the LED can be a photosensor</a></li>
<li><a href="../55471/index.html">Polystyrene houses</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>