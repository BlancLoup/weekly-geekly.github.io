<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>‚ÄúTrue, true truth and statistics‚Äù or ‚Äú15 probability distributions for all occasions‚Äù</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Statistics comes to our aid in solving many problems, for example: when there is no possibility to build a deterministic model, when there are too man...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>‚ÄúTrue, true truth and statistics‚Äù or ‚Äú15 probability distributions for all occasions‚Äù</h1><div class="post__text post__text-html js-mediator-article"><img align="right" width="341" height="192" src="https://habrastorage.org/files/58b/6be/16c/58b6be16c0f249a18d715141991517e8.jpg">  Statistics comes to our aid in solving many problems, for example: when there is no possibility to build a deterministic model, when there are too many factors, or when we need to evaluate the likelihood of the constructed model taking into account the available data.  Attitude towards statistics is ambiguous.  There is an opinion that there are three types of lies: lies, impudent lies and statistics.  On the other hand, many ‚Äúusers‚Äù of statistics believe it too much, not fully understanding how it works: applying, for example, the <a href="https://habr.com/ru/post/311092/">Student‚Äôs test</a> to any data without checking its normality.  Such negligence is capable of causing serious mistakes and turning the ‚Äúfans‚Äù of the <a href="https://habr.com/ru/post/311092/">Student‚Äôs test</a> into haters of statistics.  Let us try to put the currents above i and figure out which models of random variables should be used to describe certain phenomena and what kind of genetic relationship exists between them. <br><a name="habracut"></a><br>  First of all, this material will be of interest to students studying probability theory and statistics, although ‚Äúmature‚Äù specialists will be able to use it as a reference book.  In one of the following papers, I will show an example of using statistics to build a test assessing the significance of indicators of stock trading strategies. <br><a name="spisok_raspredeleniy"></a><br>  The paper will consider <a href="https://habr.com/ru/post/311092/">discrete distributions</a> : <br><br><ol><li>  <a href="https://habr.com/ru/post/311092/">Bernoulli</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">binomial</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">geometric</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Pascal (negative binomial)</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">hypergeometric</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Poisson</a> </li></ol><br>  as well as <a href="https://habr.com/ru/post/311092/">continuous distributions</a> : <br><br><ol><li>  <a href="https://habr.com/ru/post/311092/">Gauss (normal)</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">chi-square</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Student's tutorial</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Fisher</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Cauchy</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">exponential (exponential) and Laplace (double exponential, double exponential)</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">Weibull</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">gamma (Erlang)</a> ; </li><li>  <a href="https://habr.com/ru/post/311092/">beta</a> . </li></ol><br>  At the end of the article will be asked a <a href="https://habr.com/ru/post/311092/">question</a> for reflection.  I will present my thoughts on this in the next article. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Some of the cited continuous distributions are special cases of <a href="https://habr.com/ru/post/311092/">the Pearson distribution</a> . <br><a name="discretnye_raspredeleniya"></a><br><h1>  Discrete distributions </h1><br>  Discrete distributions are used to describe events with non-differentiable characteristics defined at isolated points.  Simply put, for events whose outcome can be assigned to a certain discrete category: success or failure, an integer (for example, playing roulette, dice), heads or tails, etc. <br><br>  Describes the discrete distribution of the probability of occurrence of each of the possible outcomes of the event.  As for any distribution (including continuous) for discrete events, the notions of expectation and variance are defined.  However, it should be understood that the expectation for a discrete random event is in general unrealizable as the outcome of a single random event, but rather as a quantity to which the arithmetic average of the outcomes of events will tend when their number increases. <br><br>  In the simulation of discrete random events, combinatorics plays an important role, since the probability of the outcome of an event can be defined as the ratio of the number of combinations giving the desired outcome to the total number of combinations.  For example: in the basket are 3 white balls and 7 black ones.  When we choose 1 ball from the basket, we can do it in 10 different ways (total number of combinations), but only 3 options for which the white ball will be chosen (3 combinations giving the desired outcome).  Thus, the probability of choosing a white ball: <img src="https://tex.s2cms.ru/svg/3%20%5Cover%2010">  ( <a href="https://habr.com/ru/post/311092/">Bernoulli distribution</a> ). <br><br>  Samples with and without return should also be distinguished.  For example, to describe the probability of choosing two white balls, it is important to determine whether the first ball will be returned to the basket.  If not, then we are dealing with a sample without return ( <a href="https://habr.com/ru/post/311092/">hypergeometric distribution</a> ) and the probability will be as follows: <img src="https://tex.s2cms.ru/svg/%7B3%20%5Cover%2010%7D%5Ctimes%7B2%20%5Cover%209%7D">  - the probability to choose a white ball from the initial sample multiplied by the probability to choose the white ball from the remaining ones in the basket again.  If the first ball is returned to the basket, then this is a return sample ( <a href="https://habr.com/ru/post/311092/">Binomial distribution</a> ).  In this case, the probability of choosing two white balls will be <img src="https://tex.s2cms.ru/svg/%5Cleft(3%20%5Cover%2010%20%5Cright)%5E2">  . <br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Bernulli"></a><br><h3>  Bernoulli distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eb8/2e6/0a2/eb82e60a22cc93d1246ac951f4321dab.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2591%25D0%25B5%25D1%2580%25D0%25BD%25D1%2583%25D0%25BB%25D0%25BB%25D0%25B8">from here</a> ) <br><br>  If we formalize the basket example as follows: let the outcome of the event can take one of two values ‚Äã‚Äã0 or 1 with probabilities <img src="https://tex.s2cms.ru/svg/q">  and <img src="https://tex.s2cms.ru/svg/p">  accordingly, then the probability distribution of obtaining each of the proposed outcomes will be called the Bernoulli distribution: <br><br><a name="1_1_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/Bin_%7Bp%2Cq%7D%5Cleft(x%5Cright)%3D%5Cbegin%7Bcases%7Dq%2Cx%3D0%5C%5C%20p%2C%20x%3D1%5Cend%7Bcases%7D%20(1.1.1)" alt="Bin_ {p, q} \ left (x \ right) = \ begin {cases} q, x = 0 \\ p, x = 1 \ end {cases}"></div><br><br>  According to the established tradition, the outcome with a value of 1 is called ‚Äúsuccess‚Äù, and the outcome with a value of 0 is called ‚Äúfailure‚Äù.  Obviously, getting the outcome "success or failure" comes with a probability <img src="https://tex.s2cms.ru/svg/p%2Bq%3D1">  . <br><br>  The expectation and variance of the Bernoulli distribution: <br><br><a name="1_1_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BBin_%7Bp%2Cq%7D%5C%7D%3Dp%5C%20%5C%20%5C%20%5C%20%5Cleft(1.1.2%5Cright)" alt="E \ {Bin_ {p, q} \} = p \ \ \ \ \ left (1.1.2 \ right)"></div><br><a name="1_1_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BBin_%7Bp%2Cq%7D%5C%7D%3Dpq%3Dp%5Cleft(1-p%5Cright)%5C%20%5C%20%5C%20%5C%20%5Cleft(1.1.3%5Cright)" alt="D \ {Bin_ {p, q} \} = pq = p \ left (1-p \ right) \ \ \ \ \ left (1.1.3 \ right)"></div><br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="binominalnoe"></a><br><h3>  Binomial distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e28/ea4/486/e28ea44869db7ac80f181360ac5c42f1.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B8%25D0%25BD%25D0%25BE%25D0%25BC%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  amount <img src="https://tex.s2cms.ru/svg/k">  success in <img src="https://tex.s2cms.ru/svg/n">  tests, the outcome of which is distributed according to <a href="https://habr.com/ru/post/311092/">Bernoulli</a> with probability of success <img src="https://tex.s2cms.ru/svg/p">  (example with the return of the balls in the basket), described by the binomial distribution: <br><br><a name="1_2_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/B_%7Bn%2Cp%7D(k)%3DC%5Ek_np%5Ekq%5E%7Bn-k%7D%5C%20%5C%20(1.2.1)" alt="B_ {n, p} (k) = C ^ k_np ^ kq ^ {n-k} \ \ (1.2.1)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/C%5Ek_n%3D%7Bn!%5Cover%7Bk!(n-k)!%7D%7D" alt="C ^ k_n = {n! \ Over {k! (N-k)!}}">  - number of combinations of <img src="https://tex.s2cms.ru/svg/n">  by <img src="https://tex.s2cms.ru/svg/k">  . <br><br>  In other words, the binomial distribution describes the sum of <img src="https://tex.s2cms.ru/svg/n">  independent random variables capable of <a href="https://habr.com/ru/post/311092/">Bernoulli</a> distribution with probability of success <img src="https://tex.s2cms.ru/svg/n">  . <br>  Expectation and variance: <br><br><a name="1_2_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BB_%7Bn%2Cp%7D%5C%7D%3Dnp%5C%20%5C%20(1.2.2)" alt="E \ {B_ {n, p} \} = np \ \ (1.2.2)"></div><br><a name="1_2_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BB_%7Bn%2Cp%7D%5C%7D%3Dnpq%5C%20%5C%20(1.2.3)" alt="D \ {B_ {n, p} \} = npq \ \ (1.2.3)"></div><br>  Binomial distribution is valid only for the sample with the return, that is, when the probability of success remains constant for the entire series of tests. <br><br>  If values <img src="https://tex.s2cms.ru/svg/X">  and <img src="https://tex.s2cms.ru/svg/Y">  have binomial distributions with parameters <img src="https://tex.s2cms.ru/svg/%5C%7B%20n_x%2C%20p%20%5C%7D">  and <img src="https://tex.s2cms.ru/svg/%5C%7B%20n_y%2C%20p%20%5C%7D">  accordingly, their sum will also be distributed binomially with parameters <img src="https://tex.s2cms.ru/svg/%5C%7B%20n_x%2Bn_y%2C%20p%20%5C%7D">  . <br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="geometricheskoe"></a><br><h3>  Geometric distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cc3/39d/9a2/cc339d9a266d7308a1cb216518d496dd.jpg"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2582%25D1%2580%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  Imagine the situation that we pull the balls out of the basket and return them until the white ball is pulled out.  The number of such operations is described by a geometric distribution.  In other words: the geometric distribution describes the number of tests <img src="https://tex.s2cms.ru/svg/n">  before the first success with the likelihood of success in each trial <img src="https://tex.s2cms.ru/svg/p">  .  If a <img src="https://tex.s2cms.ru/svg/n">  implies a test number in which success occurred, then the geometric distribution will be described by the following formula: <br><br><a name="1_3_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/Geom_p(n)%3Dq%5E%7Bn-1%7Dp%5C%20%5C%20%20(1.3.1)" alt="Geom_p (n) = q ^ {n-1} p \ \ (1.3.1)"></div><br>  The expectation and variance of the geometric distribution: <br><br><a name="1_3_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BGeom_p%5C%7D%3D%7B1%5Cover%7Bp%7D%7D%5C%20%5C%20%5C%20(1.3.2)" alt="E \ {Geom_p \} = {1 \ over {p}} \ \ \ (1.3.2)"></div><br><a name="1_3_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BGeom_p%5C%7D%3D%7Bq%5Cover%7Bp%5E2%7D%7D%5C%20%5C%20%5C%20(1.3.3)" alt="D \ {Geom_p \} = {q \ over {p ^ 2}} \ \ \ (1.3.3)"></div><br>  The geometrical distribution is genetically related to the <a href="https://habr.com/ru/post/311092/">exponential</a> distribution, which describes a continuous random variable: the time before an event occurs, with a constant intensity of events.  Geometric distribution is also a special case of a <a href="https://habr.com/ru/post/311092/">negative binomial distribution</a> . <br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Paskal"></a><br><h3>  Pascal distribution (negative binomial distribution) </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2fb/1ce/4c3/2fb1ce4c3a308edaa72bfd39e2bdaa29.gif"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25BE%25D0%25BC%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  The distribution of Pascal is a generalization of the <a href="https://habr.com/ru/post/311092/">geometric</a> distribution: describes the distribution of the number of failures <img src="https://tex.s2cms.ru/svg/k">  in independent trials, the outcome of which is distributed according to <a href="https://habr.com/ru/post/311092/">Bernoulli</a> with probability of success <img src="https://tex.s2cms.ru/svg/p">  before offensive <img src="https://tex.s2cms.ru/svg/r">  success in the amount.  With <img src="https://tex.s2cms.ru/svg/r%3D1">  we get the <a href="https://habr.com/ru/post/311092/">geometric</a> distribution for <img src="https://tex.s2cms.ru/svg/k%2B1">  . <br><br><a name="1_4_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/NB_%7Br%2Cp%7D(k)%3DC%5Ek_%7Bk%2Br-1%7Dp%5Erq%5Ek%5C%20%5C%20(1.4.1)" alt="NB_ {r, p} (k) = C ^ k_ {k + r-1} p ^ rq ^ k \ \ (1.4.1)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/C%5Ek_n%3D%7Bn!%5Cover%7Bk!(n-k)!%7D%7D" alt="C ^ k_n = {n! \ Over {k! (N-k)!}}">  - number of combinations of <img src="https://tex.s2cms.ru/svg/n">  by <img src="https://tex.s2cms.ru/svg/k">  . <br><br>  The expectation and variance of the negative binomial distribution: <br><br><a name="1_4_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BNB_%7Br%2Cp%7D%5C%7D%3D%7Brq%5Cover%7Bp%7D%7D%5C%20%5C%20%5C%20(1.4.2)" alt="E \ {NB_ {r, p} \} = {rq \ over {p}} \ \ \ (1.4.2)"></div><br><a name="1_4_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BNB_%7Br%2Cp%7D%5C%7D%3D%7Brq%5Cover%7Bp%5E2%7D%7D%5C%20%5C%20%5C%20(1.4.3)" alt="D \ {NB_ {r, p} \} = {rq \ over {p ^ 2}} \ \ \ (1.4.3)"></div><br>  The sum of independent random variables distributed by Pascal is also distributed by Pascal: let <img src="https://tex.s2cms.ru/svg/X">  has a distribution <img src="https://tex.s2cms.ru/svg/NB_%7Br_x%2Cp%7D">  , but <img src="https://tex.s2cms.ru/svg/Y">  - <img src="https://tex.s2cms.ru/svg/NB_%7Br_y%2Cp%7D">  .  Let also <img src="https://tex.s2cms.ru/svg/X">  and <img src="https://tex.s2cms.ru/svg/Y">  independent, then their amount will have distribution <img src="https://tex.s2cms.ru/svg/NB_%7Br_x%2Br_y%2Cp%7D"><br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="gipergeometricheskoe"></a><br><h3>  Hypergeometric distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/239/d88/ba6/239d88ba6b0e64d8687a5e326409e79e.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B8%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B3%25D0%25B5%25D0%25BE%25D0%25BC%25D0%25B5%25D1%2582%25D1%2580%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  So far, we have considered examples of samples with a return, that is, the probability of outcome did not change from trial to trial. <br><br>  Now consider the situation without returning and describe the probability of the number of successful samples from the aggregate with a previously known number of successes and failures (a known number of white and black balls in the basket, trump cards in the deck, defective parts in the game, etc.). <br><br>  Let the total population contain <img src="https://tex.s2cms.ru/svg/N">  objects of which <img src="https://tex.s2cms.ru/svg/D">  marked as ‚Äú1‚Äù and <img src="https://tex.s2cms.ru/svg/N-D">  as "0".  We will consider the choice of the object with the label ‚Äú1‚Äù as success, and with the label ‚Äú0‚Äù as failure.  We will carry out n tests, and the selected objects will no longer participate in further tests.  Probability of occurrence <img src="https://tex.s2cms.ru/svg/k">  success will obey the hypergeometric distribution: <br><br><a name="1_5_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/HG_%7BN%2CD%2Cn%7D(k)%3D%7BC%5Ek_DC%5E%7Bn-k%7D_%7BN-D%7D%5Cover%7BC%5En_N%7D%7D%5C%20%5C%20(1.5.1)" alt="HG_ {N, D, n} (k) = {C ^ k_DC ^ {nk} _ {N-D} \ over {C ^ n_N}} \ \ (1.5.1)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/C%5Ek_n%3D%7Bn!%5Cover%7Bk!(n-k)!%7D%7D" alt="C ^ k_n = {n! \ Over {k! (N-k)!}}">  - number of combinations of <img src="https://tex.s2cms.ru/svg/n">  by <img src="https://tex.s2cms.ru/svg/k">  . <br><br>  Expectation and variance: <br><br><a name="1_5_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BHG_%7BN%2CD%2Cn%7D%5C%7D%3D%7BnD%5Cover%7BN%7D%7D%5C%20%5C%20(1.5.2)" alt="E \ {HG_ {N, D, n} \} = {nD \ over {N}} \ \ (1.5.2)"></div><br><a name="1_5_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BHG_%7BN%2CD%2Cn%7D%5C%7D%3Dn%7BD%5Cover%7BN%7D%7D%7BN-D%5Cover%7BN%7D%7D%7BN-n%5Cover%7BN-1%7D%7D%5C%20%5C%20(1.5.3)" alt="D \ {HG_ {N, D, n} \} = n {D \ over {N}} {N-D \ over {N}} {N-n \ over {N-1}} \ \ (1.5.3)"></div><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Puasson"></a><br><h3>  Poisson distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ed1/225/43f/ed122543fe69a57cf99e55cefa0283fd.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259F%25D1%2583%25D0%25B0%25D1%2581%25D1%2581%25D0%25BE%25D0%25BD%25D0%25B0">from here</a> ) <br><br>  The Poisson distribution differs significantly from the distributions discussed above by its ‚Äúsubject‚Äù area: now it is not the likelihood of one or another test outcome that is considered, but the intensity of events, that is, the average number of events per unit of time. <br><br>  Poisson distribution describes the probability of occurrence <img src="https://tex.s2cms.ru/svg/k">  independent events over time <img src="https://tex.s2cms.ru/svg/t">  with an average intensity of events <img src="https://tex.s2cms.ru/svg/%5Clambda">  : <br><br><a name="1_6_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/P_%7B%5Clambda%2Ct%7D(k)%3D%7B%5Cleft(%5Clambda%20t%5Cright)%5Ek%5Cover%7Bk!%7D%7De%5E%7B-%5Clambda%20t%7D%5C%20%5C%20%5C%20(1.6.1)" alt="P _ {\ lambda, t} (k) = {\ left (\ lambda t \ right) ^ k \ over {k!}} E ^ {- \ lambda t} \ \ \ (1.6.1)"></div><br>  The expectation and variance of the Poisson distribution: <br><br><a name="1_6_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BP_%7B%5Clambda%2Ct%7D%5C%7D%3D%5Clambda%20t%5C%20%5C%20%5C%20(1.6.2)" alt="E \ {P _ {\ lambda, t} \} = \ lambda t \ \ \ (1.6.2)"></div><br><a name="1_6_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BP_%7B%5Clambda%2Ct%7D%5C%7D%3D%5Clambda%20t%5C%20%5C%20%5C%20(1.6.3)" alt="D \ {P _ {\ lambda, t} \} = \ lambda t \ \ \ (1.6.3)"></div><br>  The variance and the expectation of the Poisson distribution are identically equal. <br><br>  The Poisson <a href="https://habr.com/ru/post/311092/">distribution</a> , combined with the <a href="https://habr.com/ru/post/311092/">exponential distribution</a> describing the time intervals between the onset of independent events, form the mathematical basis of the theory of reliability. <br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="nepreryvnye_raspredeleniya"></a><br><h1>  Continuous distribution </h1><br>  Continuous distributions, as opposed to discrete ones, are described by probability density functions (distributions) <img src="https://tex.s2cms.ru/svg/f(x)">  , defined, in general, at some intervals. <br><br>  If the probability density is known for <img src="https://tex.s2cms.ru/svg/x">  : <img src="https://tex.s2cms.ru/svg/f(x)">  and the transformation is defined <img src="https://tex.s2cms.ru/svg/y%3Dg(x)">  , the probability density for y can be obtained automatically: <br><br><a name="2_0_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/f_y(y)%3Df%5Cleft(g%5E%7B-1%7D(y)%5Cright)%5Cleft%7C%7Bdg%5E%7B-1%7D%5Cover%7Bdy%7D%7D(y)%5Cright%7C%5C%20%5C%20%5C%20(2.0.1)" alt="f_y (y) = f \ left (g ^ {- 1} (y) \ right) \ left | {dg ^ {- 1} \ over {dy}} (y) \ right | \ \ \ (2.0.1 )"></div><br>  subject to uniqueness and differentiability <img src="https://tex.s2cms.ru/svg/g%5E%7B-1%7D(x)">  . <br><br>  Probability density <img src="https://tex.s2cms.ru/svg/h(z)">  sums of random variables <img src="https://tex.s2cms.ru/svg/x">  and <img src="https://tex.s2cms.ru/svg/y">  ( <img src="https://tex.s2cms.ru/svg/z%3Dx%2By">  a) with distributions <img src="https://tex.s2cms.ru/svg/f(x)">  and <img src="https://tex.s2cms.ru/svg/g(y)">  described by convolution <img src="https://tex.s2cms.ru/svg/f">  and <img src="https://tex.s2cms.ru/svg/g">  : <br><br><a name="2_0_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/h(z)%3D%5Cint%20f(t)g(z-t)dt%3D(f*g)(z)%5C%20%5C%20%5C%20(2.0.2)" alt="h (z) = \ int f (t) g (z-t) dt = (f * g) (z) \ \ \ (2.0.2)"></div><br><a name="bd"></a>  If the distribution of the sum of random variables belongs to the same distribution as the terms, such a distribution is called infinitely divisible.  Examples of infinitely divisible distributions: <a href="https://habr.com/ru/post/311092/">normal</a> , <a href="https://habr.com/ru/post/311092/">chi-square</a> , <a href="https://habr.com/ru/post/311092/">gamma</a> , <a href="https://habr.com/ru/post/311092/">Cauchy distribution</a> . <br><br>  Probability density <img src="https://tex.s2cms.ru/svg/h(z)">  products of random variables x and y ( <img src="https://tex.s2cms.ru/svg/z%3Dxy">  a) with distributions <img src="https://tex.s2cms.ru/svg/f(x)">  and <img src="https://tex.s2cms.ru/svg/g(y)">  can be calculated as follows: <br><br><a name="2_0_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/h(z)%3D%5Cint%20f(t)g(z%2Ft)dt%5C%20%5C%20%5C%20(2.0.3)" alt="h (z) = \ int f (t) g (z / t) dt \ \ \ (2.0.3)"></div><br><br><a name="Pirson"></a>  Some of the distributions below are special cases of the Pearson distribution, which, in turn, is a solution to the equation: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%7Bdf%5Cover%7Bdx%7D%7D(x)%3D%7Ba_0%2Ba_1x%5Cover%7Bb_0%2B2b_1x%2Bb_2x%5E2%7D%7Df(x)%5C%20%5C%20%5C%20(2.0.4)" alt="{df \ over {dx}} (x) = {a_0 + a_1x \ over {b_0 + 2b_1x + b_2x ^ 2}} f (x) \ \ \ (2.0.4)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/a_i">  and <img src="https://tex.s2cms.ru/svg/b_i">  - distribution parameters.  There are 12 types of Pearson distribution, depending on the values ‚Äã‚Äãof the parameters. <br><br>  The distributions that will be discussed in this section have close relationships with each other.  These relationships are expressed in the fact that some distributions are special cases of other distributions, or they describe transformations of random variables that have other distributions. <br><a name="schema"></a><br>  The diagram below shows the relationship between some of the continuous distributions that will be considered in this paper.  In the diagram, solid arrows show the transformation of random variables (the beginning of the arrow indicates the initial distribution, the end of the arrow indicates the resultant), and the dotted one shows the generalization ratio (the beginning of the arrow indicates the distribution, which is a special case of the one pointed to by the end of the arrow).  For particular cases of the Pearson distribution over the dotted arrows, the corresponding type of Pearson distribution is indicated. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/274/386/74a/27438674a49c4a5788d6b40ac4fe2005.gif"></div><br>  The following overview of distributions covers many cases that occur in data analysis and process modeling, although, of course, it does not contain absolutely all of the distributions known to science. <br><br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Gauss"></a><br><h3>  Normal distribution (Gaussian distribution) </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0fb/4ca/80e/0fb4ca80e8524ba17ef92b52647f0e99.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  Probability density of normal distribution <img src="https://tex.s2cms.ru/svg/N_%7B%5Cmu%2C%5Csigma%7D(x)">  with parameters <img src="https://tex.s2cms.ru/svg/%5Cmu">  and <img src="https://tex.s2cms.ru/svg/%5Csigma">  described by the Gauss function: <br><a name="2_1_1"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/f(x)%3D%7B1%5Cover%7B%5Csigma%20%5Csqrt%7B2%20%5Cpi%7D%7D%7De%5E%7B(x-%5Cmu)%5E2%5Cover%7B2%5Csigma%5E2%7D%7D%5C%20%5C%20%5C%20(2.1.1)" alt="f (x) = {1 \ over {\ sigma \ sqrt {2 \ pi}}} e ^ {(x- \ mu) ^ 2 \ over {2 \ sigma ^ 2}} \ \ \ (2.1.1)"></div><br>  If a <img src="https://tex.s2cms.ru/svg/%5Csigma%3D1">  and <img src="https://tex.s2cms.ru/svg/%5Cmu%3D0">  then this distribution is called standard. <br><br>  The expectation and variance of the normal distribution: <br><br><a name="2_1_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BN_%7B%5Cmu%2C%5Csigma%7D%5C%7D%3D%5Cmu%5C%20%5C%20%5C%20(2.1.2)" alt="E \ {N _ {\ mu, \ sigma} \} = \ mu \ \ \ (2.1.2)"></div><br><a name="2_1_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BN_%7B%5Cmu%2C%5Csigma%7D%5C%7D%3D%5Csigma%5E2%5C%20%5C%20%5C%20(2.1.3)" alt="D \ {N _ {\ mu, \ sigma} \} = \ sigma ^ 2 \ \ \ (2.1.3)"></div><br>  The domain of definition of a normal distribution is the set of valid numbers. <br><br>  The normal distribution is the <a href="https://habr.com/ru/post/311092/">Pearson</a> Type VI distribution. <br><br>  The sum of squares of independent normal quantities has <a href="https://habr.com/ru/post/311092/">a chi-squared distribution</a> , and the ratio of independent Gaussian quantities is distributed along <a href="https://habr.com/ru/post/311092/">Cauchy</a> . <br><br>  Normal distribution is infinitely divisible: the sum of normally distributed values <img src="https://tex.s2cms.ru/svg/x">  and <img src="https://tex.s2cms.ru/svg/y">  with parameters <img src="https://tex.s2cms.ru/svg/%5C%7B%5Cmu_x%2C%5Csigma_x%5C%7D">  and <img src="https://tex.s2cms.ru/svg/%5C%7B%5Cmu_y%2C%5Csigma_y%5C%7D">  accordingly also has a normal distribution with parameters <img src="https://tex.s2cms.ru/svg/%5C%7B%5Cmu_%7Bx%2By%7D%2C%5Csigma_%7Bx%2By%7D%5C%7D">  where <img src="https://tex.s2cms.ru/svg/%5Cmu_%7Bx%2By%7D%3D%5Cmu_x%2B%5Cmu_y">  and <img src="https://tex.s2cms.ru/svg/%5Csigma_%7Bx%2By%7D%5E2%3D%5Csigma_x%5E2%2B%5Csigma_y%5E2">  . <br><br>  The normal distribution well models the values ‚Äã‚Äãdescribing natural phenomena, the noise of a thermodynamic nature and measurement errors. <br><br>  In addition, according to the central limit theorem, the sum of a large number of independent terms of the same order converges to the normal distribution, regardless of the distributions of the terms.  Due to this property, the normal distribution is popular in statistical analysis, many statistical tests are calculated on normally distributed data. <br><br>  The z-test is based on the infinite divisibility of the normal distribution.  This test is used to test the equality of the expectation of a sample of normally distributed values ‚Äã‚Äãto a certain value.  The value of the variance must be <b>known</b> .  If the variance value is unknown and is calculated on the basis of the analyzed sample, then a t-test is used based on <a href="https://habr.com/ru/post/311092/">the student's distribution</a> . <br><br>  Suppose we have a sample of n independent normally distributed quantities <img src="https://tex.s2cms.ru/svg/X_i">  out of population with standard deviation <img src="https://tex.s2cms.ru/svg/%5Csigma">  we hypothesize that <img src="https://tex.s2cms.ru/svg/%5Cbar%7BX%7D%3D%5Cmu">  .  Then the magnitude <img src="https://tex.s2cms.ru/svg/z%3D%7B%5Cbar%7BX%7D-%5Cmu%5Cover%7B%5Csigma%20%5Csqrt%7Bn%7D%7D%7D" alt="z = {\ bar {X} - \ mu \ over {\ sigma \ sqrt {n}}}">  will have a standard normal distribution.  By comparing the obtained z value with the quantiles of the standard distribution, one can accept or reject the hypothesis with the required level of significance. <br><br>  Due to the widespread distribution of the Gauss, many researchers who do not know the statistics very well forget to check the data for normality, or estimate the distribution density graph ‚Äúby the eye‚Äù, blindly thinking that they are dealing with Gaussian data.  Accordingly, boldly applying tests designed for normal distribution and getting completely incorrect results.  Probably, from here the rumor about statistics as the most terrible kind of lie went. <br><br>  Consider an example: we need to measure the resistance of a set of resistors of a certain value.  Resistance is of a physical nature, it is logical to assume that the distribution of resistance deviations from the nominal will be normal.  Measured, we obtain a bell-shaped probability density function for measured values ‚Äã‚Äãwith a mode in the vicinity of the resistors nominal value.  Is this a normal distribution?  If yes, then we will search for defective resistors using <a href="https://habr.com/ru/post/311092/">Student's test</a> , or z-test, if we know the distribution variance in advance.  I think that many will do just that. <br><br>  But let's take a closer look at the resistance measurement technology: resistance is defined as the ratio of the applied voltage to the flowing current.  We measured current and voltage with instruments, which, in turn, have normally distributed errors.  That is, the measured values ‚Äã‚Äãof current and voltage are <b>normally distributed random variables</b> with expected values ‚Äã‚Äãcorresponding to the true values ‚Äã‚Äãof the measured values.  This means that the obtained resistance values ‚Äã‚Äãare distributed in <a href="https://habr.com/ru/post/311092/">Cauchy</a> , and not in Gauss. <br><br>  The <a href="https://habr.com/ru/post/311092/">Cauchy</a> distribution only resembles a seemingly normal distribution, but has heavier tails.  So the proposed tests are inappropriate.  We need to build a test based on <a href="https://habr.com/ru/post/311092/">the Cauchy distribution</a> or calculate the square of resistance, which in this case will have <a href="https://habr.com/ru/post/311092/">a Fisher distribution</a> with parameters (1, 1). <br><br>  <a href="https://habr.com/ru/post/311092/">to the scheme</a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="hi"></a><br><h3>  Chi-square distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c1c/beb/a86/c1cbeba867051ee5e319d4a7b0f07088.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2585%25D0%25B8-%25D0%25BA%25D0%25B2%25D0%25B0%25D0%25B4%25D1%2580%25D0%25B0%25D1%2582">from here</a> ) <br><br>  Distribution <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  describes the amount <img src="https://tex.s2cms.ru/svg/n">  squares of random variables <img src="https://tex.s2cms.ru/svg/X_i">  each of which is distributed according to standard normal law <img src="https://tex.s2cms.ru/svg/N_%7B0%2C1%7D">  : <br><a name="2_2_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cchi%5E2_n(x)%3D%7B%7B%5Cleft(1%5Cover%202%20%5Cright)%7D%5E%7Bk%5Cover%202%7D%5Cover%7B%5CGamma%5Cleft(%7Bk%5Cover%7B2%7D%7D%5Cright)%7D%7Dx%5E%7B%7Bk%5Cover%202%7D-1%7De%5E%7B-%7Bx%5Cover%202%7D%7D%5C%20%5C%20%5C%20(2.2.1)" alt="\ chi ^ 2_n (x) = {{\ left (1 \ over 2 \ right)} ^ {k \ over 2} \ over {\ Gamma \ left ({k \ over {2}} \ right)}} x ^ {{k \ over 2} -1} e ^ {- {x \ over 2}} \ \ \ (2.2.1)"></div><br><br>  Where <img src="https://tex.s2cms.ru/svg/n">  - the number of degrees of freedom <img src="https://tex.s2cms.ru/svg/x%3D%5Csum%5Climits_%7Bi%3D1%7D%5En%20%7BX%5E2_i%7D" alt="x = \ sum \ limits_ {i = 1} ^ n {X ^ 2_i}">  . <br><br>  Distribution expectation and variance <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  : <br><br><a name="2_2_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7B%5Cchi%5E2_n%5C%7D%3Dn%5C%20%5C%20%5C%20(2.2.2)" alt="E \ {\ chi ^ 2_n \} = n \ \ \ (2.2.2)"></div><br><a name="2_2_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7B%5Cchi%5E2_n%5C%7D%3D2n%5C%20%5C%20%5C%20(2.2.3)" alt="D \ {\ chi ^ 2_n \} = 2n \ \ \ (2.2.3)"></div><br>  The domain is the set of non-negative natural numbers. <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  is an infinitely divisible distribution.  If a <img src="https://tex.s2cms.ru/svg/x">  and <img src="https://tex.s2cms.ru/svg/y">  - distributed over <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  and have <img src="https://tex.s2cms.ru/svg/n_x">  and <img src="https://tex.s2cms.ru/svg/n_y">  degrees of freedom respectively, their sum will also be distributed over <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  and have <img src="https://tex.s2cms.ru/svg/n_x%2Bn_y">  degrees of freedom. <br><br><img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  is a special case of the <a href="https://habr.com/ru/post/311092/">gamma distribution</a> (and therefore, the <a href="https://habr.com/ru/post/311092/">Pearson</a> type III distribution) and a generalization of the <a href="https://habr.com/ru/post/311092/">exponential distribution</a> .  The ratio of values ‚Äã‚Äãdistributed over <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  distributed by <a href="https://habr.com/ru/post/311092/">Fisher</a> . <br><br>  On distribution <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  based on Pearson's acceptance criterion.  Using this criterion, one can verify the reliability of a sample of a random variable to a certain theoretical distribution. <br><br>  Suppose we have a sample of some random variable <img src="https://tex.s2cms.ru/svg/X_i">  .  Based on this sample, we calculate the probabilities <img src="https://tex.s2cms.ru/svg/P_k">  hitting values <img src="https://tex.s2cms.ru/svg/X">  at <img src="https://tex.s2cms.ru/svg/n">  intervals ( <img src="https://tex.s2cms.ru/svg/k%3D1%3An">  ).  Let also there is an assumption about the analytical expression of the distribution, in accordance with which, the probabilities of hitting the selected intervals should be <img src="https://tex.s2cms.ru/svg/Q_k">  .  Then the magnitudes <img src="https://tex.s2cms.ru/svg/D_k%3DP_k-Q_k">  will be distributed according to normal law. <br><br>  Will give <img src="https://tex.s2cms.ru/svg/D_k">  to standard normal distribution: <img src="https://tex.s2cms.ru/svg/d_k%3D%7BD_k-m%5Cover%20S%7D">  , <br>  Where <img src="https://tex.s2cms.ru/svg/m%3D%7B1%5Cover%20n%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5En%20%7BD_i%7D" alt="m = {1 \ over n} \ sum \ limits_ {i = 1} ^ n {D_i}">  and <img src="https://tex.s2cms.ru/svg/S%3D%5Csqrt%20%7B%7B1%5Cover%20%7Bn-1%7D%7D%5Csum%20%5Climits_%7Bi%3D1%7D%5En%20%7BD_i%5E2%7D%7D" alt="S = \ sqrt {{1 \ over {n-1}} \ sum \ limits_ {i = 1} ^ n {D_i ^ 2}}">  . <br><br>  Derived values <img src="https://tex.s2cms.ru/svg/d_i">  have a normal distribution with parameters (0, 1), and therefore, the sum of their squares is distributed over <img src="https://tex.s2cms.ru/svg/%5Cchi%5E2">  with <img src="https://tex.s2cms.ru/svg/n-1">  degree of freedom.  The decrease in the degree of freedom is associated with an additional restriction on the sum of the probabilities of values ‚Äã‚Äãfalling into the intervals: it must be equal to 1. <br><br>  Comparing the value <img src="https://tex.s2cms.ru/svg/z%3D%5Csum%20%5Climits_%7Bi%3D1%7D%5En%20d_i%5E2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with quantiles of distribution, </font></font><img src="https://tex.s2cms.ru/svg/%5Cchi%5E2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">you can accept or reject the hypothesis about the theoretical distribution of data with the required level of significance. </font></font><br><br> <a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to the scheme</font></font></a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Student"></a><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Student's t-distribution (t-distribution) </font></font></h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c09/6f4/112/c096f411273a24f2eb53baeb27d77687.jpg"></div><br> ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A1%25D1%2582%25D1%258C%25D1%258E%25D0%25B4%25D0%25B5%25D0%25BD%25D1%2582%25D0%25B0"></a> ) <br><br>      t-:      <a href="https://habr.com/ru/post/311092/"> </a>     ,     <a href="https://habr.com/ru/post/311092/"></a>     (    <a href="https://habr.com/ru/post/311092/">f-</a> ).     <a href="https://habr.com/ru/post/311092/"></a>     ,   <a href="https://habr.com/ru/post/311092/">-</a> . <br><br> T-   <a href="https://habr.com/ru/post/311092/">z-</a>  ,               . <br><br>      <a href="https://habr.com/ru/post/311092/"></a>   :     <img src="https://tex.s2cms.ru/svg/X_i"> <a href="https://habr.com/ru/post/311092/"></a>   n    ,      ,      <img src="https://tex.s2cms.ru/svg/m">  . <br><br>   <img src="https://tex.s2cms.ru/svg/S%3D%7B%5Csum%20%5Climited_%7Bi%3D1%7D%5EnX_i%5E2%20%5Cover%20%7Bn-1%7D%7D">  .      <a href="https://habr.com/ru/post/311092/">-</a> .   <img src="https://tex.s2cms.ru/svg/t%3D%7B%7B%5Csum%20%5Climited_%7Bi%3D1%7D%5EnX_i-m%20%5Cover%20%7Bn-1%7D%7D%5Cover%20%7BS%20%5Cover%20%7B%5Csqrt%20n%7D%7D%20%7D">     <img src="https://tex.s2cms.ru/svg/T_%7Bn-1%7D(x)">  c <img src="https://tex.s2cms.ru/svg/n-1">  , : <br><a name="2_3_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/T_%7Bn%7D(x)%3D%7B%5CGamma%20%5Cleft(%7Bn%2B1%20%5Cover%202%7D%5Cright)%20%5Cover%20%7B%5Csqrt%7Bn%20%5Cpi%7D%5CGamma%20%5Cleft(%7Bn%20%5Cover%202%7D%5Cright)%5Cleft(1%2B%7Bx%5E2%20%5Cover%20n%7D%5Cright)%5E%7Bn%2B1%20%5Cover%202%7D%7D%7D%5C%20%5C%20%5C%20(2.3.1)" alt="T_{n}(x)={\Gamma \left({n+1 \over 2}\right) \over {\sqrt{n \pi}\Gamma \left({n \over 2}\right)\left(1+{x^2 \over n}\right)^{n+1 \over 2}}}\ \ \ (2.3.1)"></div><br><br>  Where <img src="https://tex.s2cms.ru/svg/%5CGamma(x)"> ‚Äî - . <br><br>                  <img src="https://tex.s2cms.ru/svg/m">    . <br><br>     : <br><br><a name="2_3_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BT_%7Bn%7D%5C%7D%3D0%5C%20%5C%20%5C%20(2.3.2)" alt="E\{T_{n}\}=0\ \ \ (2.3.2)"></div><br><a name="2_3_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BT_%7Bn%7D%5C%7D%3D%7Bn%20%5Cover%20%7Bn-2%7D%7D%5C%20%5C%20%5C%20(2.3.3)" alt="D\{T_{n}\}={n \over {n-2}}\ \ \ (2.3.3)"></div><br>  at <img src="https://tex.s2cms.ru/svg/n%3E2">  . <br><br> <a href="https://habr.com/ru/post/311092/"> </a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Fisher"></a><br><h3>   </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b23/935/969/b239359694ead445a17f74da63c7829d.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(taken </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A4%25D0%25B8%25D1%2588%25D0%25B5%25D1%2580%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> )</font></font><br><br>  Let be <img src="https://tex.s2cms.ru/svg/X">  and <img src="https://tex.s2cms.ru/svg/Y"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">independent random variables with </font></font><a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a chi-square distribution</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> with degrees of freedom</font></font><img src="https://tex.s2cms.ru/svg/n_x">  and <img src="https://tex.s2cms.ru/svg/n_y">  respectively.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then the magnitude </font></font><img src="https://tex.s2cms.ru/svg/F%3D%7Bn_yX%20%5Cover%7Bn_xY%7D%7D"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">will have a Fisher distribution with degrees of freedom </font></font><img src="https://tex.s2cms.ru/svg/(n_x%2Cn_y)"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and the </font><font style="vertical-align: inherit;">magnitude </font><font style="vertical-align: inherit;">will have a Fisher </font></font><img src="https://tex.s2cms.ru/svg/F%5E%7B-1%7D"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">distribution with degrees of freedom</font></font><img src="https://tex.s2cms.ru/svg/(n_y%2Cn_x)">  . <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fisher distribution is defined for valid non-negative arguments and has a probability density: </font></font><br><a name="2_4_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/F_%7Bn_1%2Cn_2%7D(x)%3D%7B%5Csqrt%7B%20(n_1x)%5E%7Bn_1%7Dn_2%5E%7Bn_2%7D%5Cover%20%7B(n_1x%2Bn_2)%5E%7Bn_1%2Bn_2%7D%7D%7D%20%5Cover%20%7BxB%5Cleft(%7Bn_1%20%5Cover%202%7D%2C%7Bn_2%20%5Cover%202%7D%20%5Cright)%7D%7D%5C%20%5C%20%5C%20(2.4.1)" alt="F_{n_1,n_2}(x)={\sqrt{ (n_1x)^{n_1}n_2^{n_2}\over {(n_1x+n_2)^{n_1+n_2}}} \over {xB\left({n_1 \over 2},{n_2 \over 2} \right)}}\ \ \ (2.4.1)"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The expectation and variance of the Fisher distribution: </font></font><br><br><a name="2_4_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BF_%7Bn_1%2Cn_2%7D%5C%7D%3D%7Bn_2%20%5Cover%20%7Bn_2-2%7D%7D%5C%20%5C%20%5C%20(2.4.2)" alt="E\{F_{n_1,n_2}\}={n_2 \over {n_2-2}}\ \ \ (2.4.2)"></div><br><a name="2_4_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BF_%7Bn_1%2Cn_2%7D%5C%7D%3D%7B2n_2%5E2(n_1%2Bn_2-2)%20%5Cover%20%7Bn_1(n_2-2)%5E2(n_2-4)%7D%7D%5C%20%5C%20%5C%20(2.4.3)" alt="D\{F_{n_1,n_2}\}={2n_2^2(n_1+n_2-2) \over {n_1(n_2-2)^2(n_2-4)}}\ \ \ (2.4.3)"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The expectation is defined for </font></font><img src="https://tex.s2cms.ru/svg/n_2%3E2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, and the variance is for</font></font><img src="https://tex.s2cms.ru/svg/n_2%3E4">  . <br><br>       ,      ,         <a href="https://habr.com/ru/post/311092/"></a>  (f-,    <b></b>  ). <br><br> F-:      <img src="https://tex.s2cms.ru/svg/x_i">  and <img src="https://tex.s2cms.ru/svg/y_j"> <a href="https://habr.com/ru/post/311092/"></a>    <img src="https://tex.s2cms.ru/svg/n_x">  and <img src="https://tex.s2cms.ru/svg/n_y">  respectively.          . <br><br>   <img src="https://tex.s2cms.ru/svg/F%3D%7Bn_y%20%5Csum%20%5Climited_%7Bi%3D1%7D%5E%7Bn_x%7D%20%7Bx_i%5E2%7D%20%5Cover%20n_x%20%5Csum%20%5Climited_%7Bj%3D1%7D%5E%7Bn_y%7D%20%7By_j%5E2%7D%7D">  .         <img src="https://tex.s2cms.ru/svg/(n_x-1%2Cn_y-1)">  . <br><br>   <img src="https://tex.s2cms.ru/svg/F">     ,              . <br><br> <a href="https://habr.com/ru/post/311092/"> </a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Koshi"></a><br><h3>   </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c12/8df/a7a/c128dfa7ac0e21e19d8103485b3b4fc7.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(taken </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259A%25D0%25BE%25D1%2588%25D0%25B8"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Cauchy distribution describes the ratio of two </font></font><a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">normally</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> distributed random variables. </font><font style="vertical-align: inherit;">Unlike other distributions, the expectation and variance are not defined for the Cauchy distribution. </font><font style="vertical-align: inherit;">The shift </font></font><img src="https://tex.s2cms.ru/svg/x_0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and scale </font><font style="vertical-align: inherit;">factors are used to describe the distribution.</font></font><img src="https://tex.s2cms.ru/svg/%5Cgamma">  . <br><a name="2_5_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/C_%7Bx_0%2C%5Cgamma%7D(x)%3D%7B1%5Cover%7B%5Cpi%20%5Cgamma%20%5Cleft(1%2B%5Cleft(%7Bx-x_0%5Cover%20%7B%5Cgamma%7D%7D%20%5Cright)%5E2%20%5Cright)%7D%7D%5C%20%5C%20%5C%20(2.5.1)" alt="C_{x_0,\gamma}(x)={1\over{\pi \gamma \left(1+\left({x-x_0\over {\gamma}} \right)^2 \right)}}\ \ \ (2.5.1)"></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Cauchy distribution is infinitely divisible: the sum of independent random variables distributed over Cauchy is also distributed along Cauchy. </font></font><br><br> <a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to the scheme</font></font></a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="exp"></a><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Exponential (exponential) distribution and Laplace distribution (double exponential, double exponential) </font></font></h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3bc/389/a98/3bc389a9803912e0105de334fafb1c25.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(taken </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25AD%25D0%25BA%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BD%25D1%2586%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) The </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">exponential distribution describes the time intervals between independent events occurring with an average intensity</font></font><img src="https://tex.s2cms.ru/svg/%5Clambda">  .           <a href="https://habr.com/ru/post/311092/"> </a> .     <a href="https://habr.com/ru/post/311092/"> </a>     . <br><br>   ,       ,  ,    ,    ‚Äî ,     . <br><br>      <a href="https://habr.com/ru/post/311092/"> -</a> ( n=2),  ,  <a href="https://habr.com/ru/post/311092/">-</a> . -      -  2-  ,             . <br><br>  ,      <a href="https://habr.com/ru/post/311092/"> </a> . <br><br>     ‚Äî  <a href="https://habr.com/ru/post/311092/"> </a> . <br><br>    : <br><a name="2_6_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E_%5Clambda(x)%3D%5Clambda%20e%5E%7B-%5Clambda%20x%7D%5C%20%5C%20%5C%20(2.6.1)" alt="E_\lambda(x)=\lambda e^{-\lambda x}\ \ \ (2.6.1)"></div><br>      <img src="https://tex.s2cms.ru/svg/x">  . <br><br>     : <br><br><a name="2_6_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BE_%5Clambda%5C%7D%3D%7B1%20%5Cover%20%5Clambda%7D%20%5C%20%5C%20%5C%20(2.6.2)" alt="E\{E_\lambda\}={1 \over \lambda} \ \ \ (2.6.2)"></div><br><a name="2_6_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BE_%5Clambda%5C%7D%3D%7B1%20%5Cover%20%5Clambda%5E2%7D%20%5C%20%5C%20%5C%20(2.6.3)" alt="E\{E_\lambda\}={1 \over \lambda^2} \ \ \ (2.6.3)"></div><br><a name="Laplas"></a>            ,  ,  <img src="https://tex.s2cms.ru/svg/X">  on <img src="https://tex.s2cms.ru/svg/%5Cleft%7Cx%5Crignt%7C"> ,    ,       . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/928/4bf/380/9284bf380bba750c7c322d469073d7a7.png"></div><br> ( <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%259B%25D0%25B0%25D0%25BF%25D0%25BB%25D0%25B0%25D1%2581%25D0%25B0"></a> ) <br><br>   ,   ,   ¬´¬ª        .    ,  ,      . <br><a name="2_6_4"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/L_%7B%5Calpha%2C%5Cbeta%7D(x)%3D%7B%5Calpha%20%5Cover%202%7De%5E%7B-%5Calpha%20%5Cleft%7Cx-%5Cbeta%5Cright%7C%7D%20%5C%20%5C%20%5C%20(2.6.4)" alt="L_{\alpha,\beta}(x)={\alpha \over 2}e^{-\alpha \left|x-\beta\right|} \ \ \ (2.6.4)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/%5Calpha"> ‚Äî  ,  <img src="https://tex.s2cms.ru/svg/%5Cbeta"> ‚Äî  . <br><br>   : <br><a name="2_6_5"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BL_%7B%5Calpha%2C%20%5Cbeta%7D%5C%7D%3D%5Cbeta%5C%20%5C%20%5C%20(2.6.5)" alt="E\{L_{\alpha, \beta}\}=\beta\ \ \ (2.6.5)"></div><br><a name="2_6_6"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BL_%7B%5Calpha%2C%20%5Cbeta%7D%5C%7D%3D%7B2%20%5Cover%20%7B%5Calpha%5E2%7D%7D%5C%20%5C%20%5C%20(2.6.6)" alt="D\{L_{\alpha, \beta}\}={2 \over {\alpha^2}}\ \ \ (2.6.6)"></div><br>    ,   <a href="https://habr.com/ru/post/311092/"> </a> ,           ,      , ,  ,   .. <br><br> <a href="https://habr.com/ru/post/311092/"> </a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="Veibull"></a><br><h3>   </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/01c/26e/adc/01c26eadc10e502dfc10a73059f01f38.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(taken </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2592%25D0%25B5%25D0%25B9%25D0%25B1%25D1%2583%25D0%25BB%25D0%25BB%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">from here</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ) </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Weibull distribution is described by a probability density function of the following form:</font></font><br><a name="2_7_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/W_%7Bk%2C%20%5Clambda%7D(x)%3D%7Bk%20%5Cover%20%7B%5Clambda%7D%7D%5Cleft(%7Bx%20%5Cover%20%7B%5Clambda%7D%7D%5Cright)%5E%7Bk-1%7De%5E%7B-%5Cleft(%7Bx%20%5Cover%20%7B%5Clambda%7D%7D%5Cright)%5Ek%7D%5C%20%5C%20%5C%20(2.7.1)" alt="W_{k, \lambda}(x)={k \over {\lambda}}\left({x \over {\lambda}}\right)^{k-1}e^{-\left({x \over {\lambda}}\right)^k}\ \ \ (2.7.1)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/%5Clambda">  ( <img src="https://tex.s2cms.ru/svg/%5Clambda"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt; 0) is the intensity of events (similar to the </font></font><a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">exponential distribution</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> parameter </font><font style="vertical-align: inherit;">), and </font></font><img src="https://tex.s2cms.ru/svg/k"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is the nonstationarity index (</font></font><img src="https://tex.s2cms.ru/svg/k%3E0">  ).  With <img src="https://tex.s2cms.ru/svg/k%3D1"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Weibull distribution degenerates into an </font></font><a href="https://habr.com/ru/post/311092/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">exponential distribution</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and in other cases describes the flow of independent events with non-stationary intensity.</font></font> With <img src="https://tex.s2cms.ru/svg/k%3E1">        ,   <img src="https://tex.s2cms.ru/svg/k%3C1"> ‚Äî  .      :   . <br><br>  ,   ‚Äî  <a href="https://habr.com/ru/post/311092/"> </a>     .    ,    ,   ,      .. <br><br>     : <br><br><a name="2_7_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BW_%7Bk%2C%20%5Clambda%7D%5C%7D%3D%5Clambda%20%5CGamma%5Cleft(1%20%2B%20%7B1%20%5Cover%20k%7D%20%5Cright)%5C%20%5C%20%5C%20(2.7.2)" alt="E\{W_{k, \lambda}\}=\lambda \Gamma\left(1 + {1 \over k} \right)\ \ \ (2.7.2)"></div><br><a name="2_7_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BW_%7Bk%2C%20%5Clambda%7D%5C%7D%3D%5Clambda%5E2%20%5Cleft(%5CGamma%5Cleft(1%20%2B%20%7B2%20%5Cover%20k%7D%20%5Cright%20)%20-%20%5CGamma%5Cleft(1%20%2B%20%7B1%20%5Cover%20k%7D%20%5Cright%20)%5E2%5Cright)%5C%20%5C%20%5C%20(2.7.3)" alt="D\{W_{k, \lambda}\}=\lambda^2 \left(\Gamma\left(1 + {2 \over k} \right ) - \Gamma\left(1 + {1 \over k} \right )^2\right)\ \ \ (2.7.3)"></div><br>  Where <img src="https://tex.s2cms.ru/svg/%5CGamma(x)"> ‚Äî - . <br><br> <a href="https://habr.com/ru/post/311092/"> </a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="gamma"></a><br><h3> - ( ) </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/974/578/f28/974578f288beb6dafd2046cb64aaaa2e.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0-%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  The gamma distribution is a generalization of the <a href="https://habr.com/ru/post/311092/">chi-squared distribution</a> and, accordingly, the <a href="https://habr.com/ru/post/311092/">exponential distribution</a> .  The sums of squares of <a href="https://habr.com/ru/post/311092/">normally distributed quantities</a> , as well as the sums of quantities distributed over <a href="https://habr.com/ru/post/311092/">chi-square</a> and <a href="https://habr.com/ru/post/311092/">exponentially distributed,</a> will have a gamma distribution. <br><br>  The gamma distribution is <a href="https://habr.com/ru/post/311092/">a Pearson Type</a> III <a href="https://habr.com/ru/post/311092/">distribution</a> .  The domain of the gamma distribution is natural non-negative numbers. <br><br>  Gamma distribution is determined by two non-negative parameters. <img src="https://tex.s2cms.ru/svg/k">  - the number of degrees of freedom (for the whole value of degrees of freedom, the gamma distribution is called the Erlang distribution) and the scale factor <img src="https://tex.s2cms.ru/svg/%5Ctheta">  . <br><br>  Gamma distribution is infinitely divisible: if magnitudes <img src="https://tex.s2cms.ru/svg/X">  and <img src="https://tex.s2cms.ru/svg/Y">  have distributions <img src="https://tex.s2cms.ru/svg/G_%7Bkx%2C%5Ctheta%7D">  and <img src="https://tex.s2cms.ru/svg/G_%7Bky%2C%5Ctheta%7D">  accordingly, then <img src="https://tex.s2cms.ru/svg/X%2BY">  will have distribution <img src="https://tex.s2cms.ru/svg/G_%7Bkx%2Bky%2C%5Ctheta%7D"><br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/G_%7Bk%2C%5Ctheta%7D(x)%3Dx%5E%7Bk-1%7D%7Be%5E%7B-%7Bx%20%5Cover%20%5Ctheta%7D%7D%5Cover%20%5CGamma(k)%20%5Ctheta%5Ek%7D%5C%20%5C%20%5C%20(2.8.1)" alt="G_ {k, \ theta} (x) = x ^ {k-1} {e ^ {- {x \ over \ theta}} \ over \ Gamma (k) \ theta ^ k} \ \ \ (2.8.1 )"></div><br>  Where <img src="https://tex.s2cms.ru/svg/%5CGamma(x)">  - Euler gamma function. <br><br>  Expectation and variance: <br><br><a name="2_8_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BG_%7Bk%2C%5Ctheta%7D%5C%7D%3Dk%5Ctheta%5C%20%5C%20%5C%20(2.8.2)" alt="E \ {G_ {k, \ theta} \} = k \ theta \ \ \ (2.8.2)"></div><br><a name="2_8_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BG_%7Bk%2C%5Ctheta%7D%5C%7D%3Dk%5Ctheta%5E2%5C%20%5C%20%5C%20(2.8.3)" alt="D \ {G_ {k, \ theta} \} = k \ theta ^ 2 \ \ \ (2.8.3)"></div><br>  Gamma distribution is widely used to model complex flows of events, the sums of time intervals between events, in economics, queuing theory, in logistics, describes the life expectancy in medicine.  It is a kind of analogue of the discrete <a href="https://habr.com/ru/post/311092/">negative binomial distribution</a> . <br><br>  <a href="https://habr.com/ru/post/311092/">to the scheme</a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><a name="beta"></a><br><h3>  Beta distribution </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/28c/1e1/7c4/28c1e17c4197c1f7db766fc9fe905359.png"></div><br>  (taken <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B5%25D1%2582%25D0%25B0-%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">from here</a> ) <br><br>  The beta distribution describes the fraction of the sum of two terms that falls on each of them, if the terms are random variables that have a <a href="https://habr.com/ru/post/311092/">gamma distribution</a> .  That is, if the quantities <img src="https://tex.s2cms.ru/svg/X_1">  and <img src="https://tex.s2cms.ru/svg/X_2">  have a gamma distribution <img src="https://tex.s2cms.ru/svg/%7BX_1%20%5Cover%20%7BX_1%2BX_2%7D%7D">  and <img src="https://tex.s2cms.ru/svg/%7BX_2%20%5Cover%20%7BX_1%2BX_2%7D%7D">  will have a beta distribution. <br><br>  Obviously, the domain of the beta distribution <img src="https://tex.s2cms.ru/svg/%5B0%2C%201%5D">  .  The beta distribution is the <a href="https://habr.com/ru/post/311092/">Pearson</a> Type I distribution. <br><a name="2_9_1"></a><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/B_%7B%5Calpha%2C%5Cbeta%7D%3D%7Bx%5E%7B%5Calpha%20-1%7D(1-x)%5E%7B%5Cbeta%20-%201%7D%5Cover%7BB(%5Calpha%2C%20%5Cbeta)%7D%7D%5C%20%5C%20%5C%20(2.9.1)" alt="B _ {\ alpha, \ beta} = {x ^ {\ alpha -1} (1-x) ^ {\ beta - 1} \ over {B (\ alpha, \ beta)}} \ \ \ (2.9.1 )"></div><br>  where are the parameters <img src="https://tex.s2cms.ru/svg/%5Calpha">  and <img src="https://tex.s2cms.ru/svg/%5Cbeta">  - positive natural numbers, <img src="https://tex.s2cms.ru/svg/B(x%2C%20y)">  - Euler beta function. <br><br>  Expectation and variance: <br><br><a name="2_9_2"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/E%5C%7BB_%7B%5Calpha%2C%20%5Cbeta%7D%5C%7D%3D%7B%5Calpha%20%5Cover%7B%5Calpha%2B%5Cbeta%7D%7D%5C%20%5C%20%5C%20(2.9.2)" alt="E \ {B _ {\ alpha, \ beta} \} = {\ alpha \ over {\ alpha + \ beta}} \ \ \ (2.9.2)"></div><br><a name="2_9_3"></a><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/D%5C%7BB_%7B%5Calpha%2C%20%5Cbeta%7D%5C%7D%3D%7B%5Calpha%20%5Cbeta%20%5Cover%7B(%5Calpha%2B%5Cbeta)%5E2(%5Calpha%2B%5Cbeta%2B1)%7D%7D%5C%20%5C%20%5C%20(2.9.3)" alt="D \ {B _ {\ alpha, \ beta} \} = {\ alpha \ beta \ over {(\ alpha + \ beta) ^ 2 (\ alpha + \ beta + 1)}} \ \ \ (2.9.3)"></div><br>  <a href="https://habr.com/ru/post/311092/">to the scheme</a> <br>  <a href="https://habr.com/ru/post/311092/">upstairs</a> <br><br><h1>  Instead of conclusion </h1><br>  We reviewed 15 probability distributions, which, in my opinion, cover most of the most popular statistical applications. <br><a name="zadanie"></a><br>  Finally, a small homework: to assess the reliability of exchange trading systems, use such an indicator as the profit factor.  Profit factor is calculated as the ratio of total income to total loss.  Obviously, for a revenue-generating system, the profit factor is greater than one, and the higher its value, the more reliable the system. <br><br>  <strong>Question:</strong> what is the distribution of the profit factor? <br><br>  I will present my thoughts on this in the <a href="https://habrahabr.ru/post/312096/">next</a> article. <br><br>  PS If you want to refer to the numbered formulas from this article, you can use the following link: <i>link_on_statyu</i> #x_y_z, where (xyz) is the number of the formula to which you refer. </div><p>Source: <a href="https://habr.com/ru/post/311092/">https://habr.com/ru/post/311092/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../311078/index.html">Development in InterSystems Cach√© in your favorite IDE</a></li>
<li><a href="../311084/index.html">Taming asynchronous processes in Android with RxJava. Yandex experience</a></li>
<li><a href="../311086/index.html">NanoFL: Brief Feature Description</a></li>
<li><a href="../311088/index.html">Creating a blog engine with Phoenix and Elixir / Part 1. Introduction</a></li>
<li><a href="../311090/index.html">MapReduce in Qt Concurrent</a></li>
<li><a href="../311094/index.html">Yield: what, where and why</a></li>
<li><a href="../311096/index.html">Javascript frameworks: there should be only one</a></li>
<li><a href="../311098/index.html">Logeek Night in Voronezh</a></li>
<li><a href="../311100/index.html">Several gradle chips for your Android application</a></li>
<li><a href="../311102/index.html">Intelligence services and not only: how to protect your application from backdoors</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>