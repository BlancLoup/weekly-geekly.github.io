<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Emotion recognition using a convolutional neural network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recognizing emotions has always been an exciting challenge for scientists. Recently, I have been working on an experimental SER project (Speech Emotio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Emotion recognition using a convolutional neural network</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/4f/gh/qr/4fghqrzh79wxguvk28uxvxg1ice.png"></div><br>  Recognizing emotions has always been an exciting challenge for scientists.  Recently, I have been working on an experimental SER project (Speech Emotion Recognition) to understand the potential of this technology - for this I selected the most popular repositories on <a href="https://github.com/MITESHPUTHRANNEU/Speech-Emotion-Analyzer%3Fsource%3Dpost_page---------------------------">Github</a> and made them the basis of my project. <br><br>  Before we begin to understand the project, it will be nice to remember what kind of bottlenecks SER has. <br><a name="habracut"></a><br><h2>  Main obstacles </h2><br><ul><li>  emotions are subjective, even people interpret them differently.  It is difficult to define the very concept of ‚Äúemotion‚Äù; </li><li>  commenting on audio is hard.  Should we somehow mark every single word, sentence, or the entire communication as a whole?  A set of what kind of emotions to use in recognition? </li><li>  collecting data is also not easy.  A lot of audio data can be collected from movies and news.  However, both sources are ‚Äúbiased‚Äù because the news must be neutral, and the emotions of the actors are played.  It is difficult to find an ‚Äúobjective‚Äù source of audio data. </li><li>  markup data requires large human and time resources.  Unlike drawing frames on images, it requires specially trained personnel to listen to entire audio recordings, analyze them and provide comments.  And then these comments must be appreciated by <b>many</b> other people, because the ratings are subjective. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/6a/kn/wc/6aknwc9ko-dj-nzw2dmlvfb31ry.png"></div><br><h2>  Project Description </h2><br>  Using a convolutional neural network to recognize emotions in audio recordings.  And yes, the owner of the repository did not refer to any sources. <br><br><h2>  Data Description </h2><br>  There are two datasets that were used in the RAVDESS and SAVEE repositories, I just adapted RAVDESS in my model.  There are two types of data in the RAVDESS context: speech and song. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Dataset <a href="https://zenodo.org/record/1188976%3Fsource%3Dpost_page---------------------------">RAVDESS (The Ryerson Audio-Visual Database of Emotional Speech and Song)</a> : <br><br><ul><li>  12 actors and 12 actresses recorded their speech and songs in their performance; </li><li>  actor # 18 has no recorded songs; </li><li>  emotions Disgust (aversion), Neutral (neutral) and Surprises (surprise) are absent in the "song" data. </li></ul><br>  Emotion Breakdown: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ed/c1/zk/edc1zkhvwub39whbvy-v61kt9vk.png"></div><br>  Emotion Distribution Chart: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gq/un/1a/gqun1a1oud8gnesmrvkt6jtnwmu.png"></div><br><h3>  Feature extraction </h3><br>  When we work with speech recognition tasks, <a href="https://en.wikipedia.org/wiki/Mel-frequency_cepstrum">the cepstral coefficient (MFCCs)</a> is an advanced technology, despite the fact that it appeared in the 80s. <br><br>  Quote from the <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/%3Fsource%3Dpost_page---------------------------">MFCC Tutorial</a> : <br><blockquote>  This shape determines what the output sound is.  If we can pinpoint the form, it will give us an accurate representation of the <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BC%25D0%25B0">phoneme</a> sounded.  The shape of the <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D1%2587%25D0%25B5%25D0%25B2%25D0%25BE%25D0%25B9_%25D1%2582%25D1%2580%25D0%25B0%25D0%25BA%25D1%2582">vocal tract</a> manifests itself in an envelope of a short spectrum, and the MFCC works to accurately display this envelope. </blockquote><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nv/id/_7/nvid_7_cvd_qg9puppfec9riswk.png"></div><br>  <font color="grey">Waveform</font> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wz/5d/rl/wz5drlsibxxjtuu4azisa7grico.png"></div><br>  <font color="grey">Spectrogram</font> <br><br>  We use MFCC as an input feature.  If you are interested in learning more about what MFCC is, then this <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/%3Fsource%3Dpost_page---------------------------">tutorial</a> is for you.  Downloading data and converting it to the MFCC format can be easily done using the librosa Python package. <br><br><h3>  Default Model Architecture </h3><br>  The author developed a CNN model using the Keras package, creating 7 layers - six Con1D layers and one density layer (Dense). <br><br><pre><code class="python hljs">model = Sequential() model.add(Conv1D(<span class="hljs-number"><span class="hljs-number">256</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>,padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">216</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>))) <span class="hljs-comment"><span class="hljs-comment">#1 model.add(Activation('relu')) model.add(Conv1D(128, 5,padding='same')) #2 model.add(Activation('relu')) model.add(Dropout(0.1)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 5,padding='same')) #3 model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #4 #model.add(Activation('relu')) #model.add(Conv1D(128, 5,padding='same')) #5 #model.add(Activation('relu')) #model.add(Dropout(0.2)) model.add(Conv1D(128, 5,padding='same')) #6 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(10)) #7 model.add(Activation('softmax')) opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)</span></span></code> </pre> <br><blockquote>  The author commented on layers 4 and 5 in the latest release (September 18, 2018) and the final file size of this model does not fit the network provided, so I can not achieve the same result in accuracy - 72%. </blockquote><br>  The model is simply trained with the parameters <code>batch_size=16</code> and <code>epochs=700</code> , without any training schedule, etc. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Compile Model model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) # Fit Model cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))</span></span></code> </pre> <br>  Here <code>categorical_crossentropy</code> is a function of losses, and the measure of evaluation is accuracy. <br><br><h2>  My experiment </h2><br><h3>  <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D0%25B7%25D0%25B2%25D0%25B5%25D0%25B4%25D0%25BE%25D1%2587%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7_%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585">Exploratory data analysis</a> </h3><br>  In the RAVDESS dataset, each actor shows 8 emotions, pronouncing and singing 2 sentences, 2 times each.  As a result, 4 examples of each emotion are obtained from each actor, with the exception of the above-mentioned neutral emotions, disgust, and surprise.  Each audio lasts approximately 4 seconds, in the first and last seconds most often silence. <br><br>  <b>Typical offers</b> : <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/87/u_/es87u_qdtsjmiv-slst1vzmzyay.png"></div><br><h3>  Observation </h3><br>  After I selected a dataset from 1 actor and 1 actress, and then listened to all their records, I realized that men and women express their emotions in different ways.  For example: <br><br><ul><li>  male anger (Angry) is just louder; </li><li>  men's joy (Happy) and frustration (Sad) - a feature in laughing and crying tones during the "silence"; </li><li>  female joy (Happy), anger (Angry) and frustration (Sad) are louder; </li><li>  female disgust (Disgust) contains the sound of vomiting. </li></ul><br><h3>  Experiment repetition </h3><br>  The author removed the neutral, disgust, and surprised classes to make the 10-class recognition of the RAVDESS dataset.  Trying to repeat the author‚Äôs experience, I got the following result: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/-l/yz/vm/-lyzvmb6xx5vwqtkmjrdsawgjtc.png"></div><br><br>  However, I found out that there is a data leak when the dataset for validation is identical to the test dataset.  Therefore, I repeated the separation of the data, isolating the datasets of two actors and two actresses so that they were not visible during the test: <br><br><ul><li>  actors 1 to 20 are used for Train / Valid sets in an 8: 2 ratio; </li><li>  actors 21 to 24 are isolated from the tests; </li><li>  Train Set parameters: (1248, 216, 1); </li><li>  Valid Set parameters: (312, 216, 1); </li><li>  Test Set parameters: (320, 216, 1) - (isolated). </li></ul><br>  I re-trained the model and here is the result: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/57/gj/jh/57gjjho-dwtlz1p7j4zv-eawhu0.png"></div><br><h3>  Performance test </h3><br>  From the Train Valid Gross graph it is clear that there is no convergence for the selected 10 classes.  Therefore, I decided to reduce the complexity of the model and leave only male emotions.  I isolated two actors in the test set, and put the rest in the train / valid set, 8: 2 ratio.  This ensures that there is no imbalance in the dataset.  Then I coached the male and female data separately to conduct the test. <br><br>  <b>Male dataset</b> <br><br><ul><li>  Train Set - 640 samples from actors 1-10; </li><li>  Valid Set - 160 samples from actors 1-10; </li><li>  Test Set - 160 samples from actors 11-12. </li></ul><br>  <b>Reference line: men</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/we/xy/hd/wexyhdyxjn9_i_wph4caesawaua.png"></div><br>  <b>Female dataset</b> <br><br><ul><li>  Train Set - 608 samples from actresses 1-10; </li><li>  Valid Set - 152 samples from actresses 1-10; </li><li>  Test Set - 160 samples from actresses 11-12. </li></ul><br>  <b>Reference line: women</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ja/jc/np/jajcnp6xzp7oncqgl4-2zkshigm.png"></div><br>  As you can see, error matrices are different. <br><br>  Men: Angry and Happy are the main predicted classes in the model, but they are not alike. <br><br>  Women: disorder (Sad) and joy (Happy) - basically predicted classes in the model;  anger and joy are easily confused. <br><br>  Recalling the observations from the <b>Intelligence Data Analysis</b> , I suspect that female Angry and Happy are similar to the point of confusion because their way of expression is simply to increase their voices. <br><br>  On top of that, I'm curious that if I simplify the model even further, leave only the Positive, Neutral, and Negative classes.  Or only Positive and Negative.  In short, I grouped emotions into 2 and 3 classes, respectively. <br><br>  <b>2 classes:</b> <br><br><ul><li>  Positive: joy (Happy), calm (Calm); </li><li>  Negative: angry, fearful, frustration (sad). </li></ul><br>  <b>3 classes:</b> <br><br><ul><li>  Positive: joy (Happy); </li><li>  Neutral: calm (Calm), neutral (Neutral); </li><li>  Negative: angry, fearful, frustration (sad). </li></ul><br>  Before starting the experiment, I set up the model architecture using male data, making 5-class recognition. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   -  target_class = 5 #  model = Sequential() model.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1))) #1 model.add(Activation('relu')) model.add(Conv1D(256, 8, padding='same')) #2 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(128, 8, padding='same')) #3 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #4 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #5 model.add(Activation('relu')) model.add(Conv1D(128, 8, padding='same')) #6 model.add(BatchNormalization()) model.add(Activation('relu')) model.add(Dropout(0.25)) model.add(MaxPooling1D(pool_size=(8))) model.add(Conv1D(64, 8, padding='same')) #7 model.add(Activation('relu')) model.add(Conv1D(64, 8, padding='same')) #8 model.add(Activation('relu')) model.add(Flatten()) model.add(Dense(target_class)) #9 model.add(Activation('softmax')) opt = keras.optimizers.SGD(lr=0.0001, momentum=0.0, decay=0.0, nesterov=False)</span></span></code> </pre> <br>  I added 2 layers of Conv1D, one layer of MaxPooling1D and 2 layers of BarchNormalization;  I also changed the dropout value to 0.25.  Finally, I changed the optimizer to SGD with a learning speed of 0.0001. <br><br><pre> <code class="python hljs">lr_reduce = ReduceLROnPlateau(monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, factor=<span class="hljs-number"><span class="hljs-number">0.9</span></span>, patience=<span class="hljs-number"><span class="hljs-number">20</span></span>, min_lr=<span class="hljs-number"><span class="hljs-number">0.000001</span></span>) mcp_save = ModelCheckpoint(<span class="hljs-string"><span class="hljs-string">'model/baseline_2class_np.h5'</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, monitor=<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>, mode=<span class="hljs-string"><span class="hljs-string">'min'</span></span>) cnnhistory=model.fit(x_traincnn, y_train, batch_size=<span class="hljs-number"><span class="hljs-number">16</span></span>, epochs=<span class="hljs-number"><span class="hljs-number">700</span></span>, validation_data=(x_testcnn, y_test), callbacks=[mcp_save, lr_reduce])</code> </pre> <br>  To train the model, I applied a reduction in the ‚Äútraining plateau‚Äù and saved only the best model with a minimum value of <code>val_loss</code> .  And here are the results for the different target classes. <br><br><h2>  New model performance </h2><br>  <b>Men, 5 classes</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/tw/jm/ay/twjmaytexfauezocygymudu6m_8.png"></div><br><br>  <b>Women, Grade 5</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uz/mc/7t/uzmc7t4mtmwlf_mfohv3qzzc4hq.png"></div><br>  <b>Men, Grade 2</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dw/y5/rf/dwy5rf0osgtrxfhb6kb-kitxyu8.png"></div><br>  <b>Men, Grade 3</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/15/sy/2w/15sy2wzciyl66tapqxfwpguhzze.png"></div><br><h2>  Increase (augmentation) </h2><br>  When I strengthened the architecture of the model, the optimizer and the speed of training, it turned out that the model still does not converge in training mode.  I suggested that this is a data quantity problem, since we only have 800 samples.  This led me to methods of increasing audio, in the end I doubled the datasets.  Let's take a look at these methods. <br><br><h3>  Men, Grade 5 </h3><br>  <b>Dynamic Increment</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dyn_change</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> dyn_change = np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">1.5</span></span>,high=<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (data * dyn_change)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kc/-l/iz/kc-lizrw-sintgd-ko0cdd3htlc.png"></div><br><br>  <b>Pitch Adjustment</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">pitch</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, sample_rate)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> bins_per_octave = <span class="hljs-number"><span class="hljs-number">12</span></span> pitch_pm = <span class="hljs-number"><span class="hljs-number">2</span></span> pitch_change = pitch_pm * <span class="hljs-number"><span class="hljs-number">2</span></span>*(np.random.uniform()) data = librosa.effects.pitch_shift(data.astype(<span class="hljs-string"><span class="hljs-string">'float64'</span></span>), sample_rate, n_steps=pitch_change, bins_per_octave=bins_per_octave)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/pk/rq/2x/pkrq2xfcdfsyph3eipzuwpvtu8a.png"></div><br>  <b>Bias</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">shift</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""   """</span></span> s_range = int(np.random.uniform(low=<span class="hljs-number"><span class="hljs-number">-5</span></span>, high = <span class="hljs-number"><span class="hljs-number">5</span></span>)*<span class="hljs-number"><span class="hljs-number">500</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.roll(data, s_range)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/wc/3_/ik/wc3_ikjjnyejxxbmw23pcuanr9c.png"></div><br>  <b>Adding White Noise</b> <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">noise</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-string"><span class="hljs-string">"""    """</span></span> <span class="hljs-comment"><span class="hljs-comment">#     : https://docs.scipy.org/doc/numpy-1.13.0/reference/routines.random.html noise_amp = 0.005*np.random.uniform()*np.amax(data) data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0]) return data</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uh/qh/aw/uhqhawd_gpp5sampbam9yez3lre.png"></div><br>  It is noticeable that augmentation greatly increases accuracy, up to 70 +% in the general case.  Especially in the case of the addition of white, which increases the accuracy to 87.19% - however, the test accuracy and the <a href="https://en.wikipedia.org/wiki/F1_score">F1 measure</a> fall by more than 5%.  And then I got the idea to combine several augmentation methods for a better result. <br><br><h3>  Combining several methods </h3><br>  <b>White noise + bias</b> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qv/fn/tg/qvfntgup-tjnrytyxoj2egxy1ys.png"></div><br><h2>  Testing augmentation on men </h2><br><h3>  Men, Grade 2 </h3><br>  <b>White noise + bias</b> <br><br>  For all samples <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lm/8v/h8/lm8vh88-i4moor2edj9j2rtksoi.png"></div><br>  <b>White noise + bias</b> <br><br>  Only for positive samples, since the 2-class set is unbalanced (towards negative samples). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/1c/0u/us/1c0uus8vlv-reh0hpd-jnoherm8.png"></div><br>  <b>Pitch + White Noise</b> <br>  For all samples <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/gb/qb/oz/gbqbozph3uampaicmgzewiitacy.png"></div><br>  <b>Pitch + White Noise</b> <br><br>  For positive samples only <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ip/jk/4p/ipjk4phb0cqc56qfbudr-_vwuww.png"></div><br><h2>  Conclusion </h2><br>  In the end, I was able to experiment only with a male dataset.  I re-divided the data so as to avoid imbalance and, as a consequence, data leakage.  I set up the model to experiment with male voices, as I wanted to simplify the model as much as possible to get started.  I also conducted tests using different augmentation methods;  the addition of white noise and bias have worked well on unbalanced data. <br><br><h2>  findings </h2><br><ul><li>  emotions are subjective and difficult to fix; </li><li>  it is necessary to determine in advance which emotions are suitable for the project; </li><li>  Do not always trust content with Github, even if it has many stars; </li><li>  data sharing - keep it in mind; </li><li>  exploratory data analysis always gives a good idea, but you need to be patient when it comes to working with audio data; </li><li>  Determine what you will give to the input of your model: a sentence, an entire record or an exclamation? </li><li>  lack of data is an important success factor in SER, however, creating a good dataset with emotions is a complex and expensive task; </li><li>  simplify your model in case of lack of data. </li></ul><br><h2>  Further improvement </h2><br><ul><li>  I used only the first 3 seconds as input to reduce the overall size of the data - the original project used 2.5 seconds.  I would like to experiment with full-sized recordings; </li><li>  you can pre-process the data: trim the silence, normalize the length by padding with zeros, etc .; </li><li>  try recurrent neural networks for this task. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/461435/">https://habr.com/ru/post/461435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461421/index.html">How to insure yourself against possible losses when investing on the exchange: structural products</a></li>
<li><a href="../461425/index.html">How to become a product manager and grow further</a></li>
<li><a href="../46143/index.html">Python 3.0 released</a></li>
<li><a href="../461431/index.html">‚ÄúLoves and dislikes‚Äù: DNS over HTTPS</a></li>
<li><a href="../461433/index.html">Using Identity Server 4 in Net Core 3.0</a></li>
<li><a href="../461437/index.html">370 light bulbs</a></li>
<li><a href="../461439/index.html">Starting the React and TypeScript component library</a></li>
<li><a href="../46144/index.html">We work in DVD Studio Pro. Part I.</a></li>
<li><a href="../461441/index.html">Reports on the state of storage using R. Parallel computing, graphs, xlsx, email and all this</a></li>
<li><a href="../461443/index.html">Post-analysis: what is known about the latest attack on the SKS Keyserver crypto key server network</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>