<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We teach the robot to cook pizza. Part 2: Neural Network Contest</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Content 


- Part 1: Getting the Data 


 In the last part, we managed to parse the Dodo-Pizza website and load the data on the ingredients, and most ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We teach the robot to cook pizza. Part 2: Neural Network Contest</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/bf8/697/a4c/bf8697a4c0e2493e9b6fc7009bb4b83d.png"></p><br><h3 id="soderzhanie">  Content </h3><br><ul><li>  <a href="https://habrahabr.ru/post/335444/">Part 1: Getting the Data</a> </li></ul><br><p>  In the last part, we managed to parse the Dodo-Pizza website and load the data on the ingredients, and most importantly, the pizza photos.  In total, we had 20 pizzas at our disposal.  Of course, it will not be possible to generate training data from just 20 pictures.  However, you can use the axial symmetry of the pizza: by rotating the image in one-degree increments and vertical reflection, you can turn one photo into a set of 720 images.  Too little, but still try. </p><br><p>  Let's try to train the Conditional Variational Autoencoder, and then move on to what it all was for ‚Äî generative adversary neural networks (Generative Adversarial Networks). </p><a name="habracut"></a><br><h3 id="cvae---uslovnyy-variacionnyy-avtoenkorder">  CVAE - conditional variational autoencoder </h3><br><p>  For proceedings with autoencoders, an excellent series of articles will help: </p><br><ul><li>  <a href="https://habrahabr.ru/post/331552/">Autoencoders in Keras, Part 3: Variational autoencoders (VAE)</a> </li><li>  <a href="https://habrahabr.ru/post/331664/">Autoencoders in Keras, Part 4: Conditional VAE</a> </li></ul><br><p>  I strongly recommend reading. <br>  Here we go straight to the point. </p><br><p>  The difference between CVAE and VAE is that we need to input both the encoder and the decoder, additionally submit another label.  In our case, the label will be the recipe vector that we get from OneHotEncoder. </p><br><p>  However, there is a nuance - and at what point does it make sense to submit our tag? </p><br><p>  I tried two methods: </p><br><ol><li>  at the end - after all convolutions - before a fully connected layer </li><li>  at the beginning - after the first convolution - is added as an additional channel </li></ol><br><p>  In principle, both methods have the right to exist.  It seems logical that if you add a label at the end, it will be attached to higher-level image features.  And vice versa - if you add it at the beginning, it will be tied to lower-level features.  Let's try to compare both ways. </p><br><p>  Recall that the recipe consists of a maximum of 9 ingredients.  And there are only 28 of them. It turns out that the recipe code will be a 9x29 matrix, and if you pull it out, you get a 261-dimensional vector. </p><br><p>  For an image size of 32x32, choose the size of the hidden space is equal to 512. <br>  You can choose less, but as will be seen later, this leads to a more blurred result. </p><br><p>  The code for the encoder with the first method of adding a tag is after all convolutions: </p><br><pre><code class="hljs pgsql">def create_conv_cvae(channels, height, width, code_h, code_w): input_img = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(channels, height, width)) input_code = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(code_h, code_w)) flatten_code = Flatten()(input_code) latent_dim = <span class="hljs-number"><span class="hljs-number">512</span></span> m_height, m_width = <span class="hljs-type"><span class="hljs-type">int</span></span>(height/<span class="hljs-number"><span class="hljs-number">4</span></span>), <span class="hljs-type"><span class="hljs-type">int</span></span>(width/<span class="hljs-number"><span class="hljs-number">4</span></span>) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(input_img) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) flatten_img_features = Flatten()(x) x = concatenate([flatten_img_features, flatten_code]) x = Dense(<span class="hljs-number"><span class="hljs-number">1024</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) z_mean = Dense(latent_dim)(x) z_log_var = Dense(latent_dim)(x)</code> </pre> <br><p>  The code for the encoder with the second labeling method - after the first convolution - as an additional channel: </p><br><pre> <code class="hljs pgsql">def create_conv_cvae2(channels, height, width, code_h, code_w): input_img = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(channels, height, width)) input_code = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(code_h, code_w)) flatten_code = Flatten()(input_code) latent_dim = <span class="hljs-number"><span class="hljs-number">512</span></span> m_height, m_width = <span class="hljs-type"><span class="hljs-type">int</span></span>(height/<span class="hljs-number"><span class="hljs-number">4</span></span>), <span class="hljs-type"><span class="hljs-type">int</span></span>(width/<span class="hljs-number"><span class="hljs-number">4</span></span>) def add_units_to_conv2d(conv2, units): dim1 = K.int_shape(conv2)[<span class="hljs-number"><span class="hljs-number">2</span></span>] dim2 = K.int_shape(conv2)[<span class="hljs-number"><span class="hljs-number">3</span></span>] dimc = K.int_shape(units)[<span class="hljs-number"><span class="hljs-number">1</span></span>] repeat_n = dim1*dim2 count = <span class="hljs-type"><span class="hljs-type">int</span></span>( dim1*dim2 / dimc) units_repeat = RepeatVector(count+<span class="hljs-number"><span class="hljs-number">1</span></span>)(units) #print(<span class="hljs-string"><span class="hljs-string">'K.int_shape(units_repeat): '</span></span>, K.int_shape(units_repeat)) units_repeat = Flatten()(units_repeat) # cut <span class="hljs-keyword"><span class="hljs-keyword">only</span></span> needed lehgth <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> code units_repeat = Lambda(lambda x: x[:,:dim1*dim2], output_shape=(dim1*dim2,))(units_repeat) units_repeat = Reshape((<span class="hljs-number"><span class="hljs-number">1</span></span>, dim1, dim2))(units_repeat) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> concatenate([conv2, units_repeat], axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(input_img) x = add_units_to_conv2d(x, flatten_code) #print(<span class="hljs-string"><span class="hljs-string">'K.int_shape(x): '</span></span>, K.int_shape(x)) # size here: (<span class="hljs-number"><span class="hljs-number">17</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">32</span></span>) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Flatten()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">1024</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) z_mean = Dense(latent_dim)(x) z_log_var = Dense(latent_dim)(x)</code> </pre> <br><p>  The decoder code in both cases is the same - the label is added at the very beginning. </p><br><pre> <code class="hljs lisp"> z = Input(<span class="hljs-name"><span class="hljs-name">shape=</span></span>(<span class="hljs-name"><span class="hljs-name">latent_dim</span></span>, )) input_code_d = Input(<span class="hljs-name"><span class="hljs-name">shape=</span></span>(<span class="hljs-name"><span class="hljs-name">code_h</span></span>, code_w)) flatten_code_d = Flatten()(<span class="hljs-name"><span class="hljs-name">input_code_d</span></span>) x = concatenate([z, flatten_code_d]) x = Dense(<span class="hljs-number"><span class="hljs-number">1024</span></span>)(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = Dense(<span class="hljs-number"><span class="hljs-number">16</span></span>*m_height*m_width)(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = Reshape((<span class="hljs-number"><span class="hljs-number">16</span></span>, m_height, m_width))(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = Conv2D(<span class="hljs-number"><span class="hljs-number">16</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation='relu', padding='same')(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = UpSampling2D((<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = Conv2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation='relu', padding='same')(<span class="hljs-name"><span class="hljs-name">x</span></span>) x = UpSampling2D((<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>))(<span class="hljs-name"><span class="hljs-name">x</span></span>) decoded = Conv2D(<span class="hljs-name"><span class="hljs-name">channels</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>), activation='sigmoid', padding='same')(<span class="hljs-name"><span class="hljs-name">x</span></span>)</code> </pre> <br><p>  Number of network parameters: </p><br><ol><li>  4'221'987 </li><li>  3'954'867 </li></ol><br><p>  The speed of learning for one era: </p><br><ol><li>  60 sec </li><li>  63 seconds </li></ol><br><p>  Result after 40 epochs of study: </p><br><ol><li>  loss: -0.3232 val_loss: -0.3164 </li><li>  loss: -0.3245 val_loss: -0.3191 </li></ol><br><p>  As you can see, the second method requires less memory for the INS, gives a better result, but takes a little more time to learn. </p><br><p>  It remains to visually compare the results of the work. </p><br><ol><li>  Original image (32x32) </li><li>  The result of the work is the first method (latent_dim = 64) </li><li>  The result of the work is the first method (latent_dim = 512) </li><li>  The result of the work is the second method (latent_dim = 512) </li></ol><br><p><img src="https://habrastorage.org/web/989/fbd/3fa/989fbd3fa8e646d78cd0e07341da62d5.png"><br><img src="https://habrastorage.org/web/ed1/afb/018/ed1afb018d1d4fcc8a3fc7bec0c8e751.png"><br><img src="https://habrastorage.org/web/71c/d8c/1a5/71cd8c1a586643dda745a53633d7c261.png"><br><img src="https://habrastorage.org/web/e71/5c7/095/e715c7095f804efc8725bc21d295edb7.png"></p><br><p>  Now let's see how the pizza transfer application looks like when the pizza is encoded with the original recipe and decoded with another. </p><br><pre> <code class="hljs pgsql">i = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> label <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> labels: i += <span class="hljs-number"><span class="hljs-number">1</span></span> lbls = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(batch_size): lbls.append(label) lbls = np.<span class="hljs-keyword"><span class="hljs-keyword">array</span></span>(lbls, dtype=np.float32) print(i, lbls.shape) stt_imgs = stt.predict([orig_images, orig_labels, lbls], batch_size=batch_size) save_images(stt_imgs, dst=<span class="hljs-string"><span class="hljs-string">'temp/cvae_stt'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">comment</span></span>=<span class="hljs-string"><span class="hljs-string">'_'</span></span>+str(i))</code> </pre> <br><p>  The result of the work transfer style (the second encoding method): </p><br><p><img src="https://habrastorage.org/web/d28/e0e/027/d28e0e027a1c4a5586c6e59964b2a9ef.png"></p><br><h3 id="gan---generativnaya-costyazatelnaya-set">  GAN - Generative Competitive Network </h3><br><p>  I did not manage to find the well-established Russian-language names of such networks. <br>  Options: </p><br><ul><li>  generative competition networks </li><li>  spawning networks </li><li>  spawning networks </li></ul><br><p>  I like it more: </p><br><ul><li>  generative competitive networks </li></ul><br><p>  With the theory of the work of GAN again, an excellent series of articles will help: </p><br><ul><li>  <a href="https://habrahabr.ru/post/332000/">Autoencoders in Keras, Part 5: GAN (Generative Adversarial Networks) and tensorflow</a> </li><li>  <a href="https://habrahabr.ru/post/332074/">Autoencoders in Keras, Part 6: VAE + GAN</a> </li></ul><br><p>  And for a deeper understanding - the latest article in the blog ODS: <a href="https://habrahabr.ru/company/ods/blog/322514/">Neural network game in imitation</a> </p><br><p>  However, starting to understand and try to independently implement the generative neural network - I was faced with some difficulties.  For example, there were moments when the generator gave out really psychedelic pictures. </p><br><p>  Different examples helped to understand the implementation: </p><br><p>  <a href="https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/">MNIST Generative Adversarial Model in Keras</a> ( <a href="https://github.com/osh/KerasGAN/blob/master/mnist_gan.py">mnist_gan.py</a> ), </p><br><p>  Architecture recommendations from the end of 2015 article from <a href="https://research.fb.com/publications/unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks/">facebook research</a> about <strong>DCGAN</strong> (Deep Convolutional GAN): </p><br><p>  <a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> </p><br><p>  as well as a set of recommendations to make GAN work: </p><br><p>  <a href="https://github.com/soumith/ganhacks">How to Train a GAN?</a>  <a href="https://github.com/soumith/ganhacks">Tips and tricks to make GANs work</a> . </p><br><p>  Constructing a GAN: </p><br><pre> <code class="hljs pgsql">def make_trainable(net, val): net.trainable = val <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> l <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> net.layers: l.trainable = val def create_gan(channels, height, width): input_img = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(channels, height, width)) m_height, m_width = <span class="hljs-type"><span class="hljs-type">int</span></span>(height/<span class="hljs-number"><span class="hljs-number">8</span></span>), <span class="hljs-type"><span class="hljs-type">int</span></span>(width/<span class="hljs-number"><span class="hljs-number">8</span></span>) # generator z = <span class="hljs-keyword"><span class="hljs-keyword">Input</span></span>(shape=(latent_dim, )) x = Dense(<span class="hljs-number"><span class="hljs-number">256</span></span>*m_height*m_width)(z) #x = BatchNormalization()(x) x = Activation(<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) #x = Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)(x) x = Reshape((<span class="hljs-number"><span class="hljs-number">256</span></span>, m_height, m_width))(x) x = Conv2DTranspose(<span class="hljs-number"><span class="hljs-number">256</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2DTranspose(<span class="hljs-number"><span class="hljs-number">128</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2DTranspose(<span class="hljs-number"><span class="hljs-number">64</span></span>, kernel_size=(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), strides=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)(x) x = Conv2D(channels, (<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) g = Activation(<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>)(x) generator = Model(z, g, <span class="hljs-type"><span class="hljs-type">name</span></span>=<span class="hljs-string"><span class="hljs-string">'Generator'</span></span>) # discriminator x = Conv2D(<span class="hljs-number"><span class="hljs-number">128</span></span>, (<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(input_img) #x = BatchNormalization()(x) x = LeakyReLU()(x) #x = Dropout(<span class="hljs-number"><span class="hljs-number">0.3</span></span>)(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">256</span></span>, (<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = LeakyReLU()(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Conv2D(<span class="hljs-number"><span class="hljs-number">512</span></span>, (<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = LeakyReLU()(x) x = MaxPooling2D(pool_size=(<span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>), padding=<span class="hljs-string"><span class="hljs-string">'same'</span></span>)(x) x = Flatten()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">2048</span></span>)(x) x = LeakyReLU()(x) x = Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>)(x) d = Activation(<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)(x) discriminator = Model(input_img, d, <span class="hljs-type"><span class="hljs-type">name</span></span>=<span class="hljs-string"><span class="hljs-string">'Discriminator'</span></span>) gan = Sequential() gan.<span class="hljs-keyword"><span class="hljs-keyword">add</span></span>(generator) make_trainable(discriminator, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) #discriminator.trainable = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span> gan.<span class="hljs-keyword"><span class="hljs-keyword">add</span></span>(discriminator) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> generator, discriminator, gan gan_gen, gan_ds, gan = create_gan(channels, height, width) gan_gen.<span class="hljs-keyword"><span class="hljs-keyword">summary</span></span>() gan_ds.<span class="hljs-keyword"><span class="hljs-keyword">summary</span></span>() gan.<span class="hljs-keyword"><span class="hljs-keyword">summary</span></span>() opt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-3</span></span>) gopt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>) dopt = Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>) gan_gen.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=gopt) gan.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=opt) make_trainable(gan_ds, <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) gan_ds.compile(loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, optimizer=dopt)</code> </pre> <br><p>  As you can see, the discriminator is the usual binary classifier, which produces: </p><br><p>  1 - for real pictures, <br>  0 - for fake. </p><br><p>  Learning procedure: </p><br><ul><li>  we receive a portion of real pictures </li><li>  we generate noise on the basis of which the generator generates pictures </li><li>  we form a batch for learning the discriminator, which consists of real images (they are assigned the label 1) and fakes from the generator (label 0) </li><li>  we train the discriminator </li><li>  we train the GAN (the generator is trained in it, because the discriminator‚Äôs training is turned off), giving noise to the input and waiting for the 1 mark at the output. </li></ul><br><pre> <code class="hljs mel"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(epochs): <span class="hljs-keyword"><span class="hljs-keyword">print</span></span>(<span class="hljs-string"><span class="hljs-string">'Epoch {} from {} ...'</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">format</span></span>(epoch, epochs)) n = x_train.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] image_batch = x_train[np.random.randint(<span class="hljs-number"><span class="hljs-number">0</span></span>, n, <span class="hljs-keyword"><span class="hljs-keyword">size</span></span>=batch_size),:,:,:] noise_gen = np.random.<span class="hljs-keyword"><span class="hljs-keyword">uniform</span></span>(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">size</span></span>=[batch_size, latent_dim]) generated_images = gan_gen.predict(noise_gen, batch_size=batch_size) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> epoch % <span class="hljs-number"><span class="hljs-number">10</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">print</span></span>(<span class="hljs-string"><span class="hljs-string">'Save gens ...'</span></span>) save_images(generated_images) gan_gen.save_weights(<span class="hljs-string"><span class="hljs-string">'temp/gan_gen_weights_'</span></span>+str(height)+<span class="hljs-string"><span class="hljs-string">'.h5'</span></span>, True) gan_ds.save_weights(<span class="hljs-string"><span class="hljs-string">'temp/gan_ds_weights_'</span></span>+str(height)+<span class="hljs-string"><span class="hljs-string">'.h5'</span></span>, True) # save loss df = pd.DataFrame( {<span class="hljs-string"><span class="hljs-string">'d_loss'</span></span>: d_loss, <span class="hljs-string"><span class="hljs-string">'g_loss'</span></span>: g_loss} ) df.to_csv(<span class="hljs-string"><span class="hljs-string">'temp/gan_loss.csv'</span></span>, index=False) x_train2 = np.concatenate( (image_batch, generated_images) ) y_tr2 = np.zeros( [<span class="hljs-number"><span class="hljs-number">2</span></span>*batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>] ) y_tr2[:batch_size] = <span class="hljs-number"><span class="hljs-number">1</span></span> d_history = gan_ds.train_on_batch(x_train2, y_tr2) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span>(<span class="hljs-string"><span class="hljs-string">'d:'</span></span>, d_history) d_loss.append( d_history ) noise_gen = np.random.<span class="hljs-keyword"><span class="hljs-keyword">uniform</span></span>(<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">size</span></span>=[batch_size, latent_dim]) g_history = gan.train_on_batch(noise_gen, np.ones([batch_size, <span class="hljs-number"><span class="hljs-number">1</span></span>])) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span>(<span class="hljs-string"><span class="hljs-string">'g:'</span></span>, g_history) g_loss.append( g_history )</code> </pre> <br><p>  Please note that, unlike a variational autoencoder, the real images are not used for the generator training, but only the discriminator mark.  Those.  the generator is trained on error gradients from the discriminator. </p><br><p>  The most interesting thing is that the name of competitive networks is not for a beautiful word - they really try and even follow the testimony of the losses of the discriminator and generator. </p><br><p>  If you look at the loss curves, you can see that the discriminator quickly learns to distinguish the real picture from the original garbage emitted by the generator, but then the curves begin to oscillate ‚Äî the generator learns to generate an increasingly appropriate image. </p><br><p><img src="https://habrastorage.org/web/d36/ab8/bcf/d36ab8bcfeb246549c281ff133a39bd7.png"></p><br><p>  gif-ka showing the process of learning the generator (32x32) on one pizza (the first pizza on the list is Double pepperoni): </p><br><p><img src="https://habrastorage.org/web/2c9/841/0d3/2c98410d37da40bfa03547c1b91d325a.gif"></p><br><p>  As expected, the result of the GAN operation, compared with the variational encoder, gives a clearer image. </p><br><h3 id="cvae--gan---uslovnyy-variacionnyy-avtoenkorder-i-generativnaya-costyazatelnaya-set">  CVAE + GAN - conditional variational avtoenkorder and generative adversarial network </h3><br><p>  It remains to combine CVAE and GAN together to get the best from both networks.  The basis of the union is a simple idea - the VAE decoder performs exactly the same function as the GAN generator, but they perform and learn it in different ways. </p><br><p>  In addition to what is not completely clear, how to make it all work together, it was also not clear to me that you can use different loss functions in Keras.  The examples on the githaba helped to understand this question: </p><br><p>  ‚Üí <a href="https://github.com/tatsy/keras-generative">Keras VAEs and GANs</a> </p><br><p>  Thus, the application of various loss functions in Keras can be implemented by adding your own layer ( <a href="https://keras.io/layers/writing-your-own-keras-layers/">Writing your own Keras layers</a> ), in the call () method of which you can implement the required calculation logic with a subsequent call to the add_loss () method. </p><br><p>  Example: </p><br><pre> <code class="hljs haskell"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">DiscriminatorLossLayer</span></span></span><span class="hljs-class">(</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">Layer</span></span></span><span class="hljs-class">): __name__ = 'discriminator_loss_layer' def __init__(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, **</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">kwargs</span></span></span><span class="hljs-class">): self.is_placeholder = </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">True</span></span></span><span class="hljs-class"> super(</span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">DiscriminatorLossLayer</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">).__init__(**</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">kwargs</span></span></span><span class="hljs-class">) def lossfun(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_real</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_f</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_p</span></span></span><span class="hljs-class">): y_pos = </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">K</span></span></span><span class="hljs-class">.ones_like(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_real</span></span></span><span class="hljs-class">) y_neg = </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">K</span></span></span><span class="hljs-class">.zeros_like(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_real</span></span></span><span class="hljs-class">) loss_real = keras.metrics.binary_crossentropy(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_pos</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_real</span></span></span><span class="hljs-class">) loss_fake_f = keras.metrics.binary_crossentropy(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_neg</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_f</span></span></span><span class="hljs-class">) loss_fake_p = keras.metrics.binary_crossentropy(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_neg</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_p</span></span></span><span class="hljs-class">) return </span><span class="hljs-type"><span class="hljs-class"><span class="hljs-type">K</span></span></span><span class="hljs-class">.mean(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">loss_real</span></span></span><span class="hljs-class"> + </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">loss_fake_f</span></span></span><span class="hljs-class"> + </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">loss_fake_p</span></span></span><span class="hljs-class">) def call(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">self</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inputs</span></span></span><span class="hljs-class">): y_real = inputs[0] y_fake_f = inputs[1] y_fake_p = inputs[2] loss = self.lossfun(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_real</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_f</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">y_fake_p</span></span></span><span class="hljs-class">) self.add_loss(</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">loss</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inputs</span></span></span><span class="hljs-class">=</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">inputs</span></span></span><span class="hljs-class">) return y_real</span></span></code> </pre> <br><p>  gif-ka showing the learning process (64x64): </p><br><p><img src="https://habrastorage.org/web/652/1a2/3ba/6521a23ba4cc479fbed027f2503ac3a9.gif"></p><br><p>  The result of the work transfer style: </p><br><p><img src="https://habrastorage.org/web/7d1/7c6/85e/7d17c685edb84baeb37ebed7e527f068.png"></p><br><p>  And now the fun part! </p><br><p>  Actually for the sake of what it all was started - the generation of pizza for the selected ingredients. </p><br><p>  Let's look at pizza with a recipe consisting of one ingredient (ie, with codes from 1 to 27): </p><br><p><img src="https://habrastorage.org/web/c3d/5b1/30c/c3d5b130cc8c4a9cb73b6e1017c06f10.png"></p><br><p>  As expected, only pizzas with the most popular ingredients 24, 20, 17 (tomatoes, pepperoni, mozzarella) look more or less - all other options are something dull with round shapes and incomprehensible gray spots in which only on request You can try to guess something. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  In general, the experiment can be considered partially successful.  However, even on such a toy example, one feels that the pathos expression: ‚Äúdata is new oil‚Äù - has a right to exist, especially with regard to machine learning. <br>  After all, the quality of the application based on machine learning, depends primarily on the quality and quantity of data. </p><br><p>  Generative networks are really very interesting and, it seems that in the foreseeable future we will see many different examples of their use. </p><br><p>  By the way, a good question arises: if the rights to photos belong to their creator, then who owns the rights to the picture that the neural network creates? </p><br><p>  Thank you very much for your attention! </p><br><p>  Nb.  When writing this article - not a single pizza has suffered. </p><br><h3 id="ssylki">  Links </h3><br><ul><li>  <a href="https://habrahabr.ru/post/331552/">Autoencoders in Keras, Part 3: Variational autoencoders (VAE)</a> </li><li>  <a href="https://habrahabr.ru/post/331664/">Autoencoders in Keras, Part 4: Conditional VAE</a> </li><li>  <a href="https://habrahabr.ru/post/332000/">Autoencoders in Keras, Part 5: GAN (Generative Adversarial Networks) and tensorflow</a> </li><li>  <a href="https://habrahabr.ru/post/332074/">Autoencoders in Keras, Part 6: VAE + GAN</a> </li><li>  Deep Convolutional GANs (DCGAN): <a href="https://arxiv.org/abs/1511.06434">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</a> </li><li>  <a href="https://github.com/soumith/ganhacks">How to Train a GAN?</a>  <a href="https://github.com/soumith/ganhacks">Tips and tricks to make GANs work</a> </li><li>  <a href="https://github.com/tatsy/keras-generative">Keras VAEs and GANs</a> </li><li>  <a href="https://oshearesearch.com/index.php/2016/07/01/mnist-generative-adversarial-model-in-keras/">MNIST Generative Adversarial Model in Keras</a> ( <a href="https://github.com/osh/KerasGAN/blob/master/mnist_gan.py">mnist_gan.py</a> ) </li><li>  <a href="http://www.rricard.me/machine/learning/generative/adversarial/networks/keras/tensorflow/2017/04/05/gans-part2.html">Generative Adversarial Networks Part 2 - Implementation with Keras 2.0</a> </li><li>  <a href="https://github.com/phreeza/keras-GAN">Generative Adversarial Networks with Keras</a> </li><li>  <a href="https://medium.com/towards-data-science/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0">GAN by Example using Keras on Tensorflow Backend</a> </li><li>  <a href="https://github.com/kyloon/dcgan">Keras implementation of Deep Convolutional Generative Adversarial Networks (DCGAN)</a> </li><li>  <a href="https://habrahabr.ru/company/wunderfund/blog/334568/">Generative models from OpenAI</a> </li><li>  <a href="https://habrahabr.ru/company/ods/blog/322514/">Neural network imitation game</a> </li></ul></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/337398/">https://habr.com/ru/post/337398/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337386/index.html">Allure 2: New Generation Test Reports</a></li>
<li><a href="../337388/index.html">New NASH encryption algorithm</a></li>
<li><a href="../337390/index.html">Windows shortcuts: where do they lead and can they be dangerous?</a></li>
<li><a href="../337392/index.html">Metacomputations and deep convolutional networks: an interview with ITMO professor</a></li>
<li><a href="../337394/index.html">iOS development: quick start methods</a></li>
<li><a href="../337400/index.html">The practice of forming requirements in IT projects from A to Z. Part 5. Essence of the subject area and a little about strategies</a></li>
<li><a href="../337402/index.html">New course "Design of high-loaded systems" in Technopolis</a></li>
<li><a href="../337410/index.html">Satoshi Bomb</a></li>
<li><a href="../337412/index.html">Do you need SMS marketing?</a></li>
<li><a href="../337416/index.html">The economics of tokens: why are ICOs so popular?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>