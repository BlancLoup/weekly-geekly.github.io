<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Results of the development of computer vision in one year</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part one. Classification / localization, object detection and object tracking 

 This excerpt is from a recent publication that was compiled by our re...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Results of the development of computer vision in one year</h1><div class="post__text post__text-html js-mediator-article">  <b>Part one.</b>  <b>Classification / localization, object detection and object tracking</b> <br><br><blockquote>  <i>This excerpt is from a recent publication that was compiled by our research team in the field of computer vision.</i>  <i>In the coming months, we will publish papers on various research topics in the field of Artificial Intelligence ‚Äî about its economic, technological, and social applications ‚Äî with the goal of providing educational resources for those who want to learn more about this amazing technology and its current state.</i>  <i>Our project hopes to contribute to the growing mass of work that provides all researchers with information on the latest AI developments.</i> </blockquote><br><h1>  Introduction </h1><br>  Computer vision is usually called the scientific discipline that gives cars the ability to see, or more colorfully, allowing machines to visually analyze their environment and stimuli in it.  This process usually involves evaluating one or more images or videos.  The British Association of Machine Vision (BMVA) <a href="http://www.bmva.org/visionoverview">defines</a> computer vision as <i>"automatic extraction, analysis and understanding of useful information from an image or its sequence</i> . <i>"</i> <br><br>  The term <i>understanding is</i> interestingly distinguished from the mechanical definition of vision ‚Äî and at the same time it demonstrates the significance and complexity of the field of computer vision.  A true understanding of our environment is not only achieved through a visual presentation.  In fact, the visual signals pass through the optic nerve into the primary visual cortex and are interpreted by the brain in a highly stylized sense.  The interpretation of this sensory information encompasses almost the entire totality of our natural embedded programs and subjective experience, that is, how evolution has programmed us to survive and what we learned about the world throughout life. <br><a name="habracut"></a><br>  In this regard, <i>vision</i> refers only to the transmission of images for interpretation;  and <i>computing</i> indicates that the images are more like thoughts or consciousness, relying on the many abilities of the brain.  Therefore, many believe that computer vision, a true understanding of the visual environment and its context, paves the way for future variations of Strong Artificial Intelligence thanks to a perfect mastery of work in cross-domain areas. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      But don‚Äôt grab hold of a weapon, because we haven‚Äôt practically reached the embryonic stage of development of this amazing area.  This article should just shed some light on the most significant achievements of computer vision in 2016.  And it is possible to try to inscribe some of these achievements in a sound mixture of expected short-term social interactions and, where applicable, hypothetical predictions of the completion of our life as we know it. <br><br>  Although our works are always written in the most accessible way, the sections in this particular article may seem a bit unclear due to the subject matter.  We everywhere offer definitions at a primitive level, but they only provide a superficial understanding of key concepts.  Concentrating on the works of 2016, we often make gaps for the sake of brevity. <br><br>  One of these obvious omissions relates to the functionality of convolutional neural networks (CNN), which are commonly used in computer vision.  <a href="http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf">AlexNet's</a> success in 2012, the architecture of CNN, which stunned competitors in the ImageNet competition, was a testament to a revolution that de facto took place in this area.  Subsequently, numerous researchers began using CNN-based systems, and convolutional neural networks have become a traditional technology in computer vision. <br><br>  More than four years have passed, and the CNN variants still make up the bulk of the new neural network architectures for computer vision tasks.  Researchers remake them as designer cubes.  This is real proof of the power of both open source scientific publications and in-depth training.  However, the explanation of convolutional neural networks will easily stretch into several articles, so it is better to leave it for those who are more deeply versed in the subject and have a desire to explain complex things in a clear language. <br><br>  For ordinary readers who wish to quickly understand the topic before continuing with this article, we recommend the first two sources listed below.  If you want to dive even more into the subject, then for this we also cite other sources: <br><br><ul><li>  ‚Äú <a href="http://karpathy.github.io/2015/10/25/selfie/">What a deep neural network thinks about your selfie</a> ‚Äù by Andrey Karpaty is one of the best articles that helps people understand the use and functionality of convolutional neural networks. </li><li>  <a href="https://www.quora.com/What-is-a-convolutional-neural-network">Quora: ‚ÄúWhat is a convolutional neural network?‚Äù</a> Is full of excellent references and explanations.  Especially suitable for those who have <u>not had prior understanding in this area</u> . </li><li>  <a href="http://cs231n.stanford.edu/">CS231n: Convolutional Neural Networks for Visual Recognition from Stanford University</a> is an excellent resource for more in-depth study of the topic. </li><li>  ‚Äú <a href="http://www.deeplearningbook.org/">Deep Learning</a> ‚Äù (Goodfellow, Bengio &amp; Courville, 2016) gives a detailed explanation of the functions of convolutional neural networks and functionality in <a href="http://www.deeplearningbook.org/contents/convnets.html">Chapter 9</a> .  The authors have kindly published this tutorial for free in HTML format. </li></ul><br>  For a more complete understanding of neural networks and deep learning in general, we recommend: <br><br><ul><li>  ‚Äú <a href="http://neuralnetworksanddeeplearning.com/index.html">Neural networks and deep learning</a> ‚Äù (Nielsen, 2017) is a free online tutorial that provides a truly intuitive understanding of all the complexities of neural networks and deep learning.  Even reading the first part should, in many ways, highlight for beginners the subject of this article. </li></ul><br>  In general, this article is scattered and spasmodic, reflecting the authors' admiration and the spirit of how it is supposed to be used, section by section.  The information is divided into parts in accordance with our own heuristics and judgments, a necessary compromise due to the cross-domain influence of so many scientific papers. <br><br>  We hope that readers will benefit from our generalization of information, and it will allow them to improve their knowledge, regardless of the previous baggage. <br><br>  On behalf of all participants <br>  <i>The m tank</i> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a36/f2d/947/a36f2d947ce1ef4dd7cad5bff68faf97.png"></div><br><h1>  Classification / localization </h1><br>  The task of classification with respect to images is usually to assign a label to a whole image, for example, ‚Äúcat‚Äù.  With this in mind, localization can mean determining where the object is in this image.  Usually, it is indicated by a certain bounding box around the object.  <a href="http://image-net.org/challenges/LSVRC/2016/index">The current classification methods on ImageNet are</a> already <a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">superior to groups of specially trained people</a> in the accuracy of object classification. <br><br>  <b>Fig.</b>  <b>1</b> : Computer vision tasks <br><img src="https://habrastorage.org/getpro/habr/post_images/73c/3a7/a1c/73c3a7a1cd66c7d32a7c62b0fbe9fe74.jpg"><br>  <i><font color="gray"><b>Source</b> : Fei-Fei Li, Andrej Karpathy &amp; Justin Johnson (2016) cs231n, lecture 8 - slide 8, spatial localization and detection (01/02/2016), <a href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf">pdf</a></font></i> <br><br>  However, increasing the number of classes is likely to provide new metrics for measuring progress in the near future.  In particular, Francois Chollet, the creator of <a href="https://keras.io/">Keras</a> , has applied <a href="https://arxiv.org/abs/1607.05691v1">new techniques</a> , including the popular <a href="https://arxiv.org/abs/1610.02357v2">Xception</a> architecture, to Google‚Äôs internal data set with more than 350 million images with multiple tags containing 17,000 classes. <br><br>  <b>Fig.</b>  <b>2</b> : Classification / localization results from the ILSVRC competition (2010‚Äì2016) <br><img src="https://habrastorage.org/getpro/habr/post_images/363/7ae/07c/3637ae07cacb4933099119766eb908f7.png"><br><br>  <i><font color="gray"><b>Note</b> : ImageNet Large Scale Visual Recognition Challenge (ILSVRC).</font></i>  <i><font color="gray">The improvement in results after 2011‚Äì2012 is due to the advent of AlexNet.</font></i>  <i><font color="gray">See a <a href="http://www.image-net.org/challenges/LSVRC/2016/index">review of</a> competition requirements regarding classification and localization.</font></i> <i><font color="gray"><br></font></i>  <i><font color="gray"><b>Source</b> : Jia Deng (2016).</font></i>  <i><font color="gray">Localization of objects ILSVRC2016: introduction, results.</font></i>  <i><font color="gray">Slide 2, <a href="http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf">pdf</a></font></i> <br><br>  <b>Interesting excerpts from ImageNet LSVRC (2016):</b> <br><br><ul><li>  <b>Scene classification</b> refers to the task of assigning labels to an image with a specific class of a scene, such as a greenhouse, stadium, cathedral, etc.  As part of ImageNet, last year a competition was held to classify scenes in a sample from the <a href="http://places2.csail.mit.edu/">Places2</a> data <a href="http://places2.csail.mit.edu/">set</a> : 8 million images for training with 365 categories of scenes. <br><br>  Won <a href="http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/">Hikvision</a> with a score of 9% of the top 5 errors.  The system is constructed from a set of deep neural networks in the style of Inception and non-such-deep residual networks. </li><li>  <b>Trimps-Soushen</b> won the ImageNet classification problem with a classification error of 2.99% top 5 and a localization error of 7.71%.  The developers made a system of several models (by averaging the results of the Inception, Inception-Resnet, ResNet and Wide Residual Networks models), and <a href="http://image-net.org/challenges/LSVRC/2016/results">Faster R-CNN</a> won the localization by tags.  The data set was distributed between 1000 image classes with 1.2 million images for training.  The data set for testing contained another 100 thousand images that neural networks had not seen before. </li><li>  Facebook's <b><a href="https://arxiv.org/abs/1611.05431v2">ResNeXt</a></b> neural network finished in second place with a classification error of 3.03% of the top 5.  It used a new architecture that extends the original ResNet architecture. </li></ul><br><h1>  Object detection </h1><br>  As you might guess, the object detection process does exactly what it needs to do ‚Äî it detects objects in images.  <a href="http://image-net.org/challenges/LSVRC/2016/%2523det">The definition of object detection from ILSVRC 2016</a> includes the issuance of bounding boxes and labels for individual objects.  This differs from the classification / localization task, since here classification and localization are applied to many objects, and not to one dominant object. <br><br>  <b>Fig.</b>  <b>3</b> : Object detection where face is the only class <br><img src="https://habrastorage.org/getpro/habr/post_images/908/233/59b/90823359b74d42dbe5823cff96b1eb7f.jpg"><br>  <i><font color="gray"><b>Note</b> : The picture is an example of face detection as the detection of objects of the same class.</font></i>  <i><font color="gray">The authors call one of the constant problems in this area the detection of small objects.</font></i>  <i><font color="gray">Using small faces as a test class, they explored the role of invariance in size, image resolution, and contextual reasoning.</font></i> <i><font color="gray"><br></font></i>  <i><font color="gray"><b>Source</b> : <a href="https://arxiv.org/abs/1612.04402v1">Hu, Ramanan (2016, p. 1)</a></font></i> <br><br>  One of the main trends in object detection in 2016 was the transition to faster and more effective detection systems.  This can be seen in approaches such as YOLO, SSD and R-FCN as a step to joint calculations on the entire image as a whole.  In this way, they differ from resource-intensive subnets associated with R / CNN Fast / Faster techniques.  This technique is commonly called end-to-end training / learning. <br><br>  In essence, the idea is to avoid using separate algorithms for each of the sub-problems in isolation from each other, since this usually increases the learning time and reduces the accuracy of the neural network.  It is said that such an adaptation of neural networks to work from start to finish usually occurs after the operation of the initial subnets and, thus, is a retrospective optimization.  However, the Fast / Faster R-CNN techniques remain highly efficient and are still widely used for object detection. <br><br><ul><li>  <b><a href="https://arxiv.org/abs/1512.02325v5">SSD: Single Shot MultiBox Detector</a></b> uses a single neural network that performs all the necessary calculations and eliminates the need for resource-intensive methods of the previous generation.  It demonstrates "75.1% mAP, surpassing the comparable state-of-the-art Faster R-CNN model." </li><li>  One of the most impressive developments of 2016 is the system aptly named <b><a href="https://arxiv.org/abs/1612.08242v1">YOLO9000: Better, Faster, Stronger</a></b> , which uses the YOLOv2 and YOLO9000 detection systems (YOLO means You Only Look Once).  YOLOv2 is a greatly improved <a href="https://arxiv.org/abs/1506.02640v5">YOLO model from mid-2015</a> , and it is able to show the best results in videos with very high frame rates (up to 90 FPS in low-resolution images using the regular GTX Titan X).  In addition to increasing the speed, the system surpasses the Faster RCNN with ResNet and SSD on certain data sets to identify objects. <br><br>  YOLO9000 implements a combined learning method for detecting and classifying objects, extending its ability to predict beyond the limits of available markup detection data.  In other words, it is able to detect objects that have never been encountered in the marked data.  The YOLO9000 model provides the detection of objects in real time among more than 9000 categories, which eliminates the difference in the size of data sets for classification and detection.  Additional details, pre-trained models and video demonstrations are available <a href="http://pjreddie.com/darknet/yolo/">here</a> . <br><br>  <b>Object Detection YOLOv2 works on frames of the film with James Bond</b> <br><iframe width="560" height="315" src="https://www.youtube.com/embed/VOC3huqHrss" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></li><li>  The <b><a href="https://arxiv.org/abs/1612.03144v1">Feature Pyramid Networks for Object Detection system was</a></b> developed by the FAIR (Facebook Artificial Intelligence Research) research unit.  It uses the <i>‚Äúinnate multiscale pyramidal hierarchy of deep convolutional neural networks to construct features pyramids with minimal additional costs‚Äù</i> .  This means maintaining powerful representations without losing speed and additional memory costs.  The developers achieved record numbers on the COCO (Common Objects in Context) dataset.  In combination with the Faster R-CNN base system, it surpasses the results of the 2016 winners. </li><li>  <b><a href="https://arxiv.org/abs/1605.06409v2">R-FCN: Object Detection via Region-based Fully Convolutional Networks</a></b> .  Another method in which developers abandoned the use of resource-intensive subnets for individual regions of the image hundreds of times in each picture.  Here the detector by region is completely convolutional and performs joint calculations on the entire image as a whole.  <i>‚ÄúWhen testing, the speed of work was 170 ms per image, which is 2.5‚Äì20 times faster than that of Faster R-CNN,‚Äù the</i> authors write. <br><br>  <b>Fig.</b>  <b>4</b> : The trade-off between accuracy and size of objects when objects are detected on different architectures <br><img src="https://habrastorage.org/getpro/habr/post_images/ea8/c2a/c36/ea8c2ac365880ad2856c071017582184.png"><br>  <i><font color="gray"><b>Note</b> : The mAP (mean Average Precision) indicator is plotted on the vertical axis, and the variety of meta-architectures for each feature extraction unit (VGG, MobileNet ... Inception ResNet V2) is shown on the horizontal axis.</font></i>  <i><font color="gray">In addition, small, medium, and large mAP show average accuracy for small, medium, and large objects, respectively.</font></i>  <i><font color="gray">Essentially, the accuracy depends on the size of the object, the meta-architecture and the feature extraction block.</font></i>  <i><font color="gray">At the same time, "the image size is fixed at 300 pixels."</font></i>  <i><font color="gray">Although the Faster R-CNN model has performed relatively well in this example, it is important to note that this meta-architecture is much slower than more modern approaches, such as R-FCN.</font></i> <i><font color="gray"><br></font></i>  <i><font color="gray"><b>Source</b> : <a href="https://arxiv.org/abs/1611.10012v1">Huang et al.</a></font></i>  <i><font color="gray"><a href="https://arxiv.org/abs/1611.10012v1">(2016, p. 9)</a></font></i> <br><br>  The aforementioned <a href="https://arxiv.org/abs/1611.10012v1">scientific article</a> provides a detailed comparison of the performance of the R-FCN, SSD and Faster R-CNN.  Due to the difficulty of comparing machine learning techniques, we would like to point out the merits of creating a standardized approach described by the authors.  They view these architectures as ‚Äúmeta-architectures,‚Äù because they can be combined with different feature extraction blocks, such as ResNet or Inception. <br><br>  The authors study the trade-offs between accuracy and speed in various meta-architectures, feature extraction blocks and resolutions.  For example, the choice of a feature extraction block greatly changes the results of work on various meta-architectures. <br><br>  In scientific articles describing <a href="https://arxiv.org/abs/1612.01051v2">SqueezeDet</a> and <a href="https://arxiv.org/abs/1611.08588v2">PVANet</a> , the necessity of a compromise between the tendency to increase application speed with a decrease in consumed computing resources is emphasized again and the accuracy that is required for real-time commercial applications, especially in unmanned vehicle applications.  Although the Chinese company DeepGlint showed a good example of detecting objects in real time in the stream from a video surveillance camera. <br><br>  <b>Object Detection, Object Tracking and Face Recognition in DeepGlint</b> <br><iframe width="560" height="315" src="https://www.youtube.com/embed/xhp47v5OBXQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <b>ILSVRC and COCO Detection Challenge Results</b> <br>  <a href="http://mscoco.org/">COCO</a> (Common Objects in Context) is another popular image data set.  However, it is relatively smaller in size and more closely supervised than alternatives like ImageNet.  It aims to recognize objects with a broader context of understanding the scene.  The organizers hold an annual competition for the detection of objects, segmentation and key points.  Here are the results from the <a href="http://image-net.org/challenges/LSVRC/2016/results">ILSVRC</a> and <a href="http://mscoco.org/dataset/%2523detections-leaderboard">COCO</a> competitions for object detection: <br><br><ul><li>  <b>ImageNet LSVRC, Object Detection (DET)</b> : CUImage system showed 66% meanAP.  Won in 109 out of 200 object categories. </li><li>  <b>ImageNet LSVRC, object detection on video (VID)</b> : NUIST 80.8% meanAP </li><li>  <b>ImageNet LSVRC, object detection on video with tracking</b> : CUvideo 55.8% meanAP </li><li>  <b>COCO 2016, object detection (bounding box)</b> : G-RMI (Google) 41.5% AP (absolute increase of 4.2 pp compared to the winner of 2015 - MSRAVC) </li></ul><br>  In a review of the results shown by object detection systems of 2016, ImageNet <a href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">writes</a> that MSRAVC 2015 set a very high level of performance (the first appearance of ResNet networks at this competition).  System performance has improved in all grades.  In both competitions, localization has greatly improved.  A significant improvement has been achieved on small objects. <br><br>  <b>Fig.</b>  <b>5</b> : Results of detection systems on images in the ILSVRC competition (2013‚Äì2016) <br><img src="https://habrastorage.org/getpro/habr/post_images/17d/d6c/5d3/17dd6c5d391f4be0aadb4b834252fe83.png"><br>  <i><font color="gray"><b>Note</b> : Results of detection systems on images in the ILSVRC competition (2013‚Äì2016).</font></i>  <i><font color="gray"><b>Source</b> : ImageNet 2016, online presentation, slide 2, <a href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">pdf</a></font></i> <br><br><h1>  Object tracking </h1><br>  It refers to the process of tracking a specific object of interest or several objects in a given scene.  Traditionally, this process is used in video applications and systems of interaction with the real world, where observations are made after the detection of the original object.  For example, the process is crucial for unmanned vehicle systems. <br><br><ul><li>  <b>‚Äú <a href="https://arxiv.org/abs/1606.09549v2">Fully convolutional Siamese tracking networks</a> ‚Äù</b> combines a basic tracking algorithm with a Siamese network trained from start to finish that reaches record levels in its field and runs frame by frame at a rate that is greater than what is needed for real-time applications.  This paper attempts to overcome the lack of functional richness available to tracking models from traditional online learning methods. </li><li>  <b>‚Äú <a href="https://arxiv.org/abs/1604.01802v2">Teaching deep regression networks to track objects at 100 FPS</a> ‚Äù</b> is another article whose authors are trying to overcome existing problems with the help of online teaching methods.  The authors have developed a tracker that uses a network with a mechanism for predicting events (feed-forward network) to assimilate common relationships in connection with the movement of an object, its appearance and orientation.  This allows you to effectively track new objects <u>without online learning</u> .  Shows a record result in a standard benchmark tracking, while at the same time allowing ‚Äúto track common objects at 100 FPS‚Äù. <br><br>  <b>GOTURN (Generic Object Tracking Using Regression Networks) video</b> <br><iframe width="560" height="315" src="https://www.youtube.com/embed/kMhwXnLgT_I" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></li><li>  The work <b>‚Äú <a href="https://arxiv.org/abs/1612.06615v1">Deep motion signs for visual tracking</a> ‚Äù</b> combines hand-written signs, deep RGB / appearance signs (from CNN), as well as deep signs of movement (trained on an optical image stream) to achieve record numbers.  Although deep signs of movement are commonplace in action recognition and video classification systems, the authors state that they are first used for visual tracking.  The article received the award as the <u>best article</u> at the ICPR 2016 conference, in the section ‚ÄúComputer vision and vision of robots‚Äù. <br><br>  <i>‚ÄúThis article is a study of the influence of deep signs of movement in a tracking framework through detection.</i>  <i>Further, we show that additional information contains handwritten signs, deep RGB signs, and deep signs of movement.</i>  <i>As far as we know, we are the first to propose to combine information on appearance with deep signs of movement for visual tracking.</i>  <i>Comprehensive experiments clearly assume that our mixed approach with deep signs of movement is superior to standard methods that rely only on appearance information. ‚Äù</i> </li><li>  The article <b>‚Äú <a href="https://arxiv.org/abs/1605.06457v1">Virtual Worlds as an Intermediate for Analyzing the Tracking of Multiple Objects</a> ‚Äù is</b> devoted to the problem of the absence of inherent real world variability in existing benchmarks and data sets for video tracking.  The article proposes a new method of cloning the real world by generating from scratch saturated, virtual, synthetic, photorealistic environments with full label coverage.  This approach solves some of the problems of sterility that are present in existing data sets.  The generated images are automatically tagged with exact labels, which allows them to be used for a variety of applications, in addition to detecting and tracking objects. </li><li>  <b>" <a href="https://arxiv.org/abs/1612.08274v1">Global optimal tracking of objects with fully convolutional networks</a></b> . <b>"</b>  Object diversity and interference are discussed here as the two root causes of constraints in object tracking systems.  <i>"The proposed method solves the problem of the diversity of the appearance of objects with the help of a fully convolutional network, and also works with the problem of interference by dynamic programming</i> . <i>"</i> </li></ul></li></ul></div><p>Source: <a href="https://habr.com/ru/post/346140/">https://habr.com/ru/post/346140/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346130/index.html">So why aren't you participating in the development of open source software?</a></li>
<li><a href="../346132/index.html">Stimulus 1.0: a modest JavaScript HTML framework that you already have</a></li>
<li><a href="../346134/index.html">Genetic algorithm for constructing algorithms</a></li>
<li><a href="../346136/index.html">L√ñVE Development</a></li>
<li><a href="../346138/index.html">Game Design Techniques: Mixing</a></li>
<li><a href="../346142/index.html">Hot reboot of components in React</a></li>
<li><a href="../346144/index.html">Mobile devices from the inside. Memory markup, file structure description and markup memory</a></li>
<li><a href="../346146/index.html">Python, under the pirate flag</a></li>
<li><a href="../346148/index.html">Configure Azure Application Insights for diagnosing software with microservice architecture</a></li>
<li><a href="../346152/index.html">The head of Intel suspected of selling the company's shares at $ 24 million due to the vulnerability of processors</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>