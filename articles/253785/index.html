<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Testing flash storage. IBM FlashSystem 840</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last year, they wrote about testing the IBM RamSan FlashSystem 820 . But this time, thanks to one large customer, IBM FlashSystem 840 fell into our ha...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Testing flash storage. IBM FlashSystem 840</h1><div class="post__text post__text-html js-mediator-article">  Last year, they wrote about testing the <a href="http://habrahabr.ru/company/inline_tech/blog/227887/">IBM RamSan FlashSystem 820</a> .  But this time, thanks to one large customer, <a href="http://www-03.ibm.com/systems/ru/storage/flash/840/">IBM FlashSystem 840</a> fell into our hands <a href="http://www-03.ibm.com/systems/ru/storage/flash/840/">. For</a> about a year, the system, ‚Äúchildhood diseases‚Äù, is already behind.  it's time to evaluate her professional capabilities. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e3b/17a/e15/e3b17ae150e5fd5dee2f953c338b287d.jpg" height="212" width="320"></div><br><h3>  <b>Testing method</b> </h3><br>  During testing, the following tasks were solved: <br><ul><li>  studies of the process of storage degradation during long-term write write (Write Cliff) and the effects of storage density; </li><li>  IBM FlashSystem840 storage performance research under various load profiles; </li></ul><a name="habracut"></a><br><h4>  Testbed Configuration </h4><br>  For testing, at the Customer‚Äôs site, 2 different stands were successively assembled. <br>  <u>For tests of groups 1 and 2, the</u> load is generated by a single server, and the stand has the form shown in the figure: <br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6b3/72c/948/6b372c9484763658dd50349249a4771f.jpg" height="138" width="640"></div></td></tr><tr><td>  Figure 1. Block diagram of the test stand 1. </td></tr></tbody></table><br>  Server: IBM 3850X5, connected directly by eight 8Gb FC connections to.  <abbr title="Model: FlashSystem 840 (9840-AE1) Unit Size: 2U Installed Flash Media: 4TB eMLC Flash Module - 8 pcs. RAID Level: RAID 5 Raw Capacity: 32TB Formatted Capacity: 24TB Connection Interfaces: FC 8Gb - up to 16 pcs (per 8) Firmware version: 1.1.2.6">Storage IBM FlashSystem 840</abbr> <br><br>  <u>For group 3 tests</u> , an IBM 3650M4 server is added to the described bench, also connected directly to the IBM Flash System 840 storage system. At this stage, each server is connected to the storage system via four optical links. <br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a14/60d/976/a1460d976a655cafe51c80c9f79ad1b7.jpg" height="248" width="640"></div></td></tr><tr><td>  Figure 2. Block diagram of the test bench 2. </td></tr></tbody></table>  As additional software, Symantec Storage Foundation 6.1 is installed on the test server, which implements: <br><ul><li>  Functional logical volume manager (Veritas Volume Manager); </li><li>  Functional fault-tolerant connection to disk arrays (Dynamic Multi Pathing) </li></ul><br><div class="spoiler">  <b class="spoiler_title">See tiresome details and all sorts of clever words.</b> <div class="spoiler_text">  On the test server, the following settings were made to reduce disk I / O latency: <br><ul><li> Changed the I / O scheduler from <code>cfq</code> to <code>noop</code> by assigning the <code>noop</code> value <code>noop</code> the <code>scheduler</code> parameter of the <code>Symantec VxVolume</code> </li><li>  The following parameter has been added to <code>/etc/sysctl.conf</code> minimizes the queue size at the level of the Symantec logical volume manager: <code>vxvm.vxio.vol_use_rq = 0</code> ; </li><li>  Increased the limit of simultaneous I / O requests to the device to 1024 by assigning the value 1024 to the <code>nr_requests</code> parameter of the <code>Symantec VxVolume</code> ; </li><li>  Disabled checking for the possibility of merging I / O operations (iomerge) by assigning the value 1 to the <code>nomerges</code> parameter of the <code>Symantec VxVolume</code> ; </li><li>  The queue size on FC adapters has been increased by adding the options <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> <code>/etc/modprobe.d/modprobe.conf</code> configuration file <code>ql2xmaxqdepth=64 (options qla2xxx ql2xmaxqdepth=64)</code> ; </li></ul><br>  On the storage system, the following configuration settings are performed for partitioning disk space: <br><ul><li>  The configuration of Flash modules is implemented RAID5; </li><li>  For tests of groups 1 and 2, 8 LUNs of the same volume are created on the storage system with a total volume covering the entire usable capacity of the disk array.  Block size LUN - 512byte.  Created LUNs are presented to one test server.  For tests of group 3, 16 LUNs of the same volume are created with a total volume covering the entire usable capacity of the disk array.  The created LUNs are presented in 8 pieces to each of the 2 test servers. <br></li></ul><br><h4>  Software used in the testing process </h4><br>  To create a synthetic load (performance of synthetic tests) on the storage system, the Flexible IO Tester (fio) version 2.1.4 utility is used.  All synthetic tests use the following fio configuration parameters of the [global] section: <br><ul><li> <code>thread=0</code> </li> <li> <code>direct=1</code> </li> <li> <code>group_reporting=1</code> </li> <li> <code>norandommap=1</code> </li> <li> <code>time_based=1</code> </li> <li> <code>randrepeat=0</code> </li> <li> <code>ramp_time=10</code> </li> </ul><br>  The following utilities are used to remove performance indicators under synthetic load: <br><ul><li>  <code>iostat</code> , part of the <code>sysstat</code> version 9.0.4 package with <code>txk</code> keys; </li><li>  <code>vxstat</code> , which is part of Symantec Storage Foundation 6.1 with <code>svd</code> keys; </li><li>  <code>vxdmpadm</code> , part of Symantec Storage Foundation 6.1 with the <code>-q iostat</code> keys; </li><li>  <code>fio</code> version 2.1.4, to generate a summary report for each load profile. </li></ul><br>  The removal of performance indicators during the test with the utilities <code>iostat, vxstat, vxdmpstat</code> performed at intervals of 5 seconds. <br></div></div><br><h3>  Testing program. </h3><br>  Tests are performed by creating a synthetic load on the block device (fio), which is a <code>stripe, 8 column, stripe unit size=1MiB</code> logical volume <code>stripe, 8 column, stripe unit size=1MiB</code> , created using Veritas Volume Manager from 8 LUNs presented from the system under test. <br>  Testing consisted of 3 groups of tests: <br><div class="spoiler">  <b class="spoiler_title">Ask for details</b> <div class="spoiler_text"><h5>  <b>Group 1: Tests that implement a continuous load of random write type with a change in the size of the block I / O operations (I / O).</b> </h5><br>  When creating a test load, the following fio program parameters are used (in addition to those previously defined): <br><ul><li>  <code>rw=randwrite</code> ; </li><li>  <code>blocksize=4K</code> ; </li><li>  <code>numjobs=64</code> ; </li><li>  <code>iodepth=64</code> . </li></ul><br>  A group of tests consists of three tests that differ in the total volume of LUNs presented with the tested storage system and the size of a block of I / O operations: <br><ul><li>  The test for recording performed on a fully-marked storage system - the total volume of the presented LUNs is equal to the effective storage capacity of the storage system, the test duration is 18 hours; </li><li>  Recording tests with varying block size (4,8,16,32,64,1024K), performed on a fully-marked storage system, the duration of each test is 1 hour.  Pause between tests - 2 hours. </li><li>  Recording tests with a variable block size (4,8,16,32,64,1024K), performed on a storage system filled to 70%, the duration of each test is 1 hour.  Pause between tests - 2 hours.  For this test, 8 LUNs are created on the tested storage; their total capacity is 70% of the effective storage capacity.  Created LUNs are presented to the test server, where one of them using Symantec VxVM is collected the volume on which the test load falls. </li></ul><br>  Based on the test results, the following graphs are generated based on the data output by the vxstat command, combining the test results: <br><ul><li>  IOPS as a function of time; </li><li>  Bandwidth (BandWidth) as a function of time. </li><li>  Latency as a function of time. </li></ul><br>  The analysis of the received information is carried out and conclusions are drawn about: <br><ul><li>  The presence of performance degradation during long-term load on the record; </li><li>  The performance of the service processes storage (garbage collection) limiting the performance of the disk array to write during a long peak load; </li><li>  The degree of influence of the size of a block of I / O operations on the performance of storage service processes; </li><li>  The amount of space reserved for storage for leveling storage service processes. </li><li>  Influence of storage density on the performance of service processes. </li></ul><br><h5>  <b>Group 2: Performance tests of a disk array with different types of load generated by a single server, executed at the block device level.</b> </h5><br>  During testing, the following types of loads are investigated: <br><ul><li>  load profiles (changeable software parameters fio: <code>randomrw, rwmixedread</code> ): </li></ul><br><ol><li>  random recording 100%; </li><li>  random write 30%, random read 70%; </li><li>  random read 100%. </li></ol><br><ul><li>  block sizes: 1KB, 8KB, 16KB, 32KB, 64KB, 1MB (changeable software parameter fio: <code>blocksize</code> ); </li><li>  methods of processing I / O operations: synchronous, asynchronous (variable software parameter fio: <code>ioengine</code> ); </li><li>  the number of load generating processes: 1, 2, 4, 8, 16, 32, 64, 128 (changeable software parameter fio: <code>numjobs</code> ); </li><li>  queue depth (for asynchronous I / O operations): 32, 64 (changeable software parameter fio: <code>iodepth</code> ). </li></ul><br>  A test group consists of a set of tests representing all possible combinations of the above types of load.  To level the impact of the service processes of the storage system (garbage collection) on the test results between tests, a pause is realized equal to the amount of information recorded during the test divided by the performance of the service storage systems (determined by the results of the first group of tests). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Based on the test results, the following graphs are generated for each combination of the following load types based on the data output by the fio software after each of the tests: load profile, method of processing I / O operations, queue depth, which combine tests with different I / O block values : <br><ul><li>  IOPS as a function of the number of load generating processes; </li><li>  Bandwidth as a function of the number of processes that generate the load; </li><li>  Latitude (clat) as a function of the number of load generating processes; </li></ul><br>  The analysis of the obtained results is carried out, conclusions are drawn about the load characteristics of the disk array at latency less than or about 1ms, about the maximum performance indicators of the array about the performance of the array under single-threaded load.  It also determines the optimal block size for working with an array, as a unit in which it is possible to perform the maximum number of I / O operations, while transferring the maximum amount of data. <br><br><h5>  <b>Group 3: Disk array performance tests under different types of load generated by two servers, executed at the block device level;</b> </h5><br>  To perform tests of this group, one more server is added to the stand configuration.  The disk array is divided into 16 LUNs of the same size, totally occupying the entire storage volume.  Each server is presented 8 LUN.  Tests are conducted similarly to tests of group 2, with the exception of the fact that the load is generated simultaneously by two servers.  Estimated total performance obtained by both servers during each test.  At the end of the tests, the conclusion is made about the impact of the number of servers generating the load on the storage performance. <br></div></div><br><h2>  <b>Test results</b> </h2><br><h5>  <b>Group 1: Tests that implement a continuous load of random write type with a change in the size of the block I / O operations (I / O).</b> </h5><br>  <b>Conclusions:</b> <br>  1. With a long load on the recording at a certain point in time, a significant degradation of storage performance is recorded (Figure 3).  A drop in performance is expected and is a feature of the SSD (write cliff) operation, associated with the inclusion of garbage collection (GC) processes and the limited performance of these processes.  The performance of the disk array, fixed after the write cliff effect (after a drop in performance), can be considered as the maximum average performance of the disk array. <br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5c0/23f/b2d/5c023fb2d4713bc88de191750694124e.jpg" height="640" width="472"></div></td></tr><tr><td>  Figure 3. Changing the speed of I / O operations (iops), data transfer speeds (bandwidth) and delays (Latency) during long-term recording by the 4K block </td></tr></tbody></table><br>  2. The block size with long write load affects the performance of the GC process.  So with small blocks (4K) the speed of work of GC is 640 MBytes / s, on medium and large blocks (16K-1M) CG works at a speed of about 1200Mbytes / s. <br><br>  3. The difference in the values ‚Äã‚Äãof the maximum storage operation time at peak performance recorded during the first long test and the subsequent equivalent test with the 4K unit is due to the fact that the storage system was not completely filled before the start of testing. <br><br>  4. The maximum operating time of the storage system at peak performance is significantly different with the 4K block and all other blocks, which is most likely caused by the storage space limit reserved for the execution of GC processes. <br><br>  5. About 2TB is reserved for the performance of service processes on the storage system. <br><br>  6. When tests on storage systems are 70% full, performance drops slightly later (about 10%).  There are no changes in the speed of the GC processes. <br><br><div class="spoiler">  <b class="spoiler_title">Charts and tables.</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text">  Block device performance graphs for different types of load generated by a single server. <br><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td></td><td>  Data transfer rate (bandwidth) </td><td>  I / V Speed ‚Äã‚Äã(IOPS) </td><td>  Latency </td></tr><tr><td>  Fully spaced storage (100% formated) </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3b2/b07/098/3b2b07098b6ad48b0aa458d9ea070398.jpg" height="123" width="200"></div></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc1/7a4/df6/bc17a4df6ae836123f519cd43c2d6263.jpg" height="200" width="155"></div></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/20c/cc1/d09/20ccc1d091dd74cd29707081dcb87fac.jpg" height="200" width="146"></div><br></td></tr><tr><td>  Not completely labeled storage (70% formated) </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/40e/f56/62b/40ef5662b93af4fb0a58b7d21eddc869.jpg" height="117" width="200"></div></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2c4/382/3e2/2c43823e21708cb1028d45d2a2f314d6.jpg" height="200" width="144"></div><br></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5ca/8ab/caf/5ca8abcafd691c03c62395c4400cbbf5.jpg" height="200" width="145"></div><br></td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="http://habrastorage.org/getpro/habr/post_images/aa3/886/1f9/aa38861f9fd3d0201524face9261266c.png" height="312" width="640"></div></td></tr><tr><td>  Table 1 The dependence of storage performance on the block size during long-term recording load. </td></tr></tbody></table></div></div><br><h5>  <b>Group 2: Performance tests of a disk array with different types of load generated by a single server, executed at the block device level.</b> </h5><br>  The main results of the tests presented in the graphs are tabulated. <br><div class="spoiler">  <b class="spoiler_title">Tables and graphs.</b>  <b class="spoiler_title">(All pictures are clickable)</b> <div class="spoiler_text"><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1ad/b98/385/1adb9838502aae13f07a3a9484848d37.png" height="538" width="640"></div></td></tr><tr><td>  Table 2 Storage performance with one load generating process (jobs = 1) </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bcb/d14/888/bcbd1488830e7d3e678127c1cdd8f1b9.png" height="536" width="640"></div></td></tr><tr><td>  Table 3 Maximum storage performance with delays less than 1ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="http://habrastorage.org/getpro/habr/post_images/ed7/963/63f/ed796363f0e4e0f5d3ac0af86317ede5.png" height="538" width="640"></div></td></tr><tr><td>  Table 4. Maximum storage performance with delays up to 3ms </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="http://habrastorage.org/getpro/habr/post_images/0db/ba9/016/0dbba9016f3e9f94b8829148728ba9c2.png" height="538" width="640"></div></td></tr><tr><td>  Table 5 Maximum storage performance with a different load profile. </td></tr></tbody></table><br>  Block device performance graphs for different types of load generated by two servers. <br>  (All pictures are clickable) <br><table border="1" cellpadding="2" cellspacing="2"><tbody><tr><td></td><td>  Synchronous way in / in </td><td>  Asynchronous way in / in with a queue depth of 32 </td><td>  Asynchronous way in / in with a queue depth of 64 </td></tr><tr><td>  Random reading </td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/1f1/98e/460/1f198e460faca855c434eccada710152.jpg" height="320" width="224"></a> <br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/e13/944/d36/e13944d36e09fe15c1b2b6b96bf0ff7a.jpg" height="320" width="222"></a> <br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/77d/4e5/f66/77d4e5f6653b5d6d33b514657caf2da2.jpg" height="320" width="218"></a> <br></td></tr><tr><td>  With random recording </td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/a65/976/42c/a6597642c83f6fae8b09cde37d601260.jpg" height="320" width="223"></a> <br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/f6b/2c3/d0f/f6b2c3d0f7fe9b031030ff521c8c8dbe.jpg" height="320" width="227"></a> <br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/c5b/a2a/3ae/c5ba2a3ae8e3ba6ac056c8660f902522.jpg" height="320" width="221"></a> <br></td></tr><tr><td>  With mixed load (70% read, 30% write) </td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/dbe/685/393/dbe685393e5e98123a19116b1bdd2ad6.jpg" height="320" width="214"></a> <br><br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/720/bc3/fbc/720bc3fbcd95a7e8ed4f20be51892058.jpg" height="320" width="208"></a> <br><br></td><td> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/d5f/8c7/a3f/d5f8c7a3f1adbf69473406aeccfe0e6b.jpg" height="320" width="219"></a> <br><br></td></tr></tbody></table><br></div></div><br><h4>  Conclusions: </h4><br>  1. Maximum recorded performance parameters for the storage system (from the average during the execution of each test - 3 min): <br><br>  Record: <br><ul><li>  415000 IOPS with latency 4,9ms (4KB async qd64 block) </li><li>  with synchronous I / O - 263,000 IOPS with latency 0,2ms (4K block) </li><li>  Bandwidth: 3600MB / c for large blocks </li></ul><br>  Reading: <br><ul><li>  475,000 IOPS with latency 4,3ms and 440,000 IOPS with latency 2,3 (8KB async qd64 block); </li><li>  with synchronous I / O - 225000 IOPS with latency of 0.26 ms (4K block) </li><li>  Bandwidth: 6290MB / s for large blocks </li></ul><br>  Mixed load (70/30 rw) <br><ul><li>  475,000 IOPS with latency 4,3ms (4KB async qd64 block); </li><li>  with synchronous I / O - 220000 IOPS with latency 0,27ms (4K block) </li><li>  Bandwidth 5135MB / s for large blocks. </li></ul><br>  Minimal latency fixed: <br><ul><li>  When recording - 0.177ms for 4K jobs = 32 sync </li><li>  When reading - 0,25ms for block 4K jobs = 32 sync </li></ul><br><br>  2. The storage system enters saturation mode at <br><ul><li>  Asynchronous I / O with 8 jobs on large blocks (32K-1M) and 16 jobs on small and medium blocks (4-16K). </li><li>  synchronous way in / in with 64 jobs. </li></ul><br>  3. On read operations with large blocks (16K-1M), a throughput of more than 6 GB / s was obtained, which roughly corresponds to the total throughput of the interfaces used when connecting the server to the storage system.  Thus, neither the storage controllers nor flash drives are the bottleneck of the system. <br><br>  4. An array with asynchronous i / v method produces 1.5‚Äì2 times more performance on small blocks (4-8K) than with a synchronous i / v method.  On large and medium blocks (16K-1M), the performance with synchronous and asynchronous I / V is approximately equal. <br><br>  5. The graphs below show the dependence of the maximum obtained performance indicators of the tested storage system (IOPS and data transfer rate) on the block size of I / O operations.  The nature of the graphs leads to the following conclusions: <br><ul><li>  when recording, the most effective unit for working with storage is the 8K unit. </li><li>  when reading in a synchronous manner, it is more advantageous to work in / in a 16K and 32K block </li><li>  when reading in an asynchronous manner, the best block is 16K. </li></ul><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7cf/953/1a1/7cf9531a158c2d4418bbf7afc8ac0380.jpg" height="546" width="640"></div></td></tr><tr><td>  Maximum performance in reading and writing in a synchronous manner in / in various block sizes. </td></tr></tbody></table><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8d6/3a0/67c/8d63a067cf8146496d64e82c12b3e9ac.jpg" height="540" width="640"></div></td></tr><tr><td>  Maximum performance in reading and writing asynchronously in / in (qd32) with a different block size. </td></tr></tbody></table><br><h5>  <b>Group 3: Disk array performance tests with different types of load generated by two servers, executed at the block device level.</b> </h5><br>  For each of the tests, a performance was obtained that coincided within the error of 5% with the results of tests of group 2, when the load was generated by one server.  We did not give graphics and performance data for the tests of group 3, as a result of their identity with the results of the second group. <br>  In other words, the study showed that the server is not the ‚Äúbottleneck‚Äù of the test bench. <br><br><h2>  <b>findings</b> </h2><br>  In general, the system showed excellent results.  We were unable to identify obvious bottlenecks and obvious problems.  All results are stable and predictable.  Comparing with our previous testing of IBM FlashSystem 820, it is worth noting differences management interfaces.  The 820th model is controlled by the sometimes inconvenient java applet inherited from the Texas Instruments RamSan 820.  At the same time, the 840th has a web-interface resembling the XIV Storage System and Storwize already familiar to IBM products.  Working with them is noticeably nicer and, ultimately, faster. <br>  In addition, IBM FlashSystem 840 acquired the necessary hot-swap functionality for all components and microcode update on the fly for enterprise-class devices.  The choice of possible connection interfaces and configurations of flash modules has significantly expanded. <br><br>  The disadvantages, perhaps, include the presence of performance degradation during long recording.  Although, it is rather a disadvantage of today's flash memory technologies, manifested in the fact that the manufacturer did not artificially limit the speed of the system.  Even with a long maximum load on the recording and after a drop in performance, the storage system showed remarkable results. <br><br><br>  <font color="green">PS The author expresses cordial thanks to Pavel Katasonov, Yuri Rakitin and all other company employees who participated in the preparation of this material.</font> </div><p>Source: <a href="https://habr.com/ru/post/253785/">https://habr.com/ru/post/253785/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../253775/index.html">Yandex.Browser: the interface of the future is now in beta</a></li>
<li><a href="../253777/index.html">Runscope: convenient to test API</a></li>
<li><a href="../253779/index.html">Interesting moments of LINQ to SQL. Again</a></li>
<li><a href="../253781/index.html">The use of MEMS gyroscopes and accelerometers to track human body movements</a></li>
<li><a href="../253783/index.html">Backing up to the cloud with the help of a new feature Veeam</a></li>
<li><a href="../253787/index.html">10 things you didn't know about java</a></li>
<li><a href="../253791/index.html">A short course in computer graphics, addendum: GLSL</a></li>
<li><a href="../253793/index.html">"MOEX Code". Hackathon</a></li>
<li><a href="../253795/index.html">Cognitive resistance rules and instructions</a></li>
<li><a href="../253799/index.html">zabbix_sender over HTTP - how to send data to Zabbix via HTTP | S</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>