<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Nonlinear regression in Apache Spark. We develop with our own hands</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When solving signal processing problems, the approximation of raw data by a regression model is often used. Based on the structure, the models can be ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Nonlinear regression in Apache Spark. We develop with our own hands</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/806/f9d/3f4/806f9d3f420f4ad29d30fbddf489abb7.png"><br><br>  When solving signal processing problems, the approximation of raw data by a regression model is often used.  Based on the structure, the models can be divided into three types - linear, reducible to linear and non-linear.  In the ‚ÄúSpark ML‚Äù Apache Spark machine learning module, the functionality for the first two types is represented by the classes <strong>LinearRegression</strong> and <strong>GeneralizedLinearRegression,</strong> respectively.  Training non-linear models in the standard library is not represented and requires self-development. <br><a name="habracut"></a><br>  First, we briefly consider the theoretical foundations of the construction of nonlinear models, and then proceed to the practical development of the expansion Spark ML. <br><br><h3>  A bit of math </h3><br>  Learning nonlinear models is more complicated than linear ones.  This may be due to the presence of more than one extremum and / or the "ravine" nature of the response surface.  The main incentive to use nonlinear functions is the possibility of obtaining more compact models.  It should also be noted that many analytical equations from the field of physics and engineering initially have a nonlinear form, so the use of appropriate models can be forced. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For teaching nonlinear models, there is a fairly wide range of tools, the choice of which depends on the specific type of function, the presence and type of restrictions used, etc. The article will use the widely presented quadratic error function and the Newton-Gauss method - the first-order quasi algorithm -Newton type.  This algorithm has a fairly good convergence in most cases. <br><br>  The iteration step in the Newton-Gauss method is determined by the relation: <br><br><img src="https://tex.s2cms.ru/svg/d%3D-(J%5ET%20%20J)%5E%7B-1%7D%20J%5ET%20%20r" alt="d = - (J ^ T J) ^ {- 1} J ^ T r">  where <em>J</em> is the Jacobi matrix, <em>r</em> is the residual column vector <img src="https://tex.s2cms.ru/svg/r_i%3Dy_i-f(w%3B%20x_i)" alt="r_i = y_i-f (w; x_i)">  . <br><br>  This formula logically consists of two parts: approximation of the Hesse matrix <img src="https://tex.s2cms.ru/svg/H%20%5Capprox%20(J%5ET%20%20J)" alt="H \ approx (J ^ T J)">  and gradient approximation <img src="https://tex.s2cms.ru/svg/%5Cnabla%20f%20%5Capprox%20J%5ET%20%20r" alt="\ nabla f \ approx J ^ T r">  . <br><br>  The number of rows of the Jacobi matrix is ‚Äã‚Äãdetermined by the number of training examples <em>n</em> , the number of columns is determined by the size of the weights vector <em>m</em> .  As shown in [1], the approximation of the Hessian matrix can be calculated by reading two rows of the matrix of the Jacobi matrix and multiplying them.  Received <img src="https://tex.s2cms.ru/svg/n%5E2" alt="n ^ 2">  matrix size <img src="https://tex.s2cms.ru/svg/m%20%5Ctimes%20m" alt="m \ times m">  it only remains to add.  The proposed rearrangement of operations does not alter the total computational complexity, but allows us not to load the entire Jacobi matrix into memory and conduct the calculation in parallel.  Similarly, the calculation of the approximation of the gradient is carried out only the addition are subject to <em>n</em> vectors of length <em>m</em> .  The inversion of the resulting Hesse matrix is ‚Äã‚Äãnot very complex due to the relatively small size.  To ensure the convergence of the algorithm, it is necessary to follow the positive definiteness of the calculated matrix <img src="https://tex.s2cms.ru/svg/(J%5ET%20%20J)" alt="(J ^ T J)">  that is implemented by calculating eigenvalues ‚Äã‚Äãand vectors. <br><br>  In articles [2, 3], a general scheme was proposed for applying the approach described above for Apache Spark.  The only flaw in these works, in my opinion, is the lack of a clear link with the existing Spark ML API.  We will try to fill this gap in the next section. <br><br><h3>  Implementing the Spark ML API </h3><br>  For the successful implementation of nonlinear models, we need to understand the structure and purpose of the base classes.  The Spark machine learning API has two versions: 1.x located inside the <em>mllib</em> package, 2.x in the <em>ml</em> package.  The documentation for the Spark ML module [4] indicates that the transition from API version 1.x to version 2.x is intended to provide the possibility of embedding into Pipelines and working with a typed DataFrame structure.  In our example, we will use a newer class structure, but if necessary, it can be quite simply implemented under the old structure. <br><br><h4>  Important Spark API Classes </h4><br><ul><li>  class <em>org.apache.spark.ml.feature.Instance</em> describes an instance of a training example that includes a real label, an example weight coefficient, and a feature value vector; </li><li>  <em>org.apache.spark.ml.regression. {Regressor, RegressionModel}</em> are the key classes that need to be extended.  The first is a model builder, and the second is a trained model. </li><li>  With the help of the treit introduced by us <em>org.apache.spark.ml.regression.NonLinearFunction</em> , a non-linear function contract is defined, for which the goal is to choose a weight vector.  There are only 3 methods in the contract: <em>eval</em> returns the value of the function at a point, <em>grad</em> is the gradient vector, and <em>dim</em> is the dimension of the model (the length of the weight vector). </li><li>  The library of linear algebra Breeze [5] takes over operations with matrices and has ready-made implementations of optimization functions.  To use the Newton-Gauss algorithm or another first-order algorithm, we need to implement the breeze.optimize.DiffFunction <em>treit</em> in our loss function.  The <em>calculate</em> method for the transmitted coefficient vector should return a tuple of two values: the value of the penalty function and the vector of its gradient at a point. </li></ul><br><h4>  Regressor class extension </h4><br>  The <em>org.apache.spark.ml.regression.NonLinearRegression</em> class extends the <em>Regressor</em> contract and, as a result of the training, returns an instance of <em>NonLinearRegressionModel</em> .  To create a non-linear model, you need to specify a unique string identifier and the core function of the <em>NonLinearFunction</em> model.  From the optional parameters, you can list: the maximum number of learning iterations, the initial approximation of the vector of coefficients and the required accuracy.  Nonlinear functions often have many extremes and the choice of the initial approximation, based on a priori ideas about the behavior of a particular nuclear function, allows you to direct the search to the global extremum.  It is worth noting the use of ready-made implementation of the Broyden-Fletcher-Goldfarb-Shanno algorithm with limited memory consumption (LBFGS) from the Breeze library.  The model learning code is shown below. <br><br><div class="spoiler">  <b class="spoiler_title">Code org.apache.spark.ml.regression.NonLinearRegression # train</b> <div class="spoiler_text"><pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-keyword"><span class="hljs-keyword">protected</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span></span>(dataset: <span class="hljs-type"><span class="hljs-type">Dataset</span></span>[_]): <span class="hljs-type"><span class="hljs-type">NonLinearRegressionModel</span></span> = { <span class="hljs-comment"><span class="hljs-comment">// set instance weight to 1 if not defined the column val instanceWeightCol: Column = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol)) val instances: RDD[Instance] = dataset.select( col($(labelCol)).cast(DoubleType), instanceWeightCol, col($(featuresCol))).rdd.map { case Row(label: Double, weight: Double, features: Vector) =&gt; Instance(label, weight, features) } // persists dataset if defined any storage level val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK) val costFunc = new SquaresLossFunctionRdd(kernel, instances) val optimizer = new LBFGS[BDV[Double]]($(maxIter), 10, $(tol)) // checks and assigns the initial coefficients val initial = { if (!isDefined(initCoeffs) || $(initCoeffs).length != kernel.dim) { if ($(initCoeffs).length != kernel.dim) logWarning(s"Provided initial coefficients vector (${$(initCoeffs)}) not corresponds with model dimensionality equals to ${kernel.dim}") BDV.zeros[Double](kernel.dim) } else BDV($(initCoeffs).clone()) } val states = optimizer.iterations(new CachedDiffFunction[BDV[Double]](costFunc), initial) val (coefficients, objectiveHistory) = { val builder = mutable.ArrayBuilder.make[Double] var state: optimizer.State = null while (states.hasNext) { state = states.next builder += state.adjustedValue } // checks is method failed if (state == null) { val msg = s"${optimizer.getClass.getName} failed." logError(msg) throw new SparkException(msg) } (Vectors.dense(state.x.toArray.clone()).compressed, builder.result) } // unpersists the instances RDD if (handlePersistence) instances.unpersist() // produces the model and saves training summary val model = copyValues(new NonLinearRegressionModel(uid, kernel, coefficients)) val (summaryModel: NonLinearRegressionModel, predictionColName: String) = model.findSummaryModelAndPredictionCol() val trainingSummary = new NonLinearRegressionSummary( summaryModel.transform(dataset), predictionColName, $(labelCol), $(featuresCol), objectiveHistory, model ) model.setSummary(trainingSummary) }</span></span></code> </pre> <br></div></div><br>  The code for the <em>train</em> method can be divided into three parts: obtaining training examples from a data set;  initiation of the penalty function and finding the optimal solution;  saving the vector of coefficients and learning outcomes in the model instance. <br><br><h4>  RegressionModel class extension </h4><br>  The implementation of the <em>org.apache.spark.ml.regression.NonLinearRegressionModel</em> class is rather trivial.  The predict method uses the kernel function to get the value at the point: <br><br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-keyword"><span class="hljs-keyword">protected</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict</span></span></span></span>(features: <span class="hljs-type"><span class="hljs-type">Vector</span></span>): <span class="hljs-type"><span class="hljs-type">Double</span></span> = { kernel.eval(coefficients.asBreeze.toDenseVector, features.asBreeze.toDenseVector) }</code> </pre><br>  The only requirement Spark ML API, which is worth paying attention to - the requirement for model serializability.  The necessary behavior is provided in the companion object by extending the abstract classes <em>org.apache.spark.ml.util. {MLReader, MLWriter}</em> [6].  The state of the trained model consists of two parts: the coefficient vector and the kernel function.  If with the vector of coefficients everything is already invented before us, then with the core function it is somewhat more complicated.  You cannot save the kernel function directly in the DataFrame, but there are several alternatives. <br><br>  For simplicity, the binary serialization option of the function in the Base64 string was chosen.  The disadvantages include the inaccessibility of a person reading a received line, as well as the need to support versioning of implementations. <br><br>  A much more promising approach is to preserve the function in a symbolic form.  This can be done in the image of the objects of class formula of the stats package in the R language, for example, <em>log (y) ~ a + log (x)</em> .  This method is more complicated than the first, but it solves a number of problems: a human-readable representation of functions and the possibility of deserialization by different versions of parsers while maintaining backward compatibility.  Significant complexity here is the development of a fairly flexible parser of symbolic expressions of functions. <br><br>  Perhaps a useful improvement will be the ability to select a step for the numerical differentiation of the core function.  This does not significantly affect the complexity of saving the model. <br><br><h4>  Realization of quadratic loss function </h4><br>  The final element required for learning is the loss function.  In our example, a quadratic loss function is used in the form of two realizations.  In one, the training examples are specified as a Breeze matrix, in the other, as a Spark structure RDD [Instance].  The first implementation is easy to understand (directly uses matrix expressions) and is suitable for small training sets.  It serves as a test bench for us. <br><br><div class="spoiler">  <b class="spoiler_title">Org.apache.spark.ml.regression.SquaresLossFunctionBreeze code</b> <div class="spoiler_text"><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> org.apache.spark.ml.regression <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> breeze.linalg.{<span class="hljs-type"><span class="hljs-type">DenseMatrix</span></span> =&gt; <span class="hljs-type"><span class="hljs-type">BDM</span></span>, <span class="hljs-type"><span class="hljs-type">DenseVector</span></span> =&gt; <span class="hljs-type"><span class="hljs-type">BDV</span></span>} <span class="hljs-comment"><span class="hljs-comment">/** * Breeze implementation for the squares loss function. * * @param fitmodel concrete model implementation * @param xydata labeled data combined into the matrix: * the first n-th columns consist of feature values, (n+1)-th columns contains labels */</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SquaresLossFunctionBreeze</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">val fitmodel: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">NonLinearFunction</span></span></span></span><span class="hljs-class"><span class="hljs-params">, xydata: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">BDM</span></span></span></span><span class="hljs-class"><span class="hljs-params">[</span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Double</span></span></span></span><span class="hljs-class"><span class="hljs-params">]</span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SquaresLossFunction</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">/** * The number of instances. */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> instanceCount: <span class="hljs-type"><span class="hljs-type">Int</span></span> = xydata.rows <span class="hljs-comment"><span class="hljs-comment">/** * The number of features. */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> featureCount: <span class="hljs-type"><span class="hljs-type">Int</span></span> = xydata.cols - <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-comment"><span class="hljs-comment">/** * Feature matrix. */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">X</span></span>: <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = xydata(::, <span class="hljs-number"><span class="hljs-number">0</span></span> until featureCount) <span class="hljs-comment"><span class="hljs-comment">/** * Label vector. */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> y: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = xydata(::, featureCount) <span class="hljs-comment"><span class="hljs-comment">/** * The model dimensionality (the number of weights). * * @return dimensionality */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dim</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">Int</span></span> = fitmodel.dim <span class="hljs-comment"><span class="hljs-comment">/** * Evaluates loss function value and the gradient vector * * @param weights weights * @return (loss function value, gradient vector) */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calculate</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): (<span class="hljs-type"><span class="hljs-type">Double</span></span>, <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]) = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> r: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = diff(weights) (<span class="hljs-number"><span class="hljs-number">0.5</span></span> * (rt * r), gradient(weights)) } <span class="hljs-comment"><span class="hljs-comment">/** * Calculates a positive definite approximation of the Hessian matrix. * * @param weights weights * @return Hessian matrix approximation */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hessian</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">J</span></span>: <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = jacobian(weights) posDef(<span class="hljs-type"><span class="hljs-type">Jt</span></span> * <span class="hljs-type"><span class="hljs-type">J</span></span>) } <span class="hljs-comment"><span class="hljs-comment">/** * Calculates the Jacobian matrix * * @param weights weights * @return the Jacobian */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">jacobian</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> gradData = (<span class="hljs-number"><span class="hljs-number">0</span></span> until instanceCount) map { i =&gt; fitmodel.grad(weights, <span class="hljs-type"><span class="hljs-type">X</span></span>(i, ::).t).toArray } <span class="hljs-type"><span class="hljs-type">BDM</span></span>(gradData: _*) } <span class="hljs-comment"><span class="hljs-comment">/** * Calculates the difference vector between the label and the approximated values. * * @param weights weights * @return difference vector */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">diff</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> diff = (<span class="hljs-number"><span class="hljs-number">0</span></span> until instanceCount) map (i =&gt; fitmodel.eval(weights, <span class="hljs-type"><span class="hljs-type">X</span></span>(i, ::).t) - y(i)) <span class="hljs-type"><span class="hljs-type">BDV</span></span>(diff.toArray) } <span class="hljs-comment"><span class="hljs-comment">/** * Calculates the gradient vector * * @param weights weights * @return gradient vector */</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gradient</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> <span class="hljs-type"><span class="hljs-type">J</span></span>: <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = jacobian(weights) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> r = diff(weights) <span class="hljs-number"><span class="hljs-number">2.0</span></span> * <span class="hljs-type"><span class="hljs-type">Jt</span></span> * r } }</code> </pre><br></div></div><br>  The second option is designed to run in a distributed environment.  For the calculation, the <em>RDD.treeAggregate</em> function is <em>used</em> , which allows implementing the algorithm in the ‚ÄúMap-Reduce‚Äù style. <br><br><div class="spoiler">  <b class="spoiler_title">Code org.apache.spark.ml.regression.SquaresLossFunctionRdd</b> <div class="spoiler_text"><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">package</span></span> org.apache.spark.ml.regression <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> breeze.linalg.{<span class="hljs-type"><span class="hljs-type">DenseMatrix</span></span> =&gt; <span class="hljs-type"><span class="hljs-type">BDM</span></span>, <span class="hljs-type"><span class="hljs-type">DenseVector</span></span> =&gt; <span class="hljs-type"><span class="hljs-type">BDV</span></span>} <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.broadcast.<span class="hljs-type"><span class="hljs-type">Broadcast</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.ml.feature.<span class="hljs-type"><span class="hljs-type">Instance</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.rdd.<span class="hljs-type"><span class="hljs-type">RDD</span></span> <span class="hljs-comment"><span class="hljs-comment">/** * Spark RDD implementation for the squares loss function. * * @param fitmodel concrete model implementation * @param xydata RDD with instances */</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SquaresLossFunctionRdd</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">val fitmodel: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">NonLinearFunction</span></span></span></span><span class="hljs-class"><span class="hljs-params">, val xydata: </span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">RDD</span></span></span></span><span class="hljs-class"><span class="hljs-params">[</span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Instance</span></span></span></span><span class="hljs-class"><span class="hljs-params">]</span></span></span><span class="hljs-class">) </span><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extends</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">SquaresLossFunction</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-comment"><span class="hljs-comment">/** * The model dimensionality (the number of weights). * * @return dimensionality */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dim</span></span></span></span>: <span class="hljs-type"><span class="hljs-type">Int</span></span> = fitmodel.dim <span class="hljs-comment"><span class="hljs-comment">/** * Evaluates loss function value and the gradient vector * * @param weights weights * @return (loss function value, gradient vector) */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">calculate</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): (<span class="hljs-type"><span class="hljs-type">Double</span></span>, <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]) = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bcW: <span class="hljs-type"><span class="hljs-type">Broadcast</span></span>[<span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]] = xydata.context.broadcast(weights) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> (f: <span class="hljs-type"><span class="hljs-type">Double</span></span>, grad: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]) = xydata.treeAggregate((<span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-type"><span class="hljs-type">BDV</span></span>.zeros[<span class="hljs-type"><span class="hljs-type">Double</span></span>](dim)))( seqOp = (comb, item) =&gt; (comb, item) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> ((loss, oldGrad), <span class="hljs-type"><span class="hljs-type">Instance</span></span>(label, _, features)) =&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> featuresBdv = features.asBreeze.toDenseVector <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> w: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = bcW.value <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> prediction = fitmodel.eval(w, featuresBdv) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> addedLoss: <span class="hljs-type"><span class="hljs-type">Double</span></span> = <span class="hljs-number"><span class="hljs-number">0.5</span></span> * math.pow(label - prediction, <span class="hljs-number"><span class="hljs-number">2</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> addedGrad: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = <span class="hljs-number"><span class="hljs-number">2.0</span></span> * (prediction - label) * fitmodel.grad(w, featuresBdv) (loss + addedLoss, oldGrad + addedGrad) }, combOp = (comb1, comb2) =&gt; (comb1, comb2) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> ((loss1, grad1: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]), (loss2, grad2: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>])) =&gt; (loss1 + loss2, grad1 + grad2) }) (f, grad) } <span class="hljs-comment"><span class="hljs-comment">/** * Calculates a positive definite approximation of the Hessian matrix. * * @param weights weights * @return Hessian matrix approximation */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hessian</span></span></span></span>(weights: <span class="hljs-type"><span class="hljs-type">BDV</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]): <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = { <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> bcW = xydata.context.broadcast(weights) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> (hessian: <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>]) = xydata.treeAggregate(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>](dim, dim, <span class="hljs-type"><span class="hljs-type">Array</span></span>.ofDim[<span class="hljs-type"><span class="hljs-type">Double</span></span>](dim * dim)))( seqOp = (comb, item) =&gt; (comb, item) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> ((oldHessian), <span class="hljs-type"><span class="hljs-type">Instance</span></span>(_, _, features)) =&gt; <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> grad = fitmodel.grad(bcW.value, features.asBreeze.toDenseVector) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> subHessian: <span class="hljs-type"><span class="hljs-type">BDM</span></span>[<span class="hljs-type"><span class="hljs-type">Double</span></span>] = grad * grad.t oldHessian + subHessian }, combOp = (comb1, comb2) =&gt; (comb1, comb2) <span class="hljs-keyword"><span class="hljs-keyword">match</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> (hessian1, hessian2) =&gt; hessian1 + hessian2 } ) posDef(hessian) } }</code> </pre><br></div></div><br><h4>  Build project </h4><br>  To simplify development and testing from the original Spark ML project, we borrowed <em>pom.xml</em> with a slight modification.  We fix the version of Spark to one of the released ones, in our case <strong>2.0.1</strong> .  Attention should be paid to the inheritance of our POM file from <strong>org.apache.spark: spark-parent_2.11: 2.0.1</strong> , which allows us not to restart the Maven plug-in configuration. <br><br>  To run tests that require <em>SparkContext</em> , we add <strong>org.apache.spark</strong> to the test dependencies <strong>: spark-mllib_2.11: 2.0.1: test-jar</strong> : traits <em>org.apache.spark.mllib.util.MLlibTestSparkContext</em> , <em>org.apache.spark .ml.util.TempDirectory embed</em> in the appropriate test classes.  Also useful for testing may be extensions of the Suite classes from the <em>org.apache.spark</em> package that help to work with the context, for example, <em>SparkFunSuite</em> . <br><br><h3>  On the Rights of Conclusion </h3><br>  There are several points that are not covered in this article, but studying them is extremely interesting: <br><br><ul><li>  use of weighted training set; </li><li>  the use of constraints on the optimization domain, both soft (regularization) and hard (boundary conditions); </li><li>  evaluation of statistical indicators of the model (confidence intervals of coefficients, significance, etc.). </li></ul><br>  At the moment, I have not enough information on the aspects indicated above, but I will be grateful to all of the shared sources. <br><br>  The full code can be viewed on <a href="https://github.com/Unlocker/spark-mllib-ext/">Github</a> . <br><br>  Full and comprehensive testing of this solution is still to be done, so please treat the material as a concept and a topic for discussing improvements. <br><br>  For comments and suggestions, it is preferable to use private messages, comments are better suited for discussions. <br><br>  Thank you all for your attention. <br><br><h3>  Used materials </h3><br><ol><li>  <a href="http://ieeexplore.ieee.org/document/5451114/">ieeexplore.ieee.org/document/5451114</a> </li><li>  <a href="http://www.nodalpoint.com/nonlinear-regression-using-spark-part-1-nonlinear-models/">www.nodalpoint.com/nonlinear-regression-using-spark-part-1-nonlinear-models</a> </li><li>  <a href="http://www.nodalpoint.com/non-linear-regression-using-spark-part2-sum-of-squares">www.nodalpoint.com/non-linear-regression-using-spark-part2-sum-of-squares</a> </li><li>  <a href="http://spark.apache.org/docs/latest/ml-guide.html">spark.apache.org/docs/latest/ml-guide.html</a> </li><li>  <a href="https://github.com/scalanlp/breeze">github.com/scalanlp/breeze</a> </li><li>  <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-mllib/spark-mllib-pipelines-persistence.html">jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-mllib/spark-mllib-pipelines-persistence.html</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/316988/">https://habr.com/ru/post/316988/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../316974/index.html">Firebug development tool closes: ‚ÄúUse Firefox DevTools‚Äù</a></li>
<li><a href="../316976/index.html">Database of countries, regions and cities</a></li>
<li><a href="../316978/index.html">Dear javascript</a></li>
<li><a href="../316980/index.html">Formula of trust</a></li>
<li><a href="../316986/index.html">Tuning Swift compiler. Part 1</a></li>
<li><a href="../316990/index.html">STM32: FreeRTOS and Piezo Ceramic Emitter</a></li>
<li><a href="../316994/index.html">Bad advice or how to become a terrible leader.</a></li>
<li><a href="../316996/index.html">Creating a blog engine with Phoenix and Elixir / Part 5. Connecting ExMachina</a></li>
<li><a href="../317002/index.html">We write real Pointer Analysis for LLVM. Part 1: Introducing or first meeting with the world of program analysis</a></li>
<li><a href="../317004/index.html">RubyMine 2016.3: Debugging in attach mode, updated support for Puppet, SDK synchronization via rsync</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>