<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Mathematics for artificial neural networks for beginners, part 2 - gradient descent</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part 1 - linear regression 

 In the first part, I forgot to mention that if randomly generated data is not to your liking, then you can take any suit...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Mathematics for artificial neural networks for beginners, part 2 - gradient descent</h1><div class="post__text post__text-html js-mediator-article">  <a href="https://habrahabr.ru/post/307004/">Part 1 - linear regression</a> <br><br>  In the first part, I forgot to mention that if randomly generated data is not to your liking, then you can take any suitable example <a href="http://archive.ics.uci.edu/ml/datasets.html">from here</a> .  You can feel like a <a href="http://archive.ics.uci.edu/ml/datasets/Iris">nerd</a> , <a href="http://archive.ics.uci.edu/ml/datasets/Wine">winemaker</a> , <a href="http://archive.ics.uci.edu/ml/datasets/Online%2BRetail">seller</a> .  And all this without getting up from the chair.  In the presence of many data sets and one <a href="http://archive.ics.uci.edu/ml/citation_policy.html">condition</a> - when publishing, indicate where the data came from so that others could reproduce the results. <br><br><h4>  Gradient descent </h4><br>  In the last part, an example of calculating linear regression parameters using the least squares method was shown.  Parameters were found analytically - <img src="https://habrastorage.org/files/81a/74a/133/81a74a133511412cb86b9662f71973c3.png">  where <img src="https://habrastorage.org/files/d3f/9fa/906/d3f9fa90602d483faabc47523b69ed77.png">  - pseudoinverse matrix.  This solution is visual, accurate and short.  But there is a problem that can be solved numerically.  Gradient descent is a method of numerical optimization, which can be used in many algorithms where it is required to find the extremum of a function ‚Äî neural networks, SVM, k-means, regression.  However, it is easier to perceive in its pure form (and easier to modify). <br><a name="habracut"></a><br>  The problem is that the calculation of the pseudoinverse is very <a href="https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations">expensive</a> : 2 multiplications by <img src="https://habrastorage.org/files/29b/f8a/c16/29bf8ac1665c48b29852ae890b81ba9c.png">  , finding the inverse matrix - <img src="https://habrastorage.org/files/29b/f8a/c16/29bf8ac1665c48b29852ae890b81ba9c.png">  multiplying matrix vector <img src="https://habrastorage.org/files/ea8/61d/c84/ea861dc84c1f48c2b88e4384bc5d1888.png">  where n is the number of elements in the matrix A (the number of attributes * the number of elements in the test sample) If the number of elements in matrix A is large (&gt; 10 ^ 6 - for example), then even if there are 10,000 cores available, finding the solution can be analytically delayed.  Also, the first derivative may be non-linear, which complicates the decision, the analytical solution may not be at all, or the data may simply not get into memory and an iterative algorithm will be required.  It happens that they write down the advantages that the numerical solution is not ideally accurate.  In this regard, the cost function is minimized by numerical methods.  The task of finding the extremum is called the optimization problem.  In this part I will focus on the optimization method, which is called gradient descent. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We will not deviate from linear regression and OLS and denote the loss function <img src="https://habrastorage.org/files/c56/82b/1d8/c5682b1d83a5439d8297342f2d1f63ea.png">  as <img src="https://habrastorage.org/files/9c4/ad0/bba/9c4ad0bbad2440a1901e1ff11d6bcbc9.png">  - it remained unchanged.  Let me remind you that the <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D1%2580%25D0%25B0%25D0%25B4%25D0%25B8%25D0%25B5%25D0%25BD%25D1%2582">gradient</a> is a type vector <img src="https://habrastorage.org/files/492/ef1/368/492ef1368b58441fbe7dd8d7057d4321.png">  where <img src="https://habrastorage.org/files/99d/b41/357/99db41357942468898dfb3fb74a9bc54.png">  - This is a private derivative.  One of the properties of the gradient is that it indicates the direction in which some function f increases the most.  The proof of this follows from the definition of a derivative.  <a href="http://math.stackexchange.com/questions/221968/why-must-the-gradient-vector-always-be-directed-in-an-increasing-direction">A pair of evidence</a> .  Take some point a - in the vicinity of this point, the function F must be defined and differentiable, then the anti-gradient vector will indicate the direction in which the function F decreases the fastest.  From here it is concluded that at some point b, equal to <img src="https://habrastorage.org/files/67e/ae7/c6b/67eae7c6ba4a47de8ed0792ceb25f8f9.png">  with some small <img src="https://habrastorage.org/files/6ff/81e/eb8/6ff81eeb8582430eb35025232c55d408.png">  the value of the function will be less than or equal to the value at point a.  You can imagine this as a movement down the hill - taking a step down, the current position will be lower than the previous one.  Thus, at each next step, the height will at least not increase.  <a href="https://en.wikipedia.org/wiki/Gradient_descent">Formal definition</a> .  Based on these definitions, you can get a formula for finding unknown parameters (this is just a rewritten version of the formula above): <br><br><img src="https://habrastorage.org/files/41c/464/1c7/41c4641c77e54f229797513cc112134a.png"><br><br><img src="https://habrastorage.org/files/6ff/81e/eb8/6ff81eeb8582430eb35025232c55d408.png">  - this is a step of the method.  In machine learning tasks, it is called learning speed. <br><br>  The method is very simple and intuitive, take a simple two-dimensional example to demonstrate: <br><br>  It is necessary to minimize the function of the form <img src="https://habrastorage.org/files/02f/65a/962/02f65a9622544f3ebd637d1ecc5b61db.png">  .  Minimizing means finding at what value <img src="https://habrastorage.org/files/a1f/3b2/78b/a1f3b278b9a9493584f027e2ee970b85.png">  function <img src="https://habrastorage.org/files/35a/586/d36/35a586d36a3e4614ab3ae6dfca78b61c.png">  takes the minimum value.  We define the sequence of actions: <br><br>  1) Derivative with respect to <img src="https://habrastorage.org/files/a1f/3b2/78b/a1f3b278b9a9493584f027e2ee970b85.png">  : <img src="https://habrastorage.org/files/5d8/eda/dd3/5d8edadd3f3242dca3367c873f3b8c01.png"><br>  2) Set the initial value <img src="https://habrastorage.org/files/a1f/3b2/78b/a1f3b278b9a9493584f027e2ee970b85.png">  = 0 <br>  3) Set the step size - try several values ‚Äã‚Äã- 0.1, 0.9, 1.2, to see how this choice will affect the convergence. <br>  4) 25 times in a row, execute the above formula: <img src="https://habrastorage.org/files/7ea/5c2/e0f/7ea5c2e0f20d4e3f9e12ddb03a01b72c.png">  Since there is only one unknown parameter, there is only one formula. <br><br>  The code is extremely simple.  Excluding the definition of functions, the whole algorithm fit into 3 lines: <br><br><div class="spoiler">  <b class="spoiler_title">simple_gd_console.py</b> <div class="spoiler_text"><pre><code class="python hljs">STEP_COUNT = <span class="hljs-number"><span class="hljs-number">25</span></span> STEP_SIZE = <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-comment"><span class="hljs-comment">#   def func(x): return (x - 5) ** 2 def func_derivative(x): return 2 * (x - 5) previous_x, current_x = 0, 0 for i in range(STEP_COUNT): current_x = previous_x - STEP_SIZE * func_derivative(previous_x) previous_x = current_x print("After", STEP_COUNT, "steps theta=", format(current_x, ".6f"), "function value=", format(func(current_x), ".6f"))</span></span></code> </pre> <br></div></div><br><br>  The whole process can be visualized in the following way (the blue dot is the value at the previous step, the red dot is the current one): <br><br><img src="https://habrastorage.org/files/dd8/48b/b4b/dd848bb4b5484825854e8e8f6712dfb3.gif"><br><br>  Or for a step of a different size: <br><br><img src="https://habrastorage.org/files/63b/37f/00c/63b37f00ccb74af783cd0e360363d820.gif"><br><br>  With a value of 1.2, the method diverges, each step falling not lower, but vice versa higher, rushing to infinity.  A step in a simple implementation is selected manually and its size depends on the data - on some unnormalized values ‚Äã‚Äãwith a large gradient and 0.0001 can lead to a discrepancy. <br><br><div class="spoiler">  <b class="spoiler_title">Code for generating gifs</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> anim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np STEP_COUNT = <span class="hljs-number"><span class="hljs-number">25</span></span> STEP_SIZE = <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-comment"><span class="hljs-comment">#   X = [i for i in np.linspace(0, 10, 10000)] def func(x): return (x - 5) ** 2 def bad_func(x): return (x - 5) ** 2 + 50 * np.sin(x) + 50 Y = [func(x) for x in X] def func_derivative(x): return 2 * (x - 5) def bad_func_derivative(x): return 2 * (x + 25 * np.cos(x) - 5) # -      skip_first = True def draw_gradient_points(num, points, line, cost_caption, step_caption, theta_caption): global previous_x, skip_first, ax if skip_first: skip_first = False return points, line current_x = previous_x - STEP_SIZE * func_derivative(previous_x) step_caption.set_text("Step: " + str(num)) cost_caption.set_text("Func value=" + format(func(current_x), ".3f")) theta_caption.set_text("$\\theta$=" + format(current_x, ".3f")) print("Step:", num, "Previous:", previous_x, "Current", current_x) points[0].set_data(previous_x, func(previous_x)) points[1].set_data(current_x, func(current_x)) # points.set_data([previous_x, current_x], [func(previous_x), func(current_x)]) line.set_data([previous_x, current_x], [func(previous_x), func(current_x)]) if np.abs(func(previous_x) - func(current_x)) &lt; 0.5: ax.axis([4, 6, 0, 1]) if np.abs(func(previous_x) - func(current_x)) &lt; 0.1: ax.axis([4.5, 5.5, 0, 0.5]) if np.abs(func(previous_x) - func(current_x)) &lt; 0.01: ax.axis([4.9, 5.1, 0, 0.08]) previous_x = current_x return points, line previous_x = 0 fig, ax = plt.subplots() p = ax.get_position() ax.set_position([p.x0 + 0.1, p.y0, p.width * 0.9, p.height]) ax.set_xlabel("$\\theta$", fontsize=18) ax.set_ylabel("$f(\\theta)$", fontsize=18) ax.plot(X, Y, '-r', linewidth=2.0) ax.axvline(5, color='black', linestyle='--') start_point, = ax.plot([], 'bo', markersize=10.0) end_point, = ax.plot([], 'ro') rate_capt = ax.text(-0.3, 1.05, "Rate: " + str(STEP_SIZE), fontsize=18, transform=ax.transAxes) step_caption = ax.text(-0.3, 1, "Step: ", fontsize=16, transform=ax.transAxes) cost_caption = ax.text(-0.3, 0.95, "Func value: ", fontsize=12, transform=ax.transAxes) theta_caption = ax.text(-0.3, 0.9, "$\\theta$=", fontsize=12, transform=ax.transAxes) points = (start_point, end_point) line, = ax.plot([], 'g--') gradient_anim = anim.FuncAnimation(fig, draw_gradient_points, frames=STEP_COUNT, fargs=(points, line, cost_caption, step_caption, theta_caption), interval=1500) #  ,      ImageMagick #   .mp4    magick-shmagick gradient_anim.save("images/animation.gif", writer="imagemagick")</span></span></code> </pre><br></div></div><br><br>  Here is another example of a ‚Äúbad‚Äù function.  In the first animation, the method also diverges and will wander for a long time over the hills due to a step too high.  In the second case, a local minimum was found and, by varying the speed value, it would not work to force the method to find the minimum global.  This fact is one of the drawbacks of the method - it can find a global minimum only if the function is convex and smooth.  Or if you are lucky with the initial values. <br><br><div class="spoiler">  <b class="spoiler_title">Bad function</b> <div class="spoiler_text">  Bad feature: <img src="https://habrastorage.org/files/17e/fb1/b32/17efb1b3263c4e95af9485c1e09a08cd.png"><br><br><img src="https://habrastorage.org/files/ba3/d98/dc7/ba3d98dc7ab14212a582d59c5e649617.gif"><br><br><img src="https://habrastorage.org/files/376/7a8/b8c/3767a8b8cdd34602929a78fd0f51fe89.gif"><br></div></div><br><br>  It is also possible to consider the operation of the algorithm on three-dimensional graphics.  Often draw only the <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25B7%25D0%25BE%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B8%25D1%258F">isolines of</a> a figure.  I took a simple paraboloid of rotation: <img src="https://habrastorage.org/files/b65/072/1f4/b650721f4c15467fae297c28dfecdf83.png">  .  In 3D, it looks like this: <img src="https://habrastorage.org/files/ef9/503/62a/ef950362a9a64342bdcf23bd3d9cf469.png">  , and the graph with isolines is ‚Äútop view‚Äù. <br><br><div class="spoiler">  <b class="spoiler_title">Contour plot</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/967/b34/8ea/967b348ead36449c960a91eeb529021f.gif"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">Generating a graph with isolines</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.animation <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> anim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np STEP_COUNT = <span class="hljs-number"><span class="hljs-number">25</span></span> STEP_SIZE = <span class="hljs-number"><span class="hljs-number">0.005</span></span> <span class="hljs-comment"><span class="hljs-comment">#   X = np.array([i for i in np.linspace(-10, 10, 1000)]) Y = np.array([i for i in np.linspace(-10, 10, 1000)]) def func(X, Y): return 4 * (X ** 2) + 16 * (Y ** 2) def dx(x): return 8 * x def dy(y): return 32 * y # -      skip_first = True def draw_gradient_points(num, point, line): global previous_x, previous_y, skip_first, ax if skip_first: skip_first = False return point current_x = previous_x - STEP_SIZE * dx(previous_x) current_y = previous_y - STEP_SIZE * dy(previous_y) print("Step:", num, "CurX:", current_x, "CurY", current_y, "Fun:", func(current_x, current_y)) point.set_data([current_x], [current_y]) # Blah-blah new_x = list(line.get_xdata()) + [previous_x, current_x] new_y = list(line.get_ydata()) + [previous_y, current_y] line.set_xdata(new_x) line.set_ydata(new_y) previous_x = current_x previous_y = current_y return point previous_x, previous_y = 8.8, 8.5 fig, ax = plt.subplots() p = ax.get_position() ax.set_position([p.x0 + 0.1, p.y0, p.width * 0.9, p.height]) ax.set_xlabel("X", fontsize=18) ax.set_ylabel("Y", fontsize=18) X, Y = np.meshgrid(X, Y) plt.contour(X, Y, func(X, Y)) point, = plt.plot([8.8], [8.5], 'bo') line, = plt.plot([], color='black') gradient_anim = anim.FuncAnimation(fig, draw_gradient_points, frames=STEP_COUNT, fargs=(point, line), interval=1500) #  ,      ImageMagick #   .mp4    magick-shmagick gradient_anim.save("images/contour_plot.gif", writer="imagemagick")</span></span></code> </pre><br></div></div><br><br>  Also note that all the gradient lines are perpendicular to the isolines.  This means that, moving towards the antigradient, it will not be possible to immediately jump to the minimum in one step - the gradient indicates not at all there. <br><br>  After a graphic explanation, we find the formula for calculating the unknown parameters <img src="https://habrastorage.org/files/0e2/2cc/a84/0e22cca8495e4400b39c4531440293c8.png">  linear regression with OLS. <br><br><img src="https://habrastorage.org/files/23d/424/449/23d4244497274bbab48c6cb22b2cf5ae.png"><br><br>  If the number of elements in the test sample were equal to one, then the formula could be left and considered.  In the case when there are n elements, the algorithm looks like this: <br><br>  Repeat v times <br>  { <br><img src="https://habrastorage.org/files/bab/a29/9c2/baba299c242a408781067c92bef1e161.png"><br>  for each j at the same time. <br>  }, where n is the number of elements in the training sample, v is the number of iterations <br><br>  The requirement of simultaneity means that the derivative should be calculated with the old values ‚Äã‚Äãof theta, you should not first calculate the first parameter, then the second, etc., because after changing the first parameter separately, the derivative will also change its value.  Pseudo-code simultaneous change: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> train_samples: new_theta[<span class="hljs-number"><span class="hljs-number">1</span></span>] = old_theta[<span class="hljs-number"><span class="hljs-number">1</span></span>] + a * derivative(old_theta) new_theta[<span class="hljs-number"><span class="hljs-number">2</span></span>] = old_theta[<span class="hljs-number"><span class="hljs-number">2</span></span>] + a * derivative(old_theta) old_theta = new_theta</code> </pre><br><br>  If we calculate the values ‚Äã‚Äãof the parameters one by one, then this will no longer be a gradient descent.  Suppose we have a three-dimensional figure, and if we calculate the parameters one by one, then we can think of this process as moving only one coordinate at a time ‚Äî one small step along the x coordinate, then a step along the y coordinate, and so on.  steps, instead of moving in the direction of the antigradient vector. <br><br>  The above variant of the algorithm is called a packet gradient descent.  The number of repetitions can be replaced by the phrase "Repeat until it converges."  This phrase means that the parameters will be adjusted until the previous and current values ‚Äã‚Äãof the cost function are equal.  This means that a local or global minimum has been found and the algorithm will not go on.  In practice, equality cannot be achieved and the limit of convergence is introduced. <img src="https://habrastorage.org/files/0ce/d32/9ec/0ced329ecbac4b2d85dbab8dde2510f4.png">  .  It is established by some small value and the stopping criterion is that the difference between the previous and current values ‚Äã‚Äãis less than the limit of convergence - this means that the minimum has been reached with the required accuracy.  Higher accuracy (less <img src="https://habrastorage.org/files/0ce/d32/9ec/0ced329ecbac4b2d85dbab8dde2510f4.png">  ) - more iterations of the algorithm.  It looks something like this: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">while</span></span> abs(S_current - S_previous) &gt;= Epsilon: <span class="hljs-comment"><span class="hljs-comment"># do something</span></span></code> </pre><br><br>  Since the post suddenly stretched out, I would like to finish it on this - in the next part I will continue to analyze the gradient descent for linear regression with OLS. <br><br>  <a href="https://habrahabr.ru/post/308604/">Continued</a> . <br><br>  To run the examples you need: numpy, matplotlib. <br>  ImageMagick is required to run examples that create animations. <br>  The materials used in the article - <a href="https://github.com/m9psy/neural_network_habr_guide">github.com/m9psy/neural_network_habr_guide</a> </div><p>Source: <a href="https://habr.com/ru/post/307312/">https://habr.com/ru/post/307312/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../307298/index.html">Is Saas a service or license?</a></li>
<li><a href="../307300/index.html">Cambium cnPilot - inexpensive WiFi, which surprised us</a></li>
<li><a href="../307306/index.html">SObjectizer: from simple to complex. Part II</a></li>
<li><a href="../307308/index.html">Designing identical forms in WPF using abstract classes</a></li>
<li><a href="../307310/index.html">How to quickly raise the management accounting system in an advertising agency from scratch and without a budget</a></li>
<li><a href="../307314/index.html">The relationship between the monetization of games and the behavior of gamers</a></li>
<li><a href="../307316/index.html">Storing the ssh config in an ansible project and solving the tunnel problem when using the relative path</a></li>
<li><a href="../307318/index.html">Web application for working with markdown notes</a></li>
<li><a href="../307320/index.html">Multigrain: Features of PoS Terminal Malware</a></li>
<li><a href="../307324/index.html">When unit testing is really needed</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>