<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ZFS and KVM. @home</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This symbolism is already well known, so it makes no sense to explain in detail what kind of animal it is . 


 There will be no comparison with LVM ,...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ZFS and KVM. @home</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/288/cd7/d9f/288cd7d9f5e2476e8bd2426418790351.jpg" align="left"><br><p>  This symbolism is already well known, so it makes no sense to explain in detail <a href="http://xgu.ru/wiki/ZFS">what kind</a> <a href="https://ru.wikipedia.org/wiki/ZFS">of animal it is</a> . </p><br><p>  There will be no <a href="http://xgu.ru/wiki/ZFSvsLVM">comparison</a> with <a href="http://www.ilsistemista.net/index.php/virtualization/47-zfs-btrfs-xfs-ext4-and-lvm-with-kvm-a-storage-performance-comparison.html%3Flimitstart%3D0">LVM</a> , for it is possible, but meaningless, to compare a muscle-car with a jet-truck. <br>  Graphics and comics are also not delivered. <br>  It is, let's say, an unfinished success story, because the upgrade by which it was caused can only be stopped, but not completed. <br><br clear="left"><a name="habracut"></a></p><br><h3 id="predposylki">  Prerequisites </h3><br><p>  Practically in every IT family there are several PCs and / or laptops, for sure there is an HTPC or even a NAS.  So we had two PCs, then a large wall monitor was added, and in this situation the birth of the HTPC is a matter of time.  When he was born, he was immediately named a proud name - Server.  Later it turned out that some of the favorite games can do in Linux - and the Server immediately got a Steam account. <br><br>  So a few years passed, during which the Server received 16GB of memory, virtuals with gitlab and projects appeared on it ... </p><br><p>  And then the understanding came that an upgrade is brewing, and upgrading 3 PCs at once is too expensive;  At the same time, there was already a successful experience with PCIE-passthrough and a virtual virtual machine. </p><br><p>  A suitable solution was the gradual migration of gaming machines to virtuals.  While only one is emigrated, in the future, under the guest, I will mean it. <br>  Of the three processors (i3-3220, i5-3470, i5- 3470K), VT-d supported only the second, and it went to the Server instead of i3.  A low-profile 1050Ti was bought to replace the 7970, and the old iron was sold in parts. </p><br><p> The server received the latest UEFI-firmware, Ubuntu 16.04 and the largest SSD available - Crucial M4, 256 GB, under the root pool.  The installation was carried out according to <a href="https://github.com/zfsonlinux/zfs/wiki/Ubuntu%252016.04%2520Root%2520on%2520ZFS">this manual</a> , from which the installation script was accumulated in the live system.  Then, after a successful download, <code>virt-manager</code> , <code>libvirt-bin</code> , <code>ovmf</code> and <code>xubuntu-desktop</code> were installed.  Of course, VT-x and VT-d were included. <br><br>  By the way, the separation of the binary part of the system from logs, caches and other hamsters more than once helped in the process of <s>vivisection of the system of</s> experiments, allowing you to return the working state to a simple <code>zfs rollback</code> , accessible even from initrd (but there is a feature: if you use a separate datasset for <code>/root</code> , use the parameter kernel <code>init=/bin/sh</code> for emergency boot; sh, unlike bash, will not create garbage at the <code>/root</code> mount point). <br>  To avoid guessing what happened, in <code>/etc/default/grub</code> comment out everything related to <code>HIDDEN_TIMEOUT</code> , obviously set a timeout other than 0 (I have 10), uncomment / add <code>GRUB_TERMINAL=console</code> and be sure to remove <code>quiet splash</code> from <code>GRUB_CMDLINE</code> . </p><br><h3 id="error-43-i-vse-vse-vse">  Error-43 and all-all </h3><br><ul><li>  initial configuration <br>  UEFI on the host and the guest turned out to be a pledge of painless forwarding and video card operation, but for this the video card must have a fully UEFI-compatible firmware.  I was lucky with this, and the old reference 7970 and the new 1050Ti were compatible.  To create a UEFI guest, you will need the ovmf package, also (if you will create a guest using virt-manager), you must check the last page of the wizard, select and select UEFI as the microcode and Q35 as the chipset.  On my system without performing these actions, the guest system had strange problems with fading and stuttering sound. </li><li>  probros devices <br>  For forwarding devices, you need to tell the kernel to use a special driver - vfio-pci or pci-stub (already out of date) for certain id <code>vfio-pci.ids=10de:1c82,10de:0fb9,8086:1e31</code> (I have the video card itself thrown, its sound and a four-port USB3 controller) and allow the use of iommu <code>intel_iommu=on</code> (other parameters are <code>intel_iommu=on</code> to AMD, but I will not lie for lack).  Also, in order to send a video card, you will need to explicitly prohibit its use by the video driver, since nvidia was being forwarded and the proprietary driver was not installed, then for me it looks like this: <code>modprobe.blacklist=nouveau nouveau.modeset=0</code> .  All these parameters are written in the <code>/etc/default/grub</code> file, in the <code>GRUB_CMDLINE</code> line.  Do not forget to run <code>update-grub</code> and restart before adding guest-ready devices for probros. </li><li>  nVidia, or "nothing personal, it's just business." <br>  Unfortunately, in this life, nothing happens simply and immediately, so the notorious Error 43 was waiting for me. The trick with <code>cpu=host</code> although useful, did not help.  Fortunately, it was enough to use <code>virsh edit &lt;guestname&gt;</code> , and in the <code>&lt;features&gt;/&lt;hyperv&gt;</code> add the line <code>&lt;vendor_id state='on' value='ASUS'/&gt;</code> .  But in turn, libvirt from the repository (including backports) is too rotten and did not understand this parameter.  After several unsuccessful attempts to build and install a more recent version, I turned to ppa, and <a href="https://launchpad.net/~jacob/%2Barchive/ubuntu/virtualisation%3Ffield.series_filter%3Dxenial">Jacob Zimmermann</a> found a suitable package.  I recommend, having found a stable version, not to update the driver on the video card in the guest, due to the constant mutation of the business bug with forwarding. </li></ul><br><h3 id="ispolzovanie-zfs">  Using ZFS </h3><br><ul><li>  Lxc <br>  It's all very simple - we create a dataset (for example, like this: <code>zfs create rpool/lxc</code> ), run <code>lxc init</code> , choose the type of ZFS storage, refuse to create a pool, agree to use dataset, enter the name of the dataset created a little earlier.  Now they live there gitlab, samba-dc (with the key --usentvfs, a samba over 4.3 will not start), a couple of small projects. </li><li>  Deduplication, compression and blocks. <br>  To provide guests with block devices, I use zvol - it is difficult to accidentally delete it, just to create individual images.  But zvol has one tricky feature: if it is not sparse (the <code>-s</code> key when created), the snapshot will take up as much space as the original. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      When using ZFS, there are two ways to save disk space ‚Äî compression and deduplication.  And if everything is clear with compression, deduplication should be disassembled in more detail. <br><br>  Overhead for deduplication is from ~ 200 to ~ 700 bytes in memory on real systems (for details, see the output of <code>zdb -D &lt;poolname&gt;</code> ), and DDT (deduplication table), the size of which depends directly on the number of unique blocks, should be stored in memory entirely;  and this means that it is simply unprofitable to use deduplication with a small block size (except, perhaps, a virtual farm with a dedup_ratio of a much larger unit and a titanic amount of RAM).  In addition, with a small block size (this applies primarily to zvol), it is impossible to use compression other than zle. <br><br>  Needless to say, the size of the zvol block should coincide with the cluster size of the FS located on it, and it should be aligned with the block size (modern utilities have long been aligned along the 1MB border, which is more than enough)? <br><br>  It turns out that the use of large blocks favorably affects compression, deduplication and memory overhead, while compression also minimizes the overhead from large blocks in the guest file system. <br><br>  So, I create all zvol with <code>zfs create -V &lt;size&gt; -s -b 64K &lt;zfs path&gt;</code> , adding <code>-o dedup=on</code> and <code>-o compression=lz4</code> as necessary. </li><li>  64KB is our choice. <br>  It remains to teach the guest to boot and work with filesystems with this cluster size.  In the case of Linux, using ext4, for example, with such a cluster size is impossible - it simply cannot be mounted.  However, by <a href="https://thelastmaimou.wordpress.com/2013/05/04/magic-soup-ext4-with-ssd-stripes-and-strides/">manipulating</a> <a href="https://gryzli.info/2015/02/26/calculating-filesystem-stride_size-and-stripe_width-for-best-performance-under-raid/">the</a> mount <a href="https://gryzli.info/2015/02/26/calculating-filesystem-stride_size-and-stripe_width-for-best-performance-under-raid/">parameters</a> stride and stripe-width, you can achieve the desired behavior. <br><br>  In the case of NTFS, everything is trickier and simpler at the same time: Windows creates service partitions whose contents change extremely rarely, so you can ignore the size of their cluster (and the boot partition also cannot be changed - neither bootmgr nor ntldr understands the cluster size anymore 4KB).  If, after selecting unallocated space, you click not on the "Install" button, but on "Create", the installer will create and format all the necessary partitions, and you will have the disk number and partition number in which the guest system will be installed. <br><br>  Next you need a bit of black magic: <br><ul><li>  <s>draw a pentagram, not forgetting to set one black candle that once burned for each crosshair of lines</s> </li><li>  call a <s>dremor with a rank of no lower than qwd</s> <code>cmd</code> (press <code>Shift-F10</code> ) </li><li>  we start <code>diskpart</code> , and further in it </li><li> <code>select disk &lt; &gt;</code> </li> <li> <code>select partition &lt; &gt;</code> </li> <li> <code>format FS=NTFS UNIT=64K QUICK</code> </li> <li> <code>assign LETTER=S</code> </li> <li>  we leave diskpart </li><li>  in cmd we execute <code>mkdir s:\windows</code> . </li></ul></li></ul><br><p>  We return to the installation wizard, update the disk configuration, select the largest partition created (with which the magic was done) and continue the installation as usual. </p><br><div class="spoiler">  <b class="spoiler_title">What was it?</b> <div class="spoiler_text"><pre> <code class="hljs 1c"> ,       <span class="hljs-number"><span class="hljs-number">64</span></span>,  Windows ,       ,      <span class="hljs-keyword"><span class="hljs-keyword"></span></span>  - <span class="hljs-keyword"><span class="hljs-keyword"></span></span> .  ?  ,       Windows-:    ,  ,  <span class="hljs-keyword"><span class="hljs-keyword"></span></span> ,   .</code> </pre> </div></div><br><h3 id="deduplicirovat-steam-ili-s-chego-vse-i-nachinalos">  Deduplicate Steam, or where it all began </h3><br><p>  The easiest way was to get smb or nfs balls and create Steam libraries on them.  Regular files are easily and naturally deduplicated, including both exported libraries and the Linux client library, without unnecessary gestures.  As always, the most obvious solution turned out to be in principle working, but wrong.  SMB in this mode moves with the speed of a notebook hard disk, despite the fact that the files are on the SSD.  NFS (and Windows 8.1 supports NFS v3, not 4) moved a little faster, it felt like some kind of "green" desktop drive, but it was still too little for a comfortable game.  In addition, all the operating systems from M $ love to lose network drives that are automatically connected when logon, and each time pointing Steam to the location of the library quickly get bored. <br><br>  ISCSI is the solution.  The <code>targetcli</code> was installed on the <code>targetcli</code> , configured by <a href="https://www.server-world.info/en/note%3Fos%3DUbuntu_16.04%26p%3Discsi">this manual</a> , adjusted for blockio usage and by the form <code>/dev/zvols/&lt;zfs path&gt;</code> .  For unmap to work properly, you need to use Windows 8+ (starting with 8, unmap support for the iSCSI initiator is implemented), and <code>set attributes emulate_3pc=1,emulate_tpu=1,emulate_caw=1,emulate_tpws=1,emulate_tas=1</code> for each block device.  If done correctly, then deleting files and formatting should free up space and reduce the size of the zvol. <br><br>  Now identical files in volumes exported for libraries take up space only once, but the Steam library for Linux still occupies a separate place.  Let's create a separate dataset for this library with options <code>-o dedup=on -o recordsize=64K</code> .  If the sections in zvol are aligned on the 1MB border, and the cluster size is set to 64KB, then it is logical that you need to share files on the host with the same granularity so that deduplication can find the same blocks. </p><br><h3 id="posledstviya">  Effects </h3><br><ul><li>  The 16GB of memory I have at home is still not enough for a guest with 8GB, firefox on the host, containers and other things - from time to time there are strange slowdowns accompanied by intense disk activity. </li><li>  deduplicated storage is better to make a separate pool. </li><li>  installing <code>zfs set logbias=throughput &lt;pool&gt;</code> significantly affects performance, especially when using deduplication.  Not only does this exclude an additional write-to-log / erase-log loop, it also stops the drive's write cache. </li><li>  on a gigabit zvol network on SSD it feels much faster than a hard disk, although it is noticeably slower than a local SSD </li><li>  I thought about 960EVO on 1TB </li></ul><br><h3 id="vmesto-zaklyucheniya">  Instead of conclusion </h3><br><p>  Well, my little experiment ended with undeniable success, although difficulties arose on the way.  Further, I would like to upgrade to Zen2, 32-64GB of memory, NVME SSD, transfer a second PC to the Server ... but this will be a completely different witch. </p></div><p>Source: <a href="https://habr.com/ru/post/339456/">https://habr.com/ru/post/339456/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../339444/index.html">‚ÄúColleagues, I like everything, but ...‚Äù or how to build work with the customer</a></li>
<li><a href="../339446/index.html">Trading robot for web designers</a></li>
<li><a href="../339448/index.html">Design for iPhone X</a></li>
<li><a href="../339450/index.html">Parse BGP NOTIFICATION by RFC</a></li>
<li><a href="../339454/index.html">How the brain hits a tree, or how we made a recommendation system using a neural network.</a></li>
<li><a href="../339458/index.html">Uneducated youth: an attempt to summarize and some personal</a></li>
<li><a href="../339460/index.html">When did Phoenix kill Reils?</a></li>
<li><a href="../339464/index.html">Photogrammetry research</a></li>
<li><a href="../339468/index.html">Google Chrome distribution knows who downloaded it</a></li>
<li><a href="../339470/index.html">Root worse Mikhalkov</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>