<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About distinguishing objects by color</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The article is intended, first of all, for people who have not previously worked with color. She describes those nuances, interesting moments and pitf...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About distinguishing objects by color</h1><div class="post__text post__text-html js-mediator-article">  The article is intended, first of all, for people who have not previously worked with color.  She describes those nuances, interesting moments and pitfalls that I learned when I first started working with color recognition (tasks like comparing the colors of two objects, finding a desired object by a robot at the request of a person, etc.). <br><br> <a href="https://habrahabr.ru/post/337294/"><img src="http://i.piccy.info/i9/3fc69ef8d542c02efbf212b96c48fdd5/1504765574/84073/1177581/08.jpg" alt="image"></a> <br><br><a name="habracut"></a>  Consider the original system that came up with the concept of color - man.  A computer that recognizes colors must imitate the human perception system. <br>  So here's the main question: how does a person see colors? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For the time being, let us omit how the material of objects in the field of view of a person interacts with the light, consider the waves that have already entered the eye.  Direct determination of the frequency of light is not available to us, we only compare with the expected indicators of the radiation intensity at the three basic frequencies (RGB).  These are three basic colors, for each of which there is a detector in the eye. <br><br>  Then the main thing happens - the value from all cones and rods must be converted into color.  Moreover, a person makes a comparison not in absolute values, as people with absolute hearing - with the frequency of a taken note, but in relative values ‚Äã‚Äã- <a href="https://en.wikipedia.org/wiki/Color_constancy">when shadows, glares</a> , etc. <a href="https://en.wikipedia.org/wiki/Color_constancy">fall on the subject</a> .  After that, he identifies groups of tones and (this is important!) Shades in blocks under one tag - for example: ‚Äúthis is yellow‚Äù. <br><br>  ‚Üí Additionally, you can read about the <a href="https://en.wikipedia.org/wiki/Qualia">relativity of color perception in principle</a> , and about how, exactly, and in <a href="https://www.youtube.com/watch%3Fv%3DVIg5HkyauoY">what order the colors were marked</a> <br><br>  Now about the computer. <br><br>  A signal comes to the camera, the matrix recognizes the same three primary colors, and then ... And then it compresses the data, sometimes it performs the basic correction of lens aberrations and the stream received on the computer.  The computer can already decrypt the stream using a variety of codecs and get the video.  But this is not the video we are looking for. <br>  Omit until the resolution and distortion, focus all the same on the color.  If we show a monochromatic, evenly lit rectangle to the camera, the pixels that make up its image in the frame will be different.  You can even go a little further.  Load on one of the stages of such preprocessing video insertion in the form of a block of pixels.  The same test rectangle, only we can not worry about the level of light and texture coloring. <br><br>  Below you can look at the result of such an experiment.  A tiny rectangle is drawn into the frame, then creepy algorithms pass over the image, and at the output we get Almost the same rectangle.  For clarity, I highlighted the changes by removing the green channel of the image and raising the contrast. <br><br><div style="text-align:center;"><img src="http://i.piccy.info/i9/ea22ebac1f05495f2723529d484c72a0/1504521881/12003/1176788/00017_1.png" alt="image"></div><br>  Now this picture (already with garbage) should be robbed into the program that will process it.  I used the OpenCV library for these purposes.  And it also has a number of features.  They are dedicated to how the picture is stored and transmitted.  There are various reading modes.  The essence is simple: some three-channel pictures are transmitted by devices as four-channel, even if there is no fourth channel.  The library read function counts bytes waiting for the structure {26,54,250} {40,47,245} {30,26,255} ... <br><br>  While the stream file has the format {26,54,250,111} {40,47,245,110} {30,26,255,112} ... <br>  Reading three bytes will give the following result <u>{26,54,250</u> , <u>111} {40.47</u> , <u>245,110} {30</u> , <u>26,255,112}</u> ... <br><br><img src="http://i.piccy.info/i9/1fe2f4dd662d8d85be191046592cc750/1504528542/77339/1176788/Snymokkkkk.png" alt="image"><br><br>  This error is easy to recognize when displaying an image - an object of the same color will turn into an alternation of red, green, blue and gray pixels - 4 each in a circle.  If you confuse the other way around - then most likely any processing code will generate an error, as there will be a memory access outside the allocated one.  Those.  With such results, it makes sense to change the reading mode. <br><br>  Finally we got the right file.  But how to highlight the color on it is unclear.  If the object actually denoted the pixels of the same color, there would be no problem - just sort out the pixels and choose which ones there are most.  But there are two problems.  The first is lighting.  The second is image processing by the handkerchief on which the camera is sitting.  You can enter the difference threshold at which to consider similar pixels the same - this is the same as if we lowered the resolution of the colors to our own.  But unfortunately, there are two reasons not to do so.  First, pixels of the same color can vary greatly in RGB values.  And the second - the volume in the RGB-space of each color (as people have identified them) is different.  Well, if so, then you can select a block for each color, specify 6 bounding coordinates and instruct the network to accurately determine them ... But again, not - the blocks in RGB have a complex shape, they are not parallelepipeds.  We will have to add parameters if we reduce the step (up to the 1 / 255th part is not necessary, but it will still have to be reduced) the number of network input data will increase, and the garbage data unfortunately will grow faster than the useful ones.  Need preprocessing.  Looking at the pixels of the object, you can find a great thing - the differences in the hue parameter in the HLS space will be less than that of any parameter in RGB. <br><br><div style="text-align:center;"><img src="http://i.piccy.info/i9/07ea84cb2d5d1cd9c83a041c83934532/1504528793/138813/1176788/Snymo123123k.png" alt="image"></div><br>  <i>It looks like HLS</i> <br><br>  I translated all the pixels in HLS (just in case I wrote the function manually - there are different translation systems, because I did not touch the built-in library function, but imitated the one in my graphic editor), and then indicated the basic boundary values ‚Äã‚Äãseparating the colors.  You can take them, for example, carefully looking at this diagram: <br><br><img src="http://i.piccy.info/i9/09bdda4722bdf73a62c84b8c143ada2c/1504524078/223794/1176788/satfaces_map_huge.png" alt="image"><br><br>  Now to the difficult.  From the rest of the parameters we still have not got rid of.  White, black, gray, brown are quite basic colors in the person‚Äôs view, but they have a rather weak relation to hue ‚Äî you cannot understand them without illumination and saturation.  Well, we add the same basic barriers to the eye, and see how the system behaves. <br><br>  I identified and labeled 15 primary colors, then began testing.  I used the standard Google search tools for pictures to filter the results by color, and for starters did not take a photo with the object and background, but more monotonous pictures (for example, grass, or yellow leaves).  To view and filter unnecessary images in dataset on your own is a mandatory part of the training.  I had a network without a trainer, I twisted the values ‚Äã‚Äãmanually. <br><br><img src="http://i.piccy.info/i9/d4ec328caa2fd35d3d93500878608fad/1504529612/59316/1176788/_173u3k_.jpg" alt="image"><br><br>  Then, having received hundreds of correctly identified colors in a row, and moved to the objects in the background (already taken from the working environment, those for which the algorithm will be applied).  There is a trick here, and it consists in the fact that when voting pixels for a certain color, you need to create your own branch, your candidate, for each cluster of pixels.  The search is still done on all pixels, so it is easy to create several cells under one color.  For example, in the following picture, a line search will reveal the incoherence of red pixel sets and create two ‚Äúred‚Äù format entries - 0. The first will be incremented by the pixels of one red ball, the second will be voted by the pixels of the second. <br><br><div style="text-align:center;"><img src="http://sviatko.com.ua/wp-content/uploads/gelevie-shariki-sviatko.jpg" alt="image"></div><br>  Just in case, the function returns not only a record with a maximum of votes, but also second place (so that in case the first place was taken by the background of the object) and first place among tones, without shades (I remind you that the volume of gray and black in the color space is much larger than the volume of the saturated bright colors. This means that adjusting the filters so that there are no false positives for unstressed colors is impossible in principle).  The latter is especially noticeable when searching for an object on a snowy street, when there is a lot of white around.  Therefore, one of the three results is chosen according to an additional context. <br><br><div class="spoiler">  <b class="spoiler_title">A little more detail:</b> <div class="spoiler_text">  In my case <br>  - calculate the difference between the results and if it is large (2.5 times or more) - output the first result <br>  - calculate flare and snow.  To do this, conduct color correction and repeat the calculation. <br>  - use information about the expected position of the object in the picture to reduce the weight assigned to the background color. <br><br>  An example of the ‚Äúwrong‚Äù picture: <br><br><img src="http://i.piccy.info/i9/6fc99f5d9b85581a1bdf4c33b36aab6d/1504529555/204025/1176788/_12_.jpg" alt="image"><br></div></div><br>  Now the difference in illumination is still greatly interfering with the identification of an object of the same color.  In some cases, color correction is suitable, but sometimes it is helpless - gray lights up to white, black - to gray or white. <br><br>  For this, information about the object is used (which we have distinguished by color in one of the previous frames and we track how a shadow or a glare falls on it).  We read all those pixels that voted for a defined color.  And now we consider their average value.  We get the base color of the object and make up a palette of possible changes. <br><br>  A very important thing: the color is reproduced by the monitor so as to match the perception of the person;  therefore, the storage of color data, following the playback devices themselves, is optimized for users' eyes, and the RGB values ‚Äã‚Äãdo not correspond to the real share of these colors in the color we perceive (and looking at the wide spectra of reflected light, you can make sure that all three base colors are placed in them ). <br><br>  Therefore, mixing the base color with others, and testing the highlights - you need to put the RGB values ‚Äã‚Äãin the square before the start of all procedures, and take the root at the end.  After that, I transferred to HLS and checked how much the color has changed.  I took the base color, mixed it in turn with black, white, red, blue, green, and color from the main set, which was determined by the first algorithm.  After that, I repeated the procedure several times to get several gradations of each option.  For comparisons with the frequency of 1/30 and 1/24, you can take only two iterations - this is enough with the head (you can even delete the farthest values ‚Äã‚Äãto be safe - the values ‚Äã‚Äãwill obviously coincide). <br><br>  Then something like a hash function is done - the distances between colors in the palette are taken (I took the distance L1 - L2 + G1 - G2, since this strange metric showed itself well in the experiment, it is generally worth trying any combinations of RGB and HLS differences) then placed into a sorted list, in descending order. <br><br>  examples of automatically generated palettes (for a system of objects of different colors, all the palettes were combined into one for the purpose of optimization, but the series still calculated only for those colors that relate only to one object): <br><br><img src="http://i.piccy.info/i9/e869391fe6434b6bd5ce18ae27494d76/1504531395/13910/1176788/hardfavproldullfix.png" alt="image"><br><br>  On the new frame, the same procedure is performed, and the distances coinciding within the limits of the difference below the threshold value are searched.  Then the length of the longest chain of coinciding differences in the list indicates whether the color is the same, but with a sudden highlight / shadow, or not.  As the three primary colors for mixing colors in the palette, you can take not RGB, but slightly purple instead of blue and yellow instead of green - according to the popular colors of artificial light sources. <br><br>  After it is necessary to calibrate the threshold values ‚Äã‚Äãfor a specific task. <br><br>  There are no pictures from the final tests, but at the end I loaded <a href="">that dress</a> into the algorithm and received ‚Äúwhite‚Äù and ‚Äúyellow‚Äù. <br><br>  So what is the result? <br><br><ul><li>  To determine the color, I recommend doing three steps: take the initial coefficients from studies of how people perceive colors. </li><li>  Run a set of images from Google and perform a rough adjustment. </li><li>  Run around the set of test images of this type with which the program will work and perform a fine adjustment. </li><li>  To reduce the number of errors, you need to study the context of the form and conduct additional color correction. </li><li>  To distinguish between illuminated objects, you need to build a palette of the intended colors after the highlight / shadow and compare them in some quick way. </li></ul><br>  Moments that are worth paying attention to: <br><br><ul><li>  the frame from the camera comes with distortions in the RGB values, which are different for each pixel; </li><li>  sometimes there comes an extra, fourth channel, which is not written, but gets into the picture from the programs; </li><li>  the values ‚Äã‚Äãon the monitor are correct, but they are not stored, but distorted coordinates, proportional to the root of the real values. </li></ul><br>  Now about where you can apply such a thing, and play with it: <br><br><ol><li>  in Person Recognition systems, where, in addition to the person‚Äôs face, you can add information about the physique, bag and color of your favorite T-shirt to the database; </li><li>  in trackers, as a replacement for coarse initial filters (of the diffusion value type); </li><li>  in the technique for searching for different types of objects to be captured by a manipulator; </li><li>  for segmentation of thermal and vibra-images already brought to a certain range of colors; </li><li>  for anything!  The computer distinguishes colors as a person.  For convenience, you can return with the calculated label and the average color, and some service codes.  I used the Vec3b output format, and slightly customized this type to convey here the recognition error and the degree of confidence in the answer. </li></ol><br>  <i>Picture marking flowers <a href="https://blog.xkcd.com/2010/05/03/color-survey-results/">from here</a></i> <br></div><p>Source: <a href="https://habr.com/ru/post/337294/">https://habr.com/ru/post/337294/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337284/index.html">How to embed svg</a></li>
<li><a href="../337286/index.html">Why do you need BEM</a></li>
<li><a href="../337288/index.html">Using the KOMPAS-3D API ‚Üí Lesson 4 ‚Üí Title bar</a></li>
<li><a href="../337290/index.html">Fedora Linux is banned for distribution in the Crimea (and temporarily broken DNS)</a></li>
<li><a href="../337292/index.html">Custom properties</a></li>
<li><a href="../337298/index.html">In the wake of highloadcup: php vs node.js vs go, swoole vs workerman, splfixedarray vs array and more</a></li>
<li><a href="../337300/index.html">V8 internal mechanisms and fast work with object properties</a></li>
<li><a href="../337302/index.html">git rebase for beginners</a></li>
<li><a href="../337304/index.html">Ransomware hackers hacked more than 26,000 MongoDB servers</a></li>
<li><a href="../337306/index.html">Docker Basics in X hours and Y days</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>