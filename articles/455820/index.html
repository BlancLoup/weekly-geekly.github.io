<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>VM performance analysis in VMware vSphere. Part 2: Memory</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Part 1. About CPU 

 In this article we will talk about the performance counters of RAM (RAM) in vSphere. 
 It seems to be all the more unequivocal wi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>VM performance analysis in VMware vSphere. Part 2: Memory</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/el/am/7y/elam7yyhc6vrowmmrt5ofxgj-r0.png"><br><br>  <a href="https://habr.com/ru/company/dataline/blog/452884/">Part 1. About CPU</a> <br><br>  In this article we will talk about the performance counters of RAM (RAM) in vSphere. <br>  It seems to be all the more unequivocal with the memory than with the processor: if the VM has performance problems, it's hard not to notice them.  But if they appear, to deal with them is much more difficult.  But first things first. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  A bit of theory </h3><br>  The virtual memory of virtual machines is taken from the memory of the server on which the VMs are running.  It is quite obvious :).  If the server‚Äôs RAM is not enough for everyone, ESXi begins to use memory reclamation techniques.  Otherwise, VM operating systems would crash with RAM access errors. <br><br>  What technology to use ESXi solves depending on the workload of RAM: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Memory status</b> <br></td><td>  <b>The border</b> <br></td><td>  <b>Actions</b> <br></td></tr><tr><td>  High <br></td><td>  400% of minFree <br></td><td>  After reaching the upper limit, large memory pages are divided into small ones (TPS works in standard mode). <br></td></tr><tr><td>  Clear <br></td><td>  100% of minFree <br></td><td>  Large memory pages are broken into small, TPS works forcibly. <br></td></tr><tr><td>  Soft <br></td><td>  64% of minFree <br></td><td>  TPS + Balloon <br></td></tr><tr><td>  Hard <br></td><td>  32% of minFree <br></td><td>  TPS + Compress + Swap <br></td></tr><tr><td>  Low <br></td><td>  16% of minFree <br></td><td>  Compress + Swap + Block <br></td></tr></tbody></table></div>  <a href="http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/">A source</a> <br><br>  minFree is the RAM needed for the hypervisor to work. <br><br>  Prior to ESXi 4.1 inclusive, minFree was fixed by default - 6% of the server's RAM (the percentage could be changed via the Mem.MinFreePct option on the ESXi).  In later versions, due to an increase in the amount of memory on minFree servers, it was calculated based on the amount of host memory, and not as a fixed percentage. <br><br>  The minFree value (default) is calculated as follows: <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Memory percentage reserved for minFree</b> <br></td><td>  <b>Memory range</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td></tr><tr><td>  four% <br></td><td>  4-12 GB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td></tr><tr><td>  one% <br></td><td>  Remaining memory <br></td></tr></tbody></table></div>  <a href="http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/">A source</a> <br><br>  For example, for a server with 128 GB of RAM, the MinFree value would be: <br>  MinFree = 245.76 + 327.68 + 327.68 + 1024 = 1925.12 MB = 1.88 GB <br>  The actual value may differ by a couple of hundred MB, it depends on the server and RAM. <br><div class="scrollable-table"><table><tbody><tr><td>  <b>Memory percentage reserved for minFree</b> <br></td><td>  <b>Memory range</b> <br></td><td>  <b>Value for 128 GB</b> <br></td></tr><tr><td>  6% <br></td><td>  0-4 GB <br></td><td>  245.76 MB <br></td></tr><tr><td>  four% <br></td><td>  4-12 GB <br></td><td>  327.68 MB <br></td></tr><tr><td>  2% <br></td><td>  12-28 GB <br></td><td>  327.68 MB <br></td></tr><tr><td>  one% <br></td><td>  Remaining memory (100 GB) <br></td><td>  1024 MB <br></td></tr></tbody></table></div><br><br>  Usually, for productive stands, only the High state can be considered normal.  For testing and development stands, Clear / Soft states may be acceptable.  If the RAM on the host is less than 64% MinFree, then the VMs running on it have performance problems. <br><br>  In each state, certain memory reclamation techniques are applied, starting with TPS, which have practically no effect on VM performance, ending with Swapping.  I'll tell you more about them. <br><br>  <b>Transparent Page Sharing (TPS).</b>  TPS is, roughly speaking, the deduplication of the pages of RAM of virtual machines on the server. <br><br>  ESXi searches for the same virtual machine RAM pages, counts and compares the hash-sum of pages, and removes duplicate pages, replacing them with links to the same page in the physical memory of the server.  As a result, the consumption of physical memory is reduced and it is possible to achieve some oversubscription from memory with almost no performance degradation. <br><br><img src="https://habrastorage.org/webt/ul/fz/1i/ulfz1i0bomyhsarceziylov-o6i.jpeg"><br>  <a href="https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/">A source</a> <br><br>  This mechanism works only for memory pages of 4 KB in size (small pages).  Pages that are 2 MB in size (large pages) do not even try to deduplicate the hypervisor: the chance to find identical pages of this size is not great. <br><br>  By default, ESXi allocates memory to large pages.  Splitting large pages into small ones begins when the High state threshold is reached and it occurs forcibly when the Clear state is reached (see the hypervisor state table). <br><br>  If you want TPS to start work without waiting for the host's RAM to be filled in, you need to set the <i>‚ÄúMem.AllocGuestLargePage‚Äù</i> value to 0 in the Advanced Options ESXi (default 1).  Then the allocation of large pages of memory for virtual machines will be disabled. <br><br>  Since December 2014, all releases of ESXi TPS between VMs are disabled by default, since a vulnerability was found that theoretically allows getting another VM from the VMs.  Details here.  Information about the practical implementation of the exploitation of the vulnerability TPS I have not met. <br><br>  The TPS policy is controlled by the advanced option <i>‚ÄúMem.ShareForceSalting‚Äù</i> on ESXi: <br>  0 - Inter-VM TPS.  TPS works for different VM pages; <br>  1 - TPS for VMs with the same value ‚Äúsched.mem.pshare.salt‚Äù in VMX; <br>  2 (default) - Intra-VM TPS.  TPS works for pages inside the VM. <br><br>  It definitely makes sense to turn off large pages and turn on Inter-VM TPS on test benches.  It can also be used for stands with a large number of similar VMs.  For example, on stands with VDI, physical memory savings can reach tens of percent. <br><br>  <b>Memory Ballooning.</b>  Ballooning is no longer as innocuous and transparent to the VM operating system as TPS.  But with proper use with Ballooning you can live and even work. <br><br>  Together with Vmware Tools, a special driver is installed on the VM, called the Balloon Driver (aka vmmemctl).  When the hypervisor begins to lack physical memory and it enters the Soft state, ESXi asks VMs to reclaim unused RAM through this Balloon Driver.  The driver, in turn, works at the operating system level and requests free memory from it.  The hypervisor sees which pages of physical memory the Balloon Driver has occupied, takes the memory from the virtual machine and returns it to the host.  There are no problems with the operation of the OS, since at the OS level the memory is occupied by the Balloon Driver.  By default, Balloon Driver can take up to 65% of VM memory. <br><br>  If VMware Tools are not installed on the VM or Ballooning is disabled (I do not recommend it, but there is a <a href="https://kb.vmware.com/s/article/1002586">KB</a> :), the hypervisor immediately switches to more demanding methods of removing memory.  Conclusion: make sure that VMware Tools on the VM are available. <br><br><img src="https://habrastorage.org/webt/ey/pm/s1/eypms1ugdkmhr0r4odhga1locao.png"><br>  <i>You can check the work of Balloon Driver from the OS via VMware Tools</i> . <br><br>  <b>Memory Compression.</b>  This technique applies when ESXi reaches the Hard state.  As the name implies, ESXi tries to compress 4 KB of a RAM page to 2 KB and thus free some space in the physical memory of the server.  This technique significantly increases the access time to the contents of VM RAM pages, since the page must first be released.  Sometimes not all pages can be compressed and the process itself takes some time.  Therefore, this technique is not very effective in practice. <br><br>  <b>Memory Swapping.</b>  After a short phase, Memory Compression ESXi is almost inevitable (if the VMs did not go to other hosts or turned off) goes to Swapping.  And if there is very little memory left (Low state), then the hypervisor also stops allocating VM memory pages, which can cause problems in VM guest OSs. <br><br>  This is how Swapping works.  When you turn on a virtual machine, a file with the .vswp extension is created for it.  It is equal in size to non-reserved VM RAM: this is the difference between configured and reserved memory.  When Swapping works, ESXi unloads virtual machine memory pages into this file and starts working with it instead of physical server memory.  Of course, such such ‚Äúoperational‚Äù memory is several orders of magnitude slower than the real one, even if .vswp lies on fast storage. <br><br>  Unlike Ballooning, when unused pages are selected from a VM, when Swapping'e, pages that are actively used by the OS or applications inside the VM can be moved to disk.  As a result, VM performance drops until it hangs.  VM formally works and at least it can be properly disconnected from the OS.  If you will be patient;) <br><br>  If the VM went to Swap - this is an emergency situation, which as far as possible is better not to allow. <br><br><h3>  Core Virtual Machine Memory Performance Counters </h3><br>  So we got to the main thing.  To monitor the state of memory in the VM there are the following counters: <br><br>  <b>Active</b> - shows the amount of RAM (KB) to which the VM gained access in the previous measurement period. <br><br>  <b>Usage</b> is the same as Active, but as a percentage of the configured VM RAM.  It is calculated using the following formula: active √∑ virtual machine configured memory size. <br>  High Usage and Active, respectively, are not always indicative of VM performance problems.  If a VM uses memory aggressively (at least it accesses it), this does not mean that there is not enough memory.  Rather, it is a reason to see what happens in the OS. <br>  There is a standard Alarm for Memory Usage for VM: <br><br><img src="https://habrastorage.org/webt/8u/ca/n8/8ucan84mevajwvlnr-4ov9boyro.png"><br><br>  <b>Shared</b> is the amount of RAM in a VM deduplicated using TPS (inside a VM or between a VM). <br><br>  <b>Granted</b> - the amount of physical memory of the host (Kbytes), which was given to the VM.  Includes Shared. <br><br>  <b>Consumed</b> (Granted - Shared) - the amount of physical memory (KB) that the VM consumes from the host.  Does not include Shared. <br><br>  If part of the VM memory is not given from the physical memory of the host, but from the swap file or the memory is taken from the VM via the Balloon Driver, this volume is not counted in Granted and Consumed. <br>  High values ‚Äã‚Äãfor Granted and Consumed are perfectly normal.  The operating system gradually takes the memory from the hypervisor and does not give it back.  Over time, the actively working VM values ‚Äã‚Äãof these counters approach the amount of configured memory, and remain there. <br><br>  <b>Zero</b> - the amount of RAM VM (Kbytes), which contains zeros.  Such memory is considered a free hypervisor and can be given away to other virtual machines.  After the guest OS received recorded something in the memory, it goes into Consumed and does not return back. <br><br>  <b>Reserved Overhead</b> - amount of RAM VM, (KB) reserved by the hypervisor for VM operation.  This is a small amount, but it must be available on the host, otherwise the VM will not start. <br><br>  <b>Balloon</b> - amount of RAM (Kbytes) taken from the VM using the Balloon Driver. <br><br>  <b>Compressed</b> - the amount of RAM (KB), which managed to compress. <br><br>  <b>Swapped</b> - the amount of RAM (KB), which, in the absence of physical memory on the server, moved to disk. <br>  Balloon and other memory reclamation techniques counters are zero. <br><br>  Here is a graph with Memory counters of a normally working VM with 150 GB of RAM. <br><br><img src="https://habrastorage.org/webt/0l/pp/w3/0lppw3nz9iqzcnuergtxiseb67s.png"><br><br>  On the graph below, VM has obvious problems.  Under the graph it can be seen that for this VM all the described techniques of working with RAM were used.  Balloon for this VM is much more than Consumed.  In fact, the VM is more dead than alive. <br><br><img src="https://habrastorage.org/webt/f4/ic/xk/f4icxkpxpykxua_gp-sirlzuu_u.png"><br><br><h3>  ESXTOP </h3><br>  As with the CPU, if we want to quickly assess the situation on the host, as well as its dynamics at intervals of up to 2 seconds, we should use ESXTOP. <br><br>  The ESXTOP over Memory screen is called up by the ‚Äúm‚Äù key and looks like this (the fields B, D, H, J, K, L, O are selected): <br><br><img src="https://habrastorage.org/webt/rm/wj/4k/rmwj4krvvizdtcizkrid0zjuml8.png"><br><br>  The following parameters will be interesting for us: <br><br>  <b>Mem overcommit avg</b> - average oversubscription memory on a host for 1, 5 and 15 minutes.  If above zero, then this is a reason to see what is happening, but not always an indicator of the presence of problems. <br><br>  The <b>PMEM / MB</b> and <b>VMKMEM / MB</b> lines <b>contain</b> information about the physical memory of the server and the memory available to VMkernel.  From the interesting here you can see the value of minfree (in MB), the state of the host by memory (in our case, high). <br><br>  In the <b>NUMA / MB line</b> you can see the distribution of RAM by NUMA nodes (sockets).  In this example, the distribution is uneven, which in principle is not very good. <br><br>  The following is general server statistics for memory reclamation techniques: <br><br>  <b>PSHARE / MB</b> is TPS statistics; <br><br>  <b>SWAP / MB</b> - Swap usage statistics; <br><br>  <b>ZIP / MB</b> - statistics of compression of memory pages; <br><br>  <b>MEMCTL / MB</b> - usage statistics for Balloon Driver. <br><br>  For individual VMs we may be interested in the following information.  I hid the VM names so as not to embarrass the audience :).  If the ESXTOP metric is similar to the counter in vSphere, I quote the corresponding counter. <br><br>  <b>MEMSZ</b> is the amount of memory configured on the VM (MB). <br>  MEMSZ = GRANT + MCTLSZ + SWCUR + untouched. <br><br>  <b>GRANT</b> - Granted in MB. <br><br>  <b>TCHD</b> - Active in MB. <br><br>  <b>MCTL?</b>  - whether the Balloon Driver is installed on the VM. <br><br>  <b>MCTLSZ</b> - Balloon in MB. <br><br>  <b>MCTLGT</b> is the amount of RAM (MB) that ESXi wants to withdraw from the VM via the Balloon Driver (Memctl Target). <br><br>  <b>MCTLMAX</b> is the maximum amount of RAM (MB) that ESXi can remove from the VM via the Balloon Driver. <br><br>  <b>SWCUR</b> is the current amount of RAM (MB) given to the VM from the Swap file. <br><br>  <b>SWGT</b> is the amount of RAM (MB) that ESXi wants to send to the VM from the Swap file (Swap Target). <br><br>  Also through ESXTOP you can see more detailed information about VM NUMA-topology.  To do this, select the fields D, G: <br><br><img src="https://habrastorage.org/webt/ff/7y/zd/ff7yzdsjedyntnpj4duwv0c731m.png"><br><br>  <b>NHN</b> - NUMA nodes on which the VM is located.  Here you can immediately notice wide vm, which do not fit on one NUMA node. <br><br>  <b>NRMEM</b> - how many megabytes of VM memory is taken from a remote NUMA node. <br><br>  <b>NLMEM</b> - how many megabytes of VM memory is taken from a local NUMA node. <br><br>  <b>N% L</b> is the percentage of VM memory on the local NUMA node (if less than 80%, performance problems may occur). <br><br><h3>  Memory on the hypervisor </h3><br>  If the CPU counters for the hypervisor are usually not of particular interest, then the situation with memory is the opposite.  High Memory Usage on the VM does not always indicate a performance problem, but high Memory Usage on the hypervisor just starts the work of a memory management technician and causes problems with the performance of the VM.  The alarms for Host Memory Usage should be monitored and prevented the VM from getting into Swap. <br><br><img src="https://habrastorage.org/webt/h2/x_/59/h2x_59kddpe84yzudcq03fq1rmc.png"><br><br><img src="https://habrastorage.org/webt/oc/w7/c9/ocw7c9vmbrqogjhmtpbotng4-6y.png"><br><br><h3>  Unswap </h3><br>  If a VM gets into Swap, its performance is greatly reduced.  Traces of Ballooning and compression quickly disappear after the appearance of free RAM on the host, but the virtual machine is not in a hurry to return from Swap to the server's RAM. <br>  Prior to ESXi 6.0, the only reliable and fast way to remove VMs from Swap was to reboot (more precisely, switching off / on the container).  Starting with ESXi 6.0, although not quite official, it appeared to be a working and reliable way to remove VMs from Swap.  At one of the conferences, I was able to communicate with one of the VMware engineers responsible for CPU Scheduler.  He confirmed that the method is quite working and safe.  In our experience there were no problems with it either. <br><br>  Actually, the commands for withdrawing VMs from Swap were <a href="http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/">described by</a> Duncan Epping.  I will not repeat the detailed description, just give an example of its use.  As you can see in the screenshot, some time after the execution of the specified Swap commands on the VM disappears. <br><br><img src="https://habrastorage.org/webt/e5/lm/7e/e5lm7e0e6i_yxrrlm7dfwixptv0.png"><br><br><h3>  ESXi RAM Management Tips </h3><br>  Finally, here are some tips to help you avoid problems with VM performance due to RAM: <br><br><ul><li>  Do not allow oversubscription for RAM in productive clusters.  It is advisable to always have ~ 20-30% of free memory in the cluster, so that the DRS (and the administrator) have room for maneuver, and when migrating, the VMs do not go to Swap.  Also do not forget about the margin for fault tolerance.  It is unpleasant when, when a single server fails and the VM is rebooted using HA, some of the machines also go to Swap. </li><li>  In high consolidation infrastructures, try NOT to create VMs with more than half the memory of the host.  This again will help DRS distribute virtual machines to the cluster servers without any problems.  This rule, of course, is not universal :). </li><li>  Watch out for Host Memory Usage Alarm. </li><li>  Do not forget to install VMware Tools on the VM and do not turn off Ballooning. </li><li>  Consider turning on Inter-VM TPS and turning off Large Pages in VDI and test environments. </li><li>  If the VM has performance problems, check if it uses memory from the remote NUMA node. </li><li>  Take your VM out of Swap as quickly as possible!  In addition, if the VM in Swap, for obvious reasons, the storage system suffers. </li></ul><br>  On this about the memory I have everything.  Below are related articles for those who want to go into details.  The following article will be devoted to the story. <br><br><div class="spoiler">  <b class="spoiler_title">useful links</b> <div class="spoiler_text">  <a href="http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/">http://www.yellow-bricks.com/2015/03/02/what-happens-at-which-vsphere-memory-state/</a> <br>  <a href="http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/">http://www.yellow-bricks.com/2013/06/14/how-does-mem-minfreepct-work-with-vsphere-5-0-and-up/</a> <br>  <a href="https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/">https://www.vladan.fr/vmware-transparent-page-sharing-tps-explained/</a> <br>  <a href="http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/">http://www.yellow-bricks.com/2016/06/02/memory-pages-swapped-can-unswap/</a> <br>  <a href="https://kb.vmware.com/s/article/1002586">https://kb.vmware.com/s/article/1002586</a> <br>  <a href="https://www.vladan.fr/what-is-vmware-memory-ballooning/">https://www.vladan.fr/what-is-vmware-memory-ballooning/</a> <br>  <a href="https://kb.vmware.com/s/article/2080735">https://kb.vmware.com/s/article/2080735</a> <br>  <a href="https://kb.vmware.com/s/article/2017642">https://kb.vmware.com/s/article/2017642</a> <br>  <a href="https://labs.vmware.com/vmtj/vmware-esx-memory-resource-management-swap">https://labs.vmware.com/vmtj/vmware-esx-memory-resource-management-swap</a> <br>  <a href="https://blogs.vmware.com/vsphere/2013/10/understanding-vsphere-active-memory.html">https://blogs.vmware.com/vsphere/2013/10/understanding-vsphere-active-memory.html</a> <br>  <a href="https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/memory_counters.html">https://www.vmware.com/support/developer/converter-sdk/conv51_apireference/memory_counters.html</a> <br>  <a href="https://docs.vmware.com/en/VMware-vSphere/6.5/vsphere-esxi-vcenter-server-65-monitoring-performance-guide.pdf">https://docs.vmware.com/en/VMware-vSphere/6.5/vsphere-esxi-vcenter-server-65-monitoring-performance-guide.pdf</a> <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/455820/">https://habr.com/ru/post/455820/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../455806/index.html">The birth and death of the album: we understand how music formats have changed over the past 100 years</a></li>
<li><a href="../455808/index.html">Submission of auto-requests on the FTS website in USR on python</a></li>
<li><a href="../455812/index.html">Building a microservice architecture on Golang and gRPC, part 2 (docker)</a></li>
<li><a href="../455816/index.html">How to create a cool action for Google Assistant. Just AI's life hacking</a></li>
<li><a href="../45582/index.html">Another testing of 5 browsers from extremetech.com</a></li>
<li><a href="../455826/index.html">Auto watering colors with remote control</a></li>
<li><a href="../455828/index.html">Scientists have discovered new exotic forms of synchronization</a></li>
<li><a href="../45583/index.html">O'Reilly about keeping tempting profits in the Clouds with a translator afterword</a></li>
<li><a href="../455830/index.html">A look at Go through the eyes of a .NET developer. Week # 1</a></li>
<li><a href="../455832/index.html">The story of a single SQL investigation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>