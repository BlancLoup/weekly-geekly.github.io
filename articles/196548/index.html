<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Lock-free data structures. The basics: where did the memory barriers go from?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="As soon as I became interested in lock-free algorithms, the question began to torment me - where did the need for memory barriers come from, to ‚Äúput t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Lock-free data structures. The basics: where did the memory barriers go from?</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/78b/46b/d2a/78b46bd2a01a37adca15fb0236bd9338.jpg" align="right"><br>  As soon as I became interested in lock-free algorithms, the question began to torment me - where did the need for memory barriers come from, to ‚Äúput things in order‚Äù in the code? <br>  Of course, after reading a few thousand pages of manuals on a <i>specific</i> architecture, we will find the answer.  But this answer will be suitable for this <i>particular</i> architecture.  Is it common?  In the end, we want our code to be portable.  And the C ++ 11 memory model is not sharpened for a specific processor. <br>  The most acceptable general answer was given to me by Mr. <a href="http://www.linkedin.com/in/paulmckenney">Paul McKenney</a> in his 2010 article <a href="http://irl.cs.ucla.edu/~yingdi/web/paperreading/whymb.2010.06.07c.pdf">Memory Barriers: a Hardware View of Software Hackers</a> .  The value of his article is in general: he built some simplified abstract architecture, using which he examines what a memory barrier is and why it was introduced. <br>  Generally, Paul McKenney is a famous person.  He is a developer and active promoter of <a href="http://en.wikipedia.org/wiki/Read-copy-update">RCU</a> technology, which is actively used in the Linux kernel, and also implemented in the latest version of <a href="http://libcds.sourceforge.net/">libcds</a> as another approach to safe memory release (in general, I would like to tell about RCU separately).  Also participated in the work on the C ++ 11 memory model. <br>  The article is big, I give only the first half translation.  I allowed myself to add some comments, <i>[which are highlighted in the text as follows]</i> . <br><a name="habracut"></a><br><h1>  Memory Barriers: a Hardware View of Software Hackers </h1><br>  What prompted the CPU designers to introduce memory barriers and thereby impose a pig on the developers?  The short answer is: reordering memory accesses allows for better performance, and memory barriers are needed to ‚Äúrestore order‚Äù in things like synchronization primitives <i>[and lock-free algorithms, of course]</i> , where the correctness of the primitive depends on the order of memory accesses. <br>  A detailed answer requires a good understanding of how the CPU cache works, and what is required for its even better performance.  Therefore, we further: <br><ul><li>  consider the cache structure; </li><li>  we describe how the cache coherence protocol ensures the visibility of each memory cell for different processors; </li><li>  Consider how store buffers and invalidate queues help the cache achieve maximum performance. </li></ul><br>  We will see that memory barriers are a necessary evil that is required to achieve high performance and scalability.  The root of this evil is the fact that the CPU is orders of magnitude faster than the memory and processor interface with memory. <br><br><h2>  Cache structure </h2><br><img src="https://habrastorage.org/getpro/habr/post_images/2f8/e85/f42/2f8e85f42c64689016bf981ebdb048ff.png" align="right"><br>  Modern CPUs are much faster than memory subsystems.  The 2006 sample processor could execute 10 instructions per nanosecond, but it took many tens of nanoseconds to extract data from main memory.  This disproportion in speed (more than two orders of magnitude!) Led to multi-megabyte caches on modern processors.  Caches belong to processors and, as a rule, the access time to them is several cycles. <br><div class="spoiler">  <b class="spoiler_title">Note</b> <div class="spoiler_text">  In fact, it‚Äôs common practice to have multiple cache levels.  The smallest cache is closest to the processor and is available in one clock.  The second level cache has an access time of about 10 cycles.  The most productive processors have 3 and even 4 levels of caches. <br></div></div><br>  The CPU exchanges data with the cache in blocks, called <i>cache lines</i> (cache line), the size of which is usually a power of two - from 16 to 256 bytes (depending on the CPU).  When the processor first accesses the memory cell, the cell is not in the cache ‚Äî this situation is called a <i>miss</i> (cache miss or, more precisely, ‚Äústartup‚Äù or ‚Äúwarmup‚Äù cache miss).  A slip means that the CPU must wait (stalled) for hundreds of cycles until the data has been retrieved from memory.  Finally, the data will be loaded into the cache, and subsequent calls to this address will find the data in the cache, so that the CPU will run at full speed. <br>  After some time, the cache is full, and misses will cause the data to be pushed out of the cache to give space to the new requested data.  Such misses are called capacity miss.  Moreover, preemption can occur even when the cache is not full, as it is organized in hardware in the form of a hash table with a fixed <a href="http://ru.wikipedia.org/wiki/%25D0%25A5%25D0%25B5%25D1%2588-%25D1%2582%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25B8%25D1%2586%25D0%25B0">bucket</a> size (or <i>set</i> , as its developers call CPU). <br><img src="https://habrastorage.org/getpro/habr/post_images/003/246/49a/00324649ad5fd3fdf68f199181ff94d8.png" align="right"><br>  The figure on the right shows a 2-associative (2-way) cache with a 256-byte cache line.  The line may be empty, which corresponds to an empty table cell.  The numbers on the left are addresses whose data the cell may contain.  Since the lines are aligned by 256 bytes, the lower 8 bits of the address are zero, and the hash function selects the next 4 bits as an index in the hash table.  Suppose the program code is located at addresses 0x43210E00 - 0x43210EFF, and the program itself sequentially accesses data at addresses 0x12345000 - 0x12345EFF.  Let now she addresses to the address 0x12345F00.  This address is hashed to the 0xF line and both cells (ways) of this line are empty, so 256 bytes of data can be placed in one of them.  If the program accesses the address 0x1233000, the hash of which is 0x0, then the corresponding 256-byte data can be placed in cell 1 (way 1) of the line 0x0.  If the program accesses the address 0x1233E00 (hash = 0xE), then one of the lines (ways) must be pushed out of the cache in order to free up space for the new 256 bytes of data.  If you later need access to this ejected data, we get a slip situation.  Such a slip is called an associativity miss. <br>  All this applies to reading data, and what will happen when writing?  All CPUs must be matched according to data, so before writing, you must invalidate the data from the caches of other CPUs.  Only after the invalidation is completed, can the processor safely write data.  If the data is in the CPU cache, but is read-only, this is called write miss.  Only after the CPU invalidates such data in the caches of other processors, the CPU can re-write (and read) this data.  Further, if some CPU tries to access the data while the other CPU has invalidated them for recording, it will get a miss called <i>communication miss</i> , as this situation occurs when the data is used for interaction (communication), such as , mutex or spin-lock <i>[means the mutex itself, not the data that it protects;</i>  <i>A mutex is some kind of flag]</i> . <br>  As can be seen, great efforts must be made to manage the data <i>coherence</i> for all CPUs.  With all this reading / invalidation / writing, it‚Äôs easy to imagine that the data will be lost or (worse,) different CPUs will have different data in their caches.  These problems are solved by the <i>data coherence protocol</i> (cache-coherency protocol) <br><br><h2>  Cache coherency protocol </h2><br>  The protocol manages the state of the cache lines, ensuring integrity and prohibiting data loss.  Such protocols can be very complex, with dozens of states, but for our purposes it is enough to consider the MESI protocol, which has 4 states. <br><h3>  MESI states </h3><br>  MESI is 4 possible states of the cache line: Modified-Exclusive-Shared-Invalid.  To support this protocol, each line, in addition to the actual data, must have a two-bit tag that stores its state. <br>  The <i>Modified</i> state means that the data in the cache lines has just been written by the processor-owner of the cache, and it is guaranteed that the change has not yet appeared in the caches of other CPUs.  We can say that one CPU owns the data.  Since the data in this line is the freshest, the cache line is ready to be written to memory (or to the next level cache), and the record must be made before the line can be filled with other data. <br>  The <i>Exclusive</i> state is very similar to Modified, except that the data has not yet been altered by the cache-owning processor.  This, in turn, means that the line contains the latest data consistent with the memory.  The CPU owner can write to this cache line at any time without notifying other CPUs, as well as push it out without any entry back to memory. <br>  The <i>Shared</i> state says that the line is duplicated in at least one other cache in another CPU.  The CPU owner of such a line cannot write to it without prior approval from other CPUs.  As for the Exclusive state, the line is consistent with memory, and can be pushed out without notifying other CPUs and without writing back to memory. <br>  The cache line in the <i>Invalid</i> state is empty, that is, it does not contain any data (or contains garbage).  When the cache needs to load new data, they are placed in the Invalid-line whenever possible. <br>  Since all CPUs must maintain the coherence of the system caches in general, the protocol describes the messages by which the state changes of the cache lines of all processors are coordinated. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  MESI protocol messages </h3><br>  Many of the transitions from one state to another require interaction between the CPUs.  If all CPUs are connected to a single shared bus, the following messages are sufficient: <br><ul><li>  <b>Read</b> : this message contains the physical address of the read cache line. </li><li>  <b>Read Response</b> : contains the data requested by the previous <i>Read</i> message.  <i>Read Response</i> can come from a memory subsystem or from another cache.  For example, if the data is already in a cache in the ‚Äúmodified‚Äù state, then such a cache may respond to <i>Read</i> </li><li>  <b>Invalidate</b> : contains the physical address of the cache line to be disabled.  <i>All</i> other caches must delete the data and respond to this message. </li><li>  <b>Invalidate Acknowledge</b> : The CPU that received the <i>Invalidate</i> should delete the required data and acknowledge it with the <i>Invalidate Acknowledge</i> message. </li><li>  <b>Read Invalidate</b> : This message contains the physical address of the read cache line.  At the same time, it dictates other caches to delete this data.  In essence, this is a combination of the <i>Read</i> and <i>Invalidate</i> messages.  This message requires a <i>Read Response</i> and <i>Invalidate Acknowledgment</i> response. </li><li>  <b>Writeback</b> : contains the address and the actual data to write to the memory.  Allows caches to clear the corresponding line in the <i>Modified</i> state. </li></ul><br>  As you can see, a multiprocessor machine is a messaging system, essentially a computer under the processor cover. <br><div class="spoiler">  <b class="spoiler_title">Question answer</b> <div class="spoiler_text">  <i>What happens if two processors invalidate the same cache line at the same time?</i> <br>  One of them (the ‚Äúwinner‚Äù) will access the shared bus first.  Other CPUs must disable their copies of this cache line and respond with ‚Äúinvalidate acknowledge‚Äù.  Of course, the ‚Äúloser‚Äù of the CPU can immediately initiate the ‚Äúread invalidate‚Äù transaction, so the win may be ephemeral. <br>  <i>If the message ‚Äúinvalidate‚Äù appears in a large multiprocessor system, all CPUs must answer ‚Äúinvalidate acknowledge‚Äù.</i>  <i>Wouldn't the explosive stream of such responses lead to a complete stop of the system?</i> <br>  Yes, it can result if a large (large-scale) system is built this way.  But systems such as, for example, NUMA, usually use the <i>directory-based</i> <i>[directory in NUMA - node, node]</i> protocol for supporting the coherence of caches just to prevent such cases. <br></div></div><br><br><h3>  MESI state diagram </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/b39/f5d/190/b39f5d1905719c3ef3d2cf4a927a6d64.png" align="right"><br>  Transitions in the figure on the right have the following meaning: <br><ul><li>  <b>Transition a (M -&gt; E)</b> : The cache line is written back into memory, but the CPU has left it in the cache and has the right to change it.  This transition requires a ‚Äúwriteback‚Äù message. </li><li>  <b>Transition b (E -&gt; M)</b> : CPU writes to the cache line, to which it has exclusive access.  This transition does not require any messages. </li><li>  <b>Jump c (M -&gt; I)</b> : The CPU receives a ‚Äúread invalidate‚Äù cache line in the ‚ÄúModified‚Äù state.  The CPU must delete its local copy and respond with ‚Äúread response‚Äù and ‚Äúinvalidate acknowledge‚Äù messages.  Thus, the CPU sends data and shows that it no longer has a copy of it. </li><li>  <b>Jump d (I -&gt; M)</b> : The CPU performs a read-modify-write (RMW) operation on data that is not in the cache.  It issues a ‚Äúread invalidate‚Äù signal, receiving data in a ‚Äúread response‚Äù.  The CPU completes the transition only when it receives all the ‚Äúinvalidate acknowledge‚Äù responses. </li><li>  <b>Transfer e (S -&gt; M)</b> : The CPU performs a read-modify-write (RMW) operation on data that was read-only.  It must initiate the ‚Äúinvalidate‚Äù signal and wait until it receives the full set of ‚Äúinvalidate acknowledge‚Äù responses. </li><li>  <b>Transition f (M -&gt; S)</b> : Some other processor is reading data, and this data is in the cache of our CPU.  As a result, the data becomes read-only, which can lead to writing to memory.  This transition is initiated by the reception of the ‚Äúread‚Äù signal.  The CPU responds with a ‚Äúread response‚Äù message containing the requested data. </li><li>  <b>Transition g (E -&gt; S)</b> : Some other processor reads data, and this data is in the cache of our CPU.  Data becomes shared and, therefore, read-only.  The transition is initiated by the reception of the ‚Äúread‚Äù signal.  The CPU responds with a ‚Äúread response‚Äù message containing the requested data. </li><li>  <b>Transition h (S -&gt; E)</b> : The CPU decides that it needs to write data to the cache line, and therefore sends an ‚Äúinvalidate‚Äù message.  The transition does not complete until the CPU receives the full set of invalidate acknowledge responses.  Other CPUs flush the cache line with their ‚Äúwriteback‚Äù message, so our CPU becomes the only one caching this data. </li><li>  <b>Transition i (E -&gt; I)</b> : Another CPU performs an RMW operation with data owned by our CPU, so our processor disables the cache line.  The transition begins with the ‚Äúread invalidate‚Äù message, our CPU responds with the ‚Äúread response‚Äù and ‚Äúinvalidate acknowledge‚Äù messages. </li><li>  <b>Jump j (I -&gt; E)</b> : The CPU saves the data to the new cache line and sends the ‚Äúread invalidate‚Äù message.  The CPU cannot complete the transition until it receives a ‚Äúread response‚Äù and a full set of ‚Äúinvalidate acknowledge‚Äù.  As soon as the recording is completed, the cache line goes to the ‚Äúmodified‚Äù state through the transition (b) </li><li>  <b>Jump k (I -&gt; S)</b> : The CPU loads the data into the new cache line.  The CPU sends a ‚Äúread‚Äù message and completes the transition, receiving a ‚Äúread response‚Äù </li><li>  <b>Transition l (S -&gt; I)</b> : Another CPU wants to save data to the cache line, which has the status read-only, as some third CPU (or, for example, ours) shares the data with us.  The transition begins by accepting ‚Äúinvalidate‚Äù, and our CPU responds with ‚Äúinvalidate acknowledge‚Äù </li></ul><br><div class="spoiler">  <b class="spoiler_title">Question answer</b> <div class="spoiler_text">  <i>How does iron handle such heavy transitions that require multiple delays?</i> <br>  Usually - the introduction of additional states.  But such states do not need to be stored in cache lines, that is, they are not cache line states.  At any given time, only a few cache lines can be in a transition state.  It is the presence of heavy, time-distributed transitions that makes real cache coherence support protocols much more complicated than the simplified MESI described here. <br></div></div><br><br><h3>  MESI protocol example </h3><br>  Let's take a look at how MESI works in terms of the cache line, using the example of a 4-processor system with direct memory mapping to the cache.  The data is in memory at address 0. The following table illustrates the change in data.  The first column is the serial number of the operation, the second is the number of the processor performing the operation, the third is which operation is in progress, the next four columns are the cache line status of each CPU (in the form of memory address / state), the final two columns are whether the memory contains valid data (V) or not (i). <br><img src="https://habrastorage.org/getpro/habr/post_images/a99/e79/82a/a99e7982a91c478da76845119766a52f.png"><br>  At first, the CPU cache lines are in the ‚Äúinvalid‚Äù state, and the memory contains valid data.  When CPU 0 reads data at address 0, the CPU 0 cache line goes to the ‚Äúshared‚Äù state and is aligned with the memory.  CPU 3 also reads data at address 0, so that the cache lines go to the ‚Äúshared‚Äù state in the caches of both CPUs, but are still consistent with the memory.  Next, CPU 0 loads data at address 8, which leads to crowding out the cache line and placing new data into it (read at address 8).  Next, CPU 2 reads data at address 0, but then realizes that it needs to write them (RMW operation), so it uses the ‚Äúread invalidate‚Äù signal to make sure that it has an exclusive copy of the data.  The ‚Äúread invalidate‚Äù signal invalidates the cache line of CPU 3 (although its data is still consistent with the memory).  Then, CPU 2 performs the recording, which is a part of the RMW operation, which switches the cache line to the ‚ÄúModified‚Äù state.  The data in memory is now obsolete.  CPU 1 performs an atomic increment and uses the ‚Äúread invalidate‚Äù signal to receive data.  It receives data from the CPU 2 cache, and in the CPU 2 itself, the data is disabled.  As a result, the data is located in the cache line of CPU 1 in the ‚Äúmodified‚Äù state and is still not consistent with the memory.  Finally, CPU 1 reads data at address 8, which resets the cache line to memory (using the ‚Äúwriteback‚Äù message). <br><div class="spoiler">  <b class="spoiler_title">"Question answer"</b> <div class="spoiler_text">  <i>The cache contains some data when we stopped our example.</i>  <i>And what sequence of operations should be in order to switch the cache lines of all CPUs to the ‚Äúinvalidate‚Äù state?</i> <br>  Such a sequence does not exist, except that the CPU supports a special instruction to flush the cache (‚Äúflush my cache‚Äù).  Most processors have such instructions. <br></div></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/64d/664/80b/64d66480b53046f7f1e529456e6a4b05.png" align="right"><br><h2>  Unwanted downtime during recording </h2><br>  Although the cache structure we reviewed earlier provides good read / write performance for the CPU that owns this data, the performance of the <i>first</i> write to a specific cache line is very poor.  To see this, consider the figure on the right. <br>  The figure shows the write delay by processor 0 of the cache line, which is in the cache of CPU 1. CPU 0 must wait for a rather long period of time until the cache line becomes available for writing - it will transfer from the cache of CPU 1 to the CPU 0 ownership. The time required for transferring the cache line from one CPU to another, usually <i>by orders of magnitude</i> longer than the duration of the instruction working with registers. <br>  But for CPU 0 there is no reason to wait that long: regardless of the data transmitted by CPU 1, CPU 0 will certainly overwrite it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e2a/361/14c/e2a36114c7a247db90e6ec64f35fb26e.png" align="right"><br><h3>  Store buffers </h3><br>  One method of dealing with unwanted downtime is to add a store buffer between each CPU and its cache, as shown in the figure to the right.  With such a buffer, CPU 0 can simply write a write operation to its store buffer and continue working.  After the necessary messaging, when the cache line finally transitions from CPU 1 to CPU 0, the data can be moved from the store buffer to the cache line of processor 0. <br>  However, this solution leads to additional problems, which we will consider in the next two sections. <br><br><h3>  Store forwarding </h3><br>  To see the first problem, called self-consistency, consider the following code: <br><pre><code class="hljs swift"><span class="hljs-number"><span class="hljs-number">1</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">2</span></span> b = a + <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-built_in"><span class="hljs-built_in">assert</span></span>( b == <span class="hljs-number"><span class="hljs-number">2</span></span> );</code> </pre> <br>  The variables ‚Äúa‚Äù and ‚Äúb‚Äù have an initial value of 0. The variable ‚Äúa‚Äù is in the cache of processor 1, and the variable ‚Äúb‚Äù is in the cache of processor 0. <br>  It would seem, how can this <code>assert</code> work at all?  However, if someone implemented a processor with an architecture like the one in the figure, he would be surprised.  Such a system could potentially produce the following sequence of events: <br><ul><li>  1. CPU 0 starts performing <code>a = 1</code> </li><li>  2. CPU 0 looks to see if ‚Äúa‚Äù is in its cache, and sees that it is not - miss </li><li>  3. Therefore, CPU 0 sends a ‚Äúread invalidate‚Äù signal to get exclusive rights to the cache line with ‚Äúa‚Äù </li><li>  4. CPU 0 writes ‚Äúa‚Äù to its store buffer </li><li>  .  CPU 1 receives a ‚Äúread invalidate‚Äù message and responds to it by sending the cache line with ‚Äúa‚Äù and invalidate its cache line </li><li>  6. CPU 0 starts to execute <code>b = a + 1</code> </li><li>  7. CPU 0 receives a response from CPU 1, which still contains the old, <i>zero</i> value ‚Äúa‚Äù, and places the line in its cache. </li><li>  8. CPU 0 loads ‚Äúa‚Äù from its cache - the <i>value zero</i> is <i>loaded</i> </li><li>  9. CPU 0 executes the cache write request stored in the store buffer by writing the value ‚Äúa‚Äù = 1 to the cache line. </li><li>  10. CPU 0 adds one to zero, read earlier as ‚Äúa‚Äù, and stores the result in the cache line ‚Äúb‚Äù (which, as we remember, is in the possession of CPU 0) </li><li>  11. CPU 0 executes <code>assert(b == 2)</code> and gives an error - <code>assert</code> works. </li></ul><br>  The problem is that we have two copies of ‚Äúa‚Äù - one in the cache and the second in the store buffer. <br><img src="https://habrastorage.org/getpro/habr/post_images/92d/495/285/92d495285d5667f0a115e643d871c6bc.png" align="right"><br>  This example violates a very important guarantee: <i>each CPU must always perform operations in the order they are specified in the program</i> (the so-called <i>program order</i> ).  For programmers, this guarantee is an intuitive requirement, so hardware-engineers had to implement <i>store forwarding</i> : each CPU reads data not only from its cache, but also from the store buffer.  In other words, each write operation can be directly transferred to the next read operation through the store buffer, without accessing the cache. <br>  With store forwarding, step 8 in the above example should read the correct value 1 of the variable ‚Äúa‚Äù from the store buffer.  As a result, the value of ‚Äúb‚Äù will be 2, which is what we need. <br><br><h3>  Write buffers and memory barriers </h3><br>  To see another problem, <i>global memory order</i> violation (global memory ordering), consider the following example, in which the variables ‚Äúa‚Äù and ‚Äúb‚Äù have an initial value of 0: <br><pre> <code class="hljs java"><span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">foo</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 2 </span></span>{ <span class="hljs-number"><span class="hljs-number">3</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">4</span></span> b = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">5</span></span> } <span class="hljs-number"><span class="hljs-number">6</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bar</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 8 </span></span>{ <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> ( b == <span class="hljs-number"><span class="hljs-number">0</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span>( a == <span class="hljs-number"><span class="hljs-number">1</span></span> ); <span class="hljs-number"><span class="hljs-number">11</span></span> }</code> </pre><br>  Let CPU 0 do <code>foo()</code> , and CPU 1 do <code>bar()</code> .  Suppose the memory containing ‚Äúa‚Äù is only in the cache of CPU 1, and the CPU 0 owns the memory containing ‚Äúb‚Äù.  Then the following sequence of actions is possible: <br><ul><li>  1. CPU 0 performs <code>a=1</code> .  The cache line for ‚Äúa‚Äù is not in the cache of CPU 0, so CPU 0 places the new value ‚Äúa‚Äù in its write buffer and issues a ‚Äúread invalidate‚Äù signal. </li><li>  2. CPU 1 executes <code>while (b==0) continue</code> , but ‚Äúb‚Äù is not in its cache, so it sends a ‚Äúread‚Äù message </li><li>  3. CPU 0 performs <code>b = 1</code> .  He already owns ‚Äúb‚Äù in his cache, that is, the corresponding cache line is in the ‚Äúexclusive‚Äù or ‚Äúmodified‚Äù state - so he has every right to keep the new value ‚Äúb‚Äù in his cache, without telling anyone anything </li><li>  4. CPU 0 receives the ‚Äúread‚Äù message and sends a cache line with the latest value ‚Äúb‚Äù in response, simultaneously translating this line to the ‚Äúshared‚Äù state in its cache </li><li>  5. CPU 1 receives the cache line with ‚Äúb‚Äù and places it in its cache. </li><li>  6. CPU 1 can now complete the execution <code>while (b == 0) continue</code> , since it sees that <code>b == 1</code> , and proceed to the next instruction </li><li>  7. CPU 1 executes <code>assert(a == 1)</code> .  Since CPU 1 works with the old value ‚Äúa‚Äù, the condition is not met </li><li>  8. CPU 1 receives the ‚Äúread invalidate‚Äù message and transfers the cache line with ‚Äúa‚Äù to CPU 0, simultaneously invalidating this line in its cache.  But too late </li><li>  9. CPU 0 receives the cache line with ‚Äúa‚Äù and writes from the buffer (flushes the store buffer to its cache) </li></ul><br><div class="spoiler">  <b class="spoiler_title">Question answer</b> <div class="spoiler_text">  <i>Why is CPU 0 in step 1 forced to send the message ‚Äúread invalidate‚Äù and not just ‚Äúinvalidate‚Äù?</i> <br>  Because the cache line contains not only the value of the variable ‚Äúa‚Äù.  The size of the cache line is quite large. <br></div></div><br>  Hardware engineers cannot help in this case, since the processors do not know anything about the relationship of variables in the program.  Therefore, engineers have introduced <i>memory barrier instructions</i> with which programmers can express such data links in a program.  The program fragment should be changed as follows: <br><pre> <code class="hljs java"><span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">foo</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 2 </span></span>{ <span class="hljs-number"><span class="hljs-number">3</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">4</span></span> smp_mb(); <span class="hljs-number"><span class="hljs-number">5</span></span> b =<span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">6</span></span> } <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bar</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 9 </span></span>{ <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> ( b == <span class="hljs-number"><span class="hljs-number">0</span></span> ) <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; <span class="hljs-number"><span class="hljs-number">11</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span>( a == <span class="hljs-number"><span class="hljs-number">1</span></span> ); <span class="hljs-number"><span class="hljs-number">12</span></span> }</code> </pre><br>  The memory barrier <code>smp_mb()</code> <i>[this is a real function from the Linux kernel]</i> tells the processor to reset the store buffer before making the next cache entry.  The CPU can either just stop, waiting until its store buffer becomes empty, or it can use the store buffer for subsequent entries, until all the entries already in the store buffer are executed <i>[thus some FIFO order is induced in the store buffer]</i> . <br><div class="spoiler">  <b class="spoiler_title">The sequence of the program with a memory barrier</b> <div class="spoiler_text"><ul><li>  1. CPU 0 performs <code>a=1</code> .  The cache line for ‚Äúa‚Äù is not in the cache of CPU 0, so CPU 0 places the new value ‚Äúa‚Äù in its write buffer and issues a ‚Äúread invalidate‚Äù signal. </li><li>  2. CPU 1 executes <code>while (b==0) continue</code> , but ‚Äúb‚Äù is not in its cache, so it sends a ‚Äúread‚Äù message </li><li>  3. CPU 0 executes <code>smp_mb()</code> and marks all items in its store buffer (including <code>a = 1</code> ). </li><li>  4. CPU 0 performs <code>b = 1</code> .  He already owns ‚Äúb‚Äù (that is, the corresponding cache line is in the ‚Äúmodified‚Äù or ‚Äúexclusive‚Äù state), but there are marked elements in the recording buffer.  Therefore, instead of writing the new ‚Äúb‚Äù value to its cache, it places the ‚Äúb‚Äù in its write buffer, but as an <i>unmarked</i> item </li><li>  5. CPU 0 receives a ‚Äúread‚Äù message and transfers the cache line with the initial value ‚Äúb‚Äù (= 0) to processor 1. It also changes the state of the cache line to ‚Äúshared‚Äù. </li><li>  6. CPU 1 receives the cache line with ‚Äúb‚Äù and places it in its cache. </li><li>  7. CPU 1 could now complete <code>while (b == 0) continue</code> , but it sees that <code>b=0</code> , so that it is forced to continue the cycle.  The new ‚Äúb‚Äù value is still hidden in the CPU 0 write buffer </li><li>  8. CPU 1 receives a ‚Äúread invalidate‚Äù message and transfers the cache line with ‚Äúa‚Äù to processor 0, invalidating its cache line. </li><li>  9. CPU 0 receives a cache line with ‚Äúa‚Äù and performs a previously buffered record, switching its cache line from ‚Äúa‚Äù to the ‚Äúmodified‚Äù state. </li><li>  10. Since the ‚Äúa‚Äù record is the only item marked with the <code>smp_mb()</code> ) call, CPU 0 can also write ‚Äúb‚Äù, but the cache line with ‚Äúb‚Äù is in the ‚Äúshared‚Äù state </li><li>  11. Therefore, CPU 0 sends an ‚Äúinvalidate‚Äù message to processor 1. </li><li>  12. CPU 1 receives an ‚Äúinvalidate‚Äù signal, invalidates the cache line with ‚Äúb‚Äù in its cache and sends an ‚Äúacknowledgment‚Äù signal to processor 0 </li><li>  13. CPU 1 executes <code>while (b == 0) continue</code> , but the cache line with ‚Äúb‚Äù is not in the cache, so it sends a ‚Äúread‚Äù message to the processor 0 </li><li>  14. CPU 0 receives an ‚Äúacknowledgment‚Äù message and transfers the cache line from ‚Äúb‚Äù to the ‚Äúexclusive‚Äù state, then stores the ‚Äúb‚Äù in the cache </li><li>  15. CPU 0 receives the ‚Äúread‚Äù signal and transfers the cache line with the ‚Äúb‚Äù processor 1. Along the way, the cache line with the ‚Äúb‚Äù is transferred to the ‚Äúshared‚Äù state </li><li>  16. CPU 1 receives a cache line with ‚Äúb‚Äù and writes it to its cache. </li><li>  17. CPU 1 can now complete execution <code>while (b == 0) continue</code> and continue further </li><li>  18. CPU 1 executes <code>assert(a == 1)</code> , but the cache line with ‚Äúa‚Äù is not in its cache.  As soon as he gets the value ‚Äúa‚Äù from CPU 0, he will be able to continue execution.  The ‚Äúa‚Äù value will be the freshest and <code>assert</code> will not work. </li></ul><br></div></div><br>  As you can see, even intuitively simple things lead to many complex steps in silicon. <br><br><h2>  Unwanted downtime during a sequence of entries </h2><br>  Unfortunately, each write buffer should be relatively small.  This means that if the CPU performs a large sequence of entries, it completely fills its store buffer (for example, if each entry leads to a miss).  In this case, the CPU must wait until all invalidations are completed so that it can then flush its buffer to the cache and continue execution.  The same situation may occur immediately after the memory barrier, when <i>all</i> subsequent write instructions must wait for the completion of invalidations, regardless of whether these entries lead to cache misses or not. <br>  This situation can be disrupted if the ‚Äúinvalidate acknowledge‚Äù messages are processed faster.  One way to achieve this is to enter an ‚Äúinvalidate‚Äù or ‚Äúinvalidate‚Äù <i>queue</i> for each CPU. <br><br><h3>  Invalididate queue </h3><br>  One of the reasons why invalidate acknowledgment messages is so slow is because the CPU must be sure that the corresponding cache line is indeed disabled.  Such invalidation can be quite long if the cache is busy, for example, if the processor intensively reads / writes data that is all in the cache.  In addition, in a short time interval, a whole stream of invalidation messages occurs, the CPU can not cope with them, which leads to idle rest of the CPUs. <br>  However, the CPU does not need to disable the cache line before sending a confirmation.  It can put an invalidate message in a queue, of course, with full understanding that this message will be processed before the processor sends other messages relating to this cache line. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/71e/1ea/9b4/71e1ea9b425a25b13e10adff90fd733e.png" align="right"><br><h3>  Invalidate Queues and proof of disability </h3><br>  The figure on the right shows the system with invalidate queues.  A processor with an invalidate queue can acknowledge invalidate messages as soon as they appear in the queue, instead of waiting for the cache line to become invalid.  Of course, the CPU must be consistent with its queue when it prepares to send an invalidate message: if the queue already contains an invalidation record for this cache line, the processor cannot immediately send its invalidate message.  Instead, he must wait until the corresponding entry from the queue is processed. <br>  Placing an item in the invalidate queue is essentially the processor‚Äôs promise to process this invalidate message before sending any MESI protocol signal related to a given cache line. <br>  However, buffering invalidate signals leads to an additional possibility to disrupt the order of memory operations, as described below. <br><br><h3>  Invalidate queues and memory barriers </h3><br>  Suppose processors place invalidate requests in a queue and respond immediately.  This approach minimizes the delay of cache invalidation, but may break the memory barrier, as we will see in the following example. <br>  Let the variables ‚Äúa‚Äù and ‚Äúb‚Äù initially be zero and ‚Äúa‚Äù be in the caches of both processors in the ‚Äúshared‚Äù state (that is, read-only), and ‚Äúb‚Äù is in the possession of CPU 0 (that is, the cache line in the ‚ÄúExclusive‚Äù or ‚Äúmodified).  Suppose CPU 0 performs <code>foo()</code> , and CPU 1 performs <code>bar()</code> : <br><pre> <code class="hljs java"><span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">foo</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 2 </span></span>{ <span class="hljs-number"><span class="hljs-number">3</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">4</span></span> smp_mb(); <span class="hljs-number"><span class="hljs-number">5</span></span> b = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">6</span></span> } <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">bar</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> 9 </span></span>{ <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (b == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; <span class="hljs-number"><span class="hljs-number">11</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span>(a == <span class="hljs-number"><span class="hljs-number">1</span></span>); <span class="hljs-number"><span class="hljs-number">12</span></span> }</code> </pre><br>  The sequence of operations may be as follows: <br><ul><li>  1. CPU 0 performs <code>a=1</code> .  The corresponding cache line is read-only in the CPU 0 cache, so the processor places the new ‚Äúa‚Äù value in its store buffer and sends the ‚Äúinvalidate‚Äù message so that the CPU 1 resets its cache line with ‚Äúa‚Äù </li><li>  2. CPU 1 performs <code>while(b == 0) continue</code> , but ‚Äúb‚Äù is not in its cache.  Therefore, it sends the message ‚Äúread‚Äù </li><li>  3. CPU 1 receives ‚Äúinvalidate‚Äù from CPU 0, stores it in its invalidate queue and immediately responds </li><li> 4. CPU 0    CPU 1,     <code>smp_mb()</code> ( 4)     ‚Äúa‚Äù       </li><li> 5. CPU 0  <code>b=1</code> .    -  ‚Äúb‚Äù,        ‚Äúb‚Äù    </li><li> 6. CPU 0  ‚Äúread‚Äù-   -    ‚Äúb‚Äù  1,      ‚Äúshared‚Äù </li><li> 7. CPU 1  -  ‚Äúb‚Äù       </li><li> 8.  CPU 1   <code>while(b == 0) continue</code>       </li><li> 9. CPU 1  <code>assert(a == 1)</code> .         ‚Äúa‚Äù, <code>assert</code>     </li><li>   , CPU 1     ‚Äúinvalidate‚Äù-     - ‚Äúa‚Äù </li></ul><br><div class="spoiler"> <b class="spoiler_title">-</b> <div class="spoiler_text"> <i>   1   ‚Äúinvalidate‚Äù,   ‚Äúread invalidate‚Äù?      ,     -?</i> <br>  CPU 0     ,       (shared) read-only -   ‚Äúa‚Äù.  ,   CPU 0,    ,        -.     ‚Äúinvalidate‚Äù <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, in improving the speed of responses to disability there is little use if it leads to ignoring the memory barrier. </font><font style="vertical-align: inherit;">Therefore, memory barriers must interact with the invalidate queue: when the processor executes the memory barrier, it must mark all elements in its invalidate queue and slow down all subsequent reads until the invalidate queue is completely processed by it.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Barriers to read and write </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In previous chapters, memory barriers were used to mark items in the store buffer and invalidate queue. But in the last example, there are no reasons for </font></font><code>foo()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">interacting with the invalidate queue (since there is no read), as well as no reason for </font></font><code>bar()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">interacting with the write buffer (because there is no record). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Many architectures provide weaker (and, as a result, faster) memory barriers that allow you to streamline read only or write only. Roughly speaking, the read memory barrier only interacts with the invalidate queue (marks its elements, that is, it induces some order in the queue), and the write memory barrier only interacts with the store buffer (also marks its elements, brings order in the buffer). The full barrier interacts with both.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The effect of these semi-barriers is this: the reading barrier orders only the loads for the processor that performs the barrier. </font><font style="vertical-align: inherit;">All readings before the barrier are completely completed, and only then do readings begin after the barrier. </font><font style="vertical-align: inherit;">Similarly, a recording barrier orders only recording (stores) for its processor: all recordings up to the barrier are completed, and only then begin recording (stores) after the barrier. </font><font style="vertical-align: inherit;">The full barrier organizes reads and writes, but again, only for the processor executing this barrier. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we update our example so that </font></font><code>foo</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and </font></font><code>bar</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">will use the read / write barriers, we get the following:</font></font><br><pre> <code class="hljs pgsql"><span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span> foo() <span class="hljs-number"><span class="hljs-number">2</span></span> { <span class="hljs-number"><span class="hljs-number">3</span></span> a = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">4</span></span> smp_wmb(); //   <span class="hljs-number"><span class="hljs-number">5</span></span> b = <span class="hljs-number"><span class="hljs-number">1</span></span>; <span class="hljs-number"><span class="hljs-number">6</span></span> } <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span> bar() <span class="hljs-number"><span class="hljs-number">9</span></span> { <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (b == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span>; <span class="hljs-number"><span class="hljs-number">11</span></span> smp_rmb(); //   <span class="hljs-number"><span class="hljs-number">11</span></span> <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span>(a == <span class="hljs-number"><span class="hljs-number">1</span></span>); <span class="hljs-number"><span class="hljs-number">12</span></span> }</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Some architectures have even a larger set of diverse barriers, but understanding the three options considered is enough to introduce memory barriers into the theory. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a2/b44/961/1a2b44961990fa12e41da5013b62428d.jpg" align="right"><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Translator's epilogue </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This completes the translation. </font><font style="vertical-align: inherit;">Further, the original is a brief overview of those barriers that provide modern architecture. </font><font style="vertical-align: inherit;">I am referring to those interested in the </font></font><a href="http://irl.cs.ucla.edu/~yingdi/web/paperreading/whymb.2010.06.07c.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">original</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - in terms of volume, about the same remained there. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's take stock. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, we have two operations of interaction of the processor with the cache / memory - read (load) and write (store). </font><font style="vertical-align: inherit;">Two operations give us a total of four different memory barriers:</font></font><br><pre> <code class="hljs sql">op1; // store  <span class="hljs-keyword"><span class="hljs-keyword">load</span></span> barrier ; // memory fence op2; // store  <span class="hljs-keyword"><span class="hljs-keyword">load</span></span></code> </pre><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Load / Load</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - orders the previous load-instructions with the subsequent. </font><font style="vertical-align: inherit;">As we have seen, this barrier ‚Äúbrings order‚Äù to the invalidate queue: it either processes the queue entirely, or puts some labels in it that determine the order in which the queue is processed in the future. </font><font style="vertical-align: inherit;">Relatively light barrier</font></font></li><li> <b>Store/Store</b> ‚Äì   store-  .   store buffer:      (   ‚Äì       ), ,  , -    store buffer,   -        .       </li><li> <b>Load/Store</b> ‚Äì   load-   store.     ? ,   . ,      </li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Store / Load</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - organizes the previous store-instructions with subsequent load. </font><font style="vertical-align: inherit;">Our store instructions go to the store buffer. </font><font style="vertical-align: inherit;">That is, this barrier should be ordered by the store buffer - just put some tags in it? </font><font style="vertical-align: inherit;">No, it seems that is not enough. </font><font style="vertical-align: inherit;">After all, there is still speculative reading - this barrier should also be limited. </font><font style="vertical-align: inherit;">On the basis of what I know about modern architectures, I can conclude that this barrier leads to the full processing of the store buffer, which is quite a heavy operation. </font><font style="vertical-align: inherit;">Bottom line: this is the hardest barrier of all</font></font></li></ul><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Congratulations, we brought down the memory barriers of the Sparc architecture!</font></font></b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/ad4/0b7/07a/ad40b707aff57f6c540d0c4d1daa89a1.png" align="right"><br>  Sparc (     RMO ‚Äì Relaxed Memory Ordering)   <code>membar</code> ,        ‚Äî   ,     .    ‚Äî <code>#LoadLoad</code> , <code>#LoadStore</code> , <code>#StoreStore</code> , <code>#StoreLoad</code> (      , ‚Äî      ).  <code>membar</code>       ,     : <br><ul><li> <code>Load1; membar #LoadLoad; Load2</code> </li> <li> <code>Load; membar #LoadStore; Store</code> </li> <li> <code>Store; membar #StoreLoad; Load</code> </li> <li> <code>Store1; membar #StoreStore; Store2</code> </li> </ul><br> ,     Sparc     : <br><pre> <code class="hljs smalltalk">membar <span class="hljs-symbol"><span class="hljs-symbol">#LoadLoad</span></span>|<span class="hljs-symbol"><span class="hljs-symbol">#LoadStore</span></span>|<span class="hljs-symbol"><span class="hljs-symbol">#StoreStore</span></span>|<span class="hljs-symbol"><span class="hljs-symbol">#StoreLoad</span></span></code> </pre><br>  ,         ,   ‚Äî <code>#StoreLoad</code> .      Sparc,      weakly-ordered . <br></div></div><br><br><h3>  Further </h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, we looked at where the memory barriers come from. </font><font style="vertical-align: inherit;">We learned that they are a necessary evil. </font><font style="vertical-align: inherit;">We even tried to arrange them somehow in the code.</font></font><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Displacing scheduler and memory barriers</font></font></b> <div class="spoiler_text">  ,    ‚Äî   <i></i>        ( , ,  Intel Itanium ‚Äì       load/store/RMW-).       lock-free .            /.      ,       .  ( )        . ,      ‚Äì     ,     ? <br>     .            <i></i>    - ,   ,   .   lock-free    , ,  ,   -  . <br></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I would call the considered approach to the placement of memory barriers the read / wrire approach. During the development of the C ++ 11 standard, a read / write approach to the problems of streamlining memory access was considered too tied to the architecture and the </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">acquire / release semantics</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> was developed </font><font style="vertical-align: inherit;">that formed the basis of the standard. As it was repeatedly stated in the article, the memory barriers directly affect only the processor that executes the barrier, and only indirectly (via the MESI protocol) - on other processors. The acquire / release model acts differently - it postulates how </font><font style="vertical-align: inherit;">different parallel threads (ie, processors / cores) </font><font style="vertical-align: inherit;">should </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">interact</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and almost nothing is said about how to achieve this. In fact, the implementation of this model is the use of certain memory barriers.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I am going to talk about the C ++ 11 memory model in the next article, </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Basics</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lock-free data structures</font></font></b> <div class="spoiler_text">  <a href="http://habrahabr.ru/company/ifree/blog/195770/">Start</a> <br> : <br><ul><li> <a href="http://habrahabr.ru/company/ifree/blog/195948/">   </a> </li><li> <a href="http://habrahabr.ru/company/ifree/blog/196548/">    </a> </li><li> <a href="http://habrahabr.ru/company/ifree/blog/197520/"> </a> </li></ul><br> : <br><ul><li> <a href="http://habrahabr.ru/company/ifree/blog/202190/">  </a> </li><li> <a href="http://habrahabr.ru/company/ifree/blog/206984/">RCU</a> </li><li> <a href="http://habrahabr.ru/company/ifree/blog/216013/"> </a> </li><li> <a href="http://habrahabr.ru/company/ifree/blog/219201/"> </a> </li><li> <a href="http://habrahabr.ru/post/230349/"> </a> </li><li> <a href="http://habrahabr.ru/post/250383/">Concurrent maps: </a> </li><li> <a href="http://habrahabr.ru/post/250523/">Concurrent maps: rehash, no rebuild</a> </li><li>  <a href="http://habrahabr.ru/post/250815/">Concurrent maps: skip list</a> </li><li> <a href="https://habrahabr.ru/post/251267/">Concurent maps: </a> </li><li>  <a href="https://habrahabr.ru/post/314948/">Iterators: multi-level array</a> </li><li> <a href="https://habrahabr.ru/post/317882/">Iterable list</a> </li></ul><br> : <br><ul><li> <a href="http://habrahabr.ru/company/ifree/blog/196834/">  libcds</a> </li></ul><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/196548/">https://habr.com/ru/post/196548/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../196536/index.html">Promo of new Zoloo game for fun friends groups</a></li>
<li><a href="../196538/index.html">The digest of interesting news and materials from the world of PHP for the last two weeks, No. 27 (September 22 - October 6, 2013)</a></li>
<li><a href="../196542/index.html">NVIDIA Shield - the best portable boom console</a></li>
<li><a href="../196544/index.html">Stored Functions on C in PostgreSQL</a></li>
<li><a href="../196546/index.html">We are preparing a web application for the zoo versions of Android</a></li>
<li><a href="../196550/index.html">Configuring Vim to work with Python code</a></li>
<li><a href="../196556/index.html">Test of the prototype of the iLook Media Center</a></li>
<li><a href="../196560/index.html">Introduction to the analysis of the complexity of algorithms (part 1)</a></li>
<li><a href="../196562/index.html">Wi-Fi Mesh networks for the smallest</a></li>
<li><a href="../196564/index.html">Task sync plugin for Redmine</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>