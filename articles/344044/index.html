<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>OpenDataScience and Mail.Ru Group open course materials on machine learning and new launch</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, the OpenDataScience and Mail.Ru Group conducted an open machine learning course. In the last announcement a lot has been said about the cour...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>OpenDataScience and Mail.Ru Group open course materials on machine learning and new launch</h1><div class="post__text post__text-html js-mediator-article"><p>  Recently, the OpenDataScience and Mail.Ru Group conducted an open machine learning course.  In the <a href="https://habrahabr.ru/company/ods/blog/334960/">last announcement a</a> lot has been said about the course.  In this article we will share the course materials, as well as announce a new launch. </p><br><p><img src="https://habrastorage.org/web/687/dfe/6c0/687dfe6c0f7549abb0179c90b23c5884.jpg"></p><br><p>  <strong>UPD:</strong> now the course is in English under the brand <a href="http://mlcourse.ai/">mlcourse.ai</a> with <a href="https://medium.com/open-machine-learning-course">articles</a> on Medium, and materials on Kaggle ( <a href="https://www.kaggle.com/kashnitsky/mlcourse">Dataset</a> ) and on <a href="">GitHub</a> . </p><br><p>  Who can not wait: the new launch of the course - February 1, registration is not needed, but so that we remember you and separately invited, fill out the <a href="https://docs.google.com/forms/d/1wfxD88TCTxNhpDUckJ-p7H-wxbwJ3lf1ATrOIvyGlYA/viewform%3Fedit_requested%3Dtrue">form</a> .  The course consists of a series of articles on Habr√© ( <a href="https://habrahabr.ru/company/ods/blog/322626/">Primary data analysis with Pandas</a> is the first one), supplementary lectures on the <a href="https://www.youtube.com/channel/UCgGADKKGalfwSNbpSyM5ryg">YouTube channel</a> , reproducible materials (Jupyter notebooks in the <a href="">github repositories of the</a> course), homework assignments, Kaggle Inclass competitions, tutorials and individual <a href="https://github.com/Yorko/mlcourse.ai/wiki/%25D0%2598%25D0%25BD%25D0%25B4%25D0%25B8%25D0%25B2%25D0%25B8%25D0%25B4%25D1%2583%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B5-%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D1%258B-%25D0%25B8-%25D1%2582%25D1%258C%25D1%258E%25D1%2582%25D0%25BE%25D1%2580%25D0%25B8%25D0%25B0%25D0%25BB%25D1%258B">projects</a> on data analysis.  The main news will be in <a href="https://vk.com/club158557357">the</a> VKontakte <a href="https://vk.com/club158557357">group</a> , and life during the course will be shared with Slack OpenDataScience ( <a href="http://ods.ai/">join</a> ) in the <em>#mlcourse_ai</em> channel. </p><a name="habracut"></a><br><h2 id="plan-stati">  Article layout </h2><br><ul><li>  How is our course different from others? </li><li>  Course materials </li><li>  More about the new launch </li></ul><br><h2 id="chem-kurs-otlichaetsya-ot-drugih">  How is the course different from others? </h2><br><h3 id="1-ne-dlya-novichkov">  1. Not for beginners </h3><br><p>  Often you will be told that nothing is required of you, in a couple of months you will become an expert in data analysis.  I still remember Andrew Ng's phrase from his basic Machine Learning course: ‚ÄúYou don‚Äôt have to know what a derivative is, and now you‚Äôll understand how optimization algorithms work in machine learning.‚Äù  Or "you are almost a data analysis expert", etc.  With all the immense respect for the professor - this is hard marketing and jaundice.  You will not understand optimization without knowledge of derivatives, the foundations of matan and linear algebra!  Most likely you will not even become a Middle Data Scientist, having completed a couple of courses (including ours).  It will not be easy, and more than half of you will fall off in about 3-4 weeks.  If you are wannabe, but are not ready to immerse yourself in math and programming, see the beauty of machine learning in formulas and achieve results by typing tens and hundreds of lines of code - this is not for you.  But we hope you still here. </p><br><p>  In connection with the above, we indicate the threshold of entry - knowledge of higher mathematics at the basic (but not bad) level and mastering the basics of Python.  How to prepare, if you do not have this yet, is described in detail in <a href="https://vk.com/club158557357">the VKontakte group</a> and here under the spoiler, just below.  In principle, you can complete the course without mathematics, but then see the following picture.  Of course, as far as the date of the Scientist needs to know mathematics - this is holivar, but we are here on the side of Andrei Karpaty, <a href="https://medium.com/%40karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes you should understand backprop</a> .  Well, in general, without mathematics in Data Science, it‚Äôs almost like sorting with a bubble: a problem can be solved, but it can be done better, faster and smarter.  Well, without mathematics, of course, it‚Äôs impossible to get to the state-of-the-art, and it‚Äôs very interesting to follow him. </p><br><div class="spoiler">  <b class="spoiler_title">Math and Python</b> <div class="spoiler_text"><p>  <strong>Maths</strong> </p><br><ol><li>  If quickly, then you can go through the <a href="https://yadi.sk/d/yEXkABC_353Zmh">notes</a> from the specialization of Yandex and MIPT on Coursera (share with permission). </li><li>  If you approach the issue thoroughly, one <a href="https://ocw.mit.edu/courses/mathematics/">link</a> to the MIT Open Courseware is enough.  In Russian, a cool source is the WKI <a href="http://wiki.cs.hse.ru/">-page of</a> courses of the FKN HSE.  But I would take the <a href="https://mipt.ru/education/chair/mathematics/process/progs2.php">program of</a> MIPT 2 courses and walked through the main problem book, there is a minimum of theory and a lot of practice. </li><li>  And of course, nothing can replace good books (here you can also mention the <a href="https://cache-mskm909.cdn.yandex.net/download.cdn.yandex.net/shad/shad_program_v3.pdf">program of ShAD</a> ): </li></ol><br><ul><li>  Mathematical analysis - Kudryavtsev; </li><li>  Linear algebra - Kostrikin; </li><li>  Optimization - Boyd (English); </li><li>  Probability Theory and Statistics - Kibzun. </li></ul><br><p>  <strong>Python</strong> </p><br><ol><li>  The quick version is browser tutorials √† la CodeAcademy, Datacamp and Dataquest, right there I can specify my <a href="https://github.com/Yorko/python_intro">repository</a> . </li><li>  Thorough - for example, email <a href="https://habrahabr.ru/company/mailru/blog/336880/">course</a> Coursera or MIT-shny <a href="https://www.edx.org/course/introduction-computer-science-mitx-6-00-1x-11">course</a> "Introduction to Computer Science and Programming Using Python". </li><li>  Advanced level - the <a href="https://compscicenter.ru/courses/python/2015-autumn/">course of the</a> St. Petersburg Computer Science Center. </li></ol></div></div><br><h3 id="2-s-teoriya-vs-praktika-steoriya-i-praktika">  2. <s>Theory vs.</s>  <s>Practice</s> Theory and Practice </h3><br><p>  There are a lot of machine learning courses, there are really cool ones (as specialization is ‚ÄúMachine learning and data analysis‚Äù), but many fall into one of the extremes: either too much theory (PhD guy), or, conversely, practice without understanding the basics (data monkey) . </p><br><div style="text-align:center;"><img width="70%" src="https://habrastorage.org/webt/-u/jx/au/-ujxauvf6ylizjvrr_ad1s1xrps.png"></div><br><p>  We are looking for the optimal ratio: we have a lot of theory in the articles on Habr√© (the <a href="https://habrahabr.ru/company/ods/blog/323890/">fourth article</a> on linear models is indicative), we try to present it as clearly as possible, we present it even more popularly in lectures.  But the practice of the sea - homework, 4 competitions Kaggle, projects ... and that's not all. </p><br><h3 id="3-zhivoe-obschenie">  3. Live communication </h3><br><p>  What is missing in most courses is live communication.  Beginners sometimes need just one short piece of advice to get off the ground and save hours, or even dozens of hours.  Coursera forums usually die off at some point.  The uniqueness of our course is active communication and an atmosphere of mutual support.  In Slack OpenDataScience, when passing a course, they will help with any question, chat lives and thrives, your own humor arises, someone trolls someone ... But the main thing is that homework authors and articles ‚Äî in the same chat room ‚Äî are always ready to help. </p><br><h3 id="4-kaggle-v-deystvii">  4. Kaggle in action </h3><br><p><img src="https://habrastorage.org/webt/zs/qx/5g/zsqx5gypatzik6wlmrvkjql2ogi.jpeg"><br>  <em>From the <a href="https://vk.com/weirdkerneltricks">public</a> VKontakte "Memes about machine learning for adult men."</em> </p><br><p>  Kaggle competitions are a great way to quickly get data analysis into practice.  Usually they begin to participate after completing a basic machine learning course (as a rule, the Andrew Ng course, the author is certainly charismatic and speaks well, but the course is already very outdated).  During the course we will be invited to participate in as many as 4 competitions, 2 of them are part of homework, you just need to achieve a certain result from the model, and 2 others are already full-fledged competitions where you have to create (come up with signs, choose models) and outrun your comrades. </p><br><h3 id="5-besplatno">  5. Free </h3><br><p>  Well, also an important factor, which is already there.  Now in the wake of the spread of machine learning, you will find quite a few courses offering to train you for a very tidy compensation.  And here everything is free and, without false modesty, at a very decent level. </p><br><h2 id="materialy-kursa">  Course materials </h2><br><p>  Here we briefly describe the 10 topics of the course, what they are devoted to, why a course of basic machine learning cannot do without them, and what new we have brought. </p><br><p>  <strong>Topic 1.</strong> Primary data analysis with Pandas.  <a href="https://habrahabr.ru/company/ods/blog/322626/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/dEFxoyJhm3Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img width="300" align="right" src="https://habrastorage.org/files/10c/15f/f3d/10c15ff3dcb14abdbabdac53fed6d825.jpg">  I want to immediately start with machine learning, to see math in action.  But 70-80% of the time spent working on a real project is a mess with the data, and here Pandas is very good, I use it almost every day in my work.  This article describes the basic Pandas methods for primary data analysis.  Then we analyze the data set on the outflow of clients of the telecom operator and try to ‚Äúpredict‚Äù the outflow without any training, simply relying on common sense.  In no case can one underestimate this approach. </p><br><p>  <strong>Subject 2.</strong> Visual data analysis with Python.  <a href="https://habrahabr.ru/company/ods/blog/323210/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/vm63p8Od0bM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/62b/42b/b53/62b42bb533f44cbaa8d6306332512555.png" width="300" align="right">  The role of visual data analysis is difficult to overestimate - this is how new signs are created, patterns and insights are sought in the data.  K.V.  Vorontsov gives an example of how exactly, thanks to the visualization, guessed that during boosting, classes continue to ‚Äúmove apart‚Äù as trees are added, and then this fact was proved theoretically.  In the lecture, we consider the main types of images that are usually built for the analysis of signs.  We will also discuss how to look into multidimensional space in general - with the help of the t-SNE algorithm, which sometimes helps to draw such Christmas decorations. </p><br><p>  <strong>Topic 3.</strong> Classification, decision trees and the method of nearest neighbors. <br>  <a href="https://habrahabr.ru/company/ods/blog/322534/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/p9Hny3Cs6rk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/30f/e14/6d4/30fe146d40dc414dbd61ba87a83585bb.jpeg" width="300" align="right">  Here we will start talking about machine learning and about two simple approaches to solving the classification problem.  Again, in a real project, you should start with the simplest approaches, and it is the decision trees and the nearest neighbors method (as well as linear models, the next topic) that you should try first after the heuristics.  We will touch on the important issue of model quality assessment and cross-validation.  We will discuss in detail the pros and cons of trees and the method of nearest neighbors.  The article is long, but especially decision trees deserve attention - it is on their basis that random forest and boosting are built - algorithms that you will probably use most of all in practice. </p><br><p>  <strong>Topic 4.</strong> Linear classification and regression models. <br>  <a href="https://habrahabr.ru/company/ods/blog/323890/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/oTXGQ-_oqvI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/256/a5d/ed0/256a5ded03274e0f87ccf97164c31c35.png" width="300" align="right">  This article will be the size of a small brochure and not without reason: linear models are the most widely used approach to forecasting in practice.  This article is like our miniature course: a lot of theory, a lot of practice.  We will discuss the theoretical background of the least squares method and logistic regression, as well as the advantages of the practical application of linear models.  We note here that there will be no excessive theorizing; the approach to linear models in machine learning is different from the statistical and econometric ones.  In practice, we will apply logistic regression to the very real <a href="https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2">task of</a> identifying a user by the sequence of visited sites.  After the fourth homework, many people will drop out, but if you do it, you will already have a very good idea of ‚Äã‚Äãwhat algorithms are used in production-systems. </p><br><p>  <strong>Theme 5.</strong> Compositions: bagging, random forest.  <a href="https://habrahabr.ru/company/ods/blog/324402/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/G0DmuuFeC30" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/34b/f0f/2e4/34bf0f2e450d43b4a3ec24bdacde65a0.jpg" width="300" align="right">  Here again the theory is interesting, and practice.  We will discuss why ‚Äúcrowd wisdom‚Äù works for machine learning models, and many models work better than one, even the best.  But in practice, we will smoke a random forest (the composition of many decision trees) - what is worth trying if you don‚Äôt know which algorithm to choose.  We will discuss in detail the numerous advantages of a random forest and its areas of application.  And, as always, not without flaws: there are still situations where linear models will work better and faster. </p><br><p>  <strong>Topic 6.</strong> Construction and selection of signs.  Applications in word processing tasks, images and geodata.  <a href="https://habrahabr.ru/company/ods/blog/325422/">An article on Habr√©</a> , a lecture about regression and regularization. </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/c_EsDMF3rsI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/cd7/2d8/d16/cd72d8d16d8f409898546ba5d397240f.jpg" width="300" align="right">  Here the plan of articles and lectures differs a little (only once), the fourth theme of linear models is too big.  The article describes the main approaches to the extraction, transformation and construction of signs for machine learning models.  In general, this occupation, the construction of features, is the most creative part of Data Scientist's work.  And of course, it is important to know how to work with various data (texts, images, geodata), and not just with a ready Pandas data frame. </p><br><p>  The lecture will again discuss linear models, as well as the basic technique for adjusting the complexity of ML models - regularization.  In the book "Deep Learning" they even refer to one well-known comrade (too lazy to go for proof-link), who claims that in general "all machine learning is the essence of regularization."  This, of course, is an exaggeration, but in practice, in order for the models to work well, they must be <em>tuned</em> , that is, it is the correct use of regularization. </p><br><p>  <strong>Topic 7.</strong> Teaching without a teacher: PCA, clustering.  <a href="https://habrahabr.ru/company/ods/blog/325654/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/qmW968tw3AM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5be/e8a/297/5bee8a297a98ce38da75f0a775f5eb3a.png" width="220" align="right">  Here we turn to an extensive topic of study without a teacher - this is when there is data, but the target feature that I would like to predict - here it is not.  There are such <em>unmarked</em> data a dime a dozen, and we must be able to gain from them.  We will discuss only 2 types of tasks - clustering and dimension reduction.  In your homework, you will analyze the data from accelerometers and gyroscopes of mobile phones and try to cluster on them the carriers of the phones, select types of activities. </p><br><p>  <strong>Topic 8.</strong> Training in gigabytes with Vowpal Wabbit.  <a href="https://habrahabr.ru/company/ods/blog/326418/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/MnLc7xKSAsk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/96c/ffb/939/96cffb939d3848ebb814641f0c8435f0.jpeg" width="250" align="right">  The theory here is an analysis of stochastic gradient descent; it was this optimization method that allowed us to successfully train both neural networks and linear models on large training samples.  Here we will also discuss what to do when there are too many signs (a trick to hash the values ‚Äã‚Äãof the attributes) and go to Vowpal Wabbit, a utility that allows you to train a model in gigabytes of data and sometimes even acceptable quality in minutes.  Consider many applications in various tasks - the classification of short texts, as well as the categorization of questions on StackOverflow.  While the <a href="https://www.kaggle.com/kashnitsky/vowpal-wabbit-tutorial-blazingly-fast-learning">translation of</a> this particular article (in the form of Kaggle Kernel) serves as an example of how we will submit the material in English to Medium. </p><br><p>  <strong>Topic 9.</strong> Time series analysis using Python.  <a href="https://habrahabr.ru/company/ods/blog/327242/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/nQjul-5_0_M" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/files/234/dcd/2fc/234dcd2fca894e80bc4c753e67bf613f.jpg" width="250" align="right">  Here we discuss various methods of working with time series: what stages of data preparation are needed for models, how to get short-term and long-term forecasts.  Let's go through various types of models, ranging from simple moving averages to gradient boosting.  We will also look at ways to search for anomalies in the time series and talk about the advantages and disadvantages of these methods. </p><br><p>  <strong>Theme 10.</strong> Gradient boosting.  <a href="https://habrahabr.ru/company/ods/blog/327250/">Article on Habr√©</a> </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/x5Bz9ChD7N0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p><img src="https://habrastorage.org/web/4a9/edb/082/4a9edb082408442ea47a12b75f19d122.jpg" align="right" width="40%">  Well, where without a gradient boosting ... this is Matrixnet (Yandex search engine), and Catboost is a new generation of boosting in Yandex, and the search engine Mail.Ru.  Busting solves all three main tasks of learning with a teacher - classification, regression and ranking.  And in general, he wants to be called the best algorithm, and this is close to the truth, but there are no better algorithms.  But if you do not have too much data (fits into the RAM), not too many signs (up to several thousand), and heterogeneous signs (categorical, quantitative, binary, etc.), then, as the experience of Kaggle competitions shows, <em>almost certainly the</em> gradient boost will show up best in your task.  Therefore, it was not without reason that so many cool implementations appeared - Xgboost, LightGBM, Catboost, H2O ... </p><br><p>  Again, we will not limit ourselves to the ‚Äúhow to sew Ixbust‚Äù manual, but we will examine in detail the theory of boosting, and then consider it in practice, in a lecture we will come to Catboost.  Here the task will be to beat the baseline in the competition - this will give a good idea of ‚Äã‚Äãthe methods used in many practical tasks. </p><br><h2 id="podrobnee-o-novom-zapuske">  More about the new launch </h2><br><p>  The course starts on February 5, 2018.  During the course will be: </p><br><ul><li>  live lectures in the Moscow office of Mail.Ru Group, on Mondays from February 5, 7 to 10 pm.  The video recordings of lectures are <a href="https://goo.gl/wg3582">former</a> (youtube), but there will be comments, additions and improvements to them; </li><li>  articles on Habr√© former, <a href="https://habrahabr.ru/company/ods/blog/322626/">here is the</a> first.  The articles will contain actual homework and deadlines for them, information will be duplicated in <a href="https://vk.com/club158557357">the</a> VKontakte <a href="https://vk.com/club158557357">group</a> and in the <em>#mlcourse_ai</em> channel in Slack OpenDataScience; </li><li>  competitions, projects, tutorials and other activities, they are described in <a href="https://habrahabr.ru/company/ods/blog/334960/">this</a> article and in <a href="https://github.com/Yorko/mlcourse.ai/wiki/%25D0%2592%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B9-%25D0%25B7%25D0%25B0%25D0%25BF%25D1%2583%25D1%2581%25D0%25BA-%25D0%25BA%25D1%2583%25D1%2580%25D1%2581%25D0%25B0:-%25D0%25B2%25D1%2581%25D0%25B5-%25D0%25B0%25D0%25BA%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8,-%25D0%25B7%25D0%25B0-%25D0%25BA%25D0%25BE%25D1%2582%25D0%25BE%25D1%2580%25D1%258B%25D0%25B5-%25D0%25BC%25D0%25BE%25D0%25B6%25D0%25BD%25D0%25BE-%25D0%25BF%25D0%25BE%25D0%25BB%25D1%2583%25D1%2587%25D0%25B8%25D1%2582%25D1%258C-%25D0%25B1%25D0%25B0%25D0%25BB%25D0%25BB%25D1%258B">the</a> course <a href="https://github.com/Yorko/mlcourse.ai/wiki/%25D0%2592%25D1%2582%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B9-%25D0%25B7%25D0%25B0%25D0%25BF%25D1%2583%25D1%2581%25D0%25BA-%25D0%25BA%25D1%2583%25D1%2580%25D1%2581%25D0%25B0:-%25D0%25B2%25D1%2581%25D0%25B5-%25D0%25B0%25D0%25BA%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8,-%25D0%25B7%25D0%25B0-%25D0%25BA%25D0%25BE%25D1%2582%25D0%25BE%25D1%2580%25D1%258B%25D0%25B5-%25D0%25BC%25D0%25BE%25D0%25B6%25D0%25BD%25D0%25BE-%25D0%25BF%25D0%25BE%25D0%25BB%25D1%2583%25D1%2587%25D0%25B8%25D1%2582%25D1%258C-%25D0%25B1%25D0%25B0%25D0%25BB%25D0%25BB%25D1%258B">repository</a> ; </li><li>  also once a week we will publish articles in English on Medium.  It will be similar to <a href="https://www.kaggle.com/kashnitsky/vowpal-wabbit-tutorial-blazingly-fast-learning">this</a> Kaggle Kernel about Vowpal Wabbit, only on Medium; </li><li>  From April 23 to July 15, it is planned to jointly pass the Stanford cs231n <a href="http://cs231n.stanford.edu/">course</a> on neural networks (for details, see pinned items in the <em># class_cs231n</em> channel of the ODS slaka).  This will be the second launch, now we are just passing, the course is great, the homework is complex, interesting and very useful. </li></ul><br><p>  <strong>How to connect to the course?</strong> </p><br><p>  Formal registration is not necessary.  Just do your homework, participate in competitions, and we will consider you in the ranking.  However, fill out <a href="https://docs.google.com/forms/d/1wfxD88TCTxNhpDUckJ-p7H-wxbwJ3lf1ATrOIvyGlYA/edit%3Fusp%3Ddrive_web">this</a> survey, the left e-mail will be your ID during the course, at the same time we remind you about the launch closer to the point. </p><br><p>  <strong>Sites for discussion</strong> </p><br><ul><li>  <em>#mlcourse_ai</em> channel in Slack OpenDataScience.  The main communication here, you can ask any question.  The trump card - the authors of articles and homework are also in this channel, ready to respond, to help.  But there is a lot of flood, so look at the pinned items before asking a question; </li><li>  VKontakte <a href="https://vk.com/club158557357">group</a> .  The wall will be a convenient place for official announcements. </li></ul><br><p>  Good luck!  Finally, I want to say that everything will work out, the main thing - do not drop!  This ‚Äúdo not throw‚Äù you now glanced over and most likely did not even notice.  But think: this is the main thing. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4y/u6/xx/4yu6xxufho2nulndbp2kmc-uxw4.jpeg"></div></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/344044/">https://habr.com/ru/post/344044/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../344034/index.html">We are preparing for the publication of the game in Xiaomi Mi Game Center (Unity, C #)</a></li>
<li><a href="../344036/index.html">Fresh utilities, plugins and productivity tools for the designer. First release</a></li>
<li><a href="../344038/index.html">How to transfer email from cPanel to Zimbra Collaboration 8.0</a></li>
<li><a href="../344040/index.html">Preparing your UI interface for Zabbix API using React component</a></li>
<li><a href="../344042/index.html">English phrases that should not be literally translated</a></li>
<li><a href="../344046/index.html">Extending Ansible with plugins: part 1</a></li>
<li><a href="../344048/index.html">What is virtualization and how does a virtual server work?</a></li>
<li><a href="../344050/index.html">What's new in WebStorm 2017.3</a></li>
<li><a href="../344052/index.html">Short shoulder match</a></li>
<li><a href="../344054/index.html">Practical use of multiple bounds generic in Java</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>