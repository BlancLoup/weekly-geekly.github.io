<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We assemble your OpenShift Origin Cluster</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content=""All development - in containers" - this phrase began my fascinating journey into the world of Docker. Attempts to please the requirements of the deve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We assemble your OpenShift Origin Cluster</h1><div class="post__text post__text-html js-mediator-article">  "All development - in containers" - this phrase began my fascinating journey into the world of Docker.  Attempts to please the requirements of the developers led to the choice of OpenShift Origin.  However, to start a full-fledged cluster, as it turned out, the task is not trivial.  During the construction of container infrastructure, I tried to find something on the topic, including on Habr√©, and did not find, oddly enough.  Therefore, below I will try to describe the entire basic installation process and try to protect you from the rakes, which you actually walked through. <br><br>  Let's start: <br><a name="habracut"></a><br><h3>  Environment preparation </h3><br>  All infrastructure objects are a set of dedicated VMs with a different set of resources.  Minimum hardware requirements are outlined <a href="https://docs.openshift.org/latest/install_config/install/prerequisites.html">here</a> .  It is understood that between the VM traffic goes freely and without restrictions.  If this is not the case, then you can see which ports should be open. <br><br>  At the time of installation of this cluster, I used: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Three full VM for the cluster itself (CentOS 7.2); </li><li>  One VM with user directory and DNS server (Windows Server 2012); </li><li>  One VM with NFS server for centralized data storage (CentOS 7.2); </li><li>  VM with Ansible on board. </li></ul><br>  So, suppose that we have a domain of the form habra.cloud, which we actively use for our infrastructural needs. <br><br>  Add type ‚ÄúA‚Äù records for all hosts of our cluster and select the subdomain apps.habra.cloud for future services in our cloud. <br><br>  After adding we get a picture of the form: <br><br><pre><code class="plaintext hljs">Name Type Data apps (same as parent folder) Start of Authority (SOA) [16], dc-infra.habra.cloud, (same as parent folder) Name Server (NS) 172.28.246.50. ansible Host (A) 172.28.247.200 master Host (A) 172.28.247.211 nfs Host (A) 172.28.247.51 node01 Host (A) 172.28.247.212 node02 Host (A) 172.28.247.213</code> </pre> <br>  For the apps.habra.cloud zone, the output will be as follows: <br><br><pre> <code class="plaintext hljs">Name Type Data Timestamp * Host (A) 172.28.247.211 static</code> </pre><br>  We configured DNS - you need to configure the network adapters and the names of the NFS server and cluster nodes. <br><br>  We will not dwell on this for long;  information about this car and a small truck.  I will focus only on a few points: <br><br><ul><li>  First, all settings should be done via NetworkManager, since  otherwise, Ansible will ignore your DNS settings and specific routes, if any. </li><li>  Secondly, you must specify your DNS server as the primary DNS server, and your hosts must respond to the full DNS names listed on the DNS server.  For example, for an OpenShift master node, the output should be: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># hostname master.habra.cloud</span></span></code> </pre><br><ul><li>  Thirdly, you need to specify a few more as search-domains, so that there are no problems with name resolution between subparts and containers.  To understand, I‚Äôll give a part of my /etc/resolv.conf file: </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># cat /etc/resolv.conf # Generated by NetworkManager search habra.cloud default.svc.cluster.local svc.cluster.local cloud.local default.svc svc local nameserver 172.28.246.50</span></span></code> </pre><br>  Deal with the hosts and the network.  The next step is to install the docker on all nodes of the cluster and configure docker-storage. <br><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># yum -y install docker</span></span></code> </pre> <br>  In the / etc / sysconfig / docker file in the section OPTIONS add: <br><br><pre> <code class="bash hljs">OPTIONS=<span class="hljs-string"><span class="hljs-string">'--selinux-enabled --insecure-registry 172.30.0.0/16'</span></span></code> </pre> <br>  Next, in accordance with <a href="https://docs.openshift.org/latest/install_config/install/host_preparation.html">this</a> manual, create docker-storage. <br><br>  I recommend using option ‚ÄúB‚Äù so that it can be controlled using native LVM.  Also, it is necessary to take into account that exactly in this storage OpenShift will copy docker-images from any docker registry.  At the same time, OpenShift does not delete old and unused docker images.  Therefore, I recommend doing docker-storage with a capacity of at least 30-50GB on each node.  The rest - depending on your needs. <br><br>  How I did it: <br><br><ul><li>  Mark fdisk / th / dev / sdb for LVM format, then create PV and VG: </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># fdisk /dev/sdb n t 8e w root@OpenShiftCluster# pvcreate /dev/sdb1 root@OpenShiftCluster# vgcreate docker-vg /dev/sdb1</span></span></code> </pre><br><ul><li>  Let's fix / etc / sysconfig / docker-storage-setup.  Listing / etc / sysconfig / docker-storage-setup is attached: </li></ul><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Edit this file to override any configuration options specified in # /usr/lib/docker-storage-setup/docker-storage-setup. # # For more details refer to "man docker-storage-setup" VG=docker-vg</span></span></code> </pre><br><ul><li>  Let's start the docker-storage creation procedure: </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># docker-storage-setup</span></span></code> </pre><br><ul><li>  Let us follow the recommendations from the manual after creating docker-storage: </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># systemctl is-active docker</span></span></code> </pre><br><ul><li>  <i>If the docker has not yet been run on the host:</i> </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># systemctl enable docker root@OpenShiftCluster# systemctl start docker</span></span></code> </pre><br><ul><li>  <i>If Docker is already running (the procedure will destroy all containers and images):</i> </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># systemctl stop docker root@OpenShiftCluster# rm -rf /var/lib/docker/* root@OpenShiftCluster# systemctl restart docker</span></span></code> </pre><br>  Well, the docker is up and running.  Set up SELinux as required by OpenShift. <br><br><ul><li>  First set the parameter SELINUXTYPE = targeted in the file </li></ul><div class="spoiler">  <b class="spoiler_title">/ etc / selinux / config:</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># This file controls the state of SELinux on the system. # SELINUX= can take one of these three values: # enforcing - SELinux security policy is enforced. # permissive - SELinux prints warnings instead of enforcing. # disabled - No SELinux policy is loaded. SELINUX=enforcing # SELINUXTYPE= can take one of these three values: # targeted - Targeted processes are protected, # minimum - Modification of targeted policy. Only selected processes are protected. # mls - Multi Level Security protection. SELINUXTYPE=targeted</span></span></code> </pre><br></div></div><br><ul><li>  And, since it implies working with NFS, run a couple of commands: </li></ul><br><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># setsebool -P virt_use_nfs 1 root@OpenShiftCluster# setsebool -P virt_sandbox_use_nfs 1</span></span></code> </pre><br><ul><li>  Install the remaining required </li></ul><div class="spoiler">  <b class="spoiler_title">Components:</b> <div class="spoiler_text"><pre> <code class="bash hljs">root@OpenShiftCluster<span class="hljs-comment"><span class="hljs-comment"># yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion nfs-utils nfs-utils-lib root@OpenShiftCluster# yum update root@OpenShiftCluster# yum -y install \ https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm root@OpenShiftCluster# sed -i -e "s/^enabled=1/enabled=0/" /etc/yum.repos.d/epel.repo root@OpenShiftCluster# yum -y --enablerepo=epel install pyOpenSSL</span></span></code> </pre><br></div></div><br><ul><li>  <i>In my case, Ansible is installed on a separate server, but if you want, you can use it on the Master, in this case, the last command will look like this:</i> </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># yum -y --enablerepo=epel install ansible pyOpenSSL</span></span></code> </pre><br>  Go ahead to the NFS server.  Since we have already installed clients and even set up SELinux specifically for this, it was nothing to prepare a NFS server. <br><br>  In this article, so that it does not grow to a huge size, it is assumed that the NFS server is already working with you.  If this is not the case, then <a href="https://www.howtoforge.com/tutorial/setting-up-an-nfs-server-and-client-on-centos-7/">here is a good article</a> on this topic. <br>  I add that my NFS server is looking at the directory / nfs / and I will build on this. <br><br><ul><li>  Prepare folders on the NFS server for the future private docker registry: </li></ul><br><pre> <code class="bash hljs">root@nfs<span class="hljs-comment"><span class="hljs-comment"># mkdir -R /nfs/infrastructure/registry root@nfs# chmod 755 /nfs/infrastructure root@nfs# chmod 755 /nfs/infrastructure/registry root@nfs# chown nfsnobody:nfsnobody /nfs/infrastructure root@nfs# chown nfsnobody:nfsnobody /nfs/infrastructure/registry</span></span></code> </pre><br><ul><li>  Modify the file / etc / exports - add there: </li></ul><br><pre> <code class="bash hljs">/nfs/infrastructure/registry *(rw,sync,root_squash,no_subtree_check,no_wdelay)</code> </pre><br><ul><li>  perform: </li></ul><br><pre> <code class="bash hljs">root@nfs<span class="hljs-comment"><span class="hljs-comment"># exportfs -a</span></span></code> </pre><br>  Great, now we can connect with the client to this directory. <br><br><h3>  Preparing Ansible and its inventory. </h3><br>  In case Ansible is already installed somewhere, like mine, you should perform actions on it.  In case you want to use Master for this, then you will need to do the same for it. <br><br>  So, the first - Ansible should know our VM by hostname.  It means that we either edit / etc / hosts, or, which is much more correct, set our VM on our DNS server in any way we can (if you use Master, you don‚Äôt need to do this, since everything is ready). <br><br>  Now we need to replicate our public SSH key on the cluster nodes so that Ansible can connect to them without any problems. <br><br><pre> <code class="bash hljs">root@ansible<span class="hljs-comment"><span class="hljs-comment"># for host in master.habra.cloud \ node01.habra.cloud \ node02.habra.cloud; \ do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \ done</span></span></code> </pre><br>  <i>At the same time, I already had key pairs ready.</i>  <i>If this is not the case, then you should use the ssh-keygen utility and create your own pair.</i> <br><br>  The time has come for the most important thing - editing the inventory file for Ansible with the necessary parameters. <br><br><div class="spoiler">  <b class="spoiler_title">My inventory file:</b> <div class="spoiler_text"><pre> <code class="bash hljs">root@ansible<span class="hljs-comment"><span class="hljs-comment"># cat inventory [OSEv3:children] masters nodes [masters] master.habra.cloud [nodes] master.habra.cloud openshift_schedulable=false openshift_node_labels="{'region': 'infra', 'zone': 'default'}" node01.habra.cloud openshift_node_labels="{'region': 'primary', 'zone': 'firstzone'}" node02.habra.cloud openshift_node_labels="{'region': 'primary', 'zone': 'secondzone'}" [OSEv3:vars] ansible_ssh_user=root openshift_master_default_subdomain=apps.habra.cloud containerized=false deployment_type=origin</span></span></code> </pre><br></div></div><br>  Details about the variables OpenShift in Ansible can be read <a href="https://docs.openshift.org/latest/install_config/install/advanced_install.html">here.</a> <br><br>  Just a little remains - download the latest archive with the required OpenShift roles for Ansible (you need an installed git-client): <br><br><pre> <code class="bash hljs">root@ansible<span class="hljs-comment"><span class="hljs-comment"># git clone https://github.com/openshift/openshift-ansible</span></span></code> </pre> <br>  <i>Important note.</i>  <i>If you, like me, deploy to VM - I would recommend using snapshots.</i>  <i>If something goes wrong, then during installation it will be much easier to change something in the original configuration option than to go through a long and tedious debag.</i> <br><br>  Let's get down to the gourmet - finally install OpenShift Origin on our nodes.  The following command assumes that you are in the same directory as your inventory file. <br><br>  <i>Note: Currently, the latest version of the OpenShift Origin (1.4) playbook for Ansible works correctly with Ansible version 2.2.0.0.</i>  <i>When upgrading to version 1.4, I had to roll back Ansible so that everything fell correctly.</i> <br><br><pre> <code class="bash hljs">root@ansible<span class="hljs-comment"><span class="hljs-comment"># ansible-playbook -i ./inventory openshift-ansible/playbooks/byo/config.yml</span></span></code> </pre> <br>  Installation lasts about 20 minutes.  According to the results there should be no files. <br><br><h3>  Primary setup </h3><br>  On the Master, we will request the status of the nodes and get the output: <br><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc get nodes NAME STATUS AGE master.habra.cloud Ready,SchedulingDisabled 1d node01.habra.cloud Ready 1d node02.habra.cloud Ready 1d</span></span></code> </pre> <br>  If this is the picture, open the browser and go to the URL: <a href="https://master.habra.cloud:8443/console">https://master.habra.cloud:8443/console</a> <br><br>  You can already log in with any user. <br><br>  However, the joy will not be complete. <br><br>  For complete happiness, we need to perform a few more actions.  Firstly, the router and private docker-registry are passed through.  Second, modify the docker registry so that it is located on our NFS server. <br><br><ul><li>  Let's go - deploy router and private docker-registry: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oadm manage-node master.habra.cloud --schedulable=true root@master# oc get nodes NAME STATUS AGE master.habra.cloud Ready 1d node1.habra.cloud Ready 1d node2.habra.cloud Ready 1d</span></span></code> </pre> <br>  The default installer creates tasks for deploying the router and registry in the namespace default.  But at the same time, deployment will occur only if the status of the Master  ªa will be Schedulable. <br><br><div class="spoiler">  <b class="spoiler_title">Let's check how it goes:</b> <div class="spoiler_text"><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc project default root@master# oc get all NAME REVISION DESIRED CURRENT TRIGGERED BY dc/docker-registry 4 1 1 config dc/router 3 1 1 config NAME DESIRED CURRENT READY AGE rc/docker-registry-1 0 0 0 1d rc/router-1 1 1 1 1d NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/docker-registry 172.30.7.135 &lt;none&gt; 5000/TCP 1d svc/kubernetes 172.30.0.1 &lt;none&gt; 443/TCP,53/UDP,53/TCP 1d svc/router 172.30.79.17 &lt;none&gt; 80/TCP,443/TCP,1936/TCP 1d NAME READY STATUS RESTARTS AGE po/docker-registry-1-ayuuo 1/1 Running 11 1d po/router-1-lzewh 1/1 Running 8 1d</span></span></code> </pre><br></div></div><br>  This means that the router is ready.  And our future services will be able to respond to their DNS names.  Since we have already created the necessary directories on the NFS server, it remains to point at them to OpenShift. <br><br><ul><li>  On the Master we will create the file nfs-pv.yaml with the following contents: </li></ul><br><div class="spoiler">  <b class="spoiler_title">nfs-pv.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: PersistentVolume metadata: name: registrypv spec: capacity: storage: 20Gi accessModes: - ReadWriteOnce nfs: path: /nfs/infrastructure/registry server: nfs.habra.cloud persistentVolumeReclaimPolicy: Recycle</code> </pre> </div></div><br><ul><li>  Create another nfs-claim1.yaml file: </li></ul><br><div class="spoiler">  <b class="spoiler_title">nfs-claim1.yaml</b> <div class="spoiler_text"><pre> <code class="plaintext hljs">apiVersion: v1 kind: PersistentVolumeClaim metadata: name: registry-claim1 spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi</code> </pre> </div></div><br><ul><li>  Now create a PV: </li></ul><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc create -f nfs-pv.yaml</span></span></code> </pre> <br><ul><li>  and check the result: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc get pv NAME CAPACITY ACCESSMODES RECLAIMPOLICY STATUS ... registrypv 20Gi RWO Recycle Available</span></span></code> </pre><br><ul><li>  Also we will do with pvc: </li></ul><br><div class="spoiler">  <b class="spoiler_title">Persistent Volume Claim</b> <div class="spoiler_text"><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc create -f nfs-claim1.yaml root@master# oc get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES AGE registry-claim1 Bound registrypv 20Gi RWO 1d</span></span></code> </pre></div></div><br><ul><li>  We modify our deploymentconfig for the docker-registry so that it looks at our nfs-claim1: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc volume deploymentconfigs/docker-registry --add --name=registry-storage -t pvc \ --claim-name=registry-claim1 --overwrite</span></span></code> </pre> <br><ul><li>  We are convinced that the last passed: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># oc get pods NAME READY STATUS RESTARTS AGE docker-registry-2-sdfhk 1/1 Running 1 1d</span></span></code> </pre> <br>  Almost done, a little twist DNS on Master`e. <br><br><ul><li>  Add the following lines to </li></ul><div class="spoiler">  <b class="spoiler_title">/etc/dnsmasq.conf:</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># Reverse DNS record for master host-record=master.habra.cloud,172.28.247.211 # Wildcard DNS for OpenShift Applications - Points to Router server=/habra.cloud/172.28.246.50 address=/apps.habra.cloud/172.28.247.211 server=/apps.habra.cloud/172.28.246.50 # Forward .local queries to SkyDNS server=/local/127.0.0.1#8053 # Forward reverse queries for service network to SkyDNS. # This is for default OpenShift SDN - change as needed. server=/17.30.172.in-addr.arpa/127.0.0.1#8053 # Forward .habra.cloud queries to DC server=/habra.cloud/172.28.246.50#53</span></span></code> </pre> <br></div></div><br><ul><li>  Edit /etc/origin/master/master-config.yaml in the dnsConfig section should be like this: </li></ul><br><pre> <code class="plaintext hljs">dnsConfig: bindAddress: 0.0.0.0:8053</code> </pre> <br><ul><li>  Restarting dnsmasq and master-node: </li></ul><br><pre> <code class="bash hljs">root@master<span class="hljs-comment"><span class="hljs-comment"># service dnsmasq restart root@master# service origin-master restart</span></span></code> </pre> <br>  Done!  The cluster is running. <br><br><h3>  Conclusion </h3><br>  In the end I would like to say that this is only the beginning of a long journey.  In the process of exploitation a lot of inconsistencies and flaws emerge.  Not all containers or even all services from templates will work.  For most of the tasks I encountered, I had to either modify the templates with a coarse file or create my own.  But still, how much more pleasant to operate with ready services! <br><br>  <i>Thanks for attention.</i> <br><br><div class="spoiler">  <b class="spoiler_title">Literature and Itochniki</b> <div class="spoiler_text">  <a href="https://docs.openshift.org/">docs.openshift.org</a> <br>  <a href="https://gist.github.com/jlebon/0cfcd3dcc7ac7de18a69">gist.github.com/jlebon/0cfcd3dcc7ac7de18a69</a> <br>  <a href="http://developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster/">developers.redhat.com/blog/2015/11/19/dns-your-openshift-v3-cluster</a> <br>  <a href="https://www.youtube.com/user/rhopenshift">www.youtube.com/user/rhopenshift</a> <br>  <a href="https://docs.docker.com/">docs.docker.com</a> <br>  <a href="https://kubernetes.io/docs/">kubernetes.io/docs</a> </div></div><br>  Comments and additions are welcome. </div><p>Source: <a href="https://habr.com/ru/post/324240/">https://habr.com/ru/post/324240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../324230/index.html">How to motivate users to stick to your product forever: Pope Gregory Framework</a></li>
<li><a href="../324232/index.html">Preparing a springboard for the react-application</a></li>
<li><a href="../324234/index.html">Security Week 11: 38 infected smartphones, FBI crying for encryption, Google again patched up Chrome</a></li>
<li><a href="../324236/index.html">MIPT launched an online course on innovative project management</a></li>
<li><a href="../324238/index.html">J-Bird, or as a bummer game sold</a></li>
<li><a href="../324242/index.html">Replacing Oracle with PostgreSQL and the ability to work with partitioning inside a DLP system</a></li>
<li><a href="../324244/index.html">How to search for patterns in stock data and use them in trading?</a></li>
<li><a href="../324246/index.html">Active Directory Recycle Bin: Usage Guidelines</a></li>
<li><a href="../324250/index.html">Calling Go functions from other languages</a></li>
<li><a href="../324256/index.html">Segmental</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>