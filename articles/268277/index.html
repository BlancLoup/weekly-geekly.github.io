<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Big Data from A to Z. Part 2: Hadoop</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! In the previous article, we looked at the MapReduce parallel computing paradigm. In this article, we will move from theory to practice and c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Big Data from A to Z. Part 2: Hadoop</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  In the <a href="http://habrahabr.ru/company/dca/blog/267361/">previous</a> article, we looked at the MapReduce parallel computing paradigm.  In this article, we will move from theory to practice and consider <a href="http://hadoop.apache.org/">Hadoop</a> , the powerful tool for working with big data from the Apache foundation. <br><br>  The article describes what tools and tools Hadoop includes, how to install Hadoop in yourself, provides instructions and examples for developing MapReduce-programs for Hadoop. <br><br> <a href="http://habrahabr.ru/company/dca/blog/268277/"><img src="https://habrastorage.org/files/de6/ee8/dc6/de6ee8dc6ae2412fa44f4acb2bc068ed.png"></a> <br><a name="habracut"></a><br><h2>  General information about Hadoop </h2><br>  As you know, the MapReduce paradigm was proposed by Google in its 2004 <a href="http://research.google.com/archive/mapreduce.html">MapReduce</a> article <a href="http://research.google.com/archive/mapreduce.html">: Simplified Data Processing on Large Clusters</a> .  Since the proposed article contained a description of the paradigm, but the implementation was absent - several programmers from Yahoo offered their implementation as part of the work on the web-crawler <a href="http://nutch.apache.org/">nutch</a> .  You can read more about Hadoop history in <a href="https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/">The history of Hadoop</a> article <a href="https://gigaom.com/2013/03/04/the-history-of-hadoop-from-4-nodes-to-the-future-of-data/">: From 4 nodes to the future of data</a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Initially, Hadoop was, first of all, a tool for storing data and running MapReduce-tasks, now Hadoop is a large stack of technologies that are somehow related to processing big data (not only using MapReduce). <br><br>  The core (core) components of Hadoop are: <br><br><ul><li>  <a href="http://hadoop.apache.org/docs/r1.2.1/hdfs_design.html"><b>Hadoop Distributed File System (HDFS)</b></a> is a distributed file system that allows you to store information of almost unlimited size. <br><br></li><li>  <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html"><b>Hadoop YARN</b></a> is a framework for managing cluster resources and task management, including the MapReduce framework. <br><br></li><li>  Hadoop common </li></ul><br>  There are also a large number of projects directly related to Hadoop, but not included in the Hadoop core: <br><br><ul><li>  <a href="https://hive.apache.org/"><b>Hive</b></a> is a tool for SQL-like queries on large data (turns SQL queries into a series of MapReduce ‚Äì tasks); <br><br></li><li>  <a href="https://pig.apache.org/"><b>Pig</b></a> is a high-level data analysis programming language.  One line of code in this language can turn into a sequence of MapReduce-tasks; <br><br></li><li>  <a href="http://hbase.apache.org/"><b>Hbase</b></a> is a column database that implements the <a href="https://ru.wikipedia.org/wiki/BigTable">BigTable</a> paradigm; <br><br></li><li>  <a href="http://cassandra.apache.org/"><b>Cassandra</b></a> - high-performance distributed key-value database; <br><br></li><li>  <a href="https://zookeeper.apache.org/"><b>ZooKeeper</b></a> is a service for distributed configuration storage and synchronization of changes to this configuration; <br><br></li><li>  <a href="http://mahout.apache.org/"><b>Mahout</b></a> is a library and machine for learning big data. </li></ul><br>  Separately, I would like to mention the <a href="http://spark.apache.org/">Apache Spark</a> project, which is a distributed data processing engine.  Apache Spark usually uses Hadoop components such as HDFS and YARN for its work, while it has recently become more popular than Hadoop: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb6/32c/1a5/fb632c1a5b3843bd4dc292a4a226cc01.png"><br><br>  Some of these components will be devoted to individual articles in this series of materials, but for now let us analyze how to start working with Hadoop and apply it in practice. <br><br><h2>  Installing Hadoop on a cluster using Cloudera Manager </h2><br>  Previously, the installation of Hadoop was a rather difficult task - you had to individually configure each machine in the cluster, make sure that nothing was forgotten, and carefully set up monitoring.  With the growing popularity of Hadoop, companies have emerged (such as <a href="http://www.cloudera.com/content/cloudera/en/home.html">Cloudera</a> , <a href="http://hortonworks.com/">Hortonworks</a> , <a href="https://www.mapr.com/">MapR</a> ), which provide their own Hadoop assemblies and powerful tools for managing the Hadoop cluster.  In our cycle of materials, we will use the Hadoop assembly from Cloudera. <br><br>  In order to install Hadoop on your cluster, you need to do a few simple steps: <br><br><ol><li>  Download Cloudera Manager Express to one of your cluster machines <a href="http://www.cloudera.com/content/cloudera/en/downloads/cloudera_manager/cm-5-4-7.html">from here</a> ; </li><li>  Assign rights to execute and run; </li><li>  Follow the installation instructions. </li></ol><br>  The cluster should work on one of the supported operating systems of the linux family: RHEL, Oracle Enterprise linux, SLES, Debian, Ubuntu. <br><br>  After installation, you will receive a cluster management console, where you can view installed services, add / remove services, monitor the cluster status, edit the cluster configuration: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/475/209/2d8/4752092d8a708fadd8ee2988d952e31e.png"><br><br>  For more information on installing Hadoop on a cluster using the cloudera manager, see the <a href="http://www.cloudera.com/content/cloudera/en/downloads/cloudera_manager/cm-5-4-7.html">link</a> in the Quick Start section. <br><br>  If you plan to use Hadoop to "try" - you can not bother with purchasing expensive hardware and setting up Hadoop on it, but simply download the pre-configured virtual machine from the <a href="http://www.cloudera.com/content/cloudera/en/downloads/quickstart_vms/cdh-5-4-x.html">link</a> and use the customized hadoop. <br><br><h2>  Running MapReduce programs on Hadoop </h2><br>  Now we will show how to run the MapReduce task on Hadoop.  As a task, we will use the classic <b>WordCount</b> example, which was analyzed in the <a href="http://habrahabr.ru/company/dca/blog/267361/">previous article of the cycle</a> .  In order to experiment on real data, I prepared an archive of random news from the site <a href="http://lenta.ru/">lenta.ru</a> .  Download the archive by the <a href="">link</a> . <br><br>  <b>Let me remind the wording of the problem:</b> there is a set of documents.  It is necessary for each word found in a set of documents to count how many times a word occurs in a set. <br><br>  <b>Solution</b> : <br>  Map breaks a document into words and returns a set of pairs (word, 1). <br>  Reduce summarizes the occurrences of each word: <br><table><tbody><tr><td><pre><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">map</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(doc)</span></span></span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> doc.split(): <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> word, <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre> </td><td><pre> <code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">reduce</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(word, values)</span></span></span></span>: <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> word, sum(values)</code> </pre></td></tr></tbody></table><br>  Now the task is to program this solution in the form of code that can be executed on Hadoop and run. <br><br><h1>  Method number 1.  Hadoop streaming </h1><br>  The easiest way to run a MapoDuce program on Hadoop is to use the Hadoop streaming interface.  The streaming interface assumes that map and reduce are implemented as programs that accept data from <b>stdin</b> and return the result to <b>stdout</b> . <br><br>  The program that performs the map function is called <b>mapper</b> .  A program that performs reduce is called a <b>reducer</b> , <b>respectively</b> . <br><br>  The streaming interface assumes, by default, that one incoming line in a mapper or reducer corresponds to one incoming record for a map. <br><br>  The mapper output goes to the input of the reducer in the form of pairs (key, value), with all the pairs corresponding to the same key: <br><br><ul><li>  Guaranteed to be processed by a single launch of a reducer; </li><li>  They will be served at the entrance in a row (that is, if one reducer processes several different keys, the input will be grouped by key). </li></ul><br>  So, let's implement mapper and reducer in python: <br><br><pre> <code class="hljs lua">#mapper.py import sys def do_map(doc): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> doc.split(): <span class="hljs-built_in"><span class="hljs-built_in">yield</span></span> word.<span class="hljs-built_in"><span class="hljs-built_in">lower</span></span>(), <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> line <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sys.<span class="hljs-built_in"><span class="hljs-built_in">stdin</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> key, value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> do_map(line): <span class="hljs-built_in"><span class="hljs-built_in">print</span></span>(key + <span class="hljs-string"><span class="hljs-string">"\t"</span></span> + str(value))</code> </pre> <br><pre> <code class="hljs vhdl">#reducer.py import sys def do_reduce(word, values): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> word, sum(values) prev_key = None values = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-literal"><span class="hljs-literal">line</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> sys.stdin: key, value = <span class="hljs-literal"><span class="hljs-literal">line</span></span>.split(<span class="hljs-string"><span class="hljs-string">"\t"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> key != prev_key <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> prev_key <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> None: result_key, result_value = do_reduce(prev_key, values) print(result_key + <span class="hljs-string"><span class="hljs-string">"\t"</span></span> + str(result_value)) values = [] prev_key = key values.append(int(value)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> prev_key <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> None: result_key, result_value = do_reduce(prev_key, values) print(result_key + <span class="hljs-string"><span class="hljs-string">"\t"</span></span> + str(result_value))</code> </pre> <br>  The data that Hadoop will process must be stored on HDFS.  Download our articles and put on HDFS.  To do this, use the <b>hadoop fs</b> command: <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">wget</span></span> https://www.dropbox.com/s/opp5psid1x3jt41/lenta_articles.tar.gz tar xzvf lenta_articles.tar.gz hadoop fs -put lenta_articles</code> </pre> <br>  The hadoop fs utility supports a large number of methods for manipulating the file system, many of which repeat the standard linux utilities one-on-one.  More information about its capabilities can be found at the <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html">link</a> . <br><br>  Now run the streaming task: <br><br><pre> <code class="hljs tex">yarn jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-input lenta_articles<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-output lenta_wordcount<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-file mapper.py<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-file reducer.py<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-mapper "python mapper.py"<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>-reducer "python reducer.py"</code> </pre> <br>  The yarn utility is used to launch and manage various applications (including map-reduce based) on a cluster.  Hadoop-streaming.jar is just one example of such a yarn application. <br><br>  Next are the launch options: <br><br><ul><li>  input - source data folder in hdfs; </li><li>  output - folder on hdfs, where you need to put the result; </li><li>  file - files that are needed during the work of map-reduce tasks; </li><li>  mapper is a console command that will be used for the map stage; </li><li>  reduce - the console command that will be used for the reduce-stage. </li></ul><br>  After running in the console, you will see the progress of the task and the URL to view more detailed information about the task. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/120/d38/394/120d38394178136bb5a54a6849098539.png"><br><br>  In the interface available at this URL, you can find out more detailed status of the task, see the logs of each mapper and reducer (which is very useful in case of dropped tasks). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/593/618/89b/59361889be8b9428e814af24ba11384a.png"><br><br>  The result of the work after successful execution is added to HDFS in the folder that we specified in the output field.  You can view its contents using the ‚Äúhadoop fs -ls lenta_wordcount‚Äù command. <br><br>  The result itself can be obtained as follows: <br><br><pre> <code class="hljs pgsql">hadoop fs -<span class="hljs-type"><span class="hljs-type">text</span></span> lenta_wordcount<span class="hljs-comment"><span class="hljs-comment">/* | sort -n -k2,2 | tail -n5  41  43  82  111  194</span></span></code> </pre> <br>  The ‚Äúhadoop fs -text‚Äù command displays the contents of the folder as text.  I sorted the result by the number of occurrences of words.  As expected, the most common words in the language are prepositions. <br><br><h2>  Method number 2 </h2><br>  By itself, hadoop was written in java, and the native interface of hadoop is also java-based.  Let's show how the native java application for wordcount looks like: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.io.IOException; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> java.util.StringTokenizer; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.conf.<span class="hljs-keyword"><span class="hljs-keyword">Configuration</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.fs.Path; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.IntWritable; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.io.Text; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.Job; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.Mapper; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.Reducer; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.lib.<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>.FileInputFormat; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> WordCount { <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> TokenizerMapper extends Mapper&lt;<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span>, <span class="hljs-type"><span class="hljs-type">Text</span></span>, <span class="hljs-type"><span class="hljs-type">Text</span></span>, IntWritable&gt;{ private final static IntWritable one = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> IntWritable(<span class="hljs-number"><span class="hljs-number">1</span></span>); private <span class="hljs-type"><span class="hljs-type">Text</span></span> word = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> Text(); <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span> map(<span class="hljs-keyword"><span class="hljs-keyword">Object</span></span> key, <span class="hljs-type"><span class="hljs-type">Text</span></span> <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>, Context context ) throws IOException, InterruptedException { StringTokenizer itr = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> StringTokenizer(<span class="hljs-keyword"><span class="hljs-keyword">value</span></span>.toString()); <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> (itr.hasMoreTokens()) { word.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(itr.nextToken()); context.<span class="hljs-keyword"><span class="hljs-keyword">write</span></span>(word, one); } } } <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> IntSumReducer extends Reducer&lt;<span class="hljs-type"><span class="hljs-type">Text</span></span>,IntWritable,<span class="hljs-type"><span class="hljs-type">Text</span></span>,IntWritable&gt; { private IntWritable result = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> IntWritable(); <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> <span class="hljs-type"><span class="hljs-type">void</span></span> reduce(<span class="hljs-type"><span class="hljs-type">Text</span></span> key, Iterable&lt;IntWritable&gt; <span class="hljs-keyword"><span class="hljs-keyword">values</span></span>, Context context ) throws IOException, InterruptedException { <span class="hljs-type"><span class="hljs-type">int</span></span> sum = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (IntWritable val : <span class="hljs-keyword"><span class="hljs-keyword">values</span></span>) { sum += val.<span class="hljs-keyword"><span class="hljs-keyword">get</span></span>(); } result.<span class="hljs-keyword"><span class="hljs-keyword">set</span></span>(sum); context.<span class="hljs-keyword"><span class="hljs-keyword">write</span></span>(key, result); } } <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> static <span class="hljs-type"><span class="hljs-type">void</span></span> main(String[] args) throws <span class="hljs-keyword"><span class="hljs-keyword">Exception</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">Configuration</span></span> conf = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Configuration</span></span>(); Job job = Job.getInstance(conf, "word count"); job.setJarByClass(WordCount.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); job.setMapperClass(TokenizerMapper.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); job.setReducerClass(IntSumReducer.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); job.setOutputKeyClass(<span class="hljs-type"><span class="hljs-type">Text</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); job.setOutputValueClass(IntWritable.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); FileInputFormat.addInputPath(job, <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> Path("hdfs://localhost/user/cloudera/lenta_articles")); FileOutputFormat.setOutputPath(job, <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> Path("hdfs://localhost/user/cloudera/lenta_wordcount")); <span class="hljs-keyword"><span class="hljs-keyword">System</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">exit</span></span>(job.waitForCompletion(<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>) ? <span class="hljs-number"><span class="hljs-number">0</span></span> : <span class="hljs-number"><span class="hljs-number">1</span></span>); } }</code> </pre> <br>  This class does exactly the same thing as our Python example.  We create the TokenizerMapper and IntSumReducer classes, inheriting them from the Mapper and Reducer classes, respectively.  Classes passed as template parameters indicate types of input and output values.  The native API implies that the key functions are supplied to the map functions.  Since in our case the key is empty, we simply define Object as the key type. <br><br>  In the Main method, we start the mapreduce task and determine its parameters - the name, mapper and reducer, the path in HDFS, where the input data are and where to put the result. <br><br>  For compilation we will need hadoop libraries.  I use to build Maven, for which cloudera has a repository.  Instructions for setting it up can be found at the <a href="http://www.cloudera.com/content/cloudera/en/documentation/core/latest/topics/cdh_vd_cdh5_maven_repo.html">link</a> .  As a result, the pom.xmp file (which is used by maven to describe the build of the project) I got the following): <br><br><pre> <code class="hljs xml"><span class="hljs-meta"><span class="hljs-meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">project</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">xmlns</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"http://maven.apache.org/POM/4.0.0"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">xmlns:xsi</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">xsi:schemaLocation</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">modelVersion</span></span></span><span class="hljs-tag">&gt;</span></span>4.0.0<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">modelVersion</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">repositories</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">repository</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">id</span></span></span><span class="hljs-tag">&gt;</span></span>cloudera<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">id</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">url</span></span></span><span class="hljs-tag">&gt;</span></span>https://repository.cloudera.com/artifactory/cloudera-repos/<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">url</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">repository</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">repositories</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependencies</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span>org.apache.hadoop<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span>hadoop-common<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span>2.6.0-cdh5.4.2<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span>org.apache.hadoop<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span>hadoop-auth<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span>2.6.0-cdh5.4.2<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span>org.apache.hadoop<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span>hadoop-hdfs<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span>2.6.0-cdh5.4.2<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span>org.apache.hadoop<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span>hadoop-mapreduce-client-app<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span>2.6.0-cdh5.4.2<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependency</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">dependencies</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span>org.dca.examples<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">groupId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span>wordcount<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">artifactId</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span>1.0-SNAPSHOT<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">version</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">project</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br>  Let's assemble the project in jar-package: <br><br><pre> <code class="hljs scala">mvn clean <span class="hljs-keyword"><span class="hljs-keyword">package</span></span></code> </pre> <br>  After building the project into a jar file, the launch takes place in a similar way, as in the case of the streaming interface: <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">yarn</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">jar</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">wordcount-1</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.0-SNAPSHOT</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.jar</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">WordCount</span></span></code> </pre> <br>  We wait for execution and check the result: <br><br><pre> <code class="hljs pgsql">hadoop fs -<span class="hljs-type"><span class="hljs-type">text</span></span> lenta_wordcount<span class="hljs-comment"><span class="hljs-comment">/* | sort -n -k2,2 | tail -n5  41  43  82  111  194</span></span></code> </pre> <br>  As you might guess, the result of executing our native application is the same as the result of the streaming application that we launched in the previous way. <br><br><h1>  Summary </h1><br>  In this article, we looked at Hadoop, a software stack for working with big data, described the installation process for Hadoop using the example of the cloudera distribution, showed how to write mapreduce programs using the streaming interface and the Hadoop native API. <br><br>  In the next articles of the series, we will take a closer look at the architecture of the individual components of Hadoop and Hadoop-related software, show more complex versions of MapReduce programs, analyze ways to simplify working with MapReduce, as well as the limitations of MapReduce and how to circumvent these restrictions. <br><br>  Thank you for your attention, we are ready to answer your questions. <br><br>  <a href="https://www.youtube.com/channel/UCOvuB83CWNZ0yz8qeNpWIIQ">Youtube Channel about data analysis</a> <br><br><h1>  Links to other articles of the cycle: </h1><br>  <a href="http://habrahabr.ru/company/dca/blog/267361/">Part 1: Big Data Principles, the MapReduce Paradigm</a> <br>  <a href="http://habrahabr.ru/company/dca/blog/270453/">Part 3: Techniques and strategies for developing MapReduce applications</a> <br>  <a href="https://habrahabr.ru/company/dca/blog/280700/">Part 4: Hbase</a> </div><p>Source: <a href="https://habr.com/ru/post/268277/">https://habr.com/ru/post/268277/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../268267/index.html">Sometimes less is better - why only Google authorization? + channel Rusbase</a></li>
<li><a href="../268269/index.html">Why I chose Yii2</a></li>
<li><a href="../268271/index.html">ECFG: put ~ / .emacs on a diet</a></li>
<li><a href="../268273/index.html">Compalex: comparing schemas of two databases</a></li>
<li><a href="../268275/index.html">Why the Internet needs IPFS before it's too late</a></li>
<li><a href="../268279/index.html">DroidSon Moscow 2015: how it was</a></li>
<li><a href="../268281/index.html">Paul Graham: "Design and Research"</a></li>
<li><a href="../268283/index.html">A detailed review of Affinity Designer (Mac OS). Part 1. Tools</a></li>
<li><a href="../268291/index.html">Google has released a security update for Android</a></li>
<li><a href="../268293/index.html">Paul Graham: "Revenge of the nerds." What is the difference between Lisp</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>