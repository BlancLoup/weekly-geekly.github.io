<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Intuitive RL (Reinforcement Learning): An Introduction to Advantage-Actor-Critic (A2C)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is a free translation of the article by Rudy Gilman and Katherine Wang Intuitive RL: Intro to Advantage-Actor-Critic (A2C) . 

 Reinforcement Tra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Intuitive RL (Reinforcement Learning): An Introduction to Advantage-Actor-Critic (A2C)</h1><div class="post__text post__text-html js-mediator-article"><p>  This is a free translation of the article by Rudy Gilman and Katherine Wang <a href="https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752">Intuitive RL: Intro to Advantage-Actor-Critic (A2C)</a> . </p><img vspace="10" src="https://habrastorage.org/webt/_g/tr/gc/_gtrgckvsc0pemxqwe4iv6sxzsu.png"><p>  Reinforcement Training Specialists (RL) have produced many excellent tutorials.  Most, however, describe RL in terms of mathematical equations and abstract diagrams.  We like to think about the subject from a different point of view.  RL itself is inspired by how animals learn, so why not translate the underlying RL mechanism back into the natural phenomena that it is meant to imitate?  People learn best through stories. </p><br><p>  This is a story about the Actor Advantage Critic (A2C) model.  The ‚ÄúSubject Critic‚Äù model is a popular form of the Policy Gradient model, which itself is a traditional RL algorithm.  If you understand A2C, you understand deep RL. </p><br><a name="habracut"></a><p>  After you gain an intuitive understanding of A2C, check: </p><ul><li>  <a href="https://github.com/rgilman33/simple-A2C/blob/master/3_A2C-nstep-TUTORIAL.ipynb">Our simple implementation</a> of the A2C code (for training) or our industrial <a href="https://github.com/rgilman33/baselines-A2C">version of PyTorch</a> , based on the <a href="https://github.com/openai/baselines">OpenAI TensorFlow Baselines</a> model; </li><li>  <a href="http://ufal.mff.cuni.cz/~straka/courses/npfl114/2016/sutton-bookdraft2016sep.pdf">Introduction to RL by Barto &amp; Sutton</a> , <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">David Silver‚Äôs canonical course</a> , <a href="https://arxiv.org/abs/1701.07274">review by Yusi Lee</a> and <a href="https://github.com/dennybritz/reinforcement-learning">the Denny Brits repository on GitHub</a> for deep immersion in RL; </li><li>  <a href="http://www.fast.ai/">Amazing fast.ai course</a> for intuitive and practical coverage of deep learning in general, implemented in PyTorch; </li><li>  <a href="https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2">Tutorials by Arthur Giuliani</a> on RL, implemented in TensorFlow. </li></ul><p>  <a href="https://twitter.com/embermarke">@Embermarke</a> Illustrations </p><br><p>  In RL, the agent (agent) - Fox Klukovka, - moves through the states in the environment by performing actions (actions), trying to maximize rewards during the journey. </p><img vspace="10" src="https://habrastorage.org/webt/8y/wv/jb/8ywvjb4hyom44rjn5c-nynakjua.png"><p>  A2C accepts status inputs ‚Äî sensory inputs in the Klucke case ‚Äî and generates two outputs: <br>  1) Estimation of how much remuneration will be received, starting from the moment of the current status, except for the current (already existing) remuneration. <br>  2) Recommend what action to take (policy). <br><br>  "Critic": wow, what a wonderful valley!  It will be a fruitful day for foraging!  I bet today, before sunset, I will collect 20 points. <br>  "Subject": these flowers look beautiful, I feel a craving for "A." </p><img vspace="10" src="https://habrastorage.org/webt/gf/_x/wf/gf_xwfrsu-z-64k9ijhxyakmoks.png"><p>  Deep RL models are input-output display machines, like any other classification or regression models.  Instead of categorizing images or text, deep RL models lead states to actions and / or states to state values.  A2C does both. </p><img vspace="10" src="https://habrastorage.org/webt/cb/vv/st/cbvvstverqhmym9qtsnbpptrrwc.png"><img vspace="10" src="https://habrastorage.org/webt/cq/0a/hj/cq0ahjxh9khnmukf2fnivdfsify.png"><p>  This state-action-reward set constitutes one observation.  She will write this line of data in her journal, but she is not going to think about it yet.  She will fill it when she stops to think. <br><br>  Some authors associate reward 1 with a time step of 1, others associate it with step 2, but all mean the same concept: reward is related to the state, and the action immediately precedes it. </p><img vspace="10" src="https://habrastorage.org/webt/ht/gj/vw/htgjvw00nace9trmp4ggdse9j_g.png"><p>  Klyukovka repeats the process again.  At first, she perceives her environment and produces a function V (S) and a recommendation for action. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      "Critic": This valley looks pretty standard.  V (s) = 19. <br>  "Subject": Action options look very similar.  I think I'll just go along track "C". </p><img vspace="10" src="https://habrastorage.org/webt/al/eo/oy/aleooy4igiqksso14znacoxewjq.png"><p>  Further, it acts. </p><img vspace="10" src="https://habrastorage.org/webt/qd/kz/1y/qdkz1y1xfopu075_7yhhqtzmdkm.png"><p>  Receives a reward of +20!  And records the observation. </p><img vspace="10" src="https://habrastorage.org/webt/kg/gs/-v/kggs-vsqtnvt1pos-jf9yurnl_w.png"><p>  She repeats the process again. </p><img vspace="10" src="https://habrastorage.org/webt/wy/sp/d8/wyspd8yva6igzeeg29bw5x4lfdo.png"><p>  After collecting three observations, Klyukovka stops thinking. <br><br>  Other model families are waiting for the moment of reflection until the very end of the day (Monte Carlo), while others are thinking after each step (one-steps). <br>  Before she can set up her inner critic, Klukovka needs to calculate how many points she actually gets in each given state. <br><br>  But first! <br>  Let's take a look at how Klukowka's cousin, Lys Monte Carlo, calculates the true value of each state. <br><br>  Monte Carlo models do not reflect their experience until the end of the game, and since the value of the last state is zero, it is very simple to find the true value of this previous state, as the sum of the rewards received after this point. </p><img vspace="10" src="https://habrastorage.org/webt/xb/4m/g9/xb4mg9-occctbf6y-ixw-yhlohu.png"><p>  In fact, this is just a high dispersion V (S) sample.  The agent could easily follow a different path from the same state, thereby obtaining a different aggregate reward. </p><br><p>  But Klukovka goes, stops and reflects many times, until the day comes to an end.  She wants to know how many points she really gets from each state until the end of the game, because there are several hours left until the end of the game. <br><br>  That's where she does something really clever - Fox Klukovka assesses how many points she gets for the last state in this set.  Fortunately, she has the correct assessment of the state - her critic. <br>  With this estimate, Klukovka can calculate the ‚Äúcorrect‚Äù values ‚Äã‚Äãof the preceding states exactly as a Monte Carlo one does. <br><br>  Lees Monte Carlo evaluates target tags by deploying a trajectory and adding rewards ahead of each state.  A2C cuts this trajectory and replaces it with an assessment of its critic.  This initial load reduces the variance of the estimate and allows the A2C to operate continuously, albeit by introducing a small offset. </p><img vspace="10" src="https://habrastorage.org/webt/cw/7f/jo/cw7fjo1fqmzudzpagzrp5yqsuye.png"><p>  Rewards are often reduced to reflect the fact that rewards are now better than in the future.  For simplicity, Cranberry does not lower its rewards. </p><img vspace="10" src="https://habrastorage.org/webt/xn/2b/49/xn2b49qlafvhtjjjxd-buohiafs.png"><p> Now Klukovka can go through each data line and compare its estimates of state values ‚Äã‚Äãwith its actual values.  She uses the difference between these numbers to hone prediction skills.  Every three steps throughout the day, Klukovka gathers valuable experience to think about. <br><br>  ‚ÄúI didn‚Äôt rate states 1 and 2 badly.  Aha  The next time I see feathers like these, I will increase V (S). <br><br>  It may seem crazy that Klukovka is able to use his V (S) estimate as a basis to compare it with other forecasts.  But animals (including us) do it all the time!  If you feel that things are going well, you do not need to retrain the actions that led you to this state. </p><img vspace="10" src="https://habrastorage.org/webt/lg/qz/to/lgqztow-bkbik5suijbm_ix4bte.png"><p>  By trimming our calculated outputs and replacing them with the initial load estimate, we replaced the large Monte Carlo variance with a small offset.  RL models usually suffer from high dispersion (representing all possible trajectories), and such a replacement is usually worth it. </p><br><p>  Klyukovka repeats this process all day, collecting three observations of state-action-reward and reflecting on them. </p><img vspace="10" src="https://habrastorage.org/webt/ia/d8/r2/iad8r2v24aklliudj27dnsggaek.png"><p>  Each set of three observations is a small, autocorrelated series of labeled training data.  To reduce this autocorrelation, many A2C train many agents in parallel, putting their experience together before sending it into a common neural network. </p><img vspace="10" src="https://habrastorage.org/webt/4w/qx/rd/4wqxrd2ilmgr1cjvzpymeou4dvk.png"><p>  The day is finally coming to an end.  There are only two steps left. <br>  As we said earlier, recommendations for Kluk‚Äôs actions are expressed in percent certainty about its capabilities.  Instead of simply choosing the most reliable choice, Klukovka selects from this distribution of actions.  This ensures that she does not always agree to safe, but potentially mediocre actions. </p><br><p>  I could regret it, but ... Sometimes, exploring unknown things, you can come to exciting new discoveries ... </p><img vspace="10" src="https://habrastorage.org/webt/bd/w1/mq/bdw1mqtm96hfbvp6suy7ftdliec.png"><p>  To further promote research, a value called entropy is subtracted from the loss function.  Entropy means "span" of the distribution of actions. <br>  - It looks like the game has paid off! <br></p><img vspace="10" src="https://habrastorage.org/webt/nu/u1/tr/nuu1tr5-gopjruyry7z3qph7wfk.png"><p>  Or not? <br><br>  Sometimes an agent is in a state where all actions lead to negative outcomes.  A2C, however, does an excellent job with bad situations. </p><img vspace="10" src="https://habrastorage.org/webt/3w/kq/tn/3wkqtngcomlmylol0jdj79u6wfc.png"><img vspace="10" src="https://habrastorage.org/webt/va/9x/x6/va9xx61axwvy7lio5lgtyvcftzw.png"><p>  When the sun went down, Klyukovka reflected on the latest set of solutions. </p><img vspace="10" src="https://habrastorage.org/webt/ql/k0/sw/qlk0sw5tngzheqw6t5obdw_p_mw.png"><p>  We talked about how Klyukovka sets up his inner critic.  But how does she adjust her inner "subject"?  How does she learn to make such sophisticated choices? </p><br><p>  The simple-minded fox Gradient-Policy would look at the actual incomes after the action and set up their policies to make good returns more likely: - It seems that my policy in this state led to a loss of 20 points, I think that in the future it is better to do the less likely. <br><br>  - But wait!  It is not fair to put the blame on the action "C".  This condition had an estimated value of -100, so the choice of ‚ÄúC‚Äù and its ending from -20 was in fact a relative improvement of 80!  I must make the "C" more likely in the future. <br><br>  Instead of setting up its policy in response to the total income it received by choosing action C, it adjusts its action to relative income from action C. This is called an ‚Äúadvantage‚Äù. </p><img vspace="10" src="https://habrastorage.org/webt/tl/l6/zq/tll6zqru_0k4izo07o3gyutrsdi.png"><p>  What we have called an advantage is simply a mistake.  As an advantage, Klukovka uses it to make actions that were surprisingly good, more likely.  As an error, it uses the same magnitude to push its internal critic to improve the assessment of the value of the state. <br><br>  Subject takes advantage of: <br>  ‚ÄúWow, that worked better than I thought, acting C must have been a good idea." <br>  The critic uses the error: <br>  ‚ÄúBut why was I surprised?  I probably should not have rated this state so negatively. ‚Äù <br><br>  Now we can show how total losses are calculated - we minimize this function to improve our model. <br>  ‚ÄúTotal loss = loss of action + loss of value ‚Äî entropy‚Äù <br><br>  Note that to calculate the gradients of three qualitatively different types, we take the values ‚Äã‚Äã‚Äúthrough one‚Äù.  This is effective, but can make convergence more difficult. </p><img vspace="10" src="https://habrastorage.org/webt/qa/2x/oa/qa2xoa3zbrhfvjiuvej7awoas9e.png"><p>  Like all animals, as he grows up, Klukovka will sharpen his ability to predict the values ‚Äã‚Äãof states, gain greater confidence in his actions and be less likely to be surprised at the rewards. </p><br><p>  RL agents, such as Klukovka, not only generate all the necessary data, simply interacting with the environment, but also evaluate target labels themselves.  All so, the RL models update the previous estimates to better match the new and improved estimates. <br><br>  As Dr. David Silver, head of the RL group at Google Deepmind, says: AI = DL + RL.  When an agent like Klukovka can set up his own intelligence, the possibilities are endless ... </p><img vspace="10" src="https://habrastorage.org/webt/ea/5k/fm/ea5kfmjm_1hzvkjupaxfpiaucju.png"></div><p>Source: <a href="https://habr.com/ru/post/442522/">https://habr.com/ru/post/442522/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../442506/index.html">Is Russia being punished for illegal trade in personal data?</a></li>
<li><a href="../442508/index.html">How do udalenka accelerates innovation on gitlab</a></li>
<li><a href="../442514/index.html">Distributed systems. Design patterns. Book Review</a></li>
<li><a href="../442516/index.html">A guide to using pandas for analyzing large data sets.</a></li>
<li><a href="../442520/index.html">Case Saving 300,000 p. per month on contextual advertising</a></li>
<li><a href="../442524/index.html">How to increase security in personal identification and access control systems</a></li>
<li><a href="../442526/index.html">The history of Soviet cassette players (part two): Walkman boom, gadget for the KGB and tape recorders</a></li>
<li><a href="../442528/index.html">How to make the game run at 60fps</a></li>
<li><a href="../442530/index.html">Wireshark 3.0.0: review of innovations</a></li>
<li><a href="../442532/index.html">Video recorders for video surveillance - for free</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>