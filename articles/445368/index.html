<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Load testing games with a couple of hundreds of thousands of virtual users</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 

 I work in a game company that develops online games. At the moment, all our games are divided into many ‚Äúmarkets‚Äù (one ‚Äúmarket‚Äù per count...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Load testing games with a couple of hundreds of thousands of virtual users</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br>  I work in a game company that develops online games.  At the moment, all our games are divided into many ‚Äúmarkets‚Äù (one ‚Äúmarket‚Äù per country) and in each ‚Äúmarket‚Äù there are a dozen worlds between which players are distributed during registration (well, or sometimes they can choose it themselves).  Each world has one database and one or several web / app servers.  Thus, the load is divided and distributed over the worlds / servers almost evenly and as a result we get the maximum online at 6K-8K players (this is the maximum, mostly several times less) and 200-300 requests per prime-time per world. <br><br>  Such a structure with the division of players on markets and worlds is becoming obsolete, players want something global.  In the last games, we stopped dividing people across countries and left only one / two markets (America and Europe), but still with a lot of worlds in each.  The next step will be the development of games with a new architecture and the unification of all players in one single world with a <b>single database</b> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today I wanted to tell you a little about how I was tasked to check, and what if the whole online (which is 50-200 thousand users at the same time) of one of our popular games ‚Äúsend‚Äù to play the next game built on the new architecture and can the entire system, especially the database ( <b>PostgreSQL 11</b> ), is practically able to withstand such a load and, if it cannot, find out where our maximum is.  I'll tell you a little about the problems that have arisen and the solutions to preparing for testing so many users, the process itself and a little about the results. <br><a name="habracut"></a><br><h2>  Intro </h2><br>  In the past, in <b>InnoGames GmbH</b> each game team created a game project to its own taste and color, often using different technologies, programming languages ‚Äã‚Äãand databases.  In the appendage, we have a variety of external systems responsible for payments, sending push notifications, marketing, and so on.  To work with these systems, developers also created their own unique interfaces as best they could. <br><br>  At the present time in the mobile gaming business a lot of <a href="http://mediakix.com/mobile-gaming-industry-statistics-market-revenue/">money</a> and, accordingly, a huge competition.  It is very important to get it back from each spent dollar on marketing and a little more from above, so all game companies very often ‚Äúclose‚Äù the games at the stage of closed testing, if they do not meet the analytical expectations.  Accordingly, it is not profitable to waste time on inventing the next wheel, therefore it was decided to create a unified platform that will provide developers with a ready-made solution for integration with all external systems, a database with replication and all best practices.  All the developers need is to develop and ‚Äúsuperimpose‚Äù a good game on top of it and not lose time on the development that is not related to the game itself. <br><br>  This platform is called <b>GameStarter</b> : <br><br><img src="https://habrastorage.org/webt/fz/go/g3/fzgog3jsz4rzjqi0zvbwzysz-po.jpeg" alt="image"><br><br>  So, to the point.  All future InnoGames games will be built on this platform, which has two databases - master and game (PostgreSQL 11).  Master stores basic information about players (login, password, etc.) and participates mainly in the login / registration process of the game itself.  Game is the database of the game itself, where, respectively, all game data and entities are stored, which is the core of the game, where the entire load goes. <br>  Thus, the question arose whether this whole structure could withstand such a potential number of users equal to the maximum online of one of our most popular games. <br><br><h2>  Task </h2><br>  The task itself sounded like this: check whether the database (PostgreSQL 11), with replication enabled, can withstand all the load that we currently have in the most loaded game, having at our disposal the entire PowerEdge M630 hypervisor (HV). <br>  I‚Äôll clarify that the task at the moment was <b>only to verify</b> , using the existing configs of the database, which we formed taking into account best-practices and our own experience. <br><br>  I will say right away the database, and the whole system performed well, except for a couple of moments.  But this particular game project was at the prototype stage and in the future, with the complexity of the game mechanics, requests to the database will become more complicated and the load itself may increase significantly and its character may change.  To prevent this, it is necessary to iteratively test the project with each more or less significant milestone.  Automating the ability to run these kinds of tests with a couple of hundreds of thousands of users has become the main task at this stage. <br><br><h2>  Profile </h2><br>  Like any load testing, it all starts with drawing up a load profile. <br>  Our potential value CCU60 (CCU is the maximum number of users for a certain period of time, in this case 60 minutes) is assumed to be <b>250,000</b> users.  The number of competitive virtual users (VU) is lower than CCU60 and analysts have suggested that we can safely divide into two.  Round up and take <b>150000</b> competitive VU. <br><br>  The total number of requests per second was taken from one fairly loaded game: <br><br><img src="https://habrastorage.org/webt/lv/te/69/lvte69rifceelurn7r3t7trbgs4.png"><br><br>  Thus, our target load is ~ <b>20000 queries / s</b> at <b>150000</b> VU. <br><br><h2>  Structure </h2><br><h3>  The characteristics of the "stand" </h3><br>  In the last <a href="https://habr.com/ru/post/342380/">article</a> I already talked about automating the entire process of load testing.  Then I can repeat a little, but I will tell you some moments in more detail. <br><br><img src="https://habrastorage.org/webt/zh/hz/eo/zhhzeorw5_gboyuocu9ajmb3ulo.png"><br><br>  In the diagram, the blue squares are our hypervisors (HV), a cloud consisting of multiple servers (Dell M620 - M640).  On each HV it is started by means of KVM about ten virtual computers (VM) (web / app and db in a mix).  When creating any new VM, balancing and searching through a set of parameters of a suitable HV occurs and is initially unknown on which server it will appear. <br><br><h4>  Database (Game DB): </h4><br>  But for our db1 target, we reserved a separate HV <b>targer_hypervisor</b> based on the M630. <br><br>  Brief characteristics of targer_hypervisor: <br><br>  Dell M_630 <br>  Model name: Intel¬Æ Xeon¬Æ CPU E5-2680 v3 @ 2.50GHz <br>  CPU (s): 48 <br>  Thread (s) per core: 2 <br>  Core (s) per socket: 12 <br>  Socket (s): 2 <br>  RAM: 128 GB <br>  Debian GNU / Linux 9 (stretch) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br><br><div class="spoiler">  <b class="spoiler_title">Detailed Har-ki</b> <div class="spoiler_text"> Debian GNU / Linux 9 (stretch) <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.130-2 (2018-10-27) <br>  lscpu <br>  Architecture: x86_64 <br>  CPU op-mode (s): 32-bit, 64-bit <br>  Byte Order: Little Endian <br>  CPU (s): 48 <br>  On-line CPU (s) list: 0-47 <br>  Thread (s) per core: 2 <br>  Core (s) per socket: 12 <br>  Socket (s): 2 <br>  NUMA node (s): 2 <br>  Vendor ID: GenuineIntel <br>  CPU family: 6 <br>  Model: 63 <br>  Model name: Intel¬Æ Xeon¬Æ CPU E5-2680 v3 @ 2.50GHz <br>  Stepping: 2 <br>  CPU MHz: 1309.356 <br>  CPU max MHz: 3300.0000 <br>  CPU min MHz: 1200.0000 <br>  BogoMIPS: 4988.42 <br>  Virtualization: VT-x <br>  L1d cache: 32K <br>  L1i cache: 32K <br>  L2 cache: 256K <br>  L3 cache: 30720K <br>  NUMA node0 CPU (s): 0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,4 44.46 <br>  NUMA node1 CPU (s): 1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43 45,47 <br>  Flags bp moto smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts flush_l1d <br><br>  / usr / bin / qemu-system-x86_64 --version <br>  QEMU emulator version 2.8.1 (Debian 1: 2.8 + dfsg-6 + deb9u5) <br>  Copyright ¬© 2003-2016 Fabrice Bellard and the QEMU Project developers <br></div></div><br>  Brief characteristics of db1: <br>  Architecture: x86_64 <br>  CPU (s): 48 <br>  RAM: 64 GB <br>  4.9.0-8-amd64 # 1 SMP Debian 4.9.144-3.1 (2019-02-19) x86_64 GNU / Linux <br>  Debian GNU / Linux 9 (stretch) <br>  psql (PostgreSQL) 11.2 (Debian 11.2-1.pgdg90 + 1) <br><br><div class="spoiler">  <b class="spoiler_title">PostgreSQL config with some explanations</b> <div class="spoiler_text">  seq_page_cost = 1.0 <br>  random_page_cost = 1.1 # We have SSD <br>  include '/etc/postgresql/11/main/extension.conf' <br>  log_line_prefix = '% t [% p-% l]% q% u @% h' <br>  log_checkpoints = on <br>  log_lock_waits = on <br>  log_statement = ddl <br>  log_min_duration_statement = 100 <br>  log_temp_files = 0 <br>  autovacuum_max_workers = 5 <br>  autovacuum_naptime = 10s <br>  autovacuum_vacuum_cost_delay = 20ms <br>  vacuum_cost_limit = 2000 <br>  maintenance_work_mem = 128MB <br>  synchronous_commit = off <br>  checkpoint_timeout = 30min <br>  listen_addresses = '*' <br>  work_mem = 32MB <br>  effective_cache_size = 26214MB # 50% of available memory <br>  shared_buffers = 16384MB # 25% of available memory <br>  max_wal_size = 15GB <br>  min_wal_size = 80MB <br>  wal_level = hot_standby <br>  max_wal_senders = 10 <br>  wal_compression = on <br>  archive_mode = on <br>  archive_command = '/ bin / true' <br>  archive_timeout = 1800 <br>  hot_standby = on <br>  wal_log_hints = on <br>  hot_standby_feedback = on <br></div></div><br>  <b>hot_standby_feedback</b> is set to off by default, we had it turned on, but later we had to turn it off for a successful test.  I will explain later why. <br><br>  The main active tables in the database (construction, production, game_entity, building, core_inventory_player_resource, survivor) are pre-filled with data (approximately 80GB), using a bash script. <br><br><div class="spoiler">  <b class="spoiler_title">db-fill-script.sh</b> <div class="spoiler_text"><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash --clean TRUNCATE TABLE production CASCADE; TRUNCATE TABLE construction CASCADE; TRUNCATE TABLE building CASCADE; TRUNCATE TABLE grid CASCADE; TRUNCATE TABLE core_inventory_player_resource CASCADE; TRUNCATE TABLE survivor CASCADE; TRUNCATE TABLE city CASCADE; TRUNCATE TABLE game_entity CASCADE; TRUNCATE TABLE player CASCADE; TRUNCATE TABLE core_player CASCADE; TRUNCATE TABLE core_client_device CASCADE; --core_client_device INSERT INTO core_client_device (id, creation_date, modification_date, device_model, device_name, locale, platform, user_agent, os_type, os_version, network_type, device_type) SELECT (1000000000+generate_series(0,999999)) AS id, now(), now(), 'device model', 'device name', 'en_DK', 'ios', 'ios user agent', 'android', '8.1', 'wlan', 'browser'; --core_player INSERT INTO core_player (id, guest, name, nickname, premium_points, soft_deleted, session_id, tracking_device_data_id) SELECT (1000000000+generate_series(0,999999)) AS id, true, 'guest0000000000000000000', null, 100, false, '00000000-0000-0000-0000-000000000000', (1000000000+generate_series(0,999999)) ; --player INSERT INTO player (id, creation_date, modification_date, core_player_id) SELECT (1000000000+generate_series(0,999999)) , now(), now(), (1000000000+generate_series(0,999999)) ; --city INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1000000000+generate_series(0,999999)) , 'city', now(), now(); INSERT INTO city (id, game_design, player_id) SELECT (1000000000+generate_series(0,999999)) , 'city.default', (1000000000+generate_series(0,999999)) ; --survivor INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1001000000+generate_series(0,999999)) , 'survivor', now(), now(); INSERT INTO survivor (id, game_design, owning_entity_id, type) SELECT (1001000000+generate_series(0,999999)) , 'survivor.prod_1', (1000000000+generate_series(0,999999)) , 'survivor'; --core_inventory_player_resource INSERT INTO core_inventory_player_resource (id, creation_date, modification_date, amount, player_id, resource_key) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 1000, (1000000000+generate_series(0,1999999)/2) , CONCAT('resource_', (1000000000+generate_series(0,1999999)) % 2); --grid DROP INDEX grid_area_idx; INSERT INTO grid (id, creation_date, modification_date, area, city_id) SELECT (1000000000+generate_series(0,19999999)) , NOW(), NOW(), BOX '0,0,4,4', (1000000000+generate_series(0,19999999)/20) ; create index on grid using gist (area box_ops); --building INSERT INTO game_entity (id, type, creation_date, modification_date) SELECT (1002000000+generate_series(0,99999999)) , 'building', now(), now(); INSERT INTO building (id, game_design, owning_entity_id, x, y, rotation, type) SELECT (1002000000+generate_series(0,99999999)) , 'building.building_prod_1', (1000000000+generate_series(0,99999999)/100) , 0, 0, 'DEGREES_0', 'building'; --construction INSERT INTO construction (id, creation_date, modification_date, definition, entity_id, start) SELECT (1000000000+generate_series(0,1999999)) , NOW(), NOW(), 'construction.building_prod_1-construction', (1002000000+generate_series(0,1999999)*50) , NOW(); --production INSERT INTO production (id, creation_date, modification_date, active, definition, entity_id, start_time) SELECT (1000000000+generate_series(0,49999999)) , NOW(), NOW(), true, 'production.building_prod_1_production_1', (1002000000+generate_series(0,49999999)*2) , NOW();</span></span></code> </pre> <br></div></div><br>  Replication: <br><br><pre> <code class="plaintext hljs">SELECT * FROM pg_stat_replication; pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_lsn | write_lsn | flush_lsn | replay_lsn | write_lag | flush_lag | replay_lag | sync_priority | sync_state -----+----------+---------+---------------------+--------------+---------------------+-------------+-------------------------------+--------------+-----------+------------+------------+------------+------------+-----------------+-----------------+-----------------+---------------+------------ 759 | 17035 | repmgr | xl1db2 | xxxx | xl1db2 | 51142 | 2019-01-27 08:56:44.581758+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000393 | 00:00:00.001159 | 00:00:00.001313 | 0 | async 977 | 17035 | repmgr | xl1db3 |xxxxx | xl1db3 | 42888 | 2019-01-27 08:57:03.232969+00 | | streaming | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 18/424A9F0 | 00:00:00.000373 | 00:00:00.000798 | 00:00:00.000919 | 0 | async</code> </pre><br><h4>  Application servers </h4><br>  Next, on the productive HV (prod_hypervisors) of various configuration and capacity, 15 app servers are launched: 8 cores, 4GB.  The main thing to say is: openjdk 11.0.1 2018-10-16, spring, interaction with the database via <a href="https://www.baeldung.com/hikaricp">hikari</a> (hikari.maximum-pool-size: 50) <br><br><h4>  Stress testing environment </h4><br>  The entire load testing environment consists of one <b>admin.loadtest</b> main server, and several <b>generatorN.loadtest</b> servers (in this case there were 14 of them). <br><br>  <b>generatorN.loadtest</b> is the ‚Äúbare‚Äù VM of Debian Linux 9, with Java 8 installed. 32 cores / 32 gigabytes.  Located on "non-food" HV, so as not to accidentally kill the performance of important VM. <br><br>  <b>admin.loadtest</b> is a Debian Linux 9 virtualka, 16 cores / 16 gigs, it runs Jenkins, JLTC and other additional unimportant software. <br><br>  JLTC - <a href="https://github.com/innogames/JMeter-Control-Center">jmeter load test center</a> .  A Py / Django system that monitors and automates test launches, and results analysis. <br><br><h3>  Test run </h3><br><img src="https://habrastorage.org/webt/pb/f_/th/pbf_thx7mwuois96bffvbeehtxk.png"><br><br>  The process of launching the test itself looks like this: <br><br><ul><li>  The test runs from <b>Jenkins</b> .  Select the required Job, then you must enter the desired test parameters: <ul><li>  <b>DURATION</b> - test duration </li><li>  <b>RAMPUP</b> - warm up time </li><li>  <b>THREAD_COUNT_TOTAL</b> - the desired number of virtual users (VU) or threads </li><li>  <b>TARGET_RESPONSE_TIME</b> is an important parameter, so as not to overload the entire system with it, we set the desired response time, so the test will keep the load at a level at which the response time of the entire system is no more than the specified one. </li></ul></li><li>  Launch </li><li>  Jenkins clones the test plan from Gitlab, sends it to JLTC. </li><li>  JLTC works a bit with the test plan (for example, inserts CSV simple writer). </li><li>  JLTC calculates the required number of Jmeter servers to run the desired number of VUs (THREAD_COUNT_TOTAL). </li><li>  JLTC connects to each generator loadgeneratorN and starts the jmeter-server. </li></ul><br>  During the test, <b>JMeter-client</b> generates a CSV file with the results.  So during the test, the amount of data and the size of this file grows at an <b>insane</b> pace, and it cannot be used for analysis after the test - was (as an experiment) invented by <b>Daemon</b> , who parses it <i>on the fly</i> . <br><br><h3>  Test plan </h3><br>  You can download the test plan <a href="">here</a> . <br><br>  After registration / login, users work in the <b>Behavior</b> module, consisting of several <b>Throughput controller `s</b> , setting the likelihood of execution of a particular game function.  In each Throughput controller, there is a <b>Module controller</b> , which refers to the corresponding module that implements the function. <br><br><img src="https://habrastorage.org/webt/t0/ws/qp/t0wsqpbkgo-w6dt55gvxd6aw9pm.png"><br><br><h4>  Off-topic </h4><br>  During the development of the script, we tried to use Groovy in full and thanks to our Java programmer, I discovered a couple of tricks for myself (maybe it will help someone): <br><br><ul><li>  You can declare a function somewhere at the beginning of the test plan, and then use it in other pre-, post-processors and selectors.  <a href="http://mrhaki.blogspot.com/2009/08/groovy-goodness-turn-methods-into.html">Groovy Goodness: Turn Methods into Closures</a> : <br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//     - def sum(Integer x, Integer y) { return x + y } vars.putObject('sum', this.&amp;sum) //      closure.   . //     sampler`       def sum= vars.getObject('sum'); println sum(2, 2);</span></span></code> </pre> </li><li>  <a href="http://docs.groovy-lang.org/latest/html/gapi/groovy/json/JsonSlurper.html">groovy.json.JsonSlurper</a> is a great fast JSON parser.  Together with groovy, you can elegantly parse data and process it: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> groovy.json.JsonSlurper def canBuild = vars.getObject(canBuild); <span class="hljs-comment"><span class="hljs-comment">// ""       def content = jsonSlurper.parseText(response).content def buildings = content[0].buildings //         //               def constructableBuildingDefs = buildings .collect { k,v -&gt; v } .grep{ it.definitions .grep { it2 -&gt; it2['@type'] == 'type.googleapis.com/ConstructionDefinitionDTO'} .grep { it2 -&gt; canBuild(it2) } //   .size() &gt; 0 } if (!constructableBuildingDefs) { return; } Collections.shuffle(constructableBuildingDefs) //       </span></span></code> </pre></li></ul><br><h3>  VU / Threads </h3><br>  When the user enters the desired number of VUs using the THREAD_COUNT_TOTAL parameter when configuring the Jobs in Jenkins, it is necessary to somehow run the required number of Jmeter-servers and distribute the final number of VUs between them.  This part lies with the JLTC in the part called <b>controller / provision</b> . <br><br>  In essence, the algorithm is as follows: <br><br><ul><li>  We divide the desired number of VU <b>threads_num</b> by 200-300 threads and, based on the more or less adequate size of <b>-Xmsm -Xmxm,</b> determine the required memory value for one <i>jmeter-server</i> <b>required_memory_for_jri</b> (JRI - I call Jmeter remote instance, instead of Jmeter-server). </li><li>  From threads_num and required_memory_for_jri we find the total number of jmeter-server: <b>target_amount_jri</b> and the total value of the required memory: <b>required_memory_total</b> . </li><li>  We iterate over all loadgeneratorN generators one by one and run the maximum number of jmeter-servers based on the available memory on it.  As long as the number of running instances, current_amount_jri is <b>not equal to</b> target_amount_jri. </li><li>  (If the number of generators and total memory is not enough, add a new one to the pool) </li><li>  We connect to each generator, with the help of <b>netstat, we</b> memorize all the busy ports, and we run on the random ports (which are idle) the necessary number of jmeter-servers: <br><br><pre> <code class="python hljs"> netstat_cmd= <span class="hljs-string"><span class="hljs-string">'netstat -tulpn | grep LISTEN'</span></span> stdin, stdout, stderr = ssh.exec_command(cmd1) used_ports = [] netstat_output = str(stdout.readlines()) ports = re.findall(<span class="hljs-string"><span class="hljs-string">'\d+\.\d+\.\d+\.\d+\:(\d+)'</span></span>, netstat_output) ports_ipv6 = re.findall(<span class="hljs-string"><span class="hljs-string">'\:\:\:(\d+)'</span></span>, netstat_output) p.wait() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports: used_ports.append(int(port)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> ports_ipv6: used_ports.append(int(port)) ssh.close() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">1</span></span>, possible_jris_on_host + <span class="hljs-number"><span class="hljs-number">1</span></span>): port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> port <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> used_ports: port = int(random.randint(<span class="hljs-number"><span class="hljs-number">10000</span></span>, <span class="hljs-number"><span class="hljs-number">20000</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># ...  Jmeter-    </span></span></code> </pre></li><li>  We collect all running jmeter-servers in one period in the format address: port, for example <b>generator13: 15576, generator9: 14015, generator11: 19152, generator14: 12125, generator2: 17602</b> </li><li>  The resulting list and threads_per_host are sent to the JMeter-client when running the test: <br><br><pre> <code class="bash hljs">REMOTE_TESTING_FLAG=<span class="hljs-string"><span class="hljs-string">" -R </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$REMOTE_HOSTS_STRING</span></span></span><span class="hljs-string">"</span></span> java -jar -Xms7g -Xmx7g -Xss228k <span class="hljs-variable"><span class="hljs-variable">$JMETER_DIR</span></span>/bin/ApacheJMeter.jar -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -n -t <span class="hljs-variable"><span class="hljs-variable">$TEST_PLAN</span></span> -j <span class="hljs-variable"><span class="hljs-variable">$WORKSPACE</span></span>/loadtest.log -GTHREAD_COUNT=<span class="hljs-variable"><span class="hljs-variable">$THREADS_PER_HOST</span></span> <span class="hljs-variable"><span class="hljs-variable">$OTHER_VARS</span></span> <span class="hljs-variable"><span class="hljs-variable">$REMOTE_TESTING_FLAG</span></span> -Jjmeter.save.saveservice.default_delimiter=,</code> </pre></li></ul><br>  In our case, the test occurred simultaneously with 300 Jmeter-servers, 500 threads per each, the launch format of one Jmeter-server itself with Java parameters looked like this: <br><br><pre> <code class="bash hljs">nohup java -server -Xms1200m -Xmx1200m -Xss228k -XX:+DisableExplicitGC -XX:+CMSClassUnloadingEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=70 -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -Djava.net.preferIPv6Addresses=<span class="hljs-literal"><span class="hljs-literal">true</span></span> -Djava.net.preferIPv4Stack=<span class="hljs-literal"><span class="hljs-literal">false</span></span> -jar <span class="hljs-string"><span class="hljs-string">"/tmp/jmeter-JwKse5nY/bin/ApacheJMeter.jar"</span></span> -Jserver.rmi.ssl.disable=<span class="hljs-literal"><span class="hljs-literal">true</span></span> <span class="hljs-string"><span class="hljs-string">"-Djava.rmi.server.hostname=generator12.loadtest.ig.local"</span></span> -Duser.dir=/tmp/jmeter-JwKse5nY/bin/ -Dserver_port=13114 -s -Jpoll=49 &gt; /dev/null 2&gt;&amp;1</code> </pre> <br><h3>  50ms </h3><br>  The task is to determine how much our database can withstand, and not overload it and the entire system as a whole to a critical state.  With so many Jmeter-servers, you need to somehow maintain the load at a certain level and not kill the entire system.  The parameter <b>TARGET_RESPONSE_TIME,</b> which is set when the test is started, is responsible for this.  We agreed that <b>50ms</b> is the optimal response time for which the system should be responsible. <br><br>  In JMeter, by default, there are many different timers that allow you to control throughput, but where to get it in our case is unknown.  But there is a <b>JSR223-Timer</b> , with which you can come up with something, using the <b>current</b> system <b>response time</b> .  The timer itself is located in the main <b>Behavior</b> block: <br><br><img src="https://habrastorage.org/webt/uv/o8/rb/uvo8rb9ph7mr1xhxkzxccsfa06a.png"><br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">//      = 0 vars.put('samples', '20'); vars.putObject('respAvg', ${TARGET_RESPONSE_TIME}.0); vars.putObject('sleep', 0.0); //  JSR223-Timer           "" double sleep = vars.getObject('sleep'); double respAvg = vars.getObject('respAvg'); double previous = sleep; double target = ${TARGET_RESPONSE_TIME}; if (respAvg &lt; target) { sleep /= 1.5; } if (respAvg &gt; target) { sleep *= 1.1; } sleep = Math.max(10, sleep); //      sleep = Math.min(20000, sleep); vars.putObject('sleep', sleep); return (int)sleep;</span></span></code> </pre><br><h3>  Results Analysis (daemon) </h3><br>  In addition to the graphs in Grafana, it is also necessary to have aggregated test results so that tests can later be compared in JLTC. <br><br>  One such test generates 16k-20k requests per second, it is easy to calculate that in 4 hours it generates a CSV file the size of a couple of hundred GB, so it was necessary to come up with a job that parses the data every minute, sends them to the database and cleans the main file. <br><br><img src="https://habrastorage.org/webt/pb/mj/kc/pbmjkcwgjcxupnihgzpzmqd0nvq.png"><br><br>  The algorithm is as follows: <br><br><ul><li>  We read the data from the generated jmeter-client CSV file <b>result.jtl</b> , remember and clean the file (must be cleaned correctly, otherwise the file will have the same FD with the same size as empty): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(jmeter_results_file, <span class="hljs-string"><span class="hljs-string">'r+'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> f: rows = f.readlines() f.seek(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.truncate(<span class="hljs-number"><span class="hljs-number">0</span></span>) f.writelines(rows[<span class="hljs-number"><span class="hljs-number">-1</span></span>])</code> </pre></li><li>  We write the read data in a temporary file <b>temp_result.jtl</b> : <br><br><pre> <code class="python hljs">rows_num = len(rows) open(temp_result_filename, <span class="hljs-string"><span class="hljs-string">'w'</span></span>).writelines(rows[<span class="hljs-number"><span class="hljs-number">0</span></span>:rows_num]) <span class="hljs-comment"><span class="hljs-comment"># avoid last line</span></span></code> </pre> </li><li>  We read the file <b>temp_result.jtl</b> .  Distribute the read data "by the minute": <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> r <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> f.readlines(): row = r.split(<span class="hljs-string"><span class="hljs-string">','</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) == <span class="hljs-number"><span class="hljs-number">13</span></span>: ts_c = int(row[<span class="hljs-number"><span class="hljs-number">0</span></span>]) dt_c = datetime.datetime.fromtimestamp(ts_c/<span class="hljs-number"><span class="hljs-number">1000</span></span>) minutes_data.setdefault(dt_c.strftime(<span class="hljs-string"><span class="hljs-string">'%Y_%m_%d_%H_%M'</span></span>), []).append(r)</code> </pre></li><li>  Data for each minute from <b>minutes_data is</b> written to the corresponding file in the <b>to_parse /</b> folder.  (thus, at the moment, every minute of the test has its own data file, then during aggregation <b>it will</b> not matter in what order the data entered into each file): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> key, value <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> minutes_data.iteritems(): <span class="hljs-comment"><span class="hljs-comment">#      timestamp (key) temp_ts_file = os.path.join(temp_to_parse_path, key) open(temp_ts_file, 'a+').writelines(value)</span></span></code> </pre></li><li>  Along the way, we analyze the files in the to_parse folder and if any of them did not change within a minute, then this file is a candidate for data analysis, aggregation and sending to the JLTC database: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> filename <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> os.listdir(temp_to_parse_path): data_file = os.path.join(temp_to_parse_path, filename) file_mod_time = os.stat(data_file).st_mtime last_time = (time.time() - file_mod_time) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> last_time &gt; <span class="hljs-number"><span class="hljs-number">60</span></span>: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON] File {} was not modified since 1min, adding to parse list.'</span></span>.format(data_file)) files_to_parse.append(data_file)</code> </pre></li><li>  If such files (one or several) are present, then we send them to the parse_csv_data function (and in parallel each file): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> f <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> files_to_parse: logger.info(<span class="hljs-string"><span class="hljs-string">'[DAEMON THREAD] Parse {}.'</span></span>.format(f)) t = threading.Thread( target=parse_csv_data, args=( f, jmeter_results_file_fields, test, data_resolution)) t.start() threads.append(t) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> threads: t.join()</code> </pre></li></ul><br>  The daemon itself in cron.d runs every minute: <br><br>  daemon is started every minute via cron.d: <br><br><pre> <code class="bash hljs">* * * * * root sleep 21 &amp;&amp; /usr/bin/python /var/lib/jltc/manage.py daemon</code> </pre> <br>  Thus, the file with the results does not swell to unimaginable dimensions, but is analyzed <i>on the fly</i> and cleaned. <br><br><h2>  results </h2><br><h3>  App </h3><br>  Our 150,000 virtual players: <br><br><img src="https://habrastorage.org/webt/sv/ex/ep/svexepi9ikzy0unpxur96mty5q8.png"><br><br>  The test tries to ‚Äúmatch‚Äù the response time in 50ms, so the load itself constantly jumps in the area between 16k-18k requests / c: <br><br><img src="https://habrastorage.org/webt/-z/98/oi/-z98oi-_8a41hkqbrmz2rvzmryk.png"><br><br>  Load application servers (15 app).  The two servers were ‚Äúunlucky‚Äù on a slower M620: <br><br><img src="https://habrastorage.org/webt/nc/xy/et/ncxyetqyk_a8mbhjocndd-yxgkm.png"><br><br>  Database response time (for app servers): <br><br><img src="https://habrastorage.org/webt/zr/a-/gw/zra-gwtq_vqhtfhlrjzzy1osisg.png"><br><br><h3>  Database </h3><br>  CPU util on db1 (VM): <br><br><img src="https://habrastorage.org/webt/ej/hn/in/ejhnin0jo_7rrzhlj7pqko6udnq.png"><br><br>  CPU util on the hypervisor: <br><br><img src="https://habrastorage.org/webt/uw/ik/tz/uwiktzvcdzydlsjaoexqfbr3tay.png"><br><br>  On the virtual machine, the load is lower, since it believes that it has 48 real kernels at its disposal, in fact, there are 24 of them with <b>hyperthreading</b> on the hypervisor. <br><br>  A <b>maximum of ~ 250K queries / s</b> consisting of (83% selects, 3% - inserts, 11.6% - updates (90% HOT), 1.6% deletes) goes to the base: <br><br><img src="https://habrastorage.org/webt/lx/lu/bl/lxlublobhm4nm3c45g9jikcstc4.png"><br><br><img src="https://habrastorage.org/webt/18/jw/gp/18jwgpmebrkyot3ngvarsl_3ysu.png"><br><br>  With the default value of <b>autovacuum_vacuum_scale_factor</b> = 0.2, the number of dead tuples grew very quickly over the course of the test (with increasing table sizes), which led several times to short problems with database performance, which several times destroyed the entire test.  I had to ‚Äúrein in‚Äù this growth for some tables by assigning personal values ‚Äã‚Äãof this parameter to the autovacuum_vacuum_scale_factor: <br><br><div class="spoiler">  <b class="spoiler_title">ALTER TABLE ... SET (autovacuum_vacuum_scale_factor = ...)</b> <div class="spoiler_text">  ALTER TABLE construction SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE production SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE game_entity SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE building SET (autovacuum_analyze_scale_factor = 0.01); <br>  ALTER TABLE core_inventory_player_resource SET (autovacuum_vacuum_scale_factor = 0.10); <br>  ALTER TABLE survivor SET (autovacuum_vacuum_scale_factor = 0.01); <br>  ALTER TABLE survivor SET (autovacuum_analyze_scale_factor = 0.01); <br></div></div><br><img src="https://habrastorage.org/webt/0s/ja/1e/0sja1e1m3-2gitbmhh8j24t2ktu.png"><br><br>  Ideally rows_fetched should be close to rows_returned, which we, fortunately, are seeing: <br><br><img src="https://habrastorage.org/webt/e4/k6/zo/e4k6zobp25h5asrmxhmficoea3a.png"><br><br><h4>  hot_standby_feedback </h4><br>  The problem was with the <b>hot_standby_feedback</b> parameter, which can greatly affect the performance of the <b>main</b> server if its <b>standby</b> server does not have time to apply changes from WAL files.  The documentation (https://postgrespro.ru/docs/postgrespro/11/runtime-config-replication) states that it ‚Äúdetermines whether the hot standby server will inform the master or upstream slave about the requests it is currently performing.‚Äù By It defaults to off, but has been enabled in our config.  That led to sad consequences, if there are 2 standby servers and replication lag during the load is non-zero (for various reasons), one can observe a picture that could eventually lead to the collapse of the entire test: <br><br><img src="https://habrastorage.org/webt/vl/1l/jd/vl1ljdockxrjlfsoiybq751vo9y.png"><br><br><img src="https://habrastorage.org/webt/qh/4s/m_/qh4sm_hpnunb2w0axzhk90lmf6u.png"><br><br>  This is due to the fact that when hot_standby_feedback is on, VACUUM does not want to delete dead tuples if the standby servers are lagging behind in their transaction id to prevent replication conflicts.  Detailed article <a href="https://www.cybertec-postgresql.com/en/what-hot_standby_feedback-in-postgresql-really-does/">What hot_standby_feedback in PostgreSQL really does</a> : <br><br><pre> <code class="plaintext hljs">xl1_game=# VACUUM VERBOSE core_inventory_player_resource; INFO: vacuuming "public.core_inventory_player_resource" INFO: scanned index "core_inventory_player_resource_pkey" to remove 62869 row versions DETAIL: CPU: user: 1.37 s, system: 0.58 s, elapsed: 4.20 s ‚Ä¶‚Ä¶‚Ä¶... INFO: "core_inventory_player_resource": found 13682 removable, 7257082 nonremovable row versions in 71842 out of 650753 pages &lt;b&gt;DETAIL: 3427824 dead row versions cannot be removed yet, oldest xmin: 3810193429&lt;/b&gt; There were 1920498 unused item pointers. Skipped 8 pages due to buffer pins, 520953 frozen pages. 0 pages are entirely empty. CPU: user: 4.55 s, system: 1.46 s, elapsed: 11.74 s.</code> </pre><br>  Such a large dead tuples and leads to the picture shown above.  Here are two tests, with hot_standby_feedback turned on and off: <br><br><img src="https://habrastorage.org/webt/8f/od/n6/8fodn60lohgzsu-twheqysldjzy.png"><br><br>  And this is our replication lag during the test, with which it will be necessary to do something in the future: <br><br><img src="https://habrastorage.org/webt/et/s0/7h/ets07hkl8skkv5wexxjrgi-3x7m.png"><br><br><h2>  Conclusion </h2><br>  This test, fortunately (or unfortunately for the content of the article) showed that at this stage of the prototype of the game it is quite realistic to master the desired load from the users, which is enough to give the green light for further prototyping and development.  At subsequent stages of development, it is necessary to follow the basic rules (to keep the queries simple, to avoid oversupply of indexes, as well as non-indexed readings, etc.) and, most importantly, to test the project at each significant stage of development in order to find and fix problems like possible before.  Maybe soon, I will write an article as we have already solved specific problems. <br><br>  Good luck everyone! <br><br>  Our <a href="https://github.com/innogames/">GitHub</a> just in case;) </div><p>Source: <a href="https://habr.com/ru/post/445368/">https://habr.com/ru/post/445368/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445356/index.html">Overview of the Mobile section on the DUMP-2019: maximum application and useful in everyday work</a></li>
<li><a href="../445358/index.html">Organization of the event system in Unity - through the eyes of game designer</a></li>
<li><a href="../445360/index.html">5 typical tasks for interviews on JavaScript: analysis and solutions</a></li>
<li><a href="../445362/index.html">The book "Distributed Systems. Design Patterns ¬ª</a></li>
<li><a href="../445366/index.html">How to speed up encryption in accordance with GOST 28147-89 on a Baikal-T1 processor due to a SIMD unit</a></li>
<li><a href="../445370/index.html">TSDB Analysis in Prometheus 2</a></li>
<li><a href="../445372/index.html">Machine Vision vs Human Intuition: Algorithms for Disruption of Object Recognition</a></li>
<li><a href="../445378/index.html">Labyrinths: classification, generation, search for solutions</a></li>
<li><a href="../445380/index.html">Modern PHP is beautiful and productive.</a></li>
<li><a href="../445384/index.html">Mission "Chang'e-4" - scientific equipment on the landing module and the satellite transponder</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>