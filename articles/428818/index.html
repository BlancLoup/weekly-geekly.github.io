<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DeOldify: a program for coloring black and white images</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In short, the task of this project is to color and restore old images. I‚Äôll go into the details a bit, but first let's see the pictures! By the way, m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DeOldify: a program for coloring black and white images</h1><div class="post__text post__text-html js-mediator-article">  In short, the task of this project is to color and restore old images.  I‚Äôll go into the details a bit, but first let's see the pictures!  By the way, most of the original images are taken from the r / TheWayWeWere subdit, thank you all for such high-quality large images. <br><br>  <b>These are just a few examples, and they are quite typical!</b> <br><br>  <i>Maria Anderson as Little Fairy and her page Lyubov Ryabtsova in the Sleeping Beauty ballet at the Imperial Theater, St. Petersburg, Russia, 1890</i> <i><br></i> <br><img src="https://habrastorage.org/webt/s0/vh/vl/s0vhvlbdvsrvx09bxdugq6n6vaq.jpeg"><br><a name="habracut"></a><br>  <i>Woman relaxes in her living room (1920, Sweden)</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/webt/qt/fe/7f/qtfe7f0alc4foxo_f6vija5p1ey.jpeg"><br><br>  <i>Medical students posing near the corpse, circa 1890</i> <br><br><img src="https://habrastorage.org/webt/z7/bp/xl/z7bpxlwrvbib_tzqgaaw8ihadqy.jpeg"><br><br>  <i>Surfer in Hawaii, 1890</i> <br><br><img src="https://habrastorage.org/webt/pr/7f/do/pr7fdopo8pdjs-yoxa-v-4bbwmi.jpeg"><br><br>  <i>Turning Horse, 1898</i> <br><br><img src="https://habrastorage.org/webt/yj/pk/iw/yjpkiwkgzbscgirnzjtp3sny8po.jpeg"><br><br>  <i>Interior bar Miller and Schumaker, 1899</i> <br><br><img src="https://habrastorage.org/webt/x6/d8/mo/x6d8modnwsdzhquyhiqnpwf4t2c.jpeg"><br><br>  <i>Paris in the 1880s</i> <br><br><img src="https://habrastorage.org/webt/kl/ct/pj/klctpjrr-czaqpsejgpgzbwyuqw.jpeg"><br><br>  <i>Edinburgh bird's eye view in the 1920s</i> <br><br><img src="https://habrastorage.org/webt/fo/st/3w/fost3w3afdg26wwg_6dfjri-dw8.jpeg"><br><br>  <i>Texas woman in 1938</i> <br><br><img src="https://habrastorage.org/webt/cm/47/lp/cm47lpwsw1hpb5kcnkicahf-iga.jpeg"><br><br>  <i>People at Waterloo station watching television for the first time, London, 1936</i> <br><br><img src="https://habrastorage.org/webt/in/nk/x9/innkx9gt0mixwjvdhk8ridbrqpy.jpeg"><br><br>  <i>Geography lesson in 1850</i> <br><br><img src="https://habrastorage.org/webt/j_/ju/6l/j_ju6l3-7cea5_cuyjbucbsk8sq.jpeg"><br><br>  <i>Chinese opium smokers in 1880</i> <br><br><img src="https://habrastorage.org/webt/i_/_j/tr/i__jtry3w4divci7u7b2hcaagim.jpeg"><br><br>  <b>Please note that even really old and / or poor quality photos still look pretty cool:</b> <br><br>  <i>Deadwood, South Dakota, 1877</i> <br><br><img src="https://habrastorage.org/webt/lm/bj/vl/lmbjvladhdfyfygu0mal1rbqtry.jpeg"><br><br>  <i>Brothers and Sisters in 1877 (Deadwood)</i> <br><br><img src="https://habrastorage.org/webt/mz/br/wq/mzbrwqmqehigiyahfj_obgmlflc.jpeg"><br><br>  <i>Portsmouth Square in San Francisco 1851</i> <br><br><img src="https://habrastorage.org/webt/l3/qk/w-/l3qkw-yf0byhfmc8e6ysjl-bo7o.jpeg"><br><br>  <i>Samurai, circa 1860s</i> <br><br><img src="https://habrastorage.org/webt/l_/ug/wa/l_ugwar3td8vea4rhep9gfg7r4s.jpeg"><br><br>  Of course, the model is not perfect.  This red hand drives me crazy, but otherwise it works fantastic: <br><br>  <i>Seneca tribe girl from the Iroquois, 1908</i> <br><br><img src="https://habrastorage.org/webt/js/ft/be/jsftbe0bjiodlhidemhvjj5fdf0.jpeg"><br><br>  <b>She can also paint black and white drawings:</b> <br><br><img src="https://habrastorage.org/webt/hf/gf/zu/hfgfzugkkblhev4-ke5f55dabn8.jpeg"><br><br><h1>  Technical details </h1><br>  This is a model based on deep learning.  In particular, I combined the following approaches: <br><br><ul><li>  <b><a href="https://arxiv.org/abs/1805.08318">Self-Attention GAN</a></b> .  The only thing is that the <b>pre-trained Unet is</b> used as a generator and I just changed it for spectral normalization and the Self-Attention mechanism itself.  This is a fairly simple modification.  I can tell you that the difference is striking compared to the previous version of Wasserstein GAN, which I tried to make work.  I liked the Wasserstein GAN theory, but in practice it does not work.  But I just fell in love with the Self-Attention GAN network. </li><li>  A learning structure like the <b><a href="https://arxiv.org/abs/1710.10196">progressive growth of GAN</a></b> (but not exactly like this).  The difference is that the number of layers remains constant: I just changed the size of the input data and adjusted the learning speed so that the transitions between the dimensions were successful.  It seems that it produces the same end result, but it learns faster, is more stable and performs generalization better. </li><li>  <b><a href="https://arxiv.org/abs/1706.08500">TTUR</a></b> (Two Time-Scale Update Rule) rule.  It‚Äôs pretty understandable here: just one-to-one iteration of the generator / discriminator (critic) and a higher learning speed of the discriminator. </li><li>  <b>The generator loss function</b> consists of two parts: one of them is the main function of the Perceptual Loss (or Feature Loss) based on the VGG16 - it simply pushes the generator model to replicate the input image.  The second part is the evaluation of losses from the discriminator (critic).  For the curious: only the function Perceptual Loss is not enough for a good result.  It tends to just encourage a bunch of brown / green / blue - you know, by fooling the test, what neural networks are really good at!  The key point is that GAN is essentially exploring the loss function for you, which is actually one big step towards the ideal we are aiming at in machine learning.  And of course, the results will significantly improve when the machine itself learns what you previously coded manually.  Of course, here it is. </li></ul><br>  The beauty of this model is that it is quite good in a variety of image modifications.  What you see above are the results of the coloring model, but this is just one component in the pipeline that I want to develop with the same model. <br><br>  Then I will try to bring the old images to perfection, and the next item on the agenda is the model of improving saturation and richness (defade).  Now she is in the early stages of learning.  This is basically the same model, but with some contrast / brightness settings as a simulation of faded photos and pictures taken with old / poor equipment.  I have already received some encouraging results: <br><br><img src="https://habrastorage.org/webt/jh/bs/qg/jhbsqg6a-unrsvzhxtva3z0ee7u.jpeg"><br><br><h1>  More about the project </h1><br>  What is the essence of this project?  I just want to apply GAN to make old photos look very, very good.  And more importantly, it will make the project <i>useful</i> .  And yes, I'm definitely interested in working with the video, but first you need to figure out how to take this model under control by memory consumption (this is a real beast).  It would be nice if the models did not train for two to three days on 1080Ti (unfortunately, typical of the GAN).  Although this is my child and I will actively update and improve the code in the foreseeable future, but I will try to make the program as user friendly as possible, although there will certainly be some difficulties with it. <br><br>  And I swear I'll properly document the code ... someday.  Admittedly, I am one of those people who believe in ‚Äúself-documenting code‚Äù (LOL). <br><br><h1>  Independent model launch </h1><br>  The project is built on the wonderful Fast.AI library.  Unfortunately, this is the old version, and it has yet to be updated to the new one (this is definitely on the agenda).  So, the preliminary requirements in brief: <br><br><ul><li>  <b><i>Old</i> Fast.AI library</b> .  Having dug into the project for two months, I missed a little what happened to her, because the one that is now marked as ‚Äúold‚Äù does not really look like the one I have.  Everything has changed in the last two months or so.  Therefore, if nothing works with other versions, I <a href="https://github.com/jantic/fastai">forked it here</a> .  Again, updating to the latest version is on the agenda, I apologize in advance. </li><li>  <b>All Fast.AI dependencies</b> : there are convenient requirements.txt and environment.yml files. </li><li>  <b>Pytorch 0.4.1</b> (spectral_norm is required, so the last stable release is needed). </li><li>  <b>JupyterLab</b> . </li><li> <b>Tensorboard</b> (i.e. installing Tensorflow) and <a href="https://github.com/lanpa/tensorboardX"><b>TensorboardX</b></a> .  I think this is not <i>strictly</i> necessary, but so much easier.  For your convenience, I have already provided all the necessary hooks / callbacks in the Tensorboard!  There are examples of their use.  It is noteworthy that by default, the images are processed in the Tensorboard every 200 iterations, so you get a constant and convenient view of what the model does. </li><li>  <b>ImageNet</b> : a great set of data for learning. </li><li>  <b>Powerful graphics card</b> .  I would really like to have more memory than 11 GB in my GeForce 1080Ti.  If you have something weaker, it will be difficult.  Unet and Critic are absurdly large, but the bigger they are, the better the results. </li></ul><br>  <b>If you want to start processing images right now</b> without learning the model, you can download the finished weights <a href="">here</a> .  Then open ColorizationVisualization.ipynb in JupyterLab.  Make sure that there is a line with a reference to the weight: <br><br><pre><code class="python hljs">colorizer_path = Path(<span class="hljs-string"><span class="hljs-string">'/path/to/colorizer_gen_192.h5'</span></span>)</code> </pre> <br>  Then you need to load the colorizer model after netG is initialized: <br><br><pre> <code class="python hljs">load_model(netG, colorizer_path)</code> </pre> <br>  Then simply place any images in the / test_images / folder from where you run the program.  You can visualize the results in Jupyter Notebook with the following lines: <br><br><pre> <code class="python hljs">vis.plot_transformed_image(<span class="hljs-string"><span class="hljs-string">"test_images/derp.jpg"</span></span>, netG, md.val_ds, tfms=x_tfms, sz=<span class="hljs-number"><span class="hljs-number">500</span></span>)</code> </pre> <br>  I would keep the size around 500px, plus or minus, if you run the program on a GPU with a lot of memory (for example, GeForce 1080Ti 11 GB).  If the memory is less, then you will have to reduce the size of the pictures or try to run on the CPU.  I actually tried to do the latter, but for some reason the model worked very, absurdly slowly, and I did not find the time to investigate the problem.  Connoisseurs recommended building Pytorch from source, then you‚Äôll get a big performance boost.  Hmm ... At that moment it was not before. <br><br><h1>  Additional Information </h1><br>  Jupyter <i>can</i> also render the generated images as <i>you</i> learn: you only need to set the value to <i>true</i> when creating an instance of this visualization hook: <br><br> <code>GANVisualizationHook(TENSORBOARD_PATH, trainer, 'trainer', jupyter=True, visual_iters=100</code> <br> <br>  I prefer to leave <i>false</i> and just use Tensorboard.  Believe me, you also want to do just that.  In addition, if left to work for too long, Jupyter will eat a lot of memory with such images. <br><br>  Model weights are also automatically saved during the GANTrainer training runs.  By default, they are saved every 1000 iterations (this is an expensive operation).  They are stored in the root folder that you specified for training, and the name corresponds to the save_base_name specified in the training schedule.  Weights are stored separately for each workout size. <br><br>  I would recommend navigating the code from top to bottom, starting with Jupyter Notebook.  I treat these notes simply as a convenient interface for prototyping and visualization, everything else will go into .py files as soon as I find a place for them.  I already have visualization examples that can be conveniently included and viewed: just open xVisualization in Notebook, the test images included in the project are listed there (they are in test_images). <br><br>  If you see GAN Schedules, then this is the ugliest thing in the project, just my version of the implementation of progressive GAN learning, suitable for the Unet generator. <br><br>  The pre-learned weights for the colorizer generator are also <a href="">here</a> .  The project DeFade while in work, I will try to lay out a good weight in a few days. <br><br>  Usually, when training, you will see the first good results halfway, that is, with a size of 192px (if you use the provided training examples). <br><br>  I'm sure I screwed up somewhere, so please let me know if this is the case. <br><br><h1>  Known Issues </h1><br><ul><li>  You have to <b>play a</b> little <b>with the image size</b> to get the best result.  The model clearly suffers from some dependence on the aspect ratio and size when generating images.  It used to be much worse, but the situation improved significantly with the increase in lighting / contrast and the introduction of progressive learning.  I want to completely eliminate this problem and focus on it, but do not despair if the image looks excessively saturated or with strange glitches.  Most likely, everything will be fine after a small size change.  As a rule, for oversaturated images you need to increase the size. </li><li>  In addition to the above: getting the best images really comes down to the <b>art of choosing the optimal parameters</b> .  Yes, the results are selected manually.  I am very pleased with the quality, and the model works quite reliably, but not perfectly.  The project is still ongoing!  I think the tool can be used as an "artist's AI", but it is not yet ready for the general public.  Just not time yet. </li><li>  To complicate the situation: at present, the model is <b>brutally eating memory</b> , so on my 1080Ti card it turns out to handle images with a maximum of 500-600px.  I bet there are a lot of optimization options here, but I haven't done it yet. </li><li>  I added zero padding to the Unet generator for everything that does not match the expected sizes (this is how I can load an image of an arbitrary size).  It was a very fast hack, and it leads to silly right and lower bounds on the output of test images of arbitrary size.  I am sure that there is a better way, but I have not found it yet. </li><li>  The model <i>loves</i> blue clothes.  Not quite sure why the solution is in the search! </li></ul><br><h1>  Want more? </h1><br>  I will post new results <a href="https://twitter.com/citnaj">on Twitter</a> . <br><br>  <i>Addition from the translator.</i> <br>  From the last on Twitter: <br><br>  <i>Representatives themselves nationality in their dugout, 1880</i> <br><br><img src="https://habrastorage.org/webt/pc/qb/sq/pcqbsqbpnwcxf2htl6sd4s8p1ki.jpeg"><br>  ( <a href="https://www.reddit.com/r/TheWayWeWere/comments/9jdd4w/sami_people_with_their_turf_hut_c1880/">original</a> ) <br><br>  <i>Construction of the London Underground, 1860</i> <br><br><img src="https://habrastorage.org/webt/k9/ag/td/k9agtdyfo6ugvfs0fencnkf4nge.jpeg"><br>  ( <a href="https://www.reddit.com/r/HistoryPorn/comments/8606qa/london_underground_under_construction_in_1860_990/">original</a> ) <br><br>  <i>Slums of Baltimore, 1938</i> <br><br><img src="https://habrastorage.org/webt/4i/2s/kt/4i2sktdre4ehitbtuybqzwocaia.jpeg"><br><br>  <i>Gym on the Titanic, 1912</i> <br><br><img src="https://habrastorage.org/webt/ud/_v/vn/ud_vvn-6x0v3lcbx-khx_tvtey0.jpeg"><br>  ( <a href="https://www.reddit.com/r/TheWayWeWere/comments/7riclb/gym_aboard_the_titanic_1912/">original</a> ) </div><p>Source: <a href="https://habr.com/ru/post/428818/">https://habr.com/ru/post/428818/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428808/index.html">Small convenience in student life</a></li>
<li><a href="../428810/index.html">18 digital audio technologies</a></li>
<li><a href="../428812/index.html">TypeScript: Deserializing JSON into Classes with Property Type Validation</a></li>
<li><a href="../428814/index.html">Comparison of products using Elasticsearch for a competitor price monitoring service</a></li>
<li><a href="../428816/index.html">Material Design: Shape - tips for improving the GUI of an Android application (and not only) by changing the shape of elements</a></li>
<li><a href="../428820/index.html">You're in third-person 3D: Oculus Go + Raspberry Pi</a></li>
<li><a href="../428822/index.html">The story of a little hack, or an adequate bugbound from a local internet provider</a></li>
<li><a href="../428824/index.html">Telescope beyond reasonable</a></li>
<li><a href="../428826/index.html">Ekaterinburg through the eyes of a visitor or 5 years after the first acquaintance</a></li>
<li><a href="../428828/index.html">Remote control from smartphone</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>