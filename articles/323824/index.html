<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Accelerate Home ESXi 6.5 with SSD Caching</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day! 

 In this article I want to talk about how to slightly improve the performance of an ESXi host using SSD caching. At work and at home I use...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Accelerate Home ESXi 6.5 with SSD Caching</h1><div class="post__text post__text-html js-mediator-article">  Good day! <br><br>  In this article I want to talk about how to slightly improve the performance of an ESXi host using SSD caching.  At work and at home I use products from the company VMware, the home laboratory is based on Free ESXi 6.5.  The host runs virtual machines for both the home infrastructure and for testing some working projects (somehow I had to run the VDI infrastructure on it).  Gradually, applications of thick VMs began to rest on the performance of the disk system, and everything did not fit on the SDD.  The solution was chosen lvmcache.  The logic diagram looks like this: <br><br><img src="https://habrastorage.org/files/43d/cd8/404/43dcd8404153459b88245157adecff23.png"><br><a name="habracut"></a><br>  The basis of the whole scheme is a CentOS 7 based svm VM. It presented HDD disks using RDM and a small VMDK disk from an SSD datastor.  Caching and data mirroring are implemented by software - mdadm and lvmcache.  VM disk space is mounted to the host as an NFS datastore.  Part of the SSD datastor is dedicated to VMs that need a high-performance disk subsystem. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Computing node assembled on the desktop gland: <br><br>  MB: Gygabyte GA-Z68MX-UD2H-B3 (rev. 1.0) <br>  HDD: 2 x Seagate Barracuda 750Gb, 7200 rpm <br>  SSH: OCZ Vertex 3 240Gb <br><br>  There are 2 RAID controllers on the motherboard: <br><br>  - Intel Z68 SATA Controller <br>  - Marvell 88SE9172 SATA Controller <br><br>  I didn‚Äôt manage to set up 88SE9172 in ESXi (Marvell adapters (at least 88SE91xx)), I decided to leave both controllers in ACHI mode. <br><br><h3>  Rdm </h3><br>  RDM (Raw Device Mapping) technology allows a virtual machine to directly access a physical drive.  Communication is provided through special mapping files on a separate VMFS volume.  RDM uses two compatibility modes: <br><br>  - Virtual mode - works in the same way as in the case of a virtual disk file, allows you to take advantage of the virtual disk in VMFS (file locking mechanism, instant snapshots); <br>  - Physical mode - provides direct access to the device for applications that require a lower level of control. <br><br>  In virtual mode, read / write operations are sent to the physical device.  The RDM device is represented in the guest OS as a virtual disk file, the hardware characteristics are hidden. <br><br>  In physical mode, almost all SCSI commands are transmitted to the device, in the guest OS, the device is represented as real. <br><br>  By connecting the disk drives to the VM using RDM, you can get rid of the VMFS interlayer, and in the physical compatibility mode, their state can be monitored on the VM (using SMART technology).  In addition, if something happens to the host, then you can access the VM by mounting the HDD to the working system. <br><br><h3>  lvmcache </h3><br>  lvmcache provides transparent caching of data of slow HDD devices on fast SSD devices.  LVM cache places the most frequently used blocks on a fast device.  Turning on and off caching can be done without interrupting work. <br><img src="https://habrastorage.org/files/a33/b40/999/a33b409992774c1394ada486e786ad8c.png"><br><br>  When you try to read the data, it turns out whether this data is in the cache.  If the required data is not there, then the reading takes place from the HDD, and along the way the data is written to the cache (cache miss).  Further reading of the data will come from the cache (cache hit). <br><br><h4>  Record </h4><br>  - Write-through mode - when a write operation occurs, data is recorded both in the cache and on the HDD disk, a safer option, the probability of data loss in case of an accident is small; <br>  - Write-back mode - when a write operation occurs, data is first written to the cache, then flushed to disk, there is a possibility of data loss in the event of a crash.  (A faster option, since the write operation completion signal is transmitted to the managing OS after receiving the data by the cache). <br><br>  This is how to reset the data from the cache (write-back) to the disks: <br><br> <a href=""><img src="https://habrastorage.org/files/8af/af1/961/8afaf19610324d5782d6793a6dad233e.png"></a> <br><br><h3>  System Setup </h3><br>  SSD datastore is created on the host.  I chose this scheme for using the available space: <br><br> <code>220Gb ‚Äî DATASTORE_SSD <br> 149Gb ‚Äî     <br> 61Gb ‚Äî      <br> 10Gb ‚Äî Host Swap Cache</code> <br> <br>  A virtual network looks like this: <br><br><img src="https://habrastorage.org/files/ed6/f52/cca/ed6f52cca8bc4ee99e26703217e9d0a7.png"><br><br>  Created a new vSwitch: <br><br> <code>Networking ‚Üí Virtual Switches ‚Üí Add standart virtual switch ‚Äî      (svm_vSwitch,      svm_),    .</code> <br> <br>  The VMkernel NIC connects to it via the port group: <br><br> <code>Networking ‚Üí VMkernel NICs ‚Üí Add VMkernel NIC <br> ‚Äî Port group ‚Äî New Port group <br> ‚Äî New port group ‚Äî    ‚Äî svm_PG <br> ‚Äî Virtual switch ‚Äî svm_vSwitch <br> ‚Äî IPv4 settings ‚Äî Configuration ‚Äî Static ‚Äî  IP   </code> <br> <br>  Created port group to which VM svm will be connected: <br><br> <code>Networking ‚Üí Port Groups ‚Üí Add port group ‚Äî   (svm_Network)   svm_vSwitch</code> <br> <br><h4>  Disk preparation </h4><br>  You must log in to the ssh host and run the following commands: <br><br><pre> <code class="bash hljs">    : <span class="hljs-comment"><span class="hljs-comment"># ls -lh /vmfs/devices/disks/ lrwxrwxrwx 1 root root 72 Feb 22 20:24 vml.01000000002020202020202020202020203956504257434845535433373530 -&gt; t10.ATA_____ST3750525AS_________________________________________9*E lrwxrwxrwx 1 root root 72 Feb 22 20:24 vml.01000000002020202020202020202020203956504257434b46535433373530 -&gt; t10.ATA_____ST3750525AS_________________________________________9*F   ,    ¬´mapping file¬ª: # cd /vmfs/volumes/DATASTORE_SSD/  RDM    : # vmkfstools -r /vmfs/devices/disks/vml.01000000002020202020202020202020203956504257434845535433373530 9*E.vmdk # vmkfstools -r /vmfs/devices/disks/vml.01000000002020202020202020202020203956504257434b46535433373530 9*F.vmdk</span></span></code> </pre> <br><h4>  VM preparation </h4><br>  Now these disks can be connected (Existing hard disk) to the new VM.  Template CentOS 7, 1vCPU, 1024Gb RAM, 2 RDM disk, 61Gb ssd disk, 2 vNIC (VM Network group port, svm_Network) - during installation of the OS we use Device Type - LVM, RAID Level - RAID1 <br><br>  Setting up an NFS server is quite simple: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># yum install nfs-utils # systemctl enable rpcbind # systemctl enable nfs-server # systemctl start rpcbind # systemctl start nfs-server # vi /etc/exports /data 10.0.0.1(rw,sync,no_root_squash,no_subtree_check) # exportfs -ar # firewall-cmd --add-service=nfs --permanent # firewall-cmd --add-service=rpc-bind --permanent # firewall-cmd --add-service=mountd --permanent # firewall-cmd --reload</span></span></code> </pre> <br>  Prepare cache and metadata volumes to enable caching of the volume cl_svm / data: <br><br><pre> <code class="bash hljs">     : <span class="hljs-comment"><span class="hljs-comment"># pvcreate /dev/sdc # vgextend cl_svm /dev/sdc    ,  "man" ,       1000     : # lvcreate -L 60M -n meta cl_svm /dev/sdc    : # lvcreate -L 58,9G -n cache cl_svm /dev/sdc  -  : # lvconvert --type cache-pool --cachemode writethrough --poolmetadata cl_svm/meta cl_svm/cache   -   : # lvconvert --type cache --cachepool cl_svm/cache cl_svm/data     : # lvs -o cache_read_hits,cache_read_misses,cache_write_hits,cache_write_misses CacheReadHits CacheReadMisses CacheWriteHits CacheWriteMisses 421076 282076 800554 1043571</span></span></code> </pre> <br>  Array status change notifications: <br><br>  At the end of the /etc/mdadm.conf file, add parameters containing the address to which messages will be sent in case of problems with the array, and, if necessary, specify the sender's address: <br><br> <code>MAILADDR alert@domain.ru <br> MAILFROM svm@domain.ru</code> <br> <br>  For the changes to take effect, you must restart the mdmonitor service: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#systemctl restart mdmonitor</span></span></code> </pre> <br>  Mail from VM is sent by means of ssmtp.  Since I use RDM in the virtual compatibility mode, the state of the disks will be checked by the host itself. <br><br>  <b>Host preparation</b> <br><br>  Add NFS datastore to ESXi: <br><br> <code>Storage ‚Üí Datastores ‚Üí New Datastore ‚Üí Mount NFS Datastore <br> Name: DATASTORE_NFS <br> NFS server: 10.0.0.2 <br> NFS share: /data</code> <br> <br>  Configure VM autostart: <br><br> <code>Host ‚Üí Manage ‚Üí System ‚Üí Autostart ‚Üí Edit Settings <br> Enabled ‚Äî Yes <br> Start delay ‚Äî 180sec <br> Stop delay ‚Äî 120sec <br> Stop action ‚Äî Shut down <br> Wait for heartbeat ‚Äî No <br> <br> Virtual Machines ‚Üí svm ‚Üí Autostart ‚Üí Increase Priority <br> (  ,     Inventory   )</code> <br> <br>  This policy will allow VM svm to start first, the hypervisor will mount the NFS datastore, after that the rest of the machines will turn on.  Shutdown occurs in reverse order.  The launch delay time of the VM was selected according to the results of the crash test, since with a small Start delay NFS value, the datastor could not be mounted, and the host tried to start the VMs that are not yet available.  You can also play with the <code>NFS.HeartbeatFrequency</code> parameter. <br><br>  More flexibly, autostart VMs can be configured using the command line: <br><br><pre> <code class="bash hljs">    : <span class="hljs-comment"><span class="hljs-comment"># vim-cmd hostsvc/autostartmanager/get_autostartseq      (): # update_autostartentry VMId StartAction StartDelay StartOrder StopAction StopDelay WaitForHeartbeat : # vim-cmd hostsvc/autostartmanager/update_autostartentry 3 "powerOn" "120" "1" "guestShutdown" "60" "systemDefault"</span></span></code> </pre> <br>  <b>Small optimization</b> <br><br>  Enable Jumbo Frames on the host: <br><br> <code>Jumbo Frames: Networking ‚Üí Virtual Switches ‚Üí svm_vSwitch  MTU 9000; <br> Networking ‚Üí Vmkernel NICs ‚Üí vmk1  MTU 9000</code> <br> <br>  In Advanced Settings, set the following values: <br><br> <code>NFS.HeartbeatFrequency = 12 <br> NFS.HeartbeatTimeout = 5 <br> NFS.HeartbeatMaxFailures = 10 <br> Net.TcpipHeapSize = 32 ( 0) <br> Net.TcpipHeapMax = 512 <br> NFS.MaxVolumes = 256 <br> NFS.MaxQueueDepth = 64 ( 4294967295)</code> <br> <br>  Enable Jumbo Frames on VM svm: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ifconfig ens224 mtu 9000 up # echo MTU=9000 &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-ens224</span></span></code> </pre> <br><h3>  Performance </h3><br>  Performance was measured by synthetic dough (for comparison, I took readings from the cluster at work (at night)). <br><br>  Used software on test VM: <br><br>  - CentOS OS 7.3.1611 (8 vCPU, 12Gb vRAM, 100Gb vHDD) <br>  - fio v2.2.8 <br><br><pre> <code class="bash hljs">   : <span class="hljs-comment"><span class="hljs-comment"># dd if=/dev/zero of=/dev/sdb bs=2M oflag=direct # fio -readonly -name=rr -rw=randread -bs=4k -runtime=300 -iodepth=1 -filename=/dev/sdb -ioengine=libaio -direct=1 # fio -readonly -name=rr -rw=randread -bs=4k -runtime=300 -iodepth=24 -filename=/dev/sdb -ioengine=libaio -direct=1 # fio -name=rw -rw=randwrite -bs=4k -runtime=300 -iodepth=1 -filename=/dev/sdb -ioengine=libaio -direct=1 # fio -name=rw -rw=randwrite -bs=4k -runtime=300 -iodepth=24 -filename=/dev/sdb -ioengine=libaio -direct=1</span></span></code> </pre> <br>  The results obtained are presented in tables (* during the tests noted the average CPU usage on VM svm): <br><table><caption>  VMFS6 Datastore </caption><tbody><tr><th rowspan="2">  Disc type </th><th colspan="2">  FIO depth 1 (iops) </th><th colspan="2">  FIO depth 24 (iops) </th></tr><tr><td>  randread </td><td>  randwrite </td><td>  randread </td><td>  randwrite </td></tr><tr><td>  HDD </td><td>  77 </td><td>  99 </td><td>  169 </td><td>  100 </td></tr><tr><td>  SSD </td><td>  5639 </td><td>  17039 </td><td>  40868 </td><td>  53670 </td></tr></tbody></table><br><table><caption>  NFS Datastore </caption><tbody><tr><th rowspan="2">  SSD Cache </th><th colspan="2">  FIO depth 1 (iops) </th><th colspan="2">  FIO depth 24 (iops) </th><th rowspan="2">  CPU / Ready *% </th></tr><tr><td>  randread </td><td>  randwrite </td><td>  randread </td><td>  randwrite </td></tr><tr><td>  Off </td><td>  103 </td><td>  97 </td><td>  279 </td><td>  102 </td><td>  2.7 / 0.15 </td></tr><tr><td>  On </td><td>  1390 </td><td>  722 </td><td>  6474 </td><td>  576 </td><td>  15 / 0.1 </td></tr></tbody></table><br><table><caption>  Work Cluster </caption><tbody><tr><th rowspan="2">  Disc type </th><th colspan="2">  FIO depth 1 (iops) </th><th colspan="2">  FIO depth 24 (iops) </th></tr><tr><td>  randread </td><td>  randwrite </td><td>  randread </td><td>  randwrite </td></tr><tr><td>  900Gb 10k (6D + 2P) </td><td>  122 </td><td>  1085 </td><td>  2114 </td><td>  1107 </td></tr><tr><td>  4Tb 7.2k (8D + 2P) </td><td>  68 </td><td>  489 </td><td>  1643 </td><td>  480 </td></tr></tbody></table><br>  The results that you can touch with your hands turned out when you simultaneously run five VMs with Windows 7 and an office suite (MS Office 2013 Pro + Visio + Project) at startup.  As the cache warms up, the VMs loaded faster, while the HDD practically did not participate in the load.  At each launch, I noted the time for a full load of one of the five VMs and a full load of all VMs. <br><table><caption>  Simultaneous start of five VM </caption><tbody><tr><th rowspan="2">  No </th><th rowspan="2">  Datastore </th><th colspan="2">  First start </th><th colspan="2">  Second run </th><th colspan="2">  Third launch </th></tr><tr><td>  Load time of the first VM </td><td>  Load time of all VMs </td><td>  Load time of the first VM </td><td>  Load time of all VMs </td><td>  Load time of the first VM </td><td>  Load time of all VMs </td></tr><tr><td>  one </td><td>  Hdd VMFS6 </td><td>  4 min.  8s </td><td>  6 min.  28s </td><td>  3 min.  56s </td><td>  6 min.  23s </td><td>  3 min.  40sec. </td><td>  5 minutes.  50 sec. </td></tr><tr><td>  2 </td><td>  NFS (SSD Cache Off) </td><td>  2 minutes.  20sec </td><td>  3 min.  2s </td><td>  2 minutes.  34 sec. </td><td>  3 min.  2s </td><td>  2 minutes.  34 sec. </td><td>  2 minutes.  57sec </td></tr><tr><td>  3 </td><td>  NFS (SSD Cache On) </td><td>  2 minutes.  33 sec. </td><td>  2 minutes.  50 sec. </td><td>  1 minute.  23s </td><td>  1 minute.  51 sec. </td><td>  1 minute.  0sec. </td><td>  1 minute.  13s </td></tr></tbody></table><br>  The loading time of a single VM was: <br><br> <code>‚Äî HDD VMFS6 - 50  <br> ‚Äî NFS    - 35  <br> ‚Äî NFS      - 26 </code> <br> <br>  In the form of a graph: <br><br><img src="https://habrastorage.org/files/1aa/93b/a6a/1aa93ba6aa9b4f1faa647f98f54cc78c.png"><br><br><h3>  Crush test </h3><br><h4>  Power down </h4><br>  After powering on and loading the VM host, the svm was booted with the FS scan (data remained in the cache), the NFS datastore was mounted on the host, then the rest of the VMs were loaded, problems and data loss were not observed. <br><br><h4>  HDD failure (imitation) </h4><br>  I decided to power off the SATA drive.  Unfortunately, hot swapping is not supported, it is necessary to shut down the host in an emergency.  Immediately after turning off the disk information appears in the Events. <br><br> <a href=""><img src="https://habrastorage.org/files/7fe/d74/af6/7fed74af695d4c32a31c63f2e357fb75.png"></a> <br><br>  The unpleasant moment was that when a disk was lost, the hypervisor asked the VM svm to answer the question - ‚ÄúRetry.  Click Cancel to terminate this session ‚Äù- the machine is in a freeze state. <br><br>  If we imagine that there was a temporary, insignificant problem with the disk (for example, the reason for the loop), then after the problem is fixed and the host is turned on, everything is loaded in the normal mode. <br><br><h4>  SSD failure </h4><br>  The most unpleasant situation is the failure of ssd.  Access to the data is in emergency mode.  When replacing ssd, you must repeat the system setup procedure. <br><br><h3>  Service (disk replacement) </h3><br>  If a disk is about to happen (according to the results of SMART), in order to replace it with a worker, you must perform the following procedure (on the VM svm): <br><br><pre> <code class="bash hljs">   : <span class="hljs-comment"><span class="hljs-comment"># cat /proc/mdstat    : # mdadm --detail /dev/md126 /dev/md126   : # mdadm --manage /dev/md127 --fail /dev/sda1 # mdadm --manage /dev/md126 --fail /dev/sda2     : # mdadm --manage /dev/md127 --remove /dev/sda1 # mdadm --manage /dev/md126 --remove /dev/sda2</span></span></code> </pre> <br>  In the VM settings, you need to ‚Äútear off‚Äù the dying vHDD, then replace the HDD with a new one. <br>  Then prepare an RDM drive and add svm to the VM: <br><br><pre> <code class="bash hljs">  ,  X ‚Äî  SCSI  Virtual Device Node   vHDD: <span class="hljs-comment"><span class="hljs-comment"># echo "- - -" &gt; /sys/class/scsi_host/hostX/scan   sfdisk   : # sfdisk -d /dev/sdb | sfdisk /dev/sdc     ,      : # mdadm --manage /dev/md127 --add /dev/sdc1 # mdadm --manage /dev/md126 --add /dev/sdc2 # grub2-install /dev/sdc</span></span></code> </pre> <br><h3>  Emergency data access </h3><br>  One of the disks connects to the workstation, then you need to ‚Äúcollect‚Äù RAID, disable the cache and access the data by mounting the LVM volume: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># mdadm --assemble --scan # lvremove cl_svm/cache # lvchanange -ay /dev/cl_svm/data # mount /dev/cl_svm/data /mnt/data</span></span></code> </pre> <br>  I also tried to boot the system directly from the disk, set up the network, and on another host I connected an NFS datastor - VMs are available. <br><br><h3>  Summary </h3><br>  As a result, I use lvmcache in write-through mode and a section for a 60Gb cache.  Slightly sacrificing the host's CPU and RAM resources - instead of 210Gb is very fast and 1.3Tb of slow disk space, I got 680Gb of fast and 158Gb of very fast, but fault tolerance appeared (but if you unexpectedly fail the disk you will have to participate in the data access process). </div><p>Source: <a href="https://habr.com/ru/post/323824/">https://habr.com/ru/post/323824/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../323810/index.html">Third Annual Conference of JavaScript Developers in Odessa - JS Lab</a></li>
<li><a href="../323814/index.html">Splunk - a general description of the platform, the basic features of the installation and architecture</a></li>
<li><a href="../323818/index.html">Project Centennial: a bridge from Win32 and .NET to the Windows Store and UWP</a></li>
<li><a href="../323820/index.html">NeoQuest 2017: Getting out of the dodecahedron without launching anything in qemu</a></li>
<li><a href="../323822/index.html">Maternity leave in IT, or how not to lose yourself in the profession</a></li>
<li><a href="../323826/index.html">Data centers as art: data centers at the bottom of the ocean</a></li>
<li><a href="../323828/index.html">findNclean: find and clean</a></li>
<li><a href="../323830/index.html">[NeoQuest2017] "MULTIPSPORT!" And pink spacesuit</a></li>
<li><a href="../323832/index.html">From monoids to de Morgan algebras. We build abstractions on Haskell</a></li>
<li><a href="../323834/index.html">eMule on Android</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>