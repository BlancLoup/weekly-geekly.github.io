<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Grab: Spider parsing framework</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I am the author of the python library Grab, which makes it easy to write website parsers. I wrote about her introductory article some time ago on Habr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Grab: Spider parsing framework</h1><div class="post__text post__text-html js-mediator-article">  I am the author of the python library Grab, which makes it easy to write website parsers.  I wrote about her <a href="http://habrahabr.ru/blogs/python/127584/">introductory article</a> some time ago on Habr√©.  Recently, I decided to take up a lot of parsing, began to look for free-lance orders for parsing and I needed a tool for parsing sites with many pages. <br><br>  I used to implement multithreaded parsers using python threads with the help of <a href="https://bitbucket.org/lorien/grab/src/tip/grab/tools/work.py">such libraries</a> .  The threading approach has pros and cons.  The advantage is that we launch a separate thread (thread) and do what we want in it: we can make several network calls sequentially and all this within the same context - we don‚Äôt need to switch anywhere, remember something and remember it.  The downside is that threads slow down and eat memory. <br><br>  What are the alternatives? <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Work with network resources asynchronously.  There is only one program execution flow, in which all the data processing logic is executed as soon as this data is ready, the data itself is loaded asynchronously.  In practice, this allows not working very hard to work with a network of several hundred threads, if you try to run so many threads, they will seriously slow down. <br><br>  So, I wrote an interface to multicurl - this is part of the pycurl library, which allows you to work with the network asynchronously.  I chose multicurl because Grab uses pycurl and I thought that I could use it to work with multicurl.  So it happened.  I was even somewhat surprised that on the very first day of the experiments it worked :) The architecture of parsers based on Grab: Spider is very similar to parsers based on the scrapy framework, which, in general, is not surprising and logical. <br><br>  I will give an example of the simplest spider: <br><br><pre><code class="hljs pgsql"># coding: utf<span class="hljs-number"><span class="hljs-number">-8</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> grab.spider <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Spider, Task <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> SimpleSpider(Spider): initial_urls = [<span class="hljs-string"><span class="hljs-string">'http://ya.ru'</span></span>] def task_initial(self, grab, task): grab.set_input(<span class="hljs-string"><span class="hljs-string">'text'</span></span>, u<span class="hljs-string"><span class="hljs-string">''</span></span>) grab.submit(make_request=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) yield Task(<span class="hljs-string"><span class="hljs-string">'search'</span></span>, grab=grab) def task_search(self, grab, task): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> elem <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> grab.xpath_list(<span class="hljs-string"><span class="hljs-string">'//h2/a'</span></span>): print elem.text_content() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: bot = SimpleSpider() bot.run() print bot.render_stats()</code> </pre> <br><br>  What's going on here?  For each URL in `self.initial_urls`, a task with the name initial is created, after multicurl downloads the document, a handler with the name` task_initial` is called.  The most important thing is that inside the handler we get the Grab-object associated with the requested document: we can use practically any functions from the Grab API.  In this example, we use his work with forms.  Note that we need to specify the `make_request = False` parameter so that the form is not sent right there, because we want this network request to be processed asynchronously. <br><br>  In short, working with Grab: Spider comes down to generating requests using Task objects and further processing them in special methods.  Each task has a name; it is for him that the method is then selected to process the requested network document. <br><br>  You can create a Task object in two ways.  Easy way: <br><pre> <code class="hljs lisp">Task('foo', url='http<span class="hljs-symbol"><span class="hljs-symbol">://google</span></span>.com')</code> </pre><br><br>  After the document is completely downloaded from the network, a method with the name `task_foo` will be called. <br><br>  More complicated way: <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">g</span></span> = Grab() g.setup(....   ...) Task(<span class="hljs-string"><span class="hljs-string">'foo'</span></span>, grab=g)</code> </pre><br><br>  In this way we can adjust the request parameters in accordance with our needs, set cookies, special headers, generate a POST request, whatever. <br><br>  Where can I create queries?  In any handler method, you can make the yield Task object and it will be added to the asynchronous queue for downloading.  You can also return a Task object via return.  In addition, there are two more ways to generate Task objects. <br><br>  1) You can specify in the attribute `self.initial_urls` a list of addresses and jobs with the name 'initial' will be created for them. <br><br>  2) You can define the method `task_generator` and yield'it in it as many queries.  Moreover, new requests from it will be taken as the implementation of the old.  This allows for example, without problems, to iterate over a million lines from a file of a file and not to litter, oh sorry, litter, they all memory. <br><br>  Initially, I planned to do the processing of the extracted data as in scrapy.  There it is done with the help of Pipeline objects.  For example, you got a page with a movie, propars it and returned the Pipeline object with a Movie type.  And beforehand, you wrote in the config that the Movie Pipeline should be saved to a database or to a CSV file.  Something like this.  In practice, it turned out that it is easier not to bother with the additional wrapper and write data to the database or to the file immediately in the request handler method.  Of course, this will not work in the case of paralleling methods on a cloud of machines, but still need to live up to this point, but for now it is more convenient to do everything directly in the handler method. <br><br>  Task object can be passed additional arguments.  For example, we make a request in google search.  We form the desired url and create a Task object: Task ('search', url = '...', query = query) Next, in the `task_search` method we can find out exactly which query we were looking for by referring to the` task.query` attribute <br><br>  Grab: spider automatically tries to fix network errors.  In the case of network timeout, he performs the task again.  You can configure the number of attempts using the `network_try_limit` option when creating a Spider object. <br><br>  I must say that I really liked writing parsers in asynchronous style.  And the point is not only that the asynchronous approach loads the system resources less, but also that the source code of the parser acquires a clear and understandable structure. <br><br>  Unfortunately, to thoroughly describe the work of the Spider module will take a long time.  I just wanted to tell the army of users of the Grab library, which, I know, has several people, about one of the possibilities covered by the gloom of underdocumentation. <br><br>  Summary.  If you use Grab, have a look at the spider module, you might like it.  If you do not know what Grab is, perhaps you better look at the scrapy framework, it is documented a hundred times more beautiful than Grab. <br><br>  PS I use mongodb to store the results of the parsing - it‚Äôs just awesome :) Just do not forget to install a 64bit system, otherwise you will not be able to create more than two gigabytes of database. <br><br>  PS Example of a real parser for parsing the site <a href="http://dumpz.org/119395/">dumpz.org/119395</a> <br><br>  PS Official site of the project <a href="http://grablib.org/">grablib.org</a> (there are links to the repository, google group and documentation) <br><br>  PS I write to order parsers based on Grab, details here <a href="https://grablab.org/">grablab.org</a> </div><p>Source: <a href="https://habr.com/ru/post/134918/">https://habr.com/ru/post/134918/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../134910/index.html">Java objects</a></li>
<li><a href="../134911/index.html">Asterisk 10.0.0 Is Released!</a></li>
<li><a href="../134913/index.html">There is such a profession</a></li>
<li><a href="../134916/index.html">PHP autodocumentation in NetBeans 7.01 using phpDocumentor, telling, configuring, fixing</a></li>
<li><a href="../134917/index.html">What is the preferred option for navigating the large table?</a></li>
<li><a href="../134920/index.html">Creating your own CheckBoxList control</a></li>
<li><a href="../134922/index.html">2GIS updates the online version of the directory</a></li>
<li><a href="../134923/index.html">2GIS launches API cards</a></li>
<li><a href="../134924/index.html">Bacula: realtime filesets on Windows clients</a></li>
<li><a href="../134925/index.html">Android infringes a patent on ‚Äúretrieving a phone number from a message for use in another application‚Äù!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>