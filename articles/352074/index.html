<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>A tale about how an HTTP / 2 Client engineer overclocked</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Using the example of ‚ÄúJEP 110: HTTP / 2 Client‚Äù (which will appear in the JDK in the future), Sergey Kuksenko from Oracle shows how the team started i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>A tale about how an HTTP / 2 Client engineer overclocked</h1><div class="post__text post__text-html js-mediator-article">  Using the example of ‚ÄúJEP 110: HTTP / 2 Client‚Äù (which will appear in the JDK in the future), Sergey Kuksenko from Oracle shows how the team started it, where it looked and what it did to make it faster. <br><br>  We offer you a decryption of his report with JPoint 2017. In general, it will not be about HTTP / 2.  Although, of course, it will not be possible to do without a number of details on it. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/MvJJgR6nxXQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br><h2>  HTTP / 2 (aka RFC 7540) </h2><br>  HTTP 2 is a new standard designed to replace outdated HTTP 1.1.  How does the implementation of HTTP 2 differ from the previous version in terms of performance? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The key thing about HTTP 2 is that we have one single TCP connection.  Data streams are cut into frames, and all these frames are sent through this connection. <br><br>  A separate header compression standard is also provided - RFC 7541 (HPACK).  It works very well: it allows you to shrink up to 20 bytes of HTTP header that is in the order of kilobytes.  For some of our optimizations, this is important. <br><br>  In general, the new version has a lot of interesting things - prioritization of requests, server push (when the server itself sends data to the client) and so on.  However, in the context of this narrative (in terms of performance), this is not important.  In addition, many things remain the same.  For example, what the HTTP protocol looks like from above: we have the same GET and POST methods, the same values ‚Äã‚Äãof the HTTP header fields, status codes and the ‚Äúrequest -&gt; response -&gt; final response‚Äù structure.  In fact, if you look closely, HTTP 2 is just a low-level transport under HTTP 1.1, which removes its flaws. <br><br><h2>  HTTP API (aka JEP 110, HttpClient) </h2><br>  We have an HttpClient project called JEP 110. It is almost included in JDK 9. Initially, this client wanted to be made part of the JDK 9 standard, but some controversy arose at the level of the API implementation.  And since we do not have time to finalize the HTTP API by the release of JDK 9, we decided to make it so that we can show it to the community and discuss it. <br><br>  A new incubator module (Incubator Modules aka JEP-11) appears in JDK 9.  This is a sandbox, where in order to get feedback from the community, new APIs will be formed, which are not yet standardized, but, by definition, the incubator will be standardized to the next version or removed altogether ("It is expected that the API will either be standardized or removed.  All who are interested can get acquainted with the API and send your feedback.  Perhaps the next version - JDK 10 - where it will become the standard, everything will be fixed. <br><br><ul><li>  module: jdk.incubator.httpclient <br></li><li>  package: jdk.incubator.http <br></li></ul><br>  HttpClient is the first module in the incubator.  Subsequently, other things related to the customer will appear in the incubator. <br><br>  I'll tell you literally a couple of examples about the API (this is exactly the client API that allows you to make a request).  Main classes: <br><br><ul><li>  HttpClient (its Builder); <br></li><li>  HttpRequest (its Builder); <br></li><li>  HttpResponse, which we do not build, but just get back. <br></li></ul><br>  This is the simple way to build a query: <br><br><pre><code class="java hljs">HttpRequest getRequest = HttpRequest .newBuilder(URI.create(<span class="hljs-string"><span class="hljs-string">"https://jpoint.ru/"</span></span>)) .header(<span class="hljs-string"><span class="hljs-string">"X-header"</span></span>, <span class="hljs-string"><span class="hljs-string">"value"</span></span>) .GET() .build();</code> </pre> <br><pre> <code class="java hljs">HttpRequest postRequest = HttpRequest .newBuilder(URI.create(<span class="hljs-string"><span class="hljs-string">"https://jpoint.ru/"</span></span>)) .POST(fromFile(Paths.get(<span class="hljs-string"><span class="hljs-string">"/abstract.txt"</span></span>))) .build();</code> </pre><br>  Here we specify the URL, set the header, etc.  - we receive request. <br>  How can I send a request?  For the client, there are two kinds of APIs.  The first is a synchronous request when we block at the place of this call. <br><br><pre> <code class="java hljs">HttpClient client = HttpClient.newHttpClient(); HttpRequest request = ...; HttpResponse response = <span class="hljs-comment"><span class="hljs-comment">// synchronous/blocking client.send(request, BodyHandler.asString()); if (response.statusCode() == 200) { String body = response.body(); ... } ...</span></span></code> </pre><br>  The request is gone, we received a response, interpreted it as a <code>string</code> (we can have a different handler here ‚Äî <code>string</code> , <code>byte</code> , you can write your own) and processed it. <br><br>  The second is the asynchronous API, when we do not want to block in this place and, sending an asynchronous request, we continue execution, and with the received CompletableFuture we can then do whatever we want: <br><br><pre> <code class="java hljs">HttpClient client = HttpClient.newHttpClient(); HttpRequest request = ...; CompletableFuture&gt; responseFuture = <span class="hljs-comment"><span class="hljs-comment">// asynchronous client.sendAsync(request, BodyHandler.asString()); ...</span></span></code> </pre><br>  The client can be set to one thousand and one configuration parameters, differently configured: <br><br><pre> <code class="java hljs">HttpClient client = HttpClient.newBuilder() .authenticator(someAuthenticator) .sslContext(someSSLContext) .sslParameters(someSSLParameters) .proxy(someProxySelector) .executor(someExecutorService) .followRedirects(HttpClient.Redirect.ALWAYS) .cookieManager(someCookieManager) .version(HttpClient.Version.HTTP_2)&lt;/b&gt; .build();</code> </pre><br>  The main feature here is that the client API is universal.  It works with both old HTTP 1.1 and HTTP 2 without discerning the details.  For the client, you can specify the default operation with the HTTP 2 standard. The same parameter can be specified for each individual request. <br><br><h2>  Formulation of the problem </h2><br>  So, we have a Java library - a separate module that is based on standard JDK classes, and which we need to optimize (to do some kind of performance work).  Formally, the task of the performance is as follows: we must get a reasonable client performance for an acceptable time spent by the engineer. <br><br><h2>  Choosing an approach </h2><br>  How can we start this work? <br><br><ul><li>  We can sit down to read the HTTP 2 specification. This is useful. <br></li><li>  We can begin to study the client and rewrite the govnokod that we find. <br></li><li>  We can just look at this client and rewrite it entirely. <br></li><li>  We can spoil. <br></li></ul><br>  Let's start with benchmarking.  Suddenly everything is so good there - you don't have to read the specification. <br><br><h2>  Benchmarks </h2><br>  They wrote a benchmark.  Well, if we have a competitor for comparison.  I took Jetty Client as a competitor.  I screwed Jetty Server on the side - just because I wanted the server to be in Java.  Wrote GET and POST requests of different sizes. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4fb/814/28f/4fb81428f68b493fc25955d323a4b9d5.png"><br><br>  Naturally, the question arises - what do we measure: throughput, latency (minimal, medium).  During the discussion, we decided that this is not a server, but a client.  This means that taking into account the minimum latency, gc-pauses and everything else in this context is not important.  Therefore, specifically for this work, we decided to limit ourselves to measuring the overall system throughput.  Our task is to increase it. <br><br>  The overall system throughput is the inverse of the average latency.  That is, we worked on the average latency, but it did not strain with each individual request.  Just because the client does not have such requirements as the server. <br><br><h2>  Alteration 1. TCP configuration </h2><br>  We launch GET on 1 byte.  Iron written out.  We get: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d0/f8b/411/1d0f8b41167ace7f9e688d6f4f89913e.png"><br><br>  I take the same benchmark for HTTPClient, I run on other operating systems and hardware (this is more or less server-side machines).  I receive: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e16/977/7bc/e169777bcb45068c759dde569c314cfb.png"><br><br>  In Win64, everything looks better.  But even in MacOS, things are not as bad as in Linux. <br><br>  The problem is here: <br><br><pre> <code class="java hljs">SocketChannel chan; ... <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { chan = SocketChannel.open(); <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> bufsize = client.getReceiveBufferSize(); chan.setOption(StandardSocketOptions.SO_RCVBUF, bufsize); } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> InternalError(e); }</code> </pre><br>  This is the opening of SocketChannel to connect to the server.  The problem is the lack of a single line (I highlighted it in the code below): <br><br><pre> <code class="java hljs">SocketChannel chan; ... <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { chan = SocketChannel.open(); <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> bufsize = client.getReceiveBufferSize(); chan.setOption(StandardSocketOptions.SO_RCVBUF, bufsize); chan.setOption(StandardSocketOptions.TCP_NODELAY, <span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); &lt;-- !!! } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { <span class="hljs-keyword"><span class="hljs-keyword">throw</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> InternalError(e); }</code> </pre><br>  <code>TCP_NODELAY</code> is "hello" from the last century.  There are various TCP stack algorithms.  In this context, there are two: Nagle's Algorithm and Delayed ACK.  Under some conditions, they are able to kleshitsya, causing a sharp slowdown in data transfer.  This is such a known issue for the TCP stack that people turn on <code>TCP_NODELAY</code> , which turns off Nagle's Algorithm, by default.  But sometimes even an expert (real TCP experts wrote this code) can simply forget about it and not enter this command line. <br><br>  In principle, there are a lot of explanations on the Internet about how these two algorithms conflict and why they create such a problem.  I quote a link to one article I liked: <a href="http://www.stuartcheshire.org/papers/NagleDelayedAck/">TCP Performance problems caused by the interaction between Nagle's Algorithm and Delayed ACK</a> <br><br>  A detailed description of this problem is beyond the scope of our conversation. <br><br>  After a single line was added with the inclusion of <code>TCP_NODELAY</code> , we received about the following performance gain: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f2f/22f/bbf/f2f22fbbf768b1c044014032e81412d3.png"><br><br>  I will not take it as a percentage. <br><br>  <b>Moral:</b> this is not a Java problem, this is a problem with the TCP stack and its configuration issues.  For many areas, there are well-known shoals.  So well known that people forget about them.  It is desirable to simply know about them.  If you are new to this area, you can easily gossip the basic shoals that exist.  You can check them very quickly and without any problems. <br><br>  <b>You need to know (and do not forget) a list of well-known schools for your subject area.</b> <br><br><h2>  Alteration 2. Flow-control window </h2><br>  We have the first change, and I didn‚Äôt even have to read the specification.  It turned out 9600 requests per second, but remember that Jetty gives 11 thousand. Further we profile with the help of any profiler. <br><br>  Here is what I got: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/613/9d8/841/6139d88412aa599161a10100f5bd43fb.png"><br><br>  And this is a filtered version: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/783/be2/161/783be21615c632af6c5065738b18ec65.png"><br><br>  My benchmark takes up 93% of the CPU time. <br><br>  Sending a request to the server takes 37%.  Next comes any internal detailing, working with frames, and at the end of 19% this is an entry in our SocketChannel.  We transfer the data and header of the request, as it should be in HTTP.  And then we read - <code>readBody()</code> . <br><br>  Next, we need to read the data that came to us from the server.  What then is it? <br><br><img src="https://habrastorage.org/getpro/habr/post_images/697/b54/581/697b54581454c391232a5666fec8bbd0.png"><br><br>  If the engineers correctly named the methods, and I trust them, then here they send something to the server, and this takes as much time as sending our requests.  Why do we send something when reading the server response? <br><br>  To answer this question, I had to read the specification. <br><br>  In general, a lot of performance problems are solved without knowing the specification.  Somewhere you need to replace <code>ArrayList</code> with <code>LinkedList</code> or vice versa, or <code>Integer</code> with <code>int</code> and so on.  And in this sense it is very good if there is a benchmark.  Measure - fix - work.  And you do not go into details, how it works there according to the specification. <br><br>  But in our case, the problem really showed up in the specification: in the HTTP 2 standard there is a so-called flow-control.  It works as follows.  We have two feasts: one sends data, the other receives.  The sender (sender) has a window - a flow-control window the size of a number of bytes (suppose 16 KB). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/69b/e76/313/69be763133d5c1598626a4c6d9d9c53f.png"><br><br>  Let's say we sent 8K.  The flow-control window is reduced by these 8 KB. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/14a/6a2/bfe/14a6a2bfe163a7a69fbe2574ae755014.png"><br><br>  After we sent another 8 KB, the flow-control window was 0 KB. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ef7/c2f/a2f/ef7c2fa2f656f2a20c8af2660ea96542.png"><br><br>  According to the standard in such a situation, we have no right to send anything.  If we try to send some data, the recipient will be obliged to interpret this situation as a protocol error and close the connection.  This is a kind of protection from DDOS, in some cases, so that we are not sent anything superfluous, and the sender adjusts to the capacity of the recipient. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3e7/1b6/d41/3e71b6d41b27864c70acc0f7dbf247a8.png"><br><br>  When the receiver processed the received data, it had to send a special dedicated signal called WindowUpdate indicating how many bytes to increase the flow-control window. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d62/cd5/9cb/d62cd59cb5ca4482a52c33c3ff309622.png"><br><br>  When the WindowUpdate arrives to the sender, its flow-control window increases, we can send data further. <br><br>  What is going on in the client? <br>  We received data from the server - this is the real piece of processing: <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">// process incoming data frames ... DataFrame dataFrame; do { DataFrame dataFrame = inputQueue.take(); ... int len = dataFrame.getDataLength(); sendWindowUpdate(0, len); // update connection window sendWindowUpdate(streamid, len); //update stream window } while (!dataFrame.getFlag(END_STREAM)); ...</span></span></code> </pre><br>  A certain <code>dataFrame</code> came - a data frame.  We looked at how much data there was, processed it, and sent back WindowUpdate to increase the flow-control window to the desired value. <br><br>  In fact, in each such place two flow-control window works.  We have a flow-control window specifically for this data transfer stream (request), and we also have a general flow control window for the entire connection.  Therefore, we must send two WindowUpdate requests. <br><br>  How to optimize this situation? <br><br>  The first.  At the end of the <code>while</code> , we have a checkbox that says that the last data frame was sent to us.  According to the standard, this means that no other data will come.  And we do this: <br><br><pre> <code class="java hljs"><span class="hljs-comment"><span class="hljs-comment">// process incoming data frames ... DataFrame dataFrame; do { DataFrame dataFrame = inputQueue.take(); ‚Ä¶ int len = dataFrame.getDataLength(); connectionWindowUpdater.update(len); if (dataFrame.getFlag(END_STREAM)) { break; } streamWindowUpdater.update(len); } while (true); ...</span></span></code> </pre><br>  This is a small optimization: if we catch the end-of-stream flag, then WindowUpdate can no longer be sent for this stream: we don‚Äôt wait for any data anymore, the server will not send anything. <br><br>  The second.  Who said we should send WindowUpdate every time?  Why can't we, having received many requests, process the incoming data and only then send a WindowUpdate package for all incoming requests? <br><br>  Here is <code>WindowUpdater</code> , which works for a specific flow-control window: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">final</span></span> AtomicInteger received; <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> threshold; ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">update</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> delta)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (received.addAndGet(delta) &gt; threshold) { <span class="hljs-keyword"><span class="hljs-keyword">synchronized</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> tosend = received.get(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>( tosend &gt; threshold) { received.getAndAdd(-tosend); sendWindowUpdate(tosend); } } } }</code> </pre><br>  We have a certain <code>threshold</code> .  We receive data, do not send anything.  Once we have typed the data up to this <code>threshold</code> , we send the entire WindowUpdate.  There is a kind of heuristics that works well when the <code>threshold</code> value is close to half of the flow-control window.  If we had this window initially 64 KB, and we received 8 KB each, then as soon as we received several data frames with a total volume of 32 KB, we send the window updater immediately to 32 KB.  Normal batch processing.  For good synchronization, we are also doing a completely normal double check. <br><br>  For a request of 1 byte, we get: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c16/651/403/c166514031ed42264f1578c43415db2b.png"><br><br>  The effect will be even for megabyte requests, where there are a lot of frames.  But he, naturally, is not so noticeable.  In practice, I had different benchmarks, requests of different sizes.  But here for each case I didn‚Äôt draw graphics, but picked up simple examples.  Squeeze more detailed data will be a little later. <br><br>  We received only + 23%, but the Jetty has already overtaken. <br><br>  Moral: <b>accurate reading specifications and logic are your friends.</b> <br><br>  There is a nuance of specification.  There, on the one hand, it says that when we receive a data frame, we must send WindowUpdate.  But, having carefully read the specification, we will see: there is no requirement that we are obliged to send WindowUpdate to every received byte.  Therefore, the specification allows such a batch update to the flow-control window. <br><br><h2>  Alteration 3. Locks </h2><br>  Let's explore how we scale (scale). <br><br>  The laptop is not very suitable for scaling - it has only two real and two fake cores.  We will take some server machine, in which 48 hardware threads, and launch the benchmark. <br><br>  Here, horizontally is the number of threads, and vertically shows the total throughput. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c67/6a1/16e/c676a116e6c368d8e654b6f77f16052f.png"><br><br>  Here you can see that up to four threads we scale very well.  But further, the scalability becomes very bad. <br><br>  It would seem, why do we need it?  We have one customer;  we will get the necessary data from the server from one thread and forget about it.  But first, we have an asynchronous version of the API.  To her we will come.  There certainly will be some threads.  Secondly, in our world now everything is multi-core, and to be able to work well with many threads in our library is simply useful - if only because when someone starts complaining about the performance of the single-threaded version, he can be advised to switch to multi-threaded and get a benefit.  Therefore, let's look for the culprit in bad scalability.  I usually do it like this: <br><br><pre> <code class="java hljs">#!/bin/bash (java -jar benchmarks.jar BenchHttpGet.get -t <span class="hljs-number"><span class="hljs-number">4</span></span> -f <span class="hljs-number"><span class="hljs-number">0</span></span> &amp;&gt; log.log) &amp; JPID=$! sleep <span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> kill -<span class="hljs-number"><span class="hljs-number">3</span></span> $JPID; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> : done</code> </pre><br>  I just write the file to file.  In reality, this is enough for me in 90% of cases when I work with locks without any profilers.  Only in some complicated trick cases I launch the Mission control or something else and watch the allocation of locks. <br><br>  In the log you can see in what state I have different threads: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/570/f55/1df/570f551dfbb9d78908d5c7a8543aa8d2.png"><br><br>  Here we are interested in precisely blocking, rather than waiting, when we expect events.  There are 30 thousand locks, which is quite a lot against 200 thousand runnable ones. <br><br>  But such a command line will simply show us the culprit (nothing extra is needed - just the command line): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42e/8f1/649/42e8f16494c0f66a764dde1251989b18.png"><br><br>  The culprit is caught.  This is a method inside our library that sends a data frame to the server.  Let's figure it out. <br><br><pre> <code class="java hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sendFrame</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Http2Frame frame)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">synchronized</span></span> (sendlock) { <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (frame <span class="hljs-keyword"><span class="hljs-keyword">instanceof</span></span> OutgoingHeaders) { OutgoingHeaders oh = (OutgoingHeaders) frame; Stream stream = registerNewStream(oh); List frames = encodeHeaders(oh, stream); writeBuffers(encodeFrames(frames)); } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { writeBuffers(encodeFrame(frame)); } } <span class="hljs-keyword"><span class="hljs-keyword">catch</span></span> (IOException e) { ... } } }</code> </pre><br>  Here we have a global monitor: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6bc/af4/efd/6bcaf4efd41f05a5d120f473d07154db.png"><br><br>  But this thread - <img src="https://habrastorage.org/getpro/habr/post_images/8cb/f68/12b/8cbf6812be9aef9351a343c6bd1ff8ea.png"><br><br>  - the beginning of the initiation of the request.  This is the sending of the very first header to the server (some additional actions are required here, I will talk about them now). <br><br>  This is sending all other frames to the server: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6cb/263/c8f/6cb263c8fcee6ce96da4c191839a2f07.png"><br><br>  All this under a global lock! <br><br>  <code>sendFrame</code> itself <code>sendFrame</code> us an average of 55% of the time. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/82f/454/2f2/82f4542f2063e6945f878d95f9fedaa4.png"><br><br>  But this method takes 1%: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/716/6c5/28f/7166c528fb14f376f4554454e67c55ea.png"><br><br>  Let's try to understand what can be taken out from under the global lock. <br><br>  Registration of a new stream from under the lock can not be made.  The HTTP standard imposes a restriction on the numbering of streams.  In <code>registerNewStream</code> new stream gets a number.  And if I sent streams with numbers 15, 17, 19, 21 and sent 21 and then 15 to transfer my data, it will be a protocol error.  I have to send them in ascending order.  If I take them out of the lock, they may not be sent in the order in which I am waiting. <br><br>  The second problem, which is not removed from the lock: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dff/ef4/8c2/dffef48c2c1f120711702e22469025dc.png"><br><br>  Here is the header compression. <br><br>  In its usual form, our header is put in the usual map - key value (from string to string).  In <code>encodeHeaders</code> header compression occurs.  And here the second rake of the HTTP 2 standard is the HPACK algorithm, which works with compression, statefull.  Those.  he has a state (therefore, compresses very well).  If I send two requests (two header-a), while at first I squeezed one, then the second, then the server must receive them in the same order.  If he receives them in a different order, he will not be able to decode.  This problem is the serialization point.  All coding of all HTTP requests must pass through a single point of serialization, they cannot work in parallel, and even after that, the encoded frames must be sent. <br><br>  The <code>encodeFrame</code> method takes 6% of the time, and it can theoretically be taken out from under the lock. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/af1/f85/9dc/af1f859dcc7842206ad5cdf8bdb209fc.png"><br><br>  <code>encodeFrames</code> drops the frame into the byte buffer in the form in which it is defined by the specification (before that, we prepared the internal structure of the frames).  It takes 6% of the time. <br><br>  Nothing prevents us from <code>encodeFrames</code> out <code>encodeFrames</code> from blocking, except for the method where the actual recording to the socket occurs: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bf7/fd7/9c6/bf7fd79c67c3577b40ecddada70224b1.png"><br><br>  There are some implementation nuances. <br><br>  So it turned out that <code>encodeFrames</code> can encode a frame not into one, but into several byte buffers.  This is primarily due to efficiency (so as not to make too much copying). <br><br>  If we try to take <code>writeBuffers</code> out of the lock, and <code>writeBuffers</code> from the two frames are mixed up, we will not be able to decode the frame.  Those.  we must provide some kind of atomicity.  At the same time <code>writeBuffers</code> is executed inside <code>writeBuffers</code> , and there stands its own global lock on writing to the socket. <br><br>  Let's do the first thing that comes to mind - the queue queue.  We will put the byte buffer in this queue in the correct order, and let another thread read from it. <br><br>  In this case, the <code>writeBuffers</code> method generally ‚Äúleaves‚Äù this thread.  There is no need to keep it under this lock (it has its own global lock).  The main thing for us is to ensure the order of byte-buffers that arrive there. <br><br>  So, we removed one of the most difficult operations outside and launched an additional thread.  The size of the critical section was reduced by 60%. <br><br>  But the implementation has its drawbacks: <br><br><ul><li>  For some frames in the HTTP 2 standard, there is a limit in order.  But other frames on the specification can be sent earlier.  I can send the same WindowUpdate earlier than others.  And I would like to do this, because the server is worth it - it is waiting (it has flow-control window = 0).  However, the implementation does not allow this; <br></li><li>  The second problem is that when our queue is empty, the sending thread goes to sleep and wakes up for a long time. <br></li></ul><br>  Let's solve the first problem with the frame order. <br><br>  An obvious idea - <code>Deque&lt;ByteBuffer[]&gt;</code> . <br><br>  We have an inseparable piece of byte buffers that cannot be mixed with anything;  we add it into an array, and the array itself into a queue.  Then these arrays can be intermixed with each other, and where we need a fixed order, we provide it: <br><br><ul><li>  ByteBuffer [] - atomic buffer sequence; <br></li><li>  WindowUpdateFrame - we can put it at the head of the queue and remove it from the blocking at all (it has neither protocol coding nor numbering); <br></li><li>  DataFrame - can also be removed from the lock and put in the end of the queue.  As a result, the lock is becoming less and less. <br></li></ul><br>  Pros: <br><br><ul><li>  fewer locks; <br></li><li>  sending window Update early allows the server to send data earlier. <br></li></ul><br>  But here there is one more minus.  Still, the sending stream often falls asleep and wakes up for a long time. <br><br>  Let's do this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/66e/639/ac5/66e639ac51076249bb4234e7b732cd47.png"><br><br>  We will have a little turn of our own.  In it, we add the resulting arrays of byte-buffers.  After that, between all threads that came out from under the lock, we will arrange a competition.  Who won, let him write to the socket.  And let the rest work on. <br><br>  It should be noted that another optimization turned out to be in the <code>flush()</code> method, which has the effect: if I have a lot of small data (for example, 10 arrays of three to four buffers) and an encrypted SSL connection, it can take more than one array from the queue , and larger chunks, and send them to SSLEngine.  In this case, the costs of coding are dramatically reduced. <br><br>  Each of the three optimizations presented allowed us to very well remove the problem with scaling.  Something like this (the overall effect is reflected): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a4b/cde/e72/a4bcdee724a2c0f4294240957740ce49.png"><br><br>  Moral: <b>Locks - evil!</b> <br><br>  Everyone knows that you need to get rid of locks.  Moreover, the concurrent library is becoming more advanced and interesting. <br><br><h2>  Alteration 4. Pool or GC? </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In theory, we have an HTTP Client designed for 100% use of ByteBufferPool. </font><font style="vertical-align: inherit;">But in practice ... Immediately, the bugs, here - something fell, there - the frame was underworked ... And if ByteBuffer did not return the pool back, the functionality did not break ... In general, the engineers had no time to deal with it. </font><font style="vertical-align: inherit;">And we got an unfinished version, sharpened into pools. </font><font style="vertical-align: inherit;">We have (and cry):</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> only 20% of buffers are returned to the pool; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ByteBufferPool.getBuffer () takes 12% of the time. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We get all the disadvantages of working with pools, and at the same time - all the disadvantages of working without pools. </font><font style="vertical-align: inherit;">There are no pluses in this version. </font><font style="vertical-align: inherit;">We need to move forward: either to make a normal full-fledged pool so that all ByteBuffers return to it after use, or even to cut the pools, but at the same time we even have them in the public API. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What do people think about pools? </font><font style="vertical-align: inherit;">Here is what you can hear:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No pool needed, pools are generally harmful! </font><font style="vertical-align: inherit;">eg Dr. </font><font style="vertical-align: inherit;">Cliff Click, Brian Goetz, Sergey Kuksenko, Aleksey Shipil¬®ev, ...</font></font><br></li></ul><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">some claim that the pool is cool and has an effect. </font><font style="vertical-align: inherit;">Poole needed! </font><font style="vertical-align: inherit;">eg Netty (blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead), ...</font></font><br></li></ul><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> DirectByteBuffer or HeapByteBuffer </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Before returning to the question of pools, we need to solve a sub-question - what do we use in our problem with the HTTPClient: DirectByteBuffer or HeapByteBuffer? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First, we study the question theoretically:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DirectByteBuffer is better for I / O. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">sun.nio. * copies the HeapByteBuffer to the DirectByteBuffer;</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HeapByteBuffer is better for SSL. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSLEngine works directly with byte [] in the case of HeapByteBuffer.</font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Indeed, for transferring data to the socket DirectByteBuffer is better. </font><font style="vertical-align: inherit;">Because if we follow the Write-s chain to nio, we will see the code where from HeapByteBuffer everything is copied into the internal DirectByteBuffer. </font><font style="vertical-align: inherit;">And if we have a DirectByteBuffer, we do not copy anything. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But we have another thing - an SSL connection. </font><font style="vertical-align: inherit;">The standard HTTP 2 allows you to work with both plain connection and SSL connection, but it is declared that SSL should be the de facto standard for the new web. </font><font style="vertical-align: inherit;">If we follow the chain of how OpenJDK implements it in the same way, it turns out that theoretically SSLEngine works better with HeapByteBuffer, because it can reach the byte [] array and encrypt it. </font><font style="vertical-align: inherit;">And in the case of DirectByteBuffer, he must first copy here, and then back.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And measurements show that HeapByteBuffer is always faster: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PlainConnection - HeapByteBuffer is ‚Äúfaster‚Äù by 0% -1% - I put in quotes, because 0-1% is not faster. </font><font style="vertical-align: inherit;">But there is no benefit from using DirectByteBuffer, and there are more problems;</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> SSLConnection - HeapByteBuffer is 2% -3% faster </font></font><br></li></ul><br>  Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">HeapByteBuffer is our choice! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Oddly enough, reading and copying from DirectByteBuffer is more expensive, because checks remain there. </font><font style="vertical-align: inherit;">The code there is not very well vectorized, since it works through unsafe. </font><font style="vertical-align: inherit;">And in HeapByteBuffer - intrinsic (not even vectorization). </font><font style="vertical-align: inherit;">And soon it will work even better. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, even if HeapByteBuffer were 2-3% slower than DirectByteBuffer, it might not make sense to do DirectByteBuffer. </font><font style="vertical-align: inherit;">So let's get rid of the problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's make various options.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Option 1: All in pool </font></font></h2><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We write a normal pool. </font><font style="vertical-align: inherit;">We clearly monitor the life paths of all the buffers so that they return to the pool.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We optimize the pool itself (based on the ConcurrentLinkedQueue). </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Separate pools (by buffer size). </font><font style="vertical-align: inherit;">The question arises, what size should be the buffer. </font><font style="vertical-align: inherit;">I read that Jetty made a universal ByteBufferPool, which allows you to work with byte buffers of various sizes with a granularity of 1 KB. </font><font style="vertical-align: inherit;">We just need three different ByteBufferPool, each working with its own size. </font><font style="vertical-align: inherit;">And if the pool works with buffers of only one size, everything becomes much simpler:</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> SSL packets (SSLSession.getPacketBufferSize ()); </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> header encoding (MAX_FRAME_SIZE); </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> all the rest. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Advantages of option 1: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> less ‚Äúallocation pressure‚Äù </font></font><br></li></ul><br>  Minuses: <br><br><ul><li>   .         ?   ,  ByteBuffer  -,       ,    ,   .     ,      -.     .     ,    ; <br></li><li>   ¬´ ¬ª; <br></li><li>    (   ); <br></li><li>   ,   : <br></li><li>    <code>ByteBuffer.slice()</code>  <code>ByteBuffer.wrap()</code> .     ByteBuffer,     - ,     ,   slice(). slice()   .   ,      .   ,     .      ,      -  . ,      128 ,    -,   128 ,           .  ,    .     ‚Äî  -.         - .       ,     . ,          . <br></li></ul><br><h2>  2:   ‚Äî   GC </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> GC will do all the work, especially since we do not have DirectByteBuffer, but HeapByteBuffer. </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we remove all pools, including those from the Public API, because in reality they do not carry any functionality, except for some internal technical implementation. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">well, naturally, since GC now collects everything from us, we don‚Äôt need to copy the data - we actively use </font></font><code>ByteBuffer.slice()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">/ </font></font><code>wrap()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- we cut and wrap buffers.</font></font><br></li></ul><br>  Pros: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the code has really become easier to understand; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no pools in the public API; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> we have a good ‚Äúdata locality‚Äù; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> a significant reduction in copying costs; everything works this way; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no pool costs. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> But two problems: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> First, the allocation of data is above ‚Äúallocation pressure‚Äù </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and the second problem is that we often don‚Äôt know which buffer we need. </font><font style="vertical-align: inherit;">We read from the network, from I / O, from the socket, we allocate a buffer of 32 KB, well, even if it is 16 KB. </font><font style="vertical-align: inherit;">And from the network read 12 bytes. </font><font style="vertical-align: inherit;">And what do we have to do with this buffer? </font><font style="vertical-align: inherit;">Only throw out. </font><font style="vertical-align: inherit;">We get an inefficient use of memory (when the required buffer size is unknown) - for the sake of 12 Byte, 16 KB was allocated.</font></font><br></li></ul><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Option 3: Mix </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For the sake of experiment we make a mixed version. </font><font style="vertical-align: inherit;">I will tell about it in more detail. </font><font style="vertical-align: inherit;">Here we choose the approach depending on the data. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Outgoing data:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">user data. </font><font style="vertical-align: inherit;">We know their size, with the exception of coding in the HPACK algorithm, so we always allocate buffers of the right size ‚Äî we do not have memory efficiently. </font><font style="vertical-align: inherit;">We can do all sorts of cuts and wrapping without unnecessary copying - and let GC collect.</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> for compressing HTTP headers - a separate pool, where the byte buffer comes from and then returns there. </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> all the rest is buffers of the required size (GC will collect) </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Incoming data: </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reading from socket - buffer from a pool of some normal size - 16 or 32 KB; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">send data (DataFrame) - </font></font><code>slice()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(GC collect);</font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> all the rest is returned to the pool. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, the standard HTTP 2 has nine types of frames. </font><font style="vertical-align: inherit;">If eight of them came (everything except data), then we decode the byte buffer in the same place and we do not need to copy anything from it, and we return the byte buffer back to the pool. </font><font style="vertical-align: inherit;">And if the data came, we perform a slice, so that we don‚Äôt have to copy anything, and then just throw it away - it will be assembled by the GC. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, a separate pool for encrypted SSL connection buffers, because there is a different size. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pluses of the mixed option:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the average complexity of the code (in something, but basically it is simpler than the first option with pools, because less needs to be tracked); </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no pools in the public API; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> good ‚Äúdata locality‚Äù; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> no copying costs; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> eligible pool costs; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> acceptable memory usage. </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Minus one: above ‚Äúallocation pressure‚Äù. </font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Comparison of options </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We made three options, checked, corrected bugs, achieved functional work. We measure. We look at the allocation of data. I had 32 measurement scenarios, but I did not want to draw 32 graphs here. I will show just the range averaged over all dimensions. Here, the baseline is the original unfinished code (I took it for 100%). We measured the change in the allocation rate with respect to the baseline in each of the three modifications. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c62/c4b/939/c62c4b939661212d99109bd8ba10a177.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The option where everything goes to the pool predictably loses less. A variant that does not require any pools allocates eight times more memory than a variant without pools. But do we really need memory for the allocation rate? Measuring GC-pause: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/76c/1a6/6b4/76c1a66b4dd85276a7814434b7448a8d.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With such GC-pauses at the allocation rate, this does not affect.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is seen that the first option (in the pool to the maximum) gives 25% acceleration. </font><font style="vertical-align: inherit;">Lack of pools to the maximum gives 27% of acceleration, and the mixed version gives a maximum of 36% of acceleration. </font><font style="vertical-align: inherit;">Any properly completed version already gives an increase in productivity. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a number of scenarios, the mixed version gives about 10% more than the option with pools or the option without any pools at all, so it was decided to stop there. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Moral: here I had to try various options, but there was no real need to totally work with pools by dragging them into the public API.</font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not focus on "urban legends"</font></font></b> <br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Know the opinions of authorities </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> But often "the truth is somewhere near" </font></font><br></li></ul><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Subtotals </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The above are four modifications that I wanted to talk about in terms of working with blocking calls. Then I will talk a little about something else, but first I want to make an intermediate cut. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is a comparison of HttpClient and JettyClient on different types of connections and data volumes. Bars are slices; the higher the faster. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/999/a53/94a/999a5394ad456f0fd7f820ae0c41d967.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For GET requests, we are well ahead of Jetty. I put a tick. We have an acceptable performance with an acceptable cost. In principle, you can still squeeze there, but you need to stop once, otherwise you will not see this HttpClient in Java 9 or Java 10. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With POST requests, things are not so optimistic. When sending big data in a PLAIN connection, the Jetty still wins a little bit. But when sending small data and with an SSL-encrypted connection, we also have no problems.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/778/7dc/9f5/7787dc9f501f2ac3b322414b4efabf06.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why do we not have data scaled when post size is large? </font><font style="vertical-align: inherit;">Here we run into two serialization problems: in the case of an SSL connection, this lock is a write to the socket ‚Äî it is global for writing to this particular SocketChannel. </font><font style="vertical-align: inherit;">We cannot write to the socket in parallel. </font><font style="vertical-align: inherit;">Although we are part of the JDK, the nio library for us is an external module where we cannot change anything. </font><font style="vertical-align: inherit;">So when we write a lot, we run into this bottleneck. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With SSL (with encryption) the same situation. </font><font style="vertical-align: inherit;">SSLEngine has encryption / decryption. </font><font style="vertical-align: inherit;">They can work in parallel. </font><font style="vertical-align: inherit;">But encryption is required to work consistently, even if I send data from many threads. </font><font style="vertical-align: inherit;">This is a feature of the SSL protocol. </font><font style="vertical-align: inherit;">And this is another serialization point. </font><font style="vertical-align: inherit;">Nothing can be done with this, unless you switch to some native OpenSSL standards.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Alteration 5. Asynchronous API </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's look at asynchronous requests. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Can we make such a completely simple version of the asynchronous API?</font></font><br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> &lt;T&gt; <span class="hljs-function"><span class="hljs-function">HttpResponse&lt;T&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">send</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(HttpRequest req, HttpResponse.BodyHandler&lt;T&gt; responseHandler)</span></span></span><span class="hljs-function"> </span></span>{ ... } <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> &lt;T&gt; CompletableFuture&lt;HttpResponse&lt;T&gt;&gt; sendAsync(HttpRequest req, HttpResponse.BodyHandler&lt;T&gt; responseHandler) { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> CompletableFuture.supplyAsync(() -&gt; send(req, responseHandler), executor); }</code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I gave my executor - here it is written out (executor is configured in the client; I have some default executor, but you, as a user of this client, can give any executor there). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alas, you can't just take and write an asynchronous API: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/728/61e/897/72861e8971fae8fbaebdf032ffd36c3e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The problem is that in blocking requests we often wait for something. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is a very exaggerated picture. </font><font style="vertical-align: inherit;">In reality, there is a query tree - waiting here, waiting there ... they are placed in different places.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/13f/434/07d/13f43407d2b137c5ed07e6e6b4592739.png"><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Step 1 - transition to CompletableFuture </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When we wait, we sit on wait or on condition. If we wait in the blocking API, and at the same time we put it in the Async executor, then we took the thread from the executor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">On the one hand, it is simply ineffective. On the other - we wrote an API that allows us to give our API any external executor. This, by definition, should work with a fixed thread pool (if a user can give any executor there, then we should be able to work in at least one thread). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In reality, this was a standard situation when all the threads from my executor were blocked. They are waiting for a response from the server, and the server is waiting and not sending anything until I also send something to it. I need to send something from the client, and I have no threads in the executor.</font></font> Everything.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We arrived. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is necessary to cut the entire chain of requests so that each waiting point is wrapped in a separate CompletableFuture.</font></font> Like that: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9a2/e70/134/9a2e701343e2d2ffe41ff449384ed15e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have a user thread on the left. </font><font style="vertical-align: inherit;">There we build a chain of queries. </font><font style="vertical-align: inherit;">Here the method thenCompose, in which one future came, came the second future. </font><font style="vertical-align: inherit;">On the other hand, we have a thread-thread selectormanager. </font><font style="vertical-align: inherit;">It was in the sequential version, it just did not have to be optimized. </font><font style="vertical-align: inherit;">It reads from the socket, decodes the frame and makes a complete.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When we come to thenCompose and see that we have a future that we are waiting for, has not yet been completed, we are not blocking (this is the asynchronous processing of the CompletableFuture), but leaving. </font><font style="vertical-align: inherit;">The thread will return to the executor, continue to work on something else that is required for this executor, and then we will go on this execution further. </font><font style="vertical-align: inherit;">This is a key feature CompletableFuture, which allows you to write such things effectively. </font><font style="vertical-align: inherit;">We do not steal the thread from work. </font><font style="vertical-align: inherit;">We always have someone to work with. </font><font style="vertical-align: inherit;">And it is more effective in terms of performance. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We cut out all the locks on condition or wait, go to CompletableFuture. </font><font style="vertical-align: inherit;">When the CompletableFuture is complete, then the thread is put to execution. </font><font style="vertical-align: inherit;">We get + 40% to the processing of asynchronous requests.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Step 2 - delayed launch </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have a very popular genre of puzzlers. I do not really like jigsawers, but I want to ask. Suppose we have two threads and there is a CompletableFuture. In one thread, we attach a chain of actions - thenSomething. By this ‚ÄúSomething‚Äù I mean Compose, Combine, Apply - any operations with CompletableFuture. And from the second thread we make the completion of this CompletableFuture. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The foo method is our action that should work - in which thread will it be executed? </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a7a/fa1/b39/a7afa1b39d5bcbf9ac60eb66d8e176af.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The correct answer is C.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we complete the chain - i.e. </font><font style="vertical-align: inherit;">If we call the thenSomething method and the CompletableFuture has already been completed by this time, the foo method will be called in the first thread. </font><font style="vertical-align: inherit;">And if the CompletableFuture has not yet been completed, it will be called from complete along the chain, i.e. </font><font style="vertical-align: inherit;">from the second thread. </font><font style="vertical-align: inherit;">With this key feature, we will now deal two times. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, we in the user code build a chain of requests. </font><font style="vertical-align: inherit;">We want the user to send me sendAsync.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I want in the user thread, where we do sendAsync, build a chain of requests and give the final CompletableFuture to the user. </font><font style="vertical-align: inherit;">And there in my executor my threads, data sending, waiting will go to work. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I twist and saw java code on localhost. </font><font style="vertical-align: inherit;">And what turns out: sometimes I don‚Äôt have time to complete the query chain, and the CompletableFuture is already completed:</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/f0c/fa8/730/f0cfa87307473cd0d4d8ce7c2a05515d.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I have only four hard work threads on this machine (there may be several dozen of them), and even then he doesn‚Äôt have time to complete it. </font><font style="vertical-align: inherit;">I measured it in 3% of cases. </font><font style="vertical-align: inherit;">An attempt to complete the query chain further leads to the fact that some actions on this chain, such as sending and receiving data, are invoked in a user course, although I do not want this. </font><font style="vertical-align: inherit;">Initially, I want this whole chain to be hidden, i.e. </font><font style="vertical-align: inherit;">the user should not see it. </font><font style="vertical-align: inherit;">I want it to work in the executor. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, we have methods that make Compose Async. </font><font style="vertical-align: inherit;">If instead of thenCompose I call the method </font></font><code>thenComposeAsync()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, I certainly will not translate my actions into the user stream. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pros of implementation:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nothing gets into the user thread; </font></font><br></li></ul><br>  Minuses: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">too frequent switching from one thread from executor to another thread from executor (expensive). </font><font style="vertical-align: inherit;">Nothing gets into user code, but methods </font></font><code>thenComposeAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, </font></font><code>thenApplyAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and generally any methods with an Async ending, switch the CompletableFuture execution to another thread from the same executor, even if we come from the thread of our executor (to Async) if it is fork-join by default or if it is explicitly set by executor. </font><font style="vertical-align: inherit;">However, if the CompletableFuture is already complete, what's the point of switching from this thread? </font><font style="vertical-align: inherit;">This switching from one thread to another is a waste of resources.</font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Here is such a trick was used: </font></font><br><br><pre> <code class="java hljs">CompletableFuture&lt;Void&gt; start = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CompletableFuture&lt;&gt;(); start.thenCompose(v -&gt; sendHeader()) .thenCompose(() -&gt; sendBody()) .thenCompose(() -&gt; getResponseHeader()) .thenCompose(() -&gt; getResponseBody()) ...; start.completeAsync( () -&gt; <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>, executor); <span class="hljs-comment"><span class="hljs-comment">// !!! trigger execution</span></span></code> </pre><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We first take an empty incomplete CompletableFuture, we build to it the whole chain of actions that we need to perform, and we will start the execution. </font><font style="vertical-align: inherit;">After that we will complete the CompletableFuture - we will do it </font></font><code>completeAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- with the transition to our executor immediately. </font><font style="vertical-align: inherit;">This will give us another 10% performance for asynchronous requests.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Step 3 - complete () tricks </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is another problem with the CompletableFuture: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/232/a3a/7fe/232a3a7fedb27da521ade8cc7fa9deae.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have a CompletableFuture and a dedicated SelectorManager stream that CompletableFuture completes. We can not write here </font></font><code>future.complete</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. The problem is that the SelectorManager thread is internal, it processes all reads from the socket. And we give it to the user CompletableFuture. And he can attach a chain of his actions to him. If we start the execution of user actions using response.complete on the SelectorManager, the user can kill us with our dedicated stream SelectorManager, which should be engaged in proper operation, and should not be superfluous there. We have to somehow translate the execution - take it from that stream and push it into our executor, where we have a bunch of threads. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is simply dangerous.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e9c/962/492/e9c96249224dc98a079e765c8cbb446b.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have </font></font><code>completeAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7e4/e5c/ab3/7e4e5cab39f09551b2c21a97f1a09b2e.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But by doing </font></font><code>completeAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, we get the same problem. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We very often have to switch the execution from thread to executor to another thread from the same executor along the chain. </font><font style="vertical-align: inherit;">But we do not want to do a switch from SelectorManager to executor or from some user thread to executor. </font><font style="vertical-align: inherit;">And inside the executor, we don't want our tasks to migrate. </font><font style="vertical-align: inherit;">Performance suffers from this. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We can not do </font></font><code>CompleteAsync</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">From that side, we can always make the transition to Async. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7f5/73a/b73/7f573ab73de83b07af521cfcc35694a9.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But here is the same problem. </font><font style="vertical-align: inherit;">In both cases we have secured our work, in our thread nothing will start, but this migration is expensive.</font></font><br><br>  Pros: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> nothing gets into the stream "SelectorManager" </font></font><br></li></ul><br>  Minuses: <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> frequent switching from one thread from executor to another thread from executor (expensive) </font></font><br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here's another trick: let's check, can we have the CompletableFuture already completed? </font><font style="vertical-align: inherit;">If the CompletableFuture is not complete yet, we‚Äôll go to Async. </font><font style="vertical-align: inherit;">And if it is completed, it means I know for sure that the construction of the chain to the already completed CompletableFuture will be executed in my thread, and I am already doing this executor in this thread. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/196/ddb/3d6/196ddb3d6ea35d7f785fa1382bfe95aa.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is purely optimization, which removes unnecessary things. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And it gives another 16% performance to asynchronous requests. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As a result, all three of these optimizations for CompletableFuture dispersed asynchronous requests by about 80%. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Moral: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learn new. </font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CompletableFuture ( </font></font><a href="https://habrahabr.ru/users/since/" class="user_link"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">since</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1.8)</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Rework 6 </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The last fix was never made in the code of the HTTP Client itself simply because it is associated with the Public API. But the problem can be circumvented. I will tell you about it. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/64d/c40/123/64dc401230db70d5c696864878f1a3dd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So, we have a client builder, we can give him an executor. If we did not give it an executor when creating the HTTP Client, it says that it is used by default </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's see what it is </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. I specifically emphasized what is interesting: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/195/2d5/4aa/1952d54aa61e346c445e13a9e60d2ee3.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is one plus and one minus. By and large this is the same plus and minus. The problem is that when the </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">threads </font><font style="vertical-align: inherit;">have </font><font style="vertical-align: inherit;">ended, he creates new ones. On the one hand, this is good - our task is not sitting in a queue, not waiting, it can be immediately executed. On the other hand, this is bad because a new thread is being created.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Before I made corrections from the previous paragraph (the fifth alteration), I measured, and it turned out that I </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">created 20 threads </font><font style="vertical-align: inherit;">for one request </font><font style="vertical-align: inherit;">- there was too much waiting. 100 simultaneous threads issued out of memory exception. It did not work - even on servers that are available in our lab. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I cut all the expectations, locks, made the "Fifth mess". My threads are no longer blocked, not wasted, but they work. All the same on one request through </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is created on average 12 flows. For 100 simultaneous requests, 800 threads were created. It creaked, but it worked. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In fact, for such things </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">executor can not be used. If you have very little tasks, there are a lot of them,</font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">executor will do. But in general - no. He will create you many threads, then you will rake them. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this case, you need to fix the ThreadPool executor. Must measure options. But I‚Äôll just show performance results for one that turned out to be the best candidate for correction </font></font><code>CachedThreadPool()</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">with two threads: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/36b/717/9a3/36b7179a3511a9313c25a917afc5c020.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Two threads are the best option because writing to the socket is a bottleneck that cannot be parallelized, and SSLEngine cannot work in parallel either. The numbers speak for themselves. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Moral: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Not all ThreadPools are equally useful.</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> With alterations HTTP 2 Client I have everything.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To be honest, reading the documentation, I swore a lot on the Java API. Especially in the part about byte buffer, sockets and so on. But my rules of the game were such that I should not have changed them. For me, the JDK is an external library on which this API is built. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But Comrade Norman Maurer was not as constrained as I was. And he wrote an interesting presentation - for those who want to dig deeper: </font></font><a href="https://speakerdeck.com/normanmaurer/writing-highly-performant-network-frameworks-on-the-jvm-a-love-hate-relationship"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Writing Highly Performant Network Frameworks on the JVM - A Love-Hate Relationship</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It scolds the base JDK API just in the area of ‚Äã‚Äãsockets, APIs and other things. And describes what they wanted to change and what they lacked at the JDK level when they wrote Netty. These are all the same problems that I met, but could not fix it within the framework of the rules of the game.</font></font><br><br><hr><br>  If you like to savor all the details of Java development in the same way as we do, you probably will be interested in these reports at our April <a href="https://jpoint.ru/">JPoint 2018</a> conference: <br><br><ul><li> <a href="https://jpoint.ru/talks/7ulbihlaqmk4iquimasikq/">    Java</a> ( , Devexperts) </li><li>  <a href="https://jpoint.ru/talks/1qghn5o70siuweuqeesuoa/">Program analysis: how to understand that you are a good programmer</a> (Alexey Kudryavtsev, JetBrains) </li><li> <a href="https://jpoint.ru/talks/4kniewswg8eiqciayummmc/">       </a> ( , ) </li><li> <a href="https://jpoint.ru/talks/4vfroqkizemyaw6wsms8qm/">Java EE 8 finally final! And now EE4J?</a> (David Delabass√©e, Oracle) </li></ul></div><p>Source: <a href="https://habr.com/ru/post/352074/">https://habr.com/ru/post/352074/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352064/index.html">Bluebird: belt with tools for the asynchronist</a></li>
<li><a href="../352066/index.html">New solution for maintaining the availability of IT infrastructure: Veeam Availability Orchestrator</a></li>
<li><a href="../352068/index.html">Installing Facebook image recognition package. All rake in one place</a></li>
<li><a href="../352070/index.html">Apache Ignite: in-memory distributed computing</a></li>
<li><a href="../352072/index.html">The end of procrastination or what is ICIGAI?</a></li>
<li><a href="../352076/index.html">Immigration in Chile: finding a job and obtaining a residence permit</a></li>
<li><a href="../352078/index.html">How to make a report on the identified vulnerability</a></li>
<li><a href="../352080/index.html">FastTrack Training. "Network Basics". "Understanding Cisco Architectural Games (Overview)". Eddie Martin December 2012</a></li>
<li><a href="../352082/index.html">SEO multilingual website without geo-referencing under Google</a></li>
<li><a href="../352084/index.html">Marvin Minsky "The Emotion Machine": Chapter 2 "We want to create a machine that would be proud of us"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>