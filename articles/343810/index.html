<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural network to identify individuals embedded in the smartphone</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Apple began to use in-depth training to identify individuals since iOS 10. With the release of the Vision framework, developers can now use this techn...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural network to identify individuals embedded in the smartphone</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/vg/ch/ya/vgchyavvz_oziabpxfjs5y5qgvs.png" align="left">  Apple began to use in-depth training to identify individuals since iOS 10. With the release of the Vision framework, developers can now use this technology and many other machine vision algorithms in their applications.  When developing the framework, we had to overcome significant problems in order to preserve the privacy of users and work effectively on the hardware of the mobile device.  The article discusses these problems and describes how the algorithm works. <br><br><h2>  Introduction </h2><br>  For the first time, the definition of persons in public APIs appeared in the Core Image framework through the CIDetector class.  These APIs also worked in Apple‚Äôs own applications, such as Photos.  The very first version of CIDetector used to determine the method based on the Viola-Jones algorithm <a href="https://habr.com/ru/post/343810/">[1]</a> <a name="1_1"></a>  .  CIDetector's consistent enhancements were based on the achievements of traditional machine vision. <br><br>  With the advent of deep learning and its application to machine vision problems, the accuracy of face detection systems has taken a significant step forward.  We had to completely rethink our approach in order to benefit from this paradigm shift.  Compared to traditional computer vision, models in depth learning require an order of magnitude more memory, much more disk space, and more computational resources. <br><a name="habracut"></a><br>  As of today, a typical high-performance smartphone is not a viable platform for vision models with in-depth training.  Most industry players bypass this limitation by using cloud APIs.  There, the pictures are sent for analysis to the server, where the system of in-depth training gives the result in the definition of persons.  In cloud services, powerful desktop GPUs with a large amount of available memory usually work.  Very large network models, and potentially entire ensembles of large models, can work on the server side, allowing customers (including mobile phones) to take advantage of large depth learning architectures that cannot be run locally. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Apple iCloud Photo Library - a cloud solution for storing photos and videos.  Each photo and video before sending it to iCloud Photo Library is encrypted on the device and can only be decrypted on the device with the corresponding iCloud account.  Therefore, for the work of machine vision systems in depth learning, we had to implement algorithms directly in the iPhone. <br><br>  Here had to solve several problems.  Depth learning models have to be delivered as part of the operating system, occupying the valuable NAND space.  They need to be loaded into RAM and take away significant computing resources of the GPU and / or CPU.  Unlike cloud services, where resources can be allocated exclusively for computer vision tasks, system resources are shared with other running applications on a computing device.  Finally, the calculations must be sufficiently effective to process a large collection of Photos photos at a reasonable time, but without significant power consumption or heat. <br><br>  The rest of the article discusses our approach to using algorithms to identify individuals in the depth learning system and how we successfully managed to overcome the difficulties in order to achieve maximum accuracy.  We will discuss: <br><br><ul><li>  how we fully utilized our GPU and CPU (using BNNS and Metal); </li><li>  memory optimization for neural network output, image loading and caching; </li><li>  how we implemented the neural network in such a way as not to interfere with the work of many other simultaneously performed tasks on the iPhone. </li></ul><br><h2>  Transition from Viola-Jones method to deep learning </h2><br>  When we started working on in-depth training to identify individuals in 2014, deep convolutional neural networks (GNSS) only started to show promising results in object detection tasks.  The most famous among all was the model OverFeat <a href="https://habr.com/ru/post/343810/">[2]</a> <a name="2_2"></a>  , which demonstrated some simple ideas and showed that the GNSS is quite effective in detecting objects in images. <br><br>  The OverFeat model derived the correspondence between a fully connected layers of a neural network and convolutional layers with valid filter convolutions in the same spatial dimensions as the input data.  This work clearly showed that a binary classification network with a fixed receptive field (for example, 32 √ó 32 with a natural pitch of 16 pixels) can potentially be used for images of arbitrary size (for example, 320 √ó 320) and produce a map of the appropriate size at the output (in this example 20 √ó 20).  The scientific article describing OverFeat also contained clear recipes on how to produce tighter output cards, effectively reducing the neural network pitch. <br><br>  We initially founded the architecture on some ideas from the article on OverFeat, which resulted in a fully convolutional network (see Figure 1) with a multitasking goal: <br><br><ul><li>  binary classification to predict the presence or absence of a person in the input data; </li><li>  regression to predict the parameters of the bounding box that best localizes the face in the input data. </li></ul><br>  We experimented with different training options for such a neural network.  For example, a simple learning procedure was to create a large set of data with tiles of fixed-size images that corresponded to the minimum valid size of the input data, so that each tile generated one result at the output of the neural network.  The training data set is perfectly balanced, so half of the tiles have a face (positive class), and no half (negative class) on the other half.  For each positive tile, the true coordinates (x, y, w, h) of the faces were specified.  We have trained the neural network to optimize for the multitasking goal described above.  After learning, the neural network learned to predict the presence of a face in the image and, in the event of a positive answer, gave out the coordinates and scale of the face in the frame. <br><br>  <b>Fig.</b>  <b>1. Improved GNSS face detection architecture</b> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9c7/be8/3cb/9c7be83cbe20f48ec05034f06eef1372.png"><br><br>  Since the network is fully convolved, it can effectively process an image of arbitrary size and make an output map 2D.  Each point on the map corresponds to the input image tile and contains the prediction of the neural network regarding the presence or absence of a face on this tile, as well as its location / scale (see the input and output of GNSS in Fig. 1). <br><br>  With such a neural network, you can build a fairly standard processing pipeline for identifying individuals.  It consists of a multiscale image pyramid, a neural network for determining faces, and a post-processing module.  A multiscale pyramid is required to process faces of all sizes.  The neural network is applied at each level of the pyramid, from where candidates for recognition are extracted (see Fig. 2).  The post-processing module then brings together candidates from all scales to produce a list of bounding boxes that match the final prediction of the neural network by the faces in the image. <br><br>  <b>Fig.</b>  <b>2. The process of identifying individuals</b> <br><img src="https://habrastorage.org/getpro/habr/post_images/811/393/405/81139340534bc105ad024cb5c13be37a.png"><br><br>  Such a strategy made it more realistic to launch a deep convolutional neural network and complete scanning of an image on a mobile gland.  But the complexity and size of the neural network remained the main bottlenecks in performance.  Solving this problem meant not only limiting the neural network with a simple topology, but also limiting the number of layers, the number of channels per layer and the size of the core of convolutional filters.  These limitations raised an important problem: our neural networks, which provided acceptable accuracy, are far from simple: most of them have more than 20 layers plus several network-in-network modules <a href="https://habr.com/ru/post/343810/">[3]</a> <a name="3_3"></a>  .  The use of such networks in the image scanning framework described above is absolutely impossible due to unacceptable performance and power consumption.  In fact, we cannot even load a neural network into memory.  The task boils down to how to train a simple and compact neural network that can simulate the behavior of accurate, but very complex networks. <br><br>  We decided to use an approach that is informally known as teacher-student learning <a href="https://habr.com/ru/post/343810/">[4]</a> <a name="4_4"></a>  .  This approach provides a mechanism for learning the second fine-and-deep neural network (‚Äústudent‚Äù), so that it very closely matches the output of a large complex neural network (‚Äúteacher‚Äù), which we trained as described above.  The student neural network consists of a simple repeating structure of 3 √ó 3 convolutions and subsample layers, and its architecture is adapted to maximize the use of our neural network output engine (see Figure 1). <br><br>  Now, finally, we have an algorithm for the deep neural network of determining persons, suitable for running on a mobile gland.  We repeated several training cycles and obtained a neural network model sufficiently accurate to perform the assigned tasks.  Although this model is accurate and capable of working on a mobile device, there is still a huge amount of work to make it possible in practice to deploy the model on millions of user devices. <br><br><h2>  Image Processing Pipeline Optimization </h2><br>  Practical considerations for in-depth training have greatly influenced the choice of architecture for an easy-to-use developer platform, which we call Vision.  It soon became apparent that to create a great framework, not just great algorithms are enough.  I had to greatly optimize the image processing pipeline. <br><br>  We didn‚Äôt want developers to think about scaling, color conversion, or image sources.  Face detection should work well regardless of whether the stream from the camera is used in real time, video processing, files from disk or from the web.  It should work regardless of the type and format of the image. <br><br>  We were worried about power consumption and memory usage, especially in the process of streaming and image capture.  The memory consumption worried us, including in the case of processing 64-megapixel panoramas.  We solved these problems using partial sub-sampling decoding and automatic tiling.  This made it possible to start the tasks of machine vision in large images, even with a non-standard aspect ratio. <br><br>  Another problem was the matching of color spaces.  Apple has a wide range of APIs, but we did not want to load developers with work on choosing a color space.  This takes over the Vision framework, thus lowering the threshold of entry for successfully introducing machine vision into any application. <br><br>  Vision is also optimized by efficient reuse and processing of intermediate links.  Detecting faces, determining the coordinates of faces, and some other tasks of computer vision ‚Äî they all work on the same mapped intermediate image.  By abstracting the interface to the level of algorithms and finding the optimal place to process the image or buffer, Vision is able to create and cache intermediate images - this improves the performance of various computer vision tasks even without the intervention of the developer. <br><br>  The reverse is also true.  From the perspective of the central interface, we can direct the development of the algorithm in such a direction as to optimize the reuse or sharing of intermediate data.  Vision implements several different and independent machine vision algorithms.  For different algorithms to work well together, they share the same input resolutions and color spaces where possible. <br><br><h2>  Performance Optimization for Mobile Iron </h2><br>  The pleasure of an easy-to-use framework will quickly disappear if the face detection API is not able to work in real time or in background system processes.  Users expect that the detection of faces works automatically and imperceptibly during the processing of photo albums, or triggered immediately after the shot.  They do not want the battery charge to decrease or the system to brake because of this.  Apple mobile devices are multi-tasking.  Therefore, the background process of machine vision should not significantly affect other system functions. <br><br>  We have implemented several strategies to reduce memory consumption and use of the GPU.  To reduce memory usage, we allocate intermediate layers of our neural networks by analyzing the computational graph.  This allows you to assign multiple layers to one buffer.  Being fully deterministic, such a technique nevertheless reduces memory consumption without affecting performance or fragmentation in memory, and it can be used for both CPUs and GPUs. <br><br>  The Vision detector works with five neural networks (one for each level of a multi-scale pyramid, as shown in Fig. 2).  For these five neural networks, the total weights and parameters are indicated, but they have different formats of input and output data and intermediate layers.  To further reduce memory consumption, we run a memory optimization algorithm on a shared graph compiled by these five networks, which significantly reduces memory consumption.  Also, all neural networks together use the same buffers with weights and parameters, again reducing the amount of allocated memory. <br><br>  To achieve better performance, we use the fully convoluted nature of the neural network: all scales dynamically change to match the resolution of the input image.  Compared with the alternative approach - fitting the image to a square grid of a neural network (laid with empty strips) - fitting the neural network to the image size allows you to drastically reduce the total number of operations.  Since as a result of this rearrangement, the topologies of operations do not change and, due to the high performance of the rest of the distributor, dynamic shape change does not consume more resources than allocation. <br><br>  To ensure interactivity and the absence of UI slowdowns while the deep neural network is running in the background process, we have divided the work orders for the GPU for each layer of the neural network so that each task is executed for no longer than one millisecond.  This allows the driver to change contexts, allocating resources for higher priority tasks in time, such as animation UI, which reduces and sometimes completely eliminates dropping frames. <br><br>  Together, these strategies ensure that the user will enjoy local work with low latency and private mode, although he doesn‚Äôt even know that the neural networks on the smartphone perform several hundred billion floating point operations every second. <br><br><h2>  Using the Vision framework </h2><br>  Have we been able to achieve our goal and develop a high-performance, easy-to-use API for identifying individuals?  You can try the Vision framework and decide for yourself.  Here are some resources to get you started: <br><br><ul><li>  Presentation from WWDC: ‚Äú <a href="https://developer.apple.com/videos/play/wwdc2017/506/">Vision Framework: Development on Core ML</a> ‚Äù </li><li>  <a href="https://developer.apple.com/documentation/vision">Vision Framework Help</a> </li><li>  Manual "Core ML and Vision: Machine Learning in iOS 11" <a href="https://habr.com/ru/post/343810/">[5]</a> <a name="5_5"></a></li></ul><br><h2>  Literature </h2><br><a name="1"></a>  [1] Viola, P., Jones, MJ <b>Robust Real-Time Object Detection</b> .  Published in <i>Proceedings of the Computer Vision and Pattern Recognition Conference</i> , 2001. <a href="https://habr.com/ru/post/343810/">‚Üë</a> <br><br><a name="2"></a>  [2] Sermanet, Pierre, David Eigen, Xiang Zhang, Michael Mathieu, Rob Fergus, Yann LeCun.  <b>OverFeat: Integrated Recognition, Localization and Detection Using Convolutional Networks</b> .  <i>arXiv: 1312.6229</i> [Cs], December 2013  <a href="https://habr.com/ru/post/343810/">‚Üë</a> <br><br><a name="3"></a>  [3] Lin, Min, Qiang Chen, Shuicheng Yan.  <b>Network In Network</b> .  <i>arXiv: 1312.4400</i> [Cs], December 2013  <a href="https://habr.com/ru/post/343810/">‚Üë</a> <br><br><a name="4"></a>  [4] Romero, Adriana, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio.  <b>FitNets: Hints for Thin Deep Nets</b> .  <i>arXiv: 1412.6550</i> [Cs], December 2014  <a href="https://habr.com/ru/post/343810/">‚Üë</a> <br><br><a name="5"></a>  [5] Tam, A. <b>Core ML and Vision: Machine learning in iOS Tutorial</b> .  Retrieved from <a href="https://www.raywenderlich.com/">www.raywenderlich.com</a> , September 2017.  <a href="https://habr.com/ru/post/343810/">‚Üë</a> </div><p>Source: <a href="https://habr.com/ru/post/343810/">https://habr.com/ru/post/343810/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343800/index.html">Probabilistic interpretation of classical machine learning models</a></li>
<li><a href="../343802/index.html">Cross-platform IoT: Device Operations</a></li>
<li><a href="../343804/index.html">A bit about the .NET Framework and .NET Core [plus useful links]</a></li>
<li><a href="../343806/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ291 (November 27 - December 3, 2017)</a></li>
<li><a href="../343808/index.html">Where is my payment? How fraudsters earn freelancers</a></li>
<li><a href="../343812/index.html">DotNext 2018 Piter Release Notes</a></li>
<li><a href="../343816/index.html">We collect user activity in JS and ASP</a></li>
<li><a href="../343818/index.html">TypeScript: tslib library</a></li>
<li><a href="../343820/index.html">Technical diary: half a year developing mobile PvP</a></li>
<li><a href="../343822/index.html">Heading "We read articles for you." October - November 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>