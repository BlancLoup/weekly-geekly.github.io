<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Environment for Dell VDI Virtual Machines</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today we will talk about VDI ( Virtual Desktop Infrastructure ) - the deployment environment of virtual machines. It is needed primarily for those who...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Environment for Dell VDI Virtual Machines</h1><div class="post__text post__text-html js-mediator-article">  Today we will talk about VDI ( <i>Virtual Desktop Infrastructure</i> ) - the deployment environment of virtual machines.  It is needed primarily for those who work with large amounts of data.  And in order to demonstrate the capabilities of the infrastructure, we deployed a test installation of VDI - it is running under VMware Horizon View and includes 800 workstations.  For maximum data transfer rates, we used Dell Fluid Cache for SAN technology and a Dell Compellent disk array. <br><a name="habracut"></a><br><br>  <b>Part 1: Formulate the task and describe the system configuration</b> <br><br>  Why use virtual machines?  Simply, it is profitable.  Let's start with the fact that "virtuals" allow you to save on the purchase and maintenance of equipment, they are safe and easy to manage.  Even an outdated computer and a mobile device that is no longer able to perform its basic functions can become a terminal in a virtual environment - sheer advantages!  However, this applies to "normal" jobs.  Developers, researchers, analysts and other professionals working with large amounts of data, usually use powerful workstations.  The Dell VDI platform is designed to virtualize just such machines. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Here is what we had to do: <br><br>  ‚Ä¢ Develop a holistic infrastructure for Horizon View and VDI based on vSphere for ‚Äúhard‚Äù tasks.  We were going to use Fluid Cache technologies, Compellent disk arrays, PowerEdge servers and Dell Networking switches. <br>  ‚Ä¢ Determine the performance of the assembly at each level of the implementation of the VDI stack with high requirements for the exchange with the disk subsystem.  We considered it as 80-90 IOPS per 1 VM during the whole working process. <br>  ‚Ä¢ Determine the drop in performance at peak loads (the so-called Boot and Login Storms). <br><br>  The hardware and software platform consisted of: <br><br>  ‚Ä¢ VMware Horizon View 6.0, <br>  ‚Ä¢ VMware vSphere 5.5 hypervisor, <br>  ‚Ä¢ Dell PowerEdge R720 servers with Dell Express PCIe SSDs installed, <br>  ‚Ä¢ Dell Networking S4810 switches, <br>  ‚Ä¢ Dell Compellent SC8000 disk array. <br><br>  What is written below will be interesting for system architects who plan to implement virtualization technologies for high-load user workstations. <br><br>  System components <br><br>  <i>1. Dell Fluid Cache for SAN and VDI</i> <br><br>  This technology uses the Express Flash NVMe PCIe SSD installed in servers and provides remote memory access (RDMA).  An additional cache level is a fast intermediary between the disk array and servers.  Features of the architecture is that it is initially parallelized, resistant to failures and ready for load distribution, and paired with a high-speed SAN allows you to install disks for caching at least in all servers of the assembly, at least in some of them.  Faster data sharing will be even those machines that do not contain Flash NVMe PCIe SSD.  Cached data is transmitted to the network using the RDMA protocol, the advantage of which, as is well known, is in minimal delays.  As Dell Compellent Enterprise software detects cache components, a Fluid Cache cluster is created and configured automatically in it.  The fastest way when using Fluid Cache is to write data to an external disk array - this increases the number of IOPS (I / O operations per second). <br><br>  The diagram shows the network architecture of the distributed cache. <br><img src="https://habrastorage.org/files/997/5f4/80e/9975f480e737498794bd0209605ff6e8.png"><br>  <i>2. Solution architecture</i> <br><br>  This is how high-speed data exchange between virtual machines and an external disk array looks like: <br><img src="https://habrastorage.org/files/d54/fd6/506/d54fd65069f24c6aae3089b2f824614c.png"><br>  <i>3. Programs</i> <br><br>  <i>Horizon view</i> <br><br>  Horizon view is a complete VDI solution that makes virtual desktops safe and manageable.  Its components operate at the level of software, network environment, and hardware. <br><table border="1"><tbody><tr><td>  <b>Component</b> </td><td>  <b>Explanation</b> </td></tr><tr><td>  <b>Client devices</b> </td><td>  Personal devices that give the user access to virtual desktops: specialized terminals (for example, Dell Wyse end points), mobile phones, tablets, PCs, etc. </td></tr><tr><td>  <b>Horizon View Connection Server</b> </td><td>  A software product that authorizes a user in the system and then redirects him to the terminal, virtual or physical machine. </td></tr><tr><td>  <b>Horizon View Client</b> </td><td>  Client part of the program.  It is used to access workstations running Horizon View. </td></tr><tr><td>  <b>Horizon view agent</b> </td><td>  A service that provides communication between clients and Horizon View servers. </td></tr><tr><td>  <b>Horizon View Administrator</b> </td><td>  A web interface to manage Horizon View‚Äôs infrastructure and components. </td></tr><tr><td>  <b>vCenter Server</b> </td><td>  The primary platform for administering, configuring, and managing arrays of VMware virtual machines. </td></tr><tr><td>  <b>Horizon View Composer</b> </td><td>  Application for creating a pool of virtual machines based on the base OS image.  With it, it is easier to administer the system and you can save resources. </td></tr></tbody></table><br>  <i>Virtual Jobs</i> <br><br>  It is necessary to work differently with different virtual machines.  A permanent ‚Äúvirtualka‚Äù of a particular user has its place on the disk, where the OS files, programs, and personal data are located.  A temporary virtual environment is created on the basis of the original OS image when the user connects to the system (respectively, it is destroyed upon exit).  This scheme of work saves system resources and allows you to create a unified workplace - safe and meets all the requirements of the company.  This is suitable for typical tasks, but not all employees perform typical tasks. <br>  VMware View Personal profile management is good because it combines the best of both approaches.  Windows has long been able to work not only with local user profiles, but also those stored on the server.  As a result, the user receives a temporary virtual machine from the OS image, but personal data from his profile on the server is loaded into it.  Such machines ‚Äî VMware-linked clones ‚Äî are best used with a high load on disk storage. <br><br>  <i>Horizon view desktop pools</i> <br><br>  Horizon view creates different pools for different types of virtual machines - so they are easier to administer.  For example, permanent physical and virtual machines fall into the Manual pool, and the Automated Pool contains ‚Äúdisposable‚Äù, template ones.  Through the program interface you can manage clones with the assigned user profile.  Here you can read about the features of the configuration, configuration and maintenance. <br><br>  <i>Hypervisor: VMware vSphere 5.5</i> <br><br>  The VMware vSphere 5.5 platform is needed to create VDI and cloud solutions.  It consists of three main levels: virtualization, management and interface.  The level of virtualization is infrastructure and system software.  The management level helps to create a virtual environment.  Interface is web-based utilities and vSphere client programs. <br>  According to the recommendations of VMware and Microsoft, the network core services are also used in the virtualization environment - NTP, DNS, Active Directory and others. <br><br>  <i>4. Dell hardware</i> <br><br>  Virtual machines reside on a cluster of eight nodes that are managed by the vSphere.  Each server has a PCIe SSD and Fluid Cache provider functions.  All of them are connected to three networks: 10 GB Ethernet iSCSI SAN, VDI control network and the one that provides the cache. <br>  There is another cluster of two nodes running vSphere.  There are virtual machines that make the VDI infrastructure work: vCenter Server, View Connection Server, View Composer Server, SQL Server, Active Directory, etc.  These nodes are also connected to the VDI and iSCSI SAN control networks. <br>  The Dell Compellent SC8000 disk array has two types of paired controllers and drives: Write Intensive (WI) SSD, 15K SAS HDD and 7.2K NL-SAS HDD.  As you can see, the disk subsystem is both productive and capacious. <br>  Switches are stacked in pairs - this provides fault tolerance at the network level.  They serve all three networks ‚Äî the client for virtual machines, the control for the VDI environment, and the iSCSI SAN.  Bandwidth is divided based on VLAN, and traffic prioritization is also performed.  The optimal bandwidth for these networks is 40 Gb / s, but 10 Gb / s is enough. <br><br>  Here is a general installation plan: <br><img src="https://habrastorage.org/files/cb6/1ec/4b7/cb61ec4b7f2741aba229f07ea3b47271.png"><br>  1. Management console. <br>  2. VDI and client management networks. <br>  3. vSphere servers for virtual machines. <br>  4. Fluid Cache controllers (2 pcs.). <br>  5. SAN switches (2 pcs.). <br>  6. Disk array controllers (2 pcs.). <br>  7. Shelves disk array (2 pcs.). <br>  8. Servers providing VDI infrastructure. <br><br>  5. Configuration of components. <br><br>  Servers <br><img src="https://habrastorage.org/files/20f/015/d5a/20f015d5a5a74c4e94dcfe54d0b189b8.png"><br>  Each of the eight Dell PowerEdge R720 servers that make up the cluster for a VM has: <br><br>  ‚Ä¢ two ten-core Intel Xeon CPU E5-2690 v2 @ 3.00GHz, <br>  ‚Ä¢ 256 GB of RAM, <br>  ‚Ä¢ Dell Express Flash PCIe SSD drive with a capacity of 350 GB, <br>  ‚Ä¢ Mellanox ConnectX-3 network card, <br>  ‚Ä¢ Broadcom NetXtreme II BCM57810 10 Gigabit Network Controller, <br>  ‚Ä¢ Quad-port Broadcom NetXtreme BCM5720 gigabit network controller. <br><br>  On a cluster of two Dell PowerEdge R720, we deployed virtual machines for Active Directory, VMware vCenter 5.5 server, Horizon View Connection server (primary and backup), View Composer server, file server based on Microsoft Windows Server 2012 R2 and MS SQL Server 2012 R2 .  Forty LoginVSI launcher virtual machines are used to test the installation - they create a load. <br><br>  <i>Network</i> <br><img src="https://habrastorage.org/files/4ea/4f0/ea6/4ea4f0ea6c5349cda6fe273ebd309d16.png"><br>  We used three pairs of switches in the installation: Force10 S55 1 GB and Force10 S4810 10 GB.  The first was responsible for creating the VDI environment and managing servers, disk array, switches, vSphere, etc.  VLAN for all these components are different.  The second pair, the Dell Force10 S4810 10 GB, helped organize the Fluid Cache caching network.  Another similar pair was responsible for the operation of the iSCSI network between the clusters and the Compellent SC8000 disk array. <br><br>  <i>Disk array Compellent SC8000</i> <br><table border="1"><tbody><tr><td>  <b>Role</b> </td><td>  <b>Type of</b> </td><td>  <b>amount</b> </td><td>  <b>Explanations</b> </td></tr><tr><td>  Controller </td><td>  SC8000 </td><td>  2 </td><td>  System Center Operating System (SCOS) 6.5 </td></tr><tr><td>  Shelf </td><td>  External </td><td>  2 </td><td>  24 compartments for 2.5 "disks </td></tr><tr><td>  Ports </td><td>  10 Gbps Ethernet </td><td>  2 </td><td>  Front end host connectivity </td></tr><tr><td></td><td>  SAS 6 Gbps </td><td>  four </td><td>  Back end drive connectivity </td></tr><tr><td>  Discs </td><td>  400 GB WI SSD </td><td>  12 </td><td>  11 active and 1 hot standby </td></tr><tr><td></td><td>  300 GB 15K SAS </td><td>  21 </td><td>  20 active and 1 hot standby </td></tr><tr><td></td><td>  4 TB 7.2K NL-SAS </td><td>  12 </td><td>  11 active and 1 hot standby </td></tr></tbody></table><br>  For the virtual servers of the service servers, we created a 0.5 TB disk where we placed Active Directory, SQL Server, vCenter Server, View Connection Server, View Composer and file server.  For user profiles, a 2 TB disk was allocated (approximately 2.5 GB per machine).  The master OS images are located on a 100 GB mirror disk.  Finally, for virtual machines, we decided to make 8 disks of 0.5 TB each. <br><br>  <i>Virtual network</i> <br><br>  Each host running the VMware vSphere 5.5 hypervisor has five virtual switches: <br><table border="1"><tbody><tr><td>  <b>vSwitch</b> </td><td>  <b>Purpose</b> </td></tr><tr><td>  vSwitch0 </td><td>  Control network </td></tr><tr><td>  vSwitch1 and vSwitch2 </td><td>  iSCSI SAN </td></tr><tr><td>  vSwitch3 </td><td>  Fluid Cache VLAN </td></tr><tr><td>  vSwitch4 </td><td>  VDI LAN </td></tr></tbody></table><br>  <i>Horizon view</i> <br><br>  We set up <a href="https://www.vmware.com/support/pubs/view_pubs.html">this instruction</a> . <br><table border="1"><tbody><tr><td>  <b>Role</b> </td><td>  <b>amount</b> </td><td>  <b>Type of</b> </td><td>  <b>Memory</b> </td><td>  <b>CPU</b> </td></tr><tr><td>  Horizon View Connection Servers </td><td>  2 </td><td>  VM </td><td>  16 GB </td><td>  8nos </td></tr><tr><td>  View Composer Server </td><td>  one </td><td>  VM </td><td>  8 GB </td><td>  8nos </td></tr></tbody></table><br>  <i>Virtual machines</i> <br><br>  We started setting up the original Windows 7 images using the VMware and Login VSI manuals.  Here's what we got: <br><br>  ‚Ä¢ VMware Virtual Hardware v8, <br>  ‚Ä¢ 2 virtual CPUs, <br>  ‚Ä¢ 3 GB RAM, 1.5 GB reserved, <br>  ‚Ä¢ 25 GB of disk memory <br>  ‚Ä¢ one virtual network adapter connected to a VDI network, <br>  ‚Ä¢ Windows 7 64-bit. <br><br>  VMware Horizon Guide with View Optimization Guide for Windows 7 and Windows 8 can be downloaded <a href="http://www.vmware.com/files/pdf/VMware-View-OptimizationGuideWindows7-EN.pdf">here</a> . <br><br>  <b>Part 2: We test, we describe results, we draw conclusions.</b> <br><br>  <i>Methods for testing and configuring components</i> <br><br>  Then we checked how the system behaves with different scenarios for the use of virtual machines.  We wanted to ensure that the eight hundred ‚Äúvirtual machines‚Äù worked without noticeable delays and exchanged information with the disk subsystem in the area of ‚Äã‚Äã80-90 IOPS throughout the day.  At the same time, we checked the work of the installation under the usual ‚Äúoffice‚Äù load ‚Äî both during the working day, and when virtual machines were created, and when users logged on to the system ‚Äî for example, at the beginning of the working day and after the lunch break.  Let's call these conditions Boot &amp; Login Storm. <br><br>  For testing, we used two utilities - a comprehensive program, Login VSI 4.0, which loads the virtual machines and analyzes the results, as well as Iometer - it helps to monitor the disk subsystem. <br><br>  In order to create a ‚Äústandard‚Äù load for an average office machine, we: <br><br>  ‚Ä¢ Launched up to five applications simultaneously. <br>  ‚Ä¢ Used Microsoft Internet Explorer, Microsoft Word, Microsoft Excel, Microsoft PowerPoint, PDF reader, 7-Zip, Movie player and FreeMind. <br>  ‚Ä¢ Re-run scripts approximately every 48 minutes, pause - 2 minutes. <br>  ‚Ä¢ Every 3-4 minutes measured the response time to user actions. <br>  ‚Ä¢ Set the print speed to 160 ms per character. <br><br>  Imitated a high load using the same test, and to activate the exchange with the disk subsystem in each virtual machine, Iometer was launched.  A standard set of tests generated 8-10 IOPS per machine, with a high load, this indicator was 80-90 IOPS.  We created the base OS images in the Horizon view pool with standard settings and a set of software for LoginVSI, plus an additional install of Iometer, which was run from the logon script. <br><br>  To monitor work and record performance under load, we used the following tools: <br><br>  ‚Ä¢ Dell Compellent Enterprise Manager for evaluating the performance of Fluid Cache and Compellent SC8000 storage array, <br>  ‚Ä¢ VMware vCenter statistics for monitoring vSphere, <br>  ‚Ä¢ Login VSI Analyzer for user environment assessment. <br><br>  The criterion for evaluating the performance of the disk subsystem is the response time to user actions.  If it is not higher than 10 ms, then everything is in order. <br><br>  System load at the hypervisor level: <br><br>  ‚Ä¢ The load on any CPU in the cluster should not exceed 85%. <br>  ‚Ä¢ Minimum memory ballooning (sharing of the memory area by virtual machines). <br>  ‚Ä¢ The load on each of the network interfaces is no more than 90%. <br>  ‚Ä¢ Re-sending TCP / IP packets - no more than 0.5%. <br><br>  The maximum number of requests from virtual machines was calculated by the method of <a href="http://www.loginvsi.com/documentation/index.php%3Ftitle%3DAnalyzing_Results">VSImax</a> . <br>  A single set of virtual machines was created based on the base image of Windows 7 using the VMware Horizon View Administrator interface. <br><br>  The settings were as follows: <br><br>  ‚Ä¢ Automatic Desktop Pool: machines are created upon request to "enable". <br>  ‚Ä¢ Each user is assigned a random machine. <br>  ‚Ä¢ View Storage Accelerator is allowed for all machines with an update period of 7 days. <br>  ‚Ä¢ 800 virtual machines are located on 8 physical machines (100 each). <br>  ‚Ä¢ Source OS images are on separate disks. <br><br>  <i>Test results and analysis</i> <br><br>  Scenarios were as follows: <br><br>  1. Boot Storm: the maximum load on the storage system, when at the same time many users "turn on", that is, create virtual machines. <br>  2. Login Storm: another strong load on the disk subsystem when users log on to the system at the beginning of the working day or after the break.  For this test, we turned on the virtual machines and left to log in for 20 minutes without any action. <br>  3. Standard load: after users logged in, the ‚Äústandard‚Äù office work script was launched.  The response of the system to user actions was observed by the VSImax (Dynamic) parameter from Login VSI. <br>  4. High load: similar to standard, but IOPS is 10 times higher. <br><br>  Boot storm <br><img src="https://habrastorage.org/files/e1e/dde/8ad/e1edde8add104668bfc0b074836d9f73.png"><br>  Note: <i>Hereinafter, latency is indicated by the storage system.</i> <br><br>  800 machines generated an average of 115,000 IOPS during the test.  Write operations prevailed.  Four minutes later all the cars were available for work. <br><br>  <i>Login Storm and Standard Load</i> <br><br>  We programmed the Login VSI to log into 800 virtual machines about an hour after they were loaded.  Peak IOPS did not exceed 6,500 for the entire platform (or 8-10 per machine).  There were more requests for writing than during the startup of machines and further work, since a user profile was created, OS services and application applications were launched.  After the time required for stabilization, the OS caches the main user and program data in RAM - the load on the disk subsystem is reduced.  At the Fluid Cache level, with a maximum of 6,500 IOPS, write operations are cached at 100%, reads are about 95%.  The 98 Mbps data stream causes a disk access delay of only 2 ms. <br><img src="https://habrastorage.org/files/9b3/952/f45/9b3952f459564f10ba7849652a84881d.png"><br><img src="https://habrastorage.org/files/7ea/1b9/92b/7ea1b992bc5b41e18d13849e58515616.png"><br>  <i>Performance at the physical level</i> <br><br>  The performance statistics of the physical components of the vSphere system we shot using VMware vCenter Server.  For the entire time of the test, the load on the processors, memory and all levels of the storage system was about the same.  The results were the same as we expected to see, setting the task: <br><br>  ‚Ä¢ CPU load not more than 85%. <br>  ‚Ä¢ The active disk space used is no more than 80% (when virtual machines were created) and no more than 60% during tests. <br>  ‚Ä¢ Memory ballooning is minimal (sometimes not used at all). <br>  ‚Ä¢ Network load is about 45%, including iSCSI, SAN, VDI LAN, Management LAN and vMotion LAN. <br>  ‚Ä¢ Average read / write latency on disk storage adapters - 2 ms. <br><br>  This is a graph of system performance from the user's point of view.  It depends on the number of machines that operate simultaneously: <br><img src="https://habrastorage.org/files/2bd/f65/e41/2bdf65e4122740718f3dbab481dd3a6a.png"><br>  In real life, this means that on our system, you can safely deploy jobs for thousands of users.  There will be no OS delays even if everyone is actively working at the same time. <br><br>  Here are the conclusions from the three tests of the ‚Äúusual‚Äù user environment: <br><br>  ‚Ä¢ The Dell Fluid Cache for SAN-based VDI test system handled the task perfectly: 800 users can work simultaneously. <br>  ‚Ä¢ Write operations are 98%, and only 2% are read operations. <br>  ‚Ä¢ None of the system resources loaded more than we expected. <br>  ‚Ä¢ Login Storm within 20 minutes did not exceed 8-10 IOPS and did not cause disk access delays of more than 2 ms. <br><br>  <i>Heavy load</i> <br><br>  These were the settings for Iometer for a large load on the I / O system: the block size is 20 KB, 80% of write operations, 75% of which are not from the cache, 25 ms Burst Delay. <br><img src="https://habrastorage.org/files/f08/a2f/abc/f08a2fabc5ad4356a067fce4500f73df.png"><br>  At the Fluid Cache level, the maximum I / O intensity reached 72,000 (90 IOPS per virtual machine).  Caching write operations - 100%, reading - 95%.  With a peak storage performance of 900 Mbps, the maximum delay was 7.5 ms. <br>  The load at the physical level of the installation was the same as in the previous test.  The storage, CPU, and network adapters worked as we planned. <br><img src="https://habrastorage.org/files/d62/06d/2be/d6206d2be2b94234809ee61433f017a6.png"><br>  This is a graph of the response of the system to user actions.  It depends on the number of machines that operate simultaneously: <br><img src="https://habrastorage.org/files/2b5/86c/275/2b586c2756a947b2866adb1e8e0e264e.png"><br>  Our findings from testing under high load: <br><br>  ‚Ä¢ Dell VDI based on Dell Fluid Cache for SAN can be used to simultaneously run 800 virtual machines that intensively exchange data with disk memory. <br>  ‚Ä¢ 98% fall on write operations, 2% on read operations. <br>  ‚Ä¢ None of the system resources of the base servers used by vSphere has been maximized.  The indicated limits were not exceeded. <br>  ‚Ä¢ High load on virtual machines created a delay of 7.5 ms with a total exchange rate of 72,000 IOPS (90 IOPS per each 800 machines).  This is also within the acceptable speed of the OS reaction to user actions. <br><br>  Recommendations <br><br>  VDI is a virtual machine environment with Dell Fluid Cache for SAN. <br><br>  ‚Ä¢ If you use roaming user profiles and store data in redirected folders, the load on the I / O system will be lower.  This is important during peak times when virtual machines are created and users log in to the system in large quantities.  The same applies to network share folders.  This scheme saves system resources and facilitates administration.  We recommend allocating a separate high-speed disk array for user data. <br>  ‚Ä¢ Considering the high load on the system when creating virtual machines, we suggest keeping several ready virtual machines ready.  In VMware Horizon View, you can configure the number of workplaces that will always be available to users. <br>  ‚Ä¢ We recommend that you configure the original Windows 7 and 8 images according to <a href="http://www.vmware.com/resources/techresources/10157">this instruction</a> .  If you disable unnecessary system services, then the client will speed up authorization and screen drawing, and the overall performance will only increase. <br><br>  <i>Setup Installation:</i> <br><br>  ‚Ä¢ Follow the <a href="http://kb.vmware.com/kb/2052329">recommendations of</a> VMware and Dell when installing and configuring vSphere. <br>  ‚Ä¢ Create separate virtual switches to separate iSCSI SAN, VDI, vMotion, and control network traffic. <br>  ‚Ä¢ For fault tolerance, assign each network path to at least two physical adapters. <br>  ‚Ä¢ Reserve at least 10% of physical drives to make the service more accessible. <br><br>  <i>Network level:</i> <br><br>  ‚Ä¢ Use at least two physical network adapters under the VDI network on each of the vSphere servers. <br>  ‚Ä¢ When you use a VLAN to separate traffic within the same physical network, spread the VDI, vMotion, and control logical networks about different virtual networks. <br>  ‚Ä¢ On the physical layer, separate the iSCSI SAN and Fluide Cache networks.  This will increase the overall system performance. <br>  ‚Ä¢ The vSphere virtual switches are limited to 120 ports by default.  If the machines need more network connections, then after changing the settings, the server will have to be overloaded. <br>  ‚Ä¢ On iSCSI SAN switches, disable the spanning tree protocol.  For ports connected to both servers and storage adapters, enable PortFast.  Jumbo frames and Flow control, if they are supported by switches and adapters, need to be activated on all components of the iSCSI network.  <a href="http://en.community.dell.com/techcenter/storage/w/wiki/4250.switch-configuration-guides-by-sis.aspx">Here</a> you can read more about this. <br><br>  <i>Storage System:</i> <br><br>  ‚Ä¢ Dell SC Compellent storage arrays works great with Fluid Cache technology.  This is a productive and relatively inexpensive solution for the Dell VDI environment. <br>  ‚Ä¢ Fluid Cache improves storage performance when exchanging data between a server and a disk array.  Running the Storage Center 6.5 software, the computing cluster, network environment, and storage system are working at their full potential.  A single server can not boast. <br>  ‚Ä¢ We recommend using separate disk arrays for source OS images and user-defined virtual machines.  This makes it easier to administer disks, measure performance and track bottlenecks.  In addition, it will be a good basis for further scaling of the system. <br>  ‚Ä¢ We recommend to allocate disk arrays for roaming user profiles and network folders with shared access, as well as system servers and public services. </div><p>Source: <a href="https://habr.com/ru/post/261385/">https://habr.com/ru/post/261385/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../261371/index.html">Cryptography in the fight for the bright future of the Internet</a></li>
<li><a href="../261375/index.html">Analyzing unusual firmware: analysis of the contest Best Reverser</a></li>
<li><a href="../261379/index.html">How not to ruin the architecture right away? Video from the lecture of Evgeny Krivosheev</a></li>
<li><a href="../261381/index.html">Combinatorial testing: generation of test data and not only</a></li>
<li><a href="../261383/index.html">Do I need to participate in conferences?</a></li>
<li><a href="../261387/index.html">Budget SMS</a></li>
<li><a href="../261389/index.html">ES6 Browser Development</a></li>
<li><a href="../261391/index.html">As Asterisk opened barriers for me</a></li>
<li><a href="../261395/index.html">Free webinar "Choosing a Transportation Management Systems class information system"</a></li>
<li><a href="../261397/index.html">10 fatal mistakes of the usability of online stores and something else</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>