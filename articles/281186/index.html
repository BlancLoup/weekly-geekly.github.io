<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Computer vision and mobile robots. Part 1 - V-REP, Python, OpenCV</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When I read the publication about the launch of unmanned racing cars - I thought it would be interesting to do something like that. Certainly not a ra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Computer vision and mobile robots. Part 1 - V-REP, Python, OpenCV</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/547/d05/396/547d0539640842a4904a2b3a0c1a22a6.jpg"><br><br>  When I read the <a href="https://geektimes.ru/post/266626/">publication about the launch of unmanned racing cars</a> - I thought it would be interesting to do something like that.  Certainly not a racing car, but at least a mobile robot, which is oriented in space through the camera - by pattern recognition. <br><br>  In our hackspace - creating a robot is not such a big problem.  But not everyone and not always have the opportunity to experiment with real "iron" - so it was interesting to try to solve the problem - in a virtual environment, and then revive the "iron". 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      And so the idea of ‚Äã‚Äãa series of articles arose, about solving the simplest task of orienting a robot in space - from a virtual simulation to an embodiment in a real mobile robot: <br><br>  <b>Part 1</b> .  Setting up a virtual environment, integration with python and OpenCV for pattern recognition from the virtual world. <br>  <b>Part 2</b> .  Creating a virtual mobile robot, an algorithm for autonomous movement (object search) <br>  <b>Part 3</b>  Creating a real robot, transferring logic to it. <br><br>  I want to achieve this result - so that the python script that manages the virtual robot would be as identical as the one that will control the real robot. <br><br>  The brain of the robot will be the RaspberryPi2 microcomputer - on which both python and OpenCV work without problems.  Thus, it is necessary to interface the V-REP virtual robotics system with Python and OpenCV.  That's about it and will be the first part - this publication. <br><br><div class="spoiler">  <b class="spoiler_title">Video what happened (search for green object)</b> <div class="spoiler_text"><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/161906321&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjz-Ucus_OW06UXEEq8c0cNqWu6Aw" width="392" height="315" frameborder="0" title="V-REP API Python OpenCV demo" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe><br>  <i>In the upper window - a direct image from a video camera in the 3D virtual world, in the lower window - the result of the python script that receives the image passes it to OpenCV and draws a marker around the found object.</i> <br></div></div><br>  Let's draw the architecture of our mobile robot. <br><a name="habracut"></a><br>  <b>Mobile Robot Architecture</b> <br>  A real robot will look something like this (we'll add another camera) <br><img src="https://habrastorage.org/files/cfa/85e/38f/cfa85e38fce24cf488e533c9e1939c02.jpg"><br><br>  Architecturally: <br><img src="https://habrastorage.org/files/cee/5ce/668/cee5ce668b6f4a41b33648d8d288c24f.png"><br><br>  As we can see, the ‚Äúbrain‚Äù of the robot receives an image, recognizes (via OpenCV) and transmits control commands to the wheels.  And replacing the image from the camera with a 3D image, and controlling the wheels with the management of virtual wheels of a 3D robot in the virtual world, we will get a bench for working out the logic. <br><br>  We get the following architecture for a virtual stand: <br><img src="https://habrastorage.org/files/b6e/fb5/afb/b6efb5afb5db4be4afef653c448e2a3e.png"><br><br>  In our first part today - we need to solve the V-REP bundle problem with an external python script that performs pattern recognition using OpenCV, and displays a marker around the found object. <br><br>  It turns out this architecture: <br><img src="https://habrastorage.org/files/4ac/3f5/f97/4ac3f5f972804ceba9aee97522075a1d.png"><br><br><h5>  <b>Install V-REP</b> </h5><br>  There were several Russian-language publications about the free robo-simulation system <a href="http://www.coppeliarobotics.com/">V-REP</a> : <br><ul><li>  <a href="https://habrahabr.ru/company/makeitlab/blog/253357/">We program robots - free robosimulator V-REP.</a>  <a href="https://habrahabr.ru/company/makeitlab/blog/253357/">First steps</a> </li><li>  <a href="https://habrahabr.ru/post/268313/">Using the Remote API in the V-REP Robosimulator</a> , </li><li>  <a href="https://geektimes.ru/post/260370/">V-REP is a flexible and scalable platform for robotic modeling</a> . </li></ul><br>  Therefore, the installation and the first steps - you can see in them. <br><br><h5>  <b>Scene</b> </h5><br>  We believe that the V-REP is installed and working.  We need to create a scene. <br><br>  On the stage there will be a camera that looks at an area with different 3D objects, this image needs to be transmitted to an external python script, which should call OpenCV for recognition and mark generation, and return this image back to V-REP for control. <br><br>  Fortunately, among the examples that come with V-REP was a similar example, though he used ROS (which was not required for current tasks): <br><img width="800" src="https://habrastorage.org/files/622/543/c97/622543c971d840a1985f08054467b2bf.jpg"><br><br>  On the basis of this demo, the following scene appeared: <br><ul><li>  movable tripod that rotates </li><li>  v0 (vision sensor) is fixed on this tripod </li><li>  placed v1 sensor, the data to which is transmitted from the external system </li></ul><br>  The scene <i>scene.ttt</i> can be downloaded <a href="https://github.com/nemilya/vrep-api-python-opencv">from here</a> . <br><br><h5>  <b>Python API</b> </h5><br>  We believe that you have Python installed (tested on 2.7.6) <br><br>  When you start V-REP, the plugin automatically provides a link to the Python API.  To work with the API through the python script, you must have three files in the folder: <br><ul><li>  remoteApi.so (or remoteApi.dll) </li><li>  vrep.py </li><li>  vrepConst.py </li></ul><br>  They can be copied from the V-REP directory (programming / remoteApiBindings / python / python, programming / remoteApiBindings / lib / lib /).  You can then import the V-REP module: <br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> vrep</code> </pre> <br>  The simplest script that connects to the API looks like this: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> vrep vrep.simxFinish(<span class="hljs-number"><span class="hljs-number">-1</span></span>) <span class="hljs-comment"><span class="hljs-comment"># just in case, close all opened connections clientID = vrep.simxStart('127.0.0.1', 19997, True, True, 5000, 5) if clientID!=-1: print 'Connected to remote API server' while (vrep.simxGetConnectionId(clientID) != -1): print "some work" else: print "Failed to connect to remote API Server" vrep.simxFinish(clientID)</span></span></code> </pre><br><br>  On our stage there are two objects - v0 and v1 - this is a Visual Sensor - from the first we read the picture, on the second we have to record the result.  Therefore, we need to get these objects in the context of our python script, this is done using the <a href="http://www.coppeliarobotics.com/helpFiles/en/remoteApiFunctionsPython.htm">vrep.simxGetObjectHandle</a> API command <br><br><pre> <code class="python hljs"> res, v0 = vrep.simxGetObjectHandle(clientID, <span class="hljs-string"><span class="hljs-string">'v0'</span></span>, vrep.simx_opmode_oneshot_wait) res, v1 = vrep.simxGetObjectHandle(clientID, <span class="hljs-string"><span class="hljs-string">'v1'</span></span>, vrep.simx_opmode_oneshot_wait)</code> </pre><br>  Please note that all functions available through the API are also available through internal scripts (Lua), the only difference is that in the name instead of simx - sim, that is, in our case the function call on Lua will be ‚ÄúsimGetObjectHandle‚Äù. <br><br>  To get pictures and recordings from Vision Sensor, there are two functions: <a href="http://www.coppeliarobotics.com/helpFiles/en/remoteApiFunctionsPython.htm">vrep.simxGetVisionSensorImage</a> and <a href="http://www.coppeliarobotics.com/helpFiles/en/remoteApiFunctionsPython.htm">vrep.simxSetVisionSensorImage</a> respectively. <br><br>  And in python code it will look like this (where v0 and v1 are the corresponding objects): <br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment">#     v0 err, resolution, image = vrep.simxGetVisionSensorImage(clientID, v0, 0, vrep.simx_opmode_buffer) #     v1 vrep.simxSetVisionSensorImage(clientID, v1, image, 0, vrep.simx_opmode_oneshot)</span></span></code> </pre><br>  The only thing for the Vision Sensor that receives data from an external source, in the parameters you must set the appropriate flag: <br><img src="https://habrastorage.org/files/382/f88/fdc/382f88fdc4dc4d08bb943c69171b5cef.jpg"><br><br><h5>  <b>Image Relay</b> </h5><br>  Thus, now we can make a python script that, through the V-REP API, takes an image from the video sensor and retransmits it: <br><div class="spoiler">  <b class="spoiler_title">simple_image_retranslate.py</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># simple_image_retranslate.py import vrep import time vrep.simxFinish(-1) clientID = vrep.simxStart('127.0.0.1', 19997, True, True, 5000, 5) if clientID!=-1: print 'Connected to remote API server' # get vision sensor objects res, v0 = vrep.simxGetObjectHandle(clientID, 'v0', vrep.simx_opmode_oneshot_wait) res, v1 = vrep.simxGetObjectHandle(clientID, 'v1', vrep.simx_opmode_oneshot_wait) err, resolution, image = vrep.simxGetVisionSensorImage(clientID, v0, 0, vrep.simx_opmode_streaming) time.sleep(1) while (vrep.simxGetConnectionId(clientID) != -1): err, resolution, image = vrep.simxGetVisionSensorImage(clientID, v0, 0, vrep.simx_opmode_buffer) if err == vrep.simx_return_ok: vrep.simxSetVisionSensorImage(clientID, v1, image, 0, vrep.simx_opmode_oneshot) elif err == vrep.simx_return_novalue_flag: print "no image yet" else: print err else: print "Failed to connect to remote API Server" vrep.simxFinish(clientID)</span></span></code> </pre><br></div></div>  To check, we have to start the scene (the ‚ÄúPlay‚Äù button up), and then run the file simple_image_retranslate.py on the command line <br><br>  And here is the result (v1 - displays an image from the python script): <br><img src="https://habrastorage.org/files/7de/6ae/044/7de6ae04434449ccbf3d2d223f1acf5a.jpg"><br><br>  Ok, there is a basic workpiece, now you need to connect pattern recognition (computer vision) <br><br><h5>  <b>Computer vision, OpenCV</b> </h5><br>  I think everyone heard about the open computer vision system, <a href="http://opencv.org/">OpenCV</a> , which developed <br><div class="spoiler">  <b class="spoiler_title">Gary bradski</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/f88/6e5/36b/f886e536b6294f28a3dfd8c79c2083a4.jpg"><br>  Gary Bradski visited our Hackspace booth at Innoprome 2014 </div></div><br>  First, openCV should be installed, installing opencv2 on Linux Mint (Ubuntu) looked quite simple: <br> <code>sudo apt-get install libopencv-dev python-opencv</code> <br> <br>  After this, it became possible to connect the library in python code: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cv2</code> </pre><br>  Now you need to implement the simplest recognition function.  The base was to take <a href="https://github.com/simondlevy/OpenCV-Python-Hacks/blob/master/greenball_tracker.py">this example</a> , searching for the green ball in the picture. <br><br>  The <i>def</i> function <i>track_green_object (image) appeared</i> , which returns the coordinates of the green object, if found. <br><br>  And it is also necessary to note a marker around the found object, for this we use the basic OpenCV function to draw a rectangle: <a href="http://docs.opencv.org/2.4/modules/core/doc/drawing_functions.html">cv2.rectangle</a> . <br><br>  And our code snippet is that: <br><ol><li>  gets image (from v0) </li><li>  finds an object (something green) </li><li>  adds a marker (yellow rectangle) </li><li>  returns image (in v1) </li></ol><br>  looks like that: <br><pre> <code class="python hljs"> <span class="hljs-comment"><span class="hljs-comment"># get image from vision sensor 'v0' err, resolution, image = vrep.simxGetVisionSensorImage(clientID, v0, 0, vrep.simx_opmode_buffer) if err == vrep.simx_return_ok: image_byte_array = array.array('b', image) image_buffer = I.frombuffer("RGB", (resolution[0],resolution[1]), image_byte_array, "raw", "RGB", 0, 1) img2 = numpy.asarray(image_buffer) # try to find something green ret = track_green_object(img2) # overlay rectangle marker if something is found by OpenCV if ret: cv2.rectangle(img2,(ret[0]-15,ret[1]-15), (ret[0]+15,ret[1]+15), (0xff,0xf4,0x0d), 1) # return image to sensor 'v1' img2 = img2.ravel() vrep.simxSetVisionSensorImage(clientID, v1, img2, 0, vrep.simx_opmode_oneshot)</span></span></code> </pre><br><br>  <b>Result</b> <br><br>  Now it remains to start everything together: start the scene, run the script and see what happens: <br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://player.vimeo.com/video/161906321&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhjz-Ucus_OW06UXEEq8c0cNqWu6Aw" width="392" height="315" frameborder="0" title="V-REP API Python OpenCV demo" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe><br><br>  The source code of the project can be found on <a href="https://github.com/nemilya/vrep-api-python-opencv">github</a> . <br><br>  In the next part, we will create a robot in V-REP, and program it to search for a green ball. </div><p>Source: <a href="https://habr.com/ru/post/281186/">https://habr.com/ru/post/281186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../281172/index.html">Welcome to Moscow Atlassian Meetup April 26</a></li>
<li><a href="../281176/index.html">His name is Bot. Statsbot</a></li>
<li><a href="../281178/index.html">RAML 1.0: review of innovations</a></li>
<li><a href="../281180/index.html">Lists of actions: simple, flexible, extensible AI</a></li>
<li><a href="../281184/index.html">Seven things you need to know a novice programmer</a></li>
<li><a href="../281188/index.html">What's in git 2.8? Push, grep, rebase, config and other things</a></li>
<li><a href="../281190/index.html">Angular 2 Beta, training course "Tour of Heroes" part 1</a></li>
<li><a href="../281192/index.html">Bpm'online system overview</a></li>
<li><a href="../281194/index.html">A variety of versions of Microsoft SQL Server and which one is the latest?</a></li>
<li><a href="../281196/index.html">Data Lake - from theory to practice. Hadoop and Enterprise DWH data integration methods</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>