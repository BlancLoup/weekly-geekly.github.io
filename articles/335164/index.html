<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to program a satellite photo? Problem solving Dstl Satellite Imagery Feature Detection</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! My name is Yevgeny Nekrasov, I am a research programmer at Mail.Ru Group. Today I will talk about my decision to compete in the data analysi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to program a satellite photo? Problem solving Dstl Satellite Imagery Feature Detection</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/e98/7cf/d32/e987cfd3224536c5fbab12a71504da78.jpg"><br><br>  Hi, Habr!  My name is Yevgeny Nekrasov, I am a research programmer at Mail.Ru Group.  Today I will talk about my decision to compete in the data analysis of the <a href="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection">Dstl Satellite Imagery Feature Detection</a> , which was dedicated to the segmentation of satellite images.  In this competition, I used a relatively simple approach to modeling and took 7th place out of 419 teams.  Under the cut - the story of how I did it. <br><a name="habracut"></a><br>  Immediately I will give some introductory, in January 2017, I became the proud owner of the top-end GPU NVIDIA GeForce GTX 1080, and this opened up the opportunity for me to test my theoretical knowledge in <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep Learning</a> on real practical problems.  My choice fell on the competition from Dstl on the platform of Kaggle.  This task attracted me, firstly, by the unusual data - multispectral satellite images, and secondly, the opportunity to gain valuable practical experience in such an important area as satellite image processing.  In this article, the story will be primarily about data analysis techniques and machine learning.  Nevertheless, it would be wrong to completely ignore the technical details, so I‚Äôll briefly say that I wrote all the code in Python3 and used the following libraries: numpy, scipy, pandas, skimage, tifffile, shapely, keras with TensorFlow backend. <br><br><h2>  Formulation of the problem </h2><br><h3>  Data </h3><br>  The organizers provided images of 450 ground pieces 1x1 km.  These fragments were from one region of our planet.  Each fragment was captured by four <a href="http://worldview3.digitalglobe.com/">WorldView3</a> satellite sensors: an RGB sensor, a panchromatic sensor, a multispectral sensor, and an SWIR infrared sensor.  RGB- and panchromatic sensors, respectively, produce normal color and black-and-white images.  What multispectral and SWIR sensors are shown in Fig.  1. Thus, for each fragment 4 TIFF files were given, and they differed both in spatial resolution and dynamic range, the characteristics of the images are shown in Table 1. Of these 450 fragments, 25 were in the training set, to them was provided markup (image segmentation performed by specialists) for 10 classes of objects in the vector format WKT or GeoJSON.  The remaining 425 images compiled a test suite, for them it was necessary to build a similar markup for 10 classes in WKT format. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/web/8ad/a9c/b29/8ada9cb29ee643a69b342fcc6182662a.jpg"><br>  <i>Figure 1. Spectral ranges of a multispectral sensor and a SWIR sensor</i> <br><br><img src="https://habrastorage.org/web/aa2/8b0/d4f/aa28b0d4f8f34cfd9846bb65f56cdd28.png"><br>  <i>Table 1. Characteristics of data from the sensors of the WorldView3 satellite</i> <br><br>  Now more about 10 classes of objects, these were: <br><br><ol><li>  Buildings (buildings). </li><li>  Misc.  Manmade structures (artificial structures, mainly fences) </li><li>  Road (asphalted roads). </li><li>  Track (dirt roads). </li><li>  Trees (trees). </li><li>  Crops (agricultural fields). </li><li>  Waterway (river). </li><li>  Standing water (small ponds). </li><li>  Vehicle Large (trucks). </li><li>  Vehicle Small (cars). </li></ol><br>  The distribution of the areas of these classes was very uneven on the training marking (Fig. 2).  For clarity, here is an example of an RGB image from the training set (Fig. 3) and its markup (Fig. 4). <br><br><img src="https://habrastorage.org/web/4fe/e8d/2f2/4fee8d2f21e642bb8cbfd1f63f0b9313.jpg"><br>  <i>Figure 2. Histogram of shares of areas of classes of objects relative to the total area of ‚Äã‚Äãthe fragment of the earth's surface.</i>  <i>1 - Buildings, 2 - Misc.</i>  <i>Manmade structures, 3 - Road, 4 - Track, 5 - Trees, 6 - Crops, 7 - Waterway, 8 - Standing water, 9 - Vehicle Large, 10 - Vehicle Small</i> <br><br><img src="https://habrastorage.org/web/195/fe6/be1/195fe6be18a5423db821d5f1729edf0b.jpg"><br>  <i>Figure 3. Sample RGB image from the training set</i> <br><br><img src="https://habrastorage.org/web/6e3/190/ad8/6e3190ad896045ce920d91f4e6273b3f.jpg"><br>  <i>Figure 4. Sample layout from the training set.</i>  <i>Red - Buildings, orange - Misc.</i>  <i>Manmade structures, gray - Road, yellow - Track, dark green - Trees, light green - Crops, blue - Waterway, blue - Standing water, purple - Vehicle Large, pink - Vehicle Small</i> <br><br>  The quality metric in the competition was Jaccard (Fig. 5), averaged over all 10 classes.  The organizers assessed the quality of the decisions and determined the winners exclusively by this metric, and for the final assessment, which became known only after the competition was over, the organizers used 81% of the test images (Private leaderboard), the remaining 19% of the test images were Public leaderboard and allowed participants to instantly receive preliminary assessment of the quality of their decisions. <br><br><img src="https://habrastorage.org/web/974/c89/ee6/974c89ee68b04304840a171fe10b51e4.jpg"><br>  <i>Figure 5. An illustration of the Jaccard metric</i> <br><br><h2>  The solution of the problem </h2><br><h3>  Preprocessing </h3><br>  First you need to bring the data to a form suitable for modeling.  I did the preprocessing like this: all four satellite images were scaled to the size of the RGB image (~ 3300x3300 pixels), since it has the highest spatial resolution.  Then, each image was normalized to the maximum of the dynamic range, so that the brightness values ‚Äã‚Äãof the pixels were strictly in the range [0, 1], and then combined into a single 20-channel image.  I projected vector marking into raster binary masks, which corresponded in size to a 20-channel image.  Transformations of vector marking into raster masks and back I performed using the skimage and shapely libraries. <br><br><h3>  Modeling </h3><br>  After preprocessing, we get the formulated problem of image segmentation: the training set consists of 25 20-channel images, these images have pixel-by-pixel marking for 10 classes, the test set consists of 425 20-channel images, for which we need to build similar binary masks, which then can be easily converted to vector markup in WKT format - this is what the organizers want. <br><br>  For the problem of image segmentation, one of the best models is the convolutional neural network of the <a href="https://arxiv.org/abs/1505.04597">U-net</a> architecture.  The U-net is very similar in structure to the autoencoder, but with one difference: it has the connection of the corresponding parts of the encoder and decoder.  The autoencoder part forms a high-level view of the image, and the connections allow the network to effectively segment small parts. <br><br>  I used two U-net-like artificial neural networks of almost identical architecture (Fig. 6).  The first neural network (2c) was sharpened by the segmentation of two extremely rare classes ‚Äî Vehicle Large and Vehicle Small (Fig. 2).  The second neural network (7c) is sharpened for all other classes, and I combined Waterway and Standing water into one class, and there are two reasons for this: first, in fact, Waterway and Standing water are one class - water, and second, There are very few reservoirs in the training set, so it is almost impossible to learn the difference between them from scratch of an artificial neural network, where it is better to divide the already predicted reservoirs into two classes. <br><br>  The input of both neural networks was 160 by 160 pixels.  I got the magic number 160x160 as follows: the larger the field of view of the artificial neural network, the better the neural network can understand the context in which the observed objects are, but with an increase in the neural network, the complexity of the model increases and, accordingly, the time for training and prediction increases.  I looked through the satellite images through fields of view of various sizes and realized that the field of view of 160x160 is sufficient to understand the context in this task. <br><br><img src="https://habrastorage.org/web/a73/d28/b62/a73d28b6265b42f3a818c2f0335da452.png"><br>  <i>Figure 6. The architecture of neural networks 2c and 7c.</i>  <i>In the last layer 7c there were 7 channels at the output, and 2c - 2 channels</i> <br><br><h3>  Training neural networks </h3><br>  With the training of neural networks in this task, not everything is so simple, because in the training set only 25 images are only about 10 thousand not intersecting image fragments (sprinkles) 160x160 pixels, which is very small and does not allow to fully realize the potential of 2c and 7c networks.  Therefore, I used techniques that allow us to solve the problem of a small amount of training data - this is training with partial involvement of the teacher ( <i>semi-supervised learning</i> ) and the expansion of the training set ( <i>data augmentation</i> ).  To train both networks, I used binary cross-entropy as a function of loss, that is, taught the network to predict the probability of objects in each pixel, and optimized the weights of neural networks by the <a href="https://arxiv.org/abs/1412.6980v8">Adam</a> optimizer.  In the process of training both neural networks, I used training on so-called rotational crocs - image fragments of 160x160 pixels cut from an image with random displacements and turns at a random angle (Fig. 7).  This allows us to expand the training set due to our a priori knowledge of the rotational invariance of satellite images, that is, if we rotate the satellite image at an arbitrary angle, then we will get a valid satellite image. <br><br><img src="https://habrastorage.org/web/21b/1c0/e6f/21b1c0e6f497471ca5837cc6217ae453.png"><br>  <i>Figure 7. Sample rotation pattern</i> <br><br>  I will begin the story of the training procedure from the 2c network.  Classes of cars and trucks are very rare, so at first I trained a neural network on 200 thousand rotational springs with such a sampling that a car or truck was present in a crop with a probability of about 50%.  This was necessary in order for the neural network to get an idea of ‚Äã‚Äãwhat constitutes a car or a truck.  Then he trained a neural network on 700 thousand rotational spikes with random sampling so that the network would form an adequate idea of ‚Äã‚Äãthe whole dataset as a whole. <br><br>  For network 7c, I used a more complex approach, applying training with partial involvement of the teacher.  The fact is that although we only have 25 marked satellite images at our disposal, we have been given 450 satellite images altogether, and this whole set can be used so that the neural network can learn the concept of what are satellite images in general.  I built an autoencoder, which in its structure corresponds to the 7c network and trained it on 600 thousand common crops from all 450 images.  Then I transferred the weights of the encoder part of the trained autoencoder into the 7c neural network and fixed them.  He trained a network of 400 thousand rotary crop.  He released the weights of the encoder part and finished training the neural network for another 600 thousand rotational springs. <br><br><h3>  Prediction </h3><br>  In order to fulfill the prediction, I traversed the image using a ‚Äúsliding window‚Äù, that is, I cut out 160x160 pixels from the image, performed predictions on them with neural networks 2c and 7c and collected the original image from these fragments.  Where it was possible, I used only the central part of the crop predicted by the neural network (Fig. 8) for reconstruction, since the prediction quality is most likely lower at the edges.  I performed the ‚Äúsliding window‚Äù passage from each corner of the image, then I averaged the predictions and obtained the final predictions of the probabilities of each class of objects at each point of the image. <br><br><img src="https://habrastorage.org/web/68e/6f1/dec/68e6f1dec09f4fc88d84c574c6953e49.png"><br><img src="https://habrastorage.org/web/760/05c/051/76005c0519144a82a35acd39a0d91cad.png"><br>  <i>Figure 8. Prediction scheme.</i>  <i>Red - predictions from the central cropping area, yellow - predictions from the cropping edge region, green - no predictions</i> <br><br>  However, we need not probabilities, but binary masks.  The easiest way to build them: discretization at the threshold of 0.5.  But I used a more advanced method: I performed the prediction on the training set of images, and then I maximized Jaccard on the entire training set relative to the sampling threshold.  As a result, we obtained values ‚Äã‚Äãthat often differed significantly from 0.5 (Table 2).  The reader may be asked whether there will be <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">retraining</a> here.  Neural networks were trained on rotational crocs, and those that were fed into the neural network at the prediction stage could get into the training set with a very small probability, so this is a more or less adequate approach. <br><br><img src="https://habrastorage.org/web/01c/0b0/f17/01c0b0f17d6948a784b142ba758ca355.png"><br>  <i>Table 2. Sampling thresholds for different classes of objects</i> <br><br>  In forming the reservoir prediction, I also used the model for predicting the water of <a href="https://www.kaggle.com/resolut">Vladimir Osin</a> , which he <a href="https://www.kaggle.com/resolut/waterway-0-095-lb">laid out in an open access forum of the competition</a> .  It is very simple, based on the Canopy Chlorophyl Content Index (CCCI).  The index is considered as a combination of the intensity of some channels and allows for the segmentation of water to be efficiently thresholded.  I combined my predictions of reservoirs with the results of the work of Vladimir Osin‚Äôs model, since these are essentially very different models, and as a result, their combination resulted in a visible improvement in the quality of reservoir segmentation. <br><br>  Next, it was necessary to divide the predicted water into the Waterway and Standing Water classes.  Here I used the classical methods of computer vision and a priori knowledge of the differences between rivers and small ponds.  For each area of ‚Äã‚Äãwater, I considered the parameters that should be effective in the separation of these types of water bodies: <br><br><ul><li>  area (rivers are usually larger than ponds by area); </li><li>  ellipticity (stretched rivers, ponds close to circumference); </li><li>  touching the borders of a photograph (rivers are usually longer than 1 km, and therefore cross the boundaries of a photograph, the probability of being on the boundary of a photograph for a pond is small). </li></ul><br>  Further, from these parameters I made a linear combination and divided the water classes along the threshold. <br><br>  So, now we have binary masks for all 10 classes, the problem is solved.  Then all that remained was to perform the technical procedures; it was necessary to vectorize the resulting binary masks and save them in the WKT format.  I give an example of an image from the test suite (Fig. 9) and its segmentation by the model (Fig. 10). <br><br><img src="https://habrastorage.org/web/8e1/089/b4e/8e1089b4e78343e7975be88fb62fe4cc.jpg"><br>  <i>Figure 9. Sample RGB image from the test suite</i> <br><br><img src="https://habrastorage.org/web/419/30a/58f/41930a58fccb41dc939ea1371b8c4e7b.jpg"><br>  <i>Figure 10. An example of a model prediction in an image from a test suite.</i>  <i>Red - Buildings, orange - Misc.</i>  <i>Manmade structures, gray - Road, yellow - Track, dark green - Trees, light green - Crops, blue - Waterway, blue - Standing water, purple - Vehicle Large, pink - Vehicle Small</i> <br><br><h2>  Conclusion </h2><br>  The described solution gives Public Score 0.51725, Private Score 0.43897, and this is the 7th place according to the results of the competition.  Here are the key elements of the solution that allowed me to achieve such high results: <br><br><ol><li>  U-Net architecture, now it‚Äôs a state of art for image segmentation. </li><li>  The use of techniques that allowed to work effectively in a small amount of training data.  This is training on rotational crocs and the use of test images in the process of learning neural networks. </li><li>  The use of classical methods of computer vision and a priori knowledge for the separation of classes of reservoirs. </li></ol><br>  This is not a complete list of ideas and approaches that have worked in this competition, you can still learn a lot of interesting ideas from the top published solutions: <br><br><ul><li>  <a href="https://habrahabr.ru/company/ods/blog/325096/">decision of the team of Vladimir Iglovikov and Sergey Mushinsky (3rd place)</a> </li><li>  <a href="https://habrahabr.ru/company/avito/blog/325632/">the decision of the team of Roman Solovyov and Artur Kuzin (2nd place)</a> </li><li>  <a href="http://blog.kaggle.com/2017/04/26/dstl-satellite-imagery-competition-1st-place-winners-interview-kyle-lee/">Kyle Lee solution (1st place)</a> </li></ul><br>  Thanks for attention. </div><p>Source: <a href="https://habr.com/ru/post/335164/">https://habr.com/ru/post/335164/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335154/index.html">Mobile applications: how to avoid a ban from the advertising network?</a></li>
<li><a href="../335156/index.html">The digital economy should be digital</a></li>
<li><a href="../335158/index.html">Comparing REST and GraphQL</a></li>
<li><a href="../335160/index.html">Programming ‚â† computer science</a></li>
<li><a href="../335162/index.html">Polymorphism and function pointers</a></li>
<li><a href="../335166/index.html">Very easy monitoring system with Telegram and Consul</a></li>
<li><a href="../335168/index.html">How to set up backup to S3-compatible cloud from Commvault</a></li>
<li><a href="../335170/index.html">How we look for (and why we find) cool developers. HR experience and tips for job seekers</a></li>
<li><a href="../335172/index.html">Attack of the clones. How to deal with code duplication?</a></li>
<li><a href="../335174/index.html">Smart stores, omni-channel analytics and endless loyalty: 6 retail trends in 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>