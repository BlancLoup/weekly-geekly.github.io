<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Parsing on Puthon. How to collect the archive Dovecoat</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article describes the development of a script in the language of Python . The script parses HTML-code , compiles a list of site materials, downlo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Parsing on Puthon. How to collect the archive Dovecoat</h1><div class="post__text post__text-html js-mediator-article">  This article describes the development of a script in the language of <b>Python</b> .  The script parses <b>HTML-code</b> , compiles a list of site materials, downloads articles and pre-cleans the text of the article from "extraneous" elements.  The libraries used are <b>urllib</b> (getting HTML pages), <b>lxml</b> (parsing HTML code, deleting elements and saving ‚Äúcleaned‚Äù article), <b>re</b> (working with regular expressions), <b>configobj</b> (reading configuration files). <br><br>  For writing a script, basic knowledge of Python, programming skills and debugging code is sufficient. <br><br>  The article provides explanations on the use of libraries on the example of compiling a list of SM publications.  Golubitsky, a link to a working script. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Preface or some lyrics </h4><br>  I can hardly be mistaken when I say that many habrazhiteli are familiar with the <a href="http://lurkmore.ru/%25C3%25EE%25EB%25F3%25E1%25E8%25F6%25EA%25E8%25E9">irrepressible</a> creative work of <a href="http://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BE%25D0%25BB%25D1%2583%25D0%25B1%25D0%25B8%25D1%2586%25D0%25BA%25D0%25B8%25D0%25B9,_%25D0%25A1%25D0%25B5%25D1%2580%25D0%25B3%25D0%25B5%25D0%25B9_%25D0%259C%25D0%25B8%25D1%2585%25D0%25B0%25D0%25B9%25D0%25BB%25D0%25BE%25D0%25B2%25D0%25B8%25D1%2587">Sergei Golubitsky</a> .  For almost 15 years of computer-related journalism, imenire issued 433 articles on the mountain in the untimely deceased in the Bose paper Computer and over 300 Dovecotes on the Computerra online portal.  And this is not counting analytical studies on the heroes of foreign geshefts in the Business Journal, opening the veil over the secrets of creativity in the Home Computer, articles in the Russian Journal, D` and so on.  and so on  Aspiring to the completeness of life-creativity reviews, those interested will find them through the links above. <br><br>  Last year, the author‚Äôs project ‚ÄúThe <a href="http://www.sgolub.ru/">Old Pigeon and His Friends</a> ‚Äù began its work, which was conceived (and became) in particular by a constantly growing archive of the publications of the author himself and a platform for conducting cultural and Lid discussions.  As a person who is not indifferent to the themes of network life, social mythology and self-development that the author discovers brilliantly, as well as eager for quality leisure reading, he once became a frequenter of gatherings at Dovecote and me.  As far as I can, I try not only to keep the project in sight, but also to somehow participate in its development. <br><br>  Getting involved in the field of proofreading of articles transferred to the archive from the Computerra-online portal, the first thing I decided to make an inventory of all Pigeonlings. <br><br><h4>  Formulation of the problem </h4><br>  So, the task, by the example of which we will consider parsing sites on Python, was as follows: <br><ul><li>  Make a list of all Dovecots posted on Computerra online.  The list should include the title of the article, the date of publication, information about the content of the article (text only, images, video), Synopsis, a link to the source. </li><li>  Add a list of materials published in the paper Computerra, find duplicates. </li><li>  Add a list of materials from the archive of the site Internettrading.net </li><li>  Download a list of articles already published on the portal ‚ÄúOld Dovecote‚Äù </li><li>  Download articles to a local disk for further processing, if possible, automatically clearing text from unnecessary elements. </li></ul><br><h4>  Toolkit selection </h4><br>  In a part of the programming language, my choice immediately and definitely fell on Python.  Not only because I studied it at my leisure a few years ago (and then for some time I used the Active Python shell as an advanced calculator), but also for the abundance of libraries, examples of source code and the simplicity of writing and debugging scripts.  Not least interested in the prospects for the further use of these skills to solve immediate tasks: integration with the Google Docs API, automation of word processing, etc. <br><br>  Solving a very specific task practically from scratch, the toolkit was selected in such a way as to spend the minimum time on reading documentation, comparing libraries and, ultimately, implementation.  On the other hand, the solution should have universality sufficient for easy adaptation to other sites and similar tasks.  Perhaps some tools and libraries are imperfect, but they eventually allowed to complete the plan. <br><br>  So, the choice of tools began to determine the appropriate version of Python.  Initially I tried to use Python 3.2, but during the experiments I stopped at <a href="http://www.python.org/download/">Python 2.7</a> , because  Some examples on the ‚Äútroika‚Äù did not go. <br><br>  To simplify the installation of additional libraries and packages, I used setuptools, a tool for downloading, building and installing packages. <br><br>  Additionally libraries were installed: <br><ul><li>  <a href="http://docs.python.org/release/2.7/library/urllib.html">urllib</a> - getting HTML pages of sites; </li><li>  <a href="">lxml</a> is a library for parsing XML and HTML code; </li><li>  <a href="">configobj</a> is a library for reading configuration files. </li></ul><br>  As improvised means were used: <br><ul><li>  <a href="http://notepad-plus-plus.org/">Notepad ++</a> is a text editor with syntax highlighting: </li><li>  <a href="http://getfirebug.com/">FireBug</a> - FireFox browser plugin that allows you to view the source code of HTML pages </li><li>  <a href="https://addons.mozilla.org/en-US/firefox/addon/firepath/">FirePath</a> - FireFox browser plugin for analyzing and testing XPath: </li><li>  Built-in Python GUI for debugging code. </li></ul><br>  Invaluable assistance provided articles and discussions on Habr√©: <br><ul><li>  <a href="http://habrahabr.ru/blogs/data_mining/99918/">Approaches to extracting data from web resources</a> </li><li>  <a href="http://habrahabr.ru/blogs/webdev/114772/">Examples of xpath requests to html</a> </li><li>  <a href="http://habrahabr.ru/blogs/python/114788/">"LXML" or how to parse HTML with ease</a> </li><li>  <a href="http://habrahabr.ru/blogs/python/116745/">What library you parse sites</a> </li><li>  <a href="http://habrahabr.ru/blogs/python/114503/">Easy parsing of sites using ‚ÄúBeautiful Soup‚Äù</a> </li></ul><br>  As well as manuals, examples and documentation: <br><ul><li>  <a href="http://ru.wikisource.org/wiki/%25CF%25EE%25E3%25F0%25F3%25E6%25E5%25ED%25E8%25E5_%25E2_Python_3_%2528%25CF%25E8%25EB%25E3%25F0%25E8%25EC%2529/XML">Python 3 Immersion (Pilgrim) / XML</a> </li><li>  <a href="http://lxml.de/api/index.html">Package lxml.</a>  <a href="http://lxml.de/api/index.html">API reference</a> </li><li>  <a href="http://msdn.microsoft.com/ru-ru/library/ms256086%2528v%3DVS.90%2529.aspx">XPath examples in MSDN library</a> </li><li>  <a href="http://docs.python.org/release/3.1.3/library/xml.etree.elementtree.html">The ElementTree XML API in the Pyton Documentation</a> </li><li>  <a href="http://lxml.de/tutorial.html">The lxml.etree Tutorial</a> </li></ul><br>  And, of course, the book <a href="http://www.python.ru/files/book-ods.pdf">Python Programming Language</a> <br><br><h4>  Solution Overview </h4><br>  The task includes four similar procedures for downloading materials from four different sites.  Each of them has one or several pages with a list of articles and links to the material.  In order not to spend a lot of time on the formalization and unification of the procedure, a basic script was written, on the basis of which, each site developed its own script, taking into account the peculiarities of the structure of the list of materials and the composition of HTML pages.  So, parsing materials on Internettrading.net, where HTML was apparently formed by hand, required a lot of additional checks and page parsing scripts, while those formed by CMS Drupal (‚ÄúOld Dovecote and His Friends‚Äù) and Bitrix (‚ÄúComputerra Online‚Äù, archives paper Computerra) pages contained a minimum of features. <br><br>  In the future, I will refer to the details of the historically most recent script parsing the portal of the Old Dovecote. <br><br>  The list of articles is displayed in the section ‚Äú <a href="http://www.sgolub.ru/blogpg/protograf">Prograph</a> ‚Äù.  Here is the title, link to the article and synopsis.  The list is divided into several pages.  You can go to the next page by changing the parameter in the address line in a loop (? Page = n), but it seemed to me smarter to get the link to the next page from the HTML text. <br><br>  On the article page there is a publication date in DD format Month YYYY, its own text and an indication of the source in the signature. <br><br>  To work with different data types, two objects were created: <i>MaterialList (object)</i> - list of articles (contains a method of parsing a separate page of the <i>_ParseList</i> list and a method of obtaining the URL of the next page <i>_GetNextPage</i> , stores a list of materials and their identifiers) and <i>Material (object)</i> - the article itself ( contains a method for generating an identifier based on the <i>_InitID</i> date, a method of parsing the <i>_ParsePage</i> page, a method for determining the source of the <i>_GetSection</i> publication <i>,</i> and article attributes such as the publication date, type of material, etc.) <br><br>  In addition, functions for working with elements of the document tree are defined: <br><ul><li>  <i>get_text (item, path)</i> - getting the text of the item along the <i>path</i> in the <i>item</i> document </li><li>  <i>get_value (item)</i> - getting node text in <i>item</i> </li><li>  <i>get_value_path (item, path)</i> - getting node text in <i>item</i> document along <i>path</i> </li><li>  <i>get_attr_path (item, path, attr)</i> - getting the attribute of an element along the <i>path</i> in the <i>item</i> document </li></ul><br>  And the function <i>get_month_by_name (month)</i> , which returns the month number by its name for parsing the date. <br><br>  The main code (the <i>main ()</i> procedure) contains the loading of the configuration from a file, a walk through the pages of the list of materials, loading the contents into memory and further saving to the files both the list itself (in CSV format) and the text of articles (in HTML, the file name is formed on based on material identifier). <br><br>  The configuration file stores the URL of the start page of the content list, all XPath paths for the content pages and the list of articles, file names and directory path for saving articles. <br><br><h4>  Implementation details </h4><br>  In this part, I will discuss the main points of the code, one way or another caused difficulties or smoking manuals. <br><br>  To simplify the debugging of paths inside documents and to make the code easier to read, all XPaths are moved to a separate configuration file.  The <b>configobj</b> library was quite suitable for working with the configuration file.  The configuration file has the following structure: <br> <code># Comment <br> [ Section_1 ] <br> # Comment <br> variable_1 = value_1 <br> # Comment <br> variable_2 = value_2 <br> [[Subsection_1]] <br> variable_3 = value_3 <br> [[Subsection_2]] <br> [ Section_2 ]</code> <br> <br>  Nesting of subsections can be arbitrary, comments to sections and specific variables are allowed.  An example of working with a configuration file: <br><br> <code>from configobj import ConfigObj <br> <br> #    <br> cfg = ConfigObj('sgolub-list.ini') <br> #    url   sgolub <br> url = cfg['sgolub']['url'] <br></code> <br><br>  Loading html-page is implemented using the library <b>urllib</b> .  With the help of lxml we convert the document into a tree and fix the relative links: <br><br> <code>import urllib <br> from lxml.html import fromstring <br> <br> #  html-   <br> html = urllib.urlopen(url).read(); <br> <br> #     lxml.html.HtmlElement <br> page = fromstring(html) <br> <br> #        <br> page.make_links_absolute(url) <br></code> <br><br>  When parsing the list of publications, we need to loop through all the elements of the list.  The <i>lxml.html.HtmlElement.findall (path)</i> method is suitable for <i>this</i> .  For example: <br><br> <code>for item in page.findall(path): <br> url = get_attr_path(item,cfg['sgolub']['list']['xpath_link'],'href') <br></code> <br><br>  Now is the time to make a comment about the FirePath plugin and its use to build XPath.  Indeed, as already mentioned on Habr√©, FirePath provides paths that differ from paths in lxml.  Slightly, but there is a difference.  Pretty soon, these differences were revealed and further used with FirePath as amended, for example, replacing the tbody tag with * (the most common problem).  At the same time, paths corrected in this way can be checked in FirePath, which significantly speeds up the process. <br><br>  While <code>page.findall(path)</code> returns a list of items, a find (path) method exists to get a single item.  For example: <br><br> <code>content = page.find(cfg['sgolub']['doc']['xpath_content'])</code> <br> <br>  The find and findall methods work only with simple paths that do not contain logical expressions in conditions, for example: <br><br> <code>xpath_blocks = './/*[@id='main-region']/div/div/div/table/*/tr/td' <br> xpath_nextpage = './/*[@id='main-region']/div/div/div/ul/li[@class="pager-next"]/a[@href]' <br></code> <br><br>  To use more complex conditions, such as <br> <code>xpath_purifytext = './/*[@id="fin" or @class="info"]'</code> <br>  You will need the xpath (path) method, which returns a list of elements.  Here is a sample code that cleans up selected elements from the tree (I still don‚Äôt understand how this magic works, but the elements are really removed from the tree): <br><br> <code>from lxml.html import tostring <br> <br> for item in page.xpath(cfg['computerra']['doc']['xpath_purifytext']): <br> item.drop_tree() <br> text=tostring(page,encoding='cp1251') <br></code> <br>  This fragment also uses the lxml.html.tostring method, which saves the tree (without any extra elements!) Into a string in the specified encoding. <br><br>  In conclusion, I will give two examples of working with the <b>re</b> regular expression library.  The first example implements a date parsing in the format "DD Month YYYY": <br><br> <code>import re <br> import datetime <br> <br> # content   lxml.html.HtmlElement <br> #    ,    <br> datestr=get_text(content,cfg['sgolub']['doc']['xpath_date']) <br> if len(datestr)&gt;0: <br> datesplit=re.split('\s+',datestr,0,re.U) <br> self.id = self._InitID(list,datesplit[2].zfill(4)+str(get_month_by_name(datesplit[1])).zfill(2)+datesplit[0].zfill(2)) <br> self.date = datetime.date(int(datesplit[2]),get_month_by_name(datesplit[1]),int(datesplit[0])) <br> else: <br> self.id = self._InitID(list,list.lastid[0:8]) <br> self.date = datetime.date(1970,1,1) <br></code> <br><br>  The <i>re.split</i> function <i>(regexp, string, start, options) is used</i> , which forms a list of line elements separated by a specific mask (in this case, by a space).  The <i>re.U</i> option allows <i>you</i> to work with strings containing Russian characters in Unicode.  The <i>zfill (n)</i> function finishes a string with zeros from the left to the specified number of characters. <br><br>  The second example shows how to use regular expressions to search for substrings. <br><br> <code>def _GetSection(item, path): <br> #     <br> reinfo = re.compile(r'.*¬´(?P&lt;gsource&gt;.*)¬ª.*',re.LOCALE) <br> for info in item.xpath(path): <br> src=get_value_path(info,'.').strip('\n').strip().encode('cp1251') <br> if src.startswith(' '): <br> parser = self.reinfo.search(src) <br> if parser is not None: <br> if parser.group('gsource')=='-': <br> return '-' <br> else: <br> return parser.group('gsource') <br> break <br> return '' <br></code> <br><br>  In the given example, the code of the <i>_GetSection (item, path)</i> function is shown to which the subtree is passed, indicating the source of the publication, for example, ‚ÄúFirst published in the Business Journal‚Äù.  Notice the fragment of the regular expression <i>? P &lt;gsource&gt;</i> .  <i>When placed</i> in brackets, it allows you to define named groups in a string and access them using <i>parser.group ('gsource')</i> .  The <i>re.LOCALE</i> option <i>is</i> similar to <i>re.U.</i> <br><br>  The source code of the parser is laid out in <a href="https://docs.google.com/leaf%3Fid%3D0B1xYvhbTXgpYODg2M2U3ZGEtNWQ2MS00YzgyLThmOTQtZDZmZDUyYjZkNTE4%26hl%3Den_US%26authkey%3DCIadv_0F">Google Docs</a> .  To save the site of the Old Dovecote from the flow of parsers, I post only the code, without the configuration file with links and paths. <br><br><h4>  Conclusion </h4><br>  The result of the application of technology was the archive of articles from four sites on the hard disk and lists of all publications of Pigeonfish.  The lists were manually uploaded to the Google Docs table, articles from the archive are also transferred manually for editing into Google documents. <br><br>  Plans for solving problems: <br><ul><li>  Writing a service that automatically tracks new posts </li><li>  Integration with the Google Docs API to automatically add new publications to the list </li><li>  Converting archived articles from HTML to XML format with automatic correction of some errors and uploading to Google Docs </li></ul><br><br>  PS Many thanks to all for the comments, support and constructive criticism.  I hope that most of the comments will be useful to me in the future after careful study. </div><p>Source: <a href="https://habr.com/ru/post/121815/">https://habr.com/ru/post/121815/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../121809/index.html">Phing - build PHP projects</a></li>
<li><a href="../121810/index.html">Let's talk about the margin, he's margin (part 1)</a></li>
<li><a href="../121811/index.html">Little-Endian vs Big-Endian</a></li>
<li><a href="../121812/index.html">Founder of HH.ru Yuri Virovac personally about recruiting, prospects, investments ...</a></li>
<li><a href="../121813/index.html">Microsoft introduces online advertising in xbox 360</a></li>
<li><a href="../121816/index.html">Why I love and hate using a smartphone - The Oatmeal (translation)</a></li>
<li><a href="../121818/index.html">Code Generation for Programmable Logic Controllers in Matlab</a></li>
<li><a href="../121819/index.html">Refinement of Chinese acoustics (SVEN SPS-678)</a></li>
<li><a href="../121820/index.html">Panasonic Lumix GF3 Review</a></li>
<li><a href="../121824/index.html">Install iOS 5. Activate UDID. Success story</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>