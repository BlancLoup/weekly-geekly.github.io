<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommendations for configuring AFA AccelStor when working with VMware vSphere</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, I would like to talk about the features of the operation of All Flash AccelStor arrays with one of the most popular virtualization pl...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommendations for configuring AFA AccelStor when working with VMware vSphere</h1><div class="post__text post__text-html js-mediator-article"><p>  In this article, I would like to talk about the features of the operation of All Flash AccelStor arrays with one of the most popular virtualization platforms - VMware vSphere.  In particular, focus on those parameters that will help get the maximum effect from the use of such a powerful tool as All Flash. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/di/xc/nq/dixcnqzpdskxe4whal3u7wyxcnw.jpeg"></div><a name="habracut"></a><br><p>  All Flash AccelStor NeoSapphire ‚Ñ¢ arrays are <a href="http://accelstor.ru/page/professional-models">one</a> or <a href="http://accelstor.ru/page/modeli-vysokoj-dostupnosti-ha">two</a> node devices based on SSDs with a fundamentally different approach to implementing the concept of storing data and accessing it using proprietary <a href="http://accelstor.ru/page/flexiremap-technology">FlexiRemap¬Æ</a> technology instead of the very popular RAID algorithms.  Arrays provide block access for hosts via Fiber Channel or iSCSI interfaces.  In fairness, we note that models with an iSCSI interface also have file access as a nice bonus.  But within this article we will focus on the use of block protocols as the most productive for All Flash. </p><br><p>  The whole process of deployment and subsequent configuration of the joint operation of the AccelStor array and the VMware vSphere virtualization system can be divided into several stages: </p><br><p></p><ul><li>  Implementation of the connection topology and SAN network configuration; </li><li>  Setting All Flash array; </li><li>  Configure ESXi hosts; </li><li>  Configure virtual machines. </li></ul><br><p>  AccelStor NeoSapphire ‚Ñ¢ with Fiber Channel interface and iSCSI interface were used as equipment for the examples.  As base software - VMware vSphere 6.7U1. </p><br><p>  Before deploying the systems described in this article, it is highly recommended that you read VMware documentation on performance issues ( <a href="https://www.vmware.com/content/dam/digitalmarketing/vmware/en/pdf/techpaper/performance/vsphere-esxi-vcenter-server-67-performance-best-practices.pdf">Performance Best Practices for VMware vSphere 6.7</a> ) and iSCSI settings ( <a href="https://storagehub.vmware.com/export_to_pdf/best-practices-for-running-vmware-vsphere-on-iscsi">Best Practices For Running VMware vSphere On iSCSI</a> ). </p><br><h3>  <i><b>Connection topology and SAN network configuration</b></i> </h3><br><p>  The main components of a SAN network are HBA adapters in ESXi hosts, SAN switches and array nodes.  A typical topology of such a network would look like this: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ci/wt/lg/ciwtlgk31rhm06xplvzbbwkxu0y.png"></div><br><p>  The term Switch means here a separate physical switch or a set of switches (Fabric), as well as a device shared between different services (VSAN in the case of Fiber Channel and VLAN in the case of iSCSI).  The use of two independent switches / Fabric will eliminate the possible point of failure. </p><br><p>  Direct connection of hosts to the array, although supported, is highly discouraged.  The performance of All Flash arrays is quite high.  And for maximum speed it is required to use all the ports of the array.  Therefore, at least one switch between the hosts and NeoSapphire ‚Ñ¢ is required. </p><br><p>  Having two ports on a host HBA is also a requirement for maximum performance and fault tolerance. </p><br><p>  If you use the Fiber Channel interface, you need to configure zoning to eliminate possible conflicts between initiators and targets.  Zones are built on the principle ‚Äúone port of initiator - one or several ports of the array‚Äù. </p><br><p>  If an iSCSI connection is used in case of using a switch shared with other services, then it is imperative that you isolate iSCSI traffic within a separate VLAN.  It is also highly recommended to include support for Jumbo Frames (MTU = 9000) to increase the size of packets in the network and, thereby, reduce the amount of service information during transmission.  However, it is worth remembering that in order to work correctly, it is necessary to change the MTU parameter on all components of the network along the initiator-switch-target chain. </p><br><h3>  <i><b>Configuring All Flash Array</b></i> </h3><br><p>  The array is delivered to customers with already formed <a href="http://accelstor.ru/page/flexiremap-technology">FlexiRemap¬Æ</a> groups.  Therefore, no action to unite the drives into a single structure is not necessary.  It is enough to create volumes of the required size and quantity. </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/zn/x9/nc/znx9ncuyd2rgm8-jfvwf620zaqg.png"></div><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/wx/8l/ua/wx8lualewtdkixfwg0petpxbsre.png"></div><p></p><br><p>  For convenience, there is the functionality of batch creation of several volumes of a given volume at once.  By default, thin volumes are created, since this allows for more efficient use of available storage space (including through the support of Space Reclamation).  In terms of performance, the difference between ‚Äúthin‚Äù and ‚Äúthick‚Äù volumes does not exceed 1%.  However, if you want to "squeeze all the juice" from the array, you can always convert any "thin" volume to "thick."  But it should be remembered that such an operation is irreversible. </p><br><p>  Next, it remains to ‚Äúpublish‚Äù the created volumes and set access rights to them by the hosts using ACLs (IP addresses for iSCSI and WWPN for FC) and physical separation across the ports of the array.  For iSCSI models, this is done through the creation of Target. </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/mw/8y/cg/mw8ycgqngghoyuelortgysofswi.png"></div><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/py/xe/28/pyxe28gnrsxh_cx4ltjleogwsmw.png"></div><p></p><br><p>  For FC models, publication occurs through the creation of a LUN for each port in the array. </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/qh/u3/bx/qhu3bxhm-kqinygguz298izbw7e.png"></div><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/oh/al/ey/ohaleyllpb-0bmot9kawc4-ssyg.png"></div><p></p><br><p>  To speed up the configuration process, hosts can be grouped together.  Moreover, if a multi-port FC HBA is used on the host (which in practice most often happens), the system automatically determines that the ports of such an HBA are assigned to a single host due to the WWPN, which differs by one.  Batch Target / LUN creation is also supported for both interfaces. </p><br><p>  An important note in the case of using the iSCSI interface is the creation of several targets for volumes at once to increase performance, since the queue on the target cannot be changed, and it will actually be a bottleneck. </p><br><p></p><h3>  Configuring ESXi hosts </h3><p></p><br><p>  On the ESXi side of the hosts, the basic configuration is performed according to the expected scenario.  Procedure for iSCSI connection: </p><br><p></p><ol><li>  Add Software iSCSI Adapter (not required if it has already been added, or if you are using the Hardware iSCSI Adapter); </li><li>  Creating a vSwitch through which iSCSI traffic will pass, and adding physical uplink and VMkernal to it; </li><li>  Adding array addresses to Dynamic Discovery; </li><li>  Creating a datastore </li></ol><br><p>  <b>Some important notes:</b> </p><br><blockquote><ul><li>  In the general case, of course, you can use an existing vSwitch, but in the case of a separate vSwitch, managing host settings will be much easier. </li><li>  It is necessary to separate Management traffic and iSCSI on separate physical links and / or VLAN in order to avoid performance problems. </li><li>  The IP addresses of the VMkernal and the corresponding ports of the All Flash array must be located within the same subnet, again due to performance issues. </li><li>  To ensure fault tolerance by VMware rules, a vSwitch must have at least two physical uplink </li><li>  If Jumbo Frames are used, you need to change the MTU of both the vSwitch and VMkernal </li><li>  It would not be superfluous to recall that, according to the recommendations of VMware, for physical adapters that will be used to work with iSCSI traffic, it is imperative that you configure Teaming and Failover.  In particular, each VMkernal should work only through one uplink, the second uplink must be switched to unused mode.  For fault tolerance, you need to add two VMkernal, each of which will work through its uplink. </li></ul><br></blockquote><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/_h/jb/va/_hjbvatnuhjwts4duy2epgb7bbi.png"></div><p></p><br><table><tbody><tr><th>  VMkernel Adapter (vmk #) </th><th>  Physical Network Adapter (vmnic #) </th></tr><tr><td>  vmk1 (Storage01) </td><td>  Active adapters <br>  vmnic2 <br>  Unused adapters <br>  vmnic3 <br></td></tr><tr><td>  vmk2 (Storage02) </td><td>  Active adapters <br>  vmnic3 <br>  Unused adapters <br>  vmnic2 <br></td></tr></tbody></table><br><p>  No preliminary action is required to connect via Fiber Channel.  You can immediately create a datastore. </p><br><p>  After creating the Datastore, you need to make sure that the Round Robin policy is used for the Target / LUN paths as the most efficient. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/zh/zr/t1/zhzrt15baoc1qlp0w6ds-mys3hu.png"></div><p></p><br><p>  By default, VMware settings use this policy according to the following scheme: 1000 requests through the first path, the next 1000 requests through the second path, etc.  Such a host interaction with a dual-controller array will be unbalanced.  Therefore, we recommend setting the Round Robin parameter = 1 through Esxcli / PowerCLI. </p><br><div class="spoiler">  <b class="spoiler_title">Options</b> <div class="spoiler_text"><p>  For Esxcli: </p><br><ul><li>  Print available LUNs </li></ul><br><p>  <b>esxcli storage nmp device list</b> </p><br><ul><li>  Copy Device Name </li><li>  Modify Round Robin Policy </li></ul><br><p>  <b>esxcli storage nmp psp roundrobin deviceconfig set - type = iops - iops = 1 - device = "Device_ID"</b> </p></div></div><br><p>  Most modern applications are designed to exchange large data packets in order to maximize bandwidth utilization and reduce the load on the CPU.  Therefore, by default, ESXi transmits I / O requests to the storage device in batches up to 32767KB.  However, for a number of scenarios, the exchange of smaller portions will be more productive.  For AccelStor arrays, these are the following scenarios: </p><br><ul><li>  Virtual machine uses UEFI instead of Legacy BIOS </li><li>  Used by vSphere Replication </li></ul><br><p>  For such scenarios, it is recommended to change the value of the Disk.DiskMaxIOSize parameter to 4096. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/1g/et/3g/1get3g8odkem1onrpkwjdilbnsi.png"></div><p></p><br><p>  For iSCSI connections, it is recommended to change the Login Timeout parameter to 30 (default 5) to increase the stability of the connection and turn off the delay of confirmations of sent packets DelayedAck.  Both options are located in vSphere Client: Host ‚Üí Configure ‚Üí Storage ‚Üí Storage Adapters ‚Üí Advanced Options for iSCSI adapter </p><br><p></p><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/1y/5_/c8/1y5_c80zsijgqtngo-9vt2zbbo8.png"></div><br><div style="text-align:center;"><img width="50%" src="https://habrastorage.org/webt/bh/yi/tv/bhyitvuujofnioaunwhzypx3fe0.png"></div><p></p><br><p>  A rather subtle point is the number of volumes used for the datastore.  It is clear that for ease of management there is a desire to create one large volume for the entire volume of the array.  However, the presence of several volumes and, accordingly, the datastore has a beneficial effect on the overall performance (more about the queues in the text below).  Therefore, we recommend creating at least two volumes. </p><br><p>  More recently, VMware advised to limit the number of virtual machines on one datastore, again in order to get the highest possible performance.  However, now, especially with the spread of VDI, this problem is no longer so acute.  But this does not negate the old rule - to distribute virtual machines that require intensive IO, according to different datastore.  To determine the optimal number of virtual machines for one volume, there is nothing better than to conduct <a href="http://accelstor.ru/page/zaprosit-demo">load testing of All Flash AccelStor array</a> within its infrastructure. </p><br><h3>  <i><b>Configure Virtual Machines</b></i> </h3><br><p>  When configuring virtual machines there are no special requirements, or rather, they are quite ordinary: </p><br><ul><li>  Using the highest possible version of VM (compatibility) </li><li>  It is more accurate to set the size of the RAM when the virtual machines are densely located, for example, in VDI (because by default, when starting, a paging file of a size comparable to the RAM is created, which consumes the useful capacity and has an effect on the final performance) </li><li>  Use the most productive for IO versions of adapters: network type VMXNET 3 and SCSI type PVSCSI </li><li> Use Thick Provision Eager Zeroed Disk Type for Maximum Performance and Thin Provisioning for the most efficient use of storage space. </li><li>  If possible, limit the work of non-I / O machines using Virtual Disk Limit </li><li>  Be sure to install VMware Tools </li></ul><br><h3>  <i><b>Queue Notes</b></i> </h3><br><p>  A queue (or Outstanding I / Os) is the number of I / O requests (SCSI commands) that are waiting to be processed at a time by a specific device / application.  In the event of a queue overflow, an QFULL error is issued, which ultimately results in an increase in the latency parameter.  When using disk (spindle) storage systems, theoretically, the higher the queue, the higher their performance.  However, you should not abuse it because it is easy to run into QFULL.  In the case of All Flash systems, on the one hand, everything is somewhat simpler: after all, the array has a delay of orders of magnitude lower, and therefore, more often than not, the size of the queues is not required to be separately controlled.  But on the other hand, in some usage scenarios (a strong imbalance in the IO requirements for specific virtual machines, tests for maximum performance, etc.) is required if you do not change the queue parameters, then at least understand what indicators can be achieved, and, most importantly, in what ways. </p><br><p>  There is no limit on the All Flash array AccelStor in relation to the volumes or I / O ports.  If necessary, even a single volume can get all the resources of the array.  The only limit on queues is at iSCSI targets.  It is for this reason that the above indicated the need to create several (ideally up to 8 pcs.) Targets for each volume in order to overcome this limit.  We also repeat that AccelStor arrays are very productive solutions.  Therefore, all system interface ports should be used to achieve maximum speed. </p><br><p>  On the ESXi host side, the situation is completely different.  The host itself applies the practice of equal access to resources for all participants.  Therefore, there are separate IO queues for the guest OS and HBA.  Queues to the guest OS are combined from the queues to the virtual SCSI adapter and virtual disk: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/pv/gu/sk/pvguskcy3y4yo7whrx8jiuggtdm.png"></div><br><p>  Queue to HBA depends on the specific type / vendor: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/tn/hs/6h/tnhs6h0wpivfrb5xfv2qms4tzjy.png"></div><br><p>  The final performance of the virtual machine will be determined by the lowest Queue Depth limit value among the host components. </p><br><p>  Thanks to these values, you can estimate the performance indicators that we can get in one configuration or another.  For example, we want to know the theoretical performance of the virtual machine (without blocking) with latency 0.5ms.  Then its IOPS = (1,000 / latency) * Outstanding I / Os (Queue Depth limit) </p><br><div class="spoiler">  <b class="spoiler_title">Examples</b> <div class="spoiler_text"><p>  <b>Example 1</b> </p><br><ul><li>  FC Emulex HBA Adapter </li><li>  One VM on datastore </li><li>  VMware Paravirtual SCSI Adapter </li></ul><br><p>  Here the Queue Depth limit is defined by the Emulex HBA.  Therefore, IOPS = (1000 / 0.5) * 32 = 64K </p><br><p>  <b>Example 2</b> </p><br><ul><li>  VMware iSCSI Software Adapter </li><li>  One VM on datastore </li><li>  VMware Paravirtual SCSI Adapter </li></ul><br><p>  Here, the Queue Depth limit is already defined by the Paravirtual SCSI Adapter.  Therefore, IOPS = (1000 / 0.5) * 64 = 128K </p></div></div><br><p>  The top All Flash models of AccelStor arrays (for example, <a href="http://accelstor.ru/product/neosapphire-p710">P710</a> ) are capable of providing 700K IOPS per write performance with a 4K block.  With this block size, it is clear that a single virtual machine is not able to load such an array.  To do this, we need 11 (for example 1) or 6 (for example 2) virtual machines. </p><br><p>  As a result, if all the components of the virtual data center are correctly configured, you can get very impressive results in terms of performance. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/uj/xy/cs/ujxycs-_fo-zworv8wds9kgp1j4.jpeg"></div><br><p>  <i>4K Random, 70% Read / 30% Write</i> </p><br><p>  In fact, the real world is much more difficult to describe with a simple formula.  Multiple virtual machines with different configurations and IO requirements are always located on the same host.  And the I / O processing is handled by a host processor, whose power is not infinite.  So, to unlock the full potential of the same <a href="http://accelstor.ru/product/neosapphire-p710">model P710</a> in reality, you will need from three hosts.  Plus, applications running inside virtual machines make their own adjustments.  Therefore, for accurate sizing, we suggest <a href="http://accelstor.ru/page/zaprosit-demo">using the test in the case of</a> All Flash <a href="http://accelstor.ru/page/zaprosit-demo">test models of</a> <a href="http://accelstor.ru/">AccelStor</a> arrays inside the customer's infrastructure for real-life tasks. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/447390/">https://habr.com/ru/post/447390/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447376/index.html">SNA Hackathon 2019</a></li>
<li><a href="../447380/index.html">Exceptions in Kotlin and their features</a></li>
<li><a href="../447382/index.html">The book "Unity and C #. Gamedev from idea to implementation. 2nd ed ¬ª</a></li>
<li><a href="../447384/index.html">Power semiconductors on guard environmental</a></li>
<li><a href="../447388/index.html">TL; ITMO University DR-digest: non-classical admission to university, upcoming events and the most interesting materials</a></li>
<li><a href="../447392/index.html">Three problems of services for checking English grammar, and whether they can be solved</a></li>
<li><a href="../447394/index.html">Interview with Vladimir Likhachev, father of Nikolai Likhachev, better known as Chris Kaspersky</a></li>
<li><a href="../447396/index.html">Is your company data valuable in the AI ‚Äã‚Äãera?</a></li>
<li><a href="../447398/index.html">Robotic Process Automation - a new look at old technology</a></li>
<li><a href="../447402/index.html">Splunk Universal Forwarder in docker as a system log collector</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>