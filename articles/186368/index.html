<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Pre-learning limited to Boltzmann machines for recognition of real images</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day. This topic is designed for those who have an idea about restricted Boltzmann machine (RBM) machines and their use for pre-training neural ne...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Pre-learning limited to Boltzmann machines for recognition of real images</h1><div class="post__text post__text-html js-mediator-article"><img align="left" src="https://habrastorage.org/storage2/eab/e4e/bce/eabe4ebce6a525f0d591b36792605027.jpg" alt="image"><br>  Good day.  This topic is designed for those who have an idea about restricted Boltzmann machine (RBM) machines and their use for pre-training neural networks.  In it, we will look at the features of using limited Boltzmann machines for working with images taken from the real world, understand why standard types of neurons are poorly suited for this task and how to improve them, and also express some emotions on human faces as an experiment.  Those who have no idea about RBM can get it, in particular, from here: <br><br clear="all">  <a href="http://habrahabr.ru/post/159909/">Implementing a Restricted Boltzmann machine on c #</a> , <br>  <a href="http://habrahabr.ru/post/163819/">Pre-training of the neural network using a limited Boltzmann machine</a> <br><a name="habracut"></a><br><br><h4>  Why is everything bad </h4><br>  Limited Boltzmann machines were originally developed using stochastic binary neurons, both visible and hidden.  The use of such a model for working with binary data is completely obvious.  However, the vast majority of real images are not binary, but are represented at least by shades of gray with an integer brightness value of each pixel from 0 to 255. One of the possible solutions to the problem is to change the brightness values ‚Äã‚Äãso that they lie in the interval 0..1 (divide by 255 ), and we will assume that the pixels are actually binary, and the values ‚Äã‚Äãobtained represent the probability of setting each particular pixel to one.  Let's try to use this approach for handwriting recognition ( <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> ) and voila - everything works and works wonderfully!  Why is everything bad? <br><br clear="all">  And because in a set of real images the intensity of a certain pixel is almost always almost exactly equal to the average intensity of its neighbors.  Therefore, the intensity should have a high probability of being close to the average and a small probability of being even a little distant from it.  The sigmoidal (logistic) function does not allow achieving such a distribution, although it works in some cases where it does not matter (for example, handwritten characters) <sup>[1]</sup> . <br><br><br clear="all"><br clear="all"><h4>  How to make it good ... </h4><br>  We need a way of representing visible neurons, which is able to say that the intensity is most likely equal to, say, 0.61, less likely 0.59 or 0.63, and very very unlikely 0.5 or 0.72.  The probability density function should look something like this: <br><img src="https://habrastorage.org/storage2/855/555/778/8555557789bd403553625ed07e132db5.jpg"><br>  Yes, this is a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> !  At least it can be used to model such behavior of neurons, which we will do by making the values ‚Äã‚Äãof visible neurons random variables with a normal distribution instead of <a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">a Bernoulli distribution</a> .  It should be noted that the normal distribution is convenient to use not only for working with real images, but also with many other data represented by real numbers from the range [-‚àû; + ‚àû], for which it does not make sense to reduce the values ‚Äã‚Äãto a binary form or probabilities from the range [0; 1] <sup>[2]</sup> .  Hidden neurons remain binary and we get the so-called Gaussian-Binary RBM, the distribution of the values ‚Äã‚Äãof neurons for which are given by the formulas <sup>[3]</sup> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <br clear="all"><img src="https://habrastorage.org/storage2/606/972/b3f/606972b3fa512dffcf1fb5e1a0c5bf54.gif"><br><br clear="all"><img src="https://habrastorage.org/storage2/afd/fe6/6ee/afdfe66eece78900a98d96f659103385.gif"><br><br clear="all">  and the Boltzmann machine energy is equal <br><br><img src="https://habrastorage.org/storage2/f56/647/69a/f5664769a3e748a77546d69961b6a221.png"><br><br clear="all">  where <i>hid</i> is the set of indexes of hidden neurons, <br>  <i>vis</i> - a set of indices of visible neurons, <br>  <i>b</i> - bias (offset), <br>  <i>œÉ <sub>i</sub></i> - the standard deviation for the i-th visible neuron, <br>  <i>w <sub>i, j</sub></i> - the weight of the connection between the i-th and j-th neuron, <br>  <i>N (x | Œº, œÉ <sup>2</sup> )</i> is the probability of the value of x for a variable with a normal distribution with expectation Œº and dispersion œÉ <sup>2</sup> . <br><br clear="all">  Let us consider how the RBM energy changes as <i>v <sub>i</sub></i> changes.  Component <i>b <sub>i</sub></i> (bias) is responsible for the desired value of the i-th visible neuron (intensity of the corresponding pixel of the image), and the energy itself grows quadratically with a deviation from this value: <br><img src="https://habrastorage.org/storage2/e7c/f12/465/e7cf12465f3fcff06a56df4f6376f07b.png"><br>  The last component of the formula, which depends both on <i>v <sub>i</sub></i> and <i>h <sub>i</sub></i> , and represents their interaction, depends on <i>v <sub>i</sub></i> linearly: <br><img src="https://habrastorage.org/storage2/78e/327/2a6/78e3272a621dd87e2af2273581180294.png"><br>  Summing up with a red parabola, this component shifts the energy minimum to one side or the other.  In this way, we get the behavior we need: the red parabola attempts to limit the value of the neuron by not letting it move far from a certain value, and the purple line shifts this value depending on the hidden RBM state. <br><br clear="all">  However, there are difficulties.  Firstly, for each visible neuron, it is necessary to select the appropriate parameter <i>œÉ <sub>i</sub></i> as a result of training.  Secondly, small values ‚Äã‚Äãof <i>œÉ <sub>i</sub></i> in themselves cause difficulties in learning, causing a strong effect of visible neurons on hidden and weak effects of hidden neurons on visible: <br><img src="https://habrastorage.org/storage2/7ca/cf5/22c/7cacf522ccb5485785aca7e2f7f1c795.png"><br><br clear="all">  Thirdly, the value of the visible neuron can now grow indefinitely, causing the energy to fall indefinitely, which makes learning much less stable.  To solve the first two problems, <a href="http://en.wikipedia.org/wiki/Geoffrey_Hinton">Jeffrey Hinton</a> proposes to normalize all training data before starting training so that they have zero expectation and unit variance, and then set the parameter <i>œÉ <sub>i</sub></i> in the above equations to unity <sup>[4]</sup> .  In addition, this approach allows using exactly the same formulas for collecting statistics and learning RBM using the <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">CD-n</a> method as in the usual case (using only binary neurons).  The third problem is solved by simply reducing the learning rate by 1-2 orders of magnitude. <br><br><br clear="all"><br clear="all"><h4>  ... and even better </h4><br>  As a result, we have learned how to represent real (valued) data with visible neurons of a limited Boltzmann machine, but the internal, hidden state is still binary.  Is it possible to somehow improve the hidden neurons, to force them to carry more information?  It turns out you can.  It is very easy, leaving the hidden neurons to be binary, to make them display natural numbers greater than 1. To do this, take one hidden neuron and create many copies of it with exactly the same weigth sharing weights and learning from the bias <i>b <sub>i</sub></i> , when calculating the probabilities, we will subtract fixed values ‚Äã‚Äãfrom the displacement of each neuron, obtaining a set of otherwise identical neurons with offsets <i>b <sub>i</sub></i> -0.5, <i>b <sub>i</sub></i> -1.5, <i>b <sub>i</sub></i> -2.5, <i>b <sub>i</sub></i> -3.5 ... adding noise with variance <i>œÉ (x)</i>  <i>= (1 + exp (-x)) <sup>-1</sup></i> (due to the probabilistic nature of the neurons).  Simply put, the greater the value of <i>x = b <sub>i</sub> + ‚àë v <sub>j</sub> w <sub>i, j</sub></i> at the input of such a neuron, the more copies of it are activated simultaneously, and the number of all activated copies will be the displayed natural number: <br><img src="https://habrastorage.org/storage2/68b/9c0/659/68b9c065984076545aa1e7e39c4b8615.png"><br><br><br clear="all">  However, in reality, creating a large number of copies for each neuron is expensive, because it also increases the number of sigmoid function calculations at each iteration of the RBM training / work as many times.  Therefore, we will act radically - we will create at once an infinite number of copies for each neuron!  Now we have a simple approximation, which allows us to calculate the resulting value for each neuron by a single simple formula <sup>[1,5]</sup> : <br><br clear="all"><img src="https://habrastorage.org/storage2/e22/1e7/9ee/e221e79eef264bd230bf8f1d60644cef.png"><br><br clear="all">  Thus, our hidden neurons were transformed from binary to rectified linear units with Gaussian noise, while the learning algorithm remained intact (after all, we assume that they are all the same binary neurons, with only an infinite number of copies described above).  Now they are able to represent not only 0 and 1, and even not only natural, but all non-negative real numbers!  The dispersion <i>œÉ (x)</i> ‚àä [0; 1] ensures that completely inactive neurons will not create noise and the noise will not become very large with increasing <i>x</i> .  In addition, a nice bonus: the use of such neurons still allows you to train the parameter <i>œÉ <sub>i</sub></i> for each neuron, if the preliminary normalization of data is for some reason impossible or undesirable <sup>[1,2]</sup> , but we will not dwell on this in detail. <br><br><br clear="all"><br clear="all"><h4>  Learning implementation </h4><br>  Taking the expectation from the formula of normal distribution, the value of the visible neuron can be considered by the formula <br><br clear="all"><img src="https://habrastorage.org/storage2/322/c1b/a97/322c1ba9722e65df98680053830407e8.gif"><br>  where <i>N (Œº, œÉ <sup>2</sup> )</i> is a random variable with a normal distribution, expectation Œº and dispersion œÉ <sup>2</sup> . <br><br>  Jeffrey Hinton in <a href="https://habr.com/ru/post/186368/">[4,5]</a> suggests not to use Gaussian noise in reconstructions of visible neurons during training.  Similar to the use of pure probabilities in the case of binary neurons instead of choosing 0 or 1, this speeds up learning by reducing noise and a bit less time for one step of the algorithm (do not count <i>N (0.1)</i> for each neuron).  Following the advice of Hinton, we get completely linear visible neurons: <br><br><img src="https://habrastorage.org/storage2/74c/a61/7e5/74ca617e5a6cd2f54e3af9ac383b27c8.gif"><br><br clear="all">  The value of the hidden neuron is calculated by the formula <br><br><img src="https://habrastorage.org/storage2/9a1/81f/766/9a181f76656cc19f1a670055286751f5.gif"><br><br clear="all">  To implement the learning itself, we use exactly the same formulas as for ordinary binary neurons in CD-n. <br><br><br clear="all"><br clear="all"><h4>  Experiment </h4><br>  As an experiment, we will choose something more interesting than simple recognition of faces or handwritten characters.  For example, we will recognize which emotion is expressed on a person‚Äôs face.  Use for training and testing images from bases <br>  <a href="http://www.pitt.edu/~jeffcohn/CKandCK%2B.htm">Cohn-Kanade AU-Coded Facial Expression Database (CK +)</a> , <br>  <a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html">Yale Face Database</a> , <br>  <a href="http://vis-www.cs.umass.edu/~vidit/IndianFaceDatabase/">Indian Face Database</a> , <br>  <a href="http://www.kasrl.org/jaffe.html">The Japanese Female Facial Expression (JAFFE) Database</a> . <br><br>  Of all the bases, we will only select images with a specific emotion (one of eight: neutral expression, anger, fear, disgust, joy, surprise, contempt, sadness).  Get 719 images.  70% of randomly selected images (500 pieces) are used as training, and 30% of the remaining (219 pieces) as verification data (in our case they can be used as test ones, since we do not select any parameters with their help) .  To implement we will use MATLAB 2012b.  On each image, select the face using the standard vision.CascadeObjectDetector, expand the resulting square area down by 10% so that the chin fits completely into the processed image.  The resulting image of the face will be compressed to a size of 70x64, translate into shades of gray and apply a histogram equalization to it to align the contrast on all images.  After that, each image is expanded into a 1x4480 vector and store the corresponding vectors in the matrices train_x and val_x.  In the matrices train_y and val_y, we save the corresponding desired vector-outputs of the classifier (size 1x8, 1 in the position of the emotion represented by the input vector, 0 in the remaining positions).  The data is ready, it's time to start the actual experiment. <br><br>  To implement the classifier, select the existing solution <a href="https://github.com/skaae/DeepLearnToolbox">DeepLearnToolbox</a> , fork, complete the functionality we need, fix bugs, shortcomings, inconsistencies with the <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Hinton guide</a> and get a <a href="https://github.com/SergiiKashubin/DeepLearnToolbox">new DeepLearnToolbox</a> , allowing you to simply take and use yourself for our task. <br><br>  The number of neurons in each layer of our neural network: 4480 - 200 - 300 - 500 - 8. Such small numbers of neurons in hidden layers are selected in order to eliminate overfitting and simple memorization by the network of all input images, since their number is small.  First, we will teach a neural network with a sigmoidal activation function, and for pre-training we use ordinary binary RBM. <br><br><pre><code class="matlab hljs">tx = double(train_x)/<span class="hljs-number"><span class="hljs-number">255</span></span>; ty = double(train_y); vx = double(val_x)/<span class="hljs-number"><span class="hljs-number">255</span></span>; vy = double(val_y); <span class="hljs-comment"><span class="hljs-comment">% train DBN (stack of RBMs) dbn.sizes = [200 300 500]; opts.numepochs = 100; opts.batchsize = 25; opts.momentum = 0.5; opts.alpha = 0.02; opts.vis_units = 'sigm'; % Sigmoid visible and hidden units opts.hid_units = 'sigm'; dbn = dbnsetup(dbn, tx, opts); dbn = dbntrain(dbn, tx, opts); % train NN nn = dbnunfoldtonn(dbn, 8); nn.activation_function = 'sigm'; % Sigmoid hidden units nn.learningRate = 0.05; nn.momentum = 0.5; nn.output = 'softmax'; % Softmax output to get probabilities nn.errfun = @nntest; % Error function to use with plotting % calculates misclassification rate opts.numepochs = 550; opts.batchsize = 100; opts.plot = 1; opts.plotfun = @nnplotnntest; % Plotting function nn = nntrain(nn, tx, ty, opts,vx,vy);</span></span></code> </pre> <br><br>  Neural Network Learning Schedule: <br><img src="https://habrastorage.org/storage2/d57/aa2/153/d57aa2153baa78357120c2e037f3c101.jpg"><br><br>  The average error on verification data among 10 launches with each time a new random sample of training and verification (validation) data was 36.26%. <br><br>  Now we will teach the neural network with the rectified linear activation function, and for pre-training we use the RBM described by us. <br><br><pre> <code class="matlab hljs">tx = double(train_x)/<span class="hljs-number"><span class="hljs-number">255</span></span>; ty = double(train_y); normMean = <span class="hljs-built_in"><span class="hljs-built_in">mean</span></span>(tx); normStd = std(tx); vx = double(val_x)/<span class="hljs-number"><span class="hljs-number">255</span></span>; vy = double(val_y); tx = normalize(tx, normMean, normStd); <span class="hljs-comment"><span class="hljs-comment">%normalize data to have mean 0 and variance 1 vx = normalize(vx, normMean, normStd); % train DBN (stack of RBMs) dbn.sizes = [200 300 500]; opts.numepochs = 100; opts.batchsize = 25; opts.momentum = 0.5; opts.alpha = 0.0001; % 2 orders of magnitude lower learning rate opts.vis_units = 'linear'; % Linear visible units opts.hid_units = 'NReLU'; % Noisy rectified linear hidden units dbn = dbnsetup(dbn, tx, opts); dbn = dbntrain(dbn, tx, opts); % train NN nn = dbnunfoldtonn(dbn, 8); nn.activation_function = 'ReLU'; % Rectified linear units nn.learningRate = 0.05; nn.momentum = 0.5; nn.output = 'softmax'; % Softmax output to get probabilities nn.errfun = @nntest; % Error function to use with plotting % calculates misclassification rate opts.numepochs = 50; opts.batchsize = 100; opts.plot = 1; opts.plotfun = @nnplotnntest; % Plotting function nn = nntrain(nn, tx, ty, opts,vx,vy);</span></span></code> </pre><br><br>  Neural Network Learning Schedule: <br><br><img src="https://habrastorage.org/storage2/43e/bfd/a6c/43ebfda6ccbec5a01c0e2ae220fab618.jpg"><br><br>  The average error on verification data among 10 runs with the same samples as for binary neurons was 28.40% <br><br>  A note about graphs: since we are actually interested in the network‚Äôs ability to correctly recognize emotions, rather than minimize the error function, learning continues as this ability improves, even after the error function begins to grow. <br><br>  As can be seen, the use of linear and rectified linear neurons in a limited Boltzmann machine made it possible to reduce the recognition error by 8%, not to mention the fact that it took 10 times less iterations (epochs) to train the neural network. <br><br><br clear="all"><br clear="all"><h4>  Links </h4><br>  1. <a href="https://www.coursera.org/course/neuralnets">Neural Networks for Machine Learning (video course)</a> <br>  2. <a href="http://www.ini.rub.de/data/documents/tns/masterthesis_janmelchior.pdf">Learning Natural Image Statistics with Gaussian-Binary Restricted Boltzmann Machines</a> <br>  3. <a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">Learning Multiple Layers of Features from Tiny Images</a> <br>  4. <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">A Practical Guide to Training Restricted Boltzmann Machines</a> <br>  5. <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_NairH10.pdf">Recti Ô¨Å ed Linear Units Improve Restricted Boltzmann Machines</a> </div><p>Source: <a href="https://habr.com/ru/post/186368/">https://habr.com/ru/post/186368/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../186354/index.html">Economic efficiency and feasibility of the introduction of VDI</a></li>
<li><a href="../186358/index.html">Support clean balls to share files using Powershell</a></li>
<li><a href="../186360/index.html">IT systems support in the production area</a></li>
<li><a href="../186362/index.html">Bleeding debian / ubuntu servers for small</a></li>
<li><a href="../186366/index.html">Moto X: full customization - the key to hegemony?</a></li>
<li><a href="../186374/index.html">MT6589T and MT6589M devices appear on the Chinese market</a></li>
<li><a href="../186376/index.html">OpenSSH two-factor authentication: key + one-time code</a></li>
<li><a href="../186378/index.html">Samopisnaya replacement Google.Latitude</a></li>
<li><a href="../186380/index.html">Shuttle and Alfred: quick access to SSH on OS X</a></li>
<li><a href="../186382/index.html">The history of BioForge</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>