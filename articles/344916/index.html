<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How does the frame render the Metal Gear Solid V engine: Phantom Pain</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The Metal Gear series of games gained worldwide recognition after almost two decades ago, Metal Gear Solid became a bestseller on the first PlayStatio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How does the frame render the Metal Gear Solid V engine: Phantom Pain</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/mx/fx/y1/mxfxy1jasfw-jnoib8mmsue8ids.png"></div><br>  The <a href="https://ru.wikipedia.org/wiki/Metal_Gear_(%25D1%2581%25D0%25B5%25D1%2580%25D0%25B8%25D1%258F_%25D0%25B8%25D0%25B3%25D1%2580)"><em>Metal Gear</em></a> series of games gained worldwide recognition after almost two decades ago, <a href="https://ru.wikipedia.org/wiki/Metal_Gear_Solid"><em>Metal Gear Solid</em></a> became a bestseller on the first <a href="https://ru.wikipedia.org/wiki/PlayStation">PlayStation</a> . <br><br>  The game introduced many players to the genre of "tactical espionage action" (tactical espionage action), whose name was invented by the creator of the franchise, <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25B4%25D0%25B7%25D0%25B8%25D0%25BC%25D0%25B0,_%25D0%25A5%25D0%25B8%25D0%25B4%25D1%258D%25D0%25BE">Hideo Kojima</a> . <br><br>  But personally, for the first time I played for <a href="http://metalgear.wikia.com/wiki/Big_Boss"><em>Snake</em></a> not in this part, but in <a href="https://en.wikipedia.org/wiki/Metal_Gear:_Ghost_Babel"><em>Ghost Babel</em></a> , the spin-off for the <a href="https://ru.wikipedia.org/wiki/Game_Boy_Color">GBC</a> console, less well-known, and nevertheless an excellent game with impressive depth. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The last part of the franchise, <a href="https://ru.wikipedia.org/wiki/Metal_Gear_Solid_V:_The_Phantom_Pain"><em>Metal Gear Solid V: The Phantom Pain</em></a> , was released in 2015. Thanks to the <a href="https://ru.wikipedia.org/wiki/Fox_Engine">Fox Engine engine</a> created by <a href="https://ru.wikipedia.org/wiki/Kojima_Productions">Kojima Productions</a> , it raised the entire series to a new level of visual quality. <br><br>  The analysis below is based on the PC version of the game with maximum graphics settings.  Part of the information presented here has already become public after the <a href="https://www.gdcvault.com/play/1018086/Photorealism-Through-the-Eyes-of">‚ÄúPhotorealism Through the Eyes of a FOX‚Äù</a> presentation at GDC 2013. <br><a name="habracut"></a><br><h1>  Analyzing the frame </h1><br>  Here is a shot taken from the very beginning of the game: this is the prologue in which <em>Snake</em> tries to get out of the hospital. <br><br>  <em>Snake</em> lies on the floor, trying to merge with the corpses surrounding him (he is at the bottom of the screen, with a naked shoulder). <br><br>  This is not the most beautiful scene, but it not bad illustrates the various effects that the engine can create. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a22/a4c/3e3/a22a4c3e3069a7ee6b5a02f2e611458d.jpg"></div><br>  Right in front of <em>Snake</em> are two soldiers.  They look at the burning silhouette at the end of the corridor. <br><br>  I will call this mysterious person "man on fire" in order not to spoil the plot of the game. <br><br>  So let's see how the frame is rendered! <br><br>  <i>[Approx.</i>  <i>Trans .: As usual, in the original article of Adrian there are many animations and interactive elements, so I recommend to <a href="http://www.adriancourreges.com/blog/2017/12/15/mgs-v-graphics-study/">get acquainted</a> with them for greater clarity.]</i> <br><br><h3>  Preliminary passage of the depths </h3><br>  This pass renders only the geometry of the relief under the hospital, how it looks from the player‚Äôs point of view, and displays depth information in the depth buffer. <br><br>  Below you can see the mesh generated from the <a href="https://en.wikipedia.org/wiki/Heightmap">elevation map of the</a> mesh: this is a 16-bit floating point texture containing elevation elevation values ‚Äã‚Äã(from the top view).  The slider divides the height map into different tiles, and for each tile a draw call is made with a flat grid of 16x16 vertices.  The vertex shader reads the height map and on the fly changes the position of the peaks to match the height value.  The relief is rasterized in about 150 render calls. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b3/71d/e27/4b371de27511fc2b69ac227f71c427cf.jpg"></div><br>  <i>Elevation height</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5d8/08d/898/5d808d89898955e19b1a33a231bd59c5.jpg"></div><br>  <i>Depth Map: 5%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8e0/e8a/d93/8e0e8ad931bcfddae6292060ae13a5cc.jpg"></div><br>  <i>Depth Map: 10%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6bb/bd8/36a/6bbbd836ace3aef4915fdbbb40705194.jpg"></div><br>  <i>Depth Map: 40%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d94/874/750/d94874750ea3d1c944da32df3b4f2945.jpg"></div><br>  <i>Depth Map: 100%</i> <br><br><h3>  G-Buffer Generation </h3><br>  In MGS V, as in many games of this generation, <a href="https://ru.wikipedia.org/wiki/%25D0%259E%25D1%2582%25D0%25BB%25D0%25BE%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2581%25D0%25B2%25D0%25B5%25D1%2589%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25B8_%25D0%25B7%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">deferred rendering is used</a> .  If you read <a href="https://habr.com/blog/2015/11/02/gta-v-graphics-study/">my analysis of GTA V</a> ( <a href="https://habrahabr.ru/company/ua-hosting/blog/271931/">transfer</a> to Habr√©), you may notice similar elements.  So, instead of directly calculating the final light value of each pixel in the process of rendering the scene, the engine first saves the properties of each pixel (such as albedo colors, normals, etc.) in several target renderers, called <abbr title="Geometry Buffer"><em>G-buffer</em></abbr> , and later combines all this information. <br><br>  All of the following buffers are generated simultaneously: <br><br>  G-Buffer Generation: 25% <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9fc/d62/c73/9fcd62c7313bb802a772abcfad9ab2a7.jpg"></div><br>  <i>Albedo</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/54d/153/1bc/54d1531bcf25d76c717755a4597bab93.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d48/f77/40d/d48f7740d19ad32e7891b4dcb4a9f5f8.jpg"></div><br>  <i>Specular</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bc5/8f4/f6a/bc58f4f6a0e5357fe933e13a7b46ed84.jpg"></div><br>  <i>Depths</i> <br><br>  G-Buffer Generation: 50% <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/029/cd6/c16/029cd6c162b7dd160c2ed6a4ed5f35b4.jpg"></div><br>  <i>Albedo</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/390/072/e77/390072e779c65ab2d5a7b7b8b2f45605.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3d2/f1f/97a/3d2f1f97a08d332795cadd37c8778631.jpg"></div><br>  <i>Mirror image</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/59b/72e/8b7/59b72e8b71348e377ea24806fe773a77.jpg"></div><br>  <i>Depths</i> <br><br>  G-Buffer Generation: 75% <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/157/37e/64f/15737e64f22f5b118d8a29c811521ddf.jpg"></div><br>  <i>Albedo</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/871/562/fd7/871562fd772f6e2dc92d47bb4d03b403.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/135/0a0/a0c/1350a0a0c3d13650d88c4c111b79ab1d.jpg"></div><br>  <i>Mirror image</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/473/229/77b/47322977bd44eee50f83025ccf3a8304.jpg"></div><br>  <i>Depths</i> <br><br>  G-Buffer Generation: 100% <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/36d/499/10e/36d49910ef49351cd26bef6674c7aa90.jpg"></div><br>  <i>Albedo</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b89/7b3/82c/b897b382c1145db7e3335aede590e9df.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4d4/6e7/35c/4d46e735c159a1711432f129206986f9.jpg"></div><br>  <i>Mirror image</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a13/b23/7e3/a13b237e34e598370856331530f51965.jpg"></div><br>  <i>Depths</i> <br><br>  Here we have a relatively lightweight G-buffer with three target renderers in the format <abbr title="Blue, Red, Green, Alpha, 8 bits each">B8G8R8A8</abbr> : <br><br><ul><li>  <strong>Albedo map</strong> : RGB channels contain albedo mesh diffuse color, i.e. own color, to which no lighting is applied.  The alpha channel contains the opacity / transparency of the material (usually 1 for fully opaque objects and 0 for grass or foliage). </li><li>  Normal <strong>map</strong> : the normal vector (x, y, z) of a pixel is stored in RGB channels.  The alpha channel contains a <a href="https://youtu.be/0qhPoT4coOI%3Ft%3D43m31s">roughness</a> factor <a href="https://youtu.be/0qhPoT4coOI%3Ft%3D43m31s">, depending on the viewing angle,</a> for some materials. </li><li>  <strong>Specular map</strong> : <br><ul><li>  Red: roughness </li><li>  Green: specular reflections </li><li>  Blue: material ID </li><li>  Alpha: translucency for <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D0%25B4%25D0%25BF%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D1%2585%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D1%2581%25D1%2581%25D0%25B5%25D0%25B8%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">subsurface scattering</a> (this seems to apply only to human skin and hair materials) </li></ul></li><li>  <strong>Depth map</strong> : 32-bit float value, indicating the pixel depth.  The depth is reversed (value 1 have meshes near the camera) in order to maintain high precision of floating point numbers for distant objects and to avoid <a href="https://en.wikipedia.org/wiki/Z-fighting">Z-collisions</a> .  This is important for open world games, in which the drawing distance can be very large. </li></ul><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b3e/880/6d2/b3e8806d25efcf9527c2246196615ef7.png"></div><br>  The G-buffer is rendered in the following order: first, all the opaque meshes of the main scene (characters, hospital building, etc.), then the whole relief (again) and, finally, the decals. <br><br>  It is here that the preliminary passage of the depths comes in handy: it makes the second stage (relief rendering) very fast.  Each pixel of the relief overlapped by another mesh will not have the expected depth predicted by preliminary depths, and in this case it is instantly discarded without the need to obtain texture-related textures and write this data back to the G-buffer. <br><br><h3>  Velocity map </h3><br>  To apply the <a href="https://en.wikipedia.org/wiki/Motion_blur">motion blur</a> effect at the post-processing stage, you need to know the speed of each pixel on the screen. <br><br>  If the scene is completely static, then the speed of each point is quite simple: it is derived from the depth and difference of the projection matrix between the previous and current frames.  But everything becomes more complicated when there are dynamic objects in the frame, for example, running characters, because they can move independently of the camera. <br><br>  Here the velocity map comes into play: it stores the motion vectors (velocities) of each pixel of the current frame. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/790/375/e16/790375e164696e51ab307de239b6ee12.png"></div><br>  <i>Velocity map (dynamic meshes)</i> <br><br>  First, the engine generates a velocity map only for dynamic meshes, as shown above. <br><br>  It can be noted that in our scene only one <em>person in the fire</em> is considered a dynamic mesh.  Even if <em>Snake</em> and the soldiers are technically not static meshes, the engine processes them in this way, and in our case this is perfectly acceptable, because they are barely moving.  Due to this, the engine can avoid part of the calculations: to calculate the speeds of animated characters, vertex skinning must be performed twice (for the previous and current postures), which can be quite expensive. <br><br>  The red channel is used as a mask (meaning 1 where the character is drawn), and the velocity vector itself is recorded in blue and alpha channel.  <em>A man in fire</em> does not move, so his dynamic speed is (0, 0). <br><br>  Then the engine calculates the speed of static geometry from the current depth buffer and the last two matrices of projections and combines it over the velocity map of the dynamic meshes, using the red channel as the mixing factor.  Here is the final velocity map (static and dynamic): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/637/a69/635/637a696351b58f0668cacf0b762632a6.jpg"></div><br>  <i>Velocity map (static + dynamic)</i> <br><br>  Do not pay much attention to the noise, there is almost no movement in this scene: the camera slowly performs a hit on a <em>person in fire</em> , all pixels have almost zero speed, and what you see is the accuracy of rounding off when writing components to 8-bit channels.  I enhanced the colors to make the image more distinct.  Also, I swapped the green and alpha channel, the real buffer keeps the speed in the blue and alpha channel. <br><br><h3>  Screening Ambient Light in Screen Space (Screen Space Ambient Occlusion) </h3><br>  The <a href="https://ru.wikipedia.org/wiki/SSAO">SSAO</a> effect should add a bit of darkening in areas where there is less ambient light, for example, in narrow holes or in folds.  Interestingly, Fox Engine performs two separate SSAO passes using different algorithms and combining the results in the last pass. <br><br><h4>  SSAO based on linear integrals </h4><br>  Line Integral SSAO (SSAO) is an ambient occlusion calculation technique that <a href="https://en.wikipedia.org/wiki/Avalanche_Software">Avalanche Software</a> used in Disney's <a href="https://en.wikipedia.org/wiki/Toy_Story_3:_The_Video_Game">Toy Story 3</a> game. <br><br>  Despite the frightening name, this algorithm is quite clear and well explained in this <a href="http://advances.realtimerendering.com/s2010/Ownby,Hall%2520and%2520Hall%2520-%2520Toystory3%2520(SIGGRAPH%25202010%2520Advanced%2520RealTime%2520Rendering%2520Course).pdf">report at Siggraph 2010</a> : for each pixel of the scene, a sphere is taken with its center in this pixel;  this spherical volume is then subdivided into several linear subvolumes.  Each sub-occlusion coefficient is calculated by obtaining a single value from the depth map, and the total sphere occlusion coefficient is simply a weighted sum of the coefficients of each sub-subs. <br><br>  Here, Fox Engine uses two pairs of symmetrical samples, that is, five values ‚Äã‚Äãof the depth buffer per pixel are included in the original sample. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e77/f72/41c/e77f7241cb228de66d22709729b2d3d6.jpg"></div><br>  <i>RGB: linear depth</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/85b/019/538/85b019538b4b491d3ff3bd32daa08e1c.jpg"></div><br>  <i>Alpha: LISSAO</i> <br><br>  The calculation is performed at half resolution, and the data is stored in the RGBA8 texture, where the alpha channel contains the actual ambient occlusion result, and the linear depth value is stored in RGB (Float-to-RGB encoding is used, similar to <a href="https://aras-p.info/blog/2009/07/30/encoding-floats-to-rgba-the-final/">this technique</a> ). <br><br>  The result in the alpha channel is actually noisy due to the small number of samples;  This SSAO map can be smoothed later with a depth-based blur filter: linear depth is stored in RGB channels, that is, all the necessary data can be read at a time. <br><br><h4>  Scalable Ambient Obscurance </h4><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a1e/db8/9fb/a1edb89fbc70b6d650a91d23592588ed.jpg"></div><br>  <i>SAO</i> <br><br>  In the second pass of the SSAO calculation, a variation of the <a href="http://research.nvidia.com/publication/scalable-ambient-obscurance">Scalable Ambient Obscurance</a> technique is <a href="http://research.nvidia.com/publication/scalable-ambient-obscurance">used</a> . <br><br>  It differs from the ‚Äúofficial‚Äù SAO in that it does not use any mip-levels of depths and does not reconstruct the normals;  it reads directly the height map itself and operates at half resolution, performing 11 readings per pixel (however, using a different strategy for selecting sample locations). <br><br>  It uses exactly the same mid-tones contrast filtering and a two-sided rectangular filter as in the original SAO implementation. <br><br>  Notice that the SAO parameters are changed so that the high-frequency variations (for example, at the feet of a soldier) stand out strongly in comparison with the version for LISSAO. <br><br>  As in LISSAO, the SAO map is blurred by two side passes, taking into account the depths. <br><br>  After that, the compute shader combines the LISSAO and SAO images, obtaining the final result of the SSAO: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2ce/d4a/35d/2ced4a35d2ea57f0f7b8fff9dfd182e3.jpg"></div><br>  <i>Ready SSAO</i> <br><br><h3>  Spherical Illumination Maps </h3><br>  To work with <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D0%25BE%25D0%25B1%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BE%25D1%2581%25D0%25B2%25D0%25B5%25D1%2589%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">global illumination,</a> Fox Engine uses local <a href="https://en.wikipedia.org/wiki/Sphere_mapping">spherical irradiance maps</a> : different areas are defined at the game level, and a spherical map is created for each of these regions, approximating illumination coming from different directions. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/493/9a6/402/4939a6402ce7b14eb1d94e020a3a4bbf.png"></div><br>  <i>Spherical Illumination Maps</i> <br><br>  At this stage, one after another, all the spherical maps used in our scene are generated, after which each of them is stored in a 16x16 tile of the atlas of the HDR textures.  This texture atlas is shown above: the disk in the middle of each tile is an approximate representation of what would reflect a metal sphere located in the middle of the corresponding illumination zone. <br><br>  How are these spherical maps generated?  They are calculated from <a href="http://silviojemma.com/public/papers/lighting/spherical-harmonic-lighting.pdf">spherical harmonics</a> .  The underlying mathematics are rather frightening, but, in essence, spherical harmonics are a way of coding the value of a 360-degree signal into a number of coefficients (usually nine) that produce reasonably good accuracy (second order <abbr title="Spherical harmonics">SH</abbr> ). <br><br>  And just out of nine of these numbers, you can approximately recreate the value of the signal in any direction. <br><br>  If you are familiar with the concept of how <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A4%25D1%2583%25D1%2580%25D1%258C%25D0%25B5">a Fourier transform</a> can break a signal into sinusoidal components, then the situation is quite similar, except that we decompose the signal into functions on the surface of a sphere. <br><br>  Where do these coefficients come from?  They are precomputed ‚Äî I assume that the environment of each region, marked up by the level designers, is recorded in a cubic map.  Then it is converted into a cubic irradiance map and <a href="http://graphics.stanford.edu/papers/envmap/envmap.pdf">encoded into the coefficients of spherical harmonics</a> , which the engine reads during the execution of the game. <br><br>  One may ask: why not use the cubic maps themselves for determining the illumination?  This may work, it is possible to use cubic light maps, but they have their drawbacks.  The most important is a waste of memory to store the six faces of a cubic map, while spherical harmonics reduce the costs of up to nine RGB values ‚Äã‚Äãper map.  This saves a lot of space in the memory and GPU bandwidth, which is very important when you have to deal with dozens of cards in the scene. <br><br>  All these spherical maps are generated in each frame from the pre-baked spherical harmonic coefficients and the current position and direction of the player‚Äôs camera. <br><br><h3>  Diffuse Lighting (Global Illumination) </h3><br>  It is time to apply all these generated lightmaps!  Each zone affected by the irradiance map is transferred to rasterize the video processor.  Usually, each draw call (one per illumination map) sends a mesh in the form of a parallelepiped, which represents the amount of influence of the map in the world.  The idea is that it should touch all the pixels that may be affected by a specific irradiance map. <br><br>  The diffuse map is computed in a half-resolution HDR texture by reading from the normal, depth and illumination maps. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b89/7b3/82c/b897b382c1145db7e3335aede590e9df.jpg"></div><br>  <i>Normals</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a13/b23/7e3/a13b237e34e598370856331530f51965.jpg"></div><br>  <i>Depths</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/493/9a6/402/4939a6402ce7b14eb1d94e020a3a4bbf.png"></div><br>  <i>Illumination</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9da/ae4/24a/9daae424a37c337fd2c2815b538ffac0.jpg"></div><br>  <i>Diffuse Lighting (GI): 15%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fd2/692/72c/fd269272c01cb35d7d94ceadccbd440f.jpg"></div><br>  <i>Diffuse lighting (GI): 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eec/52b/a7d/eec52ba7d4aaa426b9fd7f814aae073a.jpg"></div><br>  <i>Diffuse Lighting (GI): 80%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/09d/208/055/09d20805590d09b67b5a7744616c65bb.jpg"></div><br>  <i>Diffuse Lighting (GI): 100%</i> <br><br>  The process is repeated for each light map with an additive mixing of new fragments over the old ones. <br><br>  After accumulating in the diffuse buffer all the lighting brought in by global illumination, it scales from half to full resolution.  It is worth noting that increasing the scale is not naive <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B8%25D0%25BB%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2584%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">bilinear filtering</a> , it is a <a href="https://en.wikipedia.org/wiki/Bilateral_filter">bi-directional filter</a> that reads the half-resolution buffer and, more importantly, the original full-resolution depth map (to tie the weights to the adjacent color pixels), so the end result still contains broken edges around mesh boundaries.  Visually, it looks as if we all worked in full resolution all this time! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3a5/75f/863/3a575f8634fbd0ca369a44ba7c7c9129.png"></div><br>  <i>2x zoom (no filtering)</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5e5/6d0/e14/5e56d0e14ec419599b21f81e399461bb.png"></div><br>  <i>Bilinear 2x zoom</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/392/904/606/392904606c0f9c98860ce6950e4bf873.png"></div><br>  <i>Bidirectional 2x zoom</i> <br><br><h3>  Sources of non-shadow lighting </h3><br>  After calculating all this static illumination from global illumination, it is time to add dynamic lighting brought in by point and directional light sources.  We will work in full resolution and render one after another the volumes of influence for each light source in the scene.  For now we will only render sources that do not drop shadows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b9/01b/6eb/4b901b6ebaf79b1a74c7bc2c97596f31.jpg"></div><br>  <i>Diffuse lighting: 5%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/641/1c6/b2c/6411c6b2c1e09c8941772af9a1f4889c.jpg"></div><br>  <i>Diffuse lighting: 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6ac/577/3f1/6ac5773f1678c02607c57f09573dde75.jpg"></div><br>  <i>Diffuse lighting: 60%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/881/f6b/dfc/881f6bdfc67f388cee39548749020b10.jpg"></div><br>  <i>Diffuse lighting: 100%</i> <br><br>  In fact, simultaneously with the update of the diffuse illumination buffer, another full resolution HDR-render is rendered: the reflected illumination buffer.  Each call to the light source rendering above is actually simultaneously writing to the diffuse and reflected lighting buffer. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/46d/fa1/8ba/46dfa18ba6dac49810466f44e5d474a7.jpg"></div><br>  <i>Reflected lighting: 5%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4fd/b6a/bd3/4fdb6abd34e6c9450bb2aa39f2ddaf67.jpg"></div><br>  <i>Reflected lighting: 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a35/c9d/cba/a35c9dcba8b64c0624066712eb517f36.jpg"></div><br>  <i>Reflected lighting: 60%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/559/d11/b2c/559d11b2c0119195dcc08a53746880ac.jpg"></div><br>  <i>Reflected lighting: 100%</i> <br><br><h3>  Shadow maps </h3><br>  One can guess what we will do after sources without shadows: sources of lighting casting shadows! <br><br>  Such sources are much more costly to calculate, so their number in games is rather limited.  The reason for their high cost is that each requires the generation of <a href="https://en.wikipedia.org/wiki/Shadow_mapping">a shadow map</a> . <br><br>  In essence, this means re-rendering the scene from the point of view of each light source.  On the ceiling of the corridor, we have two directional light sources shining down, and each one generates a 4k x 4k shadow map. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/249/d2c/906/249d2c9068768d3cb1694d97cbd9b079.jpg"></div><br>  <i>Two shadow maps</i> <br><br><h3>  Light sources casting shadow </h3><br>  After the generation of shadow maps has been completed, the illumination from two directional sources on the ceiling is calculated.  Diffuse and reflected light buffers are updated at the same time.  Finally, solar lighting is applied (from a previously generated spherical map of spherical harmonics). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b47/c3e/055/b47c3e055a20fdf4318d197dc3ce98d9.jpg"></div><br>  <i>Diffuse lighting 0%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5fc/995/14d/5fc99514d0e2289761a8c7e2a69fc4d9.jpg"></div><br>  <i>Reflected lighting 0%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c50/0e1/5a8/c500e15a89040eb61a0510aa83d869f0.jpg"></div><br>  <i>Diffuse lighting 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4b8/100/4d6/4b81004d6e91ef58c1a562639c5a8da3.jpg"></div><br>  <i>Reflected lighting 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/959/cab/549/959cab54999c227928c532ab6c386ee6.jpg"></div><br>  <i>Diffuse lighting 70%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/294/7cd/18c/2947cd18cd70e7f757782f397edb2232.jpg"></div><br>  <i>Reflected lighting 70%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/155/f2c/f06/155f2cf06a44ebc2ce188f18c7e1dddb.jpg"></div><br>  <i>Diffuse lighting 100%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/61b/858/e0e/61b858e0ee9a80e64eaa5969c547fe2b.jpg"></div><br>  <i>Reflected lighting 100%</i> <br><br><h3>  Combining lighting and tonal compression maps </h3><br>  At this stage, all previously generated buffers are combined: the color of albedo is multiplied by the diffuse illumination, and then the reflected illumination is added to the result.  Then the color is multiplied by the SSAO value and the result is interpolated with the fog color (which is extracted from the fog search texture and the depth of the current pixel).  Finally, tone mapping is used to convert from <abbr title="High dynamic range">HDR</abbr> space to <abbr title="Low Dynamic Range. 8 bits per channel">LDR</abbr> .  The alpha channel stores additional information: the original HDR <a href="https://ru.wikipedia.org/wiki/%25D0%25AF%25D1%2580%25D0%25BA%25D0%25BE%25D1%2581%25D1%2582%25D1%258C">brightness of</a> each pixel. <br><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a13/b23/7e3/a13b237e34e598370856331530f51965.jpg"></div><br>  <i>Depth</i> </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/36d/499/10e/36d49910ef49351cd26bef6674c7aa90.jpg"></div><br>  <i>Albedo</i> </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/155/f2c/f06/155f2cf06a44ebc2ce188f18c7e1dddb.jpg"></div><br>  <i>Diffuse lighting</i> </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/61b/858/e0e/61b858e0ee9a80e64eaa5969c547fe2b.jpg"></div><br>  <i>Indirect lighting</i> </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2ce/d4a/35d/2ced4a35d2ea57f0f7b8fff9dfd182e3.jpg"></div><br>  <i>SSAO</i> </td></tr></tbody></table><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b8e/c9c/d44/b8ec9cd44f96fe2f6315b8e7c0cf7fac.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/97c/16d/d89/97c16dd893c9f2c81111d2281ecc4459.jpg"></div><br>  <i>Lighting combination</i> <br><br>  By the way, what kind of tonal compression map is used in MGS V?  In the interval from 0 to a certain threshold (0.6) it is completely linear and returns the original value of the channel, and above the threshold the values ‚Äã‚Äãslowly tend to the horizontal asymptote. <br><br>  Here is the function applied to each RGB channel, where <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="7.813ex" height="2.057ex" viewBox="0 -780.1 3364.1 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMATHI-41" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-3D" x="1028" y="0"></use><g transform="translate(2084,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-36" x="779" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-1"> A = 0.6 </script>  , but <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="12.484ex" height="2.057ex" viewBox="0 -780.1 5375.1 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMATHI-42" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-3D" x="1037" y="0"></use><g transform="translate(2093,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-30"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-2E" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-34" x="779" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-35" x="1279" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-33" x="1780" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-33" x="2280" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/344916/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgJj_s56C7YvV_6RSm59duzZbUVIQ#MJMAIN-33" x="2781" y="0"></use></g></g></svg></span><script type="math/tex" id="MathJax-Element-2"> B = 0.45333 </script>  : <br><br><p><math> </math> $$ display $$ ToneMap (x) = \ begin {cases} x &amp; \ text {if $ x \ le A $} \\ [2ex] min \ left (\ text {1,} A + B - \ large { \ frac {\ text {$ B $ ¬≤}} {x - A + B}} \ right) &amp; \ text {if $ x \ gt A $} \ end {cases} $$ display $$ </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e1c/a9f/a84/e1ca9fa84c3db9e137e1030bbfd2d0c1.png"></div><br>  So, for the transition from the linear space to the sRGB space, tone mapping is applied as well as <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B0%25D0%25BC%25D0%25BC%25D0%25B0-%25D0%25BA%25D0%25BE%25D1%2580%25D1%2580%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F">gamma correction</a> .  In other games, this often means that we have reached the final stages of frame rendering. <br><br>  But is this really the case?  No way, we're just starting!<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Interestingly, Fox Engine performs tonal compression quite early and continues to work in the LDR space, including passes for transparent objects, reflections, depth of field, etc. </font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Radiant and transparent objects </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this passage, the engine draws all objects with the emissivity property, for example, a green ‚ÄúExit‚Äù sign or red hot flame spots on a </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">person in a fire</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">Also, the engine renders transparent objects, such as glass.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/97c/16d/d89/97c16dd893c9f2c81111d2281ecc4459.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Radiant and transparent objects: before the passage</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a8d/fb6/4c4/a8dfb64c421fc37356713427c69b9ef8.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Radiating and transparent objects: after the passage</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the screenshot above, it is not very noticeable, but in the case of glass, reflections from the environment are also applied. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All environmental data is obtained from </font><font style="vertical-align: inherit;">a 256x256 </font></font><a href="https://en.wikipedia.org/wiki/Cube_mapping"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">cubic HDR map</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> shown below (also called a reflection probe).</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7da/08d/4d0/7da08d4d068ed46e310e0fe03b514563.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Reflection Probe The</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> cubic map is not dynamic, it is pre-baked and is used as it is in the course of the game, so we will not see dynamic meshes inside. Its task is to create ‚Äúsufficiently good‚Äù reflections from the data of a static environment. There are several probes in different places of the level. The total number of cubic cards of the whole game </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is huge</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - not only probes are required for many locations, but also different versions of the same probe, depending on the time of day / night. Plus, it is necessary to take into account different weather conditions, so for each time of day and every point the engine generates four cubic maps (for sunny, cloudy, rainy and thundery weather). The game has to deal with an impressive amount of combinations.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At GDC 2013 they showed a short clip on </font></font><a href="https://youtu.be/0qhPoT4coOI%3Ft%3D1h1m22s"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">how the engine generates</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> such lightning probes.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Screen Space Reflections </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At this stage, an image of reflections in the scene is created solely on the basis of information from pixels rendered in the previous passage. </font><font style="vertical-align: inherit;">Here, </font></font><a href="https://ru.wikipedia.org/wiki/Ray_casting"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ray tracing</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is performed </font><font style="vertical-align: inherit;">in the screen space at half resolution: several rays are ‚Äúthrown‚Äù at each pixel of the screen, the direction of these rays is calculated from the depth buffer (which gives us the pixel position) and from the normal. </font><font style="vertical-align: inherit;">Each ray is checked for collision by sampling the depth buffer at four equidistant points along the ray. </font><font style="vertical-align: inherit;">If a collision is detected, the color of the pixel at the point of its origin is used as the reflection color modulated by the roughness of the original pixel.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d35/31a/96f/d3531a96f5d1798c77fced81f26ba364.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">SSR color</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/86b/943/cb1/86b943cb1cb370cfcd0202b7309b348c.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Alpha SSR</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Obviously, we lack ‚Äúglobal‚Äù information: no object outside the screen can contribute to the reflection. </font><font style="vertical-align: inherit;">To make artifacts less visible, the reflection map uses an alpha mask to smoothly darken the opacity as it approaches the edges of the screen. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The value of SSR lies in the fact that they can provide </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">dynamic</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reflections in real time at a fairly low cost. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The noise of the SSR card is later reduced by using a </font></font><a href="https://en.wikipedia.org/wiki/Gaussian_blur"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Gaussian blur</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and blends across the stage.</font></font><br><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Heat distortion, decals and particles </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The temperature of a burning area in which a </font></font><em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">person</font></font></em><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> stands </font><em><font style="vertical-align: inherit;">on fire</font></em><font style="vertical-align: inherit;"> is so high that it creates a distortion of light. </font><font style="vertical-align: inherit;">This effect is achieved with the help of several draw calls, each of which creates a copy of the entire target render and applies distortion, locally stretching the pixels in some direction. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is especially noticeable on the first arch connecting the left wall with the ceiling. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After that, decals are applied, for example, liquid on the floor, and finally particles are drawn to render fire and smoke.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/215/016/fcb/215016fcb87a3340763802a0d6894bce.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The foundation</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/189/d67/5f3/189d675f340c3fb51ecab684cafdc9c7.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Distortion</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2b1/676/8ea/2b16768ea110eba2c086793d73cacc67.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Decals</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b0d/8d1/b4c/b0d8d1b4caf380059c110f22c86d969d.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Particles 30%</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/675/047/7bf/6750477bfa3b5fd9a290aa3fad0ace09.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Particles 60%</font></font></i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1e3/070/61d/1e307061d800820907a03f4b352c79f3.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Particles 100%</font></font></i> <br><br><h3>  Bloom </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/917/e7f/3af/917e7f3afb250b8f8797eddc8d95e344.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lightness Pass</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> At this stage, a </font></font><a href="http://en.wikipedia.org/wiki/Bloom_%2528shader_effect%2529"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bloom</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> texture is created </font><font style="vertical-align: inherit;">from the source scene. It works with very low resolution: first, the scale of the scene is reduced four times, then the lightness pass filter is applied to select only the brightest pixels, as shown in the image above. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How does the lightness pass filter separate the ‚Äúdark‚Äù and ‚Äúbright‚Äù pixels? We are no longer in HDR space, the tone mapping has brought us to the LDR space, in which it is more difficult to determine which color was originally light. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Recall that the alpha channel of the scene buffer contains the original HDR brightness of each pixel before tonal compression ‚Äî this information the filter uses to determine the "lightness" of the pixels.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d9d/868/dcc/d9d868dcc981d8874a45bb9e7f2bbf37.png"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glare in the lens</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the Fox Engine, the bloom effect refers not only to bright pixels spreading around its color: it also takes into account </font></font><a href="https://en.wikipedia.org/wiki/Lens_flare"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lens flare</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A5%25D1%2580%25D0%25BE%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25B0%25D0%25B1%25D0%25B5%25D1%2580%25D1%2580%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">chromatic aberration</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , which are procedurally generated from the lightness pass buffer. In our dark scene there are no strong sources of illumination, due to which the lens flare could stand out, they are barely visible, but you can understand how they look in the above image, where I have artificially emphasized colors. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Glare from the lens is superimposed on the lightness pass filter, and then a blurred version of the larger-radius buffer is generated. This is done by four successive iterations of </font></font><a href="http://www.daionet.gr.jp/~masa/archives/GDC2003_DSTEAL.ppt"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the Masaki Kawase blur algorithm</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Kawase technique makes it possible to achieve a </font></font><a href="https://software.intel.com/en-us/blogs/2014/07/15/an-investigation-of-fast-real-time-gpu-based-image-blur-algorithms"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">blur of a large radius, similar to a Gauss blur, but with higher performance</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/917/e7f/3af/917e7f3afb250b8f8797eddc8d95e344.jpg"></div></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2d2/66e/114/2d266e114ea437291de3090b8c67db99.png"></div></td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/411/13b/84b/41113b84b6e6c5ac62d12d6beace1900.jpg"></div><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bloom</font></font></i> </td></tr></tbody></table><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Depth of field </font></font></h3><br>  Metal Gear games are known for their long cinematic clips, so it‚Äôs natural that the engine tends to recreate the behavior of real cameras as accurately as possible using the Depth of Field (DoF) effect: only a specific area looks sharp, while other out-of-focus areas look blurry. <br><br>  The scale of the scene is reduced to half the resolution and converted back from sRGB space to linear space. <br><br>  Then the slider generates two images corresponding to the ‚Äúnear field‚Äù (the area between the camera and the focal length) and the ‚Äúfar field‚Äù (the area outside the focal length).  Separation is performed only on the basis of the depths (distance from the camera) - all pixels are closer than the soldiers, will be copied to the near field buffer, and all the others - to the far field buffer. <br><br>  Each field is processed separately, a blur is applied to it.  <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D1%2583%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25B0_%25D1%2580%25D0%25B5%25D0%25B7%25D0%25BA%25D0%25BE_%25D0%25B8%25D0%25B7%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B6%25D0%25B0%25D0%25B5%25D0%25BC%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D1%2582%25D0%25B2%25D0%25B0">The scattering circle of</a> each pixel is calculated only on the basis of the depth and configuration of the camera (aperture, focal length ...).  The value of the scattering circle tells you how ‚Äúout of focus‚Äù a pixel is - the larger the scattering circle, the more the pixel spreads around. <br><br>  After performing the blur, two fields are created: <br><br><table><tbody><tr><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8cc/bb5/d70/8ccbb5d7070b09c823eb289743e59197.png"></div><br>  <i>DoF - near field</i> </td><td><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d3/a5c/8fb/9d3a5c8fb88361c994bda0332c25194e.png"></div><br>  <i>DoF - far field</i> </td></tr></tbody></table><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d17/122/3d1/d171223d14d535cc831f5c09f2095267.png"></div><br>  I will say a few words about this ‚Äúblurring‚Äù operation: in fact, this is a relatively expensive operation in which one sprite is created and rendered for each pixel in the scene. <br><br>  The sprite itself contains a disk, which is shown above, and it can be replaced with any shape, for example, a hexagon, if you prefer hexagonal bokeh. <br><br>  The sprite is centered on the pixel that created it, it has the same color as the pixel, and its size is scaled along with the pixel scatter circle.  The idea is that the pixel ‚Äúspreads its color around‚Äù with the help of a disk - the more pixel is out of focus, the more sprite it needs to be. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/abf/8df/2aa/abf8df2aa73eaaa1c824d5c4c82b4d87.png"></div><br>  All sprites are drawn on top of each other with additive blending. <br><br>  This technique is called sprite scattering;  It is used in many games, for example, in the <a href="https://www.beyond3d.com/content/news/499"><em>Lost Planet</em></a> series, <a href="https://bartwronski.com/2014/04/07/bokeh-depth-of-field-going-insane-part-1/"><em>The Witcher</em></a> and in the post <a href="https://docs.unrealengine.com/udk/Three/rsrc/Three/DirectX11Rendering/MartinM_GDC11_DX11_presentation.pdf">-</a> processing of <a href="https://docs.unrealengine.com/udk/Three/rsrc/Three/DirectX11Rendering/MartinM_GDC11_DX11_presentation.pdf">Bokeh-DoF UE4</a> . <br><br>  After generating the blurred far and near fields, we simply mix them over the original scene: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1e3/070/61d/1e307061d800820907a03f4b352c79f3.jpg"></div><br>  <i>DoF: before</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/66f/52a/b29/66f52ab299b25d2b2afa4a6220ebfc1c.jpg"></div><br>  <i>DoF: after</i> <br><br>  This technique works and creates beautiful results, but at high resolution in almost completely defocused scenes, it can become very braking: overlapping huge sprites can lead to a wild number of redraws. <br><br>  How does the Fox Engine manage to weaken the effect of this effect? <br><br>  Well, in fact, I oversimplified my explanations when I wrote that one field is represented by a buffer of accumulation of half resolution: there is not only one buffer, there are several more, smaller size: 1/4, 1/8, 1/16 from permissions.  Depending on the pixel scattering circle, the sprite it creates results in one of these buffers: usually large sprites are written into low-resolution buffers to reduce the total number of affected pixels. <br><br>  To do this, the buffer processes the level of each buffer one by one, creating 100% sprites and allowing the vertex shader to ‚Äúkill‚Äù sprites that do not belong to the current level.  The vertex shader knows the size of the sprite, and if the size does not fit the current level, the shader simply throws it out of the visibility pyramid.  assigning it a negative depth value. <br><br>  Geometry of the sprite never gets to the rasterization stage performed by the pixel shader. <br><br>  Then all these buffers are combined to create a single half-resolution field. <br><br><h3>  Dirt on the lens and new lens flare </h3><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7a6/e40/3cf/7a6e403cfc8c46c0e771a44ca924e074.jpg"></div><br>  <i>Dirt and glare 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b51/77b/e74/b5177be74adcf7052f84636a86ae9710.jpg"></div><br>  <i>Dirt and glare 60%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6c1/f24/2f6/6c1f242f6bd8f7af1108569acb9e10c0.jpg"></div><br>  <i>Dirt and glare 100%</i> <br><br>  <em>Snake</em> is in a difficult situation and in a hostile environment, explosions are heard around, some projections cross the camera lens and pollute it. <br><br>  To reflect this effect, some dirt is artificially added to the lens over the image.  Dirt is generated from sprites. <br><br>  Then we need even more glare on the lens!  Yes, we have already added them, but there is little glare, right?  This time we add artifacts of <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BD%25D0%25B0%25D0%25BC%25D0%25BE%25D1%2580%25D1%2584%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BA%25D0%25B8%25D0%25BD%25D0%25BE%25D1%2584%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D1%2582">anamorphic lenses</a> : long vertical streaks of light in the middle of the screen, arising from a bright flame.  They are also generated only from sprites. <br><br>  All of these steps are performed in a dozen render calls that render into a half-resolution buffer, which is then combined on top of the scene with additive alpha blending. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/66f/52a/b29/66f52ab299b25d2b2afa4a6220ebfc1c.jpg"></div><br>  <i>Dirt and glare: up to</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/05f/840/5a4/05f8405a40c3fc111b23ab7bcc3f8be7.jpg"></div><br>  <i>Dirt and glare: after</i> <br><br><h3>  Motion blur </h3><br>  Remember that we generated a velocity buffer at the very beginning of the frame?  Finally it is time to apply motion blur to the scene.  The technique used by the Fox Engine engine was inspired by an <a href="http://casual-effects.com/research/McGuire2012Blur/index.html">article from the MHBO 2012</a> . <br><br>  It generates a low resolution map with square tiles containing the maximum pixel speed.  The image of the scene is locally stretched along the direction of the velocity vectors to create the impression of movement. <br><br>  In our scene, the motion blur effect is difficult to visualize, because there is almost no motion in it. <br><br><h3>  Color correction </h3><br><p>  <a href="https://en.wikipedia.org/wiki/Color_grading">Color correction</a> is performed to adjust the final color of the scene.  Artists must balance colors, apply filters, etc.  All this is done by an operation that takes the original RGB value of a pixel and matches it with the new RGB value;  work takes place in the LDR space. <br><br>  In some cases, you can come up with some kind of mathematical function that performs this conversion (this is what the tonal compression operator does, which transforms from HDR to LDR), but usually artists need more control over color conversion and no mathematical function can cope with it. <br><br>  In this case, we have to accept and apply the brute force method: use the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B0%25D0%25B1%25D0%25BB%25D0%25B8%25D1%2586%25D0%25B0_%25D0%25BF%25D0%25BE%25D0%25B8%25D1%2581%25D0%25BA%25D0%25B0">lookup table</a> (LUT) comparing each possible RGB value with another RGB value. <br><br>  Sounds crazy?  Let's estimate: there are 256 x 256 x 256 possible RGB values, that is, we have to store more than 16 million comparisons! <br><br>  It will be difficult to effectively feed them to the pixel shader ... if we don‚Äôt resort to any trick. <br><br>  And the trick is to consider the RGB space as a three-dimensional cube, defined in three axes: red, green and blue. <br><br></p><table><tbody><tr><td><img src="https://habrastorage.org/webt/io/vd/zb/iovdzboikl8pcft7n4hvwbnymck.png"><br>  RGB cube </td><td>  We take this cube to the left, cut it into 16 ‚Äúslices‚Äù and store each slice in a 16 x 16 texture. <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d7/5a9/4b6/9d75a94b6f2d2cf74a24bdcf9e59395f.png"></div><br>  As a result, we get 16 slices, which are shown on the right. </td><td><img src="https://habrastorage.org/webt/vu/0c/2t/vu0c2tirf3h00086knul5lbw6me.png"><br>  LUT </td></tr></tbody></table><br>  So, we "sampled" our cube to 16 x 16 x 16 voxels, that is, only 4,096 comparisons, which is only a small fraction of the 16 million elements.  How do we recreate intermediate elements?  With the help of <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25BD%25D0%25B5%25D0%25B9%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25B8%25D0%25BD%25D1%2582%25D0%25B5%25D1%2580%25D0%25BF%25D0%25BE%25D0%25BB%25D1%258F%25D1%2586%25D0%25B8%25D1%258F">linear interpolation</a> : for the desired RGB color, we simply look at 8 of its closest neighbors in the cube, the exact mapping of which we know. <br><br>  In practice, this means finding two layers that are closest in value to blue, and then searching each layer for the four nearest pixels by the value of red and green. <br><br>  Then, linear interpolation is simply performed: a weighted average of eight colors is calculated taking into account the distances affecting the weights.  Such interpolation from a smaller number of values ‚Äã‚Äãworks well, because color correction is usually performed with low-frequency variations. <br><br>  You can save 16 layers in a 3D texture inside a video processor, and the shader code becomes very simple: just request a search for certain 3D coordinates, the equipment performs trilinear filtering of the eight nearest points and returns the correct value.  Quick and easy. <br><br>  So, we have a way to encode color matching using this lookup table based on 16 slices, but how does the artist actually create such a table? <br><br>  It's simple enough: just lay out all the cuts next to each other to get a similar image: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/37c/cc8/3e2/37ccc83e282f196a2861cde227dcaedd.png"></div><br>  <em>256x16 LUT texture</em> <br><br>  Then we take a screenshot of the game in the scene that needs color correction.  We insert the LUT image into any corner of the screenshot, give the image to the artists and allow them to create all the magic.  They use a graphic editor, make the necessary changes, and then send us the corrected image.  The corner-built LUT will reflect the new RGB color matching. <br><br>  Now you can simply extract the modified LUT 256x16 and transfer it directly to the game engine. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/05f/840/5a4/05f8405a40c3fc111b23ab7bcc3f8be7.jpg"></div><br>  <i>Color correction + Bloom: up (LUT:</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/37c/cc8/3e2/37ccc83e282f196a2861cde227dcaedd.png"></i>  <i>)</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/970/dbf/1e2/970dbf1e20d26645d7182a6d07a8822a.jpg"></div><br>  <i>Color correction + Bloom: after (LUT:</i> <i><img src="https://habrastorage.org/getpro/habr/post_images/341/743/697/34174369753b54e8250a694fd51ecb81.png"></i>  <i>)</i> <br><br>  At this stage, before applying color correction, a buffer bloom is added over the scene. <br><br><h3>  Anti-aliasing </h3><br>  The edges of the meshes repeat the pixel grid of the frame buffer too much, so we see sharply broken boundaries that look unnatural. <br><br>  This is a limitation of deferred rendering: each pixel stores only one information;  in direct rendering, this problem is less pronounced, because <a href="https://en.wikipedia.org/wiki/Multisample_anti-aliasing">MSAA</a> can be used, which creates multiple samples of color per pixel, which provides smoother transitions on the edges. <br><br>  Fox Engine corrects aliasing on the edges, performing the <a href="http://en.wikipedia.org/wiki/Fast_approximate_anti-aliasing">FXAA</a> post-processing stage: the pixel shader seeks to recognize and correct distorted edges based on the color values ‚Äã‚Äãof adjacent pixels. <br><br>  Notice how the ‚Äúladder‚Äù, clearly visible at the border of the railing, is smoothed in the final result. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/74f/853/da1/74f853da191f3bb59b2f6482eca1b33b.png"></div><br>  <i>FXAA: up to</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/220/4f4/8f5/2204f48f5bbdc3da5aff711e071476fa.png"></div><br>  <i>FXAA: after</i> <br><br><h3>  Finishing touches </h3><br>  Have we finished anti-aliasing?  Almost, but not quite!  At the last stage, artists have the opportunity to put masks on certain areas of the image in order to darken or lighten pixels.  This is just a series of sprites drawn on top of the scene.  It is interesting to see how much Fox Engine allows artists to control the image even at the very last stage of rendering. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ba3/5e2/9f3/ba35e29f3ce39d51b2ec4f15c4693174.jpg"></div><br>  <i>Final touches: 0%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/986/682/cfa/986682cfa6cd4f8f8830a6aa5da3b900.jpg"></div><br>  <i>Final touches: 30%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0a6/017/c2b/0a6017c2b578190c977c3cd99593ca63.jpg"></div><br>  <i>Final touches: 60%</i> <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a22/a4c/3e3/a22a4c3e3069a7ee6b5a02f2e611458d.jpg"></div><br>  <i>Finishing touches: 100%</i> <br><br>  And we are done!  Now the frame can be transferred to the monitor, and the video processor will start the same process from scratch to generate a completely new frame. <br><br>  Some metrics of this scene: 2331 draw calls, 623 textures and 73 target renders. <br><br><h1>  Bonus notes </h1><br><h3>  Look at the buffers in action. </h3><br>  Here is a short clip showing the different buffers that I talked about earlier (G-Buffer, SSAO, etc.). <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Zq5mrApE98A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  If you are interested in how the video was recorded: the analysis of this game compared to the previous ones required much more effort.  None of the graphical debuggers could be used here, because MGS V shuts down when it detects <a href="https://en.wikipedia.org/wiki/DLL_injection">DLL injectors</a> that modify certain D3D functions.  I had to roll up my sleeves and forked the old version of <a href="https://reshade.me/">ReShade</a> , which I expanded with my own interceptors.  Thanks to them, I could save buffers, textures, shader binary data ( <a href="http://timjones.io/blog/archive/2015/09/02/parsing-direct3d-shader-bytecode">DXBC</a> containing all the debug data) ... <br><br>  Thanks to my interceptors, it became quite simple to create the video shown above: I could just copy any intermediate buffer into the final frame buffer just before it was transmitted to the monitor. <br><br><h3>  The true face of Ishmael </h3><br>  Ishmael is a mysterious patient lying in a bed next to <em>Snake</em> who helps him escape from the hospital.  His head is wrapped with bandages that hide his true identity.  It was interesting to you to find out, how does he look like in reality? <br><br>  Well, let's see!  Here is the diffusion buffer albedo immediately before and after rendering the bandage. <br><br>  <strong>There are no spoilers</strong> , but just in case I hid the second image. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/70a/849/2bb/70a8492bb0c91b0411f360bc2ed0f4ce.jpg"></div><br><br><div class="spoiler">  <b class="spoiler_title">Ishmael without bandage</b> <div class="spoiler_text"><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/091/302/5d1/0913025d1b078e370fe4b7d5f6948eec.jpg"></div><br>  <i>This ... is not exactly the face that should be</i> </div></div><br>  But let's take one more step! <br><br>  Instead of simply saving the albedo map in the middle of generating the G-buffer, as I did before, it would be nice to see Ishmael's face during the game itself, forbidding the engine to render the bandage. <br><br>  It's pretty easy to do: thanks to my own interceptors, I can disable the transfer of certain calls to the video processor.  A little experimenting, I found two render calls that render the dressings, and sent them to the blacklist. <br><br>  If you want to repeat the experiment yourself, here are the necessary challenges: <br><br> <code>   <br> ID3D11DeviceContext::DrawIndexed( 0x591, 0xF009, 0x0 ); <br> ID3D11DeviceContext::DrawIndexed( 0xB4F, 0xFA59, 0x0 );</code> <br> <br>  Below is a video of the game with the true face of Ishmael.  I use the hot key to switch rendering the armband.  The transition is progressive and reduces the number of triangles used to draw the dressing to complete darkening. <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/T-qHmWIBX-g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  It is worth noting that this trick with blocking draw calls does not work in all cases - sometimes the source mesh simply does not contain data for hidden surfaces.  This is logical, because it allows you to optimize performance: less unnecessary geometry is transferred to the video processor! <br><br>  For example, the model of the ‚Äúthird child‚Äù does not have triangles under a gas mask in the lower part of the face, we will never see its nose and mouth, simply because they do not exist. <br><br><h1>  Additional links </h1><br>  This completes the MGS V analysis.  I hope you become better at understanding how Fox Engine renders the frame. <br><br>  If you want to know more, below I provide links to additional materials: <br><br><ul><li>  <a href="https://www.gdcvault.com/play/1018086/Photorealism-Through-the-Eyes-of">Photorealism Through the Eyes of a FOX: The Core of Metal Gear Solid Ground Zeroes</a> (GDC 2013);  report Kojima Productions </li><li>  <a href="http://www.eurogamer.net/articles/digitalfoundry-tech-analysis-mgs5-fox-engine">Tech Analysis: Metal Gear Solid 5's FOX Engine</a> ;  Digital Foundry Report </li><li>  In <a href="https://joey35233.github.io/">MGS V PBR Texture Analysis, the</a> game map formats are documented. </li><li>  <a href="https://www.geforce.com/whats-new/guides/metal-gear-solid-v-the-phantom-pain-graphics-and-performance-guide">MGS V NVIDIA Performance Guide</a> with details on graphic settings. </li><li>  Special thanks to <a href="http://patrick.mours.net/">Patrick Mours</a> for revealing the <a href="https://reshade.me/">ReShade</a> source codes this year. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/344916/">https://habr.com/ru/post/344916/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../344902/index.html">Simple implementation of Token for mobile application interaction with WebAPI</a></li>
<li><a href="../344908/index.html">Configure Nginx + PHP-FPM and HTTPS from Let's Encrypt on AWS EC2 with Ubuntu Server 16.04 LTS</a></li>
<li><a href="../344910/index.html">Modular grid layout from scratch: analysis, calculation and construction</a></li>
<li><a href="../344912/index.html">The battle for network neutrality: wars with operators and first courts</a></li>
<li><a href="../344914/index.html">New generation of networks: the first specification 5G is introduced</a></li>
<li><a href="../344918/index.html">Give me your salary and I will tell you who you are</a></li>
<li><a href="../344920/index.html">The grand piano must disappear: the levels of professional development and their evaluation, the programmers</a></li>
<li><a href="../344922/index.html">How did I manage to hack the application</a></li>
<li><a href="../344924/index.html">Digital events in Moscow from December 18 to 24</a></li>
<li><a href="../344926/index.html">The digest of fresh materials from the world of the frontend for the last week ‚Ññ293 (December 11 - 17, 2017)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>