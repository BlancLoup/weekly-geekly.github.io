<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Thematic modeling of repositories on GitHub</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Thematic modeling is a machine learning section dedicated to extracting abstract ‚Äútopics‚Äù from a set of ‚Äúdocuments‚Äù. Each ‚Äúdocument‚Äù is represented by...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Thematic modeling of repositories on GitHub</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/b7f/300/947/b7f300947c670121196b854ade922a20.png" alt="word cloud"><br>  <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5">Thematic modeling</a> is a machine learning section dedicated to extracting abstract ‚Äútopics‚Äù from a set of ‚Äúdocuments‚Äù.  Each ‚Äúdocument‚Äù is represented by a <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag of words</a> , i.e.  many words along with their frequencies.  Introduction to thematic modeling is perfectly described by prof.  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%25A3%25D1%2587%25D0%25B0%25D1%2581%25D1%2582%25D0%25BD%25D0%25B8%25D0%25BA:Vokov">KV Vorontsov</a> in the lectures of the ShAD [ <a href="http://www.machinelearning.ru/wiki/images/e/e6/Voron-ML-TopicModeling-slides.pdf">PDF</a> ].  The most famous TM model is, of course, the <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BD%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BC%25D0%25B5%25D1%2589%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2594%25D0%25B8%25D1%2580%25D0%25B8%25D1%2585%25D0%25BB%25D0%25B5">Dirichlet Latent Distribution</a> (LDA).  Konstantin Vyacheslavovich was able to summarize all possible thematic models based on a bag of words in the form of <a href="http://link.springer.com/article/10.1007/s10994-014-5476-6">additive regularization</a> (ARTM).  In particular, LDA is also included in many ARTM models.  ARTM ideas are embodied in the project <a href="https://github.com/bigartm/bigartm">BigARTM</a> . <br><br>  Usually thematic modeling is applied to text documents.  We at <a href="http://sourced.tech/">source {d}</a> (a startup in Spain) digest the big date obtained from the GitHub repositories (and will soon take on every publicly available repository in the world).  Naturally, the idea arose to interpret each repository as a bag of words and set BigARTM apart.  In this article we will talk about how we have completed in fact the world's first case study of the largest repository of open source projects, what came of it and how to repeat it.  <strong>docker inside!</strong> <br><a name="habracut"></a><br>  TL; DR: <br><br><pre><code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">docker</span></span> run srcd/github_topics apache/spark</code> </pre>  (replace <code>apache/spark</code> with any GitHub rep as desired). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      ¬ª <a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods">OpenDocument table with extracted themes.</a> <br>  " <a href="">JSON with extracted themes.</a> <br>  " <a href="">Trained model</a> - 40MB, gzipped pickle for Python 3.4+, Pandas 1.18+. <br>  ¬ª <a href="https://data.world/vmarkovtsev/github-source-code-names">Dataset on data.world</a> . <br><br><h2>  Theory </h2><br><p>  The thematic probabilistic model on a set of documents <img src="https://tex.s2cms.ru/svg/%5Cinline%20D" alt="\ inline D">  which describes the frequency of the word <img src="https://tex.s2cms.ru/svg/%5Cinline%20w" alt="\ inline w">  in the document <img src="https://tex.s2cms.ru/svg/%5Cinline%20d" alt="\ inline d">  with themes <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t">  : </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0Ap(w%7Cd)%20%3D%20%5Csum_%7Bt%5Cin%20T%7D%20p(w%7Ct)%20p(t%7Cd)%0A" alt="p (w | d) = \ sum_ {t \ in T} p (w | t) p (t | d)"></div><p></p><br>  Where <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(w%7Ct)" alt="\ inline p (w | t)">  - probability of a word relation <img src="https://tex.s2cms.ru/svg/%5Cinline%20w" alt="\ inline w">  to the topic <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t">  , <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(t%7Cd)" alt="\ inline p (t | d)">  - the probability of a relationship topic <img src="https://tex.s2cms.ru/svg/%5Cinline%20t" alt="\ inline t">  to document <img src="https://tex.s2cms.ru/svg/%5Cinline%20d" alt="\ inline d">  , so  the formula above is just an expression of the <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25BE%25D1%2580%25D0%25BC%25D1%2583%25D0%25BB%25D0%25B0_%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B2%25D0%25B5%25D1%2580%25D0%25BE%25D1%258F%25D1%2582%25D0%25BD%25D0%25BE%25D1%2581%25D1%2582%25D0%25B8">total probability</a> , provided that the hypothesis of independence of random variables is true: <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(w%7Cd%2Ct)%20%3D%20p(w%7Ct)" alt="\ inline p (w | d, t) = p (w | t)">  .  Words are taken from the dictionary <img src="https://tex.s2cms.ru/svg/%5Cinline%20W" alt="\ inline W">  themes belong to the set <img src="https://tex.s2cms.ru/svg/%5Cinline%20T" alt="\ inline T">  which is just a series of indexes <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5B1%2C%202%2C%20%5Cdots%20n_t%5D" alt="\ inline [1, 2, \ dots n_t]">  . <br><br>  We need to recover <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(w%7Ct)" alt="\ inline p (w | t)">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20p(t%7Cd)" alt="\ inline p (t | d)">  from a given set of documents <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cleft%5C%7Bd%5Cin%20D%3A%20d%20%3D%20%5Cleft%5C%7Bw_1%20%5Cdots%20w_%7Bn_d%7D%5Cright%5C%7D%5Cright%5C%7D" alt="\ inline \ left \ {d \ in D: d = \ left \ {w_1 \ dots w_ {n_d} \ right \} \ right \}">  .  It is commonly believed that <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Chat%7Bp%7D(w%7Cd)%20%3D%20%5Cfrac%7Bn_%7Bdw%7D%7D%7Bn_d%7D" alt="\ inline \ hat {p} (w | d) = \ frac {n_ {dw}} {n_d}">  where <img src="https://tex.s2cms.ru/svg/%5Cinline%20n_%7Bdw%7D" alt="\ inline n_ {dw}">  - number of entries <img src="https://tex.s2cms.ru/svg/%5Cinline%20w" alt="\ inline w">  in document <img src="https://tex.s2cms.ru/svg/%5Cinline%20d" alt="\ inline d">  However, this implies that all words are equally important, which is not always true.  By ‚Äúimportance‚Äù, this refers to a measure that is negatively correlated with the total frequency of the word in documents.  Denote the recoverable probabilities. <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Chat%7Bp%7D(w%7Ct)%20%3D%20%5Cphi_%7Bwt%7D" alt="\ inline \ hat {p} (w | t) = \ phi_ {wt}">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Chat%7Bp%7D(t%7Cd)%20%3D%20%5Ctheta_%7Btd%7D" alt="\ inline \ hat {p} (t | d) = \ theta_ {td}">  .  So  our task is reduced to a stochastic matrix decomposition, which is incorrectly set: <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cfrac%7Bn_%7Bdw%7D%7D%7Bn_d%7D%20%E2%89%88%20%5CPhi%20%5Ccdot%20%5CTheta%20%3D%20(%5CPhi%20S)(S%5E%7B-1%7D%5CTheta)%20%3D%20%5CPhi'%20%5Ccdot%20%5CTheta'%0A" alt="\ frac {n_ {dw}} {n_d} ‚âà \ Phi \ cdot \ Theta = (\ Phi S) (S ^ {- 1} \ Theta) = \ Phi '\ cdot \ Theta'"></div><p></p><br>  In machine learning problems, <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2583%25D0%25BB%25D1%258F%25D1%2580%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F_(%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25BA%25D0%25B0)">regularization is</a> usually used as a way to improve the characteristics of a model on unknown data (as a result, <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">re-training</a> decreases, complexity, etc.);  in our case, it is simply <strong>necessary</strong> . <br><br>  Tasks like the one described above are solved using <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BC%25D0%25B0%25D0%25BA%25D1%2581%25D0%25B8%25D0%25BC%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25B4%25D0%25BE%25D0%25BF%25D0%25BE%25D0%25B4%25D0%25BE%25D0%25B1%25D0%25B8%25D1%258F">the maximum likelihood method</a> : <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Csum_%7Bd%5Cin%20D%7D%5Csum_%7Bw%5Cin%20d%7Dn_%7Bdw%7D%5Cln%20%5Csum_%7Bt%7D%5Cphi_%7Bwt%7D%20%5Ctheta_%7Btd%7D%20%5Cto%20%5Cmax_%7B%5CPhi%2C%5CTheta%7D%0A" alt="\ sum_ {d \ in D} \ sum_ {w \ in d} n_ {dw} \ ln \ sum_ {t} \ phi_ {wt} \ theta_ {td} \ to \ max _ {\ Phi, \ Theta}"></div><p></p><br><p>  under conditions </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Cphi_%7Bwt%7D%20%3E%200%3B%20%5Csum_%7Bw%5Cin%20W%7D%5Cphi_%7Bwt%7D%20%3D%201%3B%0A%5Ctheta_%7Btd%7D%20%3E%200%3B%20%5Csum_%7Bt%5Cin%20T%7D%5Ctheta_%7Btd%7D%20%3D%201.%0A" alt="\ phi_ {wt} &amp; gt; 0; \ sum_ {w \ in W} \ phi_ {wt} = 1; \ theta_ {td} &amp; gt; 0; \ sum_ {t \ in T} \ theta_ {td} = 1."></div><p></p><br><p>  The essence of ARTM is to naturally add regularization in the form of additional terms: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0A%5Csum_%7Bd%5Cin%20D%7D%5Csum_%7Bw%5Cin%20d%7Dn_%7Bdw%7D%5Cln%20%5Csum_%7Bt%7D%5Cphi_%7Bwt%7D%20%5Ctheta_%7Btd%7D%20%2B%20R(%5CPhi%2C%5CTheta)%20%5Cto%20%5Cmax_%7B%5CPhi%2C%5CTheta%7D%0A" alt="\ sum_ {d \ in D} \ sum_ {w \ in d} n_ {dw} \ ln \ sum_ {t} \ phi_ {wt} \ theta_ {td} + R (\ Phi, \ Theta) \ to \ max_ {\ Phi, \ Theta}"></div><p></p><br><p>  Since this is a simple addition, we can combine different regularizers in one optimization, for example, thin the matrix and increase the independence of the topics.  LDA is formulated in ARTM terms like this: </p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%0AR(%5CPhi%2C%5CTheta)_%7BDirichlet%7D%20%3D%20%5Csum_%7Bt%2Cw%7D%20(%5Cbeta_w%20-%201)%5Cln%20%5Cphi_%7Bwt%7D%20%2B%20%5Csum_%7Bd%2Ct%7D%20(%5Calpha_t%20-%201)%5Cln%20%5Ctheta_%7Bt%2Cd%7D%0A" alt="R (\ Phi, \ Theta) _ {Dirichlet} = \ sum_ {t, w} (\ beta_w - 1) \ ln \ phi_ {wt} + \ sum_ {d, t} (\ alpha_t - 1) \ ln \ theta_ {t, d}"></div><p></p><br><p>  Variables <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CPhi" alt="\ inline \ Phi">  and <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5CTheta" alt="\ inline \ Theta">  can be efficiently computed using an iterative <a href="https://ru.wikipedia.org/wiki/EM-%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">EM algorithm</a> .  Dozens of ready-made ARTM regularizers are ready for battle as part of <a href="https://github.com/bigartm/bigartm">BigARTM</a> . <br><br>  At this, the forced rewrite of the ShAD lecture ends and begins <br><br></p><h2>  Practice </h2><br><p>  As of October 2016, about 18 million repositories on GitHub were available for analysis.  They are actually much more, we just dropped the forks and the ‚Äúhard forks‚Äù (the fork is not marked by GitHub).  We put each repository this <img src="https://tex.s2cms.ru/svg/d" alt="d">  , and each name in the source is <img src="https://tex.s2cms.ru/svg/w" alt="w">  .  Source analysis was done with the same tools as with deep source code training in early experiments (see our presentations from the latest <a href="https://www.re-work.co/">RE ¬∑ WORK</a> conferences: <a href="https://goo.gl/4zq8g9">Berlin</a> and <a href="https://goo.gl/wRQCLS">London</a> ): primary <a href="https://github.com/github/linguist">github / linguist</a> classification and parsing based on <a href="http://pygments.org/">Pygments</a> .  General-purpose text files were discarded, such as <a href="http://readme.md/">README.md</a> . <br><br>  The names from the source should not be extracted "in the forehead", for example, <code>class FooBarBaz</code> adds 3 words to the bag: <code>foo</code> , <code>bar</code> and <code>baz</code> , and <code>int wdSize</code> adds two: <code>wdsize</code> and <code>size</code> .  In addition, the names were stamped by the <a href="http://snowballstem.org/">Snowball</a> from <a href="http://www.nltk.org/">NLTK</a> , although we didn‚Äôt specifically explore the benefits of this.  The final preprocessing stage consisted in calculating the logarithmic version of the <a href="https://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf">TF-IDF</a> weighting (again, we did not specifically investigate, just copied the <a href="https://www.quora.com/Why-is-the-performance-improved-by-using-TFIDF-instead-of-bag-of-words-in-LDA-clustering">solutions</a> from the usual NLP) and filtering too rare and commonly used names, in our case, the boundaries were 50 and 100000, respectively. <br>  After ARTM produced the result, it was necessary to manually give the names to the topics, based on keywords and repositories-representatives.  The number of topics was set at 200, and as it turned out, it was necessary to put more, because  There are a lot of topics on Gitkhab.  Tedious work took a whole week. <br><br>  Pre-processing was performed on <a href="https://cloud.google.com/dataproc/">Dataproc</a> aka Spark in the Google cloud, and the main actions were performed locally on a powerful computer.  The resulting sparse matrix had a size of about 20 GB, and it had to be converted into Vowpal Wabbit text format so that it could be digested by BigARTM CLI.  The data were milled fairly quickly, in a couple of hours: <br><br></p><pre> <code class="hljs javascript">bigartm -c dataset_vowpal_wabbit.txt -t <span class="hljs-number"><span class="hljs-number">200</span></span> -p <span class="hljs-number"><span class="hljs-number">10</span></span> --threads <span class="hljs-number"><span class="hljs-number">10</span></span> --write-model-readable bigartm.txt --regularizer <span class="hljs-string"><span class="hljs-string">"0.05 SparsePhi"</span></span> <span class="hljs-string"><span class="hljs-string">"0.05 SparseTheta"</span></span> Parsing text collection... OK. Gathering dictionary <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> batches... OK. Initializing random model <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> dictionary... OK. Number <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> tokens <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> the model: <span class="hljs-number"><span class="hljs-number">604989</span></span> ================= Processing started. Perplexity = <span class="hljs-number"><span class="hljs-number">586350</span></span> SparsityPhi = <span class="hljs-number"><span class="hljs-number">0.00214434</span></span> SparsityTheta = <span class="hljs-number"><span class="hljs-number">0.422496</span></span> ================= Iteration <span class="hljs-number"><span class="hljs-number">1</span></span> took <span class="hljs-number"><span class="hljs-number">00</span></span>:<span class="hljs-number"><span class="hljs-number">11</span></span>:<span class="hljs-number"><span class="hljs-number">57.116</span></span> Perplexity = <span class="hljs-number"><span class="hljs-number">107901</span></span> SparsityPhi = <span class="hljs-number"><span class="hljs-number">0.00613982</span></span> SparsityTheta = <span class="hljs-number"><span class="hljs-number">0.552418</span></span> ================= Iteration <span class="hljs-number"><span class="hljs-number">2</span></span> took <span class="hljs-number"><span class="hljs-number">00</span></span>:<span class="hljs-number"><span class="hljs-number">12</span></span>:<span class="hljs-number"><span class="hljs-number">03.001</span></span> Perplexity = <span class="hljs-number"><span class="hljs-number">60701.5</span></span> SparsityPhi = <span class="hljs-number"><span class="hljs-number">0.102947</span></span> SparsityTheta = <span class="hljs-number"><span class="hljs-number">0.768934</span></span> ================= Iteration <span class="hljs-number"><span class="hljs-number">3</span></span> took <span class="hljs-number"><span class="hljs-number">00</span></span>:<span class="hljs-number"><span class="hljs-number">11</span></span>:<span class="hljs-number"><span class="hljs-number">55.172</span></span> Perplexity = <span class="hljs-number"><span class="hljs-number">20993.5</span></span> SparsityPhi = <span class="hljs-number"><span class="hljs-number">0.458439</span></span> SparsityTheta = <span class="hljs-number"><span class="hljs-number">0.902972</span></span> ================= Iteration <span class="hljs-number"><span class="hljs-number">4</span></span> took <span class="hljs-number"><span class="hljs-number">00</span></span>:<span class="hljs-number"><span class="hljs-number">11</span></span>:<span class="hljs-number"><span class="hljs-number">56.804</span></span> ...</code> </pre><br><p>  <code>-p</code> sets the number of iterations.  There was no certainty which regularizers to use, so only ‚Äúsparsity‚Äù was activated.  Affected by the lack of detailed documentation (the developers have promised to fix it).  It is important to note that the amount of RAM that was required to work at the peak was no more than 30 GB, which is very cool against the backdrop of <a href="https://radimrehurek.com/gensim/">gensim</a> and, God forgive me, <a href="http://scikit-learn.org/stable/">sklearn</a> . </p><br><h2>  Topics </h2><br><p>  As a result, 200 topics can be divided into the following groups: </p><br><ul><li>  <strong>Concepts</strong> - common, broad and abstract. </li><li>  <strong>Human languages</strong> - it turned out that the code can roughly determine the native language of the programmer, probably partly due to the offset from the stemming. </li><li>  <strong>Programming languages</strong> are not so interesting.  We already know this information.  The PLs usually have the aka standard library ‚Äúbatteries‚Äù from classes and functions that are imported / included in the source code, and the corresponding names are detected by our thematic modeling.  Some topics turned out to be narrower than PL. </li><li>  <strong>General IT</strong> - would fall into the <em>Concept</em> if they had an expressive list of keywords.  Repositories are often associated with a unique set of names, for example, we say Rails, we keep ActiveObject and other Active in mind.  Partially echoes <a href="https://habrahabr.ru/post/247363/">Programming Philosophy 2 - Myth and language</a> . </li><li>  <strong>Communities</strong> - dedicated to specific, potentially narrow technologies or products. </li><li>  <strong>Games</strong> </li><li>  <strong>Brad</strong> - 2 topics could not find a reasonable explanation. </li></ul><br><h3>  Concepts </h3><br><p>  Perhaps the most interesting group with a bunch of extracted facts from everyday life: </p><br><ol><li>  There is cheese in pizza, and many repositories mention it. </li><li>  Terms from mathematics, linear algebra, cryptography, machine learning, digital signal processing, genetic engineering, elementary particle physics. </li><li>  Days of the week.  Monday, Tuesday, etc. </li><li>  All sorts of facts and characters from RPG and other fantasy games. </li><li>  IRC has pseudonyms. </li><li>  Many design patterns (we say thank you for Java and PHP). </li><li>  Colors.  Including some exotic ones (we say thank you for <a href="https://habrahabr.ru/company/ua-hosting/blog/269013/">CSS</a> ). </li><li>  The email has CC, BCC, and it is sent via SMTP protocol and received by POP / IMAP. </li><li>  How to create a good datetime picker.  It seems to be a very typical project on github, hehe. </li><li>  People work for money and spend it on buying houses and driving (obviously, from home to work and back). </li><li>  All kinds of "iron". </li><li>  A comprehensive list of HTTP, SSL, Internet, Bluetooth and WiFi terms. </li><li>  Everything you want to know about memory management. </li><li>  What to eat there is a wish to make your own firmware based on Android. </li><li>  Bar codes.  A huge number of different species. </li><li>  People.  They are divided into men and women, they live and have sex. </li><li>  Excellent list of text editors. </li><li>  Weather.  Many typical words. </li><li>  Open licenses.  Generally speaking, they should not have been included in a separate topic.  The names and texts of licenses in theory do not overlap.  From experience with Pygments, some PLs are much worse supported than others and, apparently, some were incorrectly parsed. </li><li>  Commerce.  The shops offer discounts and sell goods to customers. </li><li>  Bitcoin and blockchain. </li></ol><br><h3>  Human languages </h3><br><p>  The list of topics includes Spanish, Portuguese, French and Chinese.  Russian has not been formed into a separate topic, which indicates more likely a higher level of our programmers on GitHub, who write directly in English, than a small number of Russian repositories.  In this sense, Chinese repositories are being killed. </p><br><h3>  Programming languages </h3><br><p>  An interesting find in PL is the ‚ÄúNon-native English PHP‚Äù theme associated with PHP projects written by people for whom English is not native.  Apparently, these two groups of programmers write code in a fundamentally different way.  In addition, there are two topics related to Java: JNI and bytecode. </p><br><h3>  General IT </h3><br><p>  This is not so interesting.  There are many repositories with OS kernels ‚Äî large, noisy, and despite our efforts, they messed up some topics.  However, something worth mentioning is: </p><br><ul><li>  Lots of information about drones.  They work on Linux. </li><li>  There are many implementations of Ruby.  Often, there are ‚Äúextreme forks‚Äù when people take someone else‚Äôs code base and commit one with a loss of change history. </li><li>  onmouseup, onmousedown and onmousemove are the three giants on whom the UI stands. </li><li>  A huge number of buzzwords and technologies from the world of Javascript. </li><li>  Online learning platforms.  Especially <a href="https://moodle.org/">Moodle</a> .  Many, many Moodle. </li><li>  All ever created open CMS. </li><li>  The Coursera Machine Learning theme provides an excellent list of home-based repositories for Coursera courses on machine learning. </li></ul><br><h3>  Communities </h3><br><p>  The group is the largest in number, almost 100. Many repositories have turned out to be private cloud repositories of configurations for text editors, especially Vim and Emacs.  Since  Vim has only one topic, while Emacs has two topics, I hope this will put an end to the argument which editor is better! <br><br>  We met sites on all known web engines written in Python, Ruby, PHP, Java, Javascript, etc.  PHP sites use Wordpress, Joomla, Yii, VTiger, Drupal, Zend, Cake and Symphony engines for some reason with Doctrine (for each topic).  Python: Django, Flask, Google AppEngine.  Ruby: Rails and Rails Only.  <a href="https://habrahabr.ru/post/301532/">Reeeels</a>  Sites in Java collapsed into one mixed topic.  And of course there was a place for sites on Node.js. <br><br>  It turned out that many projects use <a href="https://github.com/tesseract-ocr/tesseract">Tesseract</a> - an open source OCR engine.  In addition, many use <a href="https://github.com/BVLC/caffe">Caffe</a> (and never Tensorflow). <br><br>  Quake 3 / idTech 3 is so popular in game devs that it deserves a separate topic.  Unity3D has two of them, and the first in the bulk is a bunch of student projects and home crafts. <br>  Cocos2D is also popular and has 2 themes.  Finally, there were 3 topics about OpenGL + WebGL.  Probably the difference in the way of working with the API and the binding used (GLUT, etc.). <br><br>  It is not surprising that <a href="https://github.com/chef/chef">Chef</a> , a configuration management tool, divided the topic of cooking (recipes, kitchen, etc.).  However, WinAPI unexpectedly found itself in the same thread with repositories about Pokemon.  There is an assumption that the stemming made the WinAPI character names look like Pokemon names ... </p><br><h3>  Games </h3><br><p>  Many topics are related to <a href="http://www.libsdl.org/">SDL</a> , as well as to Minecraft and RPG. </p><br><h2>  What can be downloaded </h2><br><p>  We prepared the Docker image so that anyone could run our trained model on an arbitrary repository with GitHub.  You just need to perform </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">docker</span></span> run srcd/github_topics apache/spark</code> </pre><br><p>  and you will see the top 5. Inside the image there is a serialized matrix of topics and words, it is available separately: <a href="">link</a> .  The format is <a href="https://docs.python.org/3/library/pickle.html">pickle 4th version</a> with a tuple of length 2, the first element is Pandas 1.8+ <a href="http://pandas.pydata.org/pandas-docs/stable/sparse.html%3Fhighlight%3Dsparsedataframe">SparseDataFrame</a> , and the second is a list with IDF.  In addition, there <a href="https://storage.googleapis.com/github-repositories-topic-modeling/topics.ods">is an OpenDocument table</a> and <a href="">JSON</a> with extracted themes. </p><br><h2>  findings </h2><br>  As it was already written above, 200 topics are too few, many topics turn out to be dual, triple or mild.  When performing a ‚Äúfair‚Äù analysis, we should set 500 or 1000, but we will have to forget about manual labeling of those.  It‚Äôs hard to figure out the infinite number of PHP themes if you‚Äôre not in the thread :).  The perennial reading of articles on Habr√© was definitely useful to me, and still I felt uncomfortable.  But still it turned out interesting.  The outstanding achievement of ARTM in my opinion is the extraction of themes about people, nature, science, and even design patterns from just the names in the source code. <br><br>  Plans to add readme files to the model and possibly other text sources.  Perhaps they will strengthen the group <em>concepts</em> . <br><br><h2>  PS </h2><br><p>  Mining of the source code in the interpretation of classical machine learning (and not just bloopers collected metrics from AST and production) is a new thing, not very popular yet, there are practically no scientific articles.  In perspective, we want and roughly introduce how to replace a part of programmers with a deep neural network that will translate the description of business tasks in natural language into code.  It sounds fantastic, but technology has actually matured, and if it works, there will be a revolution more abruptly than industrialization.  People are sorely lacking!  We are Hirim! <br><br>  The main difficulty in this business - to get access to the data.  The GitHub API limits the number of requests from registered users to 5000 per hour, which of course is not enough if we want to get 18k.  There is a project <a href="http://ghtorrent.org/">GHTorrent</a> , but this is only a faint shadow of the data that we collected.  I had to make a special pipeline on Go, which uses <a href="https://github.com/src-d/go-git">Go-Git</a> for ultra-efficient cloning.  As far as we know, three companies have a complete replica of GitHub: GitHub, <a href="https://sourcegraph.com/">SourceGraph</a> and source {d}. </p><p></p><p></p></div><p>Source: <a href="https://habr.com/ru/post/312596/">https://habr.com/ru/post/312596/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../312584/index.html">‚ÄúThe Future of TV Broadcasts‚Äù: Cloud TV and beyond</a></li>
<li><a href="../312586/index.html">Windows Server 2016 - meet first on UltraVDS</a></li>
<li><a href="../312588/index.html">What happened when we were tired of looking at the schedules of 5,000 servers in monitoring (and when there were more than 10,000 servers)</a></li>
<li><a href="../312592/index.html">We understand in MAVLink. Part 2</a></li>
<li><a href="../312594/index.html">Comparison of the performance of the 1C system under Linux and Windows</a></li>
<li><a href="../312600/index.html">NSCO algorithm (Ho-Kashyap algorithm)</a></li>
<li><a href="../312602/index.html">Open a set of free courses on programming and development for FPGA</a></li>
<li><a href="../312604/index.html">Crossover: High-paying, full-time teleworking for IT professionals</a></li>
<li><a href="../312608/index.html">ESP8266 bath control, 2 years - normal flight</a></li>
<li><a href="../312616/index.html">New PowerShell Features in Windows Server 2016</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>