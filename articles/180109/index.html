<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Hidden Markov chains, Viterbi algorithm</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We need to implement a lie detector that, according to the shaking of a person‚Äôs hands, determines whether he is telling the truth or not. Suppose whe...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Hidden Markov chains, Viterbi algorithm</h1><div class="post__text post__text-html js-mediator-article">  We need to implement a lie detector that, according to the shaking of a person‚Äôs hands, determines whether he is telling the truth or not.  Suppose when a person lies, his hands are shaking a little more.  The signal may be: <br><br><img src="https://habrastorage.org/storage2/dc3/881/b6e/dc3881b6ea209ba018596bf7186569ac.png" alt="Source signal"><br><br>  An interesting method is described in the article ‚ÄúA Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition‚Äù by LR Rabiner, which introduces the Markov hidden circuit model and describes three valuable algorithms: <a href="http://en.wikipedia.org/wiki/Forward%25E2%2580%2593backward_algorithm">The Forward-Backward Procedure</a> , <a href="http://en.wikipedia.org/wiki/Viterbi_algorithm">Viterbi Algorithm</a> and <a href="http://en.wikipedia.org/wiki/Baum%25E2%2580%2593Welch_algorithm">Baum-Welch reestimation</a> .  Despite the fact that these algorithms are of interest only in the aggregate, for a better understanding, it is better to describe them separately. <br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The Markov hidden circuit method is widely used, for example, it is used to search for <a href="http://bioinf.me/courses/bioalgo2">CG islands</a> (DNA, high density cytosine and guanine RNA sections), while <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25B5%25D1%2580,_%25D0%259B%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BD%25D1%2581">Lawrence Rabiner</a> himself used it for speech recognition.  The algorithms were useful to me in order to track the change of sleep / wake stages on the EEG and look for epileptic seizures. <br><br>  In the model of hidden Markov chains, it is assumed that the system under consideration has the following properties: <br><br><ol><li>  in each time period, the system may be in one of a finite set of states; </li><li>  the system randomly changes from one state to another (possibly to the same) and the transition probability depends only on the state it was in; </li><li>  at each moment of time, the system gives one value of the observed characteristic - a random variable depending only on the current state of the system. </li></ol><br><br>  The HMM model can be described as: <img src="https://habrastorage.org/getpro/habr/post_images/045/7b8/cb6/0457b8cb6de8f7bbd44cad8f1a7c1296.png" alt="HMM = \ langle M, S, I, T, E \ rangle"><br><ul><li>  M is the number of states; </li><li><img src="https://habrastorage.org/getpro/habr/post_images/8c8/a59/8c1/8c8a598c1d99c318f59c3001e465223e.png" alt="S = \ {S_1, ..., S_M \}">  - finite set of states; </li><li><img src="https://habrastorage.org/getpro/habr/post_images/c5a/9ac/070/c5a9ac070381e47b6c01167dbb14aaa2.png" alt="I = (P (q_0 = S_i) = \ pi_i)">  - the vector of probabilities that the system is in state i at time 0; </li><li><img src="https://habrastorage.org/getpro/habr/post_images/902/0e8/917/9020e8917097a648b76d2f9e35786590.png" alt="T = \ | P (q_t = S_j | q_ {t-1} = S_i) = a_ {ij} \ |">  - the probability matrix of the transition from state i to state j for <img src="https://habrastorage.org/getpro/habr/post_images/f35/bc4/b80/f35bc4b80fe1243b2277ae615cd731cb.png" alt="\ forall t \ in [1, T_m]">  ; </li><li><img src="https://habrastorage.org/getpro/habr/post_images/92e/de5/bb8/92ede5bb841ae6f99f3b9c71318d1b96.png" alt="E = (E_1, ..., E_M)">  , <img src="https://habrastorage.org/getpro/habr/post_images/10f/a7f/d54/10fa7fd54d130e872a7f5dba5d54a769.png" alt="E_i = f (o_t | q_t = S_i)">  - vector of distributions of observable random variables corresponding to each of the states defined as density or distribution functions, defined on O (set of observed values ‚Äã‚Äãfor all states). </li></ul><br><br>  Time t is assumed to be discrete, given by non-negative integers, where 0 corresponds to the initial moment of time, and Tm is the highest value. <br><br>  When we have 2 hidden states, and the observed values ‚Äã‚Äãobey the normal distribution with different variances, as in the example with the lie detector, for each of the states, the system functioning algorithm can be depicted like this: <br><img src="https://habrastorage.org/storage2/0d2/036/5f7/0d20365f7b09ace6168374f5b82d3eb6.png" alt="Markov hidden circuit diagram"><br><br>  A simpler description was already mentioned in <a href="http://habrahabr.ru/company/surfingbird/blog/177889/">Probabilistic models: examples and pictures</a> . <br><br>  The choice of the vector of distributions of observable quantities often lies with the researcher.  Ideally, the observed value is the hidden state, and the task of determining these states in this case becomes trivial.  In reality, the classical model described by Rabiner, for example, is becoming more complex. <img src="https://habrastorage.org/getpro/habr/post_images/9f9/0a6/70d/9f90a670d9d8debc354bb8787a9fc8f6.png" alt="E_i">  - final discrete distribution.  Something like what is on the chart below: <br><img src="https://habrastorage.org/storage2/d33/0b1/f64/d330b1f640d78c2a3d9ef5900352a538.png"><br><br>  The researcher usually has the freedom to choose the distribution of the observed states.  The stronger the observed values ‚Äã‚Äãdistinguish hidden states the better.  From a programming point of view, going through the various observables means that you need to carefully encapsulate <img src="https://habrastorage.org/getpro/habr/post_images/9f9/0a6/70d/9f90a670d9d8debc354bb8787a9fc8f6.png" alt="E_i">  .  The graph below shows an example of the source data for the lie detector problem, where the distribution of the observed characteristic (hand shake) is continuous, since it was modeled as normal with an average of 0 for both states, with a dispersion equal to 1, when a person tells the truth (purple columns) and 1.21 when lying (green columns): <br><img src="https://habrastorage.org/storage2/685/3ec/5e3/6853ec5e3713409699f700598c772fb7.png"><br><br>  In order not to touch on the issues of setting up the model, we consider a simplified problem. <br><br>  Given: <br><br>  two hidden states, where white noise with variance 1 corresponds to honest behavior, lies ‚Äî white noise with dispersion 1.21; <br>  sample of 10,000 consecutive observations o; <br>  the change of state occurs on average once every 2,500 cycles. <br>  It is required to determine the moments of change of states. <br><br>  Decision: <br><br>  Let's set the five parameters of the model: <br><ul><li>  the number of states M = 2; </li><li>  S = {1,2}; </li><li>  experience shows that the initial distribution is almost irrelevant, so I = (0.5,0.5); </li><li><img src="https://habrastorage.org/getpro/habr/post_images/0c5/ec6/63a/0c5ec663a5381b4c9fbcd35e7abcd289.png" alt="T \ leftarrow \ left (\ begin {matrix} 1-1 / 2500 &amp; 1/2500 \\ 1/2500 &amp; 1-1 / 2500 \ end {matrix} \ right)">  ; </li><li>  E = (N (0.1), N (0.1.21)). </li></ul><br>  Let us find the most plausible states for each moment of time using the Viterbi algorithm for the given model parameters. <br>  The solution of the problem is reduced to specifying the model and running the Viterbi algorithm. <br><br>  Using the model of hidden Markov chains and a sample of values ‚Äã‚Äãof the observed characteristics, we find the sequence of states that best describes the sample in a given model.  The concept can best be interpreted differently, but the well-established solution is the Viterbi algorithm, which finds such a sequence <img src="https://habrastorage.org/getpro/habr/post_images/610/b24/5e3/610b245e3c9a9516c901157b8ae7cc04.png" alt="Q ^ * = (q_0, .., q_ {T_m}), q_i \ in S">  , what <img src="https://habrastorage.org/getpro/habr/post_images/4e6/062/972/4e6062972a36d07e8d0d319a4fab62e6.png" alt="P (Q ^ *, O | HMM) = \ max \ limits_ {Q \ in \ Omega} P (Q, O | HMM)">  . <br><br>  Search task <img src="http://habr.habrastorage.org/post_images/8a2/b60/ebf/8a2b60ebfacb96a0f5706e9203496976.png" alt="Q ^ *">  is solved using dynamic programming, for this we consider the function: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d99/dee/7c3/d99dee7c36f21efb3c67c22df0fed1d2.png" alt="\ delta_t (s) = \ max \ limits_ {I = (i_0, ..., i_ {t-1})} P (q_0 = S_ {i_0}, .., q_ {t-1} = S_ {i_ {t-1}}, q_t = s)"><br>  Where <img src="https://habrastorage.org/getpro/habr/post_images/212/7a0/80b/2127a080b105ae42a467caf8dc6eecdd.png" alt="\ delta_t (s)">  - the highest probability of being able to s at the moment of time t.  Recursively, this function can be defined as follows: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/05f/98c/967/05f98c9671cd3440dfd34703dde52e86.png" alt="\ delta_0 (s) = \ pi_s f_s (o_0), \ delta_t (s) = \ max_ {s '\ in S} (\ delta_ {t-1} (s') a_ {s's}) f_s (o_t)"><br>  Simultaneously with the calculation <img src="https://habrastorage.org/getpro/habr/post_images/212/7a0/80b/2127a080b105ae42a467caf8dc6eecdd.png" alt="\ delta_t (s)">  for each moment of time we remember the most probable state from which we came, that is, <img src="https://habrastorage.org/getpro/habr/post_images/9d6/ae1/174/9d6ae1174d243a40a2c28908adf1aab2.png" alt="q_t = \ arg \ max \ limits_ {s '\ in S} (\ delta_ {t-1} (s') a_ {s's})">  at which the maximum was reached.  Wherein <img src="https://habrastorage.org/getpro/habr/post_images/a4c/5b7/422/a4c5b742289d59493ae6fab90bd0d488.png" alt="q_ {T_m} = \ arg \ max \ limits_ {s \ in S} (\ delta_ {T_m} (s))">  and that means you can recover <img src="https://habrastorage.org/getpro/habr/post_images/4f8/987/768/4f8987768ac5a42b45661fc8c46354cd.png" alt="Q ^ * = (q_0 ^ *, ..., q_ {T_m} ^ *)">  , walking on them in reverse order.  You may notice that <img src="https://habrastorage.org/getpro/habr/post_images/647/aff/620/647aff6206c520f6410e5bb627d0dc00.png" alt="\ max \ limits_ {s \ in S} (\ delta_ {t} (s)) = P (Q ^ *, O | HMM)">  - the value of the desired probability.  Required sequence <img src="http://habr.habrastorage.org/post_images/8a2/b60/ebf/8a2b60ebfacb96a0f5706e9203496976.png" alt="Q ^ *">  found.  An interesting feature of the algorithm is that the estimated function of the observed characteristics is included in <img src="https://habrastorage.org/getpro/habr/post_images/212/7a0/80b/2127a080b105ae42a467caf8dc6eecdd.png" alt="\ delta_t (s)">  in the form of a factor.  If we assume that <img src="https://habrastorage.org/getpro/habr/post_images/a61/a60/5fe/a61a605fe7f6209e4724b98e030a6df3.png" alt="image">  , then in the case of omissions in observations, it is still possible to estimate the most probable hidden states at these moments. <br><br>  The ‚ÄúPseudocode‚Äù for the Viterbi algorithm is sketched below.  It should be noted that all operations with vectors and matrices are <b>element-wise</b> . <br><br><pre><code class="python hljs">Tm&lt;<span class="hljs-number"><span class="hljs-number">-10000</span></span> <span class="hljs-comment"><span class="hljs-comment"># Max time M&lt;-2 # Number of states S&lt;-seq(from=1, to=M) # States [1,2] I&lt;-rep(1./M, each=M) # Initial distribution [0.5, 0.5] T&lt;-matrix(c(1-4./Tm, 4./Tm, 4./Tm,1-4./Tm),2) #[0.9995 0.0005; 0.0005 0.9995] P&lt;-c(function(o){dnorm(o)}, function(o){dnorm(o,sd=1.1)}) # Vector of density functions for each state (N(0,1), N(0,1.21)) E&lt;-function(P,s,o){P[[s]](o)} # Calculate probability of observed value o for state s. Es&lt;-function(E,P,o) { # Same for all states probs&lt;-c() for(s in S) { probs&lt;-append(probs, E(P,s,o)) } return(probs) } viterbi&lt;-function(S,I,T,P,E,O,tm) { delta&lt;-I*Es(E,P,O[1]) # initialization for t=1 prev_states&lt;-cbind(rep(0,length(S))) # zeros for(t in 2:Tm) { m&lt;-matrix(rep(delta,length(S)),length(S))*T # delta(s')*T[ss'] forall s,s' md&lt;-apply(m,2,max) # search for max delta(s) by s' for all s max_delta&lt;-apply(m,2,which.max) prev_states&lt;-cbind(prev_states,max_delta) delta&lt;-md*Es(E,P,O[t]) # prepare next step delta(s) } return(list(delta=delta,ps=prev_states)) # return delta_Tm(s) and paths } restoreStates&lt;-function(d,prev_states) { idx&lt;-which.max(d) s&lt;-c(idx) sz&lt;-length(prev_states[1,]) for(i in sz:2) { idx&lt;-prev_states[idx,i] s&lt;-append(s,idx,after=0) } return (as.vector(s)) }</span></span></code> </pre> <br><br>  Someone could recognize the code in <a href="http://www.r-project.org/">R</a> in this ‚Äúpseudocode‚Äù, but this is far from the best implementation. <br><br>  Let us compare the periods of constancy of the states that were specified in the simulation (the upper part of the graph) and those that were found using the Viterbi algorithm (the lower part of the graph): <br><img src="http://habrastorage.org/storage2/1f9/bf0/4cb/1f9bf04cbb86c3d2e738694590c6b4f2.png"><br><br>  It is quite worthy in my opinion, but every time you should not hope for that. <br><br>  The previously described Viterbi algorithm requires the definition of a hidden Markov chain model (Arguments of the viterbi function).  To set them, a simple initialization is used, which implies that in addition to a sample of observable quantities O, a corresponding selection of hidden states Q, which we know from somewhere, is also given.  In this case, the formulas for estimating the parameters will be as follows: <br><img src="http://mathurl.com/ark5aup.png" alt="S \ leftarrow \ left \ {s: q_t = s, t \ in [0, T_m] \ right \} \\ M \ leftarrow \ | S \ | \\ I \ leftarrow \ left (\ frac {\ sum_ {t \ in [0, T_m]} Id (q_t = s)} {T_m + 1}, s \ in S \ right) \\ T \ leftarrow \ left (\ frac {\ sum_ {t \ in [1, T_m]} Id (q_ {t-1} = s', q_t = s)} {\ sum_ {t \ in [0, T_m]} (\ sum_ { t \ in [1, T_m]} Id (q_ {t-1} = s ')}, s, s' \ in S \ right) \\ E \ leftarrow \ left (f_s (o_t) = \ frac {\ sum_ {t \ in [0, T_m]} Id (q_t = s, o_t = e)} {\ sum_ {t \ in [0, T_m]} Id (q_t = s)}, s \ in S \ right)"><br>  where Id (x) is an indicator function, <img src="http://mathurl.com/at22dw7.png" alt="Id (x) = \ begin {cases} 1, \ mbox {x is true}, \\ 0, \ mbox {x is false.} \ End {cases}"><br><br>  For the considered lie detector example, the algorithm incorrectly classified 413 out of 10,000 states (~ 4%), which is not bad at all.  The Viterbi algorithm is able to accurately detect the moments of change of hidden states, but only if the distributions of the observed characteristics are accurately known.  If only a parametric class of such distributions is known, then there is a way to select the most appropriate parameters, called the Baum-Welch algorithm. <br><br>  If interested, ask to continue ... </div><p>Source: <a href="https://habr.com/ru/post/180109/">https://habr.com/ru/post/180109/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../180097/index.html">Wasted Dreams - the story of a forgotten game (pilot article from the series "The History of the Game Industry")</a></li>
<li><a href="../180099/index.html">Corporation good in shock</a></li>
<li><a href="../1801/index.html">Turkish hackers attacked the MGTU Internet network. N.E. Baumana</a></li>
<li><a href="../18010/index.html">US Army and Security</a></li>
<li><a href="../180107/index.html">Humble Bundle can now pay Bitcoin</a></li>
<li><a href="../180111/index.html">Six weeks before Google Reader closes - we save everything we can</a></li>
<li><a href="../180113/index.html">The dangers of learning from books</a></li>
<li><a href="../180117/index.html">Portable Native Client support appeared in Chrome. Who will win the race for native speed - PNaCl or Asm.js?</a></li>
<li><a href="../180119/index.html">SEGA ported Sonic hedgehog to Andriod and iOS</a></li>
<li><a href="../18012/index.html">Grow Ornament (play)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>