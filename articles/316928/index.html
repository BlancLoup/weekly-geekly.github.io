<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What's new in Windows Server 2016 Failover Clustering</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The author of the article is Roman Levchenko ( www.rlevchenko.com ), MVP - Cloud and Datacenter Management 
 Hello! Most recently, the global availabi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What's new in Windows Server 2016 Failover Clustering</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  <i>The author of the article is Roman Levchenko ( <a href="http://www.rlevchenko.com/">www.rlevchenko.com</a> ), MVP - Cloud and Datacenter Management</i> </blockquote><br>  Hello!  Most recently, the global availability of Windows Server 2016 was announced, which means that you can start using the new version of the product in your infrastructure right now.  The list of innovations is quite extensive and we have already described some of them ( <a href="https://habrahabr.ru/company/microsoft/blog/275997/">here</a> and <a href="https://habrahabr.ru/company/microsoft/blog/277471/">here</a> ), but in this article we will examine high-availability services, which, in my opinion, are the most interesting and used (especially in virtualization environments). <br><br><img src="https://habrastorage.org/files/ee3/554/f93/ee3554f93c5043848714dda2f5fc98ab.jpg"><a name="habracut"></a><br><br><br><h1>  Cluster OS Rolling upgrade </h1><br>  Cluster migration in previous versions of Windows Server causes significant downtime due to the unavailability of the original cluster and the creation of a new based on the updated OS on the nodes, followed by the migration of roles between clusters.  Such a process carries increased requirements for personnel qualifications, certain risks and uncontrolled labor costs.  This fact is especially true for CSP or other customers who have time limits on the unavailability of services within the SLA.  It is not necessary to describe what a significant violation of SLA means for a resource provider 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Windows Server 2016 corrects the situation through the possibility of combining Windows Server 2012 R2 and Windows Server 2016 on the nodes within the same cluster during its upgrade (Cluster OS Rolling Upgrade (hereafter CRU)). <br><br><img src="https://habrastorage.org/files/d1c/87b/159/d1c87b159a474dd48e0bd3dab29f0ccf.png"><br><br>  From the name, you can guess that the cluster migration process consists mainly in a phased reinstallation of the OS on the servers, but let's talk about this in more detail later. <br><br>  We first define the list of "buns" that the CRU provides: <br><br><ul><li>  The complete absence of downtime when upgrading clusters WS2012R2 Hyper-V / SOFS.  For other cluster roles (for example, SQL Server), their unavailability is possible (less than 5 minutes), which is necessary for practicing a one-time failover. </li><li>  No need for additional hardware.  As a rule, the cluster is built from taking into account the possible unavailability of one or several nodes.  In the case of a CRU, the inaccessibility of nodes will be planned and phased.  Thus, if a cluster can safely survive the temporary absence of at least 1 of the nodes, then no additional nodes are required to reach the zero-downtime.  If you plan to upgrade multiple nodes at once (this is supported), then you need to plan in advance for the load distribution between the available nodes. </li><li>  Creating a new cluster is not required.  CRU uses the current CNO. </li><li>  The transition process is reversible (until the cluster level is raised). </li><li>  Support In-Place Upgrade.  But, it is worth noting that the recommended option for updating cluster nodes is a complete WS2016 installation without saving data (clean-os install).  In the case of In-Place Upgrade, verification of the full functionality is required after upgrading each of the nodes (event logs, etc.). </li><li>  CRU is fully supported by VMM 2016 and can be automated additionally via PowerShell / WMI. </li></ul><br>  CRU process using the example of a 2-node Hyper-V cluster: <br><br><ol><li>  A preliminary backup of the cluster (DB) and running resources is recommended.  The cluster must be in a healthy state, the nodes are available.  If necessary, you should correct the existing problems before the migration and pause the backup tasks before starting the transition. <br><br><img src="https://habrastorage.org/files/254/3c7/bba/2543c7bba3ee49369c71d50b05314fc8.png"><br><br></li><li>  Update Windows Server 2012 R2 cluster nodes using Cluster Aware Updating (CAU) or manually via WU / WSUS. </li><li>  With a configured CAU, it is necessary to temporarily disable it to prevent its possible impact on the placement of roles and the state of the nodes during the transition. <br><br><img src="https://habrastorage.org/files/30d/b69/a75/30db69a75b8e4f65b5681d6198cc337d.png"><br><br></li><li>  CPUs on the nodes must have SLAT support to support the execution of virtual machines in WS2016.  This condition is mandatory. </li><li>  At one of the nodes we perform the transfer of roles (drain roles) and exclusion from the cluster (evict): <br><br><img src="https://habrastorage.org/files/5f0/c1a/17a/5f0c1a17a1ee4beb93debdbb8d3b0ea8.png"><br><br></li><li>  After excluding the node from the cluster, we perform the recommended full WS2016 installation (clean OS install, <b>Custom: Install Windows only (advanced)</b> ) <br><br><img src="https://habrastorage.org/files/414/eb8/f31/414eb8f31e9e4d8797d1816dc030192d.png"><br><br></li><li>  After reinstallation, return the network settings back *, update the node and install the necessary roles and components.  In my case, the Hyper-V role and, of course, Failover Clustering are required. <br><br><pre><code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-NetLbfoTeam -<span class="hljs-type"><span class="hljs-type">Name</span></span> HV -TeamMembers tNIC1,tNIC2 -TeamingMode SwitchIndependent -LoadBalancingAlgorithm Dynamic</code> </pre> <br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">Add</span></span>-WindowsFeature Hyper-V, Failover-Clustering -IncludeManagementTools -<span class="hljs-keyword"><span class="hljs-keyword">Restart</span></span></code> </pre> <br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-VMSwitch -InterfaceAlias HV -<span class="hljs-type"><span class="hljs-type">Name</span></span> VM -MinimumBandwidthMode Weight -AllowManagementOS <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre> <br><br>  <i>* Switch Embedded Teaming can be used only after the completion of the transition to WS2016.</i> </li><li>  Add a node to the appropriate domain. <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">Add</span></span>-Computer -ComputerName HV01 -DomainName <span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>.com -DomainCredential <span class="hljs-keyword"><span class="hljs-keyword">domain</span></span>\rlevchenko</code> </pre> <br></li><li>  We return the node to the cluster.  The cluster will start working in mixed mode supporting the functionality of WS2012R2 without supporting the new features of WS2016.  It is recommended to complete the update of the remaining nodes within 4 weeks. <br><br><img src="https://habrastorage.org/files/40b/f9c/6b8/40bf9c6b87004e0f95e4ed36fcce4594.png"><br><br></li><li>  Moving the cluster roles back to the HV01 node to redistribute the load. </li><li>  Repeat steps (4-9) for the remaining node (HV02). </li><li>  After upgrading nodes to WS2016, you need to raise the functional level (Mixed Mode - 8.0, Full - 9.0) of the cluster to complete the migration. <br><br>  <i>PS C: \ Windows \ system32&gt; Update-ClusterFunctionalLevel</i> <i><br><br></i>  <i>Hvcl.</i> <i><br></i>  <i>Warning: You cannot undo this operation.</i>  <i>Do you want to continue?</i> <i><br></i>  <i>[Y] Yes [A] Yes to All [N] No [L] No to All [S] Suspend [?] Help (default is Y): a</i> <i><br><br></i>  <i>Name</i> <i><br></i>  <i>- hvcl</i> <br></li><li>  (optional and with caution) Upgrade the VM configuration version to incorporate new Hyper-V features.  A shutdown of the VM is required and preliminary backup is desirable.  The VM version in 2012R2 is 5.0, in 2016 RTM is 8.0. The example shows the command to update all VMs in the cluster: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-ClusterGroup|? {$_.GroupType -EQ "VirtualMachine"}|<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-VM|<span class="hljs-keyword"><span class="hljs-keyword">Update</span></span>-VMVersion</code> </pre> <br>  List of VM versions supported by 2016 RTM: <br><br><img src="https://habrastorage.org/files/d44/a0d/64a/d44a0d64ab224d6faad06ea4d266309a.png"><br></li></ol><br><h1>  Cloud witness </h1><br>  In any cluster configuration, it is necessary to take into account the peculiarities of Witness placement to provide additional voice and total quorum.  Witness in 2012 R2 can be built on the basis of a common external file share or disk available to each of the cluster nodes.  Let me remind you that the need for Witness configuration is recommended for any number of nodes, starting from 2012 R2 (dynamic quorum). <br><br>  In Windows Server 2016, a new model of quorum configuration based on Cloud Witness is available to provide the ability to build DRs based on Windows Server and other scenarios. <br><br>  Cloud Witness uses Microsoft Azure resources (Azure Blob Storage, via HTTPS, ports on the nodes must be available) to read / write service information, which changes when the status of cluster nodes changes.  The blob file is named according to the cluster's unique identifier - therefore, one Storage Account can be provided to several clusters at once (1 blob file per cluster as part of the automatically created msft-cloud-witness container).  The requirements for the size of cloud storage are minimal to ensure witness work and does not require large expenditures on its support.  The same placement in Azure eliminates the need for a third site in the configuration of Stretched Cluster and solutions for its disaster recovery. <br><br><img src="https://habrastorage.org/files/d3c/ef0/e11/d3cef0e110b34d96b1a59512b93c377b.png"><br><br>  Cloud Witness can be used in the following scenarios: <br><br><ul><li>  To ensure DR cluster located in different sites (multi-site). </li><li>  Clusters without shared storage (Exchange DAG, SQL Always-On and others). </li><li>  Guest clusters running both in Azure and on-premises. </li><li>  Storage Clusters with or without shared storage (SOFS). </li><li>  Clusters within a workgroup or in different domains (new WS2016 functionality). </li></ul><br>  The process of creating and adding Cloud Witness is quite simple: <br><br><ol><li>  Create a new Azure Storage Account (Locally-redundant storage) and in the account properties copy one of the access keys. <br><br><img src="https://habrastorage.org/files/05c/6a4/237/05c6a423762b43f49cda7e0f5b7854dd.png"><br><br></li><li>  Run the Quorum Configuration Wizard and select Select the Quorum Witness - Configure a Cloud Witness. <br><br><img src="https://habrastorage.org/files/66b/8bb/f11/66b8bbf118914ef89ba30ea1371a07f1.png"><br><br></li><li>  Enter the name of the created storage account and insert the access key. <br><br><img src="https://habrastorage.org/files/fd3/6e3/ee7/fd36e3ee73e44d6d9c51b00bdec481f6.png"><br><br></li><li>  After successfully completing the configuration wizard, Witness will appear in Core Resources. <br><br><img src="https://habrastorage.org/files/cf3/b84/a33/cf3b84a337374720a6b512633900e4ca.png"><br><br></li><li>  Blob file in container: <br><br><img src="https://habrastorage.org/files/82d/b79/c5c/82db79c5ca194d07a01509b1281ba5cf.png"><br><br>  For simplicity, you can use PowerShell: <br><br><img src="https://habrastorage.org/files/74c/fea/1af/74cfea1aff624d69975dfaec2ad26c41.png"><br><br></li></ol><br><br><h1>  Workgroup and Multi-Domain Clusters </h1><br>  In Windows Server 2012 R2 and earlier versions, global requirements must be met before creating the cluster: the nodes must be members of the same domain.  The Active Directory Detached cluster, presented in 2012 R2, has a similar requirement and does not simplify it in a significant way. <br><br>  In Windows Server 2016, it is possible to create a cluster without binding to AD within a workgroup or between nodes that are members of different domains.  The process is similar to creating a deattached cluster in 2012 R2, but it has some features: <br><br><ul><li>  Supported only within the WS2016 environment. </li><li>  Requires the role of failover clustering. <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">Install</span></span>-WindowsFeature <span class="hljs-keyword"><span class="hljs-keyword">Failover</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Clustering</span></span> -IncludeManagementTools</code> </pre> <br></li><li>  At each node, you must create a user with a membership in the Administrators group or use the built-in account.  record  Password and username must be identical. <br><br><img src="https://habrastorage.org/files/242/ea0/daa/242ea0daae9e4697b67048f48261f4e7.png"><br><br><pre> <code class="hljs cs">net localgroup administrators cluadm /<span class="hljs-keyword"><span class="hljs-keyword">add</span></span></code> </pre> <br>  If the error ‚ÄúRequested Registry access is not allowed‚Äù <b>appears,</b> you need to change the value of the <b>LocalAccountTokenFilterPolicy</b> policy. <br><br><pre> <code class="hljs tex">New-ItemProperty -Path HKLM:<span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">SOFTWARE</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Microsoft</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Windows</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">CurrentVersion</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Policies</span></span></span></span><span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">System</span></span></span></span> -Name LocalAccountTokenFilterPolicy -Value 1</code> </pre> <br></li><li>  Primary DNS -suffix on the nodes must be defined. <br><br><img src="https://habrastorage.org/files/9e9/9e2/94a/9e99e294aa0348f693f55864f38437b0.jpg"><br><br></li><li>  Cluster creation is supported through both PowerShell and GUI. <br><br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span> -<span class="hljs-type"><span class="hljs-type">Name</span></span> WGCL -Node rtm<span class="hljs-number"><span class="hljs-number">-1</span></span>,rtm<span class="hljs-number"><span class="hljs-number">-2</span></span> -AdministrativeAccessPoint DNS -StaticAddress <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.100</span></span></code> </pre> <br></li><li>  As Witness it is possible to use only Disk Witness or Cloud Witness described earlier.  File Share Witness, unfortunately, is not supported. <br></li></ul><br>  Supported usage scenarios: <br><table><tbody><tr><th>  Role </th><th>  Support status </th><th>  Comment </th></tr><tr><td>  SQL Server </td><td>  Supported by </td><td>  It is recommended to use SQL Server built-in authentication. </td></tr><tr><td>  File server </td><td>  Supported, but not recommended </td><td>  No Kerberos authentication, which is basic for SMB </td></tr><tr><td>  Hyper-v </td><td>  Supported, but not recommended </td><td>  Only Quick Migration is available.  Live Migration not supported </td></tr><tr><td>  Message Queuing (MSMQ) </td><td>  Not supported </td><td>  MSMQ requires ADDS </td></tr></tbody></table><br><h1>  Virtual Machine Load Balancing / Node Fairness </h1><br>  Dynamic optimization, available in VMM, partially migrated to Windows Server 2016 and provides basic load balancing across nodes in automatic mode.  To move resources, Live Migration and Heuristics are used, based on which the cluster decides to balance every 30 minutes or not: <br><br><ol><li>  Current% of memory usage on the node. </li><li>  The average CPU load in the 5 minute interval. </li></ol><br>  The maximum allowable load values ‚Äã‚Äãare determined by the value of <b>AutoBalancerLevel</b> : <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">get</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">cluster</span></span>| fl *autobalancer* AutoBalancerMode : <span class="hljs-number"><span class="hljs-number">2</span></span> AutoBalancerLevel : <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre><br><table><tbody><tr><th>  AutoBalancerLevel </th><th>  Aggressiveness balancing </th><th>  Comment </th></tr><tr><td>  1 (default) </td><td>  Low </td><td>  To carry out balancing when loading a node more than 80% on one of the heuristics </td></tr><tr><td>  2 </td><td>  Medium </td><td>  When loading more than 70% </td></tr><tr><td>  3 </td><td>  High </td><td>  When loading more than 60% </td></tr></tbody></table><br>  The balancer parameters can be defined in the GUI (cluadmin.msc).  By default, the Low level of aggressiveness and the constant balancing mode are used. <br><br><img src="https://habrastorage.org/files/eae/d02/5e7/eaed025e783e4748bce820416f66dea7.png"><br><br>  For verification, I use the following parameters: <br><br>  <i>AutoBalancerLevel: 2</i> <br><pre> <code class="hljs pgsql">(<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span>).AutoBalancerLevel = <span class="hljs-number"><span class="hljs-number">2</span></span></code> </pre> <br>  <i>AutoBalancerMode: 2</i> <br><pre> <code class="hljs pgsql">(<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span>).AutoBalancerMode = <span class="hljs-number"><span class="hljs-number">2</span></span></code> </pre> <br>  We simulate the load first on the CPU (about 88%) and then on the RAM (77%).  Since  the average level of aggressiveness is determined when making a decision about balancing, and our values ‚Äã‚Äãfor loading above a certain value (70%) virtual machines on a loaded node must move to a free node.  The script waits for the moment of live migration and displays the elapsed time (from the starting point of loading to the node until the VM is migrated). <br><br>  In the case of a heavy load on the CPU, the balancer moved more than 1 VM, with a RAM-1 load, the VM was moved within the designated 30 minute interval, during which the node load was checked and the VM was transferred to other nodes to achieve &lt;= 70% of resource utilization. <br><br><img src="https://habrastorage.org/files/ea6/c84/05d/ea6c8405dcbf47a483410a328485df47.png"><br><br>  When using VMM, built-in balancing on nodes is automatically disabled and replaced with a more recommended balancing mechanism based on Dynamic Optimization, which allows you to fine-tune the mode and interval for optimization. <br><br><br><h1>  Virtual machine start ordering </h1><br>  Changing the logic of starting a VM within a cluster in 2012 R2 is based on the concept of priorities (low, medium, high), the task of which is to ensure the inclusion and availability of more important VMs before launching the rest of the "dependent" VMs.  This is usually required for multi-tier services built, for example, based on Active Directory, SQL Server, IIS. <br><br>  To increase functionality and efficiency, Windows Server 2016 adds the ability to define dependencies between VMs or VM groups to decide whether to start them correctly using Set or clusters of clusters.  Mainly aimed at use in conjunction with VM, but can be used for other cluster roles. <br><br><img src="https://habrastorage.org/files/716/e57/085/716e570857da40149f5ec9652523e1ee.png"><br><br>  For example, use the following script: <br><br>  1 VM <b>Clu-VM02</b> is an application dependent on Active Directory availability running on Wirth.  <b>Clu-VM01 machine</b> .  A VM <b>Clu-VM03</b> , in turn, depends on the availability of the application, located on the VM Clu-VM02. <br><br>  Create a new set using PowerShell: <br><br><img src="https://habrastorage.org/files/a34/f2e/f2c/a34f2ef2c4af4e9c82819f33fd37113c.png"><br><br>  <b>VM with Active Directory:</b> <br>  <i>PS C: \&gt; New-ClusterGroupSet -Name AD -Group Clu-VM01</i> <i><br></i>  <i>Name: AD</i> <i><br></i>  <i>GroupNames: {Clu-VM01}</i> <i><br></i>  <i>ProviderNames: {}</i> <i><br></i>  <i>StartupDelayTrigger: Delay</i> <i><br></i>  <i>StartupCount: 4294967295</i> <i><br></i>  <i>IsGlobal: False</i> <i><br></i>  <i>StartupDelay: 20</i> <br><br>  <b>Application:</b> <br>  <i>New-ClusterGroupSet -Name Application -Group Clu-VM02</i> <br><br>  <b>Application dependent service:</b> <br>  <i>New-ClusterGroupSet -Name SubApp -Group Clu-VM03</i> <br><br>  <b>Add dependencies between the sets:</b> <br>  <i>Add-ClusterGroupSetDependency -Name Application -Provider AD</i> <i><br></i>  <i>Add-ClusterGroupSetDependency -Name SubApp -Provider Application</i> <br><br>  If necessary, you can change the set parameters using <b>Set-ClusterGroupSet</b> .  Example: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">Set</span></span>-ClusterGroupSet Application -StartupDelayTrigger Delay -StartupDelay <span class="hljs-number"><span class="hljs-number">30</span></span></code> </pre> <br>  <b>StartupDelayTrigger</b> determines the action to be taken after the start of the group: <br><br><ul><li>  Delay - wait 20 seconds (by default).  Used with StartupDelay. </li><li>  Online - wait for the availability status of the group in set. </li></ul><br>  <i>StartupDelay</i> - delay time in seconds.  20 seconds by default. <br><br>  <i>isGlobal</i> - defines the need to start a set before starting other sets of cluster groups (for example, set with Active Directory VM groups should be globally available and, therefore, start before other collections). <br><br>  Let's try to start VM Clu-VM03: <br><br>  Active Directory accessibility wait on Clu-VM01 (StartupDelayTrigger - Delay, StartupDelay - 20 seconds) <br><br><img src="https://habrastorage.org/files/72b/a66/5fa/72ba665fa2e043469161c0a3e4064e35.png"><br><br>  After starting Active Directory, the dependent application is launched on Clu-VM02 (StartupDelay is used at this stage as well). <br><br><img src="https://habrastorage.org/files/208/f2a/685/208f2a685ac34a44adfb553c5e14fb0f.png"><br><br>  And the last step is to launch the VM Clu-VM03 itself. <br><br><img src="https://habrastorage.org/files/0f0/a94/270/0f0a94270716401fbf7de5579874b1ce.png"><br><br><br><h1>  VM Compute / Storage Resiliency </h1><br>  In Windows Server 2016, new modes of operation of nodes and VMs appeared to increase their resilience in scenarios of problem interaction between cluster nodes and to prevent complete unavailability of resources due to reaction to ‚Äúsmall‚Äù problems before the emergence of more global (proactive) actions. <br><br>  <b>Isolation Mode (Isolated)</b> <br><br>  On the HV01 node, the clustering service suddenly became unavailable, i.e.  the node has problems of intra-cluster interaction.  In this scenario, the node is placed in the Isolated (ResiliencyLevel) state and temporarily excluded from the cluster. <br><br><img src="https://habrastorage.org/files/e30/088/08b/e3008808b4d449199363cb0067914df1.png"><br><br>  The virtual machines on the isolated node continue to run * and become Unmonitored (i.e., the cluster service does not ‚Äúcare‚Äù about VM data). <br><br><img src="https://habrastorage.org/files/08e/1f9/15d/08e1f915d2474d8a86ff91093afcc009.png"><br><br>  * When executing VMs on SMB: Online status and correct execution (SMB does not require ‚Äúcluster identity‚Äù for access).  In the case of the block type of VM storage, the Paused Critical status disappears due to the unavailability of Cluster Shared Volumes for the isolated node. <br><br>  If the node during ResiliencyDefaultPeriod (by default 240 seconds) does not return the clustering service to the system (in our case), then it will move the node to the Down status. <br><br>  <b>Quarantine mode (Quarantined)</b> <br><br>  Suppose that the HV01 node successfully returned the clustering service to its working state, left the Isolated mode, but within an hour the situation repeated 3 or more times (QuarantineThreshold).  In this scenario, the WSFC will place the node in quarantine mode (Quarantined) for 2 hours (QuarantineDuration) and move the VM of this node to a known "healthy" one. <br><br><img src="https://habrastorage.org/files/baa/a4f/bbe/baaa4fbbe01440a1b5289600b9515859.png"><br><br><img src="https://habrastorage.org/files/d52/42b/fa8/d5242bfa85704fa4b003aec6f58ece1f.png"><br><br>  If we are sure that the source of the problems has been eliminated, we can put the node back into the cluster: <br><br><img src="https://habrastorage.org/files/186/488/f9e/186488f9e1fd44f0a24f651a7c1c4506.png"><br><br>  It is important to note that no more than 25% of cluster nodes can be in quarantine at a time. <br>  For customization, use the above parameters and cmdlet Get-Cluster: <br><br><pre> <code class="hljs pgsql">(<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span>). QuarantineDuration = <span class="hljs-number"><span class="hljs-number">1800</span></span></code> </pre> <br>  <b>Storage resiliency</b> <br><br>  In previous versions of Windows Server, working off the inaccessibility of r / w operations for virt.  disk (loss of connection to the storage) is primitive - the VMs are shutting down and a cold boot is required at the subsequent start.  In Windows Server 2016, when such problems occur, the VM switches to the Paused-Critical (AutomaticCriticalErrorAction) status, having previously ‚Äúfrozen‚Äù its operating state (its unavailability will remain, but there will be no unexpected shutdown). <br><br>  When the connection is restored during the timeout (AutomaticCriticalErrorActionTimeout, 30 minutes by default), the VM exits paused-critical and becomes available from the ‚Äúpoint‚Äù when the problem was identified (analogy - pause / play). <br><br>  If the timeout is reached before the storage returns, the VM will turn off (turn off action) <br><br><img src="https://habrastorage.org/files/e23/440/e66/e23440e6651b4bba9cbf4532ccf714d2.png"><br><br><br><h1>  Site-Aware / Stretched Clusters and Storage Replica </h1><br>  A topic that deserves a separate post, but we will try to briefly get acquainted right now. <br><br>  Previously, we were advised to use third-party solutions (a lot of $) to create fully distributed clusters (ensuring SAN-to-SAN replication).  With the advent of Windows Server 2016, reducing the budget by several times and increasing unification when building such systems becomes a reality. <br><br>  Storage Replica allows synchronous (!) And asynchronous replication between any storage systems (including Storage Spaces Direct) and supporting any workloads ‚Äî is the basis of multi-site clusters or a full DR-solution.  SR is available only in the Datacenter edition and can be used in the following scenarios: <br><br><img src="https://habrastorage.org/files/347/a20/2c2/347a202c29f948c98a639641bec0363e.png"><br><br>  Using SR within a distributed cluster, especially the presence of automatic failover and close work with site-awareness, which was also presented in Windows Server 2016. Site-Awarieness allows you to define cluster groups of nodes and link them to a physical location (site fault domain / site) for forming custom failover policies, storing Storage Spaces Direct data and VM distribution logic.  In addition, it is possible to link not only at the site level, but also to lower levels (node, rack, chassis). <br><br><img src="https://habrastorage.org/files/d20/3bb/0ed/d203bb0edeff4f3ab2e32df5d300ee88.png"><br><br><pre> <code class="hljs pgsql"><span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain ‚Äì<span class="hljs-type"><span class="hljs-type">Name</span></span> Voronezh ‚Äì<span class="hljs-keyword"><span class="hljs-keyword">Type</span></span> Site ‚ÄìDescription ‚Äú<span class="hljs-keyword"><span class="hljs-keyword">Primary</span></span>‚Äù ‚Äì<span class="hljs-keyword"><span class="hljs-keyword">Location</span></span> ‚ÄúVoronezh DC‚Äù <span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain ‚Äì<span class="hljs-type"><span class="hljs-type">Name</span></span> Voronezh2 ‚Äì<span class="hljs-keyword"><span class="hljs-keyword">Type</span></span> Site ‚ÄìDescription ‚ÄúSecondary‚Äù ‚Äì<span class="hljs-keyword"><span class="hljs-keyword">Location</span></span> ‚ÄúVoronezh DC2‚Äù <span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain -<span class="hljs-type"><span class="hljs-type">Name</span></span> Rack1 -<span class="hljs-keyword"><span class="hljs-keyword">Type</span></span> Rack <span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain -<span class="hljs-type"><span class="hljs-type">Name</span></span> Rack2 -<span class="hljs-keyword"><span class="hljs-keyword">Type</span></span> Rack <span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain -<span class="hljs-type"><span class="hljs-type">Name</span></span> HPc7000 -<span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Chassis <span class="hljs-built_in"><span class="hljs-built_in">New</span></span>-ClusterFaultDomain -<span class="hljs-type"><span class="hljs-type">Name</span></span> HPc3000 -<span class="hljs-keyword"><span class="hljs-keyword">type</span></span> Chassis <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span>-ClusterFaultDomain ‚Äì<span class="hljs-type"><span class="hljs-type">Name</span></span> HV01 ‚ÄìParent Rack1 <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span>-ClusterFaultDomain ‚Äì<span class="hljs-type"><span class="hljs-type">Name</span></span> HV02 ‚ÄìParent Rack2 <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span>-ClusterFaultDomain Rack1,HPc7000 -parent Voronezh <span class="hljs-keyword"><span class="hljs-keyword">Set</span></span>-ClusterFaultDomain Rack2,HPc3000 -parent Voronezh2</code> </pre> <br><br><img src="https://habrastorage.org/files/561/25e/36c/56125e36c6614390a8a2bd8119a68700.png"><br><br><img src="https://habrastorage.org/files/c20/baf/a6b/c20bafa6ba934235a0fd8ff8d7d2a877.png"><br><br>  This approach within the multi-site cluster carries the following advantages: <br><br><ul><li>  Failover testing initially occurs between nodes within the domain Fault.  If all nodes in the Fault Domain are unavailable, then only move to another. </li><li>  Draining Roles (role migration under maintenance mode, etc.) checks whether it is possible to move to the site first within the local site and only then moves them to another. </li><li>  CSV balancing (redistribution of cluster disks between nodes) will also strive to work out within the framework of the native fault-domain / site. </li><li>  VMs will try to be located in the same site as their dependent CSVs.  If CSV migrates to another site, then VM after 1 minute will begin its migration to the same site. </li></ul><br>  Additionally, using site-awareness logic, it is possible to define a ‚Äúparent‚Äù site for all newly created VMs / roles: <br><br><pre> <code class="hljs pgsql">(<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">Cluster</span></span>).PreferredSite = &lt; &gt;</code> </pre> <br>  Or set up more granularly for each cluster group: <br><br><pre> <code class="hljs pgsql">(<span class="hljs-keyword"><span class="hljs-keyword">Get</span></span>-ClusterGroup -<span class="hljs-type"><span class="hljs-type">Name</span></span> ).PreferredSite = &lt;  &gt;</code> </pre> <br><h1>  Other innovations </h1><br><ul><li>  Support Storage Spaces Direct and Storage QoS. </li><li>  Resizing shared vhdx for guest clusters without downtime, support for Hyper-V replication and res.  copy at the host level. </li><li>  Improved performance and scaling of CSV Cache with support for tiered spaces, direct storage spaces and deduplication (give dozens of GB of RAM under the cache - no problem). </li><li>  Changes in the formation of cluster logs (information about the time zone, etc.) + active memory dump (a new alternative for full memory dump) to simplify diagnosing problems. </li><li>  A cluster can now use multiple interfaces within the same subnet.  Configuring different subnets on the adapters is not required for cluster identification.  Adding occurs automatically. </li></ul><br>  This completes our overview tour of the new WSFC features within Windows Server 2016.  I hope that the material turned out to be useful.  Thanks for reading and comments. <br><br>  Have a great day! </div><p>Source: <a href="https://habr.com/ru/post/316928/">https://habr.com/ru/post/316928/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../316918/index.html">Maximum availability: Veeam Availability Suite 9.5</a></li>
<li><a href="../316920/index.html">DEV Labs 2016. Online conference for C ++ developers. December 10th</a></li>
<li><a href="../316922/index.html">The report on the results of "My Circle" for November 2016, and the most popular vacancies of the month</a></li>
<li><a href="../316924/index.html">How IT professionals work. Ivan Panchenko, Postgres Professional</a></li>
<li><a href="../316926/index.html">Automate business processes or what is "complexity." Part 1</a></li>
<li><a href="../316930/index.html">What could be the work of an IT specialist of the future</a></li>
<li><a href="../316934/index.html">Josh Clark: Myths about mobile design</a></li>
<li><a href="../316938/index.html">Manipulations of users of the site using colors</a></li>
<li><a href="../316940/index.html">MTA vs MTO. Who will win? No one anybody. Work work</a></li>
<li><a href="../316944/index.html">Vapor - fast and secure REST on Swift?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>