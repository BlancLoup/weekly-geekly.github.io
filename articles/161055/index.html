<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How ZFS Works - Part 2: metaslab</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first part, I described how data is organized in vdev in ZFS. The second part describes how the algorithm for selecting the proper place where ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How ZFS Works - Part 2: metaslab</h1><div class="post__text post__text-html js-mediator-article">  In the <a href="http://habrahabr.ru/post/160943/">first part,</a> I described how data is organized in vdev in ZFS.  The second part describes how the algorithm for selecting the proper place where the recording will go at the moment works. <br><br>  Here I will complicate the task a bit - in the first part only one vdev was described;  here we will have several of them, since the algorithm should choose both vdev, where we will write the data block, and metaslab inside vdev.  In a production system, there may be several dozen vdev, and it is critical to distribute the data correctly over them - we cannot re-balance them without copying all the data.  The goal of the correct algorithm is to parallelize the data so that each device has approximately the same amount, even out uneven filling, but also not overload one of the devices (this will slow down the recording for the entire pool). <br><br><pre><code class="bash hljs">NAME STATE READ WRITE CKSUM tank ONLINE 0 0 0 c1t6d0 ONLINE 0 0 0 c1t5d0 ONLINE 0 0 0</code> </pre> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For a start, an important note: ZFS is designed to ensure that all devices in the pool are the same size.  Otherwise, for example, if you add a 2TB disk to a pool of 1TB disks, a 2TB disk will result in twice as much data and it will affect the total IOPs of the system - the allocator algorithm takes into account the percent full, not the amount of data in bytes. <br><br>  At the moment, there are four allocator algorithms in ZFS.  The variable <a href=""><code>zfs_metaslab_ops</code></a> contains a pointer to the <code>space_map_ops_t</code> structure, in which there are pointers for seven functions that each particular algorithm uses.  For example, in Illumos, the <code>metaslab_df</code> algorithm is <code>metaslab_df</code> , and the corresponding plan with points to functions looks like this: <a name="habracut"></a><br><br><pre> <code class="hljs cpp"><span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">space_map_ops_t</span></span> metaslab_df_ops = { metaslab_pp_load, metaslab_pp_unload, metaslab_df_alloc, metaslab_pp_claim, metaslab_pp_free, metaslab_pp_maxsize, metaslab_df_fragmented };</code> </pre><br><br>  Five of these functions are used in all algorithms;  the difference is actually only in <code>metaslab_*_alloc()</code> and <code>metaslab_*_fragmented()</code> is the allocator itself, and a function that decides how much free space is fragmented in a specific metaslab.  Allocators that can be used: DF (Dynamic-Fit), FF (First-Fit), and two experimental, CDF and NDF - no one knows what they mean. <br><br>  FF is the simplest of them - it writes data chunks to the first available space when tracing the AVL-tree in order, and divides the recording blocks into segments if they do not fit.  Due to this, FF is a very slow algorithm, since to write one block it must trace the entire tree until a sufficient number of segments are typed.  For writing 1GB of data, for example, worst case - 20 million segments of 512 bytes each, and very strong data fragmentation as a result.  FF is used by other algorithms as the last option if they cannot find a place differently - for example, DF uses FF if there is less than 4% of free space in this metaslab ( <code>int metaslab_df_free_pct = 4;</code> ).  The only advantage of FF is that it can only fill a fragmented meta-slab by 100%. <br><br>  * DF algorithms work differently - they build a <code>freemap</code> map of the <code>freemap</code> in the metaslab currently being recorded, sort it by size and / or proximity of pieces of continuous free space, and try to choose the most optimal data placement option in terms of speed recording, the number of movements of the disk head, and minimal fragmentation of the recorded data. <br><br>  For example, for all of them, the <code>metaslab_weight()</code> function works, which gives a small priority to the metaslabs that are on the outer regions of the disc plate (for the short-stroke effect).  If you use only SSD, then it makes sense to tailor ZFS, disabling this part of the algorithm, because the short-stroking does not apply to SSD. <br><br>  So, in the algorithms of the allocator, the data comes from the <a href="">ZIO</a> <a href="">pipeline</a> - from there, the functions <code>metaslab_alloc()</code> are called (the allocator itself is for writing) and <code>metaslab_free()</code> (free up space, collect garbage). <br><br><pre> <code class="hljs cpp">metaslab_alloc(<span class="hljs-keyword"><span class="hljs-keyword">spa_t</span></span> *spa, <span class="hljs-keyword"><span class="hljs-keyword">metaslab_class_t</span></span> *mc, <span class="hljs-keyword"><span class="hljs-keyword">uint64_t</span></span> psize, <span class="hljs-keyword"><span class="hljs-keyword">blkptr_t</span></span> *bp, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ndvas, <span class="hljs-keyword"><span class="hljs-keyword">uint64_t</span></span> txg, <span class="hljs-keyword"><span class="hljs-keyword">blkptr_t</span></span> *hintbp, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> flags)</code> </pre><br><br>  It is transmitted there: <code>*spa</code> - point to the structure of the data array itself (zpool);  * mc - the meta-slabs class, which also includes a pointer for <code>zfs_metaslab_ops</code> ;  <code>psize</code> - data size;  <code>*bp</code> - pointer to the block itself;  <code>ndvas</code> is the number of independent copies of data that is required for a given block (1 for data; 2 for most metadata; 3 in some cases for metadata that are high in the AVL tree. The meaning of duplication of metadata is that if a single block with metadata for a segment of the tree is lost, we lose everything that is under it. Such blocks are called ditto blocks, and the algorithm tries to write them in different vdevs). <br><br>  Further, <code>txg</code> is the sequence number of the transaction group that we write;  <code>*hintbp</code> - a hint used to ensure that the blocks that are next to <code>*hintbp</code> are also logically next to the disk and go to the same vdev;  <code>flags</code> - 5 bits that allow the allocator to find out if you need to use any specific allocation options - use or ignore the <code>*hintbp</code> , and whether to use ganging (please write the group of child blocks to the same vdev as their header, for more efficient work ZFS prefetch and vdev cache). <br><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">define</span></span> METASLAB_HINTBP_FAVOR 0x0 define METASLAB_HINTBP_AVOID 0x1 define METASLAB_GANG_HEADER 0x2 define METASLAB_GANG_CHILD 0x4 define METASLAB_GANG_AVOID 0x8</code> </pre><br><br><pre> <code class="hljs vbscript">/* * Allow allocations <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> switch <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> gang blocks quickly. We <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> this <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> * avoid having <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> load lots of space_maps <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> a given txg. There are, * however, some cases where we want <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> avoid <span class="hljs-string"><span class="hljs-string">"fast"</span></span> ganging <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> instead * we want <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> an exhaustive search of all metaslabs <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> this device. * Currently we don<span class="hljs-comment"><span class="hljs-comment">'t allow any gang, zil, or dump device related allocations * to "fast" gang. */ #define CAN_FASTGANG(flags) \ (!((flags) &amp; (METASLAB_GANG_CHILD | METASLAB_GANG_HEADER | \ METASLAB_GANG_AVOID)))</span></span></code> </pre><br><br><pre> <code class="hljs vbscript"> /* * <span class="hljs-keyword"><span class="hljs-keyword">If</span></span> we are doing gang blocks (hintdva <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> non-<span class="hljs-literal"><span class="hljs-literal">NULL</span></span>), try <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> keep * ourselves <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> the same vdev as our gang block header. That * way, we can hope <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> locality <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> vdev_cache, plus it makes our * fault domains something tractable. */</code> </pre><br><br>  Then, in fact, comes the most important part of the code, in which the logic of the choice of the metaslab to write: <code>metaslab_alloc_dva()</code> .  There are almost 200 lines of clever code in the function, which I will try to explain. <br><br>  First, we take all groups of meta-slabs for all vdevs, and go through them with the allocator cycle ( <code>mg_rotor</code> ), using hints, if there are any.  We skip the vdevs for which the recording is currently undesirable, for example, those in which one of the disks died, or a raidz-group is being restored.  (Don't allocate from faulted devices.) We also skip discs that had some kind of write error, for which there will be only one copy on the discs.  (Avoid writing single-copy data to a failing vdev.) <br><br>  The rotor works in a circle until the data sent to the recording runs out.  Inside this cycle, the optimal vdev is selected in turn, then in it, using <code>metaslab_group_alloc()</code> , the best metaslab is selected, then we decide how much data to write to this metaslab, comparing the percentage of vdev use with others.  This part of the code is very critical, so I quote it completely: <br><br><pre> <code class="hljs php"> offset = metaslab_group_alloc(mg, psize, asize, txg, distance, dva, d, flags); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (offset != <span class="hljs-number"><span class="hljs-number">-1</span></span>ULL) { <span class="hljs-comment"><span class="hljs-comment">/* * If we've just selected this metaslab group, * figure out whether the corresponding vdev is * over- or under-used relative to the pool, * and set an allocation bias to even it out. */</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (mc-&gt;mc_aliquot == <span class="hljs-number"><span class="hljs-number">0</span></span>) { vdev_stat_t *vs = &amp;vd-&gt;vdev_stat; int64_t vu, cu; vu = (vs-&gt;vs_alloc * <span class="hljs-number"><span class="hljs-number">100</span></span>) / (vs-&gt;vs_space + <span class="hljs-number"><span class="hljs-number">1</span></span>); cu = (mc-&gt;mc_alloc * <span class="hljs-number"><span class="hljs-number">100</span></span>) / (mc-&gt;mc_space + <span class="hljs-number"><span class="hljs-number">1</span></span>); <span class="hljs-comment"><span class="hljs-comment">/* * Calculate how much more or less we should * try to allocate from this device during * this iteration around the rotor. * For example, if a device is 80% full * and the pool is 20% full then we should * reduce allocations by 60% on this device. * * mg_bias = (20 - 80) * 512K / 100 = -307K * * This reduces allocations by 307K for this * iteration. */</span></span> mg-&gt;mg_bias = ((cu - vu) * (int64_t)mg-&gt;mg_aliquot) / <span class="hljs-number"><span class="hljs-number">100</span></span>; }</code> </pre><br><br>  For example, if we need to write 1MB of data to an array of two disks, <s>one of which is 20% full, and the second 80%, we will write 819KB to the first, and 205KB to the second.</s>  <i>Correction: the free space in the pool is compared to the free space in vdev, so the result will be slightly different.</i>  By the way, here you can do one very interesting thing - a few months ago I added latency statistics for each vdeva in ZFS (it is in <code>vdev_stat_t-&gt;vs_latency[]</code> in NexentaStor; in Illumos they haven‚Äôt added yet), and it can be used as one of the factors in recording new data, either taking into account both its and free space in any proportion, or using only it.  I also wrote such a modified algorithm, but it is not used in production systems yet.  It makes sense ever in the array there are disks of different types and speeds, or when one of the disks begins to die (to brake), but so far it is not so bad, and there are no errors on it. <br><br>  Finally, iteration after iteration, the cycle sends all data to the metaslab group for writing, and ends for this transaction group (txg), and the <a href=""><code>metaslab_weight()</code></a> (see the beginning of the article) works in the metalabs group, and through the <a href=""><code>space map</code></a> system, taking into account maxfree (maximum piece of uninterrupted free space), using the tracing of AVL-trees and the corresponding algorithm (DF, FF, CDF, NDF), stuff the data in an optimal way for the algorithm, after which we finally get the physical address of the block, which we will write to the disk, and the data goes to the queue to write to the <code>sd</code> (Scsi ‚Äã‚ÄãDevice) driver  . </div><p>Source: <a href="https://habr.com/ru/post/161055/">https://habr.com/ru/post/161055/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../161039/index.html">Guide to writing JS scripts for front-end developers under Drupal 7</a></li>
<li><a href="../161045/index.html">Review of the tablet Freelander PD10 Typhoon - "full stuffing" at a reasonable price</a></li>
<li><a href="../161047/index.html">My first client is dead</a></li>
<li><a href="../161051/index.html">Computer crime: then and now</a></li>
<li><a href="../161053/index.html">Tame and pump firelight: The Ultimate Guide</a></li>
<li><a href="../161057/index.html">Do you type blindly?</a></li>
<li><a href="../161061/index.html">8 principles to make the product that people want - squeeze out Mike Krieger's speech on 500 Startups' Warm Gun</a></li>
<li><a href="../161063/index.html">An algorithm for annotating illustrations, or why not a programmer to be a bit of a designer?</a></li>
<li><a href="../161067/index.html">Stop twisting - 2. About ways to fix the cable</a></li>
<li><a href="../161071/index.html">In SearchMan made a rating of "detectability" of applications on the App Store</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>