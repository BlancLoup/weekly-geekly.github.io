<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Veritas Access 7.3: pros, cons, pitfalls</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Software-defined storage approach 


 In this article, we will review and test the new version of software-defined storage system (Software-Defined St...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Veritas Access 7.3: pros, cons, pitfalls</h1><div class="post__text post__text-html js-mediator-article"><h4>  Software-defined storage approach </h4><br><img src="https://habrastorage.org/webt/ml/eg/cv/mlegcvwhe-us9oibdz3kythpaxc.png"><br><br>  In this article, we will review and test the new version of software-defined storage system (Software-Defined Storage, SDS) Veritas Access 7.3, a multipurpose scalable data storage based on traditional x86 servers with file, block and object access.  Our main task is to get acquainted with the product, with its functionality and capabilities. <br><br>  Veritas is synonymous with reliability and expertise in the field of information management, which has a long tradition of leadership in the backup market, and produces solutions for analyzing information and ensuring its high availability.  In the Software-Defined Storage Veritas Access solution, we see the future and are confident that over time the product will gain popularity and occupy a key position in the SDS solutions market. <br>  As a platform on which Access was created, a product with a long history of InfoScale (Veritas Storage Foundation), which in times of lack of virtualization was at its peak in Highly Available (HA) solutions, came out.  And from Veritas Access's little brother, we expect a continuation of the success story of NA in the form of Software-Defined Storage. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      One of the advantages of Veritas Access is the price.  The product is licensed by the number of active cores (approximately $ 1000/1 CORE in the GPL (global price list)), the cost is completely independent of the number of disks and the total volume of the storage system.  Behind the nodes it is wise to use non-nuclear high-performance single-processor servers.  Roughly speaking, the Access license for a 4-node cluster will cost $ 20,000‚Äì $ 30,000, which is very attractive compared to other SDS solutions, where the cost starts from $ 1,500 per terabyte of raw space in the GPL. <br><a name="habracut"></a><br>  The main advantage that distinguishes Veritas Access from Ceph and those close to it in the spirit of SDS solutions is its ‚Äúboxing‚Äù: Veritas Access is a complete Enterprise-level product.  The implementation and support of Access is not fraught with difficulties that only specialists with deep knowledge of nix SDS systems can do.  In general, to implement any SDS solution is not a particularly difficult task, the main difficulties arise during operation in the form of adding nodes when they fail, transfer data, update, etc. In the case of Access, all problems are understandable, since the solutions are described for Storage Foundation .  And in the case of Ceph you need to have qualified personnel who can provide specialized support.  Someone is comfortable working with a ready-made solution in the form of Access'a, someone seems to be a convenient Ceph designer with a more flexible approach, but requiring knowledge.  We will not compare Access with other SDS solutions, but try to give a small overview of the new product.  Perhaps it will help to make a choice in the question, with which SDS solution you will be more comfortable working. <br><br>  For transparency of testing, as close as possible to production, we, together with colleagues from one of Veritas distributors - OCS, assembled a five-node stand on physical servers: a two-node cluster in the Data Center of Open Technologies and a three-node in the Data Center OCS.  Below are photos and diagrams. <br><br><h3>  File vs Block vs Object Storage </h3><br>  Veritas Access works on distribution across all protocols, providing file, block and object access.  The only difference is that file access is very easy to configure via the WEB interface, and the configuration of the other two is currently poorly described and complicated. <br><br><img src="https://habrastorage.org/webt/jf/gz/1p/jfgz1p7wqgwd_m2rge25pdel8r0.png"><br><br>  NFS, S3 work on nodes in active-active mode.  iSCSI and CIFS in the current release 7.3 works in active-passive.  Services connected via NFS, S3 will continue to work if any of the nodes fail, and services connected via CIFS, iSCSI may not survive the loss of the node on which CIFS (Samba) is active.  It will take some time until the cluster realizes that the node has failed and will start the CIFS (Samba) service on another node, similarly with iSCSI.  Active-active for iSCSI and CIFS is promised in future releases. <br><br>  iSCSI in 7.3 is presented as tech preview, in the version of Veritas Access 7.3.1, the output of which is promised on December 17, 2017, there will be a full-fledged iSCSI implementation in active-active. <br><br>  More about modes: <br><br><ul><li>  Active-active: two (and more) cluster nodes work with the application, the so-called ‚Äúapplication stands on the storage system with two legs‚Äù, that is, the output of one cluster node working with the application will not lead to access denial and data loss. </li><li>  Active-passive: the application works with only one cluster node, in this case ‚Äúthe application is on the storage system with one foot‚Äù.  The loss of a node is critical, the loss of the recorded data (the appearance of an error) and a brief termination of access are possible.  For file garbage, recording video archives from cameras is not critical.  And if you decide to back up the CIFS database, and at the time of the backup recording, the node that took the load will stop working - ‚Äúoh‚Äù will happen and you will have to start the backup over again.  Accordingly, if a node fails to accept the load or the node to which the data mirror is written, nothing bad will happen, only the performance will decrease slightly. </li></ul><br><h3>  Why SDS? </h3><br>  A bit of theory. <br><br>  Traditional storage systems do an excellent job with current tasks, ensuring the necessary performance and availability of data.  But the cost of storing and managing different generations of storage systems is definitely not the greatest strengths of traditional solutions.  Scaling up the Scale-up architecture, where performance increases by replacing individual system components, is not able to restrain the growth of unstructured data in the modern IT world. <br><br>  In addition to the Scale-up vertical scaling architecture, Scale-out solutions have appeared, where scaling is done by adding new nodes and distributing the load between them.  Using this approach, the problem of growing unstructured data is solved quickly and easily. <br><br>  <u>Disadvantages of traditional storage systems:</u> <br><br>  ‚Ä¢ High price <br>  ‚Ä¢ Short term relevance of storage <br>  ‚Ä¢ Complicated storage management of different generations and manufacturers <br>  ‚Ä¢ Scale-up scaling architecture <br><br>  <u>Software-Defined Storage Scale-out Scale-Out Pros:</u> <br><br>  ‚Ä¢ No dependency on the hardware of the system; for nodes you can use any x86 server <br>  ‚Ä¢ Flexible and reliable solution with simple scaling and redundancy support <br>  ‚Ä¢ Predictable level of policy-based application service. <br>  ‚Ä¢ Low cost <br>  ‚Ä¢ Productivity <br><br>  <u>Disadvantages of software-defined storage system:</u> <br><br>  ‚Ä¢ Lack of a single point for technical support <br>  ‚Ä¢ Requires more qualified engineers <br>  ‚Ä¢ The need for N-times the amount of resources (excess) <br><br>  When trying to expand the traditional storage system after 3-5 years of use, the user is faced with dramatically increasing costs, which force them to buy a new storage system.  When separate devices from different manufacturers appear, control over ensuring the quality and reliability of the storage subsystem as a whole is lost, which, as a rule, makes it very difficult to solve business problems. <br><br>  Software-defined storage allows you to look at the problem of locating growing data on the other hand: its implementation preserves the performance and availability of information without splitting the zoo from various manufacturers and storage generations, which makes working with data more accessible in all plans. <br><br><h3>  Key Business Benefits </h3><br><ul><li>  An affordable and reliable storage system that supports your existing hardware: both old and new on the x86 platform. </li><li>  High performance, meeting the requirements of unstructured data - the ideal solution for storing large numbers of small files with low latency, as well as maintaining streaming loads by providing high write speeds. </li><li>  High availability.  Thanks to a multi-node scalable file system, which provides the ability to switch to another resource at the local, metro and global distances from each other without interrupting access. </li><li>  Unified storage solution for new and outdated applications.  Multi-protocol storage solution (CIFS, NFS, iSCSI and S3) </li><li>  Easy to use WEB, SSH and RESTful API. </li><li>  Flexible scalable architecture.  Easily add new nodes to improve performance, resiliency and increase capacity. </li></ul><br><img src="https://habrastorage.org/webt/nb/wy/du/nbwydujn2qxso-c8vjfpgy3dhv4.png"><br><br><img src="https://habrastorage.org/webt/yt/sp/nk/ytspnktvvkp4ljts0xqgm0t6y7i.png"><br><br><h3>  Main technical specifications </h3><br><ul><li>  Highly available scalable file system provides fault tolerance both at the local level and globally with replication tools. </li><li>  Manage storage of policy-based data that allows you to flexibly configure the level of availability, performance, data protection, and volume security. </li><li>  Multiprotocol access to files and objects: work with NFS, SMB3 / CIFS, iSCSI, FTP and S3, including support for reading and writing to one volume using different protocols. </li><li>  Hybrid cloud: <br>  - OpenStack: support for Cinder and Manila <br>  - Amazon (AWS): S3 interface and protocol support. </li><li>  Centralized user interface - a single window for managing all nodes with a simple and intuitive interface. </li><li>  Heterogeneous memory support DAS, SSD, SAN, as well as clouds. </li><li>  Storage Tiering: support for TIRING between different storage volumes: cloud, SAN, DAS and SSD. </li><li>  Read Caching: the ability to use SSD for cache. </li><li>  Storage Optimization: support for deduplication and compression </li><li>  Snapshots. </li><li>  Direct Backup Ready: a built-in NetBackup client for backup, and support for Veritas Enterprise Vault. </li><li>  Flexible authentication with support for LDAP, NIS and AD. </li><li>  Flexible management with support for WEB, SSH and RESTful API. </li></ul><br><img src="https://habrastorage.org/webt/xl/vi/k9/xlvik9afohlldwune0zkb-vb5to.png"><br><br><h3>  ARCHITECTURE </h3><br>  Veritas Access is flexible and easily scaled by increasing nodes.  This solution is suitable, first of all, for working with unstructured data, as well as with other storage tasks.  The need for software-defined storage systems dictates the transition to multi-purpose, multiprotocol products that combine reliability, high performance and affordable cost. <br><br>  The Veritas Access cluster consists of connected server nodes.  Together they form a unified cluster that shares all major resources.  The minimal reference architecture consists of a two-node converged storage solution. <br><br><img src="https://habrastorage.org/webt/n_/p3/97/n_p397st81e1ct5es_rmb_y7m0c.png"><br><br>  <b>Foundation Veritas Access - Veritas Storage Foundation</b> <br><br>  The lessons learned during the life cycle of the Veritas Storage Foundation have found their logical use in the modern SDS class product, which is Veritas Access.  Basically, the following components find their manifestation in Access: <br><br><ul><li>  <b>Veritas Cluster Server (VCS) is an</b> application services clustering system. </li><li>  <b>Veritas Cluster File System (CFS)</b> is a cluster file system. </li><li>  <b>Veritas Cluster Volume Manager (CVM)</b> is a cluster logical volume manager. </li><li>  <b>Flexible Storage Sharing (FSS)</b> is a technology that allows you to combine all local storage resources of nodes into a single pool that is accessible to any node in the cluster. </li></ul><br>  Veritas Access 7.3 hosts x86 servers running Linux Red Hat release 6.6 - 6.8.  Starting with release 7.3.0.1 - RedHat 7.3 - 7.4.  Clustering occurs at the expense of VCS technology.  Any available disks are used as a storage pool.  To add a node to the cluster, you must have at least 4 Ethernet ports in the server.  At least 2 ports go to client access and at least 2 ports go to inter-node interconnect.  For inter-node interconnect, Veritas recommends using InfiniBand.  Minimum requirements for interconnect 1 GB ethernet. <br><br>  On RAM, the recommendation is 32 GB per production.  Veritas Cluster File System (CFS) allows you to cache data at the expense of RAM, therefore, any memory size above 32 GB will not be superfluous, but it is better not to use less.  8 GB is enough for testing, there are no software limitations. <br><br>  Three-node configuration is best avoided; we recommend using two nodes or four, five, etc. at once.  The current limit of one cluster is 20 nodes.  The InfoScale cluster supports 60, so 20 nodes is not the limit. <br><br>  The VCS cluster provides the fault-tolerant operation of data access services via NFS, CIFS, S3, FTP, Oracle Direct NFS and the corresponding infrastructure on Veritas Access nodes, and in certain configurations it is possible to write and read the same data using different protocols. <br><br>  For organizing file systems of up to 3 PByte, you can use a scalable file system (Scale-out), which allows, in particular, to connect external cloud storage within a single namespace.  When building distributed storage systems or disaster recovery, file replication can be configured between different Veritas Access repositories.  This technology allows you to asynchronously replicate the file system of the source cluster to a remote cluster with a minimum interval of 15 minutes, with the remote file system remaining open for reading during replication.  It supports load balancing between replication links and instantaneous switching of the remote file system to write mode when the source is unavailable and switching the replication service from one node to another in the event of a failure. <br><br>  The number of parallel replication operations is unlimited.  It is important to note that the replication technology uses Veritas CFS / VxFS functionality (File Change Log files and snapshots at the file system level) to quickly identify changes, thus avoiding wasted time on file system scans and significantly increase replication performance. <br><br><img src="https://habrastorage.org/webt/l4/bv/te/l4bvtey8gadob8l41xpzo270sxo.png"><br><br><h3>  ARCHITECTURE OF WORK WITH DISKS </h3><br><img src="https://habrastorage.org/webt/j5/gd/za/j5gdzazrsmg6aij4ngfumdqmbyk.png"><br><img src="https://habrastorage.org/webt/tk/lp/2p/tklp2pdw42inroncuegivazxvhy.png"><br><br><h3>  Stand scheme together with OCS: </h3><br><img src="https://habrastorage.org/webt/zn/jn/rj/znjnrju-hgpmcda03oafxyjea0m.png"><br><br><h3>  Photo booth in Open Technologies: </h3><br><table><tbody><tr><th><img src="https://habrastorage.org/webt/hm/bh/i4/hmbhi4ich5kskueqikd2p6rfnoi.jpeg"></th><th><img src="https://habrastorage.org/webt/wl/xd/-p/wlxd-pa4f5bv0m-ky15vmfpogfi.jpeg"></th></tr></tbody></table><br><h3>  Download and install </h3><br>  You can get the Linux distribution of Red Hat Enterprise for a 30-day subscription <a href="https://access.redhat.com/products/red-hat-enterprise-linux/evaluation">via this link</a> .  After registration of the trial, Red Hat distributions will be available for download, in particular, the necessary releases 6.6‚Äì6.8. <br><br>  You can get the Veritas Access distribution <a href="https://www.veritas.com/content/trial/en/us/veritas-access.html">here</a> .  To do this, fill out the trial form.  During the installation process, the system itself will offer a key for 60 days.  You can add a basic license later via the web administration panel in the Settings ‚Üí Licensing section. <br><br>  All documentation for version 7.3 is available <a href="https://sort.veritas.com/documents/doc_details/isa/7.3/Linux/ProductGuides/">here</a> . <br><br><img src="https://habrastorage.org/webt/a7/es/lq/a7eslqklpwgyojuqfqjan_1b4mo.png"><br><br>  <b>Installation of Veritas Access can be divided into two stages:</b> <br><br>  1. Installing and Configuring Red Hat Enterprise Linux <br>  2. Install Veritas Access <br><br>  <b>Installing Linux Red Hat Enterprise Linux</b> <br><br>  A typical installation of Linux without any difficulties. <br><br><img src="https://habrastorage.org/webt/7a/3c/kz/7a3ckzb1gpipfi284_maahb5wou.png"><br><img src="https://habrastorage.org/webt/5n/a9/xi/5na9xiaxhy4qb8ba14xanv8aevw.png"><br><br>  Red Hat Requirements: <br><img src="https://habrastorage.org/webt/-i/2-/ai/-i2-ai8h5jf138rhjpo2ubc1rdw.png"><br><br>  With a Red Hat subscription, the Veritas Access installer will pull up the RPM modules it needs.  Since the release of Access 7.3, all the necessary modules have been added to the installer repository. <br><br>  On public interfaces, in addition to IP addresses, you must set the Connection automatically or ONBOOT = yes flag in the interface configuration file. <br><br><img src="https://habrastorage.org/webt/vt/ac/xg/vtacxgoy3jnfvtu2ypebs0kwhyu.png"><br><br>  <b>Installing Veritas Access:</b> <br><br>  Installation files must be placed on the same node.  The installation is started by the command: <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ./installaccess node1_ip node2_ip nodeN_ip</span></span></code> </pre> <br>  where node1_ip, node2_ip is any of the ip addresses of the public interface. <br><br>  During the installation of Veritas Access, there are moments that should be paid attention to: <br><br><ol><li>  The Veritas Access installer is designed for an ideal installation.  Any step left-right is critical for it and may lead to an installation error.  Carefully enter the addresses, masks, names. </li><li>  The installation must be run locally from any host or through the server management mechanism in the absence of physical access to them, not ssh. </li><li>  If you failed to install Veritas Access the first time, I recommend reinstalling Red Hat and starting the Veritas Access installation from scratch.  In the event of an error, the installer does not allow the system to roll back before the installation begins, which leads to a re-installation with partially running Veritas Access services and may cause problems. </li></ol><br>  After installing Veritas Access, 2 management consoles will be available: <br><br><img src="https://habrastorage.org/webt/g3/kc/pl/g3kcpl9hue4nbvuchasb5djcn2u.png"><br>  ‚Ä¢ web - in our case it is <a href="https://172.25.10.250:14161/">https://172.25.10.250:14161</a> <br>  ‚Ä¢ ssh - on 172.25.10.250 with login master (default password is master) and root <br><br>  After installing Access, you must provide the cluster with all the disks for each node that you plan to use in the cluster for data.  This process is not automatic, as you may have other plans for some of the cluster disks. <br>  CLISH commands: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># vxdisk list # vxdisk export diskname1-N # vxdisk scandisks</span></span></code> </pre> <br>  For ‚Äúnon-standard‚Äù server disks, you must specify the disk model and the address where the S / N disk resides.  In my case, the command on SATA disks is: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># vxddladm addjbod vid=ATA pid="WDC WD1003FBYX-0" serialnum=18/131/16/12</span></span></code> </pre> <br>  This is a small minus of SDS systems regarding hard drives not from <a href="https://sort-static.veritas.com/public/documents/isa/7.3.0.1/linux/compatibilitylists/pdf/access_7301_hcl.pdf">Hardware Compatibility Lists</a> .  Each vendor lays its standards in the disks, seeing a terabyte in a different number of bytes, sectors, placing identifiers in different addresses.  And the situation is quite normal.  If the SDS storage system does not correctly detect the disks, it needs a little help, in the case of Access, the instruction is <a href="https://www.veritas.com/support/en_US/article.000024381">here</a> . <br><br>  And then Access'a is traced: a nonstandard situation has arisen - there is a document with its solution, which is searched for by Google, in addition there is a detailed reference in the vxddladm module itself.  There is no need to read bourgeois forums, finish on the knee with an unpredictable result in production.  If the problem cannot be solved by yourself, you can always contact technical support. <br><br>  As a result, each disk must be accessible to each node in the cluster. <br><br><img src="https://habrastorage.org/webt/jr/zm/bk/jrzmbkimcntq6r-a_31khzr3rk4.png"><br><br>  After installation, the folder with the installation files will be deleted. <br><br><h3>  How it all works </h3><br>  In our case, the cluster consists of two nodes: <br><br><img src="https://habrastorage.org/webt/jd/02/tb/jd02tbwaugljrlkjrypppjisg44.png"><br><br>  Each node has physical and virtual IP addresses.  Physical IPs are assigned to each node as unique, virtual IPs serve all nodes in the cluster.  By physical interfaces, you can check the availability of the site, connect - directly via ssh.  If a node is unavailable, its physical interfaces will be unavailable.  Each node's virtual IP serves all cluster nodes and is active as long as there is at least one live cluster node.  Clients work only with virtual addresses. <br><br>  Each balloon displays information on how IP is available. <br><br><img src="https://habrastorage.org/webt/mt/di/iw/mtdiiw3nafysbmipioe1enn_vqg.png"><br><br>  At any time, one node performs the role of a master; in the screenshot, this is the va73_02 node.  The master node performs administration tasks, load balancing.  The node master role can be transferred or taken by another node in case of loss of availability or the fulfillment of a number of conditions laid down in the logic of the Veritas Access cluster.  In case of loss of the internal interconnect, an unpleasant situation can happen in which each node will become a master.  The reliability of internal interconnect needs to be given special attention. <br><br><h3>  Control </h3><br>  Veritas Access has 3 management interfaces: CLISH in root mode and master mode, WEB and REST API. <br><table><tbody><tr><th>  <b>ssh master mode (login root, master)</b> </th><th>  web </th></tr><tr><td><img src="https://habrastorage.org/webt/nv/qd/cu/nvqdcub_gp0czcodfmb5m78cfbu.png"><img src="https://habrastorage.org/webt/ok/qg/yn/okqgyntrka3_njpd94x0ykofsvy.png"></td><td><img src="https://habrastorage.org/webt/6v/ch/bg/6vchbgrmbnhr-zpve2p7sinhphg.png"></td></tr></tbody></table><br>  <b>ssh master mode</b> <br><br>  The most complete console management cluster Veritas Access, access is carried out by ssh on IP management with the master login (default password master).  It has an intuitive, simplified set of commands and detailed help. <br><br>  <b>ssh node mode</b> <br><br>  The usual Linux management console, accessed via ssh with root login on the IP management. <br><br>  <b>WEB</b> <br><br>  WEB console management cluster, access is carried out by root or master login to IP control on the port: 14161, https protocol. <br><br>  WEB console interprets commands in <b>ssh master mode</b> . <br><br><img src="https://habrastorage.org/webt/9a/gc/2r/9agc2rmaioid-mkzajs4rnufgpo.png"><br><br>  With each release, there is an expansion of management capabilities for WEB. <br><br><h3>  Initial setup </h3><br><img src="https://habrastorage.org/webt/1d/ps/nr/1dpsnrnqsp3jfxsd4vdyx1hyimo.png"><br><br>  Just a few words on launching Veritas Access as a storage system. <br><br>  1. Combining drives in the storage pool <br><br><img src="https://habrastorage.org/webt/gv/ui/wn/gvuiwncrg34sv9jtg95eim_gyts.png"><br><br>  2. Create a file system based on the type of disks, data and the required protection (analogous to RAID) <br><br><img src="https://habrastorage.org/webt/ud/1d/-3/ud1d-3ystjzzvmjegjxrsmvu_dg.png"><br><br>  3. Sharing the file system for the desired protocols.  One file system can be shared using several protocols and, for example, access to the same files via both NFS and CIFS, either temporarily or permanently. <br><br><img src="https://habrastorage.org/webt/di/qj/sj/diqjsjnucg0vcpuz19twatqy6ea.png"><br><br>  4. You are great. <br><br><img src="https://habrastorage.org/webt/i-/rc/p6/i-rcp6do3pwjaa31ruuiyffaxjw.png"><br><br><h3>  ISCSI external storage and any other storage devices </h3><br>  Veritas Access supports the use of any third-party iSCSI or storage systems for storage.  Third-party storage drives must be submitted separately in RAW format.  If this is not possible, RAID 5 of 3 disks is used.  Data protection is provided by the Veritas Access file system and is presented in the section above.  Data protection at the hardware raid level is not necessary, the settings of the Access file system indicate the number of nodes on which your data will be mirrored. <br><br>  ISCSI-connected volumes can be included in shared pools. <br>  Adding an iSCSI disk looks like this: <br><br>  1. Turn on iSCSI in the storage section: <br><br><img src="https://habrastorage.org/webt/jj/be/40/jjbe40xzqwj3ckxhzgcb02cz_78.png"><br><br>  2. Add an iSCSI device: <br><br><img src="https://habrastorage.org/webt/uh/7w/w9/uh7ww9og5xyzw9zo6tndvw_3ody.png"><br><br>  3. Connect an iSCSI disk: <br><br><img src="https://habrastorage.org/webt/ce/-s/r4/ce-sr4s2_edal2nwuvhdhmsg5as.png"><br><br><img src="https://habrastorage.org/webt/z4/fd/m3/z4fdm3zxxj0xrkaub7xywl3psj8.png"><br><br>  As a test disk iSCSI - win2012 <br><br><img src="https://habrastorage.org/webt/gv/ac/j4/gvacj4nhfngf1gdut8slowsr2sa.png"><br><br><h3>  Distribution on iSCSI storage volumes </h3><br>  The configuration process in the current release is a bit different, you can read it in detail in page 437 in the <a href="https://www.veritas.com/support/en_US/doc/access_command_ref_73_lin">Command Reference Guide</a> (iSCSI target service).  For many teams in the current release there is no possibility to see the settings, i.e.  It is better to pre-record all parameters in the TXT file. <br><br><img src="https://habrastorage.org/webt/iz/8c/rr/iz8crrismqfarizosm-sivtzdgy.png"><br><br>  <b>Important note, in version 7.3 there is a bug, because of which the iSCSI service does not start!</b> <br><br><img src="https://habrastorage.org/webt/tw/f-/9n/twf-9n26lpvrztfotcfhpyn0c9e.png"><br><br>  It can be fixed as follows: <br><br>  On nodes, you need to fix the file /opt/VRTSnas/pysnas/target/target_manager.py <br>  In line 381, correct [1] to [-1]. <br><br><img src="https://habrastorage.org/webt/hi/15/re/hi15rezp6in4s6-8xhzlqvnbhki.png"><br><br>  The process of starting iSCSI distribution looks like this: <br><br><pre> <code class="bash hljs">va73.Target&gt; iscsi service start ACCESS Target SUCCESS V-288-0 iSCSI Target service started va73.Target&gt; iscsi target portal add 172.25.10.247 ACCESS Target SUCCESS V-288-0 Portal add successful. va73.Target&gt; iscsi target create iqn.2017-09.com.veritas:target ACCESS Target SUCCESS V-288-0 Target iqn.2017-09.com.veritas:target created successfull va73.Target&gt; iscsi target store add testiscsi iqn.2017-09.com.veritas:target ACCESS Target SUCCESS V-288-0 FS testiscsi is added to iSCSI target iqn.2017-09.com.veritas:target. va73.Target&gt; iscsi lun create lun3 3 iqn.2017-09.com.veritas:target 250g ACCESS Target SUCCESS V-288-0 Lun lun3 created successfully and added to target iqn.2017-09.com.veritas:target va73.Target&gt; iscsi service stop ACCESS Target SUCCESS V-288-0 iSCSI Target service stopped va73.Target&gt; iscsi service start ACCESS Target SUCCESS V-288-0 iSCSI Target service started</code> </pre> <br><br>  <b>Another important note: without restarting the iSCSI service, the new parameters do not apply!</b> <br><br>  If something goes wrong, the iSCSI target logs of the work can be viewed in the following ways: <br><br><pre> <code class="bash hljs">/opt/VRTSnas/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/iscsi.target.log /opt/VRTSnas/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/api.log /var/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>/messages</code> </pre> <br>  On this, in fact, everything.  Connect our iSCSI volume to VMware ESXi. <br><br><img src="https://habrastorage.org/webt/tc/lr/f_/tclrf_wjpkpt6nk9tn09egxltsc.png"><br><br><img src="https://habrastorage.org/webt/co/4o/0k/co4o0kleyv7tbfydzhrxvs_pxds.png"><br><br>  Install a virtual machine on iSCSI volume, no load operation: <br><br><img src="https://habrastorage.org/webt/5i/ql/kk/5iqlkkoalq_a0j4baqmtychdx1w.png"><br><br>  Looks like a simulated failure of a SLAVE node under load (copying 10 GB) <br><br><img src="https://habrastorage.org/webt/qs/g2/3h/qsg23hrqnr7dwniglfqhfibewvw.png"><br><br><img src="https://habrastorage.org/webt/-r/vs/lb/-rvslbgsr3sj6_69tydltafgkk4.png"><br><br><h3>  Veritas Access Integration with Veritas NetBackup </h3><br>  An important advantage of Veritas Access is its built-in integration with Veritas NetBackup backup software, provided by the default NetBackup Client agent installed on all nodes, which is configured through the Veritas Access CLISH command interface.  The following types of backup operations are available: <br><br>  ‚Ä¢ full; <br>  ‚Ä¢ differential incremental; <br>  ‚Ä¢ cumulative-incremental; <br>  ‚Ä¢ A snapshot of the VxFS level checkpoint. <br><br><img src="https://habrastorage.org/webt/fq/2n/x3/fq2nx3_ue8qbi379cqziyhml2wk.png"><br><br>  <b>Long-term backup storage for NetBackup</b> <br><br>  Veritas Access's integration with NetBackup allows it to be seen as a cheaper and easier alternative to tape storage for long-term backup storage.  In this case, the integration can be performed using third-party free software OpenDedup, which is installed on the NetBackup media server and is connected as a logical device to the NetBackup Storage Unit.  OpenDedup is installed on a volume with the specialized OpenDedup SDFS file system, which is located inside the S3 bucket container on the Veritas Access storage.  When backing up backups, the NetBackup (Storage Lifecycle Policy) policy controls the write to the logical device NetBackup Storage Unit, and the data is sent in deduplicated form via the S3 protocol to the Veritas Access storage.  It should be noted that several media servers can simultaneously write to the same S3 bucket container, which, unlike tape media that can only compress data streams with much lower efficiency, provides global deduplication when storing long-term copies. <br><br>  Everything is configured quite simply, but since version 8.1 certificates are required. <br><br>  Storage volumes: <br><br><img src="https://habrastorage.org/webt/xa/cq/f2/xacqf2jirstgagmdq72anatd9au.png"><br><br>  NetBackup interface: <br><br><img src="https://habrastorage.org/webt/q5/xr/49/q5xr49rrst2zqi0b46tdpbguqsi.png"><br><br><h3>  Upgrade Veritas Access to a higher version </h3><br>  We started to study and test Veritas Access version 7.2, but after a week of testing, a new 7.3 was released. <br><br>  There was a question of an interesting case - update. <br><br>  Case is up to date, with which the owners of SDS solutions will sooner or later face. <br><br>  The possibility of updating is provided in the admin panel on the master login. <br><br><img src="https://habrastorage.org/webt/xb/2h/gd/xb2hgd_tlwwolwkvxngvektueri.png"><br><br><h3>  Veritas Access as a storage system for VMware over NFS </h3><br>  It should be noted ease of use NFS.  Its use does not require the introduction and development of complex FC-infrastructure, complex processes of setting up a zoning or trial with iSCSI.  Using NFS to access the datastor is also simple because the granularity of storage is equal to the VMDK file, and not entirely to the datastor, as in the case of block protocols.  NFS datastore is a common network-mounted globe mounted on a host with virtual machine disk files and their configs.  This, in turn, facilitates backup and recovery, since the copy and restore unit is a simple file, a separate virtual disk of a separate virtual machine.  You can not discount the fact that using NFS you automatically receive thin provisioning, and deduplication frees you space directly to the datastor level, which makes it available to the administrator and VM users, and not to the storage level, as in the case of using LUN.  It also looks extremely attractive in terms of using virtual infrastructure. <br><br>  Finally, using the NFS datastore, you are not limited to a limit of 2 TB.  This is most welcome if, for example, you have to administer a large number of relatively lightly loaded I / O machines.  They can all be placed on one big datastor, which is much easier to back up and manage than a dozen separate VMFS LUNs of 2 TB each. <br><br>  In addition, you can freely not only increase, but also reduce the datastore.  This can be a very useful opportunity for a dynamic infrastructure with a large number of heterogeneous VMs, such as cloud provider environments, where VMs are constantly being created and deleted, and this particular datastor for hosting these VMs can not only grow but also shrink. <br><br>  But there are also disadvantages: <br><br>  Well, firstly, it is the impossibility to use RDM (Raw-device mapping), which may be necessary, for example, to implement the MS Cluster Service cluster, if you want to use it.  You cannot boot from NFS (at least in a simple and usual way, such as boot-from-SAN).  The use of NFS is associated with a slight increase in the load on the stack, since a number of operations that are implemented on the host side in the case of a block SAN are supported by a stack in the case of NFS.  This is all sorts of blocking, access control, and so on. <br><br>  VMware connections to Veritas Access via NFS look like this, you see, it's very simple: <br><br><img src="https://habrastorage.org/webt/4c/ba/w5/4cbaw5ykk1b8wr9hedp7hwtynvg.png"><br><img src="https://habrastorage.org/webt/gl/du/wv/glduwvoky0ntn2wlalnnh1dli0s.png"><br><br>  To check the fault tolerance and performance, we have allocated a virtual machine on win 2008 R2 on the ball, which is in the mirror on two nodes. <br><br>  It looks like imitation of failure of the master node without load, at the moment of pulling out the cable latency rose from 0.7 to 7: <br><br><img src="https://habrastorage.org/webt/v5/5k/ef/v55kef-ppe5agy5gr_haby_qhq0.png"><br><br>  This is how the imitation of a master node failure under load looks like (copying 10 GB): <br><br><img src="https://habrastorage.org/webt/td/zy/7l/tdzy7lusywbh3hdfful7gkvl8we.png"><br><img src="https://habrastorage.org/webt/a7/cp/hn/a7cphnkxkmpunb63xdjbvz-_i1w.png"><br><br>  This is the simulation of the failure of a SLAVE node under load (copying 10 GB): <br><br><img src="https://habrastorage.org/webt/v0/na/no/v0nano9b1t1f_owi_pnf3g1sr90.png"><br><img src="https://habrastorage.org/webt/w9/68/uu/w968uutmsnixqam611gxnvkwequ.png"><br><br><h3>  S3 AND OBJECT STORAGE </h3><br>  One of the main drawbacks of NFS, iSCSI, CIFS is the difficulty of using over long distances.  The task of forwarding NFS balloons to a neighboring city can be called at least interesting, and there will be no difficulty in doing this for S3.  The popularity of object storage is growing, more and more applications support object storages and S3 in particular. <br><br>  To set up and test S3 is a convenient and free tool - S3 Browser.  Setting up S3 on Veritas Access is pretty simple, but distinctive.  To access via S3, you need to get the Access Key and Secret Key keychain.  Domain users see their keys through the Access WEB interface, the keys for the root user in the current release are obtained by scripts via the CLISH console. <br><br><img src="https://habrastorage.org/webt/oz/uh/c6/ozuhc6uuc-rhsxelhaiawn8h7ji.png"><br><br>  Connected via S3 NetBackup: <br><br><img src="https://habrastorage.org/webt/hf/i-/6s/hfi-6suldpv0_ltwz0vraxgsjjc.png"><br><br><h3>  REPLICATION </h3><br>  Veritas Access supports synchronous and asynchronous replication.  Replication goes in basic functionality without additional licenses and is configured quite simply.  Asynchronous replication is carried out on the basis of file systems, synchronous - on the basis of volumes.  To test replication performance, we combined our Veritas Access cluster and the OCS distributor cluster with file system-level replication tools.  An IPSEC tunnel was established for communication between sites. <br><br>  Once again, the stand layout: <br><br><img src="https://habrastorage.org/webt/zn/jn/rj/znjnrju-hgpmcda03oafxyjea0m.png"><br><br>  Setting up replication via a web browser: <br><br><img src="https://habrastorage.org/webt/yq/xf/_7/yqxf_7fzzgtoob1viimmq6i_hfy.png"><br><br>  After successful authorization appears Replication link: <br><br><img src="https://habrastorage.org/webt/oq/fj/of/oqfjofd7qgfdelgyxffmwifoese.png"><br><br>  Checked performance. <br><br>  We mounted two balls from each cluster into two folders: <br><br><pre> <code class="bash hljs">[root@vrts-nbu03 mnt]<span class="hljs-comment"><span class="hljs-comment"># showmount -e vrts-access.veritas.lab Export list for vrts-access.veritas.lab: /vx/VMware-Test * /vx/test_rpl_in * [root@vrts-nbu03 mnt]# showmount -e 192.168.3.210 Export list for 192.168.3.210: /vx/NAS * /vx/test_rpl * [root@vrts-nbu03 mnt]# mount -t nfs 192.168.3.210:/vx/test_rpl /mnt/repl_out/ [root@vrts-nbu03 mnt]# mount -t nfs vrts-access.veritas.lab:/vx/test_rpl_in /mnt/repl_in/ [root@vrts-nbu03 mnt]#</span></span></code> </pre> <br><br>  Copy the data to the source folder: <br><img src="https://habrastorage.org/webt/q7/nt/wz/q7ntwz2vu051z0szllplzk8vp08.png"><br><img src="https://habrastorage.org/webt/2i/18/jx/2i18jxuulzhykxgi4ai546l0u1s.png"><br><br>  We started the replication task in order not to wait for the timer: <br><pre> <code class="bash hljs">va73&gt; va73&gt; replication job sync test_rpl_job Please <span class="hljs-built_in"><span class="hljs-built_in">wait</span></span>... ACCESS replication SUCCESS V-288-0 Replication job sync <span class="hljs-string"><span class="hljs-string">'test_rpl_job'</span></span>. va73&gt;</code> </pre><br>  Files appeared in the recipient's folder: <br><br><img src="https://habrastorage.org/webt/cp/gn/-c/cpgn-cwx1ve72qlc6tl88jsods4.png"><br><img src="https://habrastorage.org/webt/cd/vb/7-/cdvb7-lujen17gfbfbrnger10sa.png"><br><br><h3>  FINDINGS </h3><br>  Veritas Access is an interesting Software-Defined Storage solution that it is not a shame to offer a customer.  It is a truly affordable, scalable storage system with support for file, block and object access.  Access provides the ability to build high-performance and cost-effective storage for unstructured data.  Integration capabilities with OpenStack, cloud providers and other Veritas technologies allow you to apply this solution in the following areas: <br><br>  ‚Ä¢ <b>Media holdings:</b> storage of photo and video content; <br>  ‚Ä¢ <b>Public sector:</b> storage of video archives by systems like ‚Äúsafe city‚Äù; <br>  ‚Ä¢ <b>Sports:</b> storage of video archives and other important information by stadiums and other sports facilities; <br>  ‚Ä¢ <b>Telecommunications:</b> storage of primary data CDR billing (Call Detail Records); <br>  ‚Ä¢ <b>Financial sector:</b> storage of statements, payment documents, passport scans, etc .; <br>  ‚Ä¢ <b>Insurance companies:</b> storage of documentation, scans of passports, photos, etc .; <br>  ‚Ä¢ <b>Medical sector:</b> storage of X-rays, MRI scans, test results, etc .; <br>  ‚Ä¢ <b>Cloud providers:</b> storage organization for OpenStack. <br>  ‚Ä¢ <b>Alternative to tape storage systems</b> <br><br><img src="https://habrastorage.org/webt/4x/9u/xv/4x9uxvsduljdj4i7kiv2inyslzs.png"><br><br>  Strengths: <br><br>  ‚Ä¢ Easy scalability; <br>  ‚Ä¢ Any x86 server; <br>  ‚Ä¢ Relatively low cost. <br><br>  Weak sides: <br><br>  ‚Ä¢ In the current release, the product requires increased attention from engineers; <br>  ‚Ä¢ Weak documentation; <br>  ‚Ä¢ CIFS, iSCSI work in active-passive mode. <br><br>  The Veritas Access team regularly releases new releases as scheduled (roadmap), fixing bugs and adding new features.  Of the interesting things in the new release of Veritas Access 7.3.1 of December 17, 2017 is expected: a full implementation of iSCSI, erasure coding, up to 32 nodes in a cluster. <br><br>  If you have questions about work, functionality or configuration - write, call, we are always ready for help and cooperation. <br><br>  Dmitry Smirnov, <br>  Design engineer, <br>  Open Technologies <br>  Tel: +7 495 787-70-27 <br>  dsmirnov@ot.ru </div><p>Source: <a href="https://habr.com/ru/post/344628/">https://habr.com/ru/post/344628/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../344618/index.html">‚ÄúI should always be in sight‚Äù - Interview with Oleg Shelayev from ZeroTurnaround (part 1)</a></li>
<li><a href="../344620/index.html">How to migrate data from VMware to OpenStack: DRaaS and migration</a></li>
<li><a href="../344622/index.html">Vivaldi Sync - first acquaintance</a></li>
<li><a href="../344624/index.html">HP leaves random keylogger in laptop keyboard driver</a></li>
<li><a href="../344626/index.html">We understand and work with gulp</a></li>
<li><a href="../344630/index.html">Tutorial on creating a tracker cryptocurrency for android on Kotlin</a></li>
<li><a href="../344632/index.html">SOC for beginners. How to organize monitoring of incidents and response to attacks in the 24x7 mode</a></li>
<li><a href="../344634/index.html">ATOL Online Online Cash Desk Service: API and CMS Integration</a></li>
<li><a href="../344638/index.html">Corona SDK - Crush Crush Game</a></li>
<li><a href="../344640/index.html">Mail for Good: how the community of programmers helps NGOs</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>