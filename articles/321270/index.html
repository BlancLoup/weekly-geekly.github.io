<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to evaluate big tasks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="There are many ways to evaluate user stories. We use our own methodology to evaluate and work out tasks before writing code. As we reached it and why ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to evaluate big tasks</h1><div class="post__text post__text-html js-mediator-article">  There are many ways to evaluate user stories.  We use our own methodology to evaluate and work out tasks before writing code.  As we reached it and why our approach is better, than Planing Poker, read under a cat. <br><br><img src="https://habrastorage.org/files/381/575/d08/381575d08a0841d0bd27da21f7acefbd.jpg" alt="image"><br><a name="habracut"></a><br><h2>  Something about Planning Poker </h2><br>  For three years we have been using Planning Poker.  With this approach, each programmer in a closed evaluation of history in the 0.5, 1, 2, 3, 5, 8, 13, 20, 40 conventional units (story points).  Then the people who gave the highest and lowest scores explain why this task seems so difficult to them, or vice versa - simple.  The discussion continues until everyone comes to a single assessment. <br><br>  After the sprint is completed, the master calculates how many story points in completed stories.  Based on the collected statistics, it determines how many tasks will fit in the next sprint. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  What's the problem </h2><br><h4>  Understanding history on the go </h4><br>  To evaluate a user story, developers need to understand how to implement it, at least in general terms.  To understand how to implement, you need to understand what the client wants.  All this is cleared up and discussed during the assessment.  For one story, the team spends 5-30 minutes.  At the same time, 2-3 people who are better versed in the subject are actively involved in the discussion.  The rest of the time is wasted. <br><br>  Since the time for evaluation is still very limited, developers often miss important points.  This can lead to the fact that the story is greatly underestimated, or it will have to be redone from scratch. <br><br>  Sometimes it turns out that the manager did not recognize important nuances from the client, and the evaluation of the story has to be postponed. <br><br><h4>  The illusion of accurate evaluation of large stories </h4><br>  We estimate one story at 20. Another ten stories are estimated at 2 each.  The score of ten small stories in the sum equals one big.  However, a great story is almost always done longer than several small ones with the same rating. <br><br>  With this method of evaluation, <b>20&gt; 2 * 10</b> . <br><br>  Why it happens?  The greater the size of the story, the more opportunities to forget to take something into account when evaluating it.  Even if everything is taken into account, the probability to underestimate by 50% one story is greater than the probability to underestimate ten stories as much. <br><br><h4>  How many percent did you finish your job? </h4><br>  In the middle of the task of size 20, the manager asks the developer when he finishes.  The developer will either answer something unintelligible, or give an overly optimistic estimate, or take his optimistic estimate and multiply it by 2. The manager is unlikely to achieve an accurate assessment. <br><br>  This is due to the fact that for the answer the developer divides the task into the part that he did and the part that remained, after which he quickly assesses the remaining part.  He may be mistaken when trying to take into account everything that he still has to do, and in assessing the size of this part.  Moreover, he gives this assessment alone and very quickly, so as not to make the manager wait. <br><br><h4>  40 = unknown </h4><br>  The team gives a score of 40, which means that programmers really do not know how long the task will take.  They do not fully understand how they will do it. <br><br>  The same problem, albeit to a lesser extent, concerns ratings of 20, 13, 8. If the story described by a pair of sentences is estimated at 13 or 20, this also means that developers do not have a complete understanding of how to do it.  Trying the whole team to describe in detail the solution to the problem during the assessment is an inefficient waste of time.  This can be done by one person.  To solve this problem, we assigned a person responsible for each task, who described the solution before the team assessment.  However, there were no clear criteria for how detailed the solution should be.  Sometimes a couple of sentences is enough, and sometimes you need a few paragraphs to make the solution of the problem understandable. <br><br><h2>  Divide into parts </h2><br>  Trying to deal with these problems, we came up with the following idea: the history should be divided into subtasks until no one has any questions about how long each solution will take.  Subtasks must be of equal size so that we can get a rating just by adding them together. <br><br>  As a result, our design-evaluation process looks like this: <br><br><ol><li>  At the entrance, a user story is minimal functionality that will benefit the customer.  The user history must have acceptance criteria that the manager or client can understand so that it can be accepted. </li><li>  If the story looks too big, then MVP stands out: everything is thrown out, without which the client can do at first.  Upon completion of this story, the following is done, in which already added functionality will be expanded. </li><li>  Behind this story, a responsible person is appointed who, if necessary, breaks the story into separate tasks so that each task can be done in parallel with others.  Often there are user stories that do not make sense to break into tasks: either they do not parallel, or the story is too small.  If possible, tasks should be done so that they can be tested independently.  Then testing can be started after the completion of the first task, and not wait until they finish the whole story. </li><li>  Then the responsible person breaks each task into subtasks.  Each subtask is a paragraph with a description from one sentence to a paragraph.  It may make no sense to non-programmers.  Separate subtasks like ‚Äúwrite a class‚Äù and ‚Äúwrite tests for this class‚Äù - this is normal.  When breaking into subtasks, the responsible person is guided by the following rules: <br><br><ul><li>  all subtasks should be about the same minimum size (1 story point) </li><li>  for each subtask it should be clear what should be done in it </li><li>  the subtask has the acceptance criteria that the programmer understands (the class is written, the tests pass) </li></ul><br></li><li>  The team evaluates the task.  In a team assessment, a review is made of how well the responsible person worked out the decision and divided the task into subtasks, whether there are too many or too small subtasks among the subtasks.  Wherein: <br><br><ul><li>  if a specific subtask is incomprehensible to someone, it is described in more detail; </li><li>  if the team decides that the subtask is too large, it is broken into pieces; </li><li>  if, when reading the description of subtasks, many questions arise that the responsible person cannot answer, the task is sent to a more detailed study; </li><li>  if the team does not like the solution to the problem, it is changed on the fly or sent for redesign. </li></ul></li></ol><br>  After evaluating all the tasks we get an assessment of the user history.  The number of subtasks in history is its assessment in story points. <br><br><h2>  Arguments for </h2><br><h4>  Rapid Team Evaluation </h4><br>  Since the number of assessment options is very limited, the team assessment is faster.  You can further accelerate the team assessment, if you do not vote on each subtask, and ask open-ended questions: <br><br><ul><li>  What are the subtasks incomprehensible? </li><li>  What subtasks should be divided? </li><li>  What are the subtasks worth combining? </li></ul><br><h4>  Formalization of design quality </h4><br>  Clear criteria for the division of tasks into subtasks and the assessment of the result of the design by the whole team reduces the likelihood of forgetting about any important points and do not allow for framing while designing.  When the person responsible for working out the task breaks it into fairly small parts, he automatically has questions for both the customer of the feature and the existing code, which may have to be refactored.  This is better than when the same questions arise in the midst of working on a task. <br><br>  Moreover, if some questions did not occur to the designer, they will be asked during the team evaluation.  When a task is worked out only while working on a ticket, only one person is responsible for this and he may not ask any important questions, as a result of which the task may have to be redone. <br><br>  We often enough history is worked through by one person, and fulfilled by another.  But even if you want to assign the task to the developer prior to the assessment, it is still useful to separate the design and assessment tasks separately by the team.  At the evaluation stage, someone from the team can tell how to make the task easier, or it may turn out that it should not be done at all - somewhere there is already the necessary functionality.  Although, of course, assigning tasks to developers does not add flexibility to the process.  I recommend building the process so that the designed task can be performed by any developer in the team. <br><br><h4>  Clear understanding of how the task is completed </h4><br>  Since each subtask has acceptance criteria that are understandable to the programmer, he always understands how many subtasks he has done and how many are left.  Therefore, he can easily answer the question when the task will go to the acceptance. <br><br><h2>  Arguments against </h2><br><h4>  It takes time to prepare the task for the team assessment. </h4><br>  Dividing one task into subtasks usually takes about 5-30 minutes, but for some stories consisting of several tasks, in difficult cases it can take a couple of days. <br><br>  Is it worth spending so much time just for the sake of a more accurate estimate?  Of course not.  But in fact, almost all the time is spent not on splitting the task, but on working out and designing its solution.  To describe in detail the solution of some stories you may need to discuss it with several people and view the existing code.  It takes a lot of time.  The process of dividing into subtasks takes half an hour at worst. <br><br>  A pre-designed solution that a team can evaluate before starting work is worth it to spend so much time on it. <br><br><h4>  Well this is not agile. </h4><br>  Usually with Agile approaches, the design is carried out simultaneously with the development.  From this point of view, our approach is less agile.  However, the main thing in Agile is to deliver value to the customer as quickly as possible.  If you select MVP, then it is not so important in what order you develop it.  You will still receive feedback from the client only after the completion of the MVP development. <br><br><h4>  Binding effect is possible </h4><br>  Since the team openly assesses the split that the design engineer provided for them, a binding effect may arise - team members will agree with the split that has already been set, especially if it was made by a more authoritative developer.  In our practice, this problem does not manifest itself much, usually people do not hesitate to say that the subtask is too big, too small or incomprehensible.  But, if your team has problems with this, you can move each subtask to a closed vote with options: <br><br><ul><li>  too big to divide; </li><li>  too small to combine with another; </li><li>  just right. </li></ul><br>  At the same time, before voting, anyone who does not understand the subtask can ask questions. <br><br><h4>  A developer cannot grow by performing tasks in which everything is already thought out before him </h4><br>  When performing a detailed task, the programmer has much less freedom for creativity.  If a developer only writes code for stories designed for him, he will not grow and can be heavily demotivated.  Therefore, it is important that each developer in the team has a chance to engage in design - everyone should be divided into stories into tasks and subtasks.  It's not scary if more experienced developers are more likely to work on user stories.  It is important that even the juniors have a bit of design. <br><br><h4>  You can not quickly assess the big story </h4><br>  The process of evaluating tasks through partitioning is quite laborious, so it will not be possible to use it for a quick assessment.  For a preliminary approximate assessment of stories, you can use an expert assessment of two or three people.  Such an assessment will be inaccurate, but it is enough to prioritize it.  It‚Äôs not worth giving a preliminary estimate in story points so that the accuracy of this estimate does not arise. <br><br>  I recommend using the good old man-weeks for a preliminary assessment: half a week, one, two, a month.  After evaluating the history of the team, people who gave an expert assessment can quickly find out how wrong they were - it is enough to translate their man-weeks into story points (we know the average speed of one developer from statistics). <br><br>  If possible, try not to evaluate the tasks prior to prioritization.  If, after evaluating the story, it is postponed for a long time, then you will lose time: <br><br><ul><li>  this time you could spend on something useful to the client now; </li><li>  by the time you take up this story, its assessment may become irrelevant due to refactorings or new features. </li></ul><br>  Therefore, the assessment should come tasks that already need to do.  If for prioritization you need to know how long the story will take, you should get by with a preliminary estimate. <br><br>  If you are satisfied with the assessment using Planning Poker, but there are problems with the quality of the design, you can break the story into subtasks after evaluation and prioritization.  Then the development process will be as follows: <br><br><ul><li>  evaluate stories using Planning Poker, </li><li>  prioritize </li><li>  the developer paints the solution to the subtasks, </li><li>  the team evaluates this decision </li><li>  the developer writes the code. </li></ul><br>  With this approach, you still get a review of the design result before the developer starts writing code.  Although totally such a process is likely to take more time. <br><br><h2>  Example </h2><br>  For example, let's take the following feature: we want to implement push notifications in the browser with our js-plugin for the subscription, with blackjack and poetess.  Ideally, we want: <br><br><ul><li>  js-plugin works in all browsers that support push notifications; </li><li>  js-plugin works on both https and http sites; </li><li>  notifications can be sent both manually and by mass to the event for individual consumers; </li><li>  track deliveries and clicks in notifications. </li></ul><br>  Let's select the MVP from this feature - we‚Äôll remove all that is possible so that we can get a useful feature for at least one of the clients: <br><br><ul><li>  remove Safari support, since it has its own implementation of push notifications; </li><li>  remove support for http sites, since for their support you need to write a complex hack; </li><li>  let's leave only the mass sending of notifications, since most of the clients with https sites need mass sending first of all; </li><li>  in the beginning we will do without statistics, so we‚Äôll postpone tracking of delivery and clicks until later. </li></ul><br>  MVP features can be broken down into the following tasks, each of which can be done independently: <br><br><ol><li>  js-library responsible for the subscription and display notifications. </li><li>  Micro service sending push notifications. </li><li>  UI for sending mass notifications. </li></ol><br>  A little we study the Google documentation web push notifications, then we divide the first task into subtasks: <br><br><ol><li>  Implement the following methods in the js library: <br><ul><li>  verification that the browser supports push notifications; </li><li>  check whether the site user is subscribed to notifications; </li><li>  subscription to notifications. </li></ul></li><li>  Write tests that verify that these methods work correctly in browsers that support push notifications. <br>  <i>We use for this BrowserStack.</i>  <i>Alternatively, at this point there may be a task to test js-methods manually in different browsers.</i> </li><li>  Learn to work with serviceWorkers. <br>  <i>ServiceWorker handles push notifications.</i>  <i>We have not used them before, so we will allocate 1S to study a new technology.</i> </li><li>  Write a serviceWorker handling notifications.  Add the registration of this serviceWorker to the subscription method. </li><li>  Write tests on serviceWorker. </li></ol><br>  As a result, we evaluated the first task in 5S (if, of course, the team agrees with such a partition).  The rest of the tasks are broken down in the same way. <br><br><h2>  About statistics </h2><br>  Team evaluation tells you when the story is ready.  For this, the number of story points completed in previous weeks is taken into account. <br><br>  The history is considered completed only if it has passed acceptance or is even used.  The real interest is precisely how fast we deliver value to customers, and not how quickly tasks get into acceptance. <br><br>  In order for such an assessment to work, it is necessary to agree on the maximum time for review and acceptance.  For example, in our case the person responsible for the review of the task is obliged to make the first review within 2 days from the moment of sending the task to the review, and the revision according to the results of the review should be taken within a day. <br><br>  If it is not possible to select a small MVP and some features are stretched into two sprints, then the sprint statistics will jump.  But even in this case, you can calculate the speed of the team, if you take the average result for the last few sprints. <br><img src="https://habrastorage.org/files/45f/dfc/b32/45fdfcb32c1f4e338cb1a17f007cd0b3.png" alt="image"><br><br><h4>  Reassessment of tasks </h4><br>  No matter how well you work out the task, there is always the chance that something will be left out.  If in the process of working on a feature there appear new unaccounted subtasks, they should be underestimated.  Then you will always know the estimated time to complete the task.  You can also collect statistics as far as story points you assessed for the sprint.  This will help to understand the estimation error. <br><br>  Undervalued story points cannot be used when calculating team speed.  If the team in the past sprint made stories for 50 story points and during the sprint, re-evaluated them for 10 story points - this does not mean that you can take 60 story points in the next sprint.  Take 50, as this time you might have underestimated something. <br><br><h2>  Conclusion </h2><br>  When we began to pre-divide stories into subtasks, there were fewer problems with underestimating large tasks.  There are no more situations when the task has to be reworked from scratch due to incorrect design. <br><br>  I recommend that you try this approach, if you spend too much time on Planning Poker, there are problems with evaluating large tasks or designing. </div><p>Source: <a href="https://habr.com/ru/post/321270/">https://habr.com/ru/post/321270/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321258/index.html">ReactOS at FOSDEM 2017</a></li>
<li><a href="../321260/index.html">Procedural level generation for MERC in Unity</a></li>
<li><a href="../321262/index.html">Logging with rsyslog, file names in tags, multi-line messages and fault tolerance</a></li>
<li><a href="../321266/index.html">Standard exchange 1C-Bitrix on BASH: Detailed parsing of the script of incremental unloading</a></li>
<li><a href="../321268/index.html">Let's Encrypt and Express. To each server - on the green lock</a></li>
<li><a href="../321274/index.html">Features of the distributed storage architecture in Dropbox</a></li>
<li><a href="../321276/index.html">As we did not go to Y Combinator, ‚Äú... the profit plan is simple - here the drugs are legal, $ 70 cant ...‚Äù</a></li>
<li><a href="../321278/index.html">Local multiplayer in Unity using Unet</a></li>
<li><a href="../321280/index.html">The history of the development of TWIME - the new high-speed interface of the Moscow Exchange</a></li>
<li><a href="../321282/index.html">Riot loafers, or again about the accounting of working time</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>