<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>An example of working with the ICE method from a Google and Microsoft product manager</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Working with priorities is a task that requires preparation, experience and consideration of a variety of technologies, scientific approaches, and als...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>An example of working with the ICE method from a Google and Microsoft product manager</h1><div class="post__text post__text-html js-mediator-article">  Working with priorities is a task that requires preparation, experience and consideration of a variety of technologies, scientific approaches, and also author's methods. <br><br>  This article is a translation of material from Hackernoon.com.  Its author proposes the use of its own tool for assessing priorities in the framework of the ICE Scoring method.  This article describes the approach in detail and explores a simple and accessible example that is understandable to any product manager. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1b3/e0f/767/1b3e0f7674f4c85963bcf7d28b2fe514.jpg" alt="image"><br><a name="habracut"></a><br>  <a href="https://twitter.com/ItamarGilad">Itamar Gilad</a> is a renowned product management consultant and successful speaker.  In his many years of experience, he has held product management positions at Google, Microsoft, and other well-known companies.  We offer a translation of his article: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Let's say you manage a product for a small business and its customers.  Your goal is to improve customer engagement and retention.  You have two ideas on the agenda: <br><br><ul><li>  Implementing the main toolbar (dashboard), which allows a business owner to track involvement statistics and all trends. </li><li>  Chatbot (chatbot) to automate communication with customers. </li></ul><br>  The idea with the toolbar arose several times in negotiations with customers, and you feel that it has good potential, but there is a risk that only experienced users will use it. <br><br>  The idea of ‚Äã‚Äãchatbot like the whole company, and the leadership is quite optimistic about it.  Also, the feature looks winning for customers. <br><br>  What would you choose? <br><br>  This prioritization issues underlie product management.  Payment for the wrong choice can be very large and include the cost of development, deployment, maintenance, and other unplanned costs. <br><br>  We are often tempted to make a decision on the basis of inconclusive signals: majority opinions, opinions of bosses, industry trends, etc. But time shows that these signals are exactly at the level of a random number generator. <br><br>  This post is about how, in my opinion, to find the best ideas.  It consists of three parts: <br><br><ul><li>  ICE indicators </li><li>  Levels of trust </li><li>  additional verification </li></ul><br><h2>  Ice scoring </h2><br>  <a href="https://habr.com/company/hygger/blog/422131/">ICE Scoring</a> is a prioritization method that was first used by Sean Ellis, known for his active participation in the development of companies such as DropBox and Eventbrite, and in promoting the term Growth Hacking.  ICE was originally designed to prioritize growth experiments, but soon became used to evaluate any ideas. <br><br>  In ICE, you rate ideas like this: <br><img src="https://habrastorage.org/getpro/habr/post_images/b76/530/a92/b76530a92019abd918d0a970377b155d.jpg" alt="image"><br><ul><li>  <b>Impact</b> demonstrates how an idea has a positive effect on a key indicator that you are trying to improve. </li><li>  <b>Ease of implementation</b> or simplicity is an assessment of how much effort and resources are required to implement this idea. </li><li>  <b>Confidence</b> demonstrates how confident you are in assessing the impact and ease of implementation. </li></ul><br>  ICE values ‚Äã‚Äãare ranked on a scale from 1 to 10, so that all factors in a balanced way influence the total number.  Under the values ‚Äã‚Äãof 1-10, you can mean anything, as long as the values ‚Äã‚Äãare consistent with each other. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a34/729/ac7/a34729ac7b8d45ad002852c50065e320.jpg" alt="image"><br><br>  Now let's take a look at an example of how this works. <br><br><h3>  First ICE </h3><br>  So, you decided to calculate ICE points for two ideas: dashboard and chatbot.  At this early stage, you use rough values ‚Äã‚Äãbased solely on your own intuition. <br>  Impact - you assume that a dashboard will significantly increase user retention, but only experienced ones - you give 5 out of 10. Chatbot, on the other hand, can be an innovative solution for many customers, so you give it 8 out of 10. <br><br>  Ease of implementation - you estimate that 10 person-weeks are required for the dashboard, and 20 for the chat bot. Later, you will receive better quality estimates from the team.  You use this simple table (chosen by you and your team) to convert your score to Ease: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e83/6c9/bc4/e836c9bc4d7e5dae34d4078920bb7ccb.jpg" alt="image"><br><br>  Thus, the toolbar is set to Ease 4 out of 10 and the chatbot is set to 2. <br><br><h3>  Confidence calculation </h3><br>  There is only one way to calculate confidence - this is the search for supporting evidence.  For this, I created a tool that can be seen below.  It lists the general types of tests and evidence that you may have and the level of confidence they provide: test results, date of completion, personal confidence, thematic support, opinions of other people, market data, etc. <br><br>  When using the tool, consider what indicators you already have, how many of them, and what you need in order to gain more confidence. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5dd/fc0/2cb/5ddfc02cb9d045d4a202fa08f61018c8.jpg" alt="image"><br><br>  If another proof of evidence is possible in your product or industry, feel free to create your own version of this tool. <br><br>  Let's go back to the example to evaluate the tool in action. <br><br>  Supporting evidence for chatbot: personal confidence (you think that this is a good idea), thematic support (in the industry also consider it a good idea) and the opinion of others (your superiors and colleagues consider it a good idea).  This gives it a total confidence value of 0.1 out of 10 or Near Zero Confidence.  The tool clearly does not consider opinions as a reliable indicator. <br><br>  What about the dashboard?  Here is personal confidence (you think this is a good idea) and occasional support (several clients have asked for it).  This actually increases its confidence value to 0.5 out of 10, which is a low confidence.  Unfortunately, clients do not predict their future behavior well. <br><br>  ICE scoring in this case: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/dca/194/b2e/dca194b2ed43ec4f9959c345db0e5773.jpg" alt="image"><br><br>  At this point, the toolbar looks like the best idea, but our tool shows that you have not gone beyond the limits of low confidence.  There is simply not enough information to make a decision. <br><br><h3>  Verification of assessment and feasibility </h3><br>  Then you meet with your colleagues responsible for the development and UX, and together begin to evaluate both ideas.  Both projects seem feasible at first glance.  The main developer offers a rough estimate of labor costs: working with the toolbar will take 12 person-weeks for the release, and with the chatbot it will take 16 person-weeks.  According to your Ease scale, this makes it easy to implement in 4 and 3, respectively. <br><br>  In parallel, you make detailed calculations.  On closer inspection, the dashboard looks a little less promising and gets 3. The chatbot still looks at 8. <br>  Using the trust tool shows that both ideas now pass the Estimates &amp; Plans test and gain some confidence.  Now the toolbar moves to 0.8 and the chat bot to 0.4. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/09d/a69/85d/09da6985d42b3125238bb03ee2d559fd.jpg" alt="image"><br><br>  Chatbot rehabilitated a little.  Nevertheless, the level of trust is low for a good reason - these are mostly numbers from nowhere, and you understand that you need to gather more evidence. <br><br><h3>  Market data </h3><br>  You send a questionnaire to existing customers, inviting them to choose one of 5 possible new features, including a chatbot and a toolbar.  Get hundreds of answers.  The results are very positive for the chatbot - this is function number 1 in the questionnaire, and 38% of respondents choose it.  Dashboard takes 3rd place with 17% of the vote. <br><br>  This gives both functions market support, but the chatbot score is up 1.5.  For the control panel, confidence also increased, but only to 1. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b8/770/1e9/8b87701e9fda431b450131b1489211e4.jpg" alt="image"><br><br>  Obviously, the chatbot has advanced a lot.  It seems that your colleagues and industry data proved their case.  Should this data be taken as 100%?  Probably not - the project is quite expensive, and we have all that average confidence.  Unfortunately, the survey results do not give a very significant signal.  We continue to work. <br><br><h3>  Word to customers </h3><br>  To learn more, you run a user study on 10 existing customers, showing them interactive prototypes of both features.  In parallel, you conduct telephone interviews with 20 survey participants who have chosen one of the two suggested features. <br><br>  The study shows a more interesting picture: <br><br><ul><li>  8 out of 10 study participants found the dashboard useful and said they would use it at least once a week.  Their understanding of this function correlated well with what you had in mind initially, and they had no problems with its use.  Telephone interviews confirmed understanding and a desire to use the feature on average once a week. </li><li>  9 out of 10 study participants said they would use the chatbot willingly.  Their level of enthusiasm was very high - everyone immediately understood why this could be useful and many asked him as soon as possible.  However, there were problems with usability, and some customers expressed concern that their customers would not like the repetitive and "hackneyed" answers of the bot. </li></ul><br>  This qualitative research gives you more food for thought.  The toolbar seems more popular than you expected.  The chatbot is now more like a project with a high level of risk and a high price.  Considering our trust tool, you assign toolbars and chatbot trust values ‚Äã‚Äã3 and 2.5, respectively. <br><br>  You set up an effect like this: 6 for a dashboard and 9 for a chatbot.  Finally, based on usability research, you understand that getting a quality UI for a chatbot will require more work ‚Äî you reduce Ease to 2. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7b4/789/663/7b478966379bf45bae1241f5d12fd456.jpg" alt="image"><br><br>  The table has changed again, and now the toolbar is in the lead. <br>  You bring results to your team and your leadership.  According to ICE, the toolbar should be declared the winner, however, on the other hand, the confidence indicators of both features are far from high.  Not wanting to let go of a potentially good feature, the team decides to continue testing both. <br><br><h3>  Final tests and winner! </h3><br>  You decide to start by creating a chatbot version for a minimum viable product (MVP).  Development takes 6 weeks, and you run the MVP for 200 respondents who agreed to take part in the testing.  167 people activate the feature, but its use drops sharply every day, and by the end of the second week you have only 24 active users left. <br><br>  In subsequent surveys, a clear picture emerges - chatbot is more difficult to use, it is much less useful than the participants expected, and, even worse, it creates a negative for customers who value personal contact. <br><br>  You can modify MVP chatbot and make it much more useful for your customers, but it takes about 40-50 person-weeks. <br><br>  It is also obvious that far fewer customers than previously expected will call the feature useful.  Therefore, you reduce the impact from 9 to 2. This significantly changes the feature, so you can no longer trust the results of user research, so reduce trust by 0.5 using the trust tool. <br><br>  You run the MVP toolbar on 200 other clients for 5 weeks.  The results are very good: 87% of the participants use this feature, many of them daily.  Feedback is overwhelmingly positive.  You understand that the impact is higher than you expected - 8 points instead of 6. The development team estimates that it will take another 10 person-weeks to launch the toolbar in full, so the ease of implementation gets 4. As a result, you increase the trust rating with 3 to 6.5. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/418/a15/0fd/418a150fd22c58b0f45eb8ac6f302859.jpg" alt="image"><br><br>  At this point, prioritization becomes quite simple.  Now everyone agrees that the dashboard is the right feature for product development.  You keep the chatbot feature in your idea bank, but it will naturally remain "at the bottom", given the low ICE. <br><br><h4>  findings </h4><br>  <b>1. Stop investing in bad ideas.</b> <br>  Our example shows how risky it is to bet on features that require a lot of effort and are based on feelings, opinions, industry data, market trends, etc. Most of the ideas are in fact much less useful and more expensive than we think before developing.  The only real way to find the best ideas is to test them and reduce the level of uncertainty. <br><br>  <b>2. Worry about benefits, not results</b> <br>  Adding a feature prioritization step reduces the speed of product development - it seems so at first glance.  But in fact it does not reduce, but increases the speed.  By assessing confidence, you simply do not do some bad features.  It also focuses the team on specific short-term goals and increases team productivity.  This process allows us to learn about the product, consumers, market and ultimately get a better product that has already been tested on real users.  Therefore, we are waiting for fewer surprises on the launch day. <br><br>  <b>3. Encourage diversity of approaches.</b> <br>  In fact, we often have to choose not between two ideas, but between dozens.  We reduce the cost of developing an idea based on confidence in it.  This allows us to test many different ideas in parallel and avoid the pitfalls associated with traditional long-term planning. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2c2/a9f/ac7/2c2a9fac78b005b229b533ad6e74f821.jpg" alt="image"><br><br>  In this example, the team tests 4 ideas in parallel, completing several projects (yellow squares), each of which gradually refines the idea and tests it to increase confidence. <br><br>  <b>4. Get the location of management and stakeholders.</b> <br>  Usually, when I explain this method, people are most concerned with how to get the consent of their management and stakeholders to implement such a prioritization process. <br><br>  Can we limit their power over the product?  You will be surprised.  I heard a lot from managers that they are forced to immerse themselves in the process of making product decisions due to the lack of strong options.  A weak or strong option is, of course, a subjective concept, but until you see the real state of affairs with real evidence and a clear level of confidence in the evaluation of the feature. <br><br>  On the other hand, the next time the CEO forces you to make your super idea, show him how an idea is evaluated using factors of influence, effort and confidence, how ICE indicators for this idea are compared with indicators of other ideas, and how we can test her to clarify the confidence factor. <br><br>  You can read about the shortcomings of the ICE method, as well as an alternative way of prioritization in our previous article ‚Äú <a href="https://habr.com/company/hygger/blog/422131/">RICE and ICE Scoring: Simple Prioritization Techniques for Advanced Product Managers</a> .‚Äù <br><br>  <i>Was this article helpful to you?</i>  <i>Would you like to read the materials of this author?</i>  <i>Please tell about it in the comments.</i> </div><p>Source: <a href="https://habr.com/ru/post/424323/">https://habr.com/ru/post/424323/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424311/index.html">Asynchronous business logic today</a></li>
<li><a href="../424313/index.html">EveryLang is a program that can do almost anything.</a></li>
<li><a href="../424315/index.html">New round of import substitution. Where to run and what to do?</a></li>
<li><a href="../424319/index.html">The structure of the online store. Part 2</a></li>
<li><a href="../424321/index.html">We collect NetFlow cheaply and angrily</a></li>
<li><a href="../424325/index.html">Training Splunk - training center now in Russia</a></li>
<li><a href="../424327/index.html">Zuckerberg finances: How to "make friends" optical technology and biomedicine</a></li>
<li><a href="../424329/index.html">Take it and do it: how to pump in programming and development</a></li>
<li><a href="../424331/index.html">Kubernetes 1.12: a review of major innovations</a></li>
<li><a href="../424333/index.html">My experience of dating and working with the Robot Framework</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>