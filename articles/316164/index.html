<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Progress does not stand still: OpenMP 4.5</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Everything flows, everything changes, and OpenMP continues to evolve. Almost three years ago, the standard began to support not only parallelism in ta...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Progress does not stand still: OpenMP 4.5</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/files/2a5/704/76b/2a570476b8b944a8bbab8c871ab8551b.png"></div><br><br>  Everything flows, everything changes, and OpenMP continues to evolve.  Almost three years ago, the standard began to support not only parallelism in tasks, but also data (vectorization), about which I <a href="https://habrahabr.ru/company/intel/blog/204668/">wrote</a> in detail.  It's time to see what appeared in the latest version released in November 2015, and what is already supported by Intel compilers at the moment.  Well, let's get started! <br><a name="habracut"></a><br><h2>  <font color="#0071c5">Taskloop construction</font> </h2><br>  Consider a loop that we want to parallelize on tasks using OpenMP.  Previously, we would simply have written the parallel for directive in front of it, and found our happiness.  Something like this: <br><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel for for (int j = 0; j&lt;n; j++) do_useful_job(j);</span></span></code> </pre> <br>  But ‚Äúeverything changes when they come‚Äù - new standards and opportunities.  Now there is an opportunity not just to give some chunks of iterations to execution for all threads, but to create tasks (tasks) for them that will be distributed among the threads.  This is implemented using the taskloop construction: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp taskloop</span></span></code> </pre><br>  Having written this directive before the for loop, we thereby say that it is necessary to divide the iterations of this loop into pieces, and create a task for each of them.  Further, these tasks will be performed by threads, but there is no direct binding to perform some piece of iterations by a specific thread.  At the same time, we can control the number of tasks using the <i>num_tasks</i> clause, as well as the size of the pieces themselves through <i>grainsize</i> .  If we set 32 ‚Äã‚Äãtasks using <i>num_tasks (32)</i> , then with the number of iterations equal to 1024, each will perform 32 iterations of the loop.  Honestly, in the previous OpenMP 4.0 standard it was possible to do this: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp taskgroup { for (int tmp = 0; tmp &lt; 32; tmp++) #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp task for (int i = tmp * 32; i &lt; tmp * 32 + 32; i++) do_useful_job(i); }</span></span></code> </pre><br>  But now our code will be even easier, especially if the cycles are nested or iterators are used. <br><br>  With the help of <i>grainsize,</i> we can specify the number of iterations that should be performed by one task, and thus the number of tasks will be calculated automatically.  The question arises, what is better to use - the ‚Äúclassic‚Äù <i>parallel for</i> , or <i>taskloop</i> construction? <br>  If your code does not use work with tasks, it will hardly make sense to replace <i>parallel for</i> with <i>taskloop</i> .  The advantage will be manifested in case of imbalance in loop iterations and in the presence of other tasks that can be performed in parallel with the iterations.  Here is an example from the latest OpenMP 4.5 documentation: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">long_running_task</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">void</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">loop_body</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> i, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> j)</span></span></span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">parallel_work</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">void</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i, j; <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp taskgroup { #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp task long_running_task(); </span><span class="hljs-comment"><span class="hljs-meta"><span class="hljs-comment">// can execute concurrently #pragma omp taskloop private(j) grainsize(500) nogroup for (i = 0; i &lt; 10000; i++) { // can execute concurrently for (j = 0; j &lt; i; j++) { loop_body(i, j); } } } }</span></span></span></span></code> </pre><br>  Here we create a task that will perform the long-running function <i>long_running_task</i> , and in the same task group ( <i>taskgroup</i> ) we iterate the loop using taskloop, ‚Äúgiving‚Äù each task 500 iterations.  The function will be executed, possibly in parallel with loop iterations.  The <i>nogroup clause</i> allows us not to create an implicit group ( <i>taskgroup</i> ) for the <i>taskloop</i> construction, so we will not <i>exit the</i> <i>parallel_work</i> function until all tasks have been executed (with loop iterations and the <i>long_running_task</i> function). <br><br>  It is worth noting that working with tasks is more efficient due to the fact that there will not be an oversubscription situation in which too many logical flows are used, which significantly increases costs in the operating system due to the fact that it has to enter time-based access to hardware resources.  Working directly with the physical flows, the developer takes responsibility for ensuring compliance with the parallelism in the application and the available hardware resources.  By the way, the OpenMP 3.0 standard also made it possible to work with tasks, not threads. <br><br>  A vivid example that shows the need for tasks is the use of library functions in parallel regions.  Let's say the same <i>dgemm</i> from MKL, which can be both sequential and parallel.  As a result, it may turn out that we will work with a large number of logical threads, which, quite likely, will negatively affect performance. <br><br>  Taskloop functionality is already supported by the Intel compiler.  By the way, there was also an opportunity to set tasks with different priorities through the <i>priority</i> clause.  In this case, the runtime will perform tasks with high priority first.  True, in the compiler this is not yet. <br><br><h2>  <font color="#0071c5">Doacross parallelism</font> </h2><br>  There are a number of algorithms that have well-structured iterative dependencies.  An example would be many algorithms of stencil computations used in scientific calculations for solving differential equations by the method of finite differences, in problems of computational mechanics. <br><br>  In these cases, we may need to parallelize these cycles, but the ‚Äúcomplexity‚Äù of the calculations at each iteration should be essential so that the threads do not spend most of their time waiting for each other. <br><br>  In general, the order in which the iterations are performed is not defined.  But even in standard 4.0, the ordered construction appeared, which allows you to specify certain parts of the loop that should be executed in a sequential order.  Here is a simple example where this can be useful: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp for ordered schedule(dynamic) for(int n=0; n&lt;100; ++n) { files[n].compress(); #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp ordered send(files[n]); }</span></span></code> </pre><br>  In a cycle, 100 files are ‚Äúcompressed‚Äù in parallel, but their ‚Äútransfer‚Äù is carried out strictly in increasing sequence.  If one of the streams has compressed the 10th file, but the 9th has not been sent yet, then the stream will wait for sending and will not start the compression of the new one.  Thus, all threads work in parallel to the part of the <i>ordered</i> , where execution takes place sequentially.  This will work well if the code outside the ordered block is executed for substantial time. <br><br>  In the new standard, and the compiler from Intel also supports this ‚Äúfeature‚Äù, now it is possible to use well ordered and additional clauses <i>to</i> indicate well-structured dependencies in cycles. <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp for ordered(2) for (int i = 0; i &lt; M; i++) for (int j = 0; j &lt; N; j++) { a[i][j] = foo (i, j); #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp ordered depend (sink: i - 1, j) depend (sink: i, j - 1) b[i][j] = bar (a[i][j], b[i - 1][j], b[i][j - 1]); #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp ordered depend (source) baz (a[i][j], b[i][j]); }</span></span></code> </pre><br>  Let's look at this example.  In this case, only the outer loop is distributed for execution by threads.  The <i>ordered depend (sink)</i> directive blocks the execution of iterations <i>i, j</i> until the execution in iterations <i>i-1, j and i, j-1</i> reaches the next <i>ordered depend depend (source)</i> directive. <br><br>  In this example, the compiler ignores <i>depend (sink: i, j - 1)</i> , since only the iterations of the outer loop are distributed among the threads, which means that when iterating <i>i, j</i> , iteration <i>i, j-1 is</i> guaranteed to be executed. <br>  By the way, for the directive ordered it is now possible to specify the simd <i>clause</i> for use in cycles when vectoring: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp ordered simd</span></span></code> </pre><br>  In this case, you can specify a part of the code in the SIMD loop, which will be executed in lexical order.  Thus, a small area is defined that will not be vectorized. <br><br><h2>  <font color="#0071c5">Total information</font> </h2><br>  C ++ links were previously allowed to be used only in <i>shared</i> clauses, but now there is no such restriction and they may well be <i>private / firstprivate</i> in clauses. <br>  Another expected improvement is due to reductions in cycles.  Let me remind you that they allow you to solve the synchronization problem: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">int</span></span> sum = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel for reduction(+:sum) for (int i = 0; i &lt; N; i++) sum += val[i];</span></span></code> </pre><br>  For example, in this case, we will calculate the value of the variable sum correctly, by creating our own copy in each stream, and summing the obtained values ‚Äã‚Äãfor each of them at the end.  For this, the <i>reduction</i> clause is used, with an operator and variable. <br>  So reductions could not be done for the elements of the array.  That is, if sum is an array of size <i>N</i> , and we would like to make a reduction for <i>sum [i]</i> , then you need to invent something like this yourself (perhaps not the best solution): <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel { int sum_private[N]; #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp for for (int i=0 ; i &lt; N ; i++ ) { for (int j=0; j &lt;=N; j++) { sum_private[i] += val[j]; } } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp critical { for(int i=0; i &lt; N; i++) sum[i] += sum_private[i]; } }</span></span></code> </pre><br>  In standard 4.0, it became possible to create our own reductions (user defined reduction), but for this we needed to create our own data type (wrapper) - structure or class: <br><br><pre> <code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">user_def_red</span></span></span><span class="hljs-class"> {</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> sum[<span class="hljs-number"><span class="hljs-number">10</span></span>]; };</code> </pre><br>  Define the operation, for example, addition: <br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_user_def_red</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(struct user_def_red * x, struct user_def_red * y)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i = <span class="hljs-number"><span class="hljs-number">0</span></span>; i&lt;<span class="hljs-number"><span class="hljs-number">10</span></span>; i++) x-&gt;sum[i] += y-&gt;sum[i]; }</code> </pre><br>  And declare the reduction itself: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp declare reduction(Add_test: struct user_def_red: \ add_user_def_red(&amp;omp_out, &amp;omp_in)) initializer( \ omp_priv={{ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}} )</span></span></code> </pre><br>  And only after that, it was possible to use an array as a variable for reduction: <br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel for reduction(Add_test: Sum_red) for (n = 0; n &lt; 10; ++n) for (m = 0; m &lt;= n; ++m) Sum_red.sum[n] += val[m];</span></span></code> </pre><br>  It is worth noting that the latest version of the Intel 17.0 Update 1 compiler still "did not master" support for the reduction of arrays for C ++. <br>  In addition, the standard now allows declaring non-static class members as private inside class member functions (also non-static), using only its name, that is, without explicit reference using the this pointer. <br><br><h2>  <font color="#0071c5">Sync Tools</font> </h2><br>  Modern processors support transactional memory, such as <i>IBM BlueGene / Q</i> or <i>Intel TSX (Intel Transactional Synchronization Extensions)</i> .  About this memory, you can easily find many posts, such as <a href="https://habrahabr.ru/post/221667/">this</a> .  In general, the idea is quite interesting and can give a performance boost under certain conditions.  It is worth noting that applications may have different requirements for the synchronization mechanism: in some cases, conflicts when accessing shared data occur quite often, in others we protect system calls (for example, I / O operations), or we add synchronization for reinsurance in general, because that conflicts are almost never occur (hash cards).  I would like to be able to choose the implementation of synchronization objects depending on our needs. <br><br>  But, as we know, if you simply use OpenMP directives for synchronization, such as critical, or work with the <i>locking</i> mechanism through the functions <i>omp_init_lock</i> , <i>omp_set_lock</i> and <i>omp_unset_lock</i> , then the compiler is unlikely to create code that uses transactional memory and the corresponding instructions. <br>  Now, at the standard level, it is possible to specify which types of synchronization objects we want to use.  This is done with the help of new functions "with prompts": <br><br><pre> <code class="cpp hljs">omp_init_lock_with_hint(<span class="hljs-keyword"><span class="hljs-keyword">omp_lock_t</span></span> *lock, <span class="hljs-keyword"><span class="hljs-keyword">omp_lock_hint_t</span></span> hint) omp_init_nest_lock_with_hint(<span class="hljs-keyword"><span class="hljs-keyword">omp_nest_lock_t</span></span> *lock, <span class="hljs-keyword"><span class="hljs-keyword">omp_lock_hint_t</span></span> hint)</code> </pre><br>  Through the hint argument we can specify the type of synchronization that satisfies us: <br><br><pre> <code class="cpp hljs">omp_lock_hint_none omp_lock_hint_uncontended omp_lock_hint_contended omp_lock_hint_nonspeculative omp_lock_hint_speculative</code> </pre><br>  Thus, if we need to use transactional memory, we set <i>hint</i> as <i>omp_lock_hint_speculative</i> , and the compiler will generate the appropriate instructions.  For the Intel compiler, we will use <i>Intel TSX</i> as an implementation: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">example_locks</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">omp_lock_t</span></span> lock; omp_init_lock_with_hint(&amp;lock, omp_hint_speculative); <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel { omp_set_lock(&amp;lock); do_some_protected_job(); omp_unset_loc(&amp;lock); } }</span></span></code> </pre><br>  It is possible and for the critical section to set the type through the <i>hint</i> clause, while it must have an explicit name: <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">example_critical_with_hint</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">pragma</span></span></span><span class="hljs-meta"> omp parallel for for (int i=0; I &lt; N; i++) { Data d= get_some_data(i); #pragme omp critical (HASH) hint(omp_hint_speculative) hash.insert(d); } }</span></span></code> </pre><br><br><h2>  <font color="#0071c5">What is left behind the scenes?</font> </h2><br>  In addition to all the above, many small improvements have appeared in the standard that make our lives better.  For example, the ability to set <i>linear</i> clause for <i>#pragma omp declare simd</i> additional options <i>val</i> , <i>uval</i> and <i>ref</i> .  This allows us to clearly indicate what is really ‚Äúlinear‚Äù - the addresses or values ‚Äã‚Äãthemselves.  This will be especially true for developers in Fortran, where function arguments are passed by reference rather than value, which led to a loss of performance when using the <i>declare simd</i> directive due to the generation of gather instructions. <br><br>  I deliberately did not begin to say anything about a huge topic that deserves special attention - OpenMP tools for offload (unloading calculations to various accelerators). <br>  This part has undergone significant changes that can affect even the compilability of code written using the previous standard.  I hope this topic will be interesting and then I will write a sequel.  As the saying goes, "to be continued ...". </div><p>Source: <a href="https://habr.com/ru/post/316164/">https://habr.com/ru/post/316164/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../316152/index.html">Release DataGrip 2016.3</a></li>
<li><a href="../316154/index.html">How to become a product manager. Part 1</a></li>
<li><a href="../316156/index.html">Co-authorship on Habr√©</a></li>
<li><a href="../316158/index.html">Raise your own package repository for Ubuntu (Debian)</a></li>
<li><a href="../316160/index.html">What does the November update Steam for indie developers mean?</a></li>
<li><a href="../316166/index.html">CTFzone write-ups - Going 300, going 500, OSINT sold</a></li>
<li><a href="../316168/index.html">Microsoft data center from Cheyenne switches to wind energy</a></li>
<li><a href="../316170/index.html">Lectures of the Technosphere. Preparatory course "Algorithms and data structures" (spring 2016)</a></li>
<li><a href="../316174/index.html">Security Week 47: Android bookmarks, Wi-Fi security, NTP vulnerability</a></li>
<li><a href="../316180/index.html">The tale of backend, ruby ‚Äã‚Äãand rails</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>