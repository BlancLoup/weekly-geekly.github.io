<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep learning for newbies: fine tuning neural network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Introducing the third (and latest) article in a series designed to help you quickly understand deep learning technology; we will move f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep learning for newbies: fine tuning neural network</h1><div class="post__text post__text-html js-mediator-article"><h2>  Introduction </h2><br>  Introducing the third (and latest) article in a series designed to help you quickly understand <i>deep learning</i> technology;  we will move from basic principles to non-trivial features in order to get decent performance on two data sets: MNIST (handwritten classification) and CIFAR-10 (classification of small images into ten classes: airplane, car, bird, cat, deer, dog, frog , horse, ship and truck). <br><a name="habracut"></a><br><div style="text-align:center;"><img src="https://habrastorage.org/files/ed4/2fc/1e2/ed42fc1e2b0841a98f7a5ca291ef24e2.jpg"></div><br>  Last time, we looked at the <i>convolutional neural network</i> model and showed how using a simple but effective regularization method called dropout, you can quickly reach an accuracy of 78.6% using the Keras deep learning network framework. <br><br>  You now have the basic skills required to apply in-depth training to most interesting tasks (an exception is the task of processing <i>non</i> - <i>linear time series</i> , which is beyond the scope of this manual, and for which <i>recurrent neural networks</i> (RNN) are usually preferable.) The final part of this manual will contain what is very important, but is often overlooked in such articles ‚Äî tricks and tricks of fine-tuning the model to teach it to generalize better than the basic model with which  you started. <br><br>  This part of the manual assumes familiarity with the <a href="https://habrahabr.ru/company/wunderfund/blog/314242/">first</a> and <a href="https://habrahabr.ru/company/wunderfund/blog/314872/">second</a> articles of the cycle. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Configuring hyperparameters and base model </h2><br>  Typically, the process of developing a neural network begins with the development of a simple network, either directly using those architectures that have already been successfully used to solve such problems, or using those hyperparameters that have previously produced good results.  Ultimately, we hope, we will reach a level of performance that will serve as a good starting point, after which we can try to change all the fixed parameters and extract the maximum performance from the network.  This process is commonly referred to as <i>setting up hyper parameters</i> , because it involves changing the network components that must be installed before starting the training. <br><br>  Although the method described here may provide more tangible benefits on the CIFAR-10, due to the relative complexity of quickly creating a prototype on it in the absence of a graphics processor, we will focus on improving its performance on MNIST.  Of course, if resources allow, I encourage you to try out similar methods on CIFAR and see with your own eyes how much they gain compared to the standard CNN approach. <br><br>  The starting point for us will be the original CNN, presented below.  If any code fragments seem incomprehensible to you, I suggest to get acquainted with the previous two parts of this cycle, where all the basic principles are described. <br><br><div class="spoiler">  <b class="spoiler_title">Base Model Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist <span class="hljs-comment"><span class="hljs-comment"># subroutines for fetching the MNIST dataset from keras.models import Model # basic class for specifying and training a neural network from keras.layers import Input, Dense, Flatten, Convolution2D, MaxPooling2D, Dropout from keras.utils import np_utils # utilities for one-hot encoding of ground truth values batch_size = 128 # in each iteration, we consider 128 training examples at once num_epochs = 12 # we iterate twelve times over the entire training set kernel_size = 3 # we will use 3x3 kernels throughout pool_size = 2 # we will use 2x2 pooling throughout conv_depth = 32 # use 32 kernels in both convolutional layers drop_prob_1 = 0.25 # dropout after pooling with probability 0.25 drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5 hidden_size = 128 # there will be 128 neurons in both hidden layers num_train = 60000 # there are 60000 training examples in MNIST num_test = 10000 # there are 10000 test examples in MNIST height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale num_classes = 10 # there are 10 classes (1 per digit) (X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data X_train = X_train.reshape(X_train.shape[0], depth, height, width) X_test = X_test.reshape(X_test.shape[0], depth, height, width) X_train = X_train.astype('float32') X_test = X_test.astype('float32') X_train /= 255 # Normalise data to [0, 1] range X_test /= 255 # Normalise data to [0, 1] range Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels inp = Input(shape=(depth, height, width)) # NB Keras expects channel dimension first # Conv [32] -&gt; Conv [32] -&gt; Pool (with dropout on the pooling layer) conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', activation='relu')(inp) conv_2 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', activation='relu')(conv_1) pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2) drop_1 = Dropout(drop_prob_1)(pool_1) flat = Flatten()(drop_1) hidden = Dense(hidden_size, activation='relu')(flat) # Hidden ReLU layer drop = Dropout(drop_prob_2)(hidden) out = Dense(num_classes, activation='softmax')(drop) # Output softmax layer model = Model(input=inp, output=out) # To define a model, just specify its input and output layers model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function optimizer='adam', # using the Adam optimiser metrics=['accuracy']) # reporting the accuracy model.fit(X_train, Y_train, # Train the model using the training set... batch_size=batch_size, nb_epoch=num_epochs, verbose=1, validation_split=0.1) # ...holding out 10% of the data for validation model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!</span></span></code> </pre> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Learning listing</b> <div class="spoiler_text"><pre> <code class="python hljs">Train on <span class="hljs-number"><span class="hljs-number">54000</span></span> samples, validate on <span class="hljs-number"><span class="hljs-number">6000</span></span> samples Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.3010</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9073</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0612</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9825</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1010</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9698</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0400</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9893</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0753</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9775</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0376</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9903</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0629</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9809</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0321</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9913</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0520</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9837</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0346</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9902</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0466</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9850</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0361</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9912</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0405</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9871</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0330</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9917</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0386</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9879</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0326</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9908</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0349</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9894</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0369</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9908</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0315</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9901</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0277</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9923</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0287</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9906</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0346</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9922</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">12</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">4</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0273</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9909</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0264</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9930</span></span> <span class="hljs-number"><span class="hljs-number">9888</span></span>/<span class="hljs-number"><span class="hljs-number">10000</span></span> [============================&gt;.] - ETA: <span class="hljs-number"><span class="hljs-number">0</span></span>s [<span class="hljs-number"><span class="hljs-number">0.026324689089493085</span></span>, <span class="hljs-number"><span class="hljs-number">0.99119999999999997</span></span>]</code> </pre> <br></div></div><br>  As you can see, our model achieves an accuracy of 99.12% on the test set.  This is slightly better than the results of the MLP, discussed in the first part, but we still have room to grow! <br><br>  In this guide, we will share ways to improve such ‚Äúbasic‚Äù neural networks (without departing from the CNN architecture), and then we will evaluate the performance gains that we will receive. <br><br><h2><img src="https://tex.s2cms.ru/svg/L_2" alt="L_2">  -regularization </h2><br>  In the previous article, we said that one of the main problems of machine learning is the problem of <b>overfitting</b> , when the model in the pursuit of minimizing training costs loses the ability to generalize. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/e9e/98f/7ee/e9e98f7ee6d242a39ad9c836667d4985.png"></div><br>  As already mentioned, there is an easy way to keep retraining under control - the <i>dropout</i> method. <br><br>  But there are other regularizers that can be applied to our network.  Perhaps the most popular of them is <img src="https://tex.s2cms.ru/svg/L_2" alt="L_2">  -regularization (also called weight reduction, weight decay), which uses a more direct approach to regularization than the dropout.  Usually, the root cause of retraining is the complexity of the model (in terms of the number of its parameters), which is too high for the problem being solved and the training set available.  In a sense, the task of the regularizer is to reduce the complexity of the model, while maintaining the number of its parameters. <img src="https://tex.s2cms.ru/svg/L_2" alt="L_2">  -regularization is performed by <i>imposing fines (penalizing) on ‚Äã‚Äãthe weights</i> with the highest values, minimizing them <img src="https://tex.s2cms.ru/svg/L_2" alt="L_2">  -normal using the parameter Œª - the regularization coefficient, which expresses a preference for minimizing the norm with respect to minimizing losses on the training set.  That is, for each weight œâ we add to the objective function <img src="https://tex.s2cms.ru/svg/%5Cmathcal%7BL%7D(%5Cvec%7B%5Chat%7By%7D%7D%2C%5Cvec%7By%7D)" alt="\ mathcal {L} (\ vec {\ hat {y}}, \ vec {y})">  addend <img src="https://tex.s2cms.ru/svg/%7B%5Clambda%5Cover%202%7D%20%7C%7C%5Cvec%7Bw%7D%7C%7C%5E2%20%3D%20%7B%5Clambda%5Cover%202%7D%20%5Csum_%7Bi%3D1%7D%5EWw_i%5E2" alt="{\ lambda \ over 2} || \ vec {w} || ^ 2 = {\ lambda \ over 2} \ sum_ {i = 1} ^ Ww_i ^ 2">  (the factor ¬Ω is used so that the gradient of this term in the parameter œâ equals Œªœâ, and not 2Œªœâ - for the convenience of applying the backpropagation method). <br><br>  Please note that it is crucial to choose the right Œª.  If the coefficient is too small, the effect of regularization will be negligible, but if it is too large, the model will reset all weights.  Here we take Œª = 0.0001;  to add this regularization method to our model, we need one more import, after which it is enough just to add the <code>W_regularizer</code> parameter to each layer where we want to apply regularization. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.regularizers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> l2 <span class="hljs-comment"><span class="hljs-comment"># L2-regularisation # ... l2_lambda = 0.0001 # ... # This is how to add L2-regularisation to any Keras layer with weights (eg Convolution2D/Dense) conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', W_regularizer=l2(l2_lambda), activation='relu')(inp)</span></span></code> </pre> <br><h2>  Network initialization </h2><br>  One of the moments that we lost sight of in the previous article is the principle of <i>choosing the initial weights</i> for the layers that make up the model.  Obviously, this question is very important: setting all weights to 0 will be a serious obstacle to learning, since none of the weights will initially be active.  Assigning values ‚Äã‚Äãfrom the interval of ¬± 1 to the weights is also usually not the best option - in fact, sometimes (depending on the task and complexity of the model), the correct model initialization may depend on whether it reaches the highest performance or does not converge at all.  Even if the task does not imply such an extreme, the successfully chosen method of initializing the scales can significantly affect the model's ability to learn, since it presets the model parameters taking into account the loss function. <br><br>  Here I will give the two most interesting methods. <br><br>  <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Xavier</a> initialization method (sometimes the Glorot method).  The main idea of ‚Äã‚Äãthis method is to simplify the passage of the signal through the layer during both direct and reverse propagation of errors for the <b>linear activation function</b> (this method also works well for the sigmoid function, since the section where it is <i>unsaturated</i> is also linear).  When calculating weights, this method relies on a probability distribution (uniform or normal) with a variance equal to <img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(W)%20%3D%20%7B2%20%5Cover%7Bn_%7Bin%7D%20%2B%20n_%7Bout%7D%7D%7D" alt="\ mathrm {Var} (W) = {2 \ over {n_ {in} + n_ {out}}}">  where <img src="https://tex.s2cms.ru/svg/n_%7Bin%7D" alt="n_ {in}">  and <img src="https://tex.s2cms.ru/svg/n_%7Bout%7D" alt="n_ {out}">  - the number of neurons in the previous and subsequent layers, respectively. <br><br>  The <a href="https://arxiv.org/pdf/1502.01852.pdf">Ge (He)</a> initialization method is a variation of the Zawier method, more appropriate for the <b>ReLU</b> activation function, compensating for the fact that this function returns zero for half of the definition domain.  Namely, in this case <img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(W)%20%3D%20%7B2%20%5Cover%7Bn_%7Bin%7D%7D%7D" alt="\ mathrm {Var} (W) = {2 \ over {n_ {in}}}"><br><br>  To obtain the desired variance for initializing Zavier, consider what happens to the variance of the output values ‚Äã‚Äãof a linear neuron (without the displacement component), assuming that the weights and input values <i>do not correlate</i> and have a <i>zero expectation</i> : <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(%5Csum_%7Bi%3D1%7D%5E%7Bn_%7Bin%7D%7Dw_ix_i)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn_%7Bin%7D%7D%5Cmathrm%7BVar%7D(w_ix_i)%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn_%7Bin%7D%7D%5Cmathrm%7BVar%7D(W)%5Cmathrm%7BVar%7D(X)%20%3D%20n_%7Bin%7D%5Cmathrm%7BVar%7D(W)%5Cmathrm%7BVar%7D(X)" alt="\ mathrm {Var} (\ sum_ {i = 1} ^ {n_ {in}} w_ix_i) = \ sum_ {i = 1} ^ {n_ {in}} \ mathrm {Var} (w_ix_i) = \ sum_ {i = 1} ^ {n_ {in}} \ mathrm {Var} (W) \ mathrm {Var} (X) = n_ {in} \ mathrm {Var} (W) \ mathrm {Var} (X)"></div><p></p><br><br>  From this it follows that in order to preserve the dispersion of the input data after passing through the layer, it is necessary that the dispersion be <img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(W)%20%3D%20%7B1%20%5Cover%7Bn_%7Bin%7D%7D%7D" alt="\ mathrm {Var} (W) = {1 \ over {n_ {in}}}">  .  We can use the same argument in back propagation of an error to get <img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(W)%20%3D%20%7B1%20%5Cover%7Bn_%7Bout%7D%7D%7D" alt="\ mathrm {Var} (W) = {1 \ over {n_ {out}}}">  .  Since we usually cannot meet both of these requirements, we choose the dispersion of weights as their average: <img src="https://tex.s2cms.ru/svg/%5Cmathrm%7BVar%7D(W)%20%3D%20%7B2%20%5Cover%7Bn_%7Bin%7D%20%2B%20n_%7Bout%7D%7D%7D" alt="\ mathrm {Var} (W) = {2 \ over {n_ {in} + n_ {out}}}">  That in practice usually works great. <br><br>  These two methods are suitable for most of the examples that you come across (although research also deserves the <i>orthogonal</i> initialization method, especially with respect to recurrent networks).  It is not difficult to specify the initialization method for the layer: you just need to specify the <code>init</code> parameter, as described below.  We will use a uniform Ge initialization ( <code>he_uniform</code> ) for all ReLU layers and a uniform Zawier initialization ( <code>glorot_uniform</code> ) for the output softmax layer (since in essence it is a generalization of the logistic function to multiple similar data). <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Add He initialisation to a layer conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', init='he_uniform', W_regularizer=l2(l2_lambda), activation='relu')(inp) # Add Xavier initialisation to a layer out = Dense(num_classes, init='glorot_uniform', W_regularizer=l2(l2_lambda), activation='softmax')(drop)</span></span></code> </pre> <br><h2>  Batch normalization (batch normalization) </h2><br>  <b>Butch normalization</b> is a method of accelerating deep learning proposed by <a href="https://arxiv.org/abs/1502.03167">Ioffe and Szegedy</a> in early 2015, already quoted on arXiv 560 times!  The method solves the following problem, which impedes the effective training of neural networks: as the signal propagates through the network, even if we normalize it at the input, passing through the inner layers, it can be greatly distorted both by the mean and the dispersion (this phenomenon is called the <i>internal covariance shift</i> ), which is fraught with serious inconsistencies between gradients at different levels.  Therefore, we have to use stronger regularizers, thereby slowing down the pace of learning. <br><br>  Butch normalization offers a very simple solution to this problem: to <i>normalize the</i> input data in such a way as to get a zero expectation and unit variance.  Normalization is performed before entering each layer.  This means that during training we normalize <code>batch_size</code> examples, and during testing we normalize the statistics obtained from the entire training set, since we cannot see the test data in advance.  Namely, we calculate the expectation and variance for a specific batch (package) <img src="https://tex.s2cms.ru/svg/%5Cmathcal%7BB%7D%3Dx_1%2C%20...%2C%20x_m" alt="\ mathcal {B} = x_1, ..., x_m">  in the following way: <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Cmu_%7B%5Cmathcal%7BB%7D%7D%3D%7B1%5Cover%20m%7D%5Csum_%7Bi%3D1%7D%5Emx_i" alt="\ mu _ {\ mathcal {B}} = {1 \ over m} \ sum_ {i = 1} ^ mx_i"></div><p></p><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Csigma%5E2_%7B%5Cmathcal%7BB%7D%7D%3D%7B1%5Cover%20m%7D%5Csum_%7Bi%3D1%7D%5Em(x_i%20-%20%5Cmu_%7B%5Cmathcal%7BB%7D%7D)%5E2" alt="\ sigma ^ 2 _ {\ mathcal {B}} = {1 \ over m} \ sum_ {i = 1} ^ m (x_i - \ mu _ {\ mathcal {B}}) ^ 2"></div><p></p><br>  Using these statistical characteristics, we transform the activation function in such a way that it has zero expectation and unit variance over the entire batch: <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/%5Chat%7Bx_i%7D%20%3D%20%7Bx_i-%5Cmu_%7B%5Cmathcal%7BB%7D%7D%5Cover%5Csqrt%7B%5Csigma_%7B%5Cmathcal%7BB%7D%7D%5E2%2B%5Cepsilon%7D%7D" alt="\ hat {x_i} = {x_i- \ mu _ {\ mathcal {B}} \ over \ sqrt {\ sigma _ {\ mathcal {B}} ^ 2+ \ epsilon}}"></div><p></p><br>  where Œµ&gt; 0 is the parameter that protects us from division by 0 (in case the standard deviation of the batch is very small or even zero).  Finally, to get the final activation function <i>y</i> , we need to make sure that during normalization we did not lose the ability to generalize, and since we applied scaling and shifting operations to the original data, we can allow arbitrary scaling and shifting of normalized values, having obtained the final function activation: <br><br><p></p><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/y_i%20%3D%20%5Cgamma%5Chat%7Bx_i%7D%20%2B%20%5Cbeta" alt="y_i = \ gamma \ hat {x_i} + \ beta"></div><p></p><br>  Where Œ≤ and Œ≥ are the parameters of the batch normalization that systems can be trained in (they can be optimized using gradient descent on training data).  This generalization also means that the batch normalization can be useful to apply directly to the input data of the neural network. <br><br>  This method, when applied to deep convolutional networks, almost always successfully achieves its goal ‚Äî to accelerate learning.  Moreover, it can happen to be an excellent <i>regularizer</i> , allowing not so careful to choose the pace of learning, power <img src="https://tex.s2cms.ru/svg/L_2" alt="L_2">  - a regularizer and a dropout (sometimes there is no need for them at all).  Regularization here is a consequence of the fact that the result of the network for a particular example is no <i>longer deterministic</i> (it depends on the entire batch, within which this result is obtained), which simplifies the generalization. <br><br>  And finally, although the authors of the method recommend applying the batch normalization <i>to</i> the neuron activation function, recent studies show that if not more useful, then at least it is also advantageous to use it <i>after</i> activation, which we will do in this guide. <br><br>  In Keras, adding a batch normalization to your network is very simple: the <code>BatchNormalization</code> layer is responsible for it, to which we pass several parameters, the most important of which is <code>axis</code> (along which data axis statistical characteristics will calculate).  In particular, while working with convolutional layers, we'd better normalize along separate channels, therefore, choose <code>axis=1</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers.normalization <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> BatchNormalization <span class="hljs-comment"><span class="hljs-comment"># batch normalisation # ... inp_norm = BatchNormalization(axis=1)(inp) # apply BN to the input (NB need to rename here) # conv_1 = Convolution2D(...)(inp_norm) conv_1 = BatchNormalization(axis=1)(conv_1) # apply BN to the first conv layer</span></span></code> </pre> <br><h2>  Extension of the training set (data augmentation) </h2><br>  While the methods described above dealt mainly with the fine-tuning of the <i>model</i> itself, it is useful to explore options for adjusting the <i>data</i> , especially when it comes to image recognition tasks. <br><br>  Imagine that we trained a neural network to recognize handwritten numbers that were about the same size and neatly aligned.  Now let's imagine what will happen if someone gives this network to test slightly shifted numbers of different sizes and slopes - then her confidence in the right class will drop dramatically.  Ideally, it would be good to be able to train the network in such a way that it remains resistant to such <i>distortions</i> , but our model can be trained only on the basis of those samples that we provided to it, despite the fact that it performs some kind of statistical analysis of the training set and extrapolates it. <br><br>  Fortunately, for this problem there is a solution that is simple but effective, especially on image recognition tasks: artificially expand the training data with distorted versions during training!  This means the following: before setting an example for the model input, we apply all the transformations we deem necessary, and then let the network directly observe what effect it has on applying to the data and teaching it to behave well on these examples.  For example, here are some examples of shifted, scaled, deformed, tilted digits from the MNIST set. <br><br><img src="https://habrastorage.org/files/92e/e09/959/92ee09959123461ca76ee856737a43a8.bmp"><img src="https://habrastorage.org/files/22f/0f7/da7/22f0f7da757a42d097691b25f83aa5ac.bmp"><img src="https://habrastorage.org/files/ac4/6c7/ded/ac46c7ded90b4485b3ab6ff7aad84bc5.bmp"><img src="https://habrastorage.org/files/a16/ff4/78f/a16ff478fb3d4bfabdb1a6b7ebc4cf75.bmp"><img src="https://habrastorage.org/files/dcc/8ea/c92/dcc8eac927b7492e88831361a3aaf2c3.bmp"><br><br>  Keras provides a great interface for extending the training set ‚Äî the <code>ImageDataGenerator</code> class.  We initialize the class, telling it what types of transformation we want to apply to the images, and then we run the training data through the generator, calling the <code>fit</code> method, and then the <code>flow</code> method, getting a continuously expanding iterator for the batch types we replenish.  There is even a special method <code>model.fit_generator</code> that will teach our model using this iterator, which greatly simplifies the code.  There is a small flaw: this is how we lose the <code>validation_split</code> parameter, which means we will have to separate the validation subset of the data ourselves, but it only takes four lines of code. <br><br>  Here we will use random horizontal and vertical shifts.  <code>ImageDataGenerator</code> also provides us with the ability to perform random turns, scaling, deformation, and specular reflection.  All these transformations are also worth trying, except perhaps mirror images, since in real life we ‚Äã‚Äãare unlikely to meet handwritten numbers that have been deployed in this way. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.preprocessing.image <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ImageDataGenerator <span class="hljs-comment"><span class="hljs-comment"># data augmentation # ... after model.compile(...) # Explicitly split the training and validation sets X_val = X_train[54000:] Y_val = Y_train[54000:] X_train = X_train[:54000] Y_train = Y_train[:54000] datagen = ImageDataGenerator( width_shift_range=0.1, # randomly shift images horizontally (fraction of total width) height_shift_range=0.1) # randomly shift images vertically (fraction of total height) datagen.fit(X_train) # fit the model on the batches generated by datagen.flow()---most parameters similar to model.fit model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), samples_per_epoch=X_train.shape[0], nb_epoch=num_epochs, validation_data=(X_val, Y_val), verbose=1)</span></span></code> </pre> <br><h2>  Ensembles </h2><br>  One interesting feature of neural networks that can be seen when they are used to distribute data into more than two classes is that, under different initial conditions of learning, they are easier to distribute into one class, while others are confusing.  Using the example of MNIST, one can find that a single neural network is perfectly able to distinguish threes from fives, but does not learn to properly separate units from sevens, while sharing with another network is the other way around. <br><br>  This discrepancy can be dealt with using the method of statistical <b>ensembles</b> ‚Äî place <i>one</i> network and construct several <i>copies of it</i> with different initial values ‚Äã‚Äãand calculate their average result on the same input data.  Here we will build three separate models.  The differences between them can be easily represented in the form of a diagram, also constructed in Keras. <br><br>  <b>Core network</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/780/895/8a6/7808958a6f31419c8fdeaa41972c1c32.png"></div><br>  <b>Ensemble</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/files/0a4/008/ada/0a4008ada0514ed59092ad97b6469247.png"></div><br>  And again Keras allows you to carry out your plans by adding the minimum amount of code - we wrap up the method for constructing the component parts of the model in a cycle, combining their results in the last layer of <code>merge</code> . <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.layers <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> merge <span class="hljs-comment"><span class="hljs-comment"># for merging predictions in an ensemble # ... ens_models = 3 # we will train three separate models on the data # ... inp_norm = BatchNormalization(axis=1)(inp) # Apply BN to the input (NB need to rename here) outs = [] # the list of ensemble outputs for i in range(ens_models): # conv_1 = Convolution2D(...)(inp_norm) # ... outs.append(Dense(num_classes, init='glorot_uniform', W_regularizer=l2(l2_lambda), activation='softmax')(drop)) # Output softmax layer out = merge(outs, mode='ave') # average the predictions to obtain the final output</span></span></code> </pre> <br><h2>  Early stopping </h2><br>  I will describe here another method as an introduction to a wider area of <i>optimization of hyperparameters</i> .  So far, we have used the validation data set solely to monitor the progress of training, which is undoubtedly not rational (as this data is not used for constructive purposes).  In fact, the validation set can serve as a basis for assessing network hyperparameters (such as depth, number of neurons / nuclei, regularization parameters, etc.).  Imagine that a network is run with different combinations of hyperparameters, and then the decision is made based on their performance on the validation set.  Please note that we <b>do not need to know anything about the test set</b> before we <b>finally</b> decide on the hyperparameters, because otherwise the signs of the test set will involuntarily join the learning process.  This principle is also known as the <i>golden rule of machine learning</i> , and has been violated in many early approaches. <br><br>  Perhaps the easiest way to use a validation set is to set the number of ‚Äú <i>epochs</i> ‚Äù (cycles) using a procedure known as <b>early stop</b> ‚Äî just stop the learning process if over a given number of epochs (the patience parameter) losses do not begin to decrease.  Since our data set is relatively small and saturated quickly, we will set the patience to five eras, and we will increase the maximum number of epochs to 50 (this number is unlikely to ever be reached). <br><br>  The early stop mechanism is implemented in Keras through the EarlyStopping <i>callback</i> function class.  Callbacks are called after each learning epoch using the <code>callbacks</code> parameter passed to the <code>fit</code> or <code>fit_generator</code> .  As usual, everything is very compact: our program is increased only by one line of code. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.callbacks <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EarlyStopping <span class="hljs-comment"><span class="hljs-comment"># ... num_epochs = 50 # we iterate at most fifty times over the entire training set # ... # fit the model on the batches generated by datagen.flow()---most parameters similar to model.fit model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), samples_per_epoch=X_train.shape[0], nb_epoch=num_epochs, validation_data=(X_val, Y_val), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping</span></span></code> </pre> <br><h2>  Just show me the code. </h2><br>  After applying the six optimization techniques described above, the code of our neural network will look like this. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> keras.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mnist <span class="hljs-comment"><span class="hljs-comment"># subroutines for fetching the MNIST dataset from keras.models import Model # basic class for specifying and training a neural network from keras.layers import Input, Dense, Flatten, Convolution2D, MaxPooling2D, Dropout, merge from keras.utils import np_utils # utilities for one-hot encoding of ground truth values from keras.regularizers import l2 # L2-regularisation from keras.layers.normalization import BatchNormalization # batch normalisation from keras.preprocessing.image import ImageDataGenerator # data augmentation from keras.callbacks import EarlyStopping # early stopping batch_size = 128 # in each iteration, we consider 128 training examples at once num_epochs = 50 # we iterate at most fifty times over the entire training set kernel_size = 3 # we will use 3x3 kernels throughout pool_size = 2 # we will use 2x2 pooling throughout conv_depth = 32 # use 32 kernels in both convolutional layers drop_prob_1 = 0.25 # dropout after pooling with probability 0.25 drop_prob_2 = 0.5 # dropout in the FC layer with probability 0.5 hidden_size = 128 # there will be 128 neurons in both hidden layers l2_lambda = 0.0001 # use 0.0001 as a L2-regularisation factor ens_models = 3 # we will train three separate models on the data num_train = 60000 # there are 60000 training examples in MNIST num_test = 10000 # there are 10000 test examples in MNIST height, width, depth = 28, 28, 1 # MNIST images are 28x28 and greyscale num_classes = 10 # there are 10 classes (1 per digit) (X_train, y_train), (X_test, y_test) = mnist.load_data() # fetch MNIST data X_train = X_train.reshape(X_train.shape[0], depth, height, width) X_test = X_test.reshape(X_test.shape[0], depth, height, width) X_train = X_train.astype('float32') X_test = X_test.astype('float32') Y_train = np_utils.to_categorical(y_train, num_classes) # One-hot encode the labels Y_test = np_utils.to_categorical(y_test, num_classes) # One-hot encode the labels # Explicitly split the training and validation sets X_val = X_train[54000:] Y_val = Y_train[54000:] X_train = X_train[:54000] Y_train = Y_train[:54000] inp = Input(shape=(depth, height, width)) # NB Keras expects channel dimension first inp_norm = BatchNormalization(axis=1)(inp) # Apply BN to the input (NB need to rename here) outs = [] # the list of ensemble outputs for i in range(ens_models): # Conv [32] -&gt; Conv [32] -&gt; Pool (with dropout on the pooling layer), applying BN in between conv_1 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', init='he_uniform', W_regularizer=l2(l2_lambda), activation='relu')(inp_norm) conv_1 = BatchNormalization(axis=1)(conv_1) conv_2 = Convolution2D(conv_depth, kernel_size, kernel_size, border_mode='same', init='he_uniform', W_regularizer=l2(l2_lambda), activation='relu')(conv_1) conv_2 = BatchNormalization(axis=1)(conv_2) pool_1 = MaxPooling2D(pool_size=(pool_size, pool_size))(conv_2) drop_1 = Dropout(drop_prob_1)(pool_1) flat = Flatten()(drop_1) hidden = Dense(hidden_size, init='he_uniform', W_regularizer=l2(l2_lambda), activation='relu')(flat) # Hidden ReLU layer hidden = BatchNormalization(axis=1)(hidden) drop = Dropout(drop_prob_2)(hidden) outs.append(Dense(num_classes, init='glorot_uniform', W_regularizer=l2(l2_lambda), activation='softmax')(drop)) # Output softmax layer out = merge(outs, mode='ave') # average the predictions to obtain the final output model = Model(input=inp, output=out) # To define a model, just specify its input and output layers model.compile(loss='categorical_crossentropy', # using the cross-entropy loss function optimizer='adam', # using the Adam optimiser metrics=['accuracy']) # reporting the accuracy datagen = ImageDataGenerator( width_shift_range=0.1, # randomly shift images horizontally (fraction of total width) height_shift_range=0.1) # randomly shift images vertically (fraction of total height) datagen.fit(X_train) # fit the model on the batches generated by datagen.flow()---most parameters similar to model.fit model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size), samples_per_epoch=X_train.shape[0], nb_epoch=num_epochs, validation_data=(X_val, Y_val), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=5)]) # adding early stopping model.evaluate(X_test, Y_test, verbose=1) # Evaluate the trained model on the test set!</span></span></code> </pre> <br></div></div><br><div class="spoiler"> <b class="spoiler_title"> </b> <div class="spoiler_text"><pre> <code class="python hljs">Epoch <span class="hljs-number"><span class="hljs-number">1</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.3487</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9031</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0579</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9863</span></span> Epoch <span class="hljs-number"><span class="hljs-number">2</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1441</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9634</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0424</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9890</span></span> Epoch <span class="hljs-number"><span class="hljs-number">3</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.1126</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9716</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0405</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9887</span></span> Epoch <span class="hljs-number"><span class="hljs-number">4</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0929</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9757</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0390</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9890</span></span> Epoch <span class="hljs-number"><span class="hljs-number">5</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0829</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9788</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0329</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9920</span></span> Epoch <span class="hljs-number"><span class="hljs-number">6</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0760</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9807</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0315</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9917</span></span> Epoch <span class="hljs-number"><span class="hljs-number">7</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0740</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9824</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0310</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9917</span></span> Epoch <span class="hljs-number"><span class="hljs-number">8</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0679</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9826</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0297</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9927</span></span> Epoch <span class="hljs-number"><span class="hljs-number">9</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0663</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9834</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0300</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9908</span></span> Epoch <span class="hljs-number"><span class="hljs-number">10</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0658</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9833</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0281</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9923</span></span> Epoch <span class="hljs-number"><span class="hljs-number">11</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0600</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9844</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0272</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9930</span></span> Epoch <span class="hljs-number"><span class="hljs-number">12</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0563</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9857</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0250</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9923</span></span> Epoch <span class="hljs-number"><span class="hljs-number">13</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0530</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9862</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0266</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9925</span></span> Epoch <span class="hljs-number"><span class="hljs-number">14</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">31</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0517</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9865</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0263</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9923</span></span> Epoch <span class="hljs-number"><span class="hljs-number">15</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0510</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9867</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0261</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9940</span></span> Epoch <span class="hljs-number"><span class="hljs-number">16</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0501</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9871</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0238</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9937</span></span> Epoch <span class="hljs-number"><span class="hljs-number">17</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0495</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9870</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0246</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9923</span></span> Epoch <span class="hljs-number"><span class="hljs-number">18</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">31</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0463</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9877</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0271</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9933</span></span> Epoch <span class="hljs-number"><span class="hljs-number">19</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0472</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9877</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0239</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9935</span></span> Epoch <span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0446</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9885</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0226</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9942</span></span> Epoch <span class="hljs-number"><span class="hljs-number">21</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0435</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9890</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0218</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9947</span></span> Epoch <span class="hljs-number"><span class="hljs-number">22</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0432</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9889</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0244</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9928</span></span> Epoch <span class="hljs-number"><span class="hljs-number">23</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0419</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9893</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0245</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9943</span></span> Epoch <span class="hljs-number"><span class="hljs-number">24</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0423</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9890</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0231</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9933</span></span> Epoch <span class="hljs-number"><span class="hljs-number">25</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0400</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9894</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0213</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9938</span></span> Epoch <span class="hljs-number"><span class="hljs-number">26</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0384</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9899</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0226</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9943</span></span> Epoch <span class="hljs-number"><span class="hljs-number">27</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0398</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9899</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0217</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9945</span></span> Epoch <span class="hljs-number"><span class="hljs-number">28</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0383</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9902</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0223</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9940</span></span> Epoch <span class="hljs-number"><span class="hljs-number">29</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">31</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0382</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9898</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0229</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9942</span></span> Epoch <span class="hljs-number"><span class="hljs-number">30</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">31</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0379</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9900</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0225</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9950</span></span> Epoch <span class="hljs-number"><span class="hljs-number">31</span></span>/<span class="hljs-number"><span class="hljs-number">50</span></span> <span class="hljs-number"><span class="hljs-number">54000</span></span>/<span class="hljs-number"><span class="hljs-number">54000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">30</span></span>s - loss: <span class="hljs-number"><span class="hljs-number">0.0359</span></span> - acc: <span class="hljs-number"><span class="hljs-number">0.9906</span></span> - val_loss: <span class="hljs-number"><span class="hljs-number">0.0228</span></span> - val_acc: <span class="hljs-number"><span class="hljs-number">0.9943</span></span> <span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-number"><span class="hljs-number">10000</span></span> [==============================] - <span class="hljs-number"><span class="hljs-number">2</span></span>s [<span class="hljs-number"><span class="hljs-number">0.017431972888592554</span></span>, <span class="hljs-number"><span class="hljs-number">0.99470000000000003</span></span>]</code> </pre> </div></div><br>       99.47%    ,          99.12%. ,        ,  MNIST,     .        CIFAR-10,    ,      . <br><br>       :  ,          ,         ,   ,  ,  ,    ,        (         99.79%  MNIST). <br><br><h2>  Conclusion </h2><br>            ,    : <br><br><img src="https://tex.s2cms.ru/svg/L_2" alt="L_2"> - <br>  <br> - <br>    <br>   <br>   <br><br>        ,   Keras,        MNIST    90  . <br><br>     .      <a href="https://habrahabr.ru/company/wunderfund/blog/314242/"></a>  <a href="https://habrahabr.ru/company/wunderfund/blog/314872/"></a> . <br><br> ,      ,              . <br><br>  Thank! <br><br><blockquote><div class="spoiler">  <b class="spoiler_title">Oh, and come to work with us?</b>  <b class="spoiler_title">:)</b> <div class="spoiler_text">  <a href="http://wunderfund.io/"><b>wunderfund.io</b></a> is a young foundation that deals with <a href="https://en.wikipedia.org/wiki/High-frequency_trading">high-frequency algorithmic trading</a> .  High-frequency trading is a continuous competition of the best programmers and mathematicians of the whole world.  By joining us, you will become part of this fascinating fight. <br><br>  We offer interesting and challenging data analysis and low latency tasks for enthusiastic researchers and programmers.  Flexible schedule and no bureaucracy, decisions are quickly made and implemented. <br><br>  Join our team: <a href="http://wunderfund.io/">wunderfund.io</a> </div></div></blockquote></div><p>Source: <a href="https://habr.com/ru/post/315476/">https://habr.com/ru/post/315476/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../315462/index.html">Human factor</a></li>
<li><a href="../315468/index.html">Stripe Service Discovery</a></li>
<li><a href="../315470/index.html">History of programming languages: from BASIC to Visual Basic</a></li>
<li><a href="../315472/index.html">Cloud on Microsoft Hyper-V, Part 3: Storage Spaces Storage</a></li>
<li><a href="../315474/index.html">Snowden to Russia, Hammond behind bars</a></li>
<li><a href="../315478/index.html">Fonts for android</a></li>
<li><a href="../315480/index.html">Managing the robot at Arduino from the application on Node.js</a></li>
<li><a href="../315482/index.html">What is the strength of the affiliate network, why does power become weak as it progresses, and what to do about it</a></li>
<li><a href="../315484/index.html">Artificial intelligence, challenges and risks - through the eyes of an engineer</a></li>
<li><a href="../315486/index.html">One of the vulnerabilities of WPS technology</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>