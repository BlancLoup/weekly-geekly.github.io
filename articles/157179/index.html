<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The local learning rate of neuron weights in the back-propagation error algorithm</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, in one of the last lectures on neural networks on the cursor, it was about how to improve the convergence of the back-propagation error algorithm ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The local learning rate of neuron weights in the back-propagation error algorithm</h1><div class="post__text post__text-html js-mediator-article">  Hi, in one of the last lectures on <a href="http://class.coursera.org/neuralnets-2012-001/">neural networks on the cursor,</a> it was about how to improve the convergence of <a href="http://habrahabr.ru/post/154369/">the back-propagation error algorithm</a> in general, and in particular, consider a model where each neuron weight has its own learning speed (neuron local gain).  I have long wanted to implement some sort of algorithm that would automatically adjust the speed of network learning, but all the <s>laziness</s> did not reach the hands, and then suddenly such a simple and straightforward way.  In this short article I will talk about this model and give some examples of when this model can be useful. <br><br><a name="habracut"></a><br><br><h4>  Theory </h4><br>  Let's start with the theory, to begin with, let us remember what the change in one weight is equal to, in this case with regularization, but this does not change the essence: <br><img src="https://habrastorage.org/storage2/1f4/8ad/1bf/1f48ad1bfae25c24e6a6cc505068e0f1.gif"><br><ul><li>  <i>this</i> (Œ∑) is the learning rate </li><li>  m is the size of the training set </li><li>  n - layer number </li><li>  full notation <a href="http://habrahabr.ru/post/154369/">here</a> </li></ul><br>  In the basic case, the learning rate is a global parameter for all weights. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      We introduce a learning rate modifier for each weight of each neuron. <img src="https://habrastorage.org/storage2/d93/0e1/876/d930e1876c9fa9a1df85f01df1b1bd17.gif">  , and we will change the weights of neurons according to the following rule: <br><img src="https://habrastorage.org/storage2/85b/d49/676/85bd496766969ca902466d8d99e8383c.gif"><br><br>  In the first training batch, all learning rate modifiers are equal to one, then the dynamic tuning of modifiers in the learning process is as follows: <br><img src="https://habrastorage.org/storage2/72d/f70/bcc/72df70bcc8ccdc57f4d0eef477fd8d69.gif"><br><ul><li><img src="https://habrastorage.org/storage2/1f7/cde/dc5/1f7cdedc5519f56e41f44939c90727b6.gif">  - the value of the gradient at a certain point in time </li><li>  tau œÑ - current time or current training batch </li><li>  b - an additive bonus that the modifier receives if the direction of the gradient does not change along a certain dimension </li><li>  p - multiplicative penalty, in case of changing the direction of the gradient vector </li></ul><br><br>  It makes sense to make a bonus a very small number less than one, and a penalty <img src="https://habrastorage.org/storage2/01f/9ba/4c6/01f9ba4c6adb9d3a1a2691f64434293e.gif">  , thus b + p = 1. For example, a = 0.05, p = 0.95.  This setting ensures that in case of oscillation of the direction of the gradient vector, the value of the modifier will tend back to the initial one.  If you do not go into mathematics, then it can be said that the algorithm encourages those weights (growth within some dimension of the space of weights) that retain their direction relative to the previous time, and penalizes those who start to rush. <br><br>  <a href="http://en.wikipedia.org/wiki/Geoffrey_Hinton">The author of this method,</a> <a href="http://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> (by the way, he was one of the first to suggest using gradient descent to train a neural network) also advises considering the following things: <br><ul><li>  firstly, it is worthwhile to set reasonable growth limits for modifiers </li><li>  secondly, you should not use this technique for online learning, or for small batches;  it is necessary to accumulate a fairly common gradient in the batch process;  otherwise, the frequency of direction oscillations increases, and thus the meaning in the modifier is lost </li></ul><br><br><h4>  Experiments </h4><br>  All experiments were carried out on a variety of pictures 29 by 29 pixels, on which English letters are drawn.  One hidden layer of 100 neurons with a sigmoind activation function was used, at the output of the <a href="http://habrahabr.ru/post/155235/">softmax layer</a> , and cross-entropy was minimized.  In total, it is easy to calculate that the total network is 100 * (29 * 29 + 1) + 26 * (100 + 1) = 86826 weights (including offsets).  The initial weights were taken from a uniform distribution. <img src="https://habrastorage.org/storage2/bab/354/516/bab3545169a331d8d5d54f68b44205d7.gif">  .  In all three experiments, the same initialization of the scales was used.  Also used a full batch. <br><br><h5>  The first </h5><br>  A simple, easily generalizable set was used in this experiment;  The learning rate (global speed) is <b><b>0.01</b></b> .  Consider the dependence of the network error value on the data, on the epoch of learning. <br><br><img src="https://habrastorage.org/storage2/dcb/4b8/9fd/dcb4b89fdb1b6b3e5a97ce1bbf195c90.png"><br><ul><li>  <font color="red">red</font> - without modifier </li><li>  <font color="forestgreen">green</font> - bonus = 0.05, penalty = 0.95, limit = [0.1, 10] </li><li>  <font color="blue">blue</font> - bonus = 0.1, penalty = 0.9, limit = [0.01, 100] </li></ul><br><br>  It can be seen that the modifier has a positive effect on a very simple set, but it is not great. <br><br><h5>  Second </h5><br>  In contrast to the first experiment, I took a lot, which I already taught as it was.  I was aware in advance that with a learning rate of <b>0.01</b> , the oscillation of the value of the error function starts very quickly, and with a smaller value, the set is generalized.  But in this test <b>0.01</b> will be used, since  I want to see what happens. <br><br><img src="https://habrastorage.org/storage2/1ae/be5/0c2/1aebe50c23042ab06f661036f1e30daa.png"><br><ul><li>  <font color="red">red</font> - without modifier </li><li>  <font color="blue">blue</font> - bonus = 0.05, penalty = 0.95, limit = [0.1, 10] </li></ul><br><br>  Complete failure!  The modifier not only did not improve the quality, but on the contrary, it increased the oscillation, while without a modifier, the error on average decreases. <br><br><h5>  Third </h5><br>  In this experiment, I use the same set as in the second, but the global learning rate is <b>0.001</b> . <br><br><img src="https://habrastorage.org/storage2/16c/7cd/885/16c7cd885967c849f446bc52c28c26c7.png"><br><ul><li>  <font color="red">red</font> - without modifier </li><li>  <font color="blue">blue</font> - bonus = 0.005, penalty = 0.995, limit = [0.01, 100] </li></ul><br><br>  In this case, we get a very significant increase in quality.  And if, after 300 epochs, to drive out recognition on the training set and test: <br><ul><li>  <font color="red">red</font> : on the training set 94.74%, on the test set 67.18% </li><li>  <font color="blue">blue</font> : on the training set 100%;  on the test 74.4% </li></ul><br><br><h4>  Conclusion </h4><br>  And I made one conclusion for myself that this method is not a substitute for the choice of a global learning rate, but is a good addition to the already selected learning speed. </div><p>Source: <a href="https://habr.com/ru/post/157179/">https://habr.com/ru/post/157179/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../157167/index.html">Another operating system running Chromebook is Fedora</a></li>
<li><a href="../157171/index.html">What you should know before you get carried away programming</a></li>
<li><a href="../157173/index.html">American "pirate" will pay $ 1.5 million for 10 films laid out</a></li>
<li><a href="../157175/index.html">Git time machine</a></li>
<li><a href="../157177/index.html">Is there a future for OpenOffice?</a></li>
<li><a href="../157181/index.html">Curiosity made a self portrait</a></li>
<li><a href="../157187/index.html">Compatibility with habra editor for MODx Evo</a></li>
<li><a href="../157193/index.html">Independent developer makes a remake of Descent</a></li>
<li><a href="../157197/index.html">Simple automation of versioning and building a C / C ++ project in Ruby</a></li>
<li><a href="../157199/index.html">PDF generation on the server in Ruby</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>