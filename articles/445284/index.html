<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Product Analytics VKontakte based on ClickHouse</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Developing any product, be it video service or tape, stories or articles, I want to be able to measure the conditional "happiness" of the user. Unders...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Product Analytics VKontakte based on ClickHouse</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/0f/oi/nf/0foinfaynjgjrh11h6w55fr5510.jpeg"><br><br>  Developing any product, be it video service or tape, stories or articles, I want to be able to measure the conditional "happiness" of the user.  Understand whether we make our changes better or worse, correct the direction of product development, relying not on intuition and our own feelings, but on metrics and numbers that you can believe in. <br><br>  In this article I will tell you how we managed to launch product statistics and analytics on a service with a 97-million monthly audience, while receiving an extremely high performance of analytical queries.  It will be a question about ClickHouse, used engines and features of requests.  I will describe an approach to data aggregation, which allows us to obtain complex metrics in a fraction of a second, and talk about data conversion and testing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Now we have about 6 billion food events per day, in the near future we will reach 20-25 billion.  And then - not so fast we will rise to 40-50 billion by the end of the year, when we describe all the food events of interest to us. <br><br>  <b>1 rows in set.</b>  <b>Elapsed: 0.287 sec.</b>  <b>Processed 59.85 billion rows, 59.85 GB (208.16 billion rows / s., 208.16 GB / s.)</b> <br><br>  Details under the cut. <br><a name="habracut"></a><br><h1>  Foreword </h1><br>  Analytical tools were VKontakte before.  Unique users were considered, it was possible to build event schedules by cut and thereby fall into the depth of the service.  However, it was about pre-fixed sections, about aggregated data, about HLL for unique users, about some constraint and the inability to quickly answer questions a little more complicated than ‚Äúhow much?‚Äù. <br><br>  Of course, it was, is and will have hadoop, many, many logs of using services are also written, written and will be written to it.  Unfortunately, hdfs was used only by some teams to implement their own tasks.  Even more sadly, hdfs is not about quick analytical queries: there were questions to many fields, the answers to which had to be found in the code, and not in the documentation accessible to all. <br><br>  We concluded that it was impossible to live this way anymore.  Each team must have data, the queries above them must be fast, and the data itself must be accurate and rich in useful parameters. <br><br>  Therefore, we have formulated clear requirements for the new system of statistics / analytics: <br><br><ul><li>  analytical queries should be fast; </li><li>  the data is fairly accurate, ideally, these are raw events of user interaction with the service; </li><li>  the structure of the events should be described, understood and accessible; </li><li>  ensures reliable data storage, one-time delivery guarantee; </li><li>  it is possible to count unique users, the audience (daily, weekly, monthly), metrics of retention, time spent by the user in the service, quantiles of actions for unique and other metrics for a variety of sections; </li><li>  testing, data conversion and visualization is performed. </li></ul><br><h1>  On the kitchen </h1><br>  Experience suggested that we need two bases: a slow one, where we would aggregate and enrich data, and a fast one, where we could work with this data and on top of which we would build graphs.  This is one of the most common approaches, in which in a slow base, for example, in hdfs, different projections are built - on unique and on the number of events on cuts for a certain period of time. <br><br>  On a warm September afternoon, while talking over a cup of tea in the kitchen overlooking the Kazan Cathedral, we had an idea to try ClickHouse as a quick base - at that time we already used it to store technical logs.  There were many doubts related primarily to speed and reliability: the declared performance tests seemed unreal, and new releases of the database periodically broke down the existing functionality.  Because the proposal was simple - try. <br><br><h1>  First samples </h1><br>  We deployed a cluster of two machines with this configuration: <br>  2xE5-2620 v4 (32 cores in total), 256G ram, 28T places (raid10 with ext4). <br><br>  Originally was the near layout, but then we switched to far.  ClickHouse has many different table engines, but the main ones are from the MergeTree family.  We chose ReplicatedReplacingMergeTree with similar settings: <br><br><pre><code class="sql hljs">PARTITION BY dt ORDER BY (toStartOfHour(time), cityHash64(user_id), event_microsec, event_id) SAMPLE BY cityHash64(user_id) SETTINGS index_granularity = 8192;</code> </pre> <br>  <b>Replicated</b> - means that the table is replicable, and this solves one of our requirements for reliability. <br><br>  <b>Replacing</b> - the table supports deduplication by the primary key: by default, the primary key is the same as the sort key, so the ORDER BY section says exactly what the primary key is. <br><br>  <b>SAMPLE BY</b> - I also wanted to try sampling: sample returns a uniformly pseudo-random sample. <br><br>  <b>index_granularity = 8192</b> is the magic number of data lines between the index serifs (yes, it is sparse), which is used by default.  We did not change it. <br><br>  Partitioning done by day (although by default - by month).  Many requests for data were supposed to be intraday ‚Äî for example, plot a minute video view for a particular day. <br><br>  Next, we took a piece of technical logs and filled the table with about a billion lines.  Excellent compression, grouping by Int * type columns, counting unique values ‚Äã‚Äã- everything worked incredibly fast! <br><br>  Speaking of speed, I mean that not a single query lasted longer than 500 ms, and most of them fit into 50‚Äì100 ms.  And this is on two machines - and, in fact, only one was engaged in calculations. <br><br>  We looked at all this and presented that instead of the UInt8 column there would be the country id, and the Int8 column would be replaced by data, for example, about the user's age.  And we realized that ClickHouse is completely suitable for us, if we do everything right. <br><br><h1>  Strong data typing </h1><br>  The benefits of ClickHouse begin exactly when the correct data scheme is formed.  Example: platform String is bad, platform Int8 + dictionary is good, LowCardinality (String) is convenient and good (I will talk about LowCardinality a little later). <br><br>  We have created a special generator class in php, which, upon request, creates wrapper classes over events based on tables in ClickHouse, and a single entry point to logging.  Let me explain by the example of the scheme, which turned out: <br><br><ol><li>  Analyst / data engineer / developer describes the documentation: what fields, possible values, events need to be logged. </li><li>  The table is created in ClickHouse in accordance with the data structure from the previous paragraph. </li><li>  Generate wrapper classes for events based on a table. </li><li>  The product team implements filling in the fields of an object of this class, sending. </li></ol><br>  Changing the scheme at the php level and the type of logged data will not work without first changing the table in ClickHouse.  And this, in turn, cannot be done without coordination with the team, changes in documentation and description of events. <br><br>  For each event, you can set two settings that regulate the percentage of events sent to ClickHouse and hadoop, respectively.  Settings are needed first of all for gradual rolling with the ability to cut down logging if something goes wrong.  Before hadoop, data is delivered in a standard way using Kafka.  And in ClickHouse, they fly through <a href="https://habr.com/ru/company/vk/blog/430168/">the KittenHouse scheme</a> in a persistent mode, which guarantees at least one-time event delivery. <br><br>  The event is delivered to the buffer table on the desired shard, based on the remainder of dividing some hash from user_id by the number of shards in the cluster.  Next, the buffer table resets the data to the local ReplicatedReplacingMergeTree.  And on top of the local tables, the distributed table with the Distributed engine is stretched, which allows you to access data from all shards. <br><br><h1>  Denormalization </h1><br>  ClickHouse is a column DBMS.  It is not about normal forms, which means it‚Äôs better to have all the information right in the event than join-it.  Join-s too, but if the right table does not fit in the memory, the pain begins.  Therefore, we made a volitional decision: all the information we are interested in should be stored in the event itself.  For example, gender, age of the user, country, city, birthday ‚Äî all that is public information that can be useful for analytics of the audience, as well as all useful information about the interaction object.  If, for example, we are talking about a video, it is video_id, video_owner_id, the date the video was uploaded, the length, the quality at the time of the event, the maximum quality, and so on. <br><br>  In total, in each table we have from 50 to 200 columns, while in all the tables there are service fields.  For example, the error log error_log - in fact, we call an error out of range types.  In case strange values ‚Äã‚Äãwill flow into the field with age, beyond the size of the type. <br><br><h2>  Type LowCardinality (T) </h2><br>  ClickHouse has the ability to use external dictionaries.  They are stored in memory, periodically updated, and can be effectively used in various scenarios, including as classic reference books.  For example, you want to log the operating system and you have two alternatives: a string or a number + directory.  Of course, on large amounts of data, and for high-performance analytical queries, it is logical to write a number, and get a string representation from the dictionary when you need: <br><br><pre> <code class="sql hljs">dictGetString('os', 'os_name', toUInt64(os_id))</code> </pre> <br>  But there is a much more convenient way - to use the LowCardinality (String) type, which automatically builds a dictionary.  The performance of LowCardinality under the condition of low cardinality of the set of values ‚Äã‚Äãis radically higher than with String. <br><br>  For example, we use LowCardinality (String) for the event types 'play', 'pause', 'rewind'.  Or for the platform: 'web', 'android', 'iphone': <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> vk_platform, <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> dt = yesterday() <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> vk_platform Elapsed: <span class="hljs-number"><span class="hljs-number">0.145</span></span> sec. Processed <span class="hljs-number"><span class="hljs-number">1.98</span></span> billion <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span>, <span class="hljs-number"><span class="hljs-number">5.96</span></span> GB (<span class="hljs-number"><span class="hljs-number">13.65</span></span> billion <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span>/s., <span class="hljs-number"><span class="hljs-number">41.04</span></span> GB/s.)</code> </pre> <br>  The feature is still experimental, so to use it you must perform: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SET</span></span> allow_experimental_low_cardinality_type = <span class="hljs-number"><span class="hljs-number">1</span></span>;</code> </pre> <br>  But there is a feeling that after some time it will be no longer under the setting. <br><br><h1>  VKontakte data aggregation </h1><br>  Since there are a lot of columns, and there are a lot of events, then the natural desire is to cut the ‚Äúold‚Äù partitions, but before that - to assemble aggregates.  Occasionally it is necessary to analyze raw events (a month or a year ago), so we do not cut the data in hdfs - any analyst can refer to the desired floor for any date. <br><br>  As a rule, during aggregation, in the time interval we always rest on the fact that the number of rows per unit of time is equal to the product of the shear powers.  This imposes restrictions: countries begin to gather into groups such as Russia, Asia, Europe, Rest of the World, and ages into intervals in order to lower the dimension to a conditional million lines per date. <br><br><h2>  Aggregation by <b>dt, user_id</b> </h2><br>  But we have a click ClickHouse!  Can we accelerate to 50‚àí100 million lines on the date? <br>  Quick tests have shown that we can, and at that moment a simple idea arose - to leave the user in the unit.  Namely, to aggregate not by ‚Äúdate, slices‚Äù by spark, but by ‚Äúdate, user‚Äù by means of ClickHouse, while doing some ‚Äútransposing‚Äù the data. <br><br>  With this approach, we retain users in aggregated data, which means we can still count the audience indicators, retention metrics and frequencies.  We can connect the units, considering the general audience of several services, up to the entire VKontakte audience.  All this can be done on any slice that is present in the table for the same time. <br><br>  I will illustrate with an example: <br><br><img src="https://habrastorage.org/webt/1n/zq/23/1nzq23ia7micv91mecw0kqzxgrm.jpeg"><br><br>  After aggregation (many more columns on the right): <br><br><img src="https://habrastorage.org/webt/nx/ol/xl/nxolxl2vmnnlsxlx6svuaklh9go.jpeg"><br><br>  In this case, the aggregation occurs exactly by (dt, user_id).  For fields with information on the user with such an aggregation, you can use the functions any, anyHeavy (selects a common value).  You can, for example, build into an anyHeavy (platform) aggregate in order to know from which platform the user basically triggers video events.  If desired, you can use groupUniqArray (platform) and store an array of all the platforms from which the user triggered the event.  If this is not enough, you can create separate columns for the platforms and store, for example, the number of unique videos scanned to half of a specific platform: <br><br><pre> <code class="sql hljs">uniqCombinedIf(cityHash64(video_owner_id, video_id), (platform = 'android') AND (event = '50p')) as uniq_videos_50p_android</code> </pre> <br>  With this approach, a fairly wide aggregate is obtained, in which each row is a unique user, and each column contains information either on the user or on his interaction with the service. <br><br>  It turns out, to calculate the DAU service, it is enough to execute such a query on top of its aggregate: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> dt, <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> DAU <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> agg <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> dt Elapsed: <span class="hljs-number"><span class="hljs-number">0.078</span></span> sec.</code> </pre> <br>  Or calculate how many days users have been in the service for the week: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> days_in_service, <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> uniques <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> uniqUpTo(<span class="hljs-number"><span class="hljs-number">7</span></span>)(dt) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> agg2 <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> dt &gt; (yesterday() - <span class="hljs-number"><span class="hljs-number">7</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> user_id ) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ASC</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set. Elapsed: <span class="hljs-number"><span class="hljs-number">2.922</span></span> sec.</code> </pre> <br>  We can speed up the sampling, while almost without losing accuracy: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> days_in_service, <span class="hljs-number"><span class="hljs-number">10</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> uniques <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> uniqUpTo(<span class="hljs-number"><span class="hljs-number">7</span></span>)(dt) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> agg2 <span class="hljs-keyword"><span class="hljs-keyword">SAMPLE</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> / <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> dt &gt; (yesterday() - <span class="hljs-number"><span class="hljs-number">7</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> user_id ) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ASC</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set. Elapsed: <span class="hljs-number"><span class="hljs-number">0.454</span></span> sec.</code> </pre> <br>  It should be immediately noted that sampling is not based on the percentage of events, but on the percentage of users - and as a result it becomes an incredibly powerful tool. <br><br>  Or the same for 4 weeks with 1/100 sampling - about 1% less accurate results are obtained. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> days_in_service, <span class="hljs-number"><span class="hljs-number">100</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> uniques <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> uniqUpTo(<span class="hljs-number"><span class="hljs-number">7</span></span>)(dt) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> agg2 <span class="hljs-keyword"><span class="hljs-keyword">SAMPLE</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> / <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> dt &gt; (yesterday() - <span class="hljs-number"><span class="hljs-number">28</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> user_id ) <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> days_in_service <span class="hljs-keyword"><span class="hljs-keyword">ASC</span></span> <span class="hljs-number"><span class="hljs-number">28</span></span> <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set. Elapsed: <span class="hljs-number"><span class="hljs-number">0.287</span></span> sec.</code> </pre> <br><h2>  Aggregation on the other hand </h2><br>  When aggregating by (dt, user_id), we do not lose the user, do not greatly miss the information about its interaction with the service, however, we undoubtedly lose metrics about a specific interaction object.  But you can not lose it - let's build a unit by <br>  (dt, video_owner_id, video_id), sticking to the same ideas.  We preserve information about the video as much as possible, do not greatly miss the data on the interaction of the video with the user and completely skip information about a specific user. <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> starts <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> agg3 <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> (dt = yesterday()) <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> (video_id = ...) <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> (video_owner_id = ...) <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set. Elapsed: <span class="hljs-number"><span class="hljs-number">0.030</span></span> sec</code> </pre> <br>  Or the top 10 video views yesterday: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> video_id, video_owner_id, watches <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> video_agg_video_d1 <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> dt = yesterday() <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> watches <span class="hljs-keyword"><span class="hljs-keyword">DESC</span></span> <span class="hljs-keyword"><span class="hljs-keyword">LIMIT</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span> <span class="hljs-keyword"><span class="hljs-keyword">rows</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> set. Elapsed: <span class="hljs-number"><span class="hljs-number">0.035</span></span> sec.</code> </pre> <br>  As a result, we have a scheme of aggregates of the form: <br><br><ul><li>  aggregation by ‚Äúdate, user‚Äù within the product; </li><li>  aggregation by ‚Äúdate, interaction object‚Äù within the product; </li><li>  sometimes there are other projections. </li></ul><br><h1>  Azkaban and TeamCity </h1><br>  It is worthwhile to say a few words about the infrastructure.  The collection of aggregates is started at night, starting with OPTIMIZE over each of the tables with raw data in order to cause an extraordinary merging of data in the Replicated ReplacingMergeTree.  The operation may take a long time, but it is necessary to remove duplicates, if any.  It is worth noting that while I have never come across duplicates, there are no guarantees that they will not appear in the future. <br><br>  The next step is the creation of aggregates.  These are bash scripts in which the following occurs: <br><br><ul><li>  first we get the number of shards and some host from the shard: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> shard_num, <span class="hljs-keyword"><span class="hljs-keyword">any</span></span>(host_name) <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> host <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> system.clusters <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> shard_num</code> </pre> </li><li>  then the script performs sequentially for each shard (clickhouse-client -h $ host) request of the form (for aggregates by users): <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ... <span class="hljs-keyword"><span class="hljs-keyword">SAMPLE</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>/$shards_count <span class="hljs-keyword"><span class="hljs-keyword">OFFSET</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>/$shard_num</code> </pre> </li></ul><br>  This is not entirely optimal and can generate a lot of networking between hosts.  However, when adding new shards, everything continues to work out of the box, the locality of the data for the aggregates is preserved, so we decided not to worry much about this. <br><br>  We have Azkaban as our task scheduler.  I would not say that this is a super-convenient tool, but it copes well with its task, including when it comes to building slightly more complex pipelines and when one script needs to wait for the execution of several others. <br><br>  The total time it takes to convert existing events into aggregates is 15 minutes. <br><br><h2>  Testing </h2><br>  Every morning, we run automated tests that answer questions about raw data, as well as the availability and quality of the aggregates: ‚ÄúCheck that yesterday or there were no data on any data in the raw data or in aggregates less than half a percent compared to the same day a week ago. ‚Äù <br><br>  Technologically, this is the usual unit tests using JUnit and implementing the ClickHouse jdbc driver.  All tests are run in TeamCity and run for about 30 seconds in 1 thread, and in case of fail, we get VK notifications from our wonderful TeamCity bot. <br><br><h1>  Conclusion </h1><br>  Use only stable versions of ClickHouse, and your hair will be soft and silky.  It is worth adding that <b><i>ClickHouse does not slow down</i></b> . </div><p>Source: <a href="https://habr.com/ru/post/445284/">https://habr.com/ru/post/445284/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445272/index.html">JavaScript is the best programming language for beginners. Is that so or not?</a></li>
<li><a href="../445274/index.html">When ‚ÄúZo√´‚Äù! == ‚ÄúZo√´‚Äù, or why you need to normalize Unicode strings</a></li>
<li><a href="../445276/index.html">A comprehensive guide to useEffect</a></li>
<li><a href="../445278/index.html">How to create a game if you are never an artist</a></li>
<li><a href="../445280/index.html">Profitability of sites and services</a></li>
<li><a href="../445286/index.html">Brainboard: Hedera Hashgraph Distributed Registry Platform</a></li>
<li><a href="../445288/index.html">All your consumer loans and personal data "in one place" ...</a></li>
<li><a href="../445290/index.html">How to implement unified processes with all the features of the company?</a></li>
<li><a href="../445292/index.html">What I have never been told about CSS</a></li>
<li><a href="../445294/index.html">And again about the second monitor from the tablet ...</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>