<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Apache Kafka and Stream Processing with Spark Streaming</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Today we will build a system that will, using Spark Streaming, process Apache Kafka message streams and record the result of processing into...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Apache Kafka and Stream Processing with Spark Streaming</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  Today we will build a system that will, using Spark Streaming, process Apache Kafka message streams and record the result of processing into the AWS RDS cloud database. <br><br>  Imagine that a certain credit organization sets before us the task of processing incoming transactions ‚Äúon the fly‚Äù at all of its branches.  This can be done for the purpose of promptly calculating an open currency position for the treasury, limits or financial result for transactions, etc. <br><br>  How to implement this case without the use of magic and magic spells - read under the cut!  Go! 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/5w/sb/8v/5wsb8vvncrzhysct-pd6oqraqky.jpeg"></div><br>  <a href="https://www.megapixl.com/valerybrozhinsky-stock-images-videos-portfolio">(Image source)</a> <br><a name="habracut"></a><br><h2>  Introduction </h2><br>  Of course, the processing of large amounts of data in real time provides ample opportunities for use in modern systems.  One of the most popular combinations for this is the Apache Kafka and Spark Streaming tandem, where Kafka creates a stream of incoming message packets, and Spark Streaming processes these packets at a specified time interval. <br><br>  To increase the resiliency of the application, we will use checkpoints - checkpoints.  Using this mechanism, when the Spark Streaming module needs to recover the lost data, it will only need to return to the last checkpoint and resume the calculations from it. <br><br><h2>  The architecture of the developed system </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/od/ef/zc/odefzciug8ckvim4-ei6pdg49tw.png"></div><br><br>  Components Used: <br><br><ul><li>  <a href="https://kafka.apache.org/intro"><b>Apache Kafka</b></a> is a distributed publication and subscription messaging system.  Suitable for both offline and online message usage.  To prevent data loss, Kafka messages are stored on disk and replicated within the cluster.  The Kafka system is built on top of the ZooKeeper synchronization service; </li><li>  <a href="https://spark.apache.org/streaming/"><b>Apache Spark Streaming</b></a> - Spark component for streaming data processing.  The Spark Streaming module is built using a microbatch architecture, when the data stream is interpreted as a continuous sequence of small data packets.  Spark Streaming takes data from different sources and combines them into small packets.  New packages are created at regular intervals.  At the beginning of each time interval, a new packet is created, and any data received during this interval is included in the packet.  At the end of the interval, the packet increase stops.  The interval size is determined by a parameter called the batch interval; </li><li>  <a href="https://spark.apache.org/sql/"><b>Apache Spark SQL</b></a> - combines relational processing with Spark functional programming.  Under the structured data refers to data that have a scheme, that is, a single set of fields for all records.  Spark SQL supports input from a variety of structured data sources and, thanks to the availability of schema information, it can efficiently retrieve only the required fields of records, and also provides DataFrame APIs; </li><li>  <a href="https://docs.aws.amazon.com/en_us/AmazonRDS/latest/UserGuide/Welcome.html"><b>AWS RDS</b></a> is a relatively inexpensive cloud relational database, a web service that simplifies configuration, operation, and scaling, is administered directly by Amazon. </li></ul><br><h2>  Installation and start of the Kafka server </h2><br>  Before using Kafka directly, you need to make sure Java is available, because  JVM is used for work: <br><br><pre><code class="bash hljs">sudo apt-get update sudo apt-get install default-jre java -version</code> </pre> <br>  Create a new user to work with Kafka: <br><br><pre> <code class="bash hljs">sudo useradd kafka -m sudo passwd kafka sudo adduser kafka sudo</code> </pre><br>  Next, download the distribution from the official website of Apache Kafka: <br><br><pre> <code class="bash hljs">wget -P /YOUR_PATH <span class="hljs-string"><span class="hljs-string">"http://apache-mirror.rbc.ru/pub/apache/kafka/2.2.0/kafka_2.12-2.2.0.tgz"</span></span></code> </pre> <br>  Unpack the downloaded archive: <br><pre> <code class="bash hljs">tar -xvzf /YOUR_PATH/kafka_2.12-2.2.0.tgz ln -s /YOUR_PATH/kafka_2.12-2.2.0 kafka</code> </pre><br>  The next step is optional.  The fact is that the default settings do not allow full use of all the features of Apache Kafka.  For example, delete the topic, category, group on which messages can be posted.  To change this, edit the configuration file: <br><br><pre> <code class="bash hljs">vim ~/kafka/config/server.properties</code> </pre> <br>  Add the following to the end of the file: <br><br><pre> <code class="bash hljs">delete.topic.enable = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br>  Before starting the Kafka server, you need to start the ZooKeeper server, we will use the auxiliary script that comes with the Kafka distribution: <br><br><pre> <code class="bash hljs">Cd ~/kafka bin/zookeeper-server-start.sh config/zookeeper.properties</code> </pre><br>  After ZooKeeper has successfully started, we launch the Kafka server in a separate terminal: <br><br><pre> <code class="bash hljs">bin/kafka-server-start.sh config/server.properties</code> </pre> <br>  Create a new topic called Transaction: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic transaction</code> </pre> <br>  Make sure that the topic with the required number of partitions and replication was created: <br><br><pre> <code class="bash hljs">bin/kafka-topics.sh --describe --zookeeper localhost:2181</code> </pre> <br><img src="https://habrastorage.org/webt/s5/gh/bu/s5ghbuswhb0dcc0pmlvu_uloes4.png"><br><br>  Let's miss the moments of testing the producer and the consumer for the newly created topic.  For more information on how you can test sending and receiving messages, it is written in the official documentation - <a href="https://kafka.apache.org/documentation/">Send some messages</a> .  Well, we turn to writing a producer in Python using the KafkaProducer API. <br><br><h2>  Producer writing </h2><br>  The producer will generate random data - 100 messages every second.  By random data we mean a dictionary consisting of three fields: <br><br><ul><li>  <b>Branch</b> - name of the point of sale of the credit organization; </li><li>  <b>Currency</b> - the transaction currency; </li><li>  <b>Amount</b> - the amount of the transaction.  The amount will be a positive number if it is a purchase of currency by the Bank, and a negative one if it is a sale. </li></ul><br>  The code for the producer is as follows: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy.random <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> choice, randint <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_random_value</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> new_dict = {} branch_list = [<span class="hljs-string"><span class="hljs-string">'Kazan'</span></span>, <span class="hljs-string"><span class="hljs-string">'SPB'</span></span>, <span class="hljs-string"><span class="hljs-string">'Novosibirsk'</span></span>, <span class="hljs-string"><span class="hljs-string">'Surgut'</span></span>] currency_list = [<span class="hljs-string"><span class="hljs-string">'RUB'</span></span>, <span class="hljs-string"><span class="hljs-string">'USD'</span></span>, <span class="hljs-string"><span class="hljs-string">'EUR'</span></span>, <span class="hljs-string"><span class="hljs-string">'GBP'</span></span>] new_dict[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>] = choice(branch_list) new_dict[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>] = choice(currency_list) new_dict[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>] = randint(<span class="hljs-number"><span class="hljs-number">-100</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> new_dict</code> </pre><br>  Next, using the send method, we send a message to the server, to the topic we need, in JSON format: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaProducer producer = KafkaProducer(bootstrap_servers=[<span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span>], value_serializer=<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:dumps(x).encode(<span class="hljs-string"><span class="hljs-string">'utf-8'</span></span>), compression_type=<span class="hljs-string"><span class="hljs-string">'gzip'</span></span>) my_topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> data = get_random_value() <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: future = producer.send(topic = my_topic, value = data) record_metadata = future.get(timeout=<span class="hljs-number"><span class="hljs-number">10</span></span>) print(<span class="hljs-string"><span class="hljs-string">'--&gt; The message has been sent to a topic: \ {}, partition: {}, offset: {}'</span></span> \ .format(record_metadata.topic, record_metadata.partition, record_metadata.offset )) <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> Exception <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> e: print(<span class="hljs-string"><span class="hljs-string">'--&gt; It seems an Error occurred: {}'</span></span>.format(e)) <span class="hljs-keyword"><span class="hljs-keyword">finally</span></span>: producer.flush()</code> </pre><br>  When you run the script, we receive the following messages in the terminal: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/_e/3g/zj/_e3gzjrmsycjb8ntjmur6ztaspw.png"></div><br>  This means that everything works as we wanted - the producer generates and sends messages to the topic we need. <br><br>  The next step is to install Spark and process this message flow. <br><br><h2>  Install Apache Spark </h2><br>  <b>Apache Spark</b> is a versatile and high-performance cluster computing platform. <br><br>  Spark surpasses the popular implementations of the MapReduce model in performance, while providing support for a wider range of calculation types, including interactive queries and streaming processing.  Speed ‚Äã‚Äãplays an important role in processing large amounts of data, since it is speed that allows you to work online without spending minutes or hours waiting.  One of the most important advantages of Spark, providing such a high speed - the ability to perform calculations in memory. <br><br>  This framework is written in Scala, so you need to install it first: <br><br><pre> <code class="bash hljs">sudo apt-get install scala</code> </pre> <br>  Download from the official website of the Spark distribution: <br><br><pre> <code class="bash hljs">wget <span class="hljs-string"><span class="hljs-string">"http://mirror.linux-ia64.org/apache/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz"</span></span></code> </pre> <br>  Unpack the archive: <br><br><pre> <code class="bash hljs">sudo tar xvf spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz -C /usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark</code> </pre> <br>  Add the path to Spark to the bash file: <br><br><pre> <code class="bash hljs">vim ~/.bashrc</code> </pre> <br>  We add through the editor the following lines: <br><br><pre> <code class="bash hljs">SPARK_HOME=/usr/<span class="hljs-built_in"><span class="hljs-built_in">local</span></span>/spark <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> PATH=<span class="hljs-variable"><span class="hljs-variable">$SPARK_HOME</span></span>/bin:<span class="hljs-variable"><span class="hljs-variable">$PATH</span></span></code> </pre><br>  Run the command below after making changes to bashrc: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">source</span></span> ~/.bashrc</code> </pre> <br><h2>  Deploying AWS PostgreSQL </h2><br>  It remains to deploy the database, where we will upload the processed information from the streams.  For this we will use the AWS RDS service. <br><br>  Go to the AWS console -&gt; AWS RDS -&gt; Databases -&gt; Create database: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/dg/os/m7/dgosm7dwnh3fr-uksjdt_xpltsk.png"></div><br>  Select PostgreSQL and click Next: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3y/d_/8r/3yd_8rsz2swfgaxaafpkyizthac.png"></div><br>  Because  This example is dealt with solely for educational purposes; we will use the free server ‚Äúat minimum wages‚Äù (Free Tier) <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fn/6p/5b/fn6p5bjyitndy_ozs2cdcw_ssi0.png"></div><br>  Next, we put a tick in the Free Tier block, and after that we will be offered a t2.micro class instance ‚Äî albeit a weak one, but free and quite suitable for our task: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/mj/jh/wg/mjjhwg3cknoehrq8wyxk3uw5v74.png"></div><br>  Following are very important things: the name of the DB Instance, the name of the master user and his password.  Let's call the instance: myHabrTest, master user: <b>habr</b> , password: <b>habr12345</b> and click on the Next button: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/lg/jt/mf/lgjtmfdfst0pvqthojb_bdpeohc.png"></div><br><br>  The following page contains the parameters responsible for the availability of our database server from the outside (Public accessibility) and the availability of ports: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/40/z9/q7/40z9q7owar5kpnimyzrdj5laqgs.png"></div><br>  Let's create a new configuration for the VPC security group, which will allow us to access our database server through port 5432 (PostgreSQL) from the outside. <br><br>  Let's move in a separate browser window to the AWS console in the VPC Dashboard section -&gt; Security Groups -&gt; Create security group: <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fl/2i/ne/fl2inejlgnghwsh3itdrlcywdsu.png"></div><br>  Set the name for the Security group - PostgreSQL, the description, specify which VPC this group should be associated with and click the Create button: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/js/8r/tv/js8rtvp8tudwjtpgso6xota5h-g.png"></div><br>  Fill in for the newly created Inbound rules group for port 5432, as shown in the picture below.  You may not specify the port manually, but select PostgreSQL from the Type drop-down list. <br><br>  Strictly speaking, the value :: / 0 means the availability of incoming traffic for a server from all over the world, which is canonically not quite true, but let us use the following approach to parse an example: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ge/8j/bn/ge8jbntssnooajc8so36h0tjo80.png"></div><br>  We return to the browser page, where we openly ‚ÄúConfigure advanced settings‚Äù and select in the VPC security groups -&gt; Choose existing VPC security groups -&gt; PostgreSQL section: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/nk/ae/-s/nkae-ste1tp3wgvmyilicvwlk8e.png"></div><br>  Further, in the Database options -&gt; Database name -&gt; section, set the name - <b>habrDB</b> . <br><br>  The rest of the parameters, with the exception of perhaps disabling backup (backup retention period - 0 days), monitoring and Performance Insights, can be left as default.  Click on the <b>Create database</b> button: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ex/1p/po/ex1ppogq_vdsk3nnvywm7l8vq8i.png"></div><br><h2>  Thread handler </h2><br>  The final stage will be the development of Spark-joba, which will be every two seconds to process new data that came from Kafka and enter the result in the database. <br><br>  As noted above, checkpoints are the primary mechanism in SparkStreaming, which must be configured for fault tolerance.  We will use checkpoints and, in the event of a drop in procedure, the Spark Streaming module to restore the lost data will only need to go back to the last checkpoint and resume the calculations from it. <br><br>  The checkpoint can be enabled by setting the directory in a fault-tolerant, reliable file system (for example, HDFS, S3, etc.) in which the checkpoint information will be stored.  This is done using, for example: <br><br><pre> <code class="python hljs">streamingContext.checkpoint(checkpointDirectory)</code> </pre> <br>  In our example, we will use the following approach, namely, if checkpointDirectory exists, then the context will be recreated from the checkpoint data.  If the directory does not exist (that is, it is executed for the first time), then the functionToCreateContext function is called to create a new context and configure DStreams: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StreamingContext context = StreamingContext.getOrCreate(checkpointDirectory, functionToCreateContext)</code> </pre><br>  Create a DirectStream object in order to connect to the topic ‚Äútransaction‚Äù using the KafkaUtils library createDirectStream method: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.streaming.kafka <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KafkaUtils sc = SparkContext(conf=conf) ssc = StreamingContext(sc, <span class="hljs-number"><span class="hljs-number">2</span></span>) broker_list = <span class="hljs-string"><span class="hljs-string">'localhost:9092'</span></span> topic = <span class="hljs-string"><span class="hljs-string">'transaction'</span></span> directKafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {<span class="hljs-string"><span class="hljs-string">"metadata.broker.list"</span></span>: broker_list})</code> </pre><br>  Parsing incoming data in JSON format: <br><br><pre> <code class="python hljs">rowRdd = rdd.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> w: Row(branch=w[<span class="hljs-string"><span class="hljs-string">'branch'</span></span>], currency=w[<span class="hljs-string"><span class="hljs-string">'currency'</span></span>], amount=w[<span class="hljs-string"><span class="hljs-string">'amount'</span></span>])) testDataFrame = spark.createDataFrame(rowRdd) testDataFrame.createOrReplaceTempView(<span class="hljs-string"><span class="hljs-string">"treasury_stream"</span></span>)</code> </pre><br>  Using Spark SQL, we make a simple grouping and output the result to the console: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">select</span></span> from_unixtime(<span class="hljs-keyword"><span class="hljs-keyword">unix_timestamp</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> curr_time, t.branch <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> branch_name, t.currency <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> currency_code, <span class="hljs-keyword"><span class="hljs-keyword">sum</span></span>(amount) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> batch_value <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> treasury_stream t <span class="hljs-keyword"><span class="hljs-keyword">group</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> t.branch, t.currency</code> </pre><br>  Getting the query text and running it through Spark SQL: <br><br><pre> <code class="python hljs">sql_query = get_sql_query() testResultDataFrame = spark.sql(sql_query) testResultDataFrame.show(n=<span class="hljs-number"><span class="hljs-number">5</span></span>)</code> </pre><br>  And then we save the resulting aggregated data into a table in AWS RDS.  To save the aggregation results to a database table, we will use the DataFrame object's write method: <br><br><pre> <code class="python hljs">testResultDataFrame.write \ .format(<span class="hljs-string"><span class="hljs-string">"jdbc"</span></span>) \ .mode(<span class="hljs-string"><span class="hljs-string">"append"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"driver"</span></span>, <span class="hljs-string"><span class="hljs-string">'org.postgresql.Driver'</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"url"</span></span>,<span class="hljs-string"><span class="hljs-string">"jdbc:postgresql://myhabrtest.ciny8bykwxeg.us-east-1.rds.amazonaws.com:5432/habrDB"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"dbtable"</span></span>, <span class="hljs-string"><span class="hljs-string">"transaction_flow"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"user"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr"</span></span>) \ .option(<span class="hljs-string"><span class="hljs-string">"password"</span></span>, <span class="hljs-string"><span class="hljs-string">"habr12345"</span></span>) \ .save()</code> </pre><br><blockquote>  A few words about setting up a connection to AWS RDS.  We created the user and password to it in the ‚ÄúDeploying AWS PostgreSQL‚Äù step.  As a database server url, use Endpoint, which is displayed in the Connectivity &amp; security section: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/9n/sj/jd/9nsjjdun0hdy5qtwqub0xhvzunk.png"></div></blockquote><br>  In order to correctly link Spark and Kafka, you should run the job through smark-submit using the <b>spark-streaming-kafka-0-8_2.11 artifact</b> .  Additionally, we also apply an artifact for interacting with the PostgreSQL database, we will pass them through --packages. <br><br>  For the flexibility of the script, we will also take as input parameters the name of the message server and the topic from which we want to receive data. <br><br>  So, it's time to start and check the performance of the system: <br><br><pre> <code class="bash hljs">spark-submit \ --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2,\ org.postgresql:postgresql:9.4.1207 \ spark_job.py localhost:9092 transaction</code> </pre><br>  Everything worked out!  As you can see in the picture below, during the operation of the application, new aggregation results are output every 2 seconds, because we set the packaging interval to 2 seconds when we created the StreamingContext object: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cf/q1/25/cfq125zpzkyldktsuvdo175fazy.png"></div><br>  Next, we make a simple database query to check for records in the <b>transaction_flow</b> table: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7j/j9/qm/7jj9qmf4zpter3jkbblrmiqni2s.png"></div><br><h2>  Conclusion </h2><br>  This article has reviewed an example of streaming information using Spark Streaming in conjunction with Apache Kafka and PostgreSQL.  With the growth of data from various sources, it is difficult to overestimate the practical value of Spark Streaming for creating streaming and real-time applications. <br><br>  You can find the full source code in my <a href="https://github.com/igorgorbenko/kafka_project_habr">GitHub repository</a> . <br><br>  I am pleased to discuss this article, waiting for your comments, and also, I hope for constructive criticism of all concerned readers. <br><br>  I wish you success! <br><br>  <b>PS</b> It was originally planned to use a local PostgreSQL database, but considering my love for AWS, I decided to bring the database to the cloud.  In the next article on this topic, I will show how to implement the entire AWS system described above using AWS Kinesis and AWS EMR.  Follow the news! </div><p>Source: <a href="https://habr.com/ru/post/451160/">https://habr.com/ru/post/451160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../45111/index.html">This is how go.com should look like:</a></li>
<li><a href="../451112/index.html">Jetpack compose</a></li>
<li><a href="../451132/index.html">Consumer Driven Contracts or Gitlab CI through the eyes of QA test automation</a></li>
<li><a href="../451148/index.html">Object Oriented Programming in Graphic Languages</a></li>
<li><a href="../451150/index.html">Catch Me If You Can. Manager Version</a></li>
<li><a href="../451162/index.html">Correction of errors - physical constants in the present and new versions of the International System of Units (SI)</a></li>
<li><a href="../451166/index.html">What will the new storages for AI and MO systems offer?</a></li>
<li><a href="../451170/index.html">Jeff Bezos announced plans to conquer the moon</a></li>
<li><a href="../451174/index.html">Adaptation programs for the ZX Spectrum to TR-DOS with modern tools. Part 1</a></li>
<li><a href="../451176/index.html">News from the world of OpenStreetMap ‚Ññ458 (04/23/2019-29.04.2019)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>