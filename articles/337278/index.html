<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Machine learning with non-programmer hands: classifying customer requests for technical support (part 1)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! My name is Kirill and I have been an alcoholic for more than 10 years as an IT manager. I was not always like this: while studying at MIPT, I w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Machine learning with non-programmer hands: classifying customer requests for technical support (part 1)</h1><div class="post__text post__text-html js-mediator-article"><p>  Hello!  My name is Kirill and I have been an <s>alcoholic for</s> more than 10 years as an IT manager.  I was not always like this: while studying at MIPT, I wrote a code, sometimes for a reward.  But faced with the harsh reality (in which you need to make money, preferably more), he went downhill - in managers. </p><br><p><img src="https://habrastorage.org/web/0e1/913/664/0e1913664039495987c4d8777b330acd.jpg" alt="image"><br><br>  But it is not all that bad!  Recently, we and our partners have completely and completely gone into the development of their startup: the accounting system for customers and customer requests <a href="https://www.okdesk.ru/">Okdesk</a> .  On the one hand - more freedom in choosing the direction of movement.  But on the other hand, it‚Äôs impossible to simply take and pledge the budget of "3 developers for 6 months for research and development of a prototype for ...".  You have to do a lot yourself.  Including - non-core experiments associated with the development (ie, those experiments that do not relate to the main functionality of the product). <br><br>  One of such experiments was the development of an algorithm for classifying client requests by texts for further routing to a group of performers.  In this article I want to tell you how a ‚Äúnon-programmer‚Äù can master python in the background for 1.5 months in the background and write a simple ML algorithm that has practical utility. </p><a name="habracut"></a><br><h1 id="kak-uchitsya">  How to study? </h1><br><p>  In my case - distance learning on Coursera.  There are quite a few courses in machine learning and other disciplines related to artificial intelligence.  Classics is the <a href="https://www.coursera.org/learn/machine-learning">course course</a> founder Andrew Una (Andrew Ng).  But the disadvantage of this course (besides the fact that the course is in English: this is not for everyone) is a rare Octave toolkit (a free analogue of MATLAB).  To understand the algorithms is not important, but it is better to learn from the more popular tools. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I chose the specialization " <a href="https://www.coursera.org/specializations/machine-learning-data-analysis">Machine learning and data analysis</a> " from MIPT and Yandex (there are 6 courses in it; the first 2 are enough for what is written in the article).  The main advantage of specialization is a vibrant community of students and mentors in Slack, where almost every time there is someone you can contact with a question. </p><br><h1 id="chto-takoe-mashinnoe-obuchenie">  What is Machine Learning? </h1><br><p>  Within the framework of the article we will not dive into terminological disputes, therefore, those who want to find fault with insufficient mathematical accuracy, please refrain (I promise that I will not go beyond the bounds of decency :). <br><br>  So what is machine learning?  This is a set of methods for solving problems that require any intellectual expenditure of a person, but with the help of computers.  A characteristic feature of machine learning methods is that they ‚Äúlearn‚Äù from precedents (that is, from examples with known correct answers in advance). <br><br>  A more mathematically defined definition looks like this: </p><br><ol><li>  There are many objects with a set of characteristics.  Denote this set by the letter <strong><em>X</em></strong> ; </li><li>  There are many answers.  Denote this set by the letter <strong><em>Y</em></strong> ; </li><li>  There is a (unknown) relationship between the set of objects and the set of answers.  Those.  such a function that associates an object of a set <strong><em>X with</em></strong> an object of a set <strong><em>Y.</em></strong>  Let's call it a function <strong><em>y</em></strong> ; </li><li>  There is a finite subset of objects from <strong><em>X</em></strong> (training sample) for which the answers from <strong><em>Y</em></strong> are known; </li><li>  According to the training sample, it is necessary to approximate the function <strong><em>y</em></strong> as well as possible with some function <strong><em>a</em></strong> .  With the help of the function <strong><em>a,</em></strong> we want for any object from <strong><em>X</em></strong> to get with a good probability (or accuracy - if we are talking about numerical answers) the correct answer from <strong><em>Y.</em></strong>  The search for the function <strong><em>a</em></strong> is a machine learning task. </li></ol><br><p>  Here is an example from life.  Bank grants loans.  The bank has accumulated a lot of borrower questionnaires for which the outcome is already known: they returned the loan, did not return it, returned it in arrears, etc.  The object in this example is the borrower with a completed application form.  Data from the questionnaire - the parameters of the object.  The fact of repayment or non-repayment of the loan is the "answer" on the object (the borrower's questionnaire).  The set of questionnaires with known outcomes is a training sample. <br><br>  There is a natural desire to be able to predict the repayment or non-repayment of a loan by a potential borrower on his profile.  The search for a prediction algorithm is a machine learning task. <br><br>  There are many examples of machine learning tasks.  In this article we will talk more about the task of classifying texts. </p><br><h1 id="postanovka-zadachi">  Formulation of the problem </h1><br><p>  Recall that we are developing <a href="https://www.okdesk.ru/">Okdesk</a> - a cloud service for customer service.  Companies that use Okdesk in their work accept client applications via different channels: client portal, email, web forms from the site, instant messengers, and so on.  The application may fall into one category or another.  Depending on the category, the application may have one or another performer.  For example, applications for 1C should be sent to a decision to 1C specialists, and applications related to the work of an office network should be sent to a group of system administrators. <br><br>  To classify the flow of requests, you can select the dispatcher.  But, first, it costs money (salary, taxes, office rent).  And secondly, time will be spent on the classification and routing of the application and the application will be solved later.  If you could classify an application by its content automatically - it would be great!  Let's try to solve this problem by machine learning (and one IT manager). <br><br>  For the experiment, a sample of 1200 applications was taken with affixed categories.  In the sample, applications are divided into 14 categories.  The purpose of the experiment: to develop a mechanism for automatic classification of applications according to their content, which will give several times better quality than random.  According to the results of the experiment, it is necessary to make a decision on the development of the algorithm and the development of an industrial service on its basis for the classification of applications. </p><br><h1 id="instrumentariy">  Tools </h1><br><p>  The experiment was carried out using a Lenovo laptop (core i7, 8GB of RAM), the Python 2.7 programming language with the NumPy, Pandas, Scikit-learn, re libraries and the <a href="https://ru.wikipedia.org/wiki/IPython">IPython</a> shell.  I will write more about libraries used: </p><br><ol><li>  <strong>NumPy</strong> is a library containing a variety of useful methods and classes for conducting arithmetic operations with large multidimensional numeric arrays; </li><li>  <strong>Pandas</strong> is a library that allows you to easily and easily analyze and visualize data and conduct operations on them.  The main data structures (object types) are the <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html">Series</a> (one-dimensional structure) and the <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">DataFrame</a> (two-dimensional structure; in fact, the Series is of the same length). </li><li>  <strong>Scikit-learn</strong> is a library that implements most of the methods of machine learning; </li><li>  <strong>Re</strong> - library of regular expressions.  <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2583%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D1%258B%25D0%25B5_%25D0%25B2%25D1%258B%25D1%2580%25D0%25B0%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F">Regular expressions</a> are an indispensable tool in tasks related to text analysis. </li></ol><br><p>  From the Scikit-learn library, we will need some modules, the purpose of which I will write in the course of the presentation of the material.  So, we import all the necessary libraries and modules: </p><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> re <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> neighbors, model_selection, ensemble <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.grid_search <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> GridSearchCV <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> accuracy_score</code> </pre> <br><p>  And we proceed to the preparation of data. <br><br>  <em>(the construction import xxx as yy means that we include the xxx library, but in the code we will access it through yy)</em> </p><br><h1 id="podgotovka-dannyh">  Data preparation </h1><br><p>  When solving the first real (and not laboratory) tasks related to machine learning, you will discover that most of the time is spent not on learning the algorithm (choosing an algorithm, selecting parameters, comparing the quality of different algorithms, etc.).  The lion's share of resources will be spent on collecting, analyzing and preparing data. <br><br>  There are various techniques, methods, and recommendations for preparing data for different classes of machine learning tasks.  But most experts call data preparation art rather than science.  There is even such an expression - feature engineering (i.e., construction of parameters describing objects). <br><br>  In the task of classifying texts, the object has one text - text.  You cannot feed it to the machine learning algorithm (I admit that I do not know everything :).  The text needs to be digitized and formalized. <br><br>  As part of the experiment, primitive methods of text formalization were used (but even they showed a good result).  We will discuss this further. </p><br><h2 id="zagruzka-dannyh">  Data loading </h2><br><p>  Recall that as the source data we have unloading 1200 applications (distributed unevenly in 14 categories).  For each application there is a "Subject" field, a "Description" field and a "Category" field.  The "Subject" field is the abbreviated content of the application and it is mandatory, the "Description" field is an extended description and it may be empty. <br><br>  The data is loaded from the .xlsx file into the DataFrame.  There are a lot of columns in the .xlsx file (parameters of real requests), but we need only the "Subject", "Description" and "Category". <br><br>  After downloading the data, we combine the "Subject" and "Description" fields into one field for the convenience of further processing.  To do this, you must first fill in all the empty Description fields (with an empty string, for example). </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   issues  DataFrame issues = pd.DataFrame() #   issues  Theme, Description  Cat,     ,     .xlsx . u'...' ‚Äî   '‚Ä¶'  utf issues[['Theme', 'Description','Cat']] = pd.read_excel('issues.xlsx')[[u'', u'', u'']] #     Description   issues.Description.fillna('', inplace = True) #   Theme  Description (    )   Content issues['Content'] = issues.Theme + ' ' + issues.Description</span></span></code> </pre> <br><p>  So, we have a variable of DataFrame type, in which we will work with the columns Content (combined field for the "Subject" and "Description" fields) and Cat (category of the application).  Let us proceed to the formalization of the content of the application (ie, the Content column). </p><br><h2 id="formalizaciya-soderzhaniya-zayavok">  Formalization of the content of applications </h2><br><h3 id="opisanie-podhoda-k-formalizacii">  Description of the approach to formalization </h3><br><p>  As mentioned above, the first step is to formalize the text of the application.  The formalization will be carried out as follows: </p><br><ol><li>  We divide the content of the application into words.  By a word we mean a sequence of 2 or more characters, separated by delimiting characters (dashes, hyphens, full stops, spaces, new lines, etc.).  As a result, for each application we get an array of words contained in its content. </li><li>  From each application, we exclude ‚Äúparasite words‚Äù that do not carry meaning (for example, words included in greeting phrases: ‚Äúhello‚Äù, ‚Äúgood‚Äù, ‚Äúday‚Äù, etc.); </li><li>  From the resulting arrays we compose a dictionary: a set of words used to write the content of all applications; </li><li>  Next, we compose a matrix of size <em>(number of applications)</em> x <em>(the number of words in the dictionary)</em> , in which the i-th cell in the j-th column corresponds to the number of entries in the i-th application of the j-th word from the dictionary. </li></ol><br><p>  The matrix of paragraph 4 is a formalized description of the content of applications.  Speaking in a mathematized language, each row of the matrix is ‚Äã‚Äãthe coordinates of the vector of the corresponding application in the dictionary space.  For learning the algorithm, we will use the resulting matrix. <br><br>  <strong>Important point</strong> : p.3 is carried out after we select a random subsample from the training sample for the quality control of the algorithm (test sample).  This is necessary in order to better understand what quality the algorithm will show "in battle" on new data (for example, it is not difficult to implement an algorithm that will give ideally correct answers on the training set, but random data will not work better on any new data : this situation is called retraining).  The separation of the test sample before the compilation of the dictionary is important because if we had compiled a dictionary including on the test data, then the algorithm trained on the sample would turn out to be already familiar with unknown objects.  Conclusions about its quality on unknown data will be incorrect. <br><br>  Now we will show how p.p.  1-4 look in the code. </p><br><h3 id="razbivaem-soderzhanie-na-slova-i-ubiraem-slova-parazity">  We break the content into words and remove the word parasites </h3><br><p>  First of all, we will bring all the texts to lower case (‚Äúprinter‚Äù and ‚ÄúPrinter‚Äù are the same words only for a person, not for a machine): </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#       def lower(str): return str.lower() #      Content issues['Content'] = issues.Content.apply(lower)</span></span></code> </pre> <br><p>  Next, we define an auxiliary dictionary of ‚Äúwords-parasites‚Äù (its content was made by experimentally iteratively for a specific sample of applications): </p><br><pre> <code class="python hljs">garbagelist = [<span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>,<span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>, <span class="hljs-string"><span class="hljs-string">u''</span></span>]</code> </pre> <br><p>  We declare a function that splits the text of each application into words of 2 or more characters and then includes the words obtained, with the exception of ‚Äúparasite words‚Äù, into an array: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">splitstring</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(str)</span></span></span><span class="hljs-function">:</span></span> words = [] <span class="hljs-comment"><span class="hljs-comment">#     [] for i in re.split('[;,.,\n,\s,:,-,+,(,),=,/,¬´,¬ª,@,\d,!,?,"]',str): #  ""  2    if len(i) &gt; 1: #  - if i in garbagelist: None else: words.append(i) return words</span></span></code> </pre> <br><p>  For splitting text into words into separator characters, the re regular expression library and its split method are used.  The split method is passed an array of delimiter characters (the delimiter character set is replenished iteratively and experimentally) and the string to be split. <br><br>  We apply the declared function to each application.  At the output, we obtain the original DataFrame, in which a new column appeared, Words with an array of words (with the exception of ‚Äúparasite words‚Äù), of which each application consists. </p><br><pre> <code class="python hljs">issues[<span class="hljs-string"><span class="hljs-string">'Words'</span></span>] = issues.Content.apply(splitstring)</code> </pre> <br><h3 id="sostavlyaem-slovar">  Compiling a dictionary </h3><br><p>  Now we will start to compile a dictionary of words included in the content of all applications.  But before this, as was written above, we divide the training sample into a control one (also known as ‚Äútest‚Äù, ‚Äúdelayed‚Äù) and the one on which we will train the algorithm. <br><br>  The sampling is carried out using the <strong>train_test_split</strong> method of the <strong>model</strong> <strong>model module_selection of</strong> the <strong>Scikit-learn</strong> library.  In the method we pass an array with data (application texts), an array with labels (categories of applications) and a test sample size (usually 30% of the total is selected).  At the output we get 4 objects: data for training, tags for training, data for control and tags for control: </p><br><pre> <code class="python hljs">issues_train, issues_test, labels_train, labels_test = model_selection.train_test_split(issues.Words, issues.Cat, test_size = <span class="hljs-number"><span class="hljs-number">0.3</span></span>)</code> </pre> <br><p>  Now we will declare a function that will make a dictionary of the data left for training ( <strong>issues_train</strong> ), and apply the function in this data: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">WordsDic</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(dataset)</span></span></span><span class="hljs-function">:</span></span> WD = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> dataset.index: <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(len(dataset[i])): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> dataset[i][j] <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> WD: <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: WD.append(dataset[i][j]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> WD <span class="hljs-comment"><span class="hljs-comment">#    words = WordsDic(issues_train)</span></span></code> </pre> <br><p>  So, we have compiled a dictionary of words that make up the texts of all applications from the training set (with the exception of applications left under control).  The dictionary was recorded in the variable words.  The size of the array of words turned out to be 12015th elements (i.e. words). </p><br><h3 id="perevodim-soderzhanie-zayavok-v-prostranstvo-slovarya">  We translate the content of applications into the dictionary space </h3><br><p>  We proceed to the final step of preparing data for training.  Namely: we compose a matrix of size <em>(number of applications in the sample)</em> x <em>(the number of words in the dictionary)</em> , where the i-th row of the j-th column contains the number of occurrences of the j-th word from the dictionary in the i-th request from the sample. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   len(issues_train)  len(words),    train_matrix = np.zeros((len(issues_train),len(words))) # ,   [i][j]   j-   words  i-    for i in xrange(train_matrix.shape[0]): for j in issues_train[issues_train.index[i]]: if j in words: train_matrix[i][words.index(j)]+=1</span></span></code> </pre> <br><p>  Now we have everything necessary for learning: the <strong>train_matrix</strong> matrix (the formalized content of applications in the form of coordinates of vectors, corresponding to applications, in the dictionary space compiled for all applications) and <strong>labels_train</strong> labels (categories of applications from the sample left for training). </p><br><h1 id="obuchenie">  Training </h1><br><p>  We now turn to learning algorithms on tagged data (that is, data for which the correct answers are known: the <em>train_matrix</em> matrix with labels <em>labels_train</em> ).  In this section there will be little code, since most of the methods of machine learning are implemented in the Scikit-learn library.  Developing your own methods can be useful for mastering the material, but from a practical point of view there is no need for this. <br><br>  Below I will try to explain in simple language the principles of specific methods of machine learning. </p><br><h2 id="o-principah-vybora-luchshego-algoritma">  On the principles of choosing the best algorithm </h2><br><p>  You never know which machine learning algorithm will give the best result on specific data.  But, understanding the task, you can determine the set of the most appropriate algorithms so as not to go through all the existing ones.  The choice of the machine learning algorithm that will be used to solve the problem is carried out through a comparison of the quality of the algorithms on the training set. <br><br>  What counts as the quality of the algorithm depends on the task you are solving.  The choice of quality metrics is a separate large topic.  As part of the classification of applications, a simple metric was chosen: accuracy (accuracy).  Accuracy is defined as the proportion of objects in the sample for which the algorithm gave the correct answer (set the correct category of the application).  Thus, we will choose the algorithm that will give greater accuracy in predicting the categories of applications. <br><br>  It is important to say about such a concept as the hyperparameter of the algorithm.  Machine learning algorithms have external (ie, those that cannot be derived analytically from a training set) parameters that determine the quality of their work.  For example, in algorithms where you need to calculate the distance between objects, distance can be understood as different things: the <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D1%2581%25D1%2582%25D0%25BE%25D1%258F%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25B3%25D0%25BE%25D1%2580%25D0%25BE%25D0%25B4%25D1%2581%25D0%25BA%25D0%25B8%25D1%2585_%25D0%25BA%25D0%25B2%25D0%25B0%25D1%2580%25D1%2582%25D0%25B0%25D0%25BB%25D0%25BE%25D0%25B2">Manhattan distance</a> , the classical <a href="https://ru.wikipedia.org/wiki/%25D0%2595%25D0%25B2%25D0%25BA%25D0%25BB%25D0%25B8%25D0%25B4%25D0%25BE%25D0%25B2%25D0%25B0_%25D0%25BC%25D0%25B5%25D1%2582%25D1%2580%25D0%25B8%25D0%25BA%25D0%25B0">Euclidean metric</a> , etc. <br><br>  Each machine learning algorithm has its own set of hyperparameters.  The choice of the best values ‚Äã‚Äãof the hyperparameters is carried out, oddly enough, by an exhaustive search: the quality of the algorithm is calculated for each combination of parameter values ‚Äã‚Äãand then the best combination of values ‚Äã‚Äãis used for this algorithm.  The process is costly in terms of computing, but where to go. <br><br>  Cross-validation is used to determine the quality of the algorithm for each combination of hyperparameters.  I will explain what it is.  The training set is divided into N equal parts.  The algorithm is sequentially trained on a subsample of N-1 parts, and the quality is considered to be one delayed.  As a result, each of the N parts is used 1 time to calculate the quality and N-1 time to train the algorithm.  The quality of the algorithm on the combination of parameters is calculated as the average between the quality values ‚Äã‚Äãobtained during cross-validation.  Cross-validation is necessary in order for us to trust the obtained quality value more (by averaging, we level possible ‚Äúdistortions‚Äù of a particular sample break).  A little more <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">you know where</a> . <br><br><img src="https://habrastorage.org/web/570/99b/121/57099b12131149ce942b11acc4e83a31.png" alt="Wikipedia image that illustrates the concept of cross-validation"><br><br>  So, to choose the best algorithm for each algorithm: </p><br><ol><li>  All possible combinations of hyperparameter values ‚Äã‚Äãare searched (each algorithm has its own set of hyperparameters and their values); </li><li>  For each combination of hyperparameter values ‚Äã‚Äãusing cross-validation, the quality of the algorithm is calculated; </li><li>  The algorithm is selected with the combination of values ‚Äã‚Äãof the hyperparameters, which shows the best quality. </li></ol><br><p>  From the point of view of programming of the algorithm described above, there is nothing complicated.  But in this there is no need.  In the <em>Scikit-learn</em> library there is a ready-made method for selecting parameters according to the grid (the <em>GridSearchCV</em> method of the <em>grid_search</em> module).  All that is needed is to transfer the algorithm, the parameter grid and the number N to the method (the number of parts into which we divide the sample for cross-validation; they are also called "folds"). <br><br>  In the framework of solving the problem, 2 algorithms were chosen: k nearest neighbors and the composition of random trees.  About each of them the story further. </p><br><h2 id="k-blizhayshih-sosedey-knn">  k nearest neighbors (kNN) </h2><br><p>  The k-nearest-neighbor method is the easiest to understand.  It consists in the following. <br><br>  There is a training set, the data in which are already formalized (prepared for training).  That is, objects are represented as vectors of some space.  In our case, applications are presented in the form of vectors of the dictionary space.  For each vector of the training set, the correct answer is known. <br><br>  For each new object, pairwise distances between this object and the objects of the training sample are calculated.  Then, the k nearest objects are taken from the training set, and for the new object, the answer that prevails in the subsample of k nearest objects is returned (for tasks where you need to predict the number, you can take the average value of the k closest ones). </p><br><p><img src="https://habrastorage.org/web/68d/a45/6f0/68da456f00f8434e87628dbe7e3f54a7.png" alt="Illustration for the k nearest neighbors method"></p><br><p>  The algorithm can be developed: to give more weight to the value of the label on a closer object.  But for the task of classifying applications we will not do this. <br><br>  In our problem, the hyperparameters of the algorithm are the number k (since our nearest neighbors will draw a conclusion) and the determination of the distance.  The number of neighbors is enumerated in the range from 1 to 7, the determination of the distance is chosen from the Manhattan distance (the sum of the modules of the coordinate difference) and the Euclidean metric (the root of the sum of the squares of the coordinate difference). <br><br>  We execute a simple code: </p><br><pre> <code class="python hljs">%%time <span class="hljs-comment"><span class="hljs-comment">#     param_grid = {'n_neighbors': np.arange(1,8), 'p': [1,2]} #  fold-  - cv = 3 #  estimator_kNN = neighbors.KNeighborsClassifier() # ,    fold-       optimazer_kNN = GridSearchCV(estimator_kNN, param_grid, cv = cv) #        optimazer_kNN.fit(train_matrix, labels_train) #         print optimazer_kNN.best_score_ print optimazer_kNN.best_params_</span></span></code> </pre> <br><p>  After 2 minutes and 40 seconds, we learn that the best quality at 53.23% shows the algorithm on the 3 nearest neighbors, determined from the Manhattan distance. </p><br><h2 id="kompoziciya-sluchaynyh-derevev">  Arrangement of random trees </h2><br><h3 id="reshayuschie-derevya">  Decisive trees </h3><br><p>  Decisive trees are another machine learning algorithm.  The learning algorithm is a step-by-step partition of the training sample into parts (most often into two, but in general it is not necessary) according to some feature.  Here is a simple example illustrating the work of the decision tree: <br><img src="https://habrastorage.org/web/60b/e80/0e2/60be800e29434dc6a52e5b8ae982bf17.png"><br>  The decision tree has internal vertices (at which decisions are made on the further partitioning of the sample) and final vertices (sheets), which are used to predict the objects that went there. <br><br>  In the decision vertex, a simple condition is checked: the conformity of some (about this later) j object attribute to the condition x <sup>j is</sup> greater than or equal to some t.  Objects that satisfy the condition are sent to one branch, and not satisfying - to another. <br><br>  When learning the algorithm, it would be possible to split the training sample until all the nodes have one object at a time.  This approach will give an excellent result on the training sample, but on the unknown data you will get a ‚Äúhat‚Äù.  Therefore, it is important to define the so-called ‚Äústopping criterion‚Äù - the condition under which the vertex becomes a leaf and the further branching of this vertex stops.  The stopping criterion depends on the task, here are some types of criteria: the minimum number of objects at the vertex and the limit on the depth of the tree.  In solving the problem, we used the criterion of the minimum number of objects at the vertex.  A number equal to the minimum number of objects is an algorithm hyperparameter. <br><br>  New (requiring prediction) objects are driven along a trained tree and fall into the corresponding sheet.  For objects on the list, we give the following answer: </p><br><ol><li>  For classification tasks, we return the most common class of objects of the training set in this sheet; </li><li>  For regression tasks (i.e., those where the answer is a number) we return the average value on the objects of the training sample from this sheet. </li></ol><br><p>  It remains to talk about how to select for each vertex the attribute j (according to which criterion we divide the sample at a particular vertex of the tree) and the threshold t corresponding to this attribute.  For this purpose, the so-called error criterion Q (X <sub>m</sub> , j, t) is introduced.  As you can see, the error criterion depends on the sample X <sub>m</sub> (that part of the training sample that reached the vertex under consideration), the parameter j, which will be used to split the sample X <sub>m</sub> in the vertex under consideration and the threshold value t.  It is necessary to choose j and t such that the error criterion will be minimal.  Since the possible set of values ‚Äã‚Äãof j and t for each training sample is limited, the problem is solved by enumeration. <br><br>  What is the error criterion?  The draft version of the article on this site contained many formulas and accompanying explanations, a story about the informativeness criterion and its particular cases (the Ginny criterion and the entropy criterion).  But the article turned out and so bloated.  Those who want to understand the formalities and mathematics can read about everything on the Internet (for example, <a href="http://www.machinelearning.ru/wiki/images/8/89/Sem3_trees.pdf">here</a> ).  Limited to the "physical meaning" on the fingers.  The error criterion shows the level of "diversity" of objects in the resulting subsamples.  By "diversity" in classification problems we mean a variety of classes, and in regression problems (where we predict numbers), dispersion.  Thus, when dividing the sample, we want to maximally reduce the ‚Äúdiversity‚Äù in the resulting subsamples. <br><br>  With trees sorted out.  We turn to the composition of trees. </p><br><h3 id="kompoziciya-reshayuschih-derevev">  The composition of the decisive trees </h3><br><p>  Decisive trees can reveal very complex patterns in the training set.         ,         ‚Äî  .            (). <br><br>      N      ‚Äú‚Äù.    (,   )     ,     ‚Äî   ( N )  . <br><br>  ,       N  .           :   N           .       :             /     (..          ,      ).          ‚Äî         (..           ,       ; ,         ‚Äî ).      <strong></strong> . <br><br>  -  !         2- :      (  )     (  ). <br><br>  : </p><br><pre> <code class="python hljs">%%time <span class="hljs-comment"><span class="hljs-comment">#     param_grid = {'n_estimators': np.arange(20,101,10), 'min_samples_split': np.arange(4,11, 1)} #  fold-  - cv = 3 #  estimator_tree = ensemble.RandomForestClassifier() # ,    fold-       optimazer_tree = GridSearchCV(estimator_tree, param_grid, cv = cv) #        optimazer_tree.fit(train_matrix, labels_train) #         print optimazer_tree.best_score_ print optimazer_tree.best_params_</span></span></code> </pre> <br><p>  3  30  ,     65,82%    60 ,         4. </p><br><h1 id="rezultat">  Result </h1><br><h2 id="rezultaty-na-otlozhennoy-vyborke">     </h2><br><p>         (,  ‚Äî     ) . <br><br>     test_matrix,        ,     (..    ,   train_matrix,    ). </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#   len(issues_test)  len(words) test_matrix = np.zeros((len(issues_test),len(words))) # ,   [i][j]   j-   words  i-    for i in xrange(test_matrix.shape[0]): for j in issues_test[issues_test.index[i]]: if j in words: test_matrix[i][words.index(j)]+=1</span></span></code> </pre> <br><p>       accuracy_score  metrics  Scikit-learn.                 : </p><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">u' :'</span></span>, accuracy_score(optimazer_tree.best_estimator_.predict(test_matrix), labels_test) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">u'kNN:'</span></span>, accuracy_score(optimazer_kNN.best_estimator_.predict(test_matrix), labels_test)</code> </pre> <br><p>  51,39%   k    73,46%    . </p><br><h2 id="sravnenie-s-glupymi-algoritmami">   ""  </h2><br><p>     , ‚Äú ‚Äù ,  random.       ‚Äú‚Äù ,     random.   , ‚Äú‚Äù    ,     -    . <br><br>     3-  ‚Äú‚Äù : </p><br><ol><li>  random; </li><li>        ; </li><li> Random,      . </li></ol><br><p>  random  14    100/14 * 100% = 7,14% . ,          14,5% (        ).      random-   .            ,   random-: </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#  random import random # ,    random-  rand_ans = [] # for i in xrange(test_matrix.shape[0]): rand_ans.append(labels_train[labels_train.index[random.randint(0,len(labels_train))]]) #  print u' random:', accuracy_score(rand_ans, labels_test)</span></span></code> </pre> <br><p>  14,52%. <br><br> ,            ,   ‚Äú‚Äù .  Hooray! </p><br><h1 id="chto-dalshe">  What's next? </h1><br><p>        ,      90% ‚Äî     .   ,      .            ‚Äú‚Äù     <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2582%25D0%25B5%25D0%25BC%25D0%25BC%25D0%25B8%25D0%25BD%25D0%25B3"></a>        (      ,        ).  -,            (: " <em></em> ", " <em></em> ", " <em></em> "  ..) ‚Äî      (  )    . <br><br>  ,      : .       " "  "",         .          .    ,     ,        ,     . <br><br>  ,          <a href="https://www.okdesk.ru/">Okdesk</a> . <br><br> "!  !"  (with) </p><br><p>  <a href="https://habrahabr.ru/company/okdesk/blog/342796/">    ( 2).      . </a> </p></div><p>Source: <a href="https://habr.com/ru/post/337278/">https://habr.com/ru/post/337278/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../337264/index.html">Poker AI: how to teach algorithms to bluff</a></li>
<li><a href="../337268/index.html">The largest manufacturer of avionics, Rockwell Collins will buy for $ 30 billion</a></li>
<li><a href="../337270/index.html">Top 10 internships for IT professionals</a></li>
<li><a href="../337272/index.html">How to create a racist AI, without even trying. Part 2</a></li>
<li><a href="../337274/index.html">Abbreviated Properties</a></li>
<li><a href="../337280/index.html">Software Asset Management at Raiffeisenbank - process and result</a></li>
<li><a href="../337282/index.html">Footer sections</a></li>
<li><a href="../337284/index.html">How to embed svg</a></li>
<li><a href="../337286/index.html">Why do you need BEM</a></li>
<li><a href="../337288/index.html">Using the KOMPAS-3D API ‚Üí Lesson 4 ‚Üí Title bar</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>