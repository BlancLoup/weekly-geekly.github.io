<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to implement almost instant site switching between sites, when one fell</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It happens, sites fall because of a failure of a site of a hoster, channels and so on. I have been working in hosting for 7 years, and I often see suc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to implement almost instant site switching between sites, when one fell</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/9d5/2ab/bd9/9d52abbd91714dc882d08ae75f5ca64f.png" alt="image"><br><br>  It happens, sites fall because of a failure of a site of a hoster, channels and so on.  I have been working in hosting for 7 years, and I often see such problems. <br><br>  A couple of years ago, I realized that the backup site service (without modifying their website or service) is very important to customers.  Theoretically, everything is simple: <br>  1. Have a copy of all the data in another data center. <br>  2. In case of failure, switch the work to backup DC. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In practice, the system experienced 2 complete technical reorganizations (preservation of the main ideas with the change of a significant part of the toolkit), 3 moves to new equipment, 1 move between service providers (moving from a German data center to two Russian ones).  On the study of the behavior of different systems in real conditions under the client load took 2 years. <br><a name="habracut"></a><br>  So, even if the hoster uses a cluster solution to host client VDS - the vast majority of existing cluster solutions are designed to work within a single data center, or within one complex system, the failure of which leads to stopping the entire cluster. <br><br>  <b>Main</b> <br>  1. Replication - from local disks via drbd WITHOUT drbd-proxy <br>  2. Switching between data centers by announcing their own network of IP addresses, without changing them for domains <br>  3. Separate backup (regular copy, not a replica) + monitoring in the third data center <br>  4. HyperVision Based Virtualization (in this case, KVM) <br>  5. Internal L3 network with dynamic routing, IPSEC mGRE OSPF <br>  6. The organization of simple composite components, each of which can be temporarily disabled or replaced. <br>  7. The principle of the lack of "the most reliable system" <br>  8. Fatal point of failure <br><br>  The reason for such decisions will be described below. <br><br>  <b>System requirements</b> <br>  1. Launching the client service without modifications (i.e. launching projects that are not designed for clustering) <br>  2. The ability to continue work after a single equipment failure, up to the shutdown of any one data center. <br>  3. Recovery of client services within 15 minutes after a fatal failure, data loss is allowed 1 minute before the failure. <br>  4. The ability to recover data if any two data centers are lost. <br>  5. Maintenance / replacement of own equipment and service providers without downtime of client services. <br><br>  <b>Data storage</b> <br>  The best choice is the way DRBD works without proxy, the traffic is compressed simultaneously with the encryption using ipsec.  In synchronization mode B and local caching of data inside virtual machines, there is no delay of 10-12 ms (ping between Moscow-St. Petersburg data centers) doesn‚Äôt affect the speed of work, moreover, this delay is only for writing; quickly. <br><br>  Four options were explored: <br>  1, 2. Ceph in rbd (block devices) and CephFS (distributed file system) variants <br>  3. XtremeFS <br>  4. DRBD <br>  5. DRBD with DRBD-Proxy <br><br>  <b><i>Common to Ceph</i></b> <br>  Advantages: <br>  Convenient bandwidth scaling.  Simple addition of capacity, automatic distribution / redistribution of data across servers. <br><br>  Disadvantages: <br>  It works within the same data center, with two distant data centers, you need two clusters and replication between them.  It does not work well on a small number of disks (data integrity checks suspend current disk operations).  Only synchronous replication. <br>  Diagnosing a collapsed cluster is a difficult task.  It is necessary or very long to break and repair this system or have a contract for quick support with developers. <br>  When updating the cluster there is a critical moment - restarting the average in the monitor quorum.  If something goes wrong at that moment, the cluster will stop and then it will have to be assembled manually. <br><br>  <b><i>Ceph fs</i></b> <br>  It was supposed to be used as a single file system for storing all data in LXC / OpenVZ containers. <br>  Advantages: <br>  The ability to create snapshots of the file system.  The size of the file system and individual files may be larger than the size of the local disk.  All data is simultaneously available on all physical servers. <br><br>  Disadvantages: <br>  For each file opening operation, the server should contact the metadata server, check if the file exists, where to find it, etc.  Metadata caching is not provided.  Sites start to work slower and this is noticeable through the eyes.  Local caching is not implemented. <br>  Developers warn that the system is not ready yet and it is better not to store important data in it. <br><br>  <b><i>Ceph rdb</i></b> <br>  Intended use - one block device per container. <br>  Advantages: <br>  Convenient snapshot, image cloning (in the second version format).  The image size may be larger than the local disk.  Local operations are cached by the host / container operating system.  There are no delays for frequently repeated readings of small files. <br><br>  <b><i>XtremeFS</i></b> <br>  Advantages: <br>  Declared support for replication over long distances, incl.  work offline, can support partial replicas. <br><br>  Disadvantages: <br>  When tests proved to be very slow and has not been further investigated.  It feels great for distributed storage of an array of data / documents / discs so that, for example, each office has its own copy and is not intended for actively changing files, such as databases or virtual server images. <br><br>  <b><i>DRBD</i></b> <br>  Advantages: <br>  Replicating block devices.  Reading from local drives.  Ability to work autonomously, without a cluster.  Natural data storage - in case of problems with DRBD, you can connect to the device directly and work with it.  Several modes of synchronicity. <br><br>  Disadvantages: <br>  Each image can be synchronized only between two servers (there is a possibility of replication on 3-4 servers, but when switching the master server, difficulties are foreseen with the distribution of metadata between servers + the throughput is multiplied by a factor). <br>  The size of the device cannot exceed the size of the local disk. <br><br>  <b><i>DRBD with DRBD-Proxy</i></b> <br>  Special paid supplement to DRBD for long distance replication <br><br>  Advantages: <br>  1. Compresses traffic well, 2-10 times relative to work without compression. <br>  2. Large local buffer for accepting write operations and their gradual sending to a remote server without braking operations on the main one (in asynchronous replication mode). <br>  3. Sane, support, quite quick answers at some time (apparently if you get in during working hours) <br><br>  Disadvantages: <br>  Immediately upon launch, I stumbled upon a bug that has already been fixed, but the fix has not yet been published - a separately compiled binary has been sent with the fix. <br>  In tests, he proved to be extremely unstable - the simplest random write test hung a proxy service at high speed so that it was necessary to restart the entire server, and not just the proxy. <br>  From time to time goes into spinlock and stupidly eats all the processor core <br><br>  <b>Switching traffic between data centers</b> <br>  Two options were considered: <br>  1. Switching by changing records on DNS servers <br>  2. By announcing your own network of IP addresses from two data centers <br><br>  Optimally selected announcement of your network through BGP. <br><br>  <b>Change records on DNS servers</b> <br>  Advantages: <br>  Ease of implementation <br>  By pinging, you can understand to which data center traffic arrives. <br><br>  Disadvantages: <br>  Some clients are not ready to delegate their domains to foreign DNS servers. <br>  Long switching times - DNS caching is often more aggressive than specified in TTL, and even with a TTL of 5-15 minutes in an hour, someone will still break into the old server.  And individual scanners - even after a few days. <br>  It is impossible to save the IP address of client servers when moving between data centers. <br>  In the case of semi-loss of communication with the data center, the dns servers may begin to give different ip-addresses and the switching will occur only partially. <br><br>  <b>Announcement of your own network of addresses</b> <br>  Advantages: <br>  Fast guaranteed traffic switching between data centers.  For tests inside Moscow, the change in BGP announcement diverges in a few seconds.  The world may be longer, but still faster and more reliable than the VDS. <br>  It is possible to disable traffic from the half-working data center, the connection with which the system is lost, but which is visible for a part of the Internet. <br><br>  Disadvantages: <br>  Complicated configuration of internal routing.  Switching of individual resources of the system is possible only partial - the traffic will come to the data center that is closer to the client, and leave the data center where the virtual server is running. <br><br>  <b>Backup in the third data center</b> <br>  The situation of data loss in two data centers is quite real, for example, a software error, according to which the data on the main and standby server were deleted simultaneously.  Or a hardware failure on the primary server while the backup is in progress or data is being resynchronized. <br>  For such cases, a server has been installed in the third data center, which is excluded from the overall cluster system.  All he can do is monitor and store backup copies. <br><br>  <b>Virtualization method</b> <br>  Options were considered: <br>  LXC, OpenVZ, KVM, Hyper-V <br><br>  The choice is made in favor of KVM, because  It provides the greatest freedom of action. <br><br>  <b><i>Lxc</i></b> <br>  Advantages: <br>  Easy to install, works on a standard Linux kernel without modifications.  Allows basic container insulation. <br>  No performance loss on virtualization. <br><br>  Disadvantages: <br>  Low level of insulation. <br>  No live migration between servers <br>  Inside the container, only Linux systems can be launched.  Even inside Linux systems there are limitations on the functionality of the kernel modules. <br><br>  <b><i>Openvz</i></b> <br>  Advantages: <br>  No performance loss on virtualization <br>  There is a live migration <br><br>  Disadvantages: <br>  It works on a modified kernel, you need to manually build additional modules and, possibly, have compatibility problems due to non-standard environment for them. <br>  Inside the container only Linux works.  Even inside Linux systems there are limitations on the functionality of the kernel modules. <br><br>  <b><i>KVM</i></b> <br>  Advantages: <br>  Works without modifying the system kernel <br>  There is a live migration <br>  You can connect equipment directly to the virtual machine (disks, usb devices, etc.) <br><br>  Disadvantages: <br>  Performance loss on hardware virtualization <br><br>  <b><i>Hyper-v</i></b> <br>  Advantages: <br>  Good integration with Windows <br>  There is a live migration <br><br>  Disadvantages: <br>  The necessary features from Linux OS are not supported: caching on SSD, replication of local disks, connection, remote connection to the VDS console for the client. <br><br>  <b>Select internal network</b> <br>  The task is to provide an internal network, addressable, independent of the external network and the location of the internal server.  The ability to quickly switch the flow of traffic to the server when its physical location changes (moving VDS).  The possibility of arbitrary routing to each specific server (i.e., the server moves without changing the IP address).  The possibility of organizing a fully connected network between data centers is desirable.  Traffic protection is desirable. <br><br>  Initially, the tinc variant and the fully connected L2 network were used.  This is the easiest to set up and relatively flexible option.  But not always predictable.  After a series of routing experiments, I came to the conclusion that routing at the L3 level is exactly what is needed - predictably, manageably, quickly.  Inside the internal network, dynamic routing through OSPF works, the route is prescribed for each private IP address.  Those.  all routers know which one of them has access to each specific server. <br>  In the case of the L2 network, the tables would be about the same, but less transparent, since  hidden inside the software, not in the standard routing tables of the kernel. <br>  If necessary (in case of problems with OSPF), as the number of routes grows, this system can easily be replaced with completely static route registration through our own services. <br><br>  Considered options: <br>  OpenVPN, L2 tinc, GRE, mGRE <br><br>  The mGRE option is selected.  L2 traffic and multicast are currently not needed.  If necessary, it is possible to add multicast via software on nodes, there is no need for L2 traffic.  The lack of encryption was compensated by setting up IPSEC for traffic between nodes.  IPSEC, however, will also compress it. <br><br>  When setting up in real conditions, an interesting feature emerged - despite the complete disabling of filtering in the data center - its equipment looks inside the GRE protocol and analyzes what is inside there.  This removes packets with OSPF traffic and an additional 2ms delay occurs.  So IPSEC turned out to be needed not only for abstract encryption, but also for the performance of the system in principle. <br>  The data center specialists asked the equipment supplier why such filtering occurs, but have not received an answer yet (now 1-2 months). <br><br>  <b><i>Openvpn</i></b> <br>  Virtues <br>  Already familiar.  It works well.  Able to work in L2 / L3 modes. <br><br>  Disadvantages: <br>  Works on a point-to-point or star connection.  To organize a full mesh network, you will need to support a large number of tunnels. <br><br>  <b><i>Tinc</i></b> <br>  Advantages: <br>  Initially able to organize a fully connected L2 network between an arbitrary number of points.  Easy to set up.  I used it for about 1-2 years in the previous version of the system, before moving servers to Russia. <br><br>  Disadvantages: <br>  Routing uncertainty if a situation arises when two computers with the same MAC appear in the network (for example, split-brain in a cluster).  The delay in determining the change of location of the server when moving about 10-30 seconds. <br>  Driving L2 traffic, but in practice you need L3. <br><br>  <b><i>GRE</i></b> <br>  Advantages: <br>  Works in the core.  Just customizable.  You can drive L2-traffic. <br><br>  Disadvantages: <br>  No encryption <br>  Need to support a large number of interfaces <br><br>  <b><i>mGRE</i></b> <br>  Advantages: <br>  Works in the core.  Just customizable.  A mesh network is created using only one tunnel interface, simple addition of neighbors. <br><br>  Disadvantages: <br>  No encryption.  Does not know how to work with L2-traffic, there is no multicast out of the box. <br><br>  <b>The use of ready-made cluster solutions, the principle of the absence of "the most reliable supplier / hardware / program"</b> <br>  When using reliable cephfs / cephrbd disk storage, I managed to break them so that the development required by developers.  Within several days I received the necessary consultations through the IRC channel and in the course of diagnostics it became clear that it was almost impossible to diagnose and fix such a problem on my own - you need a deep knowledge of the system and a lot of experience with such diagnostics.  In addition, if such a system breaks down, the cluster stops working in principle, which is unacceptable even if there is a support contract.  In addition, contracts for round-the-clock fast support in any such products are very expensive and this will immediately put an end to the mass decision, since  It will not be possible to sell cheaply what you bought for expensive. <br><br>  Similarly, with any supplier of reliability, the insides of which are closed or not studied to the depth of the natural understanding of every detail.  When building a cluster, it was done to ensure that every component of the system in an emergency can be turned off if it stops working as it should and with something to replace, at least temporarily, with the loss of some functionality but while maintaining the work of client services as a whole. <br><br>  DRBD can be turned off, you can delete and connect to LVM volumes directly.  Synchronization between nodes will stop working and live migration will not be possible.  For an emergency, as long as there is a breakdown in drbd (probably metadata or configs or software version rollback), this is acceptable.  When the problem is tightened, replication via rsync, LVM snapshots, lvmsync, etc. is possible. <br><br>  KVM systems are never updated at the same time.  If a single server fails, all client services will continue to work on the backup while repair work is underway.  All client services are removed from the node before starting work. <br><br>  The third data center with monitoring and backup.  If you lose your backups, you can temporarily replace them with LVM snapshots on the main nodes of the system.  If monitoring is lost, the hosting system and all client services will continue to work.  This will break the auto-quench function of broken resources.  At the moment, this is an acceptable compromise.  If necessary, this system can also be duplicated. <br><br>  Internal VPN network with dynamic routing.  If this network breaks, it is possible to move all resources to one data center and work without a VPN network. <br><br>  Public network of IP addresses.  At the moment, this is a compromise and is a single point of failure.  If for some reason the address block stops working (you forgot to pay, the office that issued the block was closed, selected due to the optimization of the address space).  Access to customer resources will be lost.  Here an admission is made to the fact that such things usually do not occur unexpectedly and there will be time to prepare for the loss of the unit.  If a block of addresses is lost unexpectedly, there is a backup option to take a block of addresses from data centers.  The work of client resources can be restored within 1-2 days - basically, this is the time for the data center to reconcile and configure the address block, the work of the cluster itself does not depend on it and only need to update the DNS records of the domains. <br>  In the future, this point of failure can also be eliminated by obtaining a second block of addresses through another company. <br><br>  <b>Data center</b> <br>  In practice, data centers sometimes turn off electricity and the Internet, despite all the backup systems.  Sometimes the most reliable pieces of iron break down and the Internet drops at the same time in several data centers that worked through this piece of hardware, the work of data centers stops the state.  authorities during investigations, etc.  This is something that has been tested on its own experience in Russian and foreign data centers. <br>  In the final version of the hosting, these problems are also taken into account: the main hosting system is located in two independent data centers located in different regions of the Russian Federation: Moscow and St. Petersburg.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data centers use independent communication channels, are not legally interconnected, and have no special approvals / interactions like common routers. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fatal point of failure</font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Such a system allows you to protect the end customer from any single failures in any systems and some multiple failures, except for the existence of the work of the service provider itself. The client inevitably has to take this risk when using someone else's infrastructure, the alternative is to build and maintain its own similar system. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What happened</font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 1. The client comes and says: I do not want to fall.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">2. No adaptation of the project in most cases is needed, but you need to stand on our servers. As a rule, they give us access to the hosting, we do everything ourselves by means of support. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">3. We give the client a test address to access the server. The client checks that everything works as he is used to, fix minor inaccuracies. Hardware virtualization allows you to accurately copy any project along with the existing operating system, programs and settings, without having to repeat the settings on the new server and risk forgetting something on the old one. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">4. Switch without interruption, the longest part - the new IP-addresses. When transferring sites it turns out to make a move with a break of several minutes.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">5. A hosting contract is concluded, an invoice is issued for payment. </font><font style="vertical-align: inherit;">For a small sample project it costs 4500 per month (this includes hosting, support, and duplication). </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. Further, usually nothing falls.</font></font></div><p>Source: <a href="https://habr.com/ru/post/248837/">https://habr.com/ru/post/248837/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../248827/index.html">We get a list of members of the VKontakte community of a certain gender and age</a></li>
<li><a href="../248829/index.html">Modern Business Intelligence (BI) systems by the example of IBM Cognos BI</a></li>
<li><a href="../248831/index.html">Features of the Intel x86 Hi-End Server - HP Superdome X System</a></li>
<li><a href="../248833/index.html">IBM introduced the new z13 mainframe</a></li>
<li><a href="../248835/index.html">PHP Digest number 55 - interesting news, materials and tools (January 11 - 25, 2015)</a></li>
<li><a href="../248839/index.html">Development of software for laser thickness on FriendlyARM Smart210</a></li>
<li><a href="../248841/index.html">What I wanted to know at the beginning of my development career. Part 1</a></li>
<li><a href="../248843/index.html">How to encrypt and hide a hard disk partition using CyberSafe</a></li>
<li><a href="../248845/index.html">PostgreSQL vs MySQL</a></li>
<li><a href="../248847/index.html">Unification and search with return on C #</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>