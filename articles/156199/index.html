<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>BI in Odnoklassniki: data collection and delivery to DWH</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Last time (http://habrahabr.ru/company/odnoklassniki/blog/149391/) we talked about a system of graphs and dashboards that we use to monitor the site a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>BI in Odnoklassniki: data collection and delivery to DWH</h1><div class="post__text post__text-html js-mediator-article">  Last time <a href="http://habrahabr.ru/company/odnoklassniki/blog/149391/">(http://habrahabr.ru/company/odnoklassniki/blog/149391/)</a> we talked about a system of graphs and dashboards that we use to monitor the site and user activity.  We have to log more than two trillion (2,000,000,000,000) events per day.  In this post, we will describe how we collect this data, process it, and load it into the repository. <a name="habracut"></a><br><br><h4>  Nature of the data </h4><br>  In the monitoring system, we collect data that is not tied to a specific user.  We aggregate data by classifiers (one or several per table), which allows to significantly compress their total volume.  At the same time, we practically lose nothing - information about the total activity of all users at a certain point in time is sufficient. <br><br>  We also do not log the exact time of the event, but aggregate data by five minutes.  It also allows you to significantly reduce the amount of data and create graphics, more "light" for perception and display in the browser. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Logger </h4><br>  Almost on each server of our site (and there are more than 4000 of them), in each application, the Logger library is used based on Log4j, which aggregates all the logs in asynchronous mode into tables, five-minutes and classifiers.  Aggregation is carried out in memory, that is, work with the disk does not occur at all. <br><br>  As a result of aggregation, we compress the data 1000 times - instead of 20 billion records in 5 minutes during rush hour we get only 20 million records.  This, to put it mildly, is a very big difference. <br><br>  Every five minutes, the Logger flushes the aggregated data to the intermediate data store, and then deletes it from memory.  Moreover, it is necessary to reset these data in 2 minutes so that they reach the end user as quickly as possible and that there is a reserve of capacity (the maximum time allowed for data reset is 5 minutes, after which the data for the next five minutes will begin to reset). <br><br>  Let's compare the two approaches.  The average record length is 200 bytes. <br>  1) Non-aggregated data <br>  the amount of data per second - 20 000 000 000 (records) * 200 (bytes) / 120 seconds = 33 GB / sec <br>  the amount of data per day - 2 000 000 000 000 (records) * 200 (bytes) = 400 TB / day <br>  2) Aggregated data <br>  the amount of data per second is 20,000,000 (records) * 200 (bytes) / 120 sec = 33 MB / sec <br>  the amount of data per day - 2 000 000 000 (records) * 200 (bytes) = 400 GB / day <br><br>  If we did not aggregate the data, then only for statistics we would need 33 GB * 8 = 264 Gb / s of network capacity. <br><br>  With this approach, we use part of server resources (CPU and memory) to aggregate data, but since this part is very small, this does not really bother us. <br><br><h4>  Intermediate Data Warehouse: Evolution </h4><br>  If there were no intermediate storage, then the share of DWH servers would have 4000 parallel sessions that would write data at a speed of 33 MB per second.  In parallel with this load, it would be impossible to do any data processing. <br><br>  The task of intermediate data storage: <br>  1) quickly save the data from 4000 Logger-applications; <br>  2) quickly send fresh data to DWH-servers; <br>  3) quickly delete old data after it is uploaded to DWH. <br><br>  Although it sounds simple, we didn‚Äôt come right away to the architecture of the intermediate data warehouse that we have now and which suits us. <br><br><h5>  2008 </h5><br>  In 2008, we did not have a DWH yet, but there were 300 servers that wrote their logs into one database with 20 tables.  A separate application generated graphs, referring directly to the data in the same database.  Each table had a clustered index by date and time to ensure fast reading over a specific period. <br><br>  As the number of servers and the amount of data increased, graphics began to be generated more and more slowly.  They could easily load the base so that it stopped accepting new records.  Therefore, we decided to create a separate data warehouse that will download data from this database and efficiently serve queries from business analysis applications. <br><br><h5>  year 2009 </h5><br>  Our new system was pumping data from the database of logs, which we began to call ‚Äúintermediate storage‚Äù.  The data structure was not changed, because it was necessary that the old graphics and Logger continued to work without changes. <br><br>  Data for uploading we filtered by date and time.  At the same time there was a lot of data with the same time, and they appeared asynchronously.  It was necessary to unload data with a "large margin of safety."  Therefore, we added a bigint ID column with auto-increment to each table and created an index on this column.  After that, we could download one record only once. <br><br>  It worked well for a while.  The number of servers has grown to 700. And suddenly it turned out that the base of the logs is ‚Äúnot rubber‚Äù - the disk space ended.  Old logs that were uploaded to DWH had to be deleted, but there was no effective way to do this.  The DELETE command is impossible, because on large volumes it is too heavy and slow operation.  I had to delete the records by re-creating the tables, which led to data loss and inaccessibility of the service. <br><br><h5>  2010 </h5><br>  The number of servers and data volumes grew rapidly.  Old logs in such an inefficient way had to be deleted more and more often.  This turned into a real problem that had to be solved, and preferably in such a way that neither Logger nor DWH had to rewrite. <br><br>  We could do this with the help of views and triggers. <br><br><img src="https://habrastorage.org/storage2/322/6ef/ab9/3226efab9b9c2a2d756b689414f6088b.png" alt="image"><br><br>  For each table, an identical pair was created and renamed, adding the ending _1 and _2.  Created a view with the same name as the original table.  Servers write to this view.  A trigger instead of insert was added to the view. It writes to the table with the end of _1 one week, and the second week - to the table with the end of _2. <br><br>  Switching occurs in two stages.  At first, only the trigger is switched, and the view itself reads data from the previous table.  After DWH has downloaded all the data from the previous table to itself, the view is switched. <br><br>  After that, the data from the old tables is effectively deleted by the truncate table command. <br><br>  Thus, we got a solution that allowed deleting data without downtime and data loss, without requiring a change to the Logger. <br><br>  Over time, the old charting system was no longer used, that is, there was no need for a clustered index by date and time.  Therefore, we deleted it and made the index by ID cluster.  Thanks to this, the system has become even more efficient. <br><br><h5>  2011 </h5><br>  The number of servers has exceeded 3,000. The solution with views and triggers has become very load on the server's CPU with a fairly low disk queue.  It was necessary to reduce the load, but the presence of triggers did not allow it.  We decided that it was necessary to remake Logger, so that one week he would write in tables with _1, and the second - in _2.  So did. <br><br>  Part of the infrastructure was transferred to the new Logger.  The load on the CPU has dropped, but the disk queue has grown to a critical level.  The structure of the base is very simple - there is nothing to improve.  We had to look for another solution. <br><br><h5>  year 2012 </h5><br>  The disk queue was created by MS SQL Log Manager, working with the database log file.  At that moment we had a MS SQL 2005 Standard Edition database.  Experts argued that MS SQL 2008 Log Manager is an order of magnitude better than MS SQL 2005. We decided to check this and switched to MS SQL 2008 Enterprise Edition.  Disk queue really dropped by an order! <br><br>  At the same time, we decided to use partitioning tables to delete data.  That is, they returned to the 2009 model, but only the cluster index by ID was divided into partitions of 20,000,000 records.  After the partition is 100% full and all DWH servers have downloaded data, these partitions are deleted. <br><br>  Partitions in SQL servers are ‚Äúinvisible‚Äù to developers, and this solution does not complicate the code.  Deleting and adding partitions occurs online, no loss of connections and data. <br><br>  The result is the following architecture: <br><br><img src="https://habrastorage.org/storage2/0bb/f49/2ea/0bbf492eaa9e06e8aee3dd048894f059.png"><br><br>  Intermediate storage in the schema called Logs bases.  All of them are identical in structure and contain a complete set of possible tables.  All aggregated data is dropped into these tables once every 5 minutes.  Site administrators configure Logger for each installed application so that the load is evenly distributed across all Logs databases.  In this case, the information is not duplicated, that is, the same information can only get into one of the Logs databases. <br><br>  Initially, we had one such base, but now we have several of them.  The reason is as follows: <br>  1) in each large hosting there should be one base of Logs so that in case of problems with hosting we do not lose all the statistics; <br>  2) Logs bases no longer cope with the tasks as the collected data grows. <br><br><h4>  Downloading data from the intermediate data storage </h4><br>  In parallel with the record in the Logs database, data is being downloaded from them into the DWH bases. <br><br><img src="https://habrastorage.org/storage2/103/846/f8b/103846f8b9bcfa2e1507c97d0a0c330d.png"><br><br>  A DWH server in one thread polls each table in all Logs databases and downloads all new records that appeared after a previous download. <br><br>  We calculate how fast the data should be uploaded to fit in 5 minutes.  The number of polled tables is 300 * 4 = 1200. It turns out that, on average, 300/1200 = 0.25 seconds is allocated to one table.  What to do if on average a table takes longer?  To scale!  We put another DWH server and unload half of the tables into one server, and the second half into the second server. <br><br><img src="https://habrastorage.org/storage2/d53/a68/fbf/d53a68fbfe6747c6c9e1e718b98f0d2d.png"><br><br>  If there are more tables or data and again it‚Äôs impossible to keep within 2 minutes, then we put another DWH server, etc.  Now we have 3 such DWH servers. <br><br><h4>  In the next article </h4><br>  In the next article we will talk about how we built data loading in DWH servers, and what is spent on average 0.25 seconds per table. </div><p>Source: <a href="https://habr.com/ru/post/156199/">https://habr.com/ru/post/156199/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../156185/index.html">Elementary social share-buttons</a></li>
<li><a href="../156187/index.html">Machine translation and automatic dictionary in Yandex</a></li>
<li><a href="../156189/index.html">Panoramic photo from the stratosphere taken by GoPro</a></li>
<li><a href="../156193/index.html">Windows Upgrade Offer started distributing promo codes</a></li>
<li><a href="../156197/index.html">Background-position-XY support definition</a></li>
<li><a href="../156201/index.html">Impact of the portal on corporate culture</a></li>
<li><a href="../156203/index.html">Why Microsoft will succeed. Impressions of the presentation of Win8 and Surface</a></li>
<li><a href="../156205/index.html">Ready solutions for English localization</a></li>
<li><a href="../156207/index.html">Installation and initial configuration of Check Point R75</a></li>
<li><a href="../156209/index.html">What version control system are you using? (in real work, most of all)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>