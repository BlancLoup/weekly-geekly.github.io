<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Predictions from mathematicians. We analyze the main methods of detection of anomalies</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Abroad, the use of artificial intelligence in the industry for the predictive maintenance of various systems is becoming increasingly popular. The pur...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Predictions from mathematicians. We analyze the main methods of detection of anomalies</h1><div class="post__text post__text-html js-mediator-article">  Abroad, the use of artificial intelligence in the industry for the <a href="https://en.wikipedia.org/wiki/Predictive_maintenance">predictive maintenance of</a> various systems is becoming increasingly popular.  The purpose of this technique is to identify problems in the operation of the system during the operation phase before it fails for timely response. <br><br>  How popular is this approach in our country and in the West?  The conclusion can be made, for example, under articles on Habr√© and in Medium.  On Habr√© there are almost no articles on solving problems of predictive maintenance.  On the Medium there is a whole set.  Here, <a href="https://medium.com/sentenai/predictive-maintenance-why-its-important-and-how-to-implement-it-a153c43dd696">here</a> and <a href="https://medium.com/%40epicsystemsinc/3-ways-predictive-maintenance-saves-you-money-in-the-long-run-fd453f9f9b9c">here</a> it <a href="https://medium.com/%40epicsystemsinc/3-ways-predictive-maintenance-saves-you-money-in-the-long-run-fd453f9f9b9c">is</a> well described, what are the goals and advantages of such an approach. <br><br>  From this article you will learn: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  why do we need this technique </li><li>  which machine learning approaches are used more often for predictive maintenance, </li><li>  as I tried one of the techniques on a simple example. </li></ul><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/845/2a4/888/8452a4888db8d633dd14d426f6b80cbe.jpg"></div>  <a href="https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32"><i>A source</i></a> <br><a name="habracut"></a><br>  What opportunities gives predictive service: <br><br><ul><li>  controlled process of repair works, which are performed as needed, thus saving money, and without haste, which improves the quality of these works; </li><li>  identification of a specific equipment malfunction (the ability to purchase a specific part for replacement while the equipment is running offers tremendous advantages); </li><li>  optimization of equipment operation, loads, etc .; </li><li>  reducing the cost of regular equipment shutdown. </li></ul><br>  In the <a href="https://medium.com/bigdatarepublic/machine-learning-for-predictive-maintenance-where-to-start-5f3b7586acfb">next article on Medium</a> , the questions that need to be answered are well described in order to understand how to approach this question in a particular case. <br><br>  When collecting data or choosing data for building a model, it is important to answer three groups of questions: <br><br><ol><li>  Are all system problems predictable?  Which predict is especially important? </li><li>  What is the failure process?  Does the system stop working completely or does the mode of operation only change?  Is this a quick process, instantaneous or gradual degradation? </li><li>  Is the system performance sufficiently reflecting its performance?  Do they apply to individual parts of the system or to the system as a whole? </li></ol><br>  It is also important to understand in advance what you want to predict and what is possible to predict and what is not. <br><br>  The article on Medium also lists questions that will help determine your specific goal: <br><br><ul><li>  What to predict?  Remaining lifetime, abnormal behavior or not, the likelihood of failing over the next N hours / days / weeks? </li><li>  Is there sufficient historical data? </li><li>  Whether it is known when the system produced anomalous readings, and when it did not.  Is it possible to mark such indications? </li><li>  How far forward should the model see?  How independent are the indications, reflecting the work of the system, in the interval of hours / day / week </li><li>  What needs to be optimized?  Should the model catch as many violations as possible while giving a false alarm, or is it enough to catch several events without false alarms. </li></ul><br>  There is hope that the situation in the future will be corrected.  So far, there are difficulties in the field of predictive maintenance: there are few examples of system malfunction, or there are enough time instants of system malfunction, but they are not marked;  The failure process is unknown. <br><br>  The main way to overcome difficulties in predictive service is to use <b>anomaly search methods</b> .  Such algorithms do not require markup for training.  For testing and debugging algorithms, markup is necessary in one form or another.  Such methods are limited by the fact that they will not predict a specific breakdown, but only signal an abnormality of the indicators. <br><br>  But this is not bad. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/847/46c/6d7/84746c6d7414722b0a1b7b322000f201.jpg"></div>  <a href="https://robohub.org/a-mathematical-approach-to-robot-ethics/"><i>A source</i></a> <br><br><h2>  Methods </h2><br>  Now I want to talk about some of the features of anomaly detection approaches, and then together we will test the capabilities of some simple algorithms in practice. <br><br>  Although for a particular situation, testing of several anomaly search algorithms and choosing the best one will be required, it is possible to determine some advantages and disadvantages of the main techniques used in this area. <br><br>  First of all, it is important to understand in advance what percentage of anomalies are in the data. <br><br>  If we are talking about the variation of the semi-supervised approach (we are learning only on ‚Äúnormal‚Äù data, and working (testing) then on data with anomalies), then the most optimal will be the choice of <b>the one-class support vector method ( <a href="http://rvlasveld.github.io/blog/2013/07/12/introduction-to-one-class-support-vector-machines/">One-Class SVM</a> )</b> .  When using radial basis functions as a core, this algorithm builds a nonlinear surface around the origin.  The cleaner the training data, the better it works. <br><br>  In other cases, the need to know the ratio of anomalous and "normal" points also remains - to determine the cut-off threshold. <br><br>  If the number of anomalies in the data is more than 5%, and they are quite well separated from the main sample, you can use the standard methods of searching for anomalies. <br><br>  In this case, the most stable in terms of quality is <b>the isolation forest method ( <a href="https://towardsdatascience.com/outlier-detection-with-isolation-forest-3d190448d45e">isolation forest</a> )</b> : the data are split randomly.  A more characteristic reading is more likely to get deeper, while unusual indicators will be separated from the rest of the sample in the first iterations. <br><br>  The rest of the algorithms work better if they ‚Äúfit‚Äù the specifics of the data. <br><br>  When the data has a normal distribution, the <b>Elliptic envelope method</b> is suitable, approximating the data with a multidimensional normal distribution.  The less likely that the point belongs to the distribution, the greater the likelihood that it is anomalous. <br><br>  If the data is presented in such a way that the relative position of different points reflects their differences well, then metric methods are good choices: for example, <b>k nearest neighbors, kth nearest neighbor, ABOD (angle-based outlier detection) or LOF (local outlier factor) )</b> . <br><br>  All these methods assume that the ‚Äúright‚Äù indicators are concentrated in one area of ‚Äã‚Äãmultidimensional space.  If among the k (or kth) nearest neighbors everything is far from the target, then the point is an anomaly.  For ABOD, the reasoning is similar: if all k nearest points are in the same sector of the space relative to the considered one, then the point is an anomaly.  For LOF: if the local density (predetermined for each point by k nearest neighbors) is lower than that of k nearest neighbors, then the point is an anomaly. <br><br>  If the data is well clustered, <b>methods based on cluster analysis</b> are a good choice.  If the point is equidistant from the centers of several clusters, then it is anomalous. <br><br>  If the directions of the largest change in the dispersion are well distinguished in the data, then it seems to be a good choice - <b>search for anomalies based on the principal component method</b> .  In this case, deviations from the mean value for n1 (the most ‚Äúmain‚Äù components) and for n2 (the least ‚Äúmain‚Äù) are considered as a measure of anomaly. <br><br>  For example, it is proposed to look at the data set from <b>The Prognostics and Health Management Society (PHM Society)</b> .  This non-profit organization organizes competitions every year.  In 2018, for example, <a href="https://www.phmsociety.org/events/conference/phm/18/data-challenge">it was required to predict errors in operation and the time to failure of an ion beam etching installation</a> .  We will take the <a href="https://www.phmsociety.org/events/conference/phm/15/data-challenge">data set for 2015</a> .  It shows the readings of several sensors for 30 installations (training sample), and it is required to predict when and what error will occur. <br><br>  I did not find answers on the test sample in the network, so we will only play with the training one. <br><br>  In general, all installations are similar, but differ, for example, in the number of components, in the number of anomalies, etc.  Therefore, to study at the first 20, and test - for others does not make much sense. <br><br>  So, choose one of the installations, load it and take a look at this data.  The article will not be about <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25BD%25D1%2581%25D1%2582%25D1%2580%25D1%2583%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B8%25D0%25B7%25D0%25BD%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25B2">feature engineering</a> , so we will not peer hard. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pandas <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> pd <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.covariance <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> EllipticEnvelope <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.neighbors <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LocalOutlierFactor <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.ensemble <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> IsolationForest <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.svm <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> OneClassSVM dfa=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12a.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Component number'</span></span>,<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4'</span></span>,<span class="hljs-string"><span class="hljs-string">'S1ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S2ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S3ref'</span></span>,<span class="hljs-string"><span class="hljs-string">'S4ref'</span></span>]) dfa.head(<span class="hljs-number"><span class="hljs-number">10</span></span>)</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/301/77e/2bb/30177e2bb970e82495b18008b44d7ea4.jpg"></div><br>  As you can see, there are seven components for each of which there are indications of four sensors, which are taken every 15 minutes.  S1ref-S4ref in the description of the competition appear as reference values, but the values ‚Äã‚Äãare very different from the sensor readings.  In order not to waste time thinking about what they mean, we remove them.  If you look at the distribution of values ‚Äã‚Äãfor each attribute (S1-S4), then it turns out that S1, S2 and S4 distributions are continuous, and S3 are discrete.  In addition, if you look at the joint distribution of S2 and S4, it turns out that they are inversely proportional. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/69b/82b/cde/69b82bcdecdfca69128d88673b0518b4.jpg"></div><br>  Although the deviation from the direct dependence may indicate an error, we will not check it, but simply remove S4. <br><br>  Once again we will process a data set.  We leave S1, S2 and S3.  S1 and S2 are scaled by StandardScaler (we subtract the average and divide by the standard deviation), we translate S3 to OHE (One Hot Encoding).  Stitch readings from all components of the installation in one line.  Total 89 features.  2 * 7 = 14 - indications S1 and S2 for 7 components and 75 unique values ‚Äã‚Äãof R3.  Only 56 thousand of these lines. <br><br>  Load the file with errors. <br><br><pre> <code class="python hljs">dfc=pd.read_csv(<span class="hljs-string"><span class="hljs-string">'plant_12c.csv'</span></span>,names=[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>, <span class="hljs-string"><span class="hljs-string">'End Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>]) dfc.head()</code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3ad/419/e50/3ad419e50ac870a71c153b523a22bfed.jpg"></div><br>  Before trying the listed algorithms on our data set, let me make one more small digression.  It is necessary after all to be tested.  For this, it is proposed to take the start time of the error and the end time.  And all the readings inside this gap are considered abnormal, and outside - normal.  There are many drawbacks to this approach.  But especially one - abnormal behavior most likely occurs before the error is fixed.  Shift for loyalty window anomaly half an hour ago in time.  We will evaluate the F1-measure, accuracy (precision) and completeness (recall). <br><br>  The code for selecting features and determining the quality of a model <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">load_and_preprocess</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(plant_num)</span></span></span><span class="hljs-function">:</span></span>   <span class="hljs-comment"><span class="hljs-comment">#      ,       dfa=pd.read_csv('plant_{}a.csv'.format(plant_num),names=['Component number','Time','S1','S2','S3','S4','S1ref','S2ref','S3ref','S4ref'])   dfc=pd.read_csv('plant_{}c.csv'.format(plant_num),names=['Start Time','End Time','Type']).drop(0,axis=0)   N_comp=len(dfa['Component number'].unique())   #  15    dfa['Time']=pd.to_datetime(dfa['Time']).dt.round('15min')   #  6    (  ,    )   dfc=dfc[dfc['Type']!=6]   dfc['Start Time']=pd.to_datetime(dfc['Start Time'])   dfc['End Time']=pd.to_datetime(dfc['End Time'])   #      ,       OHE  3-    dfa=pd.concat([dfa.groupby('Time').nth(i)[['S1','S2','S3']].rename(columns={"S1":"S1_{}".format(i),"S2":"S2_{}".format(i),"S3":"S3_{}".format(i)}) for i in range(N_comp)],axis=1).dropna().reset_index()   for k in range(N_comp):       dfa=pd.concat([dfa.drop('S3_'+str(k),axis=1),pd.get_dummies(dfa['S3_'+str(k)],prefix='S3_'+str(k))],axis=1).reset_index(drop=True)   #          df_train,df_test=train_test_split(dfa,test_size=0.25,shuffle=False)   cols_to_scale=df_train.filter(regex='S[1,2]').columns   scaler=preprocessing.StandardScaler().fit(df_train[cols_to_scale])   df_train[cols_to_scale]=scaler.transform(df_train[cols_to_scale])   df_test[cols_to_scale]=scaler.transform(df_test[cols_to_scale])   return df_train,df_test,dfc #       def get_true_labels(measure_times,dfc,shift_delta):   idxSet=set()   dfc['Start Time']-=pd.Timedelta(minutes=shift_delta)   dfc['End Time']-=pd.Timedelta(minutes=shift_delta)   for idx,mes_time in tqdm_notebook(enumerate(measure_times),total=measure_times.shape[0]):       intersect=np.array(dfc['Start Time']&lt;mes_time).astype(int)*np.array(dfc['End Time']&gt;mes_time).astype(int)       idxs=np.where(intersect)[0]       if idxs.shape[0]:           idxSet.add(idx)   dfc['Start Time']+=pd.Timedelta(minutes=shift_delta)   dfc['End Time']+=pd.Timedelta(minutes=shift_delta)   true_labels=pd.Series(index=measure_times.index)   true_labels.iloc[list(idxSet)]=1   true_labels.fillna(0,inplace=True)   return true_labels #          def check_model(model,df_train,df_test,filt='S[123]'):   model.fit(df_train.drop('Time',axis=1).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))).map({-1:1,1:0})   print('F1 score: {:.3f}'.format(f1_score(df_test['Label'],y_preds)))   print('Precision score: {:.3f}'.format(precision_score(df_test['Label'],y_preds)))   print('Recall score: {:.3f}'.format(recall_score(df_test['Label'],y_preds)))   score = model.decision_function(df_test.drop(['Time','Label'],axis=1).filter(regex=(filt)))   sns.distplot(score[df_test['Label']==0])   sns.distplot(score[df_test['Label']==1]) df_train,df_test,anomaly_times=load_and_preprocess(12) df_test['Label']=get_true_labels(df_test['Time'],dfc,30)</span></span></code> </pre> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/50b/6f2/be9/50b6f2be9c3220853e92461805030e0b.jpg"></div>  <i>Test results of simple anomaly search algorithms in dataset PHM 2015 Data Challenge</i> <br><br>  Let's go back to the algorithms.  Let's try One Class SVM (OCSVM), IsolationForest (IF), EllipticEnvelope (EE) and LocalOutlierFactor (LOF) on our data.  To begin with, we will not set any parameters.  I note that LOF can work in two modes.  If novelty = False can only search for anomalies in the training sample (there is only fit_predict), if True, then it is aimed at finding anomalies outside the training sample (it can separately fit and predict).  IF has a mode of (behavior) old and new.  Use new.  He gives better results. <br><br>  OCSVM identifies anomalies well, but too many false-positive results.  For other methods, the result is even worse. <br><br>  But suppose we know the percentage of anomalies in the data.  In our case, 27%.  OCSVM has nu - the upper estimate for the percentage of errors and the lower one for the percentage of support vectors.  The remaining methods of contamination - the percentage of errors in the data.  In the IF and LOF methods, it is determined automatically, while for OCSVM and EE it is set to 0.1 by default.  Let's try to set contamination (nu) to 0.27.  Now the top result is at EE. <br><br>  Code for checking models: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">check_model</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model,df_train,df_test,filt=</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'S[123]'</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   model_type,model = model   model.fit(df_train.drop(<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   y_preds = pd.Series(model.predict(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))).map({<span class="hljs-number"><span class="hljs-number">-1</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">0</span></span>})   print(<span class="hljs-string"><span class="hljs-string">'F1 score for {}: {:.3f}'</span></span>.format(model_type,f1_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Precision score for {}: {:.3f}'</span></span>.format(model_type,precision_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   print(<span class="hljs-string"><span class="hljs-string">'Recall score for {}: {:.3f}'</span></span>.format(model_type,recall_score(df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],y_preds)))   score = model.decision_function(df_test.drop([<span class="hljs-string"><span class="hljs-string">'Time'</span></span>,<span class="hljs-string"><span class="hljs-string">'Label'</span></span>],axis=<span class="hljs-number"><span class="hljs-number">1</span></span>).filter(regex=(filt)))   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">0</span></span>])   sns.distplot(score[df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>])   plt.title(<span class="hljs-string"><span class="hljs-string">'Decision score distribution for {}'</span></span>.format(model_type))   plt.show()</code> </pre> <br>  It is interesting to look at the distribution of anomalous indices for different methods.  It can be seen that the LOF really works badly for this data.  ITS has points that the algorithm considers extremely anomalous.  However, there are normal points.  In IsoFor and OCSVM, it is clear that the choice of a cut-off threshold (contamination / nu) is important, which will change the trade-off between accuracy and completeness. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9d/0c8/80f/a9d0c880f36fe37f1de6c9290b8cccc7.png"></div><br>  It is logical that the readings of the sensors have a distribution close to normal, near stationary values.  If we really have a marked test sample, and better still a validation sample, then contamination can be reset.  Next is the question, what errors to focus more: false positive or false negative? <br><br>  LOF result is very low.  Not very impressive.  But remember that at the entrance we have OHE variables along with the variables transformed by StandardScaler.  And the default distance is Euclidean.  But if you count only S1 and S2 variables, then the situation is corrected and the result is comparable with other methods.  It is important, however, to understand that one of the key parameters of the listed metric classifiers is the number of neighbors.  It significantly affects the quality, and it must be tune.  The metric of distance itself would also be good to select. <br><br>  Now we will try to combine the two models.  At the beginning of one remove anomalies from the training set.  And then we will train OCSVM on a cleaner learning set.  According to the previous results, we observed the greatest completeness in EE.  We clear the training set through EE, train OCSVM on it and get F1 = 0.50, Accuracy = 0.34, fullness = 0.95.  Not impressive.  But we set nu = 0.27.  And our data is more or less ‚Äúclean‚Äù.  If we assume that the completeness of EE on the training sample will be the same, then 5% errors will remain.  Let us set such nu and get F1 = 0.69, Accuracy = 0.59, completeness = 0.82.  Fine.  It is important to note that in other methods such a combination does not work, as they imply that the training sample and the test number of anomalies are the same.  When teaching these methods on a pure training data set, contamination will have to be set less than in real data, and not close to zero, but it is better to select it for cross-validation. <br><br>  It is interesting to look at the search result on the sequence of indications: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2a2/0c5/26d/2a20c526d944b55116f3fcd6af064eab.png"></div><br>  The figure shows a segment of the readings of the first and second sensors for 7 components.  In the legend, the color of the corresponding errors (the beginning and end are shown by vertical lines of the same color).  Dots indicate predictions: green - correct predictions, red - false positive, violet - false negative.  The figure shows that it is difficult to visually determine the time of errors, and the algorithm copes with this task quite well.  Although it is important to understand that the testimony of the third sensor is not given here.  In addition, there are false-positive indications after the end of the error.  Those.  the algorithm sees that there are also erroneous values ‚Äã‚Äãthere, and we have marked this area as infallible.  On the right in the figure, there is a visible area before the error, which we marked as erroneous (half an hour before the error), which is recognized as error-free, which leads to false-negative model errors.  In the center of the picture is a coherent piece, recognized as an error.  The conclusion can be drawn as follows: when solving the problem of searching for anomalies, it is necessary to interact closely with engineers who understand the essence of the systems, the output of which they need to be predicted, since checking the algorithms used on the markup does not fully reflect reality and does not simulate the conditions in which such algorithms could to be used. <br><br>  Code for drawing graphics: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">plot_time_course</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(df_test,dfc,y_preds,start,end,vert_shift=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">4</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span>   plt.figure(figsize=(<span class="hljs-number"><span class="hljs-number">15</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>))   cols=df_train.filter(regex=(<span class="hljs-string"><span class="hljs-string">'S[12]'</span></span>)).columns   add=<span class="hljs-number"><span class="hljs-number">0</span></span>   preds_idx=y_preds.iloc[start:end][y_preds[<span class="hljs-number"><span class="hljs-number">0</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   true_idx=df_test.iloc[start:end,:][df_test[<span class="hljs-string"><span class="hljs-string">'Label'</span></span>]==<span class="hljs-number"><span class="hljs-number">1</span></span>].index   tp_idx=set(true_idx.values).intersection(set(preds_idx.values))   fn_idx=set(true_idx.values).difference(set(preds_idx.values))   fp_idx=set(preds_idx.values).difference(set(true_idx.values))   xtime=df_test[<span class="hljs-string"><span class="hljs-string">'Time'</span></span>].iloc[start:end]   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> col <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> cols:       plt.plot(xtime,df_test[col].iloc[start:end]+add)       plt.scatter(xtime.loc[tp_idx].values,df_test.loc[tp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'green'</span></span>)       plt.scatter(xtime.loc[fn_idx].values,df_test.loc[fn_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'violet'</span></span>)       plt.scatter(xtime.loc[fp_idx].values,df_test.loc[fp_idx,col]+add,color=<span class="hljs-string"><span class="hljs-string">'red'</span></span>)       add+=vert_shift   failures=dfc[(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&gt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">0</span></span>])&amp;(dfc[<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>]&lt;xtime.iloc[<span class="hljs-number"><span class="hljs-number">-1</span></span>])]   unique_fails=np.sort(failures[<span class="hljs-string"><span class="hljs-string">'Type'</span></span>].unique())   colors=np.array([np.random.rand(<span class="hljs-number"><span class="hljs-number">3</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> unique_fails])   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> fail_idx <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> failures.index:       c=colors[np.where(unique_fails==failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Type'</span></span>])[<span class="hljs-number"><span class="hljs-number">0</span></span>]][<span class="hljs-number"><span class="hljs-number">0</span></span>]       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'Start Time'</span></span>],color=c)       plt.axvline(failures.loc[fail_idx,<span class="hljs-string"><span class="hljs-string">'End Time'</span></span>],color=c)   leg=plt.legend(unique_fails)   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(len(unique_fails)):       leg.legendHandles[i].set_color(colors[i])</code> </pre> <br>  If the percentage of anomalies is below 5% and / or they are poorly separated from ‚Äúnormal‚Äù indicators, the methods listed above work poorly and it is worth using algorithms based on neural networks.  In the simplest case, these will be: <br><br><ul><li>  autoencoders (high error of a trained autoencoder will signal anomalous readings); </li><li>  recurrent networks (learns by sequence to predict the last reading. If the difference is large, the point is anomalous). </li></ul><br>  Separately, it is worth noting the specifics of working with time series.  It is important to understand that most of the above algorithms (except autoencoders and isolating forests) are likely to produce worse quality when adding lag tags (readings from previous times). <br><br>  Let's try to add lag signs in our example.  The description of the competition states that the values ‚Äã‚Äã3 hours before the error are not related to the error.  Then add signs for 3 hours.  Total 259 signs. <br><br>  As a result, the OCSVM and IsolationForest results almost did not change, while the Elliptic Envelope and LOF fell. <br><br>  To use information about the dynamics of the system, you should use auto-encoders with recurrent or convolutional neural networks.  Or, for example, a combination of auto-coders, compressing information, and conventional approaches to search for anomalies based on compressed information.  The opposite approach is also promising.  The primary elimination of the most uncharacteristic points with standard algorithms, and then the training of the autoencoder is already on cleaner data. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/340/9e8/dbc/3409e8dbcc5e9bcac2a1acb08bc7d146.jpg"></div>  <a href="https://www.thetalkingmachines.com/article/how-squash-mathematical-tomato-rubics-cube-surfaces-and-their-connection-reversible"><i>A source</i></a> <br><br>  There is a set of techniques for working with one-dimensional time series.  All of them are aimed at predicting future readings, and the points that differ from the prediction are considered anomalies. <br><br><h2>  <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259C%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C_%25D0%25A5%25D0%25BE%25D0%25BB%25D1%258C%25D1%2582%25D0%25B0-%25D0%25A3%25D0%25B8%25D0%25BD%25D1%2582%25D0%25B5%25D1%2580%25D1%2581%25D0%25B0">Holt-Winters Model</a> </h2><br>  Triple exponential smoothing, decomposes the series into 3 components: level, trend and seasonality.  Accordingly, if the series is presented in this form, the method works well.  Facebook Prophet works on a similar principle, but the components themselves value differently.  You can read more, for example, <a href="https://habr.com/ru/company/ods/blog/323730/">here</a> . <br><br><h2>  S (ARIMA) </h2><br>  In this method, the predictive model is based on autoregression and the moving average.  If we are talking about expanding S (ARIMA), then we can also evaluate seasonality.  Read more about the approach <a href="https://habr.com/ru/post/210530/">here</a> , <a href="https://habr.com/ru/post/207160/">here</a> and <a href="https://habr.com/ru/company/ods/blog/327242/">here</a> . <br><br><h2>  Other Predictive Service Approaches </h2><br>  When it comes to time series and there is information about the time of occurrence of errors, you can apply the methods of training with the teacher.  In addition to the need for labeled data in this case, it is important to understand that predicting an error will depend on the nature of the error.  If there are a lot of mistakes and different nature, then you will most likely need to predict each one individually, which will require even more marked data, but the prospects will be more attractive. <br><br>  There are alternative ways to use machine learning in predictive service.  For example, the prediction of system failure in the next N days (the task of classification).  It is important to understand that such an approach requires that the occurrence of an error in the operation of the system be preceded by a period of degradation (not necessarily gradual).  The most successful approach is the use of neural networks with convolutional and / or recurrent layers.  Separately, it is worth noting the methods for augmentation of time series.  The most interesting and at the same time simple for me are <a href="https://www.semanticscholar.org/paper/Data-Augmentation-for-Time-Series-Classification-Guennec-Malinowski/faadd13622f147bb0a974a8316e1e36dfe1c1091">two approaches</a> : <br><br><ul><li>  selects the continuous part of the series (for example, 70%, and the rest is removed) and stretches to its original size </li><li>  the continuous part of the series is selected (for example, 20%) and stretched or compressed.  After that, the entire row is respectively compressed or stretched to its original size. </li></ul><br>  There is also a variant with the prediction of the remaining lifetime of the system (regression task).  Here we can distinguish a separate approach: the prediction is not the life time, and the parameters of the Weibull distribution. <br><br>  You can read about the distribution itself <a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%2592%25D0%25B5%25D0%25B9%25D0%25B1%25D1%2583%25D0%25BB%25D0%25BB%25D0%25B0">here</a> , and <a href="https://ragulpr.github.io/2016/12/22/WTTE-RNN-Hackless-churn-modeling/">here</a> about using it in conjunction with recurrent meshes.  This distribution has two parameters, Œ± and Œ≤.  Œ± tells you when the event will occur, and Œ≤ tells you how confident the algorithm is.  Although the application of such an approach is promising, difficulties arise in learning the neural network in this case, since the algorithm is easier to be unsure at first than to predict an adequate lifetime. <br><br>  Separately, it is worth noting the <a href="https://en.wikipedia.org/wiki/Proportional_hazards_model">Cox regression</a> .  It allows you to predict the fault tolerance of the system for each point in time after the diagnosis, presenting it as a product of two functions.  One function is a degradation of the system, independent of its parameters, i.e.  common to any such systems.  And the second is an exponential dependence on the parameters of a particular system.  So for a person there is a common function associated with aging, more or less the same for all.  But deterioration in health is also associated with the state of the internal organs, which is different for everyone. <br><br>  I hope you now know a bit more about predictive service.  I am sure you will have questions about the methods of machine learning, which are most often used for this technology.  I will be happy to answer each of them in the comments.  If you are interested in not just asking about what was written, but you want to do something similar, our team at <a href="http://cleverdata.ru/">CleverDATA is</a> always happy to talented and enthusiastic professionals. <br><br><div class="spoiler">  <b class="spoiler_title">Are there any vacancies?</b>  <b class="spoiler_title">Of course!</b> <div class="spoiler_text"><ul><li>  <a href="https://job.lanit.ru/vacancy/Pages/CD-15.aspx%3Futm_source%3Dhabr%26utm_medium%3Dpost-2019-04-09%26utm_campaign%3Dcd">Java Developer (Big Data)</a> </li></ul></div></div></div><p>Source: <a href="https://habr.com/ru/post/447190/">https://habr.com/ru/post/447190/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447178/index.html">5 effective possibilities of using process mining technology</a></li>
<li><a href="../447180/index.html">Overview and Comparison of Ingress Controllers for Kubernetes</a></li>
<li><a href="../447182/index.html">Operating Systems: Three Easy Pieces. Part 3: Process API (Translation)</a></li>
<li><a href="../447184/index.html">What is Initial Exchange Offering (IEO) and how does it differ from ICO?</a></li>
<li><a href="../447186/index.html">How to run ML prototype in one day. Report Yandex.Taxi</a></li>
<li><a href="../447192/index.html">What role can technology play in the ancient art of mixing spices?</a></li>
<li><a href="../447194/index.html">Features rendering in the game Metro: Exodus c raytracing</a></li>
<li><a href="../447196/index.html">7. Check Point Getting Started R80.20. Access control</a></li>
<li><a href="../447198/index.html">Moon mission "Bereshit": landing-crash-fall on the moon</a></li>
<li><a href="../447204/index.html">April 17: Open lecture "Path of the game developer: from idea to launch" and the game library at VSBI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>