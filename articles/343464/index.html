<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Classifying Sounds with TensorFlow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Igor Panteleev, Software Developer, DataArt 

 For recognition of human speech, many services have been invented - it suffices to recall the Pocketsph...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Classifying Sounds with TensorFlow</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/nn/fm/ve/nnfmve5msvmeg233iornp5kgstw.jpeg"><br><br>  <i>Igor Panteleev, Software Developer, DataArt</i> <br><br>  For recognition of human speech, many services have been invented - it suffices to recall the Pocketsphinx or Google Speech API.  They are able to quite qualitatively convert phrases recorded as an audio file into printed text.  But none of these applications can sort the different sounds captured by the microphone.  What exactly was recorded: human speech, animal screams or music?  We are faced with the need to answer this question.  And they decided to create test projects for classifying sounds using machine learning algorithms.  The article describes what tools we have chosen, what problems we encountered, how we trained the model for TensorFlow, and how to launch our open source solution.  We can also upload recognition results to the <a href="https://devicehive.com/">DeviceHive</a> IoT platform in order to use them in cloud services for third-party applications. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Selection of tools and models for classification </h2><br>  First we had to choose software for working with neural networks.  The first solution that seemed appropriate to us was the <a href="https://github.com/tyiannak/pyAudioAnalysis">Python Audio Analysis</a> library. <br><br>  The main problem of machine learning is a good data set.  There are a lot of such sets for speech recognition and music classification.  With the classification of random sounds, things are not so good, but we, though not immediately, found <a href="https://serv.cusp.nyu.edu/projects/urbansounddataset/">a data set with "city" sounds</a> . <a name="habracut"></a><br><br>  During testing, we encountered the following problems: <br><br><ul><li>  pyAudioAnalysis is not flexible enough.  It works with a small range of parameters, and some of them are calculated on the fly.  For example, the number of training cycles is based on the number of samples, and this cannot be changed. </li><li>  The selected data set contains only 10 classes, and all of them belong to the city sounds group. </li></ul><br>  The next solution was the <a href="https://research.google.com/audioset/index.html">Google AudioSet dataset</a> , which is based on YouTube‚Äôs tagged video <a href="https://research.google.com/audioset/index.html">clips</a> and is available for download in two formats: <br><br><ol><li>  CSV files that contain the following information about each fragment: the ID of the video posted on YouTube, the beginning and end time of the fragment, one or more tags assigned to the passage. </li><li>  Extracted audiofichs that are saved as TensorFlow files. <br>  These audiofichs are compatible with <a href="https://research.google.com/youtube8m/index.html">YouTube-8M</a> models.  This solution also suggests using the <a href="https://github.com/tensorflow/models/tree/master/research/audioset">TensorFlow VGGish</a> model to extract features from the audio stream.  This solution met most of our requirements, and we decided to choose it. </li></ol><br><h2>  Learning model </h2><br>  The next task was to find out how the YouTube-8M interface works.  It is designed to work with video, but, fortunately, can work with audio.  This library is quite flexible, but has a fixed number of classes.  Therefore, we made some changes so that the number of classes can be passed as a parameter.  YouTube-8M can work with two types of data: aggregated features and features for each fragment.  Google AudioSet provides data in the form of features for each fragment.  Next we had to choose a model for training. <br><br><h2>  Resources, Time, and Accuracy </h2><br>  Graphic processors (GPUs) are better suited for machine learning than central processing units (CPUs).  You can find more information <a href="https://docs.devicehive.com/blog/using-gpus-for-training-tensorflow-models">here</a> , so we will not dwell on this in detail and go straight to our configuration.  For the experiments, we used a PC with a single NVIDIA GTX 970 4GB graphics card. <br><br>  In our case, learning time did not matter much.  Note that one or two hours of training was enough to make an initial decision about the chosen model and its accuracy. <br><br>  Of course, we want to get the highest possible accuracy.  But learning a more complex model (which should provide greater accuracy) will require more RAM (video card memory in the case of using a graphics processor). <br><br><h2>  Model selection </h2><br>  A full list of YouTube-8M models with descriptions is available <a href="https://github.com/google/youtube-8m">here</a> .  Since our training data is presented in the form of fragmented features, it is necessary to use the appropriate model.  Google AudioSet contains a data set that is divided into three parts: a balanced train, an unbalanced train, and a score.  Read more about this <a href="https://research.google.com/audioset/download.html">here</a> . <br><br>  For training and evaluation used a modified version of YouTube-8M.  You can find it <a href="https://research.google.com/audioset/download.html">here</a> . <br><br><h3>  Balanced learning </h3><br>  In this case, the command looks like this: <br><br>  <i>python train. 527 --train_dir = / path_to_logs --model = ModelName</i> <br><br>  For LstmModel, we changed the base learning rate to 0.001 in accordance with the documentation.  We also changed the value of lstm_cells to 256, because we did not have enough RAM. <br><br>  Let's look at the learning outcomes. <br><br><img src="https://habrastorage.org/webt/q6/bq/yn/q6bqynq8mqyvxbrl0qhrrc3huwg.jpeg"><br><br><img src="https://habrastorage.org/webt/fy/qh/fr/fyqhfrv0feyrlpzn4j9embgo4iu.jpeg"><br><br><table><tbody><tr><th>  Model name </th><th>  Studying time </th><th>  Evaluation in the last step </th><th>  average rating </th></tr><tr><td>  Logistic </td><td>  14m 3s </td><td>  0.5859 </td><td>  0.5560 </td></tr><tr><td>  Dbof </td><td>  31m 46s </td><td>  1,000 </td><td>  0.5220 </td></tr><tr><td>  Lstm </td><td>  1h 45m 53s </td><td>  0.9883 </td><td>  0.4581 </td></tr></tbody></table><br><br>  We managed to get good results at the training stage, but this does not mean that we will achieve similar indicators with a full assessment. <br><br><h3>  Unbalanced learning </h3><br>  In an unbalanced data set, there are much more samples, so we set the number of training cycles to 10 (we had to set five, because it took a lot of time to train). <br><br><img src="https://habrastorage.org/webt/cs/_o/q0/cs_oq0mtxmxwywa02hkbemk2wq8.jpeg"><br><br><img src="https://habrastorage.org/webt/ap/hj/rh/aphjrhia01-ro0u2bbkwhvizzas.jpeg"><br><br><table><tbody><tr><th>  Model name </th><th>  Studying time </th><th>  Evaluation in the last step </th><th>  average rating </th></tr><tr><td>  Logistic </td><td>  2h 4m 14s </td><td>  0.8750 </td><td>  0.5125 </td></tr><tr><td>  Dbof </td><td>  4h 39m 29s </td><td>  0.8848 </td><td>  0.5605 </td></tr><tr><td>  Lstm </td><td>  9h 42m 52s </td><td>  0.8691 </td><td>  0.5396 </td></tr></tbody></table><br><br><h3>  Learning journal </h3><br>  If you want to examine our log files, you can download and extract them by following this link.  After downloading, run <i>tensorboard --logdir / path_to_train_logs /</i> and follow the <a href="http://127.0.0.1:6006/">link</a> . <br><br><h3>  More about learning </h3><br>  YouTube-8M takes many options, and many of them affect the learning process. <br><br>  For example, you can adjust the learning rate and the number of epochs, which will greatly change the learning process.  There are also three functions for calculating losses and other useful variables that can be customized and modified to improve results. <br><br><h2>  Using a trained model with audio capture devices </h2><br>  When we have trained models, it's time to add code to interact with them. <br><br><h3>  Microphone Audio Capture </h3><br>  We need to somehow get the audio data from the microphone.  We will use the <a href="https://pypi.python.org/pypi/PyAudio">PyAudio</a> library, which has a simple interface and can work on most platforms. <br><br><h3>  Sound preparation </h3><br>  As mentioned earlier, we use the TensorFlow VGGish model as a tool for extracting features.  Here is a brief explanation of the transformation process: <br><br>  For visualization we used the sample Dog bark (‚ÄúDog barking‚Äù) from the UrbanSound data set. <br><br>  Convert audio to 16 kHz mono. <br><br><img src="https://habrastorage.org/webt/-f/dk/5i/-fdk5ige-iof14pumjpofqcwgya.jpeg"><br><br>  We calculate the spectrogram using the values ‚Äã‚Äãof STFT (Fourier transform on a small time interval) with a window size of 25 ms, a step of 10 ms, and <a href="https://en.wikipedia.org/wiki/Hann_function">a Hann</a> periodic <a href="https://en.wikipedia.org/wiki/Hann_function">window</a> . <br><br><img src="https://habrastorage.org/webt/gj/n4/l0/gjn4l0rsnvf6eio76u-41it59i4.png"><br><br>  We calculate the chalk spectrogram, leading the current spectrogram to a 64-bit chalk range. <br><br><img src="https://habrastorage.org/webt/tk/w8/xz/tkw8xzyeegidmmxuthimxk3frf4.jpeg"><br><br>  We calculate the stabilized logarithmic spectrogram using log (mel-spectrum + 0.01), where the offset is used to avoid the logarithm of zero. <br><br><img src="https://habrastorage.org/webt/i5/dx/of/i5dxof9yrmv9grbsae0b0cx46c8.jpeg"><br><br>  These features are then converted to non-intersecting fragments in 0.96 seconds, where each of them has a dimension of 64 chalk-ranges per 96 frames of 10 ms each. <br><br>  The resulting data is then fed into the VGGish model to bring the data into a vector view. <br>  Classification <br>  Finally, we need an interface to transfer data to the neural network and get results. <br><br>  Let's take the YouTube-8M interface as a basis, but change it to remove the serialization / deserialization step. <br><br>  <a href="https://github.com/devicehive/devicehive-audio-analysis">Here</a> you can see the results of our work.  Let's look at this moment in more detail. <br><br><h3>  Installation </h3><br>  PyAudio uses libportaudio2 and portaudio19-dev, so you need to install these packages to work. <br><br>  In addition, you will need some Python libraries.  You can install them with pip: <i>pip install -r requirements.txt</i> <br><br>  You also need to download and extract the archive with the saved models to the project root.  You can find it <a href="">here</a> . <br><br><h3>  Launch </h3><br>  Our project offers the possibility of using one of the three interfaces. <br><br><h4>  Pre-recorded audio file </h4><br>  Just run <i>python parse_file.py path_to_your_file.wav</i> , and you‚Äôll see <i>Speech: 0.75, Music: 0.12, Inside, large room or hall: 0.03</i> <br><br>  The result depends on the source data.  These values ‚Äã‚Äãare derived from the neural network prediction.  A higher value means a higher probability that the input data belongs to this class. <br><br><h4>  Capturing and processing data from a microphone </h4><br>  python capture.py runs a process that will constantly capture data from your microphone.  It will transmit data for classification every 5‚Äì7 seconds (by default).  You will see the results in the same way as in the previous example.  You can run it with the <i>--save_path = / path_to_samples_dir /</i> parameter, in this case all the captured data will be saved in the specified folder in the .WAV format.  This feature is useful if you want to try different models with the same patterns.  Use the --help parameter to get additional information. <br><br><h4>  Web interface </h4><br>  The python daemon.py command implements a simple web interface that is available by default at <a href="http://127.0.0.1:8000/">http://127.0.0.1:8000</a> .  We use the same code as in the previous example.  You can see the last ten predictions on <a href="http://127.0.0.1:8000/events">the event page</a> . <br><br><img src="https://habrastorage.org/webt/pb/ob/z3/pbobz3b1vtwgd69pijxusqgpnds.jpeg"><br><br><h3>  Integration with IoT </h3><br>  The last very important point - integration with IoT-infrastructure.  If you launch the web interface we mentioned in the previous section, you can find the DeviceHive client connection status and settings on the main page.  While the client is connected, forecasts will be sent to the specified device in the form of notifications. <br><br><img src="https://habrastorage.org/webt/-b/2l/_o/-b2l_o9breijlouejq-mkcudtla.jpeg"><br><br><h2>  Conclusion </h2><br>  TensorFlow is a very flexible tool that can be useful in many machine-learning applications for image and sound recognition.  Using such a tool in tandem with an IoT platform allows you to create an intelligent solution with great potential.  In the "smart cities" it can be used to ensure security - it, for example, is able to recognize the sound of breaking glass or a shot.  Even in tropical forests, such a solution could be used to track the routes of wild animals or birds, analyzing their voices.  The IoT platform can be configured to send notifications of sounds in range of the microphone.  Such a solution can be installed on local devices (at the same time, it can be deployed as a cloud system) to minimize the cost of traffic and cloud computing, customize it to send only notifications, without attachments with raw audio.  Do not forget that this is an open source project, so you can use it to create your own services. </div><p>Source: <a href="https://habr.com/ru/post/343464/">https://habr.com/ru/post/343464/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343454/index.html">‚ÄúBlender for 28 reports, each of which is equal to ten articles‚Äù - an interview with the Program Committee of HolyJS 2017 Moscow</a></li>
<li><a href="../343456/index.html">Setting up Sublime Text 3, SW4 and STM32CubeMX for developing STM32 for Linux</a></li>
<li><a href="../343458/index.html">Kotlin code style</a></li>
<li><a href="../343460/index.html">Announcement mitap Sync.NET # 5 in Kharkov</a></li>
<li><a href="../343462/index.html">Three myths about mobile payment security</a></li>
<li><a href="../343466/index.html">Introduction to Neural Networks on Golang</a></li>
<li><a href="../343468/index.html">Explanation of SNARKs. Knowledge of the adopted coefficient and reliable blind calculation of polynomials (translation)</a></li>
<li><a href="../343470/index.html">Hack bikes with Quora</a></li>
<li><a href="../343474/index.html">PhpStorm 2017.3 Available</a></li>
<li><a href="../343476/index.html">Xamarin, native iOS / Android and hybrid development tools</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>