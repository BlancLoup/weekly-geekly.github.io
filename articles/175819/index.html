<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Regularization in a limited Boltzmann machine, experiment</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hey. In this post we will conduct an experiment in which we test two types of regularization in a limited Boltzmann machine . As it turned out, RBM is...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Regularization in a limited Boltzmann machine, experiment</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/ff9/042/824/ff90428247ad77d2c0f30f3fc11fb77b.jpg" align="right">  Hey.  In this post we will conduct an experiment in which we test two types of <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25B3%25D1%2583%25D0%25BB%25D1%258F%25D1%2580%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F_(%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D0%25BA%25D0%25B0)">regularization</a> in a <a href="http://habrahabr.ru/post/159909/">limited Boltzmann machine</a> .  As it turned out, RBM is very sensitive to model parameters, such as the moment and <a href="http://habrahabr.ru/post/157179/">local field of a neuron</a> (for more information about all parameters, see <a href="http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf">Jeffrey Hinton‚Äôs RBM Practical Guide</a> ).  But for a complete picture and for getting patterns like <a href="">these</a> , I lacked one more parameter - regularization.  The limited Boltzmann machines can be treated as a variety of the Markov network, and as another neural network, but if you dig deeper, you will see an analogy with vision.  Like the <a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B0">primary visual cortex</a> , which receives information from the retina through the optic nerve (forgive me to biologists for such a simplification), RBM searches for simple patterns in the input image.  The analogy does not end there, if we interpret the very small and zero weights as the absence of weight, then we get that each hidden neuron RBM forms a certain <a href="http://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25B5">receptive field</a> , and <a href="http://habrahabr.ru/post/163819/">a deep network</a> formed from <a href="http://habrahabr.ru/post/163819/">RBM trained</a> forms simple images from simple images;  in principle, the <a href="http://ru.wikipedia.org/wiki/%25D0%2597%25D1%2580%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B0">visual cortex</a> deals with something similar, although it‚Äôs probably more complicated =) <br><br><a name="habracut"></a><br><br><h4>  L1 and L2 regularization </h4><br>  We begin, perhaps, with a brief description of what model regularization is - a way to impose a penalty on the objective function for the complexity of the model.  From Bayesian point of view, this is a way to take into account some a priori information about the distribution of model parameters.  An important feature is that regularization helps to avoid retraining the model.  Denote the model parameters as Œ∏ = {Œ∏_i}, i = 1..n.  The final objective function is <b>C = Œ∑ (E + ŒªR)</b> , where <b>E</b> is the main objective function of the model, <b>R = R (Œ∏)</b> is a function of the parameters of the model, <i>this</i> and <i>lambda</i> are the learning rate and the regularization parameter, respectively.  Thus, to calculate the gradient of the final objective function, it will be necessary to calculate the gradient of the regularization function: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/storage2/822/688/16c/82268816c201c204a0f514bd7718833b.gif"><br><br>  We consider two types of regularization whose roots are in the <a href="http://en.wikipedia.org/wiki/Lp_space">Lp metric</a> .  The regularization function of L1 and its derivatives are as follows: <br><br><img src="https://habrastorage.org/storage2/10c/d57/873/10cd578737bc62ae1f4c0556eb2c5212.gif"><br><br><img src="https://habrastorage.org/storage2/c26/089/be8/c26089be8522ceb07a89853590216402.gif"><br><br>  L2 regularization is as follows: <br><br><img src="https://habrastorage.org/storage2/3c9/e1b/8e9/3c9e1b8e9b44e0304c22a8f88e72af73.gif"><br><br><img src="https://habrastorage.org/storage2/438/281/2f7/4382812f7b2948af61fb4859e3539830.gif"><br><br>  Both regularization methods penalize the model for a large value of weights, in the first case with absolute weights, in the second with squares of weights, thus, the distribution of weights will approach normal with a center at zero and a large peak.  A more detailed comparison of L1 and L2 <a href="http://cs.nyu.edu/~rostami/presentations/L1_vs_L2.pdf">can be found here</a> .  As we will see later, about 70% of the weights will be less than 10 ^ (- 8). <br><br><h4>  RBM Regularization </h4><br>  The day before last, I described <a href="http://habrahabr.ru/post/159909/">an example of implementing RBM in C #</a> .  I will rely on the same implementation to show where the regularization is embedded, but first the formulas.  The goal of learning RBM is to maximize the likelihood that the restored image will be identical to the input one: <br><br><img src="https://habrastorage.org/storage2/151/ff7/730/151ff7730a5bd2a5755affdfafbe3e6f.gif" alt="image"><br><br>  Generally speaking, the algorithm maximizes the logarithm of probability, and in order to introduce a penalty, it is necessary to subtract the value of the regularization function from the obtained probability, as a result, the new objective function takes the following form: <br><br><img src="https://habrastorage.org/storage2/b4e/62c/abf/b4e62cabfc048b4f9d8a09a29a1a22c7.png"><br><br>  The derivative of such a function by parameter will look like this: <br><br><img src="https://habrastorage.org/storage2/859/4de/b21/8594deb2176508a0f9f7ee9d68f63ffa.png"><br><br>  The contrastive divergence algorithm consists of a positive phase and a negative one, thus, in order to add regularization, it is sufficient to subtract the value of the derivative of the regularization function from the value of the positive phase, after subtracting the negative phase: <br><br><div class="spoiler">  <b class="spoiler_title">positive and negative phases</b> <div class="spoiler_text"><pre><code class="cs hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">region</span></span></span><span class="hljs-meta"> Gibbs sampling for (int k = 0; k &lt;= _config.GibbsSamplingChainLength; k++) { //calculate hidden states probabilities hiddenLayer.Compute(); #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">region</span></span></span><span class="hljs-meta"> accumulate negative phase </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (k == _config.GibbsSamplingChainLength) { for (int i = 0; i &lt; visibleLayer.Neurons.Length; i++) { for (int j = 0; j &lt; hiddenLayer.Neurons.Length; j++) { nablaWeights[i, j] -= visibleLayer.Neurons[i].LastState * hiddenLayer.Neurons[j].LastState; </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (_config.RegularizationFactor &gt; Double.Epsilon) { //regularization of weights double regTerm = 0; switch (_config.RegularizationType) { case RegularizationType.L1: regTerm = _config.RegularizationFactor* Math.Sign(visibleLayer.Neurons[i].Weights[j]); break; case RegularizationType.L2: regTerm = _config.RegularizationFactor* visibleLayer.Neurons[i].Weights[j]; break; } nablaWeights[i, j] -= regTerm; } } } </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (_config.UseBiases) { for (int i = 0; i &lt; hiddenLayer.Neurons.Length; i++) { nablaHiddenBiases[i] -= hiddenLayer.Neurons[i].LastState; } for (int i = 0; i &lt; visibleLayer.Neurons.Length; i++) { nablaVisibleBiases[i] -= visibleLayer.Neurons[i].LastState; } } break; } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span><span class="hljs-meta"> //sample hidden states for (int i = 0; i &lt; hiddenLayer.Neurons.Length; i++) { hiddenLayer.Neurons[i].LastState = _r.NextDouble() &lt;= hiddenLayer.Neurons[i].LastState ? 1d : 0d; } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">region</span></span></span><span class="hljs-meta"> accumulate positive phase </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (k == 0) { for (int i = 0; i &lt; visibleLayer.Neurons.Length; i++) { for (int j = 0; j &lt; hiddenLayer.Neurons.Length; j++) { nablaWeights[i, j] += visibleLayer.Neurons[i].LastState* hiddenLayer.Neurons[j].LastState; } } </span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">if</span></span></span><span class="hljs-meta"> (_config.UseBiases) { for (int i = 0; i &lt; hiddenLayer.Neurons.Length; i++) { nablaHiddenBiases[i] += hiddenLayer.Neurons[i].LastState; } for (int i = 0; i &lt; visibleLayer.Neurons.Length; i++) { nablaVisibleBiases[i] += visibleLayer.Neurons[i].LastState; } } } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span><span class="hljs-meta"> //calculate visible probs visibleLayer.Compute(); } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span><span class="hljs-meta"> } #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">endregion</span></span></span></span></code> </pre> <br></div></div><br><br>  Go to the experiments.  The same data set was used as test data as in the <a href="http://habrahabr.ru/post/159909/">previous post</a> .  In all cases, training was conducted exactly 1000 epochs.  I will give two ways to visualize the patterns found, in the first case (drawing in gray tones) the dark value corresponds to the minimum value of the weight, and the white to the maximum;  in the second drawing, black corresponds to zero, an increase in the red component corresponds to an increase in a positive direction, and an increase in the blue component - in a negative one.  I will also give a histogram of the distribution of weights and small comments. <br><br><h4>  Without regularization </h4><br><img src="https://habrastorage.org/storage2/8dd/869/51b/8dd86951b973bc5872033a18239467b0.png"><img src="https://habrastorage.org/storage2/857/d11/53f/857d1153f60781284d00e12a814edcaa.png"><br><br><img src="https://habrastorage.org/storage2/89c/53a/d0b/89c53ad0b77aefefed19669560c6d16d.png"><br><br><ul><li>  error value on the training set: 0.188181367765024 </li><li>  error value on the cross qualification set: 21.0910315518859 </li></ul><br><br>  The templates turned out very blurry and difficult to analyze.  The average value of the scales is shifted to the left, and the absolute value of the scales reaches 2 or more. <br><br><h4>  L2 regularization </h4><br><img src="https://habrastorage.org/storage2/3f2/c00/dd8/3f2c00dd898e9993b07a48b4542d673c.png"><img src="https://habrastorage.org/storage2/3f7/c03/b0d/3f7c03b0d4d6574138d66c1d2316e6d9.png"><br><br><img src="https://habrastorage.org/storage2/91c/168/6d8/91c1686d8a9ebcee1903da2918b5495a.png"><br><br><ul><li>  error value on the training set: 10.1198906337165 </li><li>  error value at cross qualification set: 23.3600809429977 </li><li>  regularization parameter: 0.1 </li></ul><br><br>  Here we see clearer images.  We can already discern that in some images some peculiarities of letters are really taken into account.  Despite the fact that the error on the training set is 100 times worse than when learning without regularization, the error on the cross-qualification set does not much exceed the first experiment, which suggests that the generalizing ability of the network on unfamiliar images did not deteriorate much (it should be noted that the error calculation did not include the value of the regularization function, which allows us to compare values ‚Äã‚Äãwith previous experience).  The weights are concentrated around zero, and not much higher than 0.2 in absolute value, which is 10 times less than in previous experience. <br><br><h4>  L1 regularization </h4><br><img src="https://habrastorage.org/storage2/318/71d/1e8/31871d1e817487792855ae36c550aeeb.png"><img src="https://habrastorage.org/storage2/cfc/a9c/d8c/cfca9cd8c9bde1bc50614df0c7ff024d.png"><br><br><img src="https://habrastorage.org/storage2/872/e12/6f7/872e126f7cf323b97332c815944c7cb2.png"><br><br><ul><li>  error value on the training set: 4.42672814826447 </li><li>  error value on the cross qualification set: 17.3700437102876 </li><li>  regularization parameter: 0.005 </li></ul><br><br>  In this experience, we see clear patterns, and especially receptive fields (around the blue-red spots, all weights are almost zero).  Patterns can even be analyzed, we can see, for example, edges from W (the first row is the fourth picture), or a pattern that reflects the average size of the input images (in the fifth row 8 and 10 pictures).  The recovery error on the training set is 40 times worse than in the first experiment, but better than with L2 regularization, at the same time the error on the unknown set is better than in both previous experiments, which indicates an even better generalizing ability.  Weights are also concentrated around zero, and in most cases do not exceed it much.  The significant difference in the regularization parameter is explained by the fact that when calculating the gradient for L2, the parameter is multiplied by the weight value, as a rule, these two numbers are less than 1;  but with L1, the parameter is multiplied by | 1 |, and the final value will be of the same order as the regularization parameter. <br><br><h4>  Conclusion </h4><br>  As a conclusion, I would like to say that the BSR is really a very sensitive thing to the parameters.  And the main thing is not to break down in the process of finding a solution -) In the end, I‚Äôll give an increased representation of one of the RBM trainers with L1 regularization, but already for 5000 epochs. <br><img src="https://habrastorage.org/storage2/d55/fdf/042/d55fdf04273da3a6ab6b8d9599f0a1b5.png"><br><br>  <b>UPDATE</b> : <br>  I recently trained rbm on a full set of capital letters 4 fonts of 3 styles, during 5000 iterations with L1 regularization, it took about 14 hours, but the result is even more interesting, the features turned out to be even more local and clean <br><br><img src="https://habrastorage.org/storage2/e58/a13/b8d/e58a13b8d8ba21c15698e0dc476e5436.png"><img src="https://habrastorage.org/storage2/d66/a16/0c1/d66a160c1d431983baab5e49fc3b2db2.png"></div><p>Source: <a href="https://habr.com/ru/post/175819/">https://habr.com/ru/post/175819/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../175809/index.html">Summer is coming soon</a></li>
<li><a href="../175811/index.html">Impressions of using Google Producer</a></li>
<li><a href="../175813/index.html">bookradar.org - book search service</a></li>
<li><a href="../175815/index.html">DIY wall patch panel</a></li>
<li><a href="../175817/index.html">03 - HP T410 All-in-One Thin Client and VDI Technology</a></li>
<li><a href="../175821/index.html">Pthread_cond_timedwait: problem, solution, discussion</a></li>
<li><a href="../175823/index.html">02 - Thin Client and Fat Server</a></li>
<li><a href="../175825/index.html">Resources every Android developer should know about</a></li>
<li><a href="../175827/index.html">How we looked for Mars 3</a></li>
<li><a href="../175829/index.html">Exhibitions in China - 48 hours from idea to launch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>