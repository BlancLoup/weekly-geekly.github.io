<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DGFS - do-it-yourself fast file system</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sometimes file system tools have to store a lot of information, most of which are static. When there are few files and they are large, it is tolerable...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DGFS - do-it-yourself fast file system</h1><div class="post__text post__text-html js-mediator-article">  Sometimes file system tools have to store a lot of information, most of which are static.  When there are few files and they are large, it is tolerable.  But if the data is in a huge number of small files, access to which is pseudo-random, the situation is approaching a catastrophe. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/23c/a79/af9/23ca79af94bc9a09ceb7ee475cbe6487.png"><br><br>  I believe that a specialized read-only file system, other things being equal, has advantages over this general purpose, because: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  not necessary to manage free space; </li><li>  no need to spend money on journaling; </li><li>  you can not worry about fragmentation and store files continuously; </li><li>  it is possible to collect all the meta-information in one place and effectively cache it; </li><li>  sin not to compress the meta-information, since it was in one heap. </li></ol><br>  In this article, we will understand how to organize a file system, with the objective function of maximum performance at minimum cost. <br><a name="habracut"></a><br><h3>  Task </h3><br><ol><li>  100 million small files (~ 8K each). </li><li>  Three-level hierarchy - 100 top-level directories, each of which has 100 directories of the middle level, each of which has 100 directories of the lower level, each of which has 100 files. </li><li>  We optimize the build time and the average time to read a random file. </li></ol><br><h3>  Iron </h3><br>  First of all, the drive: all experiments are performed on a dedicated Seagate Barracuda ST31000340NS with a capacity of 1 Tb. <br>  The operating system is Windows XP. <br>  Processor: AMD Athlon II X3 445 3 GHz. <br>  Memory: 4 GB. <br><br><h3>  Disk characteristics </h3><br>  Before starting a meaningful job, I want to understand what to expect from the disk.  Therefore, a series of measurements of random positioning and disk readings were made. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/229/b2e/251/229b2e2513f3ea84256fb4b39dca01dc.gif"><br><br>  The presented graph shows histograms of reading times from the disk in arbitrary positions.  The abscissa axis shows the duration of positioning and reading.  The ordinate is the number of hits in a basket 0.1 msec wide. <br><br>  Appeals to the disk were made in a regular manner: <br><br><ul><li>  opening via CreateFile ("\\\\. \\ PhysicalDrive1" ...); </li><li>  positioning via SetFilePointer; </li><li>  and reading through ReadFile. </li></ul><br>  Several series of measurements were made, 10,000 in each series.  Each series is marked by its color: <br><br><ul><li>  all - positioning goes around the disk; </li><li>  ¬Ω - only in its younger half, etc. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/039/e67/2ae/039e672ae2b250c2e5eb564b60bc5305.gif"><br><br>  And here are the mat.  expectations and errors from the previous graph, the x-axis on a logarithmic scale, for example, 1/16 corresponds to 4 (1/2 ** 4). <br><br>  What conclusions can be made? <br><br><ol><li>  In the worst case (random search), the expectation of a single reading is 13 msec.  That is, more than 80 random reads per second from this disk can not be done. </li><li>  At 1/256 we see the cache hit for the first time. </li><li>  At 1/1024, a noticeable number of readings of ~ 1% begins to fall into the cache. </li><li>  1/256 doesn‚Äôt differ from 1/1024 anymore, the run-through of the heads is already too small, we see only their overclocking and (mostly) calm. </li><li>  We have the ability to scale, that is, it is permissible to carry out experiments on a partially filled disk and quite confidently extrapolate the results. </li></ol><br><h3>  NTFS </h3><br>  Let's try to create a file the size of a disk and get the temporary characteristics of working with it.  To do this, create a file commensurate with the size of the file system and read it in random places with regular <i>_fseeki64 / fread</i> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e55/a57/1b2/e55a571b2c932b9f73351e37bf761b7f.gif"><br><br>  The graph similar to the previously presented histograms.  And two of them, marked as (disc), are taken from the previous chart.  Paired histograms, labeled as (file), are derived from measurements of file readings.  What can be noted: <br><br><ol><li>  Times are close. </li><li>  Although the file is 90% of the file system (9xE11 bytes), reading from it at full size is, on average, almost a millisecond slower. </li><li>  Disk cache for a file works better than for a disk. </li><li>  Tails appeared at ~ 6 msec, if the worst time for full reading was about 24 msec, now it stretches to almost 30. Similarly, for 1/256.  Apparently, the metadata fell out of the cache and they have to additionally read. </li><li>  But in general, reading from a large file works very much like working with a bare disk, and in some cases can replace or emulate it.  This makes NTFS honored, because  for example, the ext2 / ext3 structure requires a minimum of 4-byte number for every 4-Kb data block: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5cc/e69/523/5cce69523fc80ce348294063c1c1c8ae.gif"><br></li></ol><br>  That is, it is necessary to spend an extra byte for every kilobyte of data, a gigabyte goes to a terabyte, and it's not so easy to cache a gigabyte.  However, ext4 seems to have no such problem, or it is masked with extents. <br><br><h3>  NTFS with hierarchy </h3><br>  Let us recall about our test problem and try to solve it in the forehead with the help of the file system.  It immediately turns out that: <br><br><ol><li>  It takes a lot of time, it takes about half an hour to create one branch out of a million files, with a tendency to gradually slow down as the disk is filled in accordance with our first schedule. </li><li>  Creating all 100 million files would therefore take 50+ hours. </li><li>  Therefore, we will take measurements on 7 million files (~ 1/16) and scale to the complete task. </li><li>  Attempting to make measurements in more close to combat conditions led to the fact that NTFS collapsed on the 12th million files with approximately the same diagnostics that the main character received in the film ‚ÄúDeath to her face‚Äù: - ‚ÄúActually, fragmented cervical vertebra, this is a very, VERY bad sign! ‚Äù </li></ol><br>  First, consider the histogram in milliseconds: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/633/7ec/919/6337ec9191dfd8c0c36b6b0be6b7d43f.gif"><br><br>  So, here is a histogram of read times of random files. <br><br><ul><li>  Two series ‚Äî 7 million as 1/16 tasks and 1 million for calibration. </li><li>  In a series of 10,000 measurements, in a basket 0.5 msec. </li><li>  The total time of 1/16 of the test is 7'13 ", that is, the expectation is 43 msec. </li><li>  The total time of 1/100 test is 5'20 '', that is, the expectation is 32 msec. </li></ul><br><br>  And now take a look at the same in terms of conditional seek'ov: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/817/20c/6c3/81720c6c304b5aac19c29d0df69d780d.gif"><br><br><ul><li>  That 1/16, that 1/100, is too large to be effectively cached.  And our files are too small for fragmentation. </li><li>  Therefore, we need to pay 1 for reading the actual data.  Everything else - the internal affairs of the file system. </li><li>  The maxima are at 3.5 (for 1/100) and 4 (for 1/16) seek. </li><li>  And the expectation is generally 4 ... 5. </li><li>  That is, to get to the information about where the file is stored, the file system (NTFS) has to do 3-4 readings.  And sometimes more than 10. </li><li>  In the case of a complete task, one must assume that the situation will not improve. </li></ul><br><br><h3>  Prototype </h3><br>  Let's build a prototype of our read-only filesystem: <br><br><ol><li>  It consists of two files - data and metadata. </li><li>  The data file is simply all the files recorded in a row. </li><li>  Metadata file - In-tree with key - file identifier.  The value is the address in the data file and its size. </li><li>  The file identifier is the coded path to it.  Recall that our data is located in a three-level directory system, where each level is encoded by a number from 0 to 100, and the file itself is similar.  This whole structure is packaged as an integer. </li><li>  The tree is written in the form of 4K pages, primitive compression is applied. </li><li>  In order to avoid fragmentation, these files have to be created in different file systems. </li><li>  Simply appending data to the end of the file (data) is ineffective.  The larger the file, the more it costs the file system (NTFS, WinXP) its extension.  To avoid this non-linearity, you have to pre-create the file of the desired size (900 GB). </li><li>  It takes this preliminary creation of more than 2 hours, all this time it takes to fill this file with zeros, apparently for security reasons. </li><li>  The same time it takes to fill this file with data. </li><li>  Building an index takes about 2 minutes. </li><li>  Recall only 100 million files, each about 8K in size. </li><li>  Index file is 521 MB.  Or ~ 5 bytes per file.  Or ~ 5 bits per kilobyte of data. </li><li>  A variant of the index with more complex compression (Huffman codes) was also tested.  This index occupies 325 MB.  Or 3 bits per kilobyte of data. </li></ol><br>  The following are histograms of times obtained from these data: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/19b/2cb/566/19b2cb56678e9019cfc0e7e05d97af9e.gif"><br><br><ul><li>  ‚ÄúAlone‚Äù - under this label is the data file, located separately from the index file, on another physical disk; </li><li>  "Ogether" - both the data file and the index are located within the same file system on the same physical disk; </li><li>  ‚ÄúIndex only‚Äù - only search for information about the file in the index is performed without reading the actual data; </li><li>  ‚ÄúWith compr.  index "- the same as" together ", but with a compressed index; </li><li>  ‚ÄúCompr.  index only - the same as ‚Äúindex only‚Äù, but with a compressed index; </li><li>  only 10,000 readings, histogram step 0.1 msec. </li></ul><br><h3>  Findings: </h3><br><br><ol><li>  The quality of index compression almost does not affect the speed of work.  CPU time is wasted on the release of pages, but the page cache works better. </li><li>  Reading the index looks like one reading from a file of the appropriate size (see the first chart).  Since 512 MB is a 1/2000 disk, the average seek time should be about 6.5 msec, plus the surcharge for what we read from the file, and not directly from the disk.  Apparently, this is it.  The index tree is three-level, the top two levels are very fast in the cache, and the bottom level of pages is pointless to cache.  As a result, any search in the tree degenerates into a single leaf page reading. </li><li>  If the data file and the index file are located on the same physical disk, the total search time is two random seek for the full distance.  One to read the data and one to go to the index file, which is located either at the very beginning or at the very end of the disk. </li><li>  If the files are separated by different devices, we have one full seek to read the data and one short look in the index.  Given that the index file is relatively small, a natural solution suggests itself - if we need a storage of several (tens) terabytes, we need to prepare a dedicated disk for the index (SSD) and several disks for the data.  It can be designed as a <a href="http://ru.wikipedia.org/wiki/JBOD">JBOD</a> and looks like a regular file system. </li><li>  This scheme has an important feature - guaranteed response time, which is difficult to achieve in conventional file systems, for example, for use in real-time systems. </li></ol><br>  Here is the time to ask - well, we have made out a very special case of a regularly arranged file system (three levels with fixed names).  And what about the usual data, with the usual paths and names? <br><br>  The answer is that nothing will change in principle.  There will still be one main index.  But it will be arranged a little differently. <br><br>  The above file ID (index key) was the encoded path to it.  So it will be, the key is the string full path to the file.  Everything else in meaning - size, position, owner, group, rights ... <br><br>  It is possible that there will be not so many combinations (owner, group, rights) and it will be advantageous to store them with Huffman codes indicating the global table of combinations.  But these are details.  In principle, the situation is no different from the one we have already tried. <br><br>  To find a file, you need to specify its full path.  To view the contents of a directory, you need to specify the path of the directory and select everything that matches the prefix.  To move up the hierarchy, bite off a piece of the path and again look for the prefix. <br><br>  But do file paths take up a lot of space?  Yes, but files in one directory have a common prefix that can be stored only once.  The common suffix is ‚Äã‚Äãalso common.  To understand the amount of data that needs to be stored, a series of experiments with file system paths were performed, which showed that they were compressed by an average of 10 times using bzip2 (BWT + suffix sort + huffman).  Thus, it remains 5-6 bytes per file. <br><br>  In total, you can count on 10-16 bytes per file, or ~ 2 bytes per kilobyte of data (if the average file is 8K).  To a terabyte file, you need a 2 gigabyte index, a regular 64 gigabyte SSD disk, therefore, it can serve 32 terabytes of data. <br><br><h3>  Analogs </h3><br>  Full counterparts and difficult to produce. <br>  For example, <a href="http://en.wikipedia.org/wiki/SquashFS">SquashFS</a> , with zero compression can be used in a similar way.  But this system repeats the traditional structure, it simply compresses files, inodes and directories.  Therefore, all those bottlenecks that we diligently tried to avoid were inherent. <br><br>  An exemplary analogue is the use of a DBMS for data storage.  In fact: we store the contents of files as blobs, and metadata as attributes of a table.  Simple and elegant.  The DBMS itself takes care of how to efficiently place files in the available disk space.  Itself creates and maintains all the necessary indices.  Plus a nice bonus - we can change our data without restrictions. <br>  But: <br><br><ul><li>  Ultimately, the DBMS still accesses the disk, directly or through the file system.  And it is fraught with danger, it is worth overlooking and - bang!  work time has slowed down at times.  But, let's say, we have an ideal DBMS, with which this will not happen. </li><li>  A DBMS cannot compress prefixes just as efficiently simply because it is not designed for a read-only mode. </li><li>  The same can be said about the metadata - in the database they will take up much more space. </li><li>  More space ‚Äî more disk readings ‚Äî slower work. </li><li>  Using hash codes from the file path does not solve the problem.  the path matching the hash code still needs to be stored somewhere just to ensure that the contents of the directory can be viewed.  Everything will result in the fact that it is necessary to reproduce the structure of the file system by means of the DBMS. </li><li>  Files larger than a page will be forced to split the DBMS into parts, and random access to them will be carried out at best for a logarithmic time of file size. </li><li>  If we can simply place all the metadata on a dedicated SSD disk, in the case of a DBMS, this will be much more difficult. </li><li>  You can forget about the guaranteed response time. </li></ul><br><h3>  Transactional </h3><br>  Obviously, the disposable file system is not needed by anyone.  Time to fill a file system of several terabytes may take tens of hours.  During this time, new changes may accumulate, therefore, there must be a method for changing data that is different from a full update.  In this case, it is desirable not to stop work.  Well, this topic is worthy of a separate story, and over time we are going to talk about it. <br><br><h3>  Total </h3><br>  So, a <b>prototype of the</b> read-only file system is presented that is simple, flexible, solves a test problem, and at the same time: <br><br><ul><li>  demonstrates the maximum possible performance on the gland allocated for it; </li><li>  its architecture does not impose objective restrictions on the amount of data; </li><li>  lets you know what you should do to get a general-purpose file system, rather than an odd job </li><li>  makes it clear which way to go in order to make our file system incrementally changeable. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/198564/">https://habr.com/ru/post/198564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../198550/index.html">IBM Zurich told how electronic blood works</a></li>
<li><a href="../198556/index.html">Speech recognition from Yandex. Under the hood of Yandex.SpeechKit</a></li>
<li><a href="../198558/index.html">Nokia World News</a></li>
<li><a href="../198560/index.html">Texas Instruments DLP LightCrafter Pico Projector Reference Design Review</a></li>
<li><a href="../198562/index.html">Beeline - a new word in the mobile Internet?</a></li>
<li><a href="../198566/index.html">Compact Multifunction Video Recorder</a></li>
<li><a href="../198568/index.html">Type safe identifiers and phantom types</a></li>
<li><a href="../198570/index.html">bb-mobile MicrON-3 - pocket Bluetooth-headset for the "shovels"</a></li>
<li><a href="../198572/index.html">Non-player use of Unity3D and Oculus Rift</a></li>
<li><a href="../198574/index.html">Nintendo Discontinued Original Wii Game Consoles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>