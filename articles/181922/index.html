<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Public discussion of the GOST project on digitized audio data compression</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dear users! 

 Continuing the recently started tradition of publishing draft standards developed by our company as part of the activity of the technic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Public discussion of the GOST project on digitized audio data compression</h1><div class="post__text post__text-html js-mediator-article">  Dear users! <br><br><img src="https://habrastorage.org/storage2/8ae/e7a/571/8aee7a5714a11bd86d520062a3a994fb.png" align="left">  Continuing the recently started <a href="http://habrahabr.ru/company/nordavind/blog/183732/">tradition of publishing draft standards</a> developed by our company as part of the activity of the technical committee for standardization <a href="http://nicohrana.ru/tk234.html">TK-234 ‚ÄúAlarm systems and anti-crime protection‚Äù</a> , we present to your attention the standard ‚ÄúSecurity television systems.  Compression of digitized audio data.  General technical requirements and algorithms evaluation methods. <br><br>  We will be extremely grateful for constructive criticism of the project, and all valuable comments and suggestions will be made in the next edition of the standard.  Standard text under the cut. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For a better understanding of the structure of this standard and the general approach, we recommend that you first familiarize yourself with the already accepted <a href="http://vsegost.com/Catalog/52/52236.shtml">standard for compression of digitized <b>video data</b></a> that we developed in 2011. <br><br>  NATIONAL STANDARD OF THE RUSSIAN FEDERATION <br><br><h4>  <b>TV security systems.</b>  <b>Compression of digitized audio data.</b> </h4><br><h5>  Classification.  General technical requirements and algorithms evaluation methods </h5><br><br>  <b>Introduction</b> <br>  Active use in the systems of security television (COT) methods of compression of digitized audio data borrowed from multimedia television applications, has led to the impossibility of carrying out investigative measures, as well as operational functions, using most of the existing COT. <br>  An important distinguishing feature of the compression methods for digitized audio data for a COT is the need to ensure high quality sound in the reconstructed audio data.  This standard allows you to streamline the existing and developed methods of compression of digitized audio data, intended for use as part of anti-crime protection systems. <br>  As a criterion for the classification of compression algorithms for digitized audio data, this standard establishes the values <i>of quality metrics that characterize the degree of deviation of the original and corresponding restored digitalized audio data</i> . <br>  This standard should be applied in conjunction with GOST R 51558-2008 ‚ÄúTelevision security tools and systems.  Classification.  General technical requirements.  Test methods. <br><br>  <b>1 area of ‚Äã‚Äãuse</b> <br>  This standard applies to digital television security systems (hereinafter referred to as COTS) and establishes general technical requirements and methods for evaluating compression algorithms for digitized audio data in a COTS. <br>  This standard applies to compression (decompression) algorithms, regardless of their implementation at the hardware level. <br>  This standard establishes a classification of compression (decompression) algorithms for digitized audio data. <br>  This standard establishes a method for comparing various compression and decompression algorithms for digitized audio data. <br>  This standard is used in conjunction with the standards of GOST R IEC 60065, GOST R 51558, GOST 13699, GOST 15971, GOST R 52633.5-2011 <br><br>  <b>2 Normative references</b> <br>  This standard uses normative references to the following standards: <br>  GOST R 51558-2008 Television security systems and systems.  General technical requirements and test methods <br>  GOST R IEC 60065-2009 Audio, video and similar electronic equipment.  Safety requirements <br>  GOST 13699-91 Recording and playback of information.  Terms and Definitions <br>  GOST 15971-90 Information processing systems.  Terms and Definitions <br>  GOST R 52633.5-2011 Information security.  Information security techniques.  Automatic training of neural network converters biometrics-access code <br><br>  <b>3 Terms and definitions</b> <br>  In this standard, the terms used in accordance with GOST 15971-90, GOST 13699, GOST R 51558, GOST R 52633.5-2011, GOST R IEC 60065-2009, as well as the following terms with the corresponding definitions are used: <br>  1. <b>audio data</b> (audio data), audio signal (audio signal), mono channel audio signal (monophonic audio): an analog signal that carries information about a change in the amplitude of sound over time. <br>  2. multi-channel audio <b>signal</b> (multi-channel audio): an audio signal consisting of combining a certain number of audio signals (channels) that carry information about the same sound;  designed for better sound transmission taking into account spatial orientation. <br>  3. <b>stereo two-channel audio signal</b> (stereophonic audio signal), <b>stereo audio signal</b> (stereo audio signal), <b>two-channel audio signal</b> (stereo audio signal): a multichannel audio signal consisting of two mono channel audio signals. <br>  4. <b>digitized audio data</b> (digitized audio data): data obtained by analog-to-digital conversion of audio data, which is a sequence of bytes in a certain format (WAV or other). <br>  5. <b>Analog-to-digital converter, ADC</b> (Analog-to-digital converter, ADC): a device that converts the input analog audio signal into digitized audio data. <br>  6. Sample rate: sampling frequency of a continuous signal during its analog-to-digital conversion to digitized audio data. <br>  7. <b>digit</b> resolution of ADC (resolution of ADC): the number of bits by which each signal sample is encoded in the ADC process. <br>  8. <b>frame</b> : a fragment of a sound signal with a specified number of values ‚Äã‚Äã(frame length). <br>  9. <b>Format of digitized audio data</b> (digitized audio data format): a representation of digitized audio data, ensuring their processing by digital computing means. <br>  10. <b>Compression (compression) of digitized audio data</b> (audio compression): processing of digitized audio data, designed to reduce their volume. <br>  11. compressed audio data: data obtained by compressing digitized audio data. <br>  12. <b>Compression of digitized audio data with lossy</b> (audio compression): compression of digitized audio data, in which there is a loss of information, and as a result, the recovered (as a result of decompression) digitized audio data differ from the original digitized audio data. <br>  13. <b>Compression of digitized audio data without loss</b> (lossless audio compression): compression of digitized audio data, in which there is no loss of information, and as a result, the restored (as a result of decompression) digitized audio data does not differ from the original digitized audio data. <br>  14. <b>Decompression of compressed audio data</b> (audio decompression): recovery of digitized data from compressed audio data. <br>  15. decoded audio data: data obtained from compressed audio data after decompression. <br>  16. audio encoder (audio encoder): software, hardware or hardware-software tools with which the digitized audio data is compressed. <br>  17. audio decoder: software, hardware, or hardware and software tools used to decompress compressed audio data. <br>  18. audio codec (software codec): a software, hardware or hardware-software module capable of performing both compression and decompression of audio data. <br>  19. Compression ratio: The reduction ratio of digitized audio data as a result of compression. <br>  20. bit rate: an estimate of the amount of compressed audio data expressed in bits, determined for a certain time interval and related to the duration of the selected time interval in seconds. <br>  21. <b>quality of restored audio data</b> (decoded audio data quality): an objective assessment of the compliance of the restored audio data with the original digitized audio data based on the calculated quality metrics. <br>  22. quality metric: analytically determined parameters characterizing the degree of deviation of the recovered audio data from the original digitized audio data. <br>  23. <b>method of evaluating the compression algorithm</b> (method of evaluating compression algorithm): a method for analytically determining the values ‚Äã‚Äãof quality metrics for compliance with the requirements for audio compression algorithms. <br>  24. <b>compression algorithm</b> (compression algorithm): a precise set of instructions and rules, describing the sequence of actions according to which the original audio data is converted to compressed, implemented using an audio encoder. <br>  25. decompression algorithm: a precise set of instructions and rules describing the sequence of actions according to which compressed audio data is converted into recovered, implemented using an audio decoder. <br>  26. time-frequency metric: a quality metric based on comparing the spectrograms of digitized and reconstructed audio data. <br>  27. <b>amplitude-time metric</b> (time-amplitude metric): a quality metric based on comparing digitized and reconstructed audio data in a waveform. <br>  28. <b>resampling the</b> audio signal: changing the sampling rate of the audio signal. <br>  29. <b>psychoacoustic model</b> (psychoacoustics model): a model for compressing lossy audio data using the perception of sound by the human ear. <br>  30. <b>psychoacoustic masking</b> (psychoacoustics masking): hiding under certain conditions one sound by another sound due to the peculiarities of the sound perceived by the human ear. <br>  31. masking threshold: the threshold level of a signal that is not distinguishable by humans due to the effect of psychoacoustic masking. <br>  32. <b>noise</b> (noise): a set of aperiodic sounds of varying intensity and frequency, not carrying useful information. <br>  33. <b>signal spectrum</b> (frequency spectrum): the result of the decomposition of the signal into simple sinusoidal functions (harmonics). <br>  34. <b>discrete Fourier transform, DFT</b> (discrete Fourier transform, DFT): a transformation that associates N samples of a discrete signal with N samples of a discrete signal spectrum <br>  35. <b>Fast</b> Fourier transform algorithm (FFT): an algorithm for quickly calculating a discrete Fourier transform. <br>  36. <b>Spectrogram</b> (spectrogram): a characteristic of the power density of a signal in time-frequency space. <br>  37. <b>window</b> (window function): a weighting function that is used to control the effects caused by the presence of side lobes in the spectral estimates (spreading of the spectrum).  It is convenient to consider the available final data record or the existing final correlation sequence as some part of the corresponding infinite sequence visible through the applicable window. <br>  38. <b>Hanna window transform</b> (short-time Fourier transform with Hann window): DFT with weight function - Hanna window. <br>  39. <b>artificial neural network</b> (artificial neural network, ANN): a mathematical model, as well as its software or hardware implementations, built in a certain sense in the image of a network of nerve cells of a living organism and used to approximate continuous functions.  An artificial neural network consists of an input layer with neurons and an output layer with neurons.  Between these layers is one or more intermediate, hidden, layers with neurons. <br>  40. <b>distorted frame</b> : a frame for which the maximum ratio of noise to masking threshold exceeds 1.5 dB. <br>  41. <b>peak</b> signal-to-noise ratio (peak-to-peak signal-to-noise ratio): the ratio between the maximum possible signal value and the noise power. <br>  42. <b>differentiation</b> (from Latin differentia - the difference) - the selection of the quotient from the general population on some grounds. <br><br>  <b>4 General Specifications</b> <br>  The requirements for the compression of digitized audio data are aimed at assessing the quality of the recovered audio data, which is determined by the quality of each individual sound fragment of the recovered audio data.  The size of the sound fragment is determined in seconds, or the number of digitized values ‚Äã‚Äãwithin the fragment. <br>  The quality of the sound fragment of the recovered audio data is determined by the values ‚Äã‚Äãof the quality metrics that characterize the degree of distortion of the recovered audio data as compared to the original digitized audio data.  The procedure for calculating metrics is given in Chapter 6 of this document. <br>  According to the quality metrics of the recovered audio data, the compression algorithms for digitized audio data belong to one of three classes (see Chapter 5 of this document). <br>  The belonging of the compression algorithm of digitized data to a specific class is determined by the values ‚Äã‚Äãof the quality metrics calculated for it and Table 1 given in Chapter 5. <br><br>  <b>5 Classification of compression algorithms</b> <br>  5.1 To assess the quality of the recovered audio data and classify the compression algorithms, the following quality metrics are used: peak signal-to-noise ratio (PSNR);  waveform difference factor;  a metric based on an objective assessment of audio data from the point of view of human perception (perceptual evaluation of audio quality, PEAQ). <br>  5.2 Classification of compression algorithms of digitized audio data is based on the values ‚Äã‚Äãof quality metrics, which reflect those aspects of changes in digitized audio data after their processing by compression and decompression algorithms that can have a critical impact on the ability to use reconstructed audio data to determine the presence of sound signals, differentiation of sounds and speech. <br>  5.3 Depending on the values ‚Äã‚Äãof the quality metrics calculated during the assessment, the compression algorithms for digitized audio data can be assigned to one of the following classes (see Table 1): <br><ul><li>  Class I - full-featured compression algorithms that ensure the quality of the recovered audio data is indistinguishable from the quality of the original audio data; </li><li>  Class II - compression algorithms that ensure the quality of the recovered audio data, sufficient to establish the presence of sound signals, differentiate sounds, speech and is not inferior in this quality to the original audio data, but distinct from the quality of the original audio data; </li><li>  Class III - compression algorithms that ensure the quality of the recovered audio data, sufficient to establish the presence of sound signals and not inferior to the quality of the original audio data, but interferes with the differentiation of sounds, the understanding of speech. </li></ul><br><img src="https://habrastorage.org/storage2/ca2/90a/252/ca290a25290d4d5121f9ec5cad52cc9d.jpg"><br>  Table 1 - Classification of compression algorithms <br>  5.4 The values ‚Äã‚Äãof the quality metrics are determined for each audio fragment (five seconds long) of the digitized audio data, and as the resultant evaluation the following values ‚Äã‚Äãare chosen: the smallest value for the PSNR and PEAQ metrics;  the largest value for the waveform difference coefficient. <br>  To calculate the PSNR metrics and waveform difference factor, the original and recovered digital audio data must be presented at a sampling frequency of 44,100 Hz, 16 bits of memory per discrete sampling value, and one audio channel.  The length of the sound fragment in five seconds in this case is 220500 digitized values. <br>  To calculate the PEAQ metric, the original and reconstructed digital audio data must be presented at a sampling rate of 48,000 Hz, 16 memory bits per sampled sampling value, and one or two audio channels.  The length of the sound fragment in five seconds in this case is 240,000 digitized values ‚Äã‚Äãfor each channel. <br>  For signals with a frequency other than the required one, you must first perform an oversampling of the audio signal. <br><br><h4>  Methods for evaluating compression algorithms </h4><br>  <b>6.1 General description of assessment methods</b> <br>  The general scheme of work of the CCO in terms of the use of compression and decompression algorithms is presented in Figure 1. <br><img src="https://habrastorage.org/storage2/59e/675/b53/59e675b531ecee674814893d2a325ecf.jpg"><br>  Figure 1 - The general scheme of the work of the TsSOT <br><br>  Analog audio data is subjected to analog-to-digital conversion, which results in digitized audio data with a specific sampling rate and number of bits per discrete digitized value.  On a computer, digitized audio data must be stored in one of the formats for storing digitized audio data. <br>  Digitized audio data is compressed, resulting in compressed audio data. <br>  Compressed audio data is used to store the archive or to transfer over the network, after which they are subjected to decompression.  As a result of the decompression of compressed audio data, recovered audio data is generated, which are used to reproduce the operator and are fed to the input of software modules for analyzing audio data. <br>  In accordance with the general scheme of work of the DSC, the classification of digitized audio data compression algorithms is performed by evaluating the quality metrics of the restored audio data from the original digitized audio data.  Depending on the characteristics of the technical implementation of a particular DSS, there are two methods of assessment: <br>  - based on the separation of digitized audio data; <br>  - based on the separation of audio data. <br>  Before evaluating the quality metrics, both audio signals (source and reconstructed) must be converted to signals with a sampling frequency of 44,100 Hz and 48,000 Hz.  For both frequencies (44100 Hz and 48000 Hz), the number of bits per discrete digitized value must be equal to 16. <br><br>  <b>6.1.1 Algorithm estimation method based on digitized audio data separation</b> <br>  To use this method, the technical implementation of the DSP should allow to obtain digitized audio data before they are processed by compression and decompression algorithms. <br>  The general scheme for implementing the assessment method based on the separation of digitized audio data is presented in Figure 2. <br><img src="https://habrastorage.org/storage2/4b3/9fb/098/4b39fb09842538e7c2f2a4f50788017e.jpg"><br>  Figure 2 - General scheme of the implementation of the evaluation method based on the separation of digitized audio data <br>  The evaluation algorithm is performed by the following sequence of actions: <br>  - to the input of the tested TsSOT serves the sequence of audio data; <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- using the capabilities of the CCTV, digitized and restored audio data is stored on storage devices; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- calculate the quality metrics values ‚Äã‚Äãand classify the compression algorithm according to Table 1. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- calculate the quality metrics values ‚Äã‚Äãand classify the compression algorithm according to Table 1. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6.1.2 Algorithm estimation method based on the separation of audio data</font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The evaluation method based on the separation of audio data should be used only in if the technical implementation of the DCSF does not allow for the application of an evaluation method based on the separation of digitized audio data. The use of this method requires the presence of an additional DSP in the test bench, which is designed to store digitized audio data.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The general scheme for implementing an assessment method based on audio data separation is presented in Figure 3. </font></font><br><img src="https://habrastorage.org/storage2/fe7/4af/47f/fe74af47fc9899d3454504897b345244.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Figure 3 - The general scheme for implementing an assessment method based on an </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">algorithm for evaluating this method involves the following actions: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- Serial audio data that is duplicated to another TsOTL is input to the tested DSC using another audio signal divider (from the test bench); </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- using the capabilities of the CCTV, the recovered audio data is stored on storage devices; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- using the capabilities of the DSC from the test bench, the digitized audio data is stored on the storage devices; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">- perform the calculation of the values ‚Äã‚Äãof the quality metrics and carry out the classification of the compression algorithm according to Table 1.</font></font><br><br><div class="spoiler">  <b class="spoiler_title">6.2.</b> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">PEAQ calculation algorithm</font></font></b> <div class="spoiler_text">                ( ).        ITU-R BS 1387.1. <br> <b>   :</b> <br> ‚Ä¢   (  )    PEAQ      48 .      48       ; <br> ‚Ä¢       (        ). <br><br> &gt; <b></b> <br><img src="https://habrastorage.org/storage2/aba/886/1fd/aba8861fd25369a767efbdcef3666761.jpg"> ‚Äî   ; <br><img src="https://habrastorage.org/storage2/81e/111/345/81e111345e0483c99d81270d18cf73c9.jpg"> ‚Äî    ,     ( ); <br><img src="https://habrastorage.org/storage2/f60/366/7b2/f603667b2808f7f205806028ece409d2.jpg"> ‚Äî   , <img src="https://habrastorage.org/storage2/fff/209/210/fff2092104062712cf401a4fa797a40c.jpg"><br>   : <img src="https://habrastorage.org/storage2/756/707/27f/75670727ff462863a75ae3df21b1e982.jpg"> ,  ,    50%; <br><img src="https://habrastorage.org/storage2/74c/d83/a12/74cd83a126df44b5ec665904fc83c459.jpg"> ‚Äî       ; <br><img src="https://habrastorage.org/storage2/78b/019/601/78b01960106698cdec2711386a929cf5.jpg"> ‚Äî    . <br><br><div class="spoiler"> <b class="spoiler_title">     5 .</b> <div class="spoiler_text"><script type="text/javascript">function gtElInit() {var lib = new google.translate.TranslateService();lib.translatePage('ru', 'en', function () {});}</script><script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=gtElInit&amp;client=wt"></script> <b>I Preprocess Signal Processing</b> <br>  <b>Applying a window transform</b> The original digitized data is divided into frames.  The digitized data of each frame is subjected to Hann's scaled window transformation using the formula (2).  Hanna's window function is: <br><img src="https://habrastorage.org/storage2/daa/3a0/98a/daa3a098a3da8a47ddc119abd0c2fc8b.jpg">  (one) <br>  Scaled version of the Hanna window function: <br><img src="https://habrastorage.org/storage2/d38/305/ba8/d38305ba826273ef340963fa29f2b208.jpg">  (2) <br>  The transition to the frequency domain is carried out by applying the <i>discrete Fourier transform</i> (DFT): <br><img src="https://habrastorage.org/storage2/a74/fac/00e/a74fac00e13fca62e3c814535f089151.jpg">  (3) <br><br>  <b>Model of the outer and middle ear</b> <br>  The frequency response of the outer and middle ear should be calculated using the following formula: <br><img src="https://habrastorage.org/storage2/0d2/124/4dc/0d21244dcd2663de559cb40d4aeac80d.jpg">  (four) <br>  By the formulas (4) the weight coefficient vector is calculated as follows: <br><img src="https://habrastorage.org/storage2/309/463/8e3/3094638e3ba1cc2f24d5b17ac4947edb.jpg">  (five) <br><br>  Using these weights (5), the <i>weighted DFT energy is</i> calculated: <br><img src="https://habrastorage.org/storage2/a02/ea3/741/a02ea37414292e0d725aac12136b556e.jpg">  (6) <br>  <b>Decomposition of the critical hearing band</b> <br>  Below are the formulas required for the transformation to the Bark scale (7) and the inverse transformation (8): <br><img src="https://habrastorage.org/storage2/1e9/0b5/fb9/1e90b5fb9df8e6a783d3b9b029a67217.jpg">  (7) <br>  where z is measured in Barks. <br><img src="https://habrastorage.org/storage2/8c6/da4/f25/8c6da4f25daf32c264fa69a7d574f1ae.jpg">  (eight) <br>  <i>Frequency bands</i> <br>  Frequency bands are determined by specifying the lower, center and upper frequencies of each band.  These values ‚Äã‚Äãin the Bark scale are given as follows: <br><img src="https://habrastorage.org/storage2/bb0/2a6/c7e/bb02a6c7e61588b29153ef21ff01d57b.jpg">  (9) <br>  The inverse transformation is performed by the following formulas: <br><img src="https://habrastorage.org/storage2/288/0ad/fcf/2880adfcf244194a3ee18c53cf9e3e15.jpg">  (ten) <br>  The value of i = 1, 2, ..., <img src="https://habrastorage.org/storage2/78b/019/601/78b01960106698cdec2711386a929cf5.jpg">  . <br>  <i>Bandwidth energy</i> <br>  For the i-th frequency band, the energy contribution from the k-th fundamental frequency of the DFT is calculated by the following formula: <br><img src="https://habrastorage.org/storage2/74c/04a/b23/74c04ab23c2001b197b78365a83bf9df.jpg">  (eleven) <br>  Then the energy of the i-th frequency band is equal to: <br><img src="https://habrastorage.org/storage2/f78/03f/975/f7803f975c38f1113af3ef9ea16df7e1.jpg">  (12) <br>  Below is the final formula for the energy of the i-th frequency band: <br><img src="https://habrastorage.org/storage2/4af/9d9/145/4af9d9145659742bd45afb1d4f26000c.jpg">  (13) <br>  <b>Internal ear noise</b> <br>  To compensate for the internal noise in the ear itself, we introduce a surcharge value for the energy of each frequency band: <br><img src="https://habrastorage.org/storage2/a2e/63e/bf7/a2e63ebf77caf9cbfad8321a965008b3.jpg">  (14) <br>  where the internal noise is modeled as follows: <br><img src="https://habrastorage.org/storage2/04b/853/8a5/04b8538a572144a699031f7d85d71cd9.jpg">  (15) <br>  Energies <img src="https://habrastorage.org/storage2/933/cff/b79/933cffb799d628dc0fa745f94254f434.jpg">  will be called further <i>images of height</i> . <br>  <b>Distribution energy within one frame</b> <br>  The characteristic of the propagation energy in the Bark scale is calculated as follows: <br><img src="https://habrastorage.org/storage2/72c/a40/cab/72ca40cab1c8818a373c7b5cd131ae3b.jpg">  (sixteen) <br>  Where <br><img src="https://habrastorage.org/storage2/ee4/829/ded/ee4829ded0e0ad056dd8f134b6b0a972.jpg">  (17) <br>  The function <i>S (i, l, E)</i> has the following form: <br><img src="https://habrastorage.org/storage2/5f7/69f/d41/5f769fd419dca9589db68f6b74238ab6.jpg">  (18) <br>  Where <br><img src="https://habrastorage.org/storage2/ea8/938/4b9/ea89384b9b6b947545f676ec39e8bdb3.jpg">  (nineteen) <br>  Below are the formulas for calculating the terms <img src="https://habrastorage.org/storage2/9e2/b91/94d/9e2b9194db3d6e61257f3ddd24c13f14.jpg">  and <img src="https://habrastorage.org/storage2/219/6d0/803/2196d0803b436874c95cc592b3f8d995.jpg">  : <br><img src="https://habrastorage.org/storage2/611/22c/3b3/61122c3b347814164d3f044830524f2a.jpg">  (20) <br>  and <br><img src="https://habrastorage.org/storage2/8ed/d3f/4b5/8edd3f4b5159ce6a430169a863baae89.jpg">  (21) <br>  Energies <img src="https://habrastorage.org/storage2/02a/183/39f/02a18339f12fc968156a5338f9a60b82.jpg">  - <i>images of uncommon excitations</i> . <br>  <b>Energy filtration</b> <br>  Let n be the frame index (frames are indexed starting from n = 0).  Then the energy of the n-th frame corresponding to formula (16) is denoted as: <img src="https://habrastorage.org/storage2/02a/183/39f/02a18339f12fc968156a5338f9a60b82.jpg">  Energy filtration is performed according to the following formula: <br><img src="https://habrastorage.org/storage2/88c/7a5/be7/88c7a5be7e380e294bad6b31a94a2ab2.jpg">  (22) <br>  Where <img src="https://habrastorage.org/storage2/f3f/e21/ce7/f3fe21ce76304868bb966db5afd2b2f1.jpg">  - time constant for extinct energy.  Initial filtering condition: <img src="https://habrastorage.org/storage2/170/a6b/6e6/170a6b6e6c196ce4d6c58acddac96ce8.jpg"><br>  End values <img src="https://habrastorage.org/storage2/ba5/c94/6d4/ba5c946d4abb438533230bc69ef1801f.jpg">  - <i>images of excitement</i> . <br>  <i>Time constants</i> <br>  The time constant for filtering the i-th band is calculated as follows: <br><img src="https://habrastorage.org/storage2/60a/3d4/81f/60a3d481f07e3e979f7a39a5c62addee.jpg">  (23) <br><img src="https://habrastorage.org/storage2/f3f/e21/ce7/f3fe21ce76304868bb966db5afd2b2f1.jpg">  can be calculated as: <br><img src="https://habrastorage.org/storage2/408/ea4/849/408ea4849c27e2f0aca98c5d797071cd.jpg">  (24) <br><br>  <b>Ii.</b>  <b>Image processing</b> <br><br>  The figure 4 below shows the scheme of preliminary calculations described in the previous chapter. <br><img src="https://habrastorage.org/storage2/ca7/917/b1e/ca7917b1ee99fbe42113fc2c4271a8a6.jpg"><br>  Figure 4 Signal preprocessing circuit <br>  The indices R and T denote the original and recovered audio signals, respectively.  The index k denotes the index of the frequency band (total frequency bands - 109), and the index n - the frame number.  For recurrent formulas at this and the next stage (stage III), zero initial conditions are always chosen. <br>  <b>Processing images of excitations</b> <br>  The inputs for this stage of the calculation are the images of the excitations <img src="https://habrastorage.org/storage2/6f5/06e/24e/6f506e24e492566e21576914d7e7d63a.jpg">  and <img src="https://habrastorage.org/storage2/a20/8a9/128/a208a9128dc430abaeb2731d2522c467.jpg">  calculated by the formula (22) for the original and tested audio signals, respectively. <br>  <i>Correction images of arousal</i> <br>  First, filtering is performed for both audio signals by the formula: <br><img src="https://habrastorage.org/storage2/fde/34e/50f/fde34e50f686cea7322514646f4e0de9.jpg">  (25) <br>  Time constant <img src="https://habrastorage.org/storage2/f3f/e21/ce7/f3fe21ce76304868bb966db5afd2b2f1.jpg">  calculated by formulas (23) and (24), but with <img src="https://habrastorage.org/storage2/ca9/3c1/572/ca93c1572ca87788dceec92b2794e7a4.jpg">  , <img src="https://habrastorage.org/storage2/435/1b2/9be/4351b29be73d6da27d6cd8fdc9306b87.jpg">  .  The initial condition for filtering is set to 0. <br>  Next, <i>the correction factor is</i> calculated: <br><img src="https://habrastorage.org/storage2/dca/534/6f1/dca5346f11fc8703ed9d7c9b00b844c3.png">  (26) <br>  Images of excitations are adjusted as follows: <br><img src="https://habrastorage.org/storage2/9fe/fea/9e1/9fefea9e1e05d2d4e0c314f2ade7ed7a.jpg">  (27) <br>  <i>Adaptation of excitation images</i> <br>  Using the same time constants and initial conditions as in the correction of the images of excitations, the output signals calculated by the formula (27) are smoothed in accordance with the following formulas: <br><img src="https://habrastorage.org/storage2/95a/ef7/a6b/95aef7a6bc8e1167266a2921572c8330.jpg">  (28) <br>  Based on the ratio between the values ‚Äã‚Äãcalculated in (28), a pair of auxiliary signals is calculated: <br><img src="https://habrastorage.org/storage2/104/7f4/fb4/1047f4fb4cb27ab83c74be255ccf19c9.jpg">  (29) <br>  If in the previous formula (29) the numerator and denominator are equal to zero, then it is necessary to perform the following actions: <img src="https://habrastorage.org/storage2/374/aa0/2d7/374aa02d7ae650280204140fcdbddc8f.jpg">  . <br>  If k = 0, then <img src="https://habrastorage.org/storage2/fb4/b8f/5ee/fb4b8f5ee62235a77fe28a0d5f9e24e5.jpg"><br>  For the formation of factors for image correction, the auxiliary signals are filtered, using the same time constants and initial conditions as in (25): <br><img src="https://habrastorage.org/storage2/ea8/e6f/504/ea8e6f50424fad04b6298f5c4c04347d.jpg">  (thirty) <br>  Where <br><img src="https://habrastorage.org/storage2/5d6/cbb/847/5d6cbb847338d458e12fcbb46fa09230.jpg">  (31) <br><img src="https://habrastorage.org/storage2/20e/910/406/20e91040641f35172d021583ef96aad0.jpg">  (32) <br>  As an end result of this stage of processing, on the basis of formula (30), <i>spectrally adapted images</i> are obtained: <br><img src="https://habrastorage.org/storage2/acf/8f0/0f2/acf8f00f2f5a038553d583ade43af2e8.jpg">  (33) <br><br>  <b>Processing modulation images</b> <br>  The inputs to this stage of the computation are images of unpredicted excitations. <img src="https://habrastorage.org/storage2/fba/0ef/16e/fba0ef16e128a9f18eb9dc9365c31d9b.jpg">  and <img src="https://habrastorage.org/storage2/41c/8a6/d23/41c8a6d23da4d579fe1c247a98550fd7.jpg">  calculated by the formula (16) for the original and tested audio signals, respectively.  The purpose of this section is to calculate the <i>modulation measures for the spectral envelopes</i> . <br>  First, the <i>average volume is</i> calculated: <br><img src="https://habrastorage.org/storage2/0a6/c03/a38/0a6c03a38bf5c0de4dda167f73f22b36.jpg">  (34) <br>  Next, you need to calculate the following differences: <br><img src="https://habrastorage.org/storage2/11d/f4c/536/11df4c53690a611a1e1d2916cf3c156a.jpg">  (35) <br>  The time constants and initial conditions are the same as in the previous section. <br>  The modulation measures of the spectral envelopes are calculated as follows: <br><img src="https://habrastorage.org/storage2/edf/ba1/fe9/edfba1fe938958f355d8507ebcfdea59.jpg">  (36) <br>  <b>Volume calculation</b> <br>  <i>Volume images</i> are calculated according to the following formulas: <br><img src="https://habrastorage.org/storage2/063/1de/97f/0631de97f1c913e5450b748456b0bf0a.jpg">  (37) <br>  Where <br><img src="https://habrastorage.org/storage2/787/e17/21a/787e1721a2469097347bc81645c29877.jpg">  (38) <br>  and <br><img src="https://habrastorage.org/storage2/8be/88b/0ab/8be88b0ab863099da3e8e9c6c638d5a8.jpg">  (39) <br>  The parameter c = 1.07664. <br>  <i>The total volumes</i> for both signals are calculated as follows: <br><img src="https://habrastorage.org/storage2/3c0/0b1/6ad/3c00b16adbeaff7e2c3821c03cfbdd75.png">  (40) <br><br>  <b>Iii.</b>  <b>Calculation of the output values ‚Äã‚Äãof the psychoacoustic model</b> <br>  The output characteristics from chapter I are used to calculate the output characteristics of chapter II in accordance with the diagram below (see figure 5). <br><img src="https://habrastorage.org/storage2/504/716/995/504716995978ca86fd0998a14cdbe3ca.jpg"><br>  Figure 5 Image Processing Scheme <br>  In turn, the values ‚Äã‚Äãof the previous chapter (II) are used to calculate the output values ‚Äã‚Äãof the variables of the psychoacoustic model (see table 1 and figure 6). <br><img src="https://habrastorage.org/storage2/450/27d/257/45027d2578ef2963f7e85836a1bc37e6.jpg"><br>  Figure 6 Scheme for calculating the output variables of the psychoacoustic model <br>  In total, the values ‚Äã‚Äãof 11 variables of the psychoacoustic model are calculated.  They are listed in Table 2. <br><img src="https://habrastorage.org/storage2/130/cbc/930/130cbc9309d227ddc5c34a1fb16b43f7.jpg"><br>  Table 2. Output variables of the psychoacoustic model <br>  For two-channel audio signals, the variable values ‚Äã‚Äãfor each channel are calculated separately, and then averaged.  The values ‚Äã‚Äãof all variables (except for the values ‚Äã‚Äãof the ADBB and MFPDB variables) for each signal channel are calculated independently of the second channel. <br>  <b>General description of the process of calculating parameters</b> <br>  All values ‚Äã‚Äãof the output variables of the model are obtained by averaging over all frames of the functions of time and frequency obtained at the previous step (as a result, a scalar value). <br>  The values ‚Äã‚Äãto be averaged must lie within the limits determined by the following condition: the beginning or end of the data to be averaged is defined as the first position from the beginning or from the end of the sequence of amplitudes of the audio signal, for which the sum of five consecutive absolute values ‚Äã‚Äãof amplitudes exceeds 200 volts any of the audio channels.  Frames that lie outside these bounds should be ignored when averaging.  The threshold value 200 is used in case the amplitudes of the input audio signals are normalized in the range from -32,768 to + 32767.  Otherwise, the threshold value <img src="https://habrastorage.org/storage2/299/fd6/d6f/299fd6d6f7f1c5ccc089427225f993fb.jpg">  is calculated as follows: <br>  (41) <br>  Where <img src="https://habrastorage.org/storage2/a3e/428/d11/a3e428d11529d7c988e9d0bc40df6ea1.jpg">  - the maximum amplitude of the audio signal. <br>  Further, the frame index n: starts from zero for the first frame that satisfies the conditions for checking borders with a threshold <img src="https://habrastorage.org/storage2/299/fd6/d6f/299fd6d6f7f1c5ccc089427225f993fb.jpg">  and counts the number of frames N up to the last frame satisfying the above mentioned condition. <br>  <b>Modulation window difference 1 (WinModDiff1B)</b> <br>  Below is the formula for calculating the <i>instantaneous modulation difference</i> : <br><img src="https://habrastorage.org/storage2/fb5/a2a/b33/fb5a2ab339acf94ee5062054b72f4d9c.png">  (42) <br>  The value of the instantaneous modulation difference is averaged over all frequency bands. <img src="https://habrastorage.org/storage2/5d5/2b9/2fa/5d52b92fa464f1de4ed3993c3d209768.jpg">  in accordance with the following formula: <br><img src="https://habrastorage.org/storage2/5f0/68c/6cc/5f068c6cca1e00c3114df4d1e7af6e81.png">  (43) <br>  The final value of the output variable is obtained by averaging formula 43 with a sliding window L = 4 (85 ms, since each step is equal to 1024 digitized values): <br><img src="https://habrastorage.org/storage2/7f0/847/c0d/7f0847c0daa150a0121186873b201559.jpg">  (44) <br>  In this case, the so-called <b>delay averaging</b> is used - the first 0.5 seconds of the signal does not participate in the calculations.  The number of skipped frames is: <img src="https://habrastorage.org/storage2/99d/a76/10e/99da7610e272fc7b8edeb07406207cd8.jpg">  (45) <br>  In formula 45, operation denotes discarding the fractional part. <br>  Thus, in the formula 44, the frame index includes only frames that go after a delay of 0.5 seconds. <br>  <b>Average Modulation Difference 1 (WinModDiff1B)</b> <br>  The value of this output variable of the psychoacoustic model is calculated by the following formula: <br><img src="https://habrastorage.org/storage2/e30/021/cbd/e30021cbd1529e89fc1b3a94a4554d96.png">  (46) <br>  Where <br><img src="https://habrastorage.org/storage2/02d/404/a6f/02d404a6f177890c570c80210afbf62d.png">  (47) <br>  Delay averaging is also used to calculate this value. <br>  <b>Average Modulation Difference 2 (WinModDiff2B)</b> <br>  First, the value of the instantaneous modulation difference is calculated by the formula: <br><img src="https://habrastorage.org/storage2/643/e57/788/643e5778859de7a08e6bdf3f28ce471a.jpg">  (48) <br>  Then, the modulation difference averaged over the frequency bands is calculated: <br><img src="https://habrastorage.org/storage2/69e/7e6/776/69e7e67763a097646a418249eabd5dca.png">  (49) <br>  The final variable value of the psychoacoustic model is calculated as follows: <br><img src="https://habrastorage.org/storage2/29c/19d/ef6/29c19def64da9822e49ba06144880218.png">  (50) <br>  Where <br><img src="https://habrastorage.org/storage2/7ca/a9b/93d/7caa9b93da92adc07eaa9c705e884c56.jpg">  (51) <br>  Delay averaging is also used to calculate this value. <br>  <b>Noise Volume (RmsNoiseLoudB)</b> <br>  Below is the formula for finding the values ‚Äã‚Äãof the instantaneous volume of noise: <br><img src="https://habrastorage.org/storage2/1fd/deb/e06/1fddebe060e46df3e90db76d217aa758.png">  (52) <br>  Where <br><img src="https://habrastorage.org/storage2/035/361/208/035361208465311672868c037d2ddba7.jpg">  (53) <br>  Where: <br><img src="https://habrastorage.org/storage2/f7b/971/5fb/f7b9715fbfc14685882f20c175c7832f.jpg">  (54) <br><img src="https://habrastorage.org/storage2/c8e/b0e/910/c8eb0e910f201b4b88ff354dd1bb390e.png">  (55) <br><img src="https://habrastorage.org/storage2/7c6/1d1/880/7c61d18804706baf78062a206b33bd86.jpg">  (56) <br>  but <br>  Further, if the instantaneous volume is less than 0, then it is set to 0: <br><img src="https://habrastorage.org/storage2/edd/60e/422/edd60e422b809443379cab9029dc9b59.jpg">  (57) <br>  The value of the final output variable of the psychoacoustic model is averaged by the instantaneous volume: <br><img src="https://habrastorage.org/storage2/d2e/a0a/04e/d2ea0a04e2fc8e8d0e0ba2b15ce55e3e.jpg">  (58) <br>  Delay averaging is used to calculate this value.  Together with averaging with a delay, the volume threshold is used to find the value of the instantaneous volume of the noise from which the averaging process starts.  Thus, averaging starts from the first value determined by the condition of exceeding the volume threshold, but no later than 0.5 seconds from the beginning of the signal (in accordance with averaging with a delay). <br>  <i>The condition for exceeding the threshold volume</i> <br>  The instantaneous loudness values ‚Äã‚Äãof the noise at the beginning of both signals (source and test) are ignored until 50 ms passes after the total volume exceeds one of the signals in one of the signals, the threshold value is 0.1. <br><br>  The condition of exceeding the threshold can be represented as: <br><img src="https://habrastorage.org/storage2/fd3/d64/9c9/fd3d649c9852497339f8d26d1d34b24a.jpg">  (59) <br>  The following formula is used to calculate the number of frames to be skipped after exceeding the threshold: <br><img src="https://habrastorage.org/storage2/8f4/e34/411/8f4e34411f630c4920b5b2dbb2f7f4e4.jpg">  (60) <br><br>  <b>The bandwidth of the original and restored audio signals (BandwidthRefB and BandwidthTestB)</b> <br>  The operations of calculating the bandwidths of the original and recovered audio signals are described in terms of operations on the output values ‚Äã‚Äãof the DFT, expressed in decibels (dB).  First of all, for each frame the following operations are performed: <br>  ‚Ä¢ For recovered signal: the largest component is located after the 21.6 kHz frequency.  This value is called the threshold level. <br>  ‚Ä¢ For source signal: performing a down search starting at 21.6 kHz, the first value is found, which is 10 dB above the threshold level.  The frequency corresponding to this value is called the bandwidth for the original signal. <br>  ‚Ä¢ For recovered signal: performing a search downwards, starting with the bandwidth value of the original signal, is the first value that exceeds the threshold level value by 5 dB.  We denote the frequency corresponding to this value as the bandwidth for the recovered signal. <br>  If the found frequencies for the original signal do not exceed 8.1 kHz, then the bandwidth for this frame is ignored. <br>  The bandwidths for all frames are called the <i>DFT base frequencies.</i> <br>  The basic frequency of the DFT for the nth frame is denoted as <img src="https://habrastorage.org/storage2/2f7/cf6/9f4/2f7cf69f4954434cd669591f8c4a534b.jpg">  for the original signal and how‚Äî <img src="https://habrastorage.org/storage2/85e/73c/ddd/85e73cddde7e707a6fb95259eb73ea2c.jpg">  for the recovered signal.  To calculate the final values ‚Äã‚Äãof the psychoacoustic model variables, the widths of the bands of the original and restored signals, it is necessary to perform the following formulas, respectively: <br><img src="https://habrastorage.org/storage2/784/553/2c1/7845532c1cf8df5f9fa5b93544a8a795.png">  (61) <br><img src="https://habrastorage.org/storage2/ad0/430/2ee/ad04302eeebebae3384aea64da4f1ec3.jpg">  (62) <br>  where the summation is carried out only for those frames in which the main frequency of the DFT exceeds 8.1 kHz. <br>  <b>The ratio of the noise level to the masking threshold (Total NMRB)</b> <br>  <i>The masking threshold is</i> calculated using the following formula: <br><img src="https://habrastorage.org/storage2/f00/1dc/acb/f001dcacb008721e69cba296bac8f834.jpg">  (63) <br>  Where <br><img src="https://habrastorage.org/storage2/ba2/060/8f2/ba20608f226321ed0a71a123a6a4a457.png">  (64) <br>  The <i>noise</i> level <i>is</i> calculated as follows: <br><img src="https://habrastorage.org/storage2/213/7d1/235/2137d123510de422708527a4d7bd872b.jpg">  (65) <br>  where k denotes the index of the fundamental frequency of the DFT. <br>  <i>The ratio of the noise level to the masking threshold</i> in the k-th frequency band is expressed by the following formula: <br><img src="https://habrastorage.org/storage2/5b3/9a7/ea6/5b39a7ea6ecd8ae6eb91d77320276508.png">  (66) <br>  The final ratio of the noise level to the masking threshold (in decibels) is calculated as: <br><img src="https://habrastorage.org/storage2/ec3/9de/752/ec39de7525ad26d586d449f4a0a27660.jpg">  (67) <br>  <b>Frame relative distortion (RelDistFramesB)</b> <br>  <i>The maximum ratio of noise to the frame masking threshold is</i> calculated as follows: <br><img src="https://habrastorage.org/storage2/97b/4c5/26e/97b4c526e8bdbf638b5b115eb749d753.jpg">  (68) <br>  Distorted is considered to be the frame in which the maximum ratio of noise to the masking threshold exceeds 1.5 dB. <br>  The final value of the output variable of the psychoacoustic model is the ratio of the number of distorted frames to the total number of frames. <br><br>  <b>Maximum probability of detecting distortion (MFPDB)</b> <br>  First of all, let's calculate <i>asymmetric excitation:</i> <br><img src="https://habrastorage.org/storage2/b82/35f/514/b8235f514e89c28f81d0acf5de8cf622.jpg">  (69) <br>  Where <br><img src="https://habrastorage.org/storage2/363/1e9/8c1/3631e98c18fc740767b0dce97afd4dca.png">  (70) <br>  Next, a <i>step is</i> calculated <i>to detect the distortion</i> : <br><img src="https://habrastorage.org/storage2/b3e/ba6/ed9/b3eba6ed935d4ca2d4c631bebe97bb55.jpg">  (71) <br>  Where <br><img src="https://habrastorage.org/storage2/c56/006/f4a/c56006f4a30461ec3be3efecb4ee2f39.jpg">  (72) <br>  <i>The probability of detection is calculated as follows:</i> <br><img src="https://habrastorage.org/storage2/476/b25/b55/476b25b5541dfcb782f4168e8bdce145.png">  (73) <br>  where b is calculated as: <br><img src="https://habrastorage.org/storage2/016/d7f/7f3/016d7f7f33a9aa8eb782f7b8b549e82a.png">  (74) <br>  We calculate the <i>number of steps above the detection probability threshold:</i> <br><img src="https://habrastorage.org/storage2/9ae/e5e/db6/9aee5edb69f7d65756daf23e3f08c180.jpg">  (75) <br>  Characteristics (73) and (75) are calculated for each channel of the signal.  For each frequency and time, the <i>total detection probability and the total number of steps above the threshold</i> are selected as the larger value from all channels: <br><img src="https://habrastorage.org/storage2/9fc/518/065/9fc518065ffa5e948ead0b5953f4921a.jpg">  (76) <br>  where indices 1 and 2 denote the channel number. <br>  For single-channel signals, the above values ‚Äã‚Äãare calculated as: <br><img src="https://habrastorage.org/storage2/2f2/668/102/2f2668102fdfd428e642ede65984fd80.png">  (77) <br>  The following computational procedure is performed: <br><img src="https://habrastorage.org/storage2/3da/117/014/3da11701475b76a6ca1cd95409730d60.png">  (78) <br>  Where <img src="https://habrastorage.org/storage2/ccb/904/fbf/ccb904fbf691bc36fc3293fea0551722.jpg">  and the initial condition is zero. <br>  <i>The maximum probability of detecting distortion is</i> calculated by the recurrence formula: <br><img src="https://habrastorage.org/storage2/610/180/edd/610180edd6c514acc299dd3b36391906.png">  (79) <br>  The final value of the output variable of the psychoacoustic model is calculated as follows: <br><img src="https://habrastorage.org/storage2/9b3/902/3b3/9b39023b32493ae21fe31788c94e7808.png">  (80) <br>  <b>Average block distortion (ADBB)</b> <br>  First, the <i>sum of the total number of steps above the detection threshold is</i> calculated: <br><img src="https://habrastorage.org/storage2/598/fdd/dd8/598fdddd83352e5b75d4fcc9fc3109e4.png">  (81) <br>  Moreover, the summation is carried out for all values ‚Äã‚Äãfor which <img src="https://habrastorage.org/storage2/109/fc3/b7e/109fc3b7e7f38ba0935bafb716cbf491.jpg"><br>  The final characteristic is: <br><img src="https://habrastorage.org/storage2/dd6/31e/e4d/dd631ee4d4d984f1de2fedd360d6170c.jpg">  (82) <br>  <b>Harmonic error structure (EHSB)</b> <br>  The DFT outputs for the original and reconstructed signals are denoted as <img src="https://habrastorage.org/storage2/e7b/e62/640/e7be626404c892d731c48a5d18f787d2.jpg">  and <img src="https://habrastorage.org/storage2/108/5ff/f14/1085fff144f84724abdc6a92f8649881.jpg">  respectively. <br>  Calculate the characteristic: <br><img src="https://habrastorage.org/storage2/c07/b0c/cfc/c07b0ccfcee5857268caf9ad21fdc1f4.jpg">  (83) <br>  A vector of length M is formed from the values ‚Äã‚Äãof D [k]: <br><img src="https://habrastorage.org/storage2/c0a/4b0/dee/c0a4b0dee0813f4d45449c9c9ec97b20.png">  (84) <br>  Normalized autocorrelation is calculated by the formula: <br><img src="https://habrastorage.org/storage2/4c7/0b1/9e3/4c70b19e3ec1deee12cb6c74cb55f74f.png">  (85) <br>  Where <br>  Let ‚ÄîC [l] = C [l, 0].  Next, you need to calculate: <br><img src="https://habrastorage.org/storage2/068/273/b01/068273b019b1c242664f8f1677f52fb5.jpg">  (86) <br>  When calculating (85) in case the signals are equal, it is necessary to set the normalized autocorrelation equal to one in order to avoid dividing by 0. <br>  A window function of the following form is introduced: <br><img src="https://habrastorage.org/storage2/ed4/883/26a/ed488326a291692e7b64e93cda706dce.png">  (87) <br>  The window transform (87) is applied to the normalized autocorrelation: <br><img src="https://habrastorage.org/storage2/351/fea/b9d/351feab9d518a681e0b9b30aff8730ae.jpg">  (88) <br>  Where <br><img src="https://habrastorage.org/storage2/37c/8d5/13c/37c8d513ca4060ee14b074fd830c47d1.png">  (89) <br>  The power spectrum is calculated by the formula: <br><img src="https://habrastorage.org/storage2/b48/17c/084/b4817c0842cc5b1f71edcd5c915f5941.jpg">  (90) <br>  The search for the maximum peak of the power spectrum starts with k = 1 and ends when <img src="https://habrastorage.org/storage2/8fd/9fb/aa9/8fd9fbaa9151a3fa2a6d0e4bb3baae31.jpg">  or <img src="https://habrastorage.org/storage2/fee/689/c4b/fee689c4b1ba76dc6935dca4b975c692.jpg">  The maximum peak value found is denoted as <img src="https://habrastorage.org/storage2/0ca/8c1/65e/0ca8c165e8c78cd296f57155208fdfcc.jpg">  Then the final value of the output variable of the psychoacoustic model is calculated using the following formula: <br><img src="https://habrastorage.org/storage2/acd/e7e/5e6/acde7e5e67f05d7595e16cfb18f96919.png">  (91) <br>  When calculating this value, low energy frames are excluded.  To define low energy frames, a threshold value is entered: <br><img src="https://habrastorage.org/storage2/14a/232/237/14a232237bfbd9d92544d6a53e2e45b1.png">  (92) <br>  Where <img src="https://habrastorage.org/storage2/ef4/f45/383/ef4f4538315416d5048a5e9d110fd6ae.jpg">  for amplitudes stored as a 16-bit integer. <br>  The frame energy is estimated using the following formula: <br><img src="https://habrastorage.org/storage2/af2/705/ca3/af2705ca3a5d1c687d9db0c3b1acecd1.png">  (93) <br>  When calculating the harmonic structure of the error, the frame is ignored if: <br><img src="https://habrastorage.org/storage2/f50/6cb/b9d/f506cbb9da42ecb9b3a9e888f7fdceb9.png">  (94) <br><br>  Iv.  Rationing of the output variables of the psychoacoustic model <br>  The normalization of the output variables of the psychoacoustic model obtained at the previous step is performed in accordance with the following formula: <br><img src="https://habrastorage.org/storage2/ec1/b97/a97/ec1b97a9730355eed49adf6e9fde1be5.png">  (95) <br>  Where <img src="https://habrastorage.org/storage2/662/020/81f/66202081f074abbfe7c4c968d8113a0c.jpg">  - value of the i-th output variable of the psychoacoustic model, values <img src="https://habrastorage.org/storage2/29f/12f/a3d/29f12fa3dd4edccbaf99017344a34964.jpg">  and <img src="https://habrastorage.org/storage2/7a5/d13/70e/7a5d1370eefa6ff2f7e1c5815fbd1628.jpg">  are shown in Table 3 below. <br><img src="https://habrastorage.org/storage2/cc1/cac/bd7/cc1cacbd7fe4e1c6951902aa88a60d1f.jpg"><br>  Table 3. Constants for rationing the values ‚Äã‚Äãof the output variables of the psychoacoustic model <br><br>  <b>V. Evaluation of the quality of the recovered signal using an artificial neural network</b> <br><img src="https://habrastorage.org/storage2/1a0/b25/5df/1a0b255df5079b485518597836717a35.jpg">  (96) <br>  where bmin = ‚àí3.98 and bmax = 0.22, and the function sig (x) is an asymmetric sigmoid: <br><img src="https://habrastorage.org/storage2/23b/5ec/04c/23b5ec04c633e75fd44e5028021469fa.png">  (97) <br>  Value <img src="https://habrastorage.org/storage2/14c/f4a/035/14cf4a03529bf7edc931c9b80c102081.jpg">  is calculated as follows: <br><img src="https://habrastorage.org/storage2/e1d/80e/022/e1d80e022cac8fffaa7ef5567dcc56a8.jpg">  (98) <br>  Where <img src="https://habrastorage.org/storage2/f4c/e66/ded/f4ce66ded9a46a744fd4ad121c2e397f.jpg">  - the normalized value of the i-th output variable, I - the number of output variables (equal to 11), J - the number of neurons in the hidden layer (equal to 3), <img src="https://habrastorage.org/storage2/e07/68e/c6d/e0768ec6d7bde646d27a1e940917bb91.jpg">  - the weights and offsets of the neural network are shown in Tables 4-6 below. <br><img src="https://habrastorage.org/storage2/334/b53/65b/334b5365b0db0f48a0c5ff7deacdacf8.jpg"><br>  Table 4 Neural Network Weights <br>  &lt; <img src="http://habrastorage.org/storage2/5bd/a81/d19/5bda81d198116f564a1341af53cd562b.jpg"><br>  Table 5 Neural Network Offsets <br><img src="https://habrastorage.org/storage2/f1b/d01/111/f1bd011110c604c23c508a3363b2b498.jpg"><br>  Table 6 The weights and displacements of the neural network <br>  This metric value (PEAQ) is a real number belonging to the [-3.98;  0.22]. </div></div></div></div><br><br><div class="spoiler">  <b class="spoiler_title">6.3 Algorithm for calculating PSNR</b> <div class="spoiler_text">  Peak signal to noise ratio between the original audio signal <img src="https://habrastorage.org/storage2/771/1b6/c61/7711b6c619987f8a7ad501fe59eb8999.jpg">  and restored <img src="https://habrastorage.org/storage2/ba3/d2f/56a/ba3d2f56ae71954a7a62de9035bfdcba.jpg">  calculated by the formulas: <br><img src="https://habrastorage.org/storage2/090/ce7/625/090ce76253ca21da4beefea129e027fe.jpg">  (99) <br><img src="https://habrastorage.org/storage2/e94/b0c/ca5/e94b0cca5e91ccbacbf79a65f02628c6.jpg">  (100) <br>  where, in turn: <br><img src="https://habrastorage.org/storage2/5fd/4ca/67c/5fd4ca67ce9364344de9843007bdaa5c.jpg">  (101) <br><img src="https://habrastorage.org/storage2/f61/980/f75/f61980f751ebbf163944dad3807834ba.jpg">  and <img src="https://habrastorage.org/storage2/2c6/87c/820/2c687c8205601183dcc37b2389d0cf07.jpg">  - i-th digitized values ‚Äã‚Äãof the original and restored audio signals, respectively, i = 1, 2, ..., n, and <img src="https://habrastorage.org/storage2/161/dd3/ce3/161dd3ce34f475b37b2bd497ed2a54ec.jpg">  - the maximum value among the digitized values ‚Äã‚Äãof the original audio signal. </div></div><br><br><div class="spoiler">  <b class="spoiler_title">6.4 Algorithm for calculating the metric "waveform difference factor"</b> <div class="spoiler_text">  Let be <img src="https://habrastorage.org/storage2/3c6/ece/164/3c6ece164a63d3591f2f97fa958e5be9.jpg">  - source mono channel audio signal (or one channel of the original multichannel audio signal).  Similarly <img src="https://habrastorage.org/storage2/b50/312/010/b50312010183f2c6cf73e8d5984da30d.jpg">  - reconstructed mono channel audio signal (or one channel of reconstructed multichannel audio signal).  Both signals consist of the same number of N values. <br>  Arrays of amplitude values ‚Äã‚Äãof signals <img src="https://habrastorage.org/storage2/3c6/ece/164/3c6ece164a63d3591f2f97fa958e5be9.jpg">  and <img src="https://habrastorage.org/storage2/b50/312/010/b50312010183f2c6cf73e8d5984da30d.jpg">  represented as a relative change in the signal amplitude values: <img src="https://habrastorage.org/storage2/1fe/d12/e6a/1fed12e6af935546b9a2c7d0a41dc49f.jpg">  (102) <br>  The value of the ‚Äúwaveform difference factor‚Äù metric K is calculated as the standard deviation of the amplitude value arrays <img src="https://habrastorage.org/storage2/1eb/c37/5ef/1ebc375ef11398ab187e5370989995b8.jpg">  and <img src="https://habrastorage.org/storage2/bbd/c3d/0b3/bbdc3d0b38b50c8a8e272e923d019635.jpg">  : <br><img src="https://habrastorage.org/storage2/aa0/fba/103/aa0fba103396462aaf742f7048edb496.jpg">  (103) </div></div><br><br>  <b>7 Comparison methods for compression of digitized audio data</b> <br>  7.1 Two or more compression algorithms are comparable with each other if they belong to the same class according to Table 1. <br>  7.2 Of the two or more comparable compression algorithms, the algorithm that provides the best values ‚Äã‚Äãof at least two of the three metrics given in Table 1 is considered the <b><i>best</i></b> . <b><i>The best</i></b> value of the metric is recognized as a <b><i>larger</i></b> value ‚Äî for the PSNR and PEAQ metrics, and a <b><i>smaller</i></b> value ‚Äî for the metric ‚Äúform difference coefficient signals. " <br><br>  <b>References:</b> <br>  1. <a href="http://www-mmsp.ece.mcgill.ca/Documents/Reports/2002/KabalR2002v2.pdf">P.Kabal, An Examinationa and Intrapretation of ITU-R BS.1387 Perceptual Evaluation of Audio Quality</a> <br>  2. <a href="http://www-mmsp.ece.mcgill.ca/documents/Downloads/PQevalAudio/">PQevalAudio</a> </div><p>Source: <a href="https://habr.com/ru/post/181922/">https://habr.com/ru/post/181922/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../181906/index.html">LiveDC - Quick access to p2p files</a></li>
<li><a href="../181908/index.html">Texet TM-607TV: phone disguised as an Android smartphone</a></li>
<li><a href="../181910/index.html">Recovery of iPhoto when the import hangs</a></li>
<li><a href="../181912/index.html">ChibiOS: Lightweight RTOS</a></li>
<li><a href="../181914/index.html">Working with class variables of the heir class in the base class</a></li>
<li><a href="../181924/index.html">FeatureBranch</a></li>
<li><a href="../181928/index.html">How does Yandex.Mail for domains work?</a></li>
<li><a href="../181930/index.html">Cambot - Robot Photographer on Raspberry Pi</a></li>
<li><a href="../181932/index.html">Review of possible problems when installing Ubuntu second system on SSD</a></li>
<li><a href="../181934/index.html">We bring Skype icons to the general style of the system.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>