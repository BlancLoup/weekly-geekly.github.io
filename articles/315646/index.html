<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ceph storage cluster on VMWare in 10 minutes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This manual is written for those who want to try the flexibility and convenience of Ceph distributed file storage in a virtual environment on a home P...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ceph storage cluster on VMWare in 10 minutes</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/83c/e4f/8f2/83ce4f8f2f764cf993a725eeb2beb019.jpg"><br><br>  This manual is written for those who want to try the flexibility and convenience of Ceph distributed file storage in a virtual environment on a home PC.  Immediately I will clarify that 10 minutes is the time to deploy the cluster itself (installing and configuring Ceph on virtual machines).  But the creation and cloning of virtual machines and the installation of the OS will take some time. <br><br>  By the end of the article we will have a virtual cluster of three machines, and the Windows PC itself as a storage client.  Next we throw there photos of cats, drop one node, then pick up, drop another, download photos of cats back, be glad. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Who else is not in the Ceph topic, you can read the introductory article <a href="https://habrahabr.ru/post/313644/">Introducing Ceph in pictures</a> and see the fashion <a href="https://www.youtube.com/watch%3Fv%3DQBkH1g4DuKE">promo</a> from the manufacturer. <br><a name="habracut"></a><br>  My booth was: <br><br>  <strong>PC settings</strong> <br>  Processor: i5-6500 <br>  Memory: 16 GB (but in theory, 8 GB should be enough) <br>  Disk: normal home HDD with 100 GB of free space <br>  OS: Windows 7 x64 <br><br>  <strong>Virtual machine options (all three are the same)</strong> <br>  Processor: 1 core <br>  Memory: 1024 MB <br>  Subnet: 192.168.1.0/24 <br>  Drives: 4 virtual disks of 10 GB each (1 for OS, 3 for storage) <br>  OS: Ubuntu Server 16.04.2 x64 <br><br>  I want to remind you that the goal of the experiment is to try the possibilities and create a platform for further rituals.  But since 9 virtual disks (system disks are not counted) are on the same physical disk, you should not expect any acceptable performance from this system.  Performance will be extremely low, but quite sufficient for testing. <br><br><h2>  Plan </h2><br>  - Create a cn1 virtual machine (ceph-node-1) <br>  - Install the OS, customize <br>  - Making two clones (cn2 and cn3), changing IP addresses <br>  - Raise Ceph Cluster <br>  - Configure iSCSI-target on one of the machines <br>  - We connect Windows as a client <br>  - We try to upload / download / delete files, drop nodes <br><br><h2>  Create a virtual machine </h2><br>  Virtual machine parameters are listed above.  For storage, I created preallocated disks without splits.  VMWare subnet can take NAT with DHCP.  In the future, we should have this: <br><br>  192.168.1.11 - cn1 (there will be another iSCSI-target) <br>  192.168.1.12 - cn2 <br>  192.168.1.13 - cn3 <br>  192.168.1.1 - Client (iSCSI-initiator) <br><br>  Gateway / mask: 192.168.1.2 / 255.255.255.0 <br><br><h2>  Install and configure the OS </h2><br>  This manual should work on Ubuntu Server 16.04.2, on other versions or distributions some settings will be different. <br><br>  All settings I will produce from the root with a simple password.  And here I must necessarily wag his finger and mention that for the sake of security on the battle cluster, it is better not to do that. <br><br>  Install the OS: <br><br><div class="spoiler">  <b class="spoiler_title">Installation</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/696/2d6/5e7/6962d65e7bb943769fa52f7bf7c2d354.jpg"><br></div></div><br>  Now that the OS is installed, we boot into it and check the issued address: <br><br><pre><code class="bash hljs">ip -4 addr | grep inet</code> </pre> <br>  I turned out to be 192.168.1.128.  Connect to it via SSH.  Set the root password: <br><br><pre> <code class="bash hljs">sudo su passwd root</code> </pre><br>  Let's configure the network interface (at this moment we will be thrown out with SSH, because the IP has changed): <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /etc/network cp interfaces interfaces.BAK IFACE=ens32 ADDR=192.168.1.11 MASK=255.255.255.0 GW=192.168.1.2 sed -i <span class="hljs-string"><span class="hljs-string">"s/iface </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$IFACE</span></span></span><span class="hljs-string"> inet dhcp/iface </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$IFACE</span></span></span><span class="hljs-string"> inet static\naddress </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$ADDR</span></span></span><span class="hljs-string">\nnetmask </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$MASK</span></span></span><span class="hljs-string">\ngateway </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$GW</span></span></span><span class="hljs-string">\ndns-nameservers </span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$GW</span></span></span><span class="hljs-string">/g"</span></span> interfaces ip addr flush to 192.168.1.0/24 &amp;&amp; ip addr add <span class="hljs-variable"><span class="hljs-variable">$ADDR</span></span>/24 dev <span class="hljs-variable"><span class="hljs-variable">$IFACE</span></span></code> </pre><br><div class="spoiler">  <b class="spoiler_title">How do I get it now?</b> <div class="spoiler_text">  As you read the listings, you may encounter not very readable one-liners, which are designed to reduce the number of manual edits in order to use more direct copy-paste from the article to the terminal.  But I will lead the decryption. <br><br>  In addition, I did not fully understand the problem, but in Ubunt 16 there is a certain embarrassment with the configuration of network interfaces (although it works fine on 14).  I urge adult admins to highlight this topic in the comments.  From here dances with ip addr flush / add. <br><br>  The essence of the problem is that when the daemon is restarted, a new secondary address is added, and the old one is not deleted, while ip addr delete instead of deleting a specific one, deletes all addresses (removes one in 14 ubunt).  Nobody bothers with this, because the old address does not bother anyone, but in our case, the old address must be deleted with the clones, otherwise the miracle will not happen.  Reboot of the machine helps, but this is a bad way. <br></div></div><br>  Connect to a new address (192.168.1.11) and allow you to connect to the machine with root: <br><br><pre> <code class="bash hljs">su <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /etc/ssh/ cp sshd_config sshd_config.BAK sed -i <span class="hljs-string"><span class="hljs-string">"s/PermitRootLogin.*/PermitRootLogin yes/g"</span></span> sshd_config</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Decryption</b> <div class="spoiler_text">  In the / etc / ssh / sshd_config file, look for the PermitRootLogin directive and set it to yes <br></div></div><br>  We set the host name and host names of neighbors: <br><br><pre> <code class="bash hljs">HOSTNAME=cn1 <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-variable"><span class="hljs-variable">$HOSTNAME</span></span> &gt; /etc/hostname sed -i <span class="hljs-string"><span class="hljs-string">"s/127.0.1.1.*/127.0.1.1\t</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HOSTNAME</span></span></span><span class="hljs-string">/g"</span></span> /etc/hosts <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">" # Ceph-nodes 192.168.1.11 cn1 192.168.1.12 cn2 192.168.1.13 cn3"</span></span> &gt;&gt; /etc/hosts</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Decryption</b> <div class="spoiler_text">  In the / etc / hostname file, write the host name cn1. <br>  In the / etc / hosts file, we change the name to the entry 127.0.1.1 to cn1, if it was stated otherwise during the installation of the OS.  We also add names to all the nodes of the cluster at the end of the file. <br></div></div><br>  After changing the address, a default route may be required, without which we will not be able to deliver the necessary packages.  From the packages, it is necessary to put ntp to synchronize the time on all nodes.  In htop it is convenient to look with your eyes at the Ceph demons and their threads. <br><br><pre> <code class="bash hljs">ip route add default via 192.168.1.2 dev ens32 apt update apt install mc htop ntp -y</code> </pre><br>  Everything is ready, the first car received the axis, the necessary packages and settings, which the clones will inherit.  Turn off the car: <br><br><pre> <code class="bash hljs">shutdown -h now</code> </pre><br><h2>  Making clones, setting up a network </h2><br>  At this place, we make a snapshot of the machine and create two complete clones indicating this snapshot.  This is the birth of cn2 and cn3 machines.  Since the clones completely inherited the settings of the cn1 node, we have three machines with the same IP addresses.  Therefore, we turn them on one by one in the reverse order, simultaneously changing the IP addresses to the correct ones. <br><br>  <strong>Turn on cn3</strong> <br>  We start the machine, connect with the root via SSH at 192.168.1.11, change the interface settings to 192.168.1.13, after which the SSH session will fall off at the old address: <br><br><pre> <code class="bash hljs">HOST_ID=3 <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> cn<span class="hljs-variable"><span class="hljs-variable">$HOST_ID</span></span> &gt; /etc/hostname sed -i <span class="hljs-string"><span class="hljs-string">"s/127.0.1.1.*/127.0.1.1\tcn</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HOST_ID</span></span></span><span class="hljs-string">/g"</span></span> /etc/hosts sed -i <span class="hljs-string"><span class="hljs-string">"s/address.*/address 192.168.1.1</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HOST_ID</span></span></span><span class="hljs-string">/g"</span></span> /etc/network/interfaces ip addr flush to 192.168.1.0/24 &amp;&amp; ip addr add 192.168.1.1<span class="hljs-variable"><span class="hljs-variable">$HOST_ID</span></span>/24 dev ens32</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Decryption</b> <div class="spoiler_text">  We change the host name in the / etc / hostname file, change the name to the entry 127.0.1.1 in the / etc / hosts file, remove all the 192.168.1.0/24 addresses from the interfaces and hang the correct one. <br></div></div><br>  <strong>Turn cn2 on</strong> <br>  On cn3, by this moment the correct address is already, and on cn2 it is still 192.168.1.11, we connect to it and make the same setup for the address 192.168.1.12: <br><br><pre> <code class="bash hljs">HOST_ID=2 <span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> cn<span class="hljs-variable"><span class="hljs-variable">$HOST_ID</span></span> &gt; /etc/hostname sed -i <span class="hljs-string"><span class="hljs-string">"s/127.0.1.1.*/127.0.1.1\tcn</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HOST_ID</span></span></span><span class="hljs-string">/g"</span></span> /etc/hosts sed -i <span class="hljs-string"><span class="hljs-string">"s/address.*/address 192.168.1.1</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$HOST_ID</span></span></span><span class="hljs-string">/g"</span></span> /etc/network/interfaces ip addr flush to 192.168.1.0/24 &amp;&amp; ip addr add 192.168.1.1<span class="hljs-variable"><span class="hljs-variable">$HOST_ID</span></span>/24 dev ens32</code> </pre><br><div class="spoiler">  <b class="spoiler_title">Decryption</b> <div class="spoiler_text">  Listing differs from the previous one only in the first line. <br></div></div><br>  Now when there are no conflicting addresses left, and the session with cn2 has fallen off, <br>  <strong>Turn on cn1</strong> . <br><br>  Connecting to 192.168.1.11, and henceforth we will do all the work on this node (including managing the other nodes).  To do this, we will generate SSH keys and decompose them into all the nodes (this is not my whim, Ceph calls for it).  In the process, you will need to enter passwords and nod your head at his questions: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#   cn1    ,     ssh-keygen #   cn1    for node_id in 1 2 3; do ssh-copy-id cn$node_id; done #   cn1  cn2,  ,   cn2    ssh cn2 ssh-keygen for node_id in 1 2 3; do ssh-copy-id cn$node_id; done exit #   cn1 #   cn3 ssh cn3 #   cn1  cn3,  ,   cn3    ssh-keygen for node_id in 1 2 3; do ssh-copy-id cn$node_id; done exit #   cn1</span></span></code> </pre><br>  Thus, we brought the cluster subnet to the state described at the beginning of the article.  All nodes have root access on their neighbors without a password * runs a finger * <br><br>  The cluster is still not a cluster, but is ready for clustering, so to speak.  However, before shooting <s>your</s> revolver into the sky and starting to measure 10 minutes on the stopwatch, I strongly recommend you turn off the cars and make snapshots with the name "ready_for_ceph", which you can return to and ceph unceasingly, if something happens went wrong during our rites. <br><br><h2>  Raise Ceph Cluster </h2><br>  10 minutes have gone ... <br><br>  We start all the machines and connect to cn1 via SSH.  To begin with, we will install ceph-deploy on all the nodes, which is designed to simplify the installation and configuration of some cluster components.  Then we will create a cluster, install the Ceph distribution kit on all nodes, add three monitors to the cluster (one is possible, but then the monitor‚Äôs fall will be equivalent to the fall of the entire cluster) and fill the storage with disks. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  ceph-deploy   : for node_id in 1 2 3; do ssh cn$node_id apt install ceph-deploy -y; done #     Ceph: mkdir /etc/ceph cd /etc/ceph #   : ceph-deploy new cn1 cn2 cn3 #   Ceph  : ceph-deploy install cn1 cn2 cn3 #  ,    : ceph-deploy mon create-initial #   Ubuntu  systemd #    -     , #     systemd : for node_id in 1 2 3; do ssh cn$node_id systemctl enable ceph-mon; done #        : ceph-deploy disk zap {cn1,cn2,cn3}:{sdb,sdc,sdd} #     : # ceph-deploy disk zap cn1:sdb cn1:sdc cn1:sdd cn2:sdb cn2:sdc cn2:sdd cn3:sdb cn3:sdc cn3:sdd #    OSD    : ceph-deploy osd create {cn1,cn2,cn3}:{sdb,sdc,sdd}</span></span></code> </pre><br>  The cluster is almost ready, we check its status with the command ceph -s or ceph status: <br><br><pre> <code class="hljs ruby">root@cn1<span class="hljs-symbol"><span class="hljs-symbol">:/etc/ceph</span></span><span class="hljs-comment"><span class="hljs-comment"># ceph -s cluster 0cb14335-e366-48df-b361-3c97550d6ef4 health HEALTH_WARN too few PGs per OSD (21 &lt; min 30) monmap e1: 3 mons at {cn1=192.168.1.11:6789/0,cn2=192.168.1.12:6789/0,cn3=192.168.1.13:6789/0} election epoch 6, quorum 0,1,2 cn1,cn2,cn3 osdmap e43: 9 osds: 9 up, 9 in flags sortbitwise,require_jewel_osds pgmap v107: 64 pgs, 1 pools, 0 bytes data, 0 objects 308 MB used, 45672 MB / 45980 MB avail 64 active+clean</span></span></code> </pre><br>  The key line in this report is health, and it is in the HEALTH_WARN state.  This is better than HEALTH_ERR, because at least the cluster works with us, although not so much.  And immediately under HEALTH_WARN it is written why it is _WARN, namely: ‚Äútoo few PGs per OSD (21 &lt;min 30)‚Äù, which tells us about the need to increase the number of placement groups so that at least 21 PG fall to one OSD.  Then multiply 9 OSD by 21 and get 189, then round to the nearest power of two and get 256. While the current number of PG = 64, which is clearly seen in the pgmap line.  All this is described in the <a href="http://docs.ceph.com/docs/master/">Ceph documentation</a> . <br><br>  In this case, we satisfy the requirement of the cluster and do it: <br><br><pre> <code class="bash hljs">PG_NUM=256 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> rbd pg_num <span class="hljs-variable"><span class="hljs-variable">$PG_NUM</span></span> ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> rbd pgp_num <span class="hljs-variable"><span class="hljs-variable">$PG_NUM</span></span></code> </pre><br>  We give the system a few seconds to rebuild the map and check the cluster status again: <br><br><pre> <code class="hljs ruby">root@cn1<span class="hljs-symbol"><span class="hljs-symbol">:/etc/ceph</span></span><span class="hljs-comment"><span class="hljs-comment"># ceph -s cluster 0cb14335-e366-48df-b361-3c97550d6ef4 health HEALTH_OK monmap e1: 3 mons at {cn1=192.168.1.11:6789/0,cn2=192.168.1.12:6789/0,cn3=192.168.1.13:6789/0} election epoch 6, quorum 0,1,2 cn1,cn2,cn3 osdmap e50: 9 osds: 9 up, 9 in flags sortbitwise,require_jewel_osds pgmap v151: 256 pgs, 1 pools, 0 bytes data, 0 objects 319 MB used, 45661 MB / 45980 MB avail 256 active+clean</span></span></code> </pre><br>  We see the cherished HEALTH_OK, which tells us that the cluster is healthy and ready to go. <br><br>  By default, the pool replication factor is 3 (read about the size and min_size variables in the <a href="https://habrahabr.ru/post/313644/">introductory article</a> ).  This means that each object is stored in triplicate on different disks.  Let's look at it with our eyes: <br><br><pre> <code class="hljs ruby">root@cn1<span class="hljs-symbol"><span class="hljs-symbol">:/etc/ceph</span></span><span class="hljs-comment"><span class="hljs-comment"># ceph osd pool get rbd size size: 3 root</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">@cn</span></span></span><span class="hljs-comment">1:/etc/ceph# ceph osd pool get rbd min_size min_size: 2</span></span></code> </pre><br>  Now we will reduce the size to 2, and min_size to 1 (in production, it is strongly advised not to do this! But as part of a virtual stand, this should improve performance) <br><br><pre> <code class="bash hljs">ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> rbd size 2 ceph osd pool <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> rbd min_size 1</code> </pre><br>  What's next?  Next you need to try the cluster in the work. <br><br>  You can perform file sharing with a cluster in one of three well-known ways (block device, file system and object storage).  Windows 7 out of the box prefers traditional iSCSI, and therefore our way is a block device.  In this case, we need to install the iSCSI-target on some node (let it be cn1). <br><br><h2>  Install and configure iSCSI-target </h2><br>  We need not a simple target, but with RBD (Rados Block Device) support.  The tgt-rbd package will do, so install it on cn1: <br><br><pre> <code class="hljs sql">apt <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> tgt-rbd -y <span class="hljs-comment"><span class="hljs-comment">#    rbd: tgtadm --lld iscsi --mode system --op show | grep rbd rbd (bsoflags sync:direct) # ,  </span></span></code> </pre><br>  Ceph by default creates an rbd pool for block devices, in which we will create an rbd image: <br><br><pre> <code class="hljs pgsql">rbd <span class="hljs-keyword"><span class="hljs-keyword">create</span></span> -p rbd rbd1 <span class="hljs-comment"><span class="hljs-comment">--size 4096 --name client.admin --image-feature layering #  rbd -  , rbd1 -  , 4096 -   </span></span></code> </pre><br>  Now we ask the target to give the image of rbd1 for any IP-address by writing it to the config: <br><br><pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">echo</span></span> <span class="hljs-string"><span class="hljs-string">'&lt;target virtual-ceph:iscsi&gt; driver iscsi bs-type rbd backing-store rbd/rbd1 # Format: &lt;pool_name&gt;/&lt;rbd_image_name&gt; initiator-address ALL &lt;/target&gt;'</span></span> &gt; /etc/tgt/conf.d/ceph.conf <span class="hljs-comment"><span class="hljs-comment">#   : systemctl restart tgt</span></span></code> </pre><br>  We look at the stopwatch: 8 minutes.  Fulfilled. <br><br><h2>  Connecting a Windows PC via iSCSI </h2><br>  Winda is equipped with a built-in utility iscsicpl.exe, which will help us connect the image of rbd1 as a local disk.  Launch and go to the "Targets" tab.  Enter the cn1 node (192.168.1.11) IP-address in the "Object" field and click "Quick Connection".  If we configured everything correctly, then our iSCSI target will be listed.  Choose it and connect. <br><br><img src="https://habrastorage.org/files/5bc/f05/67f/5bcf0567fc304cfc9a16fb322ccc43d2.png"><br><br>  After that, the system will appear unallocated disk.  In the disk management console diskmgmt.msc we see a new 4-gig device.  You need to create a partition on it and format it, marking it as Ceph. <br><br><img src="https://habrastorage.org/files/241/ff1/379/241ff137934d4f948fb40ef67acef7b5.png"><br><br>  Now you can go to my computer and enjoy the result. <br><br><img src="https://habrastorage.org/files/39e/d31/7ca/39ed317cafbd48faa4c7396d0ca9fd5c.png"><br><br><h2>  Tests </h2><br>  It's time to fill the cluster with photos of cats.  As photos of cats I will use 4 images of distributions of ubunt of different versions, which have come to hand, with a total volume of 2.8 gigabytes. <br><br>  We try to fill them with our new local disk.  As you can first mistakenly notice that the files are filled quickly, but this is just filling the buffers, of which the data will be gradually written to the physical disks of the cluster.  In my case, the first 3 files of 700-800 megabytes flew off quickly, and the last one was despondent, and the speed dropped to the expected one. <br><br>  If you run the ceph -w command on the cluster node, you can monitor the cluster status in real time.  Reading / writing data, blade disk, node or monitor, all this is displayed in this log in real time. <br><br>  You can drop and raise nodes one by one, the main thing is not to drop cn1 (because there is only one iSCSI-target on it without multipassing) and not to drop two nodes at once.  But if you increase the size back to 3, then you can afford it. <br><br>  It‚Äôs time for vandalism: try dropping the cn3 node by clicking the stop button in VMWare, watching ceph -w on cn1.  First, we will see how the cluster gets worried that some OSDs do not respond for a long time: <br><br><pre> <code class="hljs pgsql">osd<span class="hljs-number"><span class="hljs-number">.6</span></span> <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.13</span></span>:<span class="hljs-number"><span class="hljs-number">6800</span></span>/<span class="hljs-number"><span class="hljs-number">6812</span></span> failed (<span class="hljs-number"><span class="hljs-number">2</span></span> reporters <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> different host <span class="hljs-keyword"><span class="hljs-keyword">after</span></span> <span class="hljs-number"><span class="hljs-number">21.000229</span></span> &gt;= grace <span class="hljs-number"><span class="hljs-number">20.000000</span></span>) osd<span class="hljs-number"><span class="hljs-number">.7</span></span> <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.13</span></span>:<span class="hljs-number"><span class="hljs-number">6804</span></span>/<span class="hljs-number"><span class="hljs-number">7724</span></span> failed (<span class="hljs-number"><span class="hljs-number">2</span></span> reporters <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> different host <span class="hljs-keyword"><span class="hljs-keyword">after</span></span> <span class="hljs-number"><span class="hljs-number">21.000356</span></span> &gt;= grace <span class="hljs-number"><span class="hljs-number">20.000000</span></span>) osd<span class="hljs-number"><span class="hljs-number">.8</span></span> <span class="hljs-number"><span class="hljs-number">192.168</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span><span class="hljs-number"><span class="hljs-number">.13</span></span>:<span class="hljs-number"><span class="hljs-number">6808</span></span>/<span class="hljs-number"><span class="hljs-number">8766</span></span> failed (<span class="hljs-number"><span class="hljs-number">2</span></span> reporters <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> different host <span class="hljs-keyword"><span class="hljs-keyword">after</span></span> <span class="hljs-number"><span class="hljs-number">21.000715</span></span> &gt;= grace <span class="hljs-number"><span class="hljs-number">20.000000</span></span>) osdmap e53: <span class="hljs-number"><span class="hljs-number">9</span></span> osds: <span class="hljs-number"><span class="hljs-number">6</span></span> up, <span class="hljs-number"><span class="hljs-number">9</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span></code> </pre><br>  Within 5 minutes, the cluster will remain in a state of hope that the disks will still come to their senses and come back.  But after 5 minutes (this is the default value), the cluster will put up with the loss and begin rebalancing data from dead OSDs, smearing the missing objects on other disks, having previously marked the non-responding OSD as dropped (out) and having corrected the placement group card: <br><br><pre> <code class="hljs objectivec">osd<span class="hljs-number"><span class="hljs-number">.6</span></span> <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> (down <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-number"><span class="hljs-number">300.042041</span></span>) osd<span class="hljs-number"><span class="hljs-number">.7</span></span> <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> (down <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-number"><span class="hljs-number">300.042041</span></span>) osd<span class="hljs-number"><span class="hljs-number">.8</span></span> <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> (down <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> <span class="hljs-number"><span class="hljs-number">300.042041</span></span>)</code> </pre><br>  Until rebalancing is completed, ceph -s will show HEALTH_WARN status, however the files will be available, but not without a loss in performance, yes.  The reason HEALTH_WARN will write this: <br><br><pre> <code class="hljs matlab">health HEALTH_WARN <span class="hljs-number"><span class="hljs-number">102</span></span> pgs degraded <span class="hljs-number"><span class="hljs-number">102</span></span> pgs stuck unclean recovery <span class="hljs-number"><span class="hljs-number">677</span></span>/<span class="hljs-number"><span class="hljs-number">1420</span></span> objects degraded (<span class="hljs-number"><span class="hljs-number">47.676</span></span><span class="hljs-comment"><span class="hljs-comment">%) 1 mons down, quorum 0,1 cn1,cn2</span></span></code> </pre><br>  Continue without me. </div><p>Source: <a href="https://habr.com/ru/post/315646/">https://habr.com/ru/post/315646/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../315624/index.html">Without TK: how do developers get involved in this</a></li>
<li><a href="../315632/index.html">Egor Bugaenko about MVC on jug.msk.ru</a></li>
<li><a href="../315634/index.html">Opencart Integration with 1C Enterprise</a></li>
<li><a href="../315640/index.html">Cleaning the air in the data center as a way to extend the life of the equipment and reduce costs</a></li>
<li><a href="../315642/index.html">Alistair Coburn: Team Development and Agile</a></li>
<li><a href="../315648/index.html">Cameras, HUD and WTF: improve usability of the next VR game</a></li>
<li><a href="../315652/index.html">JetBrains Night in Moscow. Video. First steps with TeamCity DSL</a></li>
<li><a href="../315654/index.html">File system, cheap and fast</a></li>
<li><a href="../315656/index.html">Tracking Problems: How Mobile Workers Cheat Companies Due to Control Technology Deficiencies</a></li>
<li><a href="../315660/index.html">Some subtleties of using Service Workers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>