<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we broke Glusterfs</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The story began a year ago when our friend, a colleague and a great enterprise expert on large-scale enterprises came to us with the words: ‚ÄúGuys, I h...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we broke Glusterfs</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/2z/_z/ym/2z_zym7l5zuwpxy0kc8arplu28y.jpeg"><br><br>  The story began a year ago when our friend, a colleague and a great enterprise expert on large-scale enterprises came to us with the words: ‚ÄúGuys, I have a wonderful store here with all the fancy features lying around.  90Tb.  We did not see any particular need for it, but, naturally, we did not refuse.  We set up a couple of backups and safely forgot for a while. <br><br>  Periodically, there were tasks such as transferring large files between hosts, building WAL for Postgre replicas, etc. Gradually, we began to transfer all the scattered good into this fridge, one way or another connected with our project.  We set up rotate, alerts on successful and not-so-successful backup attempts.  Over the year, this storage has become one of the important elements of our infrastructure in the operation group. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Everything was fine until our expert again came to us and said that he wants to take his gift.  And it is necessary to return it urgently. <br><br>  The choice was small - to shove it all up again anywhere or to assemble your own fridge from blackjack and sticks.  By this time, we were already taught, we saw enough of not quite fault-tolerant systems, and fault tolerance became our second self. <br><br>  Of the many options, our view is particularly hooked Gluster, Glaster.  All the same, how to call.  If only the result was.  Therefore, we began to mock at him. <br><a name="habracut"></a><br><h3>  What is Glaster and why is he needed? </h3><br>  This is a distributed file system that has long been friends with Openstack and is integrated into oVIrt / RHEV.  Although our <a href="https://cloud.croc.ru/services/laas/kvm/%3Futm_source%3Dhabr%26utm_medium%3Dblog%26utm_campaign%3D23_04_18">IaaS is</a> not on Openstack, Glaster has a large active community and has native qemu support in the form of the libgfapi interface.  Thus, we kill two birds with one stone: <br><br><ol><li>  We raise a stack for backups, fully supported by us.  No longer have to be afraid of waiting for the vendor to send a broken part. </li><li>  We are testing a new type of storage (volume type), which we can provide to our customers in the future. </li></ol><br>  <b>Hypotheses that we tested:</b> <br><br><ol><li>  What glaster works.  Verified </li><li>  That it is fault tolerant - we can reboot any node and the cluster will continue to work, data will be available.  We can rebuild several nodes, the data will not be lost.  Verified </li><li>  That it is reliable - that is, it does not fall by itself, does not expire with memory, etc. Partially true, it took a long time to understand that the problem is not in our hands and heads, but in the Striped configuration of the woluma, which could not work stable in none of the configurations we assembled (details at the end). </li></ol><br>  The month was spent on experiments and assemblies of various configurations and versions, then there was a test operation in production as the second destination for our technical backups.  We wanted to see how he behaves for half a year before relying on him completely. <br><br><h3>  How to raise </h3><br>  We had enough experimentation sticks with a surplus - a rack with Dell Poweredge r510 and a pack of not very nimble SATA-double-bytes who were inherited from the old S3. <br><br>  We figured we didn‚Äôt need storage for more than 20 TB, and after that it took us about half an hour to fill in two old Dell Power Edge r510s with 10 disks, select another server for the role of arbitrator, download the sachets and secure it.  It turned out this scheme. <br><br><img src="https://habrastorage.org/webt/bh/4j/m1/bh4jm1v7znnuol9tc7bvpxjaglg.jpeg"><br><br>  We chose striped-replicated with the arbitrator, because it is fast (data is spread evenly over several bricks), quite reliably (replica 2), you can survive the fall of one node without getting a split-brain.  How wrong we were ... <br><br>  The main disadvantage of our cluster in the current configuration is a very narrow channel, only 1G, but for our purpose it is quite enough.  Therefore, this post is not about testing the speed of the system, but about its stability and what to do in case of accidents.  Although in the future we plan to switch it to Infiniband 56G from rdma and conduct performance tests, but this is another story. <br><br>  I will not go deep into the cluster assembly process, everything is quite simple here. <br><br>  <b>Create a directory for briks:</b> <br><br><pre><code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..9} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> mkdir -p /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span> ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  <b>We roll xfs on discs for briks:</b> <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {b..k} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> mkfs.xfs /dev/sd<span class="hljs-variable"><span class="hljs-variable">$i</span></span> ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  <b>Add mount points in / etc / fstab:</b> <br><br><pre> <code class="hljs objectivec">/dev/sdb /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick0/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdc /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick1/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdd /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick2/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sde /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick3/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdf /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick4/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdg /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick5/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdh /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick6/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdi /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick7/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdj /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick8/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> /dev/sdk /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/brick9/ xfs defaults <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span></code> </pre><br>  <b>We mount:</b> <br><br><pre> <code class="bash hljs">mount -a</code> </pre> <br>  <b>Add the directory for volyum to brika, which will be called holodilnik:</b> <br><br><pre> <code class="bash hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> {0..9} ; <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> mkdir -p /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick<span class="hljs-variable"><span class="hljs-variable">$i</span></span>/holodilnik ; <span class="hljs-keyword"><span class="hljs-keyword">done</span></span></code> </pre> <br>  Next, we need to remove the cluster hosts and create a Volyum. <br><br>  <b>We put packages on all three hosts:</b> <br><br><pre> <code class="bash hljs">pdsh -w server[1-3] -- yum install glusterfs-server -y</code> </pre> <br>  <b>Run Glaster:</b> <br><br><pre> <code class="bash hljs">systemctl <span class="hljs-built_in"><span class="hljs-built_in">enable</span></span> glusterd systemctl start glusterd</code> </pre> <br>  <b>It is useful to know that Glaster has several processes, here are their purposes:</b> <br><br>  <b>glusterd</b> = management daemon <br>  The main demon, manages Volyum, pulls the rest of the demons responsible for brik and data recovery. <br><br>  <b>glusterfsd</b> = per-brick daemon <br>  Each brika launches its glusterfsd daemon. <br><br>  <b>glustershd</b> = self-heal daemon <br>  Responsible for rebuild data from replicated volums in cases of cluster node nodes. <br><br>  <b>glusterfs</b> = usually client-side, but also NFS on servers <br>  For example, arrives with the glusterfs-fuse native client package. <br><br>  <b>Pirim nodes:</b> <br><br><pre> <code class="bash hljs">gluster peer probe server2 gluster peer probe server3</code> </pre><br>  <b>We collect Volyum, here the order of bricks is important - replicated bricks follow each other:</b> <br><br><pre> <code class="bash hljs">gluster volume create holodilnik stripe 10 replica 3 arbiter 1 transport tcp server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick0/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick0/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick0/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick1/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick1/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick1/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick2/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick3/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick3/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick3/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick4/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick4/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick4/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick5/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick5/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick5/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick6/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick7/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick7/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick7/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick8/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick8/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick8/holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick9/holodilnik server2:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick9/holodilnik server3:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick9/holodilnik force</code> </pre><br>  We had to try a large number of combinations of parameters, kernel versions (3.10.0, 4.5.4) and Glusterfs itself (3.8, 3.10, 3.13) in order for Glaster to start to behave stably. <br><br>  <b>We also experimentally set the following parameter values:</b> <br><br><pre> <code class="hljs sql">gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.write-behind <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik nfs.disable <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik cluster.lookup-<span class="hljs-keyword"><span class="hljs-keyword">optimize</span></span> <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.stat-prefetch <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik server.allow-insecure <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik storage.batch-fsync-delay-usec <span class="hljs-number"><span class="hljs-number">0</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.client-io-threads <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik network.frame-<span class="hljs-keyword"><span class="hljs-keyword">timeout</span></span> <span class="hljs-number"><span class="hljs-number">60</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.quick-<span class="hljs-keyword"><span class="hljs-keyword">read</span></span> <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.flush-behind <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.io-<span class="hljs-keyword"><span class="hljs-keyword">cache</span></span> <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.read-ahead <span class="hljs-keyword"><span class="hljs-keyword">off</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.cache-<span class="hljs-keyword"><span class="hljs-keyword">size</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.io-<span class="hljs-keyword"><span class="hljs-keyword">thread</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">count</span></span> <span class="hljs-number"><span class="hljs-number">64</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.high-prio-threads <span class="hljs-number"><span class="hljs-number">64</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik performance.normal-prio-threads <span class="hljs-number"><span class="hljs-number">64</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik network.ping-<span class="hljs-keyword"><span class="hljs-keyword">timeout</span></span> <span class="hljs-number"><span class="hljs-number">5</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik server.event-threads <span class="hljs-number"><span class="hljs-number">16</span></span> gluster volume <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> holodilnik client.event-threads <span class="hljs-number"><span class="hljs-number">16</span></span></code> </pre> <br>  <b>Additional useful options:</b> <br><br><pre> <code class="hljs ruby">sysctl vm.swappiness=<span class="hljs-number"><span class="hljs-number">0</span></span> sysctl vm.vfs_cache_pressure=<span class="hljs-number"><span class="hljs-number">120</span></span> sysctl vm.dirty_ratio=<span class="hljs-number"><span class="hljs-number">5</span></span> echo <span class="hljs-string"><span class="hljs-string">"deadline"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/block</span></span><span class="hljs-regexp"><span class="hljs-regexp">/sd[bk]/queue</span></span><span class="hljs-regexp"><span class="hljs-regexp">/scheduler echo "256" &gt; /sys</span></span><span class="hljs-regexp"><span class="hljs-regexp">/block/sd</span></span>[bk]/queue/nr_requests echo <span class="hljs-string"><span class="hljs-string">"16"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/proc/sys</span></span><span class="hljs-regexp"><span class="hljs-regexp">/vm/page</span></span>-cluster blockdev --setra <span class="hljs-number"><span class="hljs-number">4096</span></span> /dev/sd[bk]</code> </pre> <br>  It is worth adding that these parameters are good in our case with backups, namely with linear operations.  For random read / write cases, you need to select something else. <br><br>  <b>Now we will look at the pros and cons of different types of connection to Glaster and the results of negative test cases.</b> <br><br>  To connect to Volyum, we tested all the main options: <br><br>  <b>1. Gluster Native Client (glusterfs-fuse) with the backupvolfile-server option.</b> <br><br>  Minuses: <br>  - installation of additional software on customers; <br>  - speed. <br><br>  Plus / minus: <br>  - long inaccessibility of data in case of a dump of one of the nodes of the cluster.  The problem is corrected by the server-side network.ping-timeout parameter.  Setting the parameter to 5, the ball falls off, respectively, for 5 seconds. <br>  A plus: <br>  - it works quite stably, there were no massive problems with beaten files. <br><br>  <b>2. Gluster Native Client (gluster-fuse) + VRRP (keepalived).</b> <b><br></b>  <b>Configured moving IP between two nodes of the cluster and extinguished one of them.</b> <br><br>  Minus: <br>  - installation of additional software. <br><br>  A plus: <br>  - configurable timeout when switching in case of cluster node's dump. <br><br>  As it turned out, specifying the backupvolfile-server parameter or the keepalived setting is optional, the client itself connects to the Glaster daemon (no matter which address), recognizes the remaining addresses and starts writing to all the nodes of the cluster.  In our case, we saw symmetric traffic from the client to server1 and server2.  Even if you give him a VIP address, the client will still use Glusterfs cluster addresses.  We came to the conclusion that this parameter is useful when the client tries to connect to the Glusterfs server, which is unavailable, when it starts, then it will contact the host specified in the backupvolfile-server. <br><br>  <b>Comment from official documentation:</b> <br><blockquote>  GlusterFS ‚Äúround robin‚Äù style connection.  In <b>/ etc / fstab</b> , the node is used;  It means that there is no need for further information.  The performance based on tests, but not drastically so.  The gain is an automatic HA client failover, which is typically worth the effect on performance. </blockquote><br>  <b>3. NFS-Ganesha server, with Pacemaker.</b> <br><br>  <b>The recommended type of connection, if for some reason you do not want to use the native client.</b> <br><br>  Minuses: <br>  - even more additional software; <br>  - fussing with pacemaker; <br>  - caught the <a href="https://www.spinics.net/lists/gluster-users/msg33510.html">bug</a> . <br><br>  <b>4. NFSv3 and NLM + VRRP (keepalived).</b> <br>  <b>Classic NFS with lock support and moving IP between two cluster nodes.</b> <br><br>  Pros: <br>  - fast switching in case of failure of the node; <br>  - easy keepalived settings; <br>  - nfs-utils is installed on all our client hosts by default. <br><br>  Minuses: <br>  - the client hangs NFS in status D after a few minutes rsync to the mount point; <br>  - drop the node with the client entirely - BUG: soft lockup - CPU stuck for Xs! <br>  - caught a lot of cases when the files broke with the errors stale file handle, Directory not empty with rm -rf, Remote I / O error, etc. <br><br>  The worst option, moreover, in later versions of Glusterfs he became deprecated, we do not advise anyone. <br><br>  As a result, we chose a glusterfs-fuse without keepalived and with a backupvolfile-server parameter.  As in our configuration, he is the only one who showed stability, despite the relatively low speed of work. <br><br>  In addition to the need to configure a highly available solution, in productive operation, we should be able to restore the service in case of accidents.  Therefore, after assembling a stably operating cluster, we proceeded to destructive tests. <br><br><h3>  Unusual shutdown of the node (cold reboot) </h3><br>  We launched rsync of a large number of files from one client, hard put out one of the nodes of the cluster and got very funny results.  After the node crashes, the recording first stopped for 5 seconds (the network.ping-timeout 5 parameter is responsible for this), after that the write speed to the ball doubled, as the client can no longer replicate data and starts sending all traffic to the remaining node, continuing to abut in our 1G channel. <br><br><img src="https://habrastorage.org/webt/zl/er/mw/zlermwq50dauk4uisq3f1qqivgg.png"><br><br>  When the server started up, the automatic data disinfection process started in the cluster, for which the glustershd daemon is responsible, and the speed dropped significantly. <br><br>  <b>So you can see the number of files that are treated after the node's dump:</b> <br><br><pre> <code class="bash hljs">gluster volume heal holodilnik info</code> </pre> <br><div class="spoiler">  <b class="spoiler_title">...</b> <div class="spoiler_text">  Brick server2: / export / brick1 / holodilnik <br>  /2018-01-20-weekly/billing.tar.gz <br>  Status: Connected <br>  Number of entries: 1 <br><br>  Brick server2: / export / brick5 / holodilnik <br>  /2018-01-27-weekly/billing.tar.gz <br>  Status: Connected <br>  Number of entries: 1 <br><br>  Brick server3: / export / brick5 / holodilnik <br>  /2018-01-27-weekly/billing.tar.gz <br>  Status: Connected <br>  Number of entries: 1 <br>  ... </div></div><br>  At the end of the treatment, the counters were reset and the recording speed returned to the previous indicators. <br><br><h3>  Disc blade and its replacement </h3><br>  Blade disc with a briquette, as well as its replacement, did not slow down the speed of writing to the ball.  Probably, the fact is that the bottleneck here is, again, the channel between the nodes of the cluster, and not the speed of the disks.  As soon as we have additional Infiniband cards, we will conduct tests with a wider channel. <br><br>  It is worth noting that when you change a disc that has flown out, it should return with the same name in sysfs (/ dev / sdX).  It often happens that a new drive is assigned the next letter.  I highly recommend not to enter it in this form, since during the subsequent reboot it will take the old name, the block device names will be eaten and the bricks will not rise.  Therefore, it is necessary to carry out several actions. <br><br>  Most likely, the problem is that somewhere in the system remained the mount point of the ejected disk.  Therefore, we do umount. <br><br><pre> <code class="bash hljs">umount /dev/sdX</code> </pre> <br>  We also check which process can hold this device: <br><br><pre> <code class="bash hljs">lsof | grep sdX</code> </pre> <br>  And we stop this process. <br><br>  After that, you need to make a rescan. <br>  We look in dmesg-H for more detailed information on the location of the ejected disk: <br><br> <code>[Feb14 12:28] quiet_error: 29686 callbacks suppressed <br> [ +0.000005] Buffer I/O error on device sdf, logical block 122060815 <br> [ +0.000042] lost page write due to I/O error on sdf <br> [ +0.001007] blk_update_request: I/O error, dev sdf, sector 1952988564 <br> [ +0.000043] XFS (sdf): metadata I/O error: block 0x74683d94 ("xlog_iodone") error 5 numblks 64 <br> [ +0.000074] XFS (sdf): xfs_do_force_shutdown(0x2) called from line 1180 of file fs/xfs/xfs_log.c. Return address = 0xffffffffa031bbbe <br> [ +0.000026] XFS (sdf): Log I/O Error Detected. Shutting down filesystem <br> [ +0.000029] XFS (sdf): Please umount the filesystem and rectify the problem(s) <br> [ +0.000034] XFS (sdf): xfs_log_force: error -5 returned. <br> [ +2.449233] XFS (sdf): xfs_log_force: error -5 returned. <br> [ +4.106773] sd 0:2:5:0: [sdf] Synchronizing SCSI cache <br> [ +25.997287] XFS (sdf): xfs_log_force: error -5 returned. <br> <br>  <b>sd 0:2:5:0</b> ‚Äî : <br> h == hostadapter id (first one being 0) <br> c == SCSI channel on hostadapter (first one being 2) ‚Äî   PCI- <br> t == ID (5) ‚Äî       <br> l == LUN (first one being 0) <br></code> <br>  Rescan: <br><br><pre> <code class="hljs javascript">echo <span class="hljs-number"><span class="hljs-number">1</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/</span></span>block/sdY/device/<span class="hljs-keyword"><span class="hljs-keyword">delete</span></span> echo <span class="hljs-string"><span class="hljs-string">"2 5 0"</span></span> &gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/</span></span><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class">/</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">scsi_host</span></span></span><span class="hljs-class">/</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">host0</span></span></span><span class="hljs-class">/</span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">scan</span></span></span></span></code> </pre> <br>  where sdY is the wrong drive name. <br><br>  Next, to replace the breeze, we need to create a new mount directory, roll out the file system and mount it: <br><br><pre> <code class="hljs objectivec">mkdir -p /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/newvol/brick mkfs.xfs /dev/sdf -f mount /dev/sdf /<span class="hljs-keyword"><span class="hljs-keyword">export</span></span>/newvol/</code> </pre> <br>  We are replacing brick: <br><br><pre> <code class="bash hljs">gluster volume replace-brick holodilnik server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/sdf/brick server1:/<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/newvol/brick commit force</code> </pre> <br>  <b>We start the treatment:</b> <br><br><pre> <code class="hljs pgsql">gluster volume heal holodilnik <span class="hljs-keyword"><span class="hljs-keyword">full</span></span> gluster volume heal holodilnik <span class="hljs-keyword"><span class="hljs-keyword">info</span></span> <span class="hljs-keyword"><span class="hljs-keyword">summary</span></span></code> </pre> <br>  <b>Dump of the arbitrator:</b> <br><br><img src="https://habrastorage.org/webt/qp/sp/zr/qpspzrabrihbaq859zwzpimtjks.png"><br><br>  The same 5‚Äì7 seconds of inaccessibility are balls and 3 seconds of drawdown associated with syntax of metadata per quorum node. <br><br><h3>  Summary </h3><br>  The results of the destructive tests pleased us, and we partially introduced it into the food, but we were not happy for long ... <br><br>  <b>Problem 1, it‚Äôs a known bug</b> <br>  When deleting a large number of files and directories (about 100,000), we eaten this ‚Äúbeauty‚Äù: <br><br><pre> <code class="hljs lua">rm -rf /mnt/holodilnik/* rm: cannot <span class="hljs-built_in"><span class="hljs-built_in">remove</span></span> <span class="hljs-string"><span class="hljs-string">'backups/public'</span></span>: Remote I/O <span class="hljs-built_in"><span class="hljs-built_in">error</span></span> rm: cannot <span class="hljs-built_in"><span class="hljs-built_in">remove</span></span> <span class="hljs-string"><span class="hljs-string">'backups/mongo/5919d69b46e0fb008d23778c/mc.ru-msk'</span></span>: Directory <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> empty rm: cannot <span class="hljs-built_in"><span class="hljs-built_in">remove</span></span> <span class="hljs-string"><span class="hljs-string">'billing/2018-02-02_before-update_0.10.0/mongodb/'</span></span>: Stale file handle</code> </pre> <br>  I read about 30 such applications that start in 2013.  There is no solution to the problem anywhere. <br><br>  Red Hat recommends <a href="https://access.redhat.com/solutions/1264803">updating the version</a> , but it did not help us. <br><br>  Our workaround is to simply clean up the remnants of broken directories in bricks on all nodes: <br><br><pre> <code class="bash hljs">pdsh -w server[1-3] -- rm -rf /<span class="hljs-built_in"><span class="hljs-built_in">export</span></span>/brick[0-9]/holodilnik/&lt;failed_dir_path&gt;</code> </pre> <br>  But further - worse. <br><br>  <b>Problem 2, the worst</b> <br>  We tried to unpack the archive with a large number of files inside the Striped Volum balls and got dangling tar xvfz in Uninterruptible sleep.  Which is treated only by the reboot of the client node. <br><br>  Realizing that it was impossible to continue to live like this, we turned to the last configuration that we had not tried, which did not inspire confidence in us, erasure coding.  Its only complexity is in understanding the principle of assembling volyum. <br><br>  Having banished all the same destructive tests, we got the same pleasant results.  Millions of files were uploaded and deleted.  As soon as we tried, we failed to break the Dispersed volume.  We have seen a higher load on the CPU, but for now this is uncritical. <br><br>  Now it backs up a piece of our infrastructure and is used as a file storage for our internal applications.  We want to live with him, see how he works under different loads.  While it is clear that the type of volyum "strayp" works strangely, and the rest is very good.  Further plans are to collect 50 TB of dispersed volume 4 + 2 on six servers with a wide Infiniband channel, drive out performance tests and continue to delve deeper into its principles of operation. </div><p>Source: <a href="https://habr.com/ru/post/353666/">https://habr.com/ru/post/353666/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../353656/index.html">Lessons learned in creating the first game, and why I want to write my own engine</a></li>
<li><a href="../353658/index.html">Escape from hell async / await</a></li>
<li><a href="../353660/index.html">Experience of implementing "Continent TLS VPN" in a cluster configuration</a></li>
<li><a href="../353662/index.html">How I passed the test task for the summer internship in Yandex</a></li>
<li><a href="../353664/index.html">Impose flex-calendar</a></li>
<li><a href="../353668/index.html">Interfaces: how to tell the user if ‚ÄúOops, something went wrong‚Äù</a></li>
<li><a href="../353672/index.html">Distribution of static content - an account for milliseconds</a></li>
<li><a href="../353674/index.html">Divorce as it is (or "confirmation confirmation discord!")</a></li>
<li><a href="../353676/index.html">Attention! S in Ethereum stands for Security. Part 4. Tools</a></li>
<li><a href="../353678/index.html">Reverse engineering "Kazakov 3", part of the network: create a local server</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>