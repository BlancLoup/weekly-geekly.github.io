<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Proper preparation and working with ZFS under FreeBSD</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Some time ago, the problem arose of building a sufficiently capacious array for storing operational incremental backups. Moreover, they did not partic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Proper preparation and working with ZFS under FreeBSD</h1><div class="post__text post__text-html js-mediator-article">  Some time ago, the problem arose of building a sufficiently capacious array for storing operational incremental backups.  Moreover, they did not particularly want to spend money, but the place was necessary.  The solution was simple and fairly convenient.  Next, a lot of text. <br><br><a name="habracut"></a><br>  The basis was taken chassis <a href="http://supermicro.com/products/chassis/2U/825/SC825TQ-560LP.cfm">SuperChassis 825TQ-560LPB</a> <a href="http://supermicro.com/products/chassis/2U/825/SC825TQ-560LP.cfm"><img src="https://habrastorage.org/getpro/habr/post_images/440/0c6/308/4400c63083547cb3e44ec7fdbb751624.jpg"></a> <br><br>  In which 8 Hitachi 1TB disks and the LSI Logic SAS HBA SAS3081E-R controller were inserted.  We planned the scheme in advance without a hardware raid based only on the capabilities of ZFS. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The first version of the server on FreeBSD 7.2-STABLE lived for about 3 months, during which time the main bugs were caught in the form of a limitation of the used memory vm.kmem_size and vfs.zfs.arc_max. <br>  With a lack of memory allocated to the kernel, the system fell into a panic, with a lack of arc_max buffers, the operation speed dropped very strongly.  To boot the system, a 1GB USB flash drive was used, which made it possible to quietly change the version of the system by simply replacing the flash drive with the new build. <br><br>  The scheme of the constructed array was based on addressing by the name of the disk (device). <br>  Further in the text will be inserts from dmesg and console commands of work. <br><blockquote><code>mpt0: &lt;LSILogic SAS/SATA Adapter&gt; port 0x2000-0x20ff mem 0xdc210000-0xdc213fff,0xdc200000-0xdc20ffff irq 16 at device 0.0 on pci3 <br> mpt0: [ITHREAD] <br> mpt0: MPI Version=1.5.20.0 <br> mpt0: Capabilities: ( RAID-0 RAID-1E RAID-1 ) <br> mpt0: 0 Active Volumes (2 Max) <br> mpt0: 0 Hidden Drive Members (14 Max) <br> <br> da0 at mpt0 bus 0 scbus0 target 0 lun 0 <br> da0: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da0: 300.000MB/s transfers <br> da0: Command Queueing enabled <br> da0: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da1 at mpt0 bus 0 scbus0 target 1 lun 0 <br> da1: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da1: 300.000MB/s transfers <br> da1: Command Queueing enabled <br> da1: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da2 at mpt0 bus 0 scbus0 target 2 lun 0 <br> da2: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da2: 300.000MB/s transfers <br> da2: Command Queueing enabled <br> da2: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da3 at mpt0 bus 0 scbus0 target 3 lun 0 <br> da3: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da3: 300.000MB/s transfers <br> da3: Command Queueing enabled <br> da3: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da4 at mpt0 bus 0 scbus0 target 4 lun 0 <br> da4: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da4: 300.000MB/s transfers <br> da4: Command Queueing enabled <br> da4: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da5 at mpt0 bus 0 scbus0 target 5 lun 0 <br> da5: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da5: 300.000MB/s transfers <br> da5: Command Queueing enabled <br> da5: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da6 at mpt0 bus 0 scbus0 target 7 lun 0 <br> da6: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da6: 300.000MB/s transfers <br> da6: Command Queueing enabled <br> da6: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da7 at mpt0 bus 0 scbus0 target 8 lun 0 <br> da7: &lt;ATA Hitachi HDE72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da7: 300.000MB/s transfers <br> da7: Command Queueing enabled <br> da7: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> ugen3.2: &lt;JetFlash&gt; at usbus3 <br> umass0: &lt;JetFlash Mass Storage Device, class 0/0, rev 2.00/1.00, addr 2&gt; on usbus3 <br> umass0: SCSI over Bulk-Only; quirks = 0x0100 <br> umass0:2:0:-1: Attached to scbus2 <br> da8 at umass-sim0 bus 0 scbus2 target 0 lun 0 <br> da8: &lt;JetFlash Transcend 1GB 8.07&gt; Removable Direct Access SCSI-2 device <br> da8: 40.000MB/s transfers <br> da8: 963MB (1972224 512 byte sectors: 64H 32S/T 963C) <br> Trying to mount root from ufs:/dev/ufs/FBSDUSB <br></code> </blockquote><br>  In principle, everything is more or less clear.  There is a controller, there are moons, there are disks, there are names of connected devices to the controller. <br><blockquote><pre> backupstorage # zpool create storage raidz da0 da1 da2 da3 da4 da5 da6 da7
 backupstorage # zpool status -v
   pool: storage
  state: ONLINE
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             da0 ONLINE 0 0 0
             da1 ONLINE 0 0 0
             da2 ONLINE 0 0 0
             da3 ONLINE 0 0 0
             da4 ONLINE 0 0 0
             da5 ONLINE 0 0 0
             da6 ONLINE 0 0 0
             da7 ONLINE 0 0 0<font></font>
<font></font>
 errors: No known data errors
</pre></blockquote><br><br>  Everything works until one of the disks dies.  I started to crumble the seventh disk and the controller for timeout threw it out of the array.  And I was not sure that the disc itself fell down, because the discs were completely new and were not subjected to any loads. <br>  DFT from Hitachi did not find any errors and said that everything was normal, only sometimes it slowed down.  MHDD found many sectors with an access time of about 500 ms, after 50 such sectors I just changed the drive under warranty. <br><br>  But the disc is really half the trouble.  I overcame the disc, but the problems caught by its failures made me think. <br><br>  <b>Problem one: Array with disc numbering not bound to moons</b> <br>  ZFS in Solaris was designed with reference to the mapping of disks to controller numbers and moons on these controllers.  In the case of using such a accounting scheme, you only need to replace the dead disk and synchronize the data in the array.  In FreeBSD, as in Linux, device naming usually goes transparent and does not depend on the physical port on the controller.  This is actually the biggest ambush. <br>  For example, take and pull out from the system 5 disk, emulating a hardware failure on the disk. <br><blockquote><pre> backupstorage # camcontrol rescan all
 Re-scan of bus 0 was successful
 Re-scan of bus 1 was successful
 Re-scan of bus 2 was successful<font></font>
<font></font>
 mpt0: mpt_cam_event: 0x16
 mpt0: mpt_cam_event: 0x12
 mpt0: mpt_cam_event: 0x16
 mpt0: mpt_cam_event: 0x16
 mpt0: mpt_cam_event: 0x16
 (da4: mpt0: 0: 4: 0): lost device
 (da4: mpt0: 0: 4: 0): Synchronize cache failed, status == 0x4a, scsi status == 0x0
 (da4: mpt0: 0: 4: 0): removing device entry<font></font>
<font></font>
 backupstorage # zpool status -v
   pool: storage
  state: DEGRADED
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage DEGRADED 0 0 0
           raidz1 DEGRADED 0 0 0
             da0 ONLINE 0 0 0
             da1 ONLINE 0 0 0
             da2 ONLINE 0 0 0
             da3 ONLINE 0 0 0
             da4 REMOVED 0 0 0
             da5 ONLINE 0 0 0
             da6 ONLINE 0 0 0
             da7 ONLINE 0 0 0
</pre></blockquote><br>  All perfectly.  The controller saw that he had lost the disk and marked it as missing. <br>  You can insert a new disk and synchronize the array.  BUT if you reboot the server, then <br>  a very interesting picture is waiting for you (I will remove the extra information from the dmesg quoting in order not to overload the screen with text) <br><blockquote> <code>da0 at mpt0 bus 0 scbus0 target 0 lun 0 <br> da0: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da0: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da1 at mpt0 bus 0 scbus0 target 1 lun 0 <br> da1: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da1: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da2 at mpt0 bus 0 scbus0 target 2 lun 0 <br> da2: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da2: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da3 at mpt0 bus 0 scbus0 target 3 lun 0 <br> da3: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da3: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da4 at mpt0 bus 0 scbus0 target 5 lun 0 <br> da4: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da4: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da5 at mpt0 bus 0 scbus0 target 7 lun 0 <br> da5: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da5: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> da6 at mpt0 bus 0 scbus0 target 8 lun 0 <br> da6: &lt;ATA Hitachi HDE72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da6: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> SMP: AP CPU #1 Launched! <br> GEOM: da0: the primary GPT table is corrupt or invalid. <br> GEOM: da0: using the secondary instead -- recovery strongly advised. <br> GEOM: da1: the primary GPT table is corrupt or invalid. <br> GEOM: da1: using the secondary instead -- recovery strongly advised. <br> GEOM: da2: the primary GPT table is corrupt or invalid. <br> GEOM: da2: using the secondary instead -- recovery strongly advised. <br> GEOM: da3: the primary GPT table is corrupt or invalid. <br> GEOM: da3: using the secondary instead -- recovery strongly advised. <br> GEOM: da4: the primary GPT table is corrupt or invalid. <br> GEOM: da4: using the secondary instead -- recovery strongly advised. <br> GEOM: da5: the primary GPT table is corrupt or invalid. <br> GEOM: da5: using the secondary instead -- recovery strongly advised. <br> GEOM: da6: the primary GPT table is corrupt or invalid. <br> GEOM: da6: using the secondary instead -- recovery strongly advised. <br> ugen3.2: &lt;JetFlash&gt; at usbus3 <br> umass0: &lt;JetFlash Mass Storage Device, class 0/0, rev 2.00/1.00, addr 2&gt; on usbus3 <br> umass0: SCSI over Bulk-Only; quirks = 0x0100 <br> umass0:2:0:-1: Attached to scbus2 <br> da7 at umass-sim0 bus 0 scbus2 target 0 lun 0 <br> da7: &lt;JetFlash Transcend 1GB 8.07&gt; Removable Direct Access SCSI-2 device <br> da7: 40.000MB/s transfers <br> da7: 963MB (1972224 512 byte sectors: 64H 32S/T 963C) <br> Trying to mount root from ufs:/dev/ufs/FBSDUSB <br> GEOM: da4: the primary GPT table is corrupt or invalid. <br> GEOM: da4: using the secondary instead -- recovery strongly advised. <br> GEOM: da5: the primary GPT table is corrupt or invalid. <br> GEOM: da5: using the secondary instead -- recovery strongly advised. <br> GEOM: da0: the primary GPT table is corrupt or invalid. <br> GEOM: da0: using the secondary instead -- recovery strongly advised. <br> GEOM: da1: the primary GPT table is corrupt or invalid. <br> GEOM: da1: using the secondary instead -- recovery strongly advised. <br> GEOM: da2: the primary GPT table is corrupt or invalid. <br> GEOM: da2: using the secondary instead -- recovery strongly advised. <br> GEOM: da3: the primary GPT table is corrupt or invalid. <br> GEOM: da3: using the secondary instead -- recovery strongly advised. <br> GEOM: da6: the primary GPT table is corrupt or invalid. <br> GEOM: da6: using the secondary instead -- recovery strongly advised. <br> GEOM: da4: the primary GPT table is corrupt or invalid. <br> GEOM: da4: using the secondary instead -- recovery strongly advised. <br></code> </blockquote><br>  What do we see? <br>  We see that in comparison with the first dmesg dump, the devices became 8 instead of 9 and the flash drive became da7 instead of da8.  ZFS begins to curse that there are problems with reading GPT tags and that everything is bad. <br>  Zpool stupidly collapsed in front of his eyes, as the numbering of the disks left 1 up, which in turn caused a loss of an array of landmarks for work and confusion in the labels of the disks. <br><blockquote><pre> backupstorage # zpool status -v
   pool: storage
  state: UNAVAIL
 status: label is missing
         or invalid.  There are no replicas to continue
         functioning.
 action: Destroy and re-create the pool from a backup source.
    see: http://www.sun.com/msg/ZFS-8000-5E
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage UNAVAIL 0 0 0 insufficient replicas
           raidz1 UNAVAIL 0 0 0 insufficient replicas
             da0 ONLINE 0 0 0
             da1 ONLINE 0 0 0
             da2 ONLINE 0 0 0
             da3 ONLINE 0 0 0
             da4 FAULTED 0 0 0 corrupted data
             da5 FAULTED 0 0 0 corrupted data
             da6 FAULTED 0 0 0 corrupted data
             da6 ONLINE 0 0 0
</pre></blockquote><br>  Please note that we have 2 !!!  disk da6.  All this is because we have a physical disk defined as da6 and there is a GPT da6 label on the disk that used to be da6. <br><br>  now try to stick the disk that pulled out. <br><blockquote> <code>backupstorage# camcontrol rescan all <br> Re-scan of bus 0 was successful <br> Re-scan of bus 1 was successful <br> Re-scan of bus 2 was successful <br> <br> da8 at mpt0 bus 0 scbus0 target 4 lun 0 <br> da8: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device <br> da8: 300.000MB/s transfers <br> da8: Command Queueing enabled <br> da8: 953869MB (1953525168 512 byte sectors: 255H 63S/T 121601C) <br> GEOM: da8: the primary GPT table is corrupt or invalid. <br> GEOM: da8: using the secondary instead -- recovery strongly advised. <br></code> </blockquote><br>  The disk was defined, but da8 became.  Of course, we can try to rebuild the array, but it will not lead to anything good.  Therefore, just overload. <br><blockquote><pre> backupstorage # zpool status -v
   pool: storage
  state: ONLINE
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             da0 ONLINE 0 0 0
             da1 ONLINE 0 0 0
             da2 ONLINE 0 0 0
             da3 ONLINE 0 0 0
             da4 ONLINE 0 0 0
             da5 ONLINE 0 0 0
             da6 ONLINE 0 0 0
             da7 ONLINE 0 0 0<font></font>
<font></font>
 errors: No known data errors
</pre></blockquote><br><br>  After rebooting, zfs calmly finds all the disks, swears a bit in the log and continues to work. <br><br><blockquote> <code>GEOM: da0: the primary GPT table is corrupt or invalid. <br> GEOM: da0: using the secondary instead -- recovery strongly advised. <br> GEOM: da1: the primary GPT table is corrupt or invalid. <br> GEOM: da1: using the secondary instead -- recovery strongly advised. <br> GEOM: da2: the primary GPT table is corrupt or invalid. <br> GEOM: da2: using the secondary instead -- recovery strongly advised. <br> GEOM: da3: the primary GPT table is corrupt or invalid. <br> GEOM: da3: using the secondary instead -- recovery strongly advised. <br> GEOM: da4: the primary GPT table is corrupt or invalid. <br> GEOM: da4: using the secondary instead -- recovery strongly advised. <br> GEOM: da5: the primary GPT table is corrupt or invalid. <br> GEOM: da5: using the secondary instead -- recovery strongly advised. <br> GEOM: da6: the primary GPT table is corrupt or invalid. <br> GEOM: da6: using the secondary instead -- recovery strongly advised. <br> GEOM: da7: the primary GPT table is corrupt or invalid. <br> GEOM: da7: using the secondary instead -- recovery strongly advised. <br> GEOM: da0: the primary GPT table is corrupt or invalid. <br> GEOM: da0: using the secondary instead -- recovery strongly advised. <br> GEOM: da2: the primary GPT table is corrupt or invalid. <br> GEOM: da2: using the secondary instead -- recovery strongly advised. <br> GEOM: da7: the primary GPT table is corrupt or invalid. <br> GEOM: da7: using the secondary instead -- recovery strongly advised. <br></code> </blockquote><br>  The resilver is almost painless, since we didn‚Äôt work with the data on the broken array. <br><blockquote><pre> GEOM: da0: the primary GPT table is corrupt or invalid.
 GEOM: da0: using the secondary instead - recovery strongly advised.
 GEOM: da3: the primary GPT table is corrupt or invalid.
 GEOM: da3: using the secondary instead - recovery strongly advised.
 ====================<font></font>
<font></font>
 backupstorage # zpool status -v
   pool: storage
  state: ONLINE
  scrub: resilver completed after 0h0m with 0 errors on Wed Nov 25 11:06:15
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             da0 ONLINE 0 0 0
             da1 ONLINE 0 0 0
             da2 ONLINE 0 0 0 512 resilvered
             da3 ONLINE 0 0 0 512 resilvered
             da4 ONLINE 0 0 0
             da5 ONLINE 0 0 0
             da6 ONLINE 0 0 0 512 resilvered
             da7 ONLINE 0 0 0 512 resilvered<font></font>
<font></font>
 errors: No known data errors
</pre></blockquote><br>  In general, we were convinced that such a scheme is not very reliable with automatic disk numbering in the controller.  Although if the number of disks does not change - they can be rearranged in any order.  The array will work properly. <br><br>  <b>Problem two: geomlabel on disks.</b> <br>  In order to try to solve the problem with the name of the disks, it was decided to try to label them with labels and add tags to the array.  When you create a label on the disk - it is written to the end of the disk, when the disk is initialized, the system reads the label and creates virtual devices / dev / label / label name. <br><blockquote><pre> backupstorage # zpool create storage raidz label / disk0 label / disk1 label / disk2 label / disk3 label / disk4 label / disk5 label / disk6 label / disk7
 backupstorage # zpool status -v
   pool: storage
  state: ONLINE
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             label / disk0 ONLINE 0 0 0
             label / disk1 ONLINE 0 0 0
             label / disk2 ONLINE 0 0 0
             label / disk3 ONLINE 0 0 0
             label / disk4 ONLINE 0 0 0
             label / disk5 ONLINE 0 0 0
             label / disk6 ONLINE 0 0 0
             label / disk7 ONLINE 0 0 0<font></font>
<font></font>
 errors: No known data errors
 backupstorage # ls / dev / label / disk *
 / dev / label / disk0 / dev / label / disk2 / dev / label / disk4 / dev / label / disk6
 / dev / label / disk1 / dev / label / disk3 / dev / label / disk5 / dev / label / disk7
</pre></blockquote><br>  The array lives normally, now we pull out disk1 and set aside. <br><br>  Reboot the server and look in the log. <br><blockquote><pre> GEOM: da0: corrupt or invalid GPT detected.
 GEOM: da0: GPT rejected - may not be recoverable.
 GEOM: da1: corrupt or invalid GPT detected.
 GEOM: da1: GPT rejected - may not be recoverable.
 GEOM: da2: corrupt or invalid GPT detected.
 GEOM: da2: GPT rejected - may not be recoverable.
 GEOM: da3: corrupt or invalid GPT detected.
 GEOM: da3: GPT rejected - may not be recoverable.
 GEOM: da4: corrupt or invalid GPT detected.
 GEOM: da4: GPT rejected - may not be recoverable.
 GEOM: da5: corrupt or invalid GPT detected.
 GEOM: da5: GPT rejected - may not be recoverable.
 GEOM: da6: corrupt or invalid GPT detected.
 GEOM: da6: GPT rejected - may not be recoverable.
 GEOM: label / disk0: corrupt or invalid GPT detected.
 GEOM: label / disk0: GPT rejected - may not be recoverable.
 GEOM: label / disk2: corrupt or invalid GPT detected.
 GEOM: label / disk2: GPT rejected - may not be recoverable.
 GEOM: label / disk3: corrupt or invalid GPT detected.
 GEOM: label / disk3: GPT rejected - may not be recoverable.
 GEOM: label / disk4: corrupt or invalid GPT detected.
 GEOM: label / disk4: GPT rejected - may not be recoverable.
 GEOM: label / disk5: corrupt or invalid GPT detected.
 GEOM: label / disk5: GPT rejected - may not be recoverable.
 GEOM: label / disk6: corrupt or invalid GPT detected.
 GEOM: label / disk6: GPT rejected - may not be recoverable.
 GEOM: label / disk7: corrupt or invalid GPT detected.
 GEOM: label / disk7: GPT rejected - may not be recoverable.
 da7 at umass-sim0 bus 0 scbus2 target 0 lun 0
 da7: &lt;JetFlash Transcend 1GB 8.07&gt; Removable Direct Access SCSI-2 device
 da7: 40.000MB / s transfers
 da7: 963MB (1972224 512 byte sectors: 64H 32S / T 963C)
 Trying to mount root from ufs: / dev / ufs / FBSDUSB
 GEOM: da0: corrupt or invalid GPT detected.
 GEOM: da0: GPT rejected - may not be recoverable.
 GEOM: da1: corrupt or invalid GPT detected.
 GEOM: da1: GPT rejected - may not be recoverable.
 GEOM: label / disk2: corrupt or invalid GPT detected.
 GEOM: label / disk2: GPT rejected - may not be recoverable.
 GEOM: da2: corrupt or invalid GPT detected.
 GEOM: da2: GPT rejected - may not be recoverable.
 GEOM: label / disk3: corrupt or invalid GPT detected.
 GEOM: label / disk3: GPT rejected - may not be recoverable.
 GEOM: da3: corrupt or invalid GPT detected.
 GEOM: da3: GPT rejected - may not be recoverable.
 GEOM: label / disk4: corrupt or invalid GPT detected.
 GEOM: label / disk4: GPT rejected - may not be recoverable.
 GEOM: da4: corrupt or invalid GPT detected.
 GEOM: da4: GPT rejected - may not be recoverable.
 GEOM: label / disk5: corrupt or invalid GPT detected.
 GEOM: label / disk5: GPT rejected - may not be recoverable.
 GEOM: da5: corrupt or invalid GPT detected.
 GEOM: da5: GPT rejected - may not be recoverable.
</pre></blockquote><br>  But with all the abuse, the array turns out to be alive.  with a live structure and DEGRADED status, which is much better than in the first case. <br><blockquote><pre> backupstorage # zpool status
   pool: storage
  state: DEGRADED
 status: One or more devices has experienced an unrecoverable error.  An
         attempt was made to correct the error.  Applications are unaffected.
 action: Determine if clear
         using 'zpool clear' or replace the device with 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-9P
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage DEGRADED 0 0 0
           raidz1 DEGRADED 0 0 0
             label / disk0 ONLINE 0 0 0
             label / disk1 REMOVED 0 94 0
             label / disk2 ONLINE 0 0 0
             label / disk3 ONLINE 0 0 0
             label / disk4 ONLINE 0 0 0
             label / disk5 ONLINE 0 0 0
             label / disk6 ONLINE 0 0 0
             label / disk7 ONLINE 0 0 0<font></font>
<font></font>
 errors: No known data errors
 ====================
</pre></blockquote><br>  Now we remove the disk that we pulled out and insert it back into the system. <br>  The label on the disc is recorded at the very end of the disc, so the regular zeroing of the beginning of the disc will not help us.  you have to either end the end or just the whole disk.  It all depends on your time. <br><blockquote><pre> mpt0: mpt_cam_event: 0x16
 mpt0: mpt_cam_event: 0x12
 mpt0: mpt_cam_event: 0x16
 da8 at mpt0 bus 0 scbus0 target 1 lun 0
 da8: &lt;ATA Hitachi HDT72101 A31B&gt; Fixed Direct Access SCSI-5 device
 da8: 300.000MB / s transfers
 da8: Command Queueing enabled
 da8: 953869MB (1953525168 512 byte sectors: 255H 63S / T 121601C)
</pre></blockquote><br>  Give the disk ‚Äúdisk8‚Äù label and try to replace the ‚Äúdead‚Äù disk with a new one. <br><blockquote><pre> backupstorage # ls / dev / label /
 disk0 disk2 disk3 disk4 disk5 disk6 disk7 disk8<font></font>
<font></font>
 backupstorage # zpool replace storage label / disk1 label / disk8
 cannot replace label / disk1 with label / disk8: label / disk8 is busy
 backupstorage # zpool replace -f storage label / disk1 label / disk8
 cannot replace label / disk1 with label / disk8: label / disk8 is busy
</pre></blockquote><br>  The system refuses to change the disk for us, citing its busyness.  Replace the "dead" disk can only be a direct device name.  Why this was done, I did not fully understand.  We discard the idea of ‚Äã‚Äãreplacing the labels on the new disk and try to simply label the ‚Äúdisk1‚Äù label on the new disk. <br><blockquote><pre> backupstorage # glabel label disk1 da8
</pre></blockquote><br>  Now we need to say that the disk has returned to the system. <br><blockquote><pre> backupstorage # zpool online storage label / disk1
 backupstorage # zpool status
   pool: storage
  state: ONLINE
 status: One or more devices has experienced an unrecoverable error.  An
         attempt was made to correct the error.  Applications are unaffected.
 action: Determine if clear
         using 'zpool clear' or replace the device with 'zpool replace'.
    see: http://www.sun.com/msg/ZFS-8000-9P
  scrub: resilver completed after 0h0m with 0 errors on Wed Nov 25 18:29:17 2009
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             label / disk0 ONLINE 0 0 0 6.50K resilvered
             label / disk1 ONLINE 0 94 1 10.5K resilvered
             label / disk2 ONLINE 0 0 0 6K resilvered
             label / disk3 ONLINE 0 0 0 3.50K resilvered
             label / disk4 ONLINE 0 0 0 6.50K resilvered
             label / disk5 ONLINE 0 0 0 6.50K resilvered
             label / disk6 ONLINE 0 0 0 5.50K resilvered
             label / disk7 ONLINE 0 0 0 3K resilvered<font></font>
<font></font>
 errors: No known data errors
</pre></blockquote><br>  Everything falls into place and after synchronization you can reset the pool status to normal. <br>  And here the problem begins.  Since the disk label made by glabel is written at the end of the disk, zfs basically does not know anything about where it is recorded.  And when the disk is full, it erases this label.  The disk in the array is assigned a physical name and we return to point 1 of our problems. <br><br>  <b>Solution to the problem</b> <br>  The solution to the problem turned out to be a bit trite and simple.  A long time ago, FreeBSD began to be able to do GPT partitions on large volumes of disks.  In FreeBSD 7.2, GPT partition naming did not work and was accessed by the direct device name / dev / da0p1 (example for the first GPT partition) <br>  In FreeBSD 8.0, a change was made to the GPT tag system.  And now GPT partitions can be named and referred to as virtual devices via the / dev / gpt / partition label. <br><br>  All we really need to do is to name the partitions on the disks and build an array of them.  How to do this is written in ZFS Boot Howto which is very quickly googled. <br><blockquote><pre> backupstorage # gpart create -s GPT ad0
 backupstorage # gpart add -b 34 -s 1953525101 -i 1 -t freebsd-zfs -l disk0 ad0
 backupstorage # gpart show
 =&gt; 34 1953525101 da0 GPT (932G)
           34 1953525101 1 freebsd-zfs (932G)
 backupstorage # gpart show -l
 =&gt; 34 1953525101 da0 GPT (932G)
           34 1953525101 1 disk0 (932G)
 backupstorage # ls / dev / gpt
 disk0
</pre></blockquote><br>  After creating a GPT partition on a <b>gpart show,</b> you can see where the data area on the disk begins and ends.  Next, we create this partition and give it the label ‚Äúdisk0‚Äù. <br>  We perform this operation for all disks in the system and collect an array from the resulting partitions. <br><blockquote><pre> backupstorage # zpool status -v
   pool: storage
  state: ONLINE
  scrub: none requested
 config:<font></font>
<font></font>
         NAME STATE READ WRITE CKSUM
         storage ONLINE 0 0 0
           raidz1 ONLINE 0 0 0
             gpt / disk0 ONLINE 0 0 0
             gpt / disk1 ONLINE 0 0 0
             gpt / disk2 ONLINE 0 0 0
             gpt / disk3 ONLINE 0 0 0
             gpt / disk4 ONLINE 0 0 0
             gpt / disk5 ONLINE 0 0 0
             gpt / disk6 ONLINE 0 0 0
             gpt / disk7 ONLINE 0 0 0<font></font>
<font></font>
 errors: No known data errors
</pre></blockquote><br>  The server is calmly experiencing any reboot and disk swapping, as well as replacing them with the same label.  The speed of this array on the network reaches 70-80 megabytes per second.  The local writing speed to the array, depending on the buffer occupancy, reaches 200 megabytes per second. <br><br>  PS: when using gpt tags, I came across a strange glitch that the system did not see the new label on the disk, but then it passed by itself. <br>  PPS: to enthusiasts who try to run FreeBSD 8.0 from a flash drive (if they haven‚Äôt repaired this yet) - this dirty hack comes in handy. <br><blockquote> <code>Index: sys/kern/vfs_mount.c <br> =================================================================== <br> RCS file: /home/ncvs/src/sys/kern/vfs_mount.c,v <br> retrieving revision 1.308 <br> diff -u -r1.308 vfs_mount.c <br> --- sys/kern/vfs_mount.c 5 Jun 2009 14:55:22 -0000 1.308 <br> +++ sys/kern/vfs_mount.c 29 Sep 2009 17:08:25 -0000 <br> @@ -1645,6 +1645,9 @@ <br> options = NULL; <br> + /* NASTY HACK: wait for USB sticks to appear */ <br> + pause("usbhack", hz * 10); <br> + <br> root_mount_prepare(); <br> mount_zone = uma_zcreate("Mountpoints", sizeof(struct mount), <br></code> </blockquote><br>  In the new USB stack, when loading the USB kernel, devices are not always detected on time and an error occurs when mounting system partitions. <br>  This hack exposes in the system a timeout of 10 seconds to wait for readiness of the USB drive. <br><br>  PPPS: if you have questions, ask. <br><br>  Settings loader.conf for a machine with 2 gigabytes of memory. <br><blockquote>  vm.kmem_size = "1536M" <br>  vm.kmem_size_max = "1536M" <br>  vfs.zfs.arc_max = "384M" <br></blockquote><br><br>  <font>¬© Aborche 2009</font> <br><img src="https://habrastorage.org/getpro/habr/post_images/cff/19a/9b6/cff19a9b6d0354bbc4d1a45beda58acf.jpg"></div><p>Source: <a href="https://habr.com/ru/post/77722/">https://habr.com/ru/post/77722/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../77710/index.html">Little tricks when registering LLC</a></li>
<li><a href="../77711/index.html">IT in healthcare</a></li>
<li><a href="../77712/index.html">Google Groups finally on their domains</a></li>
<li><a href="../77716/index.html">Co-location or Collocation?</a></li>
<li><a href="../77719/index.html">Techno-geeks having fun</a></li>
<li><a href="../77724/index.html">Amazon loses $ 2 on each e-book</a></li>
<li><a href="../77726/index.html">Bulshytbingo.ru</a></li>
<li><a href="../77727/index.html">Google chef: ‚ÄúOnly villains are worried about online privacy.‚Äù</a></li>
<li><a href="../77729/index.html">Foolish children, or the wrong picture of the world</a></li>
<li><a href="../77733/index.html">We close access to the site</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>