<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommender systems: Bayes theorem and naive Bayes classifier</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this part we will not talk about recommender systems per se. Instead, we will separately concentrate on the main machine learning tool, the Bayes t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommender systems: Bayes theorem and naive Bayes classifier</h1><div class="post__text post__text-html js-mediator-article">  In this part we will not talk about recommender systems per se.  Instead, we will separately concentrate on the main machine learning tool, the Bayes theorem, and consider one simple example of its application ‚Äî the naive Bayes classifier.  Disclaimer: I‚Äôm unlikely to tell something new to a reader familiar with the subject, let's talk mainly about the basic philosophy of machine learning. <br><br><img src="http://clip2net.com/clip/m7004/1345979346-clip-264kb.png" alt="image"><br><a name="habracut"></a><br>  Bayes' theorem either remembers or is trivial to deduce anyone who has passed even the most basic course of probability theory.  Remember what conditional probability is <img src="http://mathurl.com/9h48nft.png" alt="image">  <i>x</i> events provided <i>y</i> event?  Directly by definition: <img src="http://mathurl.com/8cr5jjf.png" alt="image">  where <img src="http://mathurl.com/9olnpmd.png" alt="image">  Is the joint probability of <i>x</i> and <i>y</i> , and <i>p</i> ( <i>x</i> ) and <i>p</i> ( <i>y</i> ) are the probabilities of each event separately.  Hence, the joint probability can be expressed in two ways: <br><img src="http://mathurl.com/9vb7uhw.png" alt="image">  . <br><br>  Well, here's the Bayes theorem: <br><img src="https://habrastorage.org/getpro/habr/post_images/520/1ed/d6d/5201edd6d33dade84e2e69e578c7db72.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      You probably think that I am making fun of you - how can a trivially tautological rewriting of the definition of conditional probability be the main tool of anything, especially such a big and nontrivial science as machine learning?  However, let's begin to understand;  at first, we simply rewrite the Bayes theorem in other notation (yes, I continue to mock): <br><img src="http://habrastorage.org/getpro/habr/post_images/138/e30/ccb/138e30ccb4c824996cfc08804c60b9fa.png" alt="image"><br><br>  Now let's relate this to the typical machine learning task.  Here D is the data, what we know, and Œ∏ are the parameters of the model that we want to train.  For example, <a href="http://habrahabr.ru/company/surfingbird/blog/139863/">in the SVD model, the</a> data are the ratings that users put on products, and the parameters of the model are the factors that we train for users and products. <br><br>  Each of the probabilities also has its own meaning. <img src="http://habrastorage.org/getpro/habr/post_images/3a1/310/83c/3a131083c486867287a21d6b223d457a.png" alt="image">  - this is what we want to find, the probability distribution of the parameters of the model <i>after</i> we have taken into account the data;  this is called <i>posterior probability</i> .  This probability, as a rule, cannot be directly found, and this is precisely where the Bayes theorem is needed. <img src="http://mathurl.com/c8edm5v.png" alt="image">  - this is the so-called <i>likelihood</i> , the probability of the data, provided the model parameters are fixed;  it is usually easy to find; in fact, the design of the model usually consists in setting the likelihood function.  BUT <img src="https://habrastorage.org/getpro/habr/post_images/a7f/f23/018/a7ff23018d643ebd4a20be82edd6750d.png" alt="image">  - prior probability, it is a mathematical formalization of our intuition about the subject, a formalization of what we knew before, even before any experiments. <br><br>  Here, probably, neither the time nor the place to go into it, but the merit of Reverend Thomas Bayes was, of course, not to rewrite the definition of conditional probability in two lines (there were no such definitions then), but just to put forward and develop such a view on the very concept of probability.  Today, the ‚ÄúBayesian approach‚Äù refers to the consideration of probabilities from the standpoint of ‚Äúdegrees of confidence‚Äù rather than the frictional (from the word frequency, not freak!) ‚ÄúFraction of successful experiments when the total number of experiments is infinite‚Äù.  In particular, this allows us to talk about the probabilities of one-time events - in fact there is no ‚Äúnumber of experiments tending to infinity‚Äù for events like ‚ÄúRussia will become the world champion in 2018‚Äù or, closer to our subject, ‚ÄúVasya will like the film‚Äù Tractor drivers "";  it's more like a dinosaur: either like it or not.  Well, mathematics, of course, is always the same everywhere; Kolmogorov's axioms of probability don't care what they think of them. <br><br>  To consolidate the traversed - a simple example.  Consider the task of categorizing texts: for example, suppose that we are trying to sort the news flow based on an existing database with topics: sports, economics, culture ... We will use the so-called bag-of-words model: submit a document (multi) set words that it contains.  As a result, each test case x takes values ‚Äã‚Äãfrom a set of categories V and is described by attributes <img src="http://mathurl.com/cuazv5v.png" alt="image">  .  We need to find the most likely value of this attribute, i.e. <br><img src="https://habrastorage.org/getpro/habr/post_images/fba/dde/ff4/fbaddeff45df38458d8387da8177c0ae.png" alt="image"><br>  By Bayes theorem, <br><img src="http://mathurl.com/bpl9pvf.png" alt="image"><br><br>  Estimate <img src="https://habrastorage.org/getpro/habr/post_images/06d/22b/7c6/06d22b7c6dc806c84226d2be5a1cbd26.png" alt="image">  easy: let's just estimate its frequency of occurrence.  But appreciate the different <img src="http://mathurl.com/ck34r7x.png" alt="image">  it won't work - there are too many of them <img src="http://mathurl.com/ck34r7x.png" alt="image">  - This is the probability of exactly such a set of words in messages on different topics.  Obviously, such statistics take nowhere. <br><br>  To cope with this, the naive Bayes classifier (the naive Bayes classifier ‚Äî sometimes even called idiot's Bayes) implies conditional independence of attributes, subject to a given value of the objective function: <br><img src="https://habrastorage.org/getpro/habr/post_images/7c8/de2/d30/7c8de2d301c1aa296a1da6b53d192084.png" alt="image"><br>  Now train the individual <img src="http://mathurl.com/cyy8sxn.png" alt="image">  much simpler: it is enough to calculate the statistics of the occurrence of words in categories (there is one more detail that leads to two different variants of naive Bayes, but we will not go into details now). <br><br>  Note that the naive Bayes classifier makes a damn strong assumption: in the text classification, we assume that different words in the text on the same topic appear independently of each other.  This, of course, is complete nonsense - but, nevertheless, the results are quite decent.  In fact, the naive Bayes classifier is much better than it seems.  His estimates of probabilities are optimal, of course, only in the case of real independence;  but the classifier itself is optimal in a much wider class of problems, and here's why.  At first, attributes are, of course, dependent, but their dependency is the same for different classes and ‚Äúmutually reduces‚Äù when evaluating probabilities.  The grammatical and semantic dependencies between the words are the same in the text about football and in the text about Bayesian learning.  Secondly, naive bayes are very bad for estimating probabilities, but as a classifier it is much better (usually, even if <img src="https://habrastorage.org/getpro/habr/post_images/715/cf6/2bf/715cf62bf8639499dea3dfafd48efc40.png" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/9d8/b0c/ffb/9d8b0cffbb00de50a782c9bffbc945eb.png" alt="image">  , naive Bayes will issue <img src="https://habrastorage.org/getpro/habr/post_images/9d5/876/8c2/9d58768c25cff552985b86d2f47aa914.png" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/0d2/82b/5ed/0d282b5ede87774b1f7cef0e30462b7a.png" alt="image">  , but the classification will often be correct). <br><br>  In the next series, we will complicate this example and consider the LDA model, which is able to highlight topics in the corpus of documents without any set of marked documents, moreover, so that one document can contain several topics, and also apply it to the task of recommendations. </div><p>Source: <a href="https://habr.com/ru/post/150207/">https://habr.com/ru/post/150207/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../150201/index.html">Vulnerability in Kohana?</a></li>
<li><a href="../150203/index.html">My view on Scrum</a></li>
<li><a href="../150204/index.html">Solving the problem of installing Cloud9 ide on Ubuntu 12.04</a></li>
<li><a href="../150205/index.html">Recoding video from Intel Quick Sync Video - do it quickly</a></li>
<li><a href="../150206/index.html">Bypassing the proactive protection of Kaspersky Lab products. Video demonstration</a></li>
<li><a href="../150208/index.html">Ghosts in ROM</a></li>
<li><a href="../150209/index.html">How to make friends with Truecrypt loader and Grub 2?</a></li>
<li><a href="../150210/index.html">Compiling packages</a></li>
<li><a href="../150211/index.html">‚Ç¨ 3,000 received by ReactOS from an unknown donor</a></li>
<li><a href="../150212/index.html">Tent: decentralized social web</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>