<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to the task of recognizing emotions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recognizing emotions is a hot topic in artificial intelligence. The most interesting areas of application of such technologies include: driver status ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to the task of recognizing emotions</h1><div class="post__text post__text-html js-mediator-article"><p>  Recognizing emotions is a hot topic in artificial intelligence.  The most interesting areas of application of such technologies include: driver status recognition, marketing research, video analytics systems for smart cities, human-machine interaction, monitoring of students taking online courses, wearable devices, etc. </p><br><p>  This year, MDG has dedicated its <a href="https://mlschool.speechpro.ru/">machine learning summer school</a> to this topic.  In this article I will try to give a brief excursion into the problem of recognizing the emotional state of a person and tell you about the approaches to its solution. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/626/db4/5bb/626db45bb7b0aad7fdbc2970c0b4183f.jpg" alt="image"></div><a name="habracut"></a><br><h3 id="chto-takoe-emocii">  What are emotions? </h3><br><p>  Emotion is a special kind of mental processes that express the human experience of his relationship to the world around him and himself.  According to one of the theories, the author of which is the Russian physiologist PK  Anokhin, the ability to experience emotions was developed in the process of evolution as a means of more successful adaptation of living beings to the conditions of existence.  Emotion proved to be useful for survival and allowed living beings to respond quickly and economically to external influences. </p><br><p>  Emotions play a huge role in human life and interpersonal communication.  They can be expressed in various ways: facial expressions, posture, motor responses, voice and autonomic responses (heart rate, blood pressure, respiration rate).  However, the person‚Äôs face is most expressive. </p><br><p>  Each person expresses emotions in several different ways.  The famous American psychologist <a href="https://www.youtube.com/watch%3Fv%3DX4KfdomhIIo">Paul Ekman</a> , exploring the non-verbal behavior of isolated tribes in Papua New Guinea in the 70s of the last century, found that a number of emotions, namely: anger, fear, sadness, disgust, scorn, surprise and joy are universal and can be understood by man regardless of his culture. </p><br><p>  People are able to express a wide range of emotions.  It is believed that they can be described as a combination of basic emotions (for example, nostalgia is something between a sadness and joy).  But such a categorical approach is not always convenient, since  does not allow quantitatively characterize the power of emotion.  Therefore, along with discrete models of emotions, a series of continuous ones were developed.  In the model of J. Russell, a two-dimensional basis is found, in which each emotion is characterized by a sign (valence) and intensity (arousal).  Due to its simplicity, the Russell model has recently become increasingly popular in the context of the task of automatic classification of facial expressions. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/f59/cd1/a24/f59cd1a24e77d64759641a076d86bef1.png" alt="image"></div><br><p>  So, we found out that if you are not trying to hide emotional arousal, then your current state can be assessed by facial expressions.  Moreover, using modern achievements in the field of deep learning it is even possible to build a lie detector, based on the series ‚ÄúLie to me‚Äù, the scientific basis of which was directly the work of Paul Ekman.  However, this task is not so simple.  As <a href="http://people.ict.usc.edu/~gratch/CSCI534/Readings/Barrett-context.pdf">studies of</a> neurobiologist Liza Feldman Barrett have shown, in recognizing emotions a person actively uses contextual information: voice, actions, situation.  Take a look at the pictures below, this is true.  Using only the face area, correct prediction cannot be done.  In this regard, to solve this problem, it is necessary to use both additional modalities and information about signal changes over time. </p><br><div style="text-align:center;"><img height="200" src="https://habrastorage.org/webt/el/a6/t7/ela6t7ig73rti-peenhb7_f0sgs.jpeg" alt="image"></div><br><p>  Here we consider approaches to the analysis of only two modalities: audio and video, since these signals can be obtained in a contactless way.  To get to the task in the first place you need to get the data.  Here is a list of the largest publicly accessible bases of emotions known to me.  Images and videos in these bases were manually tagged, some using Amazon Mechanical Turk. </p><br><table><thead><tr><th>  Title </th><th>  Data </th><th>  Markup </th><th>  Year of issue </th></tr></thead><tbody><tr><td>  <a href="https://www2.informatik.uni-hamburg.de/wtm/OMG-EmotionChallenge/">OMG-Emotion challenge</a> </td><td>  audio / video </td><td>  7 categories, valence / arousal </td><td>  2018 </td></tr><tr><td>  <a href="https://sites.google.com/view/emotiw2018">EmotiW challenge</a> </td><td>  audio / video </td><td>  6 categories </td><td>  2018 </td></tr><tr><td>  <a href="http://mohammadmahoor.com/affectnet/">AffectNet</a> </td><td>  Images </td><td>  7 categories, valence / arousal </td><td>  2017 </td></tr><tr><td>  <a href="https://ibug.doc.ic.ac.uk/resources/afew-va-database/">AFEW-VA</a> </td><td>  video </td><td>  valence / arousal </td><td>  2017 </td></tr><tr><td>  <a href="http://cbcsl.ece.ohio-state.edu/EmotionNetChallenge/">EmotioNet challenge</a> </td><td>  Images </td><td>  16 categories </td><td>  2017 </td></tr><tr><td>  <a href="https://www.behnaznojavan.com/emoreact">EmoReact</a> </td><td>  audio / video </td><td>  17 categories </td><td>  2016 </td></tr></tbody></table><br><h3 id="klassicheskiy-podhod-k-zadache-klassifikacii-emociy">  The classic approach to the task of classifying emotions </h3><br><p>  The easiest way to determine emotions by face is based on the classification of key points (facial landmarks), the coordinates of which can be obtained using various <a href="http://www.menpo.org/menpofit/pdm.html">PDM</a> , <a href="https://github.com/TadasBaltrusaitis/OpenFace">CML</a> , <a href="http://www.menpo.org/menpofit/aam.html">AAM</a> , <a href="http://cmp.felk.cvut.cz/~uricamic/clandmark/index.php%3Fpage%3Dinfo">DPM</a> or <a href="https://github.com/1adrianb/2D-and-3D-face-alignment">CNN</a> algorithms.  Usually mark up from 5 to 68 points, tying them to the position of the eyebrows, eyes, lips, nose, jaw, which allows you to partially capture facial expressions.  The normalized coordinates of points can be directly submitted to the classifier (for example, SVM or Random Forest) and get a basic solution.  Naturally, the position of persons should be aligned. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/getpro/habr/post_images/c60/24a/dbe/c6024adbeaecca98404dcaae3361785e.jpg" alt="image"></div><br><p>  Simple use of coordinates without a visual component leads to a significant loss of useful information, therefore, various descriptors are computed at these points to improve the system: <a href="http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_local_binary_pattern.html">LBP</a> , <a href="http://scikit-image.org/docs/dev/auto_examples/features_detection/plot_hog.html">HOG</a> , <a href="https://ianlondon.github.io/blog/how-to-sift-opencv/">SIFT</a> , <a href="https://gilscvblog.com/2015/11/07/performance-evaluation-of-binary-descriptor-introducing-the-latch-descriptor/">LATCH</a> , etc. After concatenating the descriptors and reducing the dimension using PCA, the resulting feature vector can be used to classify of emotions. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/u7/lh/oa/u7lhoatsm4vbqzb_zlksw9ekygy.jpeg" alt="image"></div><br><p>  <a href="http://www.mdpi.com/1424-8220/18/2/401/pdf">article link</a> </p><br><p>  However, this approach is already considered obsolete, since it is well known that deep convolutional networks are the best choice for analyzing visual data. </p><br><h3 id="klassifikaciya-emociy-s-primeneniem-deep-learning">  Emotion classification using deep learning </h3><br><p>  In order to build a neural network classifier, it is enough to take some kind of network with the basic architecture, previously trained on ImageNet, and retrain the last few layers.  So you can get a good basic solution for classifying various data, but given the specifics of the problem, the neural networks used for large-scale <a href="https://arxiv.org/abs/1711.04598">face recognition</a> tasks will be more suitable. </p><br><p>  So, to build a classifier of emotions for individual images is quite simple, but as we found out, the snapshots do not quite accurately reflect the true emotions that a person experiences in this situation.  Therefore, to improve the accuracy of the system, it is necessary to analyze the sequence of frames.  This can be done in two ways.  The first way is to feed the high-level features received from CNN, which classify each individual frame, into a recurrent network (for example, LSTM) to capture the time component. </p><br><div style="text-align:center;"><img height="300" src="https://habrastorage.org/webt/ik/wi/zh/ikwizhwy65xydfoyu15ql3szjbi.jpeg" alt="image"></div><br><p>  <a href="https://dl.acm.org/citation.cfm%3Fid%3D3143012">article link</a> </p><br><p>  The second way is to directly feed a sequence of frames taken from the video with a certain step to the input of the 3D-CNN.  CNN-like convolutions use three degrees of freedom convolutions that convert a four-dimensional entry into three-dimensional feature maps. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/iv/a2/kd/iva2kdloqzpghk8gfcymk6g5sc4.jpeg" alt="image"></div><br><p>  <a href="http://www.diva-portal.org/smash/get/diva2:1174434/FULLTEXT01.pdf">article link</a> </p><br><p>  In fact, in general, these two approaches can be combined by constructing such a monster. </p><br><div style="text-align:center;"><img height="400" src="https://habrastorage.org/webt/oo/ir/yo/ooiryorm9sh8n0cht5oud1yqbd4.jpeg" alt="image"></div><br><p>  <a href="https://arxiv.org/pdf/1705.07871.pdf">article link</a> </p><br><h3 id="klassifikaciya-emociy-po-rechi">  Classification of emotions by speech </h3><br><p>  Based on the visual data, it is possible to predict the sign of emotion with high accuracy, but when determining the intensity, it is preferable to use <a href="https://arxiv.org/pdf/1704.08619.pdf">speech signals</a> .  It is a bit more difficult to analyze audio due to the strong variability of speech duration and the voices of speakers.  Usually, they do not use the original sound wave, but various sets of <a href="http://cslt.riit.tsinghua.edu.cn/~fzheng/PAPERS/2018/1805E_ACII-Asia_Emotion-Recog-Channenge_ZXT.pdf">features</a> , for example: F0, MFCC, LPC, i-vectors, etc. In the problem of recognizing emotions through speech, the <a href="https://audeering.com/technology/opensmile/">OpenSMILE</a> open library, which contains a rich set of algorithms for analyzing speech and music signals.  After extraction, tags can be submitted to SVM or LSTM for classification. </p><br><p> Recently, however, convolutional neural networks have begun to penetrate into the field of sound analysis, displacing established approaches.  In order to apply them, the sound is presented in the form of spectrograms in a linear or mel scale, after which it is operated with the obtained spectrograms as with ordinary two-dimensional images.  At the same time, the problem of an arbitrary spectrogram size along the time axis is elegantly solved with the help of <a href="https://arxiv.org/pdf/1803.10963.pdf">statistical pooling</a> or by including a recurrent network in the architecture. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/kq/4d/ts/kq4dtsbybmnn3nsq6cwq_wek19u.jpeg" alt="image"></div><br><p>  <a href="https://arxiv.org/ftp/arxiv/papers/1707/1707.09917.pdf">article link</a> </p><br><h3 id="audiovizualnoe-raspoznavanie-emociy">  Audiovisual recognition of emotions </h3><br><p>  So, we considered a number of approaches to the analysis of audio and video modalities, the final stage remained - the unification of classifiers for the conclusion of the final decision.  The simplest way is to directly combine their ratings.  In this case, it is enough to take a maximum or average.  A more difficult option is to combine at the embedding level for each modality.  SVM is often used for this, but this is not always correct, since embeddings can have different norms.  In this regard, more advanced algorithms have been developed, for example: <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp%3Ftp%3D%26arnumber%3D7837868">Multiple Kernel Learning</a> and <a href="https://arxiv.org/pdf/1501.00102.pdf">ModDrop</a> . </p><br><p>  And of course it is worth mentioning the class of so-called <a href="https://arxiv.org/pdf/1704.08619.pdf">end-to-end</a> solutions that can be trained directly on raw data from several sensors without any preprocessing. </p><br><p>  In general, the task of automatic recognition of emotions is far from a solution.  Judging by the results of last year's Emotion Recognition in the Wild contest, the <a href="https://drive.google.com/file/d/1-mVVbabm8ePTMJKwO0itdMXB3j5vEw7h/view">best solutions</a> reach an accuracy of about 60%.  I hope that the information presented in this article will be enough to try to build your own system of recognition of emotions. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/418151/">https://habr.com/ru/post/418151/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../418141/index.html">RE: Gata / AFR Beginner Skipper Race</a></li>
<li><a href="../418143/index.html">PVS-Studio as SAST solution</a></li>
<li><a href="../418145/index.html">The first lawsuit against Roskomnadzor from the company that suffered while blocking Telegram</a></li>
<li><a href="../418147/index.html">Silent Ruby Exposures: Transactional Rails / PostgreSQL Thriller</a></li>
<li><a href="../418149/index.html">Phishing with the title tag</a></li>
<li><a href="../418153/index.html">Videos from Kolesa Android Meetup: About MVVM, Anti-Patterns and Modular Development</a></li>
<li><a href="../418155/index.html">Diode. Light-emitting diode. Zener diode</a></li>
<li><a href="../418157/index.html">The book "Elegant Objects. Java Edition ¬ª</a></li>
<li><a href="../418159/index.html">Where to go to the designer: prestigious awards of Russia, Eastern Europe and the CIS countries</a></li>
<li><a href="../418161/index.html">Stanford developed streaming batteries operating at room temperature.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>