<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>OpenCV 2.4 Flat Object Recognition</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello to all. I would like to talk about the principles underlying object recognition using OpenCV. Fortunately, for some time I had a chance to work ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>OpenCV 2.4 Flat Object Recognition</h1><div class="post__text post__text-html js-mediator-article">  Hello to all.  I would like to talk about the principles underlying object recognition using OpenCV.  Fortunately, for some time I had a chance to work in the computer vision laboratory of the VMK MSU, and I got a little insight into the wisdom of this branch of computer science.  The task, which I will consider here, was proposed at the <a href="http://prooptics.blogspot.com/2011/07/microsoft-computer-vision-school-moscow.html">Microsoft Computer Vision School Moscow 2011</a> at the seminars of Victor Erukhimov, one of the developers of the OpenCV software.  Almost in the same form, the code in question can be found in the OpenCV 2.4 demos. <br><a name="habracut"></a><br><h4>  Task formalization </h4><br>  We set the following task: an image of a scene from an ordinary USB camera and an image of a target flat object (for example, a book) are input.  The task is to find the target object in the scene image. <br><br><h4>  Let's start with the camera </h4><br>  First, let's connect the core.hpp and opencv.hpp files h, which are responsible for the opencv and features2d.hpp base classes, which defines the classes of various detectors and descriptors (we will be interested in SURF). <br><br><pre><code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt;iostream&gt; #include "opencv2/opencv.hpp" #include "opencv2/core/core.hpp" #include "opencv2/nonfree/features2d.hpp" #include &lt;vector&gt; using namespace std; using namespace cv; void readme(string &amp;message) { cout &lt;&lt; message &lt;&lt; endl; }</span></span></span></span></code> </pre> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Next, the main body begins, we will assume that we are passing 1 parameter to the executable file, the path to the picture to the sample (flat object).  The constructor of the VideoCapture class accepts a device number (camera) as input, 0 is the default device (probably an integrated camera).  Next, read the target image in img_object. <br><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">main</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">( </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> argc, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">char</span></span></span></span><span class="hljs-function"><span class="hljs-params">** argv )</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(argc != <span class="hljs-number"><span class="hljs-number">2</span></span>) { <span class="hljs-built_in"><span class="hljs-built_in">string</span></span> message = <span class="hljs-string"><span class="hljs-string">": ./cv_test &lt;img_object&gt;"</span></span>; readme(message); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1</span></span>; } <span class="hljs-function"><span class="hljs-function">VideoCapture </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">cap</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span></span>; <span class="hljs-comment"><span class="hljs-comment">//   ( 1).      0 . if(!cap.isOpened()) //    { string message = "        "; readme(message); return -1; } Mat img_object = imread( argv[1], CV_LOAD_IMAGE_GRAYSCALE );</span></span></code> </pre><br><br>  In an infinite loop, we will receive a regular frame from the device, this is the input image of the scene in which to find the target object. <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(;;) { Mat frame; cap &gt;&gt; frame; <span class="hljs-comment"><span class="hljs-comment">//      Mat img_scene = frame; if( !img_object.data || !img_scene.data ) //       { string message = "   "; readme(message); }</span></span></code> </pre><br><br><h4>  Finding key points </h4><br>  Now we proceed to the recognition of the object.  The first thing to do is to detect the key points on the image.  Simplistically, we can assume that these are points in the places of a sharp gradient gradient in the image along x and y (corner points).  The principle of their determination is based on the use of the autocorrelation matrix and the pyramid of images (for scale invariance).  The autocorrelation matrix consists of derivatives with respect to x and y in image I. <br><img src="http://savepic.ru/3219457.png" alt="image"><br>  the meaning is that the metric used (lambdas are eigenvalues, det is the determinant, trace is the trace of the matrix, alpha is a constant) <br><img src="http://savepic.ru/3212289.png" alt="image"><br>  allows you to identify corner points, because  at these points there will be significant gradients in x and y, and R will be at the local maximum.  By setting the minHessian parameter, we determine the threshold by which the key point will be determined or not. <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//--  1.   . int minHessian = 400; SurfFeatureDetector detector( minHessian ); std::vector&lt;KeyPoint&gt; keypoints_object, keypoints_scene; detector.detect( img_object, keypoints_object ); detector.detect( img_scene, keypoints_scene );</span></span></code> </pre><br><br><h4>  Finding handles </h4><br>  Further, it is necessary to calculate the descriptor, - the vector encoding the geometry of the local neighborhood around the point.  The basis of this, as a rule, is SIFT (SURF is a fast SIFT).  The principle is as follows: <br><img src="http://s019.radikal.ru/i640/1210/3c/f2a46812a3ec.png" alt="image"><br><br>  The patch around this point is divided into deterministic blocks, in each block the dominant gradient direction is calculated and the magnitude + is rotated in the direction of the dominant direction (rotation invariance).  This ‚Äúgradient pattern‚Äù describes a local patch. <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//--  2.  . SurfDescriptorExtractor extractor; Mat descriptors_object, descriptors_scene; extractor.compute( img_object, keypoints_object, descriptors_object ); extractor.compute( img_scene, keypoints_scene, descriptors_scene );</span></span></code> </pre><br><br><h4>  Handle comparison </h4><br>  At the next stage, we must match the vector of descriptors, i.e.  find the corresponding points on the target object and in the scene.  For this purpose, you can use FlannBasedMatcher (it should be used for large sets of key points) or BruteForceMatcher (vice versa).  Next, we select from all the matted points only those whose distance between the descriptors is no more than 3 * min_dist, where min_dist is the minimum distance between the descriptors. <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//--  3:    . FlannBasedMatcher matcher; vector&lt; DMatch &gt; matches; matcher.match( descriptors_object, descriptors_scene, matches ); double max_dist = 0; double min_dist = 100; //--         //    for( int i = 0; i &lt; descriptors_object.rows; i++ ) { double dist = matches[i].distance; if( dist &lt; min_dist ) min_dist = dist; if( dist &gt; max_dist ) max_dist = dist; } printf("-- Max dist : %f \n", max_dist ); printf("-- Min dist : %f \n", min_dist ); //--    ,    3 * min_dist vector&lt; DMatch &gt; good_matches; for( int i = 0; i &lt; descriptors_object.rows; i++ ) { if( matches[i].distance &lt; 3 * min_dist ) { good_matches.push_back( matches[i]); } } Mat img_matches; //--    drawMatches( img_object, keypoints_object, img_scene, keypoints_scene, good_matches, img_matches, Scalar::all(-1), Scalar::all(-1), vector&lt;char&gt;(), DrawMatchesFlags::NOT_DRAW_SINGLE_POINTS );</span></span></code> </pre><br><br><h4>  Using homography </h4><br>  In computer vision, any two images of the same flat object in space are connected by homography (if we use a pin-hole camera model).  In other words, this transformation is a plane - a plane.  Those.  having a set of points on the target object and a set of points associated with it in the scene, we can find a correspondence between them in the form of a homography matrix H (and vice versa, respectively).  The basis for finding this transformation lies in the <a href="https://engineering.purdue.edu/kak/courses-i-teach/ECE661.08/solution/hw4_s1.pdf">RANSAC</a> algorithm, which is based on an iterative assessment of homography for randomly selected points (4 in the image and 4 in the scene). <br><br><pre> <code class="cpp hljs"> <span class="hljs-comment"><span class="hljs-comment">//--   vector&lt;Point2f&gt; obj; vector&lt;Point2f&gt; scene; for( int i = 0; i &lt; good_matches.size(); i++ ) { obj.push_back( keypoints_object[ good_matches[i].queryIdx ].pt ); scene.push_back( keypoints_scene[ good_matches[i].trainIdx ].pt ); } Mat H = findHomography( obj, scene, CV_RANSAC );</span></span></code> </pre><br><br>  Next, you need to take 4 points along the edges of the target object and display them using the found transformation on the scene image.  Thus, we will find the bounding box of the object in the scene.  Note that when drawing lines, we add Point2f (img_object.cols, 0) to each point, since  the img_matches image assumes the adjacent placement of the target image (left) and the scene (right). <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//--  ""     std::vector&lt;Point2f&gt; obj_corners(4); obj_corners[0] = cvPoint(0,0); obj_corners[1] = cvPoint( img_object.cols, 0 ); obj_corners[2] = cvPoint( img_object.cols, img_object.rows ); obj_corners[3] = cvPoint( 0, img_object.rows ); std::vector&lt;Point2f&gt; scene_corners(4); //--    ,   ,   perspectiveTransform( obj_corners, scene_corners, H); //--    line( img_matches, scene_corners[0] + Point2f( img_object.cols, 0), scene_corners[1] + Point2f( img_object.cols, 0), Scalar(0, 255, 0), 4 ); line( img_matches, scene_corners[1] + Point2f( img_object.cols, 0), scene_corners[2] + Point2f( img_object.cols, 0), Scalar( 0, 255, 0), 4 ); line( img_matches, scene_corners[2] + Point2f( img_object.cols, 0), scene_corners[3] + Point2f( img_object.cols, 0), Scalar( 0, 255, 0), 4 ); line( img_matches, scene_corners[3] + Point2f( img_object.cols, 0), scene_corners[0] + Point2f( img_object.cols, 0), Scalar( 0, 255, 0), 4 ); //-- Show detected matches imshow( "Good Matches &amp; Object detection", img_matches ); if(waitKey(30) &gt;= 0) break; } //--    </span></span></code> </pre><br><br><h4>  Summary </h4><br>  Unfortunately, the anticipations of many people are somewhat superior to state-of-the-art in the field of computer vision.  I used this code on the example of chocolate recognition.  I had to turn a little chocolate in my hands before I realized what the limits of the provisions of its stable recognition were.  In view of the variability of the situations that arise, the stability of recognition is the No. 1 headache.  However, this example is basic and may be modified. <br><br><iframe src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://video.yandex.ru/iframe/alexhoppus/fmq5mszsko.4723/&amp;xid=17259,15700021,15700186,15700190,15700253,15700255,15700259&amp;usg=ALkJrhiB_gzxpT6cTmWOaJ7jkPnqW33YTA" width="450" height="225" frameborder="0" scrolling="no" allowfullscreen="1"></iframe><br><br><h4>  Literature </h4><br>  1. <a href="http://www.vision.ee.ethz.ch/~surf/eccv06.pdf">www.vision.ee.ethz.ch/~surf/eccv06.pdf</a> <br>  2. <a href="http://www.sci.utah.edu/~gerig/CS6640-F2010/tutorial2-homographies.pdf">www.sci.utah.edu/~gerig/CS6640-F2010/tutorial2-homographies.pdf</a> <br>  3. <a href="https://engineering.purdue.edu/kak/courses-i-teach/ECE661.08/solution/hw4_s1.pdf">engineering.purdue.edu/kak/courses-i-teach/ECE661.08/solution/hw4_s1.pdf</a> </div><p>Source: <a href="https://habr.com/ru/post/155651/">https://habr.com/ru/post/155651/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../155641/index.html">Air Fuel Synthesis has created a prototype system for producing gasoline from humid air.</a></li>
<li><a href="../155643/index.html">Amazon can delete all Kindle books without warning.</a></li>
<li><a href="../155645/index.html">Find the system call code mkdir</a></li>
<li><a href="../155647/index.html">ABBYY Labs: Project FromWord - we play with words on Android</a></li>
<li><a href="../155649/index.html">Preparing an educational resource for Android from XDA-developers</a></li>
<li><a href="../155653/index.html">Bukhsoft-Online became the winner of the "Clouds 2012" award</a></li>
<li><a href="../155655/index.html">Autoconfiguration in the Amazon cloud using Chef-Solo</a></li>
<li><a href="../155657/index.html">‚ÄúRunet today‚Äù, October 22, 2012. Issue Expert: Dmitry Sergeev</a></li>
<li><a href="../155663/index.html">SHORT'Y - the release of short news from October 22</a></li>
<li><a href="../155665/index.html">Changes in the Apple Developer Guideline - what are they?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>