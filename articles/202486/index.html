<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Reduction of dimensions in the linear binary classification problem (eg SVM)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Required knowledge: acquaintance with the methods of linear binary classification (eg SVM (see SVM Tutorial )), linear algebra, linear programming 

 ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Reduction of dimensions in the linear binary classification problem (eg SVM)</h1><div class="post__text post__text-html js-mediator-article">  <i>Required knowledge: acquaintance with the methods of linear binary classification (eg SVM (see <a href="http://yadi.sk/d/73b0-5SOCjrAY">SVM Tutorial</a> )), linear algebra, linear programming</i> <br><br>  Consider a linear binary classification problem (if a problem is linearly inseparable, it can be reduced to that with the help of a symmetric integral L-2 kernel (see <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3DSVM">SVM</a> )). <img align="right" src="https://habrastorage.org/getpro/habr/post_images/15f/a65/b61/15fa65b6128769aacff81495cf984567.png" alt="image">  When solving such a problem, the classified elements (hereinafter samples) are represented as elements of the vector space of dimension n.  In practice, in such problems, n can be extremely large, for example, for the task of classifying genes, it can amount to tens of thousands.  The large dimension implies, besides the high computation time, the potentially high error of the numerical calculations.  In addition, the use of a large dimension may require large financial costs (to conduct experiments).  The question is: is it possible and how can one reduce n by discarding insignificant components of samples in such a way that samples are separated ‚Äúno worse‚Äù in the new space (remain linearly separable) or ‚Äúnot much worse‚Äù. <br><br>  In my article, I want to begin with a brief overview of the method from this article, <a href="http://yadi.sk/d/2MQ70PsACjSJe">Gene_Selection_for_Cancer_Classification_using</a> , and then suggest my own method. <br><a name="habracut"></a><br><h3>  Algorithms from Gene_Selection_for_Cancer_Classification_using </h3><br>  The main idea of ‚Äã‚Äãthe dimension reduction method in Gene_Selection_for_Cancer_Classification_using is to rank all components.  The following algorithm is proposed: <br><ol><li>  We assign all weights to all (remaining) components (about how - further) </li><li>  We throw out the component with the minimum weight of all samples. </li><li>  We train SVM.  If it was not possible to separate the samples, then we return the component that was thrown out at the last step and exit the algorithm, otherwise go to step 1 with the data without the component thrown out. </li></ol><br>  Basically, the article discusses the method of assigning weights to the resulting SVMs. That is, we use as weights the coefficients in the classifying function of the corresponding components.  Suppose after SVM training we got the classifying function (w, x) + b = 0, then wi component will be the weight of component i.  The article also discusses the correlation method of defining weights, but, like any other, loses the SVM method, since  when repeated in step 1, the weights in the SVM method are known from step 3. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The class of such algorithms is still forced to train SVMs on high-dimensional samples, i.e.  problems of error and speed are not solved.  All it gives is the possibility, after SVM training, to reduce the number of components of classified samples.  If it is financially expensive to get the values ‚Äã‚Äãof the reduced components experienced, then you can save. <br><br>  PS In addition, the choice of scales as wi is quite controversial.  I would suggest at least multiplying them by the sample variance of component i in all samples or by the first central absolute sampling moment of component i in all samples. <br><br><h3>  My algorithm </h3><br>  The idea is as follows: we discard the component and check whether now (without this component) the set of samples will be linearly separable.  If yes, then do not return this component, if not then return.  The question is how to test the set for linear separability? <br>  Let <b>xi</b> be samples, yi be a sample belonging to a class (yi = -1 is the first class, yi = 1 is the second class) i = 1..m.  Then the samples are linearly separable if <div style="text-align:center;"><img alt="A \ alpha \ ge0" src="https://habrastorage.org/getpro/habr/post_images/b9f/dff/d81/b9fdffd81bf28ef88fe4ed11f6033807.gif"></div>  - has a non-trivial solution, where <div style="text-align:center;"><img alt="A = \ big \ left (\ begin {array} {c.cccc} &amp; 1 &amp; 2 &amp; \ cdots &amp; n &amp; n + 1 &amp; \ cdots &amp; y_2x_ {2n} &amp; y_2 \\\ vdots &amp; \ vdots &amp; \ vdots &amp; \ ddots &amp; \ vdots \\ m &amp; y_mx_ {m1} &amp; y_mx_ {m2} &amp; \ cdots &amp; y_mx_ {mn} &amp; y_m \ end {array} \ right)" src="https://habrastorage.org/getpro/habr/post_images/9cf/08c/0ea/9cf08c0eaeb64a9ce750a5758cecb12d.gif"></div>  (conditions as when training SVM). <br>  _____________________________________________________________________________________ <br>  Let's first consider how to test the set for strict linear separability: <br><div style="text-align:center;"><img alt="A \ alpha> 0" src="https://habrastorage.org/getpro/habr/post_images/c72/8ed/6e3/c728ed6e324befa2f41f382da34e7fc3.gif"></div>  - has a solution.  According to the Farkas lemma &lt;=&gt; <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} A ^ T \ beta = 0 \\\ beta \ ge0 \ end {eqnarray}" src="https://habrastorage.org/getpro/habr/post_images/811/081/174/8110811743fa53389d03acb91177ccd9.gif"></div>  - has only a trivial solution.  We check the vector <b>0</b> for optimality in the linear programming problem with the obtained constraints and the objective function <div style="text-align:center;"><img alt="\ mathbf {1} ^ T \ beta \ to max" src="https://habrastorage.org/getpro/habr/post_images/981/798/024/981798024aaa812081e535555931abbd.gif"></div>  If <b>0</b> was optimal, then the set is strictly linearly separable. <br>  _____________________________________________________________________________________ <br>  Loose linear separability: <br><div style="text-align:center;"><img alt="A \ alpha \ ge0" src="https://habrastorage.org/getpro/habr/post_images/b9f/dff/d81/b9fdffd81bf28ef88fe4ed11f6033807.gif"></div>  - has a non-trivial solution.  According to the Farkas lemma &lt;=&gt; <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} A ^ T \ beta = 0 \\\ beta> 0 \\\ end {eqnarray}" src="https://habrastorage.org/getpro/habr/post_images/633/662/91d/63366291d57a2fbd22d6e695f05d0b32.gif"></div>  - has no solutions.  &lt;=&gt; <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} A ^ T \ beta = 0 \\\ beta \ ge1 \\\ end {eqnarray}" src="https://habrastorage.org/getpro/habr/post_images/8f5/11b/3a9/8f511b3a95c06c253b80aa0fab13473b.gif"></div>  - has no solutions.  To check the availability of a solution to the obtained constraints, you can use any method of finding the initial reference vector in a linear programming problem with these constraints. <br>  _____________________________________________________________________________________ <br>  In some problems (especially when the dimension (n) exceeds the number of samples (m)) we may want to discard the component only if the separation of the set occurs with a certain gap.  Then you can check the following conditions: <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} A \ alpha \ ge c \\ - 1 \ le \ alpha \ le 1 \\\ end {eqnarray}" src="https://habrastorage.org/getpro/habr/post_images/55c/6cf/e5a/55c6cfe5aa7d6062f8ced1353dc23349.gif"></div>  - has a solution, where <b>c</b> &gt; = 0 is the gap parameter, and the second constraint replaces the normalization alpha (so that the solution A * <b>x</b> &gt; 0 cannot simply be multiplied by a sufficiently large constant k and satisfied with the system A * k * <b>x</b> &gt; = <b>c</b> ).  These conditions by the Farkas Lemma (E is the identity matrix) &lt;=&gt; <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} (A ^ TE -E) \ beta = 0 \\\ beta \ ge 0 \\ (c ^ T -1 ^ T -1 ^ T) \ beta> 0 \ end {eqnarray }" src="https://habrastorage.org/getpro/habr/post_images/70d/eb5/8fb/70deb58fb9481f7a70a6f811fee43b30.gif"></div>  - has no solutions.  We check vector <b>0</b> for optimality in a linear programming problem with the obtained constraints (without the last line) and the objective function <div style="text-align:center;"><img alt="(c ^ T -1 ^ T -1 ^ T) \ beta \ to max" src="https://habrastorage.org/getpro/habr/post_images/8e6/d0c/e19/8e6d0ce19476314208a63eaeabc45fe8.gif"></div>  If <b>0</b> was optimal, then the set can be divided with a given gap. <br>  _____________________________________________________________________________________ <br>  In some tasks (especially when the number of samples (m) exceeds the dimension (n)) we may want to discard the component even if the separation of the set occurs with some error.  Then you can check the following conditions: <div style="text-align:center;"><img alt="\ left \ {\ begin {eqnarray} A \ alpha \ ge c \\\ left \ [\ begin {eqnarray} \ alpha \ ge 1 \\\ alpha \ le -1 \\\ end {eqnarray} \ end {eqnarray }" src="https://habrastorage.org/getpro/habr/post_images/ece/706/394/ece706394862153b9dde0435a7f77e3a.gif"></div>  - has a solution, where <b>c</b> &lt;= 0 is the error parameter, and the second restrictions replace the normalization alpha (so that the solution A * <b>x</b> &gt; negative_number_less_than_ <b>c</b> cannot be simply divided into a sufficiently large constant k and satisfy the system A * <b>x</b> / k&gt; = <b>c</b> ).  Here, everything is similar to the above case, only because of the disjunction (square bracket), we have to consider two linear programming problems. </div><p>Source: <a href="https://habr.com/ru/post/202486/">https://habr.com/ru/post/202486/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../202474/index.html">Communication on the Internet: earlier and now (part 2)</a></li>
<li><a href="../202476/index.html">Tiny Snake in JavaScript (30 lines of code)</a></li>
<li><a href="../202480/index.html">Writing your Gradle plugin for AnnotatedSql</a></li>
<li><a href="../202482/index.html">Our experience in testing LXC (Linux Containers) using the example of Debian Wheezy</a></li>
<li><a href="../202484/index.html">WidLib - declarative js framework for building widgets</a></li>
<li><a href="../202488/index.html">On content filtering in schools. Problems</a></li>
<li><a href="../202490/index.html">How I went to World Usability Day</a></li>
<li><a href="../202496/index.html">2GIS: yesterday, today, tomorrow</a></li>
<li><a href="../202498/index.html">What happens to the phone when driving - mobility procedures in cellular networks</a></li>
<li><a href="../202500/index.html">Acronis True Image: backup strategies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>