<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Using Consul to scale stateful services</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="September 22 held our first non-standard mitap for developers of high-load systems. It was very cool, a lot of positive feedback on the reports and th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Using Consul to scale stateful services</h1><div class="post__text post__text-html js-mediator-article">  <i>September 22 held our first <a href="https://habr.com/company/pixonic/blog/420789/">non-standard mitap</a> for developers of high-load systems.</i>  <i>It was very cool, a lot of positive feedback on the reports and therefore decided not only to post them, but also to decipher for Habr.</i>  <i>Today we publish the speech of Ivan Bubnov, DevOps from the company BIT.GAMES.</i>  <i>He spoke about the implementation of the Consul discovery service in an already working high-load project to enable fast scaling and failover of stateful services.</i>  <i>And also about the organization of a flexible namespace for backend applications and pitfalls.</i>  <i>Now a word to Ivan.</i> <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/X4VYCrOCD3A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  I administer the production infrastructure in the studio BIT.GAMES and tell the story of the introduction of the consul from Hashicorp in our project ‚ÄúGuild of Heroes‚Äù - fantasy RPG with asynchronous pvp for mobile devices.  We are released on Google Play, App Store, Samsung, Amazon.  DAU about 100,000, online from 10 to 13 thousand.  We make the game on Unity, so we write the client in C # and use our own scripting language BHL for game logic.  We write server part on Golang (passed to it from PHP).  Next is the schematic architecture of our project. <br><a name="habracut"></a><br><img src="https://habrastorage.org/webt/dd/-v/gu/dd-vgufl1o4g4sjqu8luww4f56k.png"><br>  <i>In fact, there are many more services, here are just the basics of game logic.</i> <br><br>  So, what we have.  From Stateless-services it is: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  nginx, which we use in the role of Frontend and Load Balancers and weighting clients by our weights on our backends; </li><li>  gamed - backend'y, compiled applications from Go.  This is the central axis of our architecture, they do the lion's share of the work and are associated with all the other backend services. </li></ul><br>  From Stateful services, the main ones here are: <br><br><ul><li>  Redis, which we use to cache ‚Äúhot‚Äù information (we also use it to organize in-game chat and store notifications for our players); </li><li>  Percona Server for Mysql is a repository of persistent information (probably the biggest and most unwieldy in any architecture).  We use MySQL fork and here we will talk about it today in more detail. </li></ul><br>  In the design process, we (like everyone) hoped that the project would be successful and provided for a sharding mechanism.  It consists of two entities database MAINDB and shards themselves. <br><br><img src="https://habrastorage.org/webt/y2/sj/h-/y2sjh-qfoxlosksflpkvxuq3cxk.png"><br><br>  MAINDB is a kind of table of contents - it stores information about the specific shard where the player's progress data is stored.  Thus, the complete chain of information retrieval looks like this: the client turns to the frontend, who in turn redistributes it according to the weight coefficient to one of the backends, the backend goes to MAINDB, localizes the player's shard, and then samples the data of the shard itself. <br><br>  But when we designed, we were not a big project, so we decided to make shards shards only nominally.  They were all on the same physical server, and most likely it is database partitioning within one server. <br><br>  For redundancy, we used classic master slave replication.  It was not a very good solution (I‚Äôll say why a bit later), but the main disadvantage of that architecture was that all our backends knew about other backend services only by IP addresses.  And in the case of another ridiculous accident in the data center of the type ‚Äú <i>sorry, our engineer touched the cable on your server while servicing the other one and we understood for a very long time why your server doesn‚Äôt contact</i> ‚Äù, we needed considerable body movements.  First of all, this is a reassembly of the backend and prezalivak from the IP backup server for the place of the one that failed.  Secondly, after the incident, it is necessary to restore from the backup'a from the reserve our master, because he was in an inconsistent state and bring it into a consistent state by the means of the same replication.  After that, we rebuilt the backends again and perezaliv again.  All this, of course, caused downtime. <br><br>  The moment came when our technical director (for which he thanks a lot) said: ‚ÄúGuys, stop suffering, we need to change something, let's look for ways out.‚Äù  First of all, we wanted to achieve a simple, understandable, and most importantly, an easily managed process of scaling and migration from place to place of our databases if necessary.  In addition, we wanted to achieve high availability by automating failover. <br><br><img src="https://habrastorage.org/webt/lb/96/ys/lb96yszgjsbjtriury8zxqr0loa.png"><br><br>  The central axis of our research was Consul from Hashicorp.  Firstly, we were advised to do it, and secondly, we were very attracted by its simplicity, friendliness and excellent technology stack in one box: a discovery service with healthchecks, a key-value storage and the most important thing we wanted to use is DNS, which would rezolvil us addresses from the domain service.consul. <br><br>  Consul also provides excellent Web UI and REST APIs to manage all of this. <br><br>  As for high availability, we chose two utilities for auto-failover: <br><br><ul><li>  MHA for MySQL </li><li>  Redis-sentinel </li></ul><br><img src="https://habrastorage.org/webt/bc/e5/dg/bce5dg_dxoi0irv9gic5e4d1jki.png"><br><br>  In the case of MHA for MySQL, we poured agents onto nodes with databases, and they monitored their state.  There was a certain timeout for the master Fail, after which a stop slave was made to maintain consistency and our backup master from the appeared master in an inconsistent state did not take the data.  And we added a web hook to these agents, who registered there a new IP backup master in Consul itself, after which it got into the DNS issue. <br><br>  With Redis-sentinel everything is even easier.  Since he himself is doing the lion‚Äôs share of the work, all that remained for us to do was to take into account in healthcheck that the Redis-sentinel should take place exclusively at the master node. <br><br>  At first everything worked fine, like a clock.  We had no problems on the test bench.  But it was worth moving to the natural data transfer environment of the loaded data center, remembering some OOM-kill ªah (this is out of memory, in which the process is killed by the system core) and restoring the service or more sophisticated things that affect the availability of the service as we immediately received a serious risk of false positives or the lack of a guaranteed response at all (if, in an attempt to escape from false positives, we twist some checks). <br><br><img src="https://habrastorage.org/webt/a9/8t/7i/a98t7i3cvbb18duz7mkcr8f4vqg.png"><br><br>  First of all, everything depends on the difficulty of writing correct healthchecks.  It seems that the task is rather trivial - check that the service is running on your server and pingani port.  But, as further practice has shown, writing a healthcheck when implementing Consul is an extremely complex and time-distributed process.  Because so many factors that affect the availability of your service in the data center, it is impossible to foresee - they are detected only after a certain time. <br><br>  In addition, the data center is not a static structure into which you have flooded and it works as intended.  But we, unfortunately (or fortunately), learned about this only later, but for now we were inspired and full of confidence that we would implement everything in production. <br><br><img src="https://habrastorage.org/webt/c3/kt/uz/c3ktuzba_fzzjqodfbfsdv6sj6k.png"><br><br>  As for scaling, I will say briefly: we tried to find a finished bike, but they are all designed for specific architectures.  And, as in the case of Jetpants, we could not meet the conditions that he imposed on the architecture of a persistent storage of information. <br><br>  Therefore, we thought about our own script binding and postponed this question.  We decided to act consistently and start with the implementation of Consul. <br><br><img src="https://habrastorage.org/webt/99/em/bk/99embkj_f7vdi-kta4rdme6ga6k.png"><br><br>  Consul is a decentralized, distributed cluster that operates based on the gossip protocol and the Raft consensus algorithm. <br><br>  We have an independent equorum of five servers (five to avoid a split-brain situation).  For each node, we pour the Consul-agent in agent mode and pour all the healthchecks (i.e. there was no such thing that we fill some healthchecks with a specific server and others with certain servers).  Healthcheck'i were written so that they pass only where there is a service. <br><br>  We also used another utility to avoid having to learn from your backend to resolve addresses from a specific domain on a non-standard port.  We used Dnsmasq - it provides the ability to completely transparently resolve to the cluster nodes those addresses that we need (which, in the real world, do not exist, so to say, but only exist within the cluster).  We prepared an automatic script for uploading to Ansible, filled it all into production, flipped the namespace, made sure that everything was complete.  And, with fingers crossed, perezalili our backend'y that addressed not by ip-addresses, but by these names from the server.consul domain. <br><br>  Everything started the first time, our joy knew no bounds.  But it was too early to rejoice, because within an hour we noticed that on all the nodes where our backends are located, the load average rate increased from 0.7 to 1.0, which is quite a bold figure. <br><br><img src="https://habrastorage.org/webt/em/ua/v_/emuav_oozpbvjdoa7yg1ldfeag0.png"><br><br>  I climbed to the server to watch what was happening and it became obvious that the CPU was eating Consul.  Here we began to understand, began to shaman with strace (a utility for unix-systems that allows you to track which syscall the process performs), reset the Dnsmasq statistics in order to understand what is happening on this node and it turned out that we missed a very important point.  Planning the integration, we missed the caching of DNS records and it turned out that our backend was tugging at Dnsmasq for each of its body movements, and that in turn addressed the Consul and it all turned into a sickly 940 DNS requests per second. <br><br>  The solution seemed obvious - just twist ttl and everything will be fine.  But it was impossible to be fanatical here, because we wanted to introduce this structure in order to get a dynamic, easily manageable and fast-changing namespace (so we could not, for example, put 20 minutes).  We unscrewed the ttl to the limit for our optimal values, we managed to reduce the rate of requests per second to 540, but this had no effect on the CPU consumption indicator. <br><br>  Then we decided to get out in a cunning way, using the custom hosts-file. <br><br><img src="https://habrastorage.org/webt/0p/li/01/0pli01ivofxzndxm9pa9xrtvu6m.png"><br><br>  It‚Äôs good that we had everything for this: a beautiful Consul template system that generates a file of any kind based on the cluster status and template script, any config file ‚Äî whatever you want.  In addition, Dnsmasq has the configuration parameter addn-hosts, which allows you to use non-system hosts file as the same additional hosts file. <br><br>  What we did, again prepared the script in Ansible, poured it into production and it began to look something like this: <br><br><img src="https://habrastorage.org/webt/p6/qe/3i/p6qe3iaqlloehk1ld7ptisb_0dg.png"><br><br>  An additional element and a static disk file appeared, which is rather quickly regenerated.  Now the chain looked quite simple: gamed refers to Dnsmasq, and that in turn (instead of pulling the Consul-agent, who will ask the servers where we have this or that node) just watched the file.  This solved the problem with the consumption of CPU Consul. <br><br>  Now everything began to look like we planned - absolutely transparent for our production, practically without consuming resources. <br><br>  We were pretty much tortured that day and went home with great fear.  They were afraid not for nothing, because at night I was awakened by an alert from monitoring and notified that we had a rather large-scale (albeit short-lived) surge of errors. <br><br><img src="https://habrastorage.org/webt/yg/yj/z_/ygyjz_upz8khxqgriqbsye0_xma.png"><br><br>  Understanding in the morning with logs, I saw that all the errors of the same kind are unknown host.  It was not clear why Dnsmasq could not kill one service or another from a file ‚Äî the feeling that it does not exist at all.  To try to understand what was happening, I added a custom metric for file re-migration ‚Äî now I knew for sure when it would be regenerated.  In addition, in the Consul template itself there is an excellent backup option, i.e.  You can see the previous state of the regenerated file. <br><br>  During the day, the incident was repeated several times and it became clear that at some point in time (although it was sporadic, unsystematic), our hosts file was re-generated without certain services.  It turned out that in a specific data center (I will not do anti-advertising) rather unstable networks - because of the network flopping, we completely unpredictably stopped going through health checkers, or even the nodes fell out of the cluster.  It looked like this: <br><br><img src="https://habrastorage.org/webt/3v/_q/ve/3v_qvengzywl6bqdchzxs2fshl4.png"><br><br>  The node fell out of the cluster, the Consul agent was immediately notified about this, and the Consul template immediately regenerated the hosts file without the required service.  This was generally unacceptable, because the problem is ridiculous: if the service is unavailable for a few seconds, the mood is timeouts and retracts (they did not connect once, and the second time it happened).  We, however, have provoked a situation in the sales department when the service just disappears from view and there was no possibility to connect to it. <br><br>  We started to think what to do with this and turn the timeout parameter into Consul, after which it is identified after how long the node falls out.  We managed to solve this problem with a rather small indicator, the nodes stopped falling out, but with healthcheck this did not help. <br><br>  We started thinking about selecting different parameters for healthchecks, trying to somehow understand when and how this happens.  But due to the fact that everything happened sporadically and unpredictably, we could not do it. <br><br>  Then we went to the Consul template and decided to make a timeout for it, after which it reacts to a change in the cluster state.  Again, it was impossible to be fanatical here, because we could come to a situation where the result would be no better than the classical DNS, when we were striving for a completely different one. <br><br>  And here our technical director once again came to the rescue and said: ‚ÄúGuys, let's try to abandon all this interactivity, we are all in production and there is no time for research, we need to solve this issue.  Let's use simple and understandable things. ‚Äù  So we came to the concept of using key-value storage as a source for generating hosts file. <br><br><img src="https://habrastorage.org/webt/ar/qm/kx/arqmkxvbfzahdo6qrlom1htydq4.png"><br><br>  What it looks like: we abandon all dynamic healthchecks, rewrite our template script to generate a file based on the data stored in the key-value storage.  In the key-value store, we describe our entire infrastructure as a key name (this is the name of the service we need) and key values ‚Äã‚Äã(this is the name of the node in the cluster).  Those.  if the node is present in the cluster, then we very easily get its IP address and write it to the hosts file. <br><br>  We all tested it, poured it into production, and it became a silver bullet in a particular situation.  Again, we were pretty much tortured for the whole day, and we went home, but returned already rested, inspired, because these problems did not recur any more and did not recur for a year.  From which I personally conclude that this was the right decision (specifically for us). <br><br>  So.  We finally achieved what we wanted and organized a dynamic namespace for our backend.  Then we went towards high availability. <br><br><img src="https://habrastorage.org/webt/m9/k0/nu/m9k0nueo_w_79ywwhcco0tgnq5u.png"><br><br>  But the fact is that, frightened by the integration of Consul and due to the problems we faced, we thought and decided that implementing auto-failover is not such a good solution, because we again risk false positives or failures.  This process is opaque and uncontrollable. <br><br>  Therefore, we chose a simpler (or more complicated) way: we decided to leave failover on the conscience of the duty administrator, but gave him another additional tool.  We replaced the master slave replication with the master replication in Read only mode.  This removes a huge amount of headaches in the failover process ‚Äî when you drop out of the wizard, all you have to do is change the value in the k / v storage using the Web UI or the command in the API and remove Read only to backup wizard. <br><br>  After the incident has been settled, the wizard comes into contact and automatically comes to a consistent state without any unnecessary actions at all.  We stopped at this option and use it as before - it‚Äôs as convenient for us as possible, and most importantly, as simple as possible, understandable and controllable. <br><br><img src="https://habrastorage.org/webt/mr/fn/tc/mrfntctgtpanvrkqhlqmbm9liue.png"><br>  <i>Consul web interface</i> <br><br>  On the right is the k / v-storage and see our services that we use in the work of gamed;  value is the name of the node. <br><br>  As for scaling, we started to implement this when shards became crowded on one server, the bases grew, became slow, the number of players increased, we swapped and we were faced with the task of dissolving all shards on our own separate servers. <br><br><img src="https://habrastorage.org/webt/0b/yc/zg/0byczgl-2ugpi8z_dgwzytcmrw4.png"><br><br>  What it looked like: using the XtraBackup utility, we restored our backup on a new pair of servers, after which the new master was hung as a slave to the old one.  He came to a consistent state, we changed the key value in the k / v-storage from the name of the node of the old master to the name of the node of the new master.  Then (when we considered that everything went correctly and all gamed with its selects, updates, inserts went to the new master), it only remained to kill replication and make the coveted drop database production, as we all like to do with unnecessary databases. <br><br><img src="https://habrastorage.org/webt/1x/6k/pw/1x6kpwybgw6-wsgscd6r1kylk-u.png"><br><br>  In this way, we got sharpened shards.  The whole process of moving took from 40 minutes to an hour and did not cause any downtime, was completely transparent for our backends and itself was completely transparent for the players (except that as soon as they moved, it became easier and more pleasant for them to play). <br><br><img src="https://habrastorage.org/webt/gm/du/bj/gmdubjovhtlet9n2-bljkeqz7oq.png"><br><br>  As for the failover processes, the switching time here is from 20 to 40 seconds plus the response time of the system administrator on duty.  That's about it now it looks like with us. <br><br>  So that I want to say in conclusion - unfortunately, our hopes for absolute, comprehensive automation have broken about the harsh reality of the data transfer environment in the loaded data center and random factors that we could not foresee. <br><br> -,     ,          ,   ,   -  ,     ,       ,     . <br><br>  - ,            ,   ;          ‚Äî    , ,    ,   . <br><br><h3>  Questions from the audience </h3><br> <b>   k/v   ‚Äî      ?</b> <br><br> K/v-     Consul-   -  ,     http- RESTful API  Web UI. <br><br>     ,   -             ,     ,   . <br><br> <b>       ,     Redis?</b> <br><br>        ,    - . <br><br> -,          backend. -,      backend',       ‚Äî    .  Those.       ,     MAINDB        ,   .        .                  -  ,       . <br><br>      - ,       inmemory key-value    -. <br><br> <b>   ?</b> <br><br>    MySQL ‚Äî Percona server. <br><br> <b>            ?      Maria,     MHA for MySQL,    Galera.</b> <br><br>      Galera.    -   ¬´ ¬ª       Galera       ,     .       ,       . <br><br>    ,     ‚Äî         ,         ,     -  ,    ,         ,   . <br><br><h3>  More reports from Pixonic DevGAMM Talks </h3><br><ul><li>  <a href="https://habr.com/company/pixonic/blog/425813/">CICD: Seamless Deploy on Distributed Cluster Systems without Downtime</a> (Egor Panov, Pixonic System Administrator); </li><li>  <a href="https://habr.com/company/pixonic/blog/426115/">Practice using the model of actors in the back-platform platform of the Quake Champions game</a> (Roman Rogozin, backend developer of Saber Interactive); </li><li>  <a href="https://habr.com/company/pixonic/blog/426875/">The architecture of the meta-server mobile online shooter Tacticool</a> (Pavel Platto, Lead Software Engineer in PanzerDog); </li><li>  <a href="https://habr.com/company/pixonic/blog/427359/">How ECS, C # Job System and SRP change the approach to architecture</a> (Valentin Simonov, Field Engineer in Unity); </li><li>  <a href="https://habr.com/company/pixonic/blog/427797/">KISS principle in development</a> (Konstantin Gladyshev, Lead Game Programmer at 1C Game Studios); </li><li>  <a href="https://habr.com/company/pixonic/blog/429312/">General game logic on the client and server</a> (Anton Grigoriev, Deputy Technical Officer in Pixonic). </li><li>  <a href="https://habr.com/company/pixonic/blog/428349/">Cucumber in the cloud: using BDD scripts for load testing a product</a> (Anton Kosyakin, Technical Product Manager in ALICE Platform). </li></ul></div><p>Source: <a href="https://habr.com/ru/post/424777/">https://habr.com/ru/post/424777/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424763/index.html">Text editor - this is not your highest mathematics, you need to think</a></li>
<li><a href="../424765/index.html">Status Management in Flutter Applications</a></li>
<li><a href="../424767/index.html">We make from Habr cake. Again</a></li>
<li><a href="../424771/index.html">Personal experience: from the idea and a clean slate to a draft version of the site</a></li>
<li><a href="../424773/index.html">Biopharma and numerical modeling: Amgen's experience and practice</a></li>
<li><a href="../424779/index.html">Multi-page SPA on Python</a></li>
<li><a href="../424781/index.html">Learning and testing neural networks on PyTorch using Ignite</a></li>
<li><a href="../424787/index.html">Interview with Aaron Patterson, speaker of the RubyRussia 2018 conference</a></li>
<li><a href="../424789/index.html">How to deploy a Ruby on Rails application with HAProxy Ingress, unicorn / puma, and web sockets</a></li>
<li><a href="../424791/index.html">Expansion of network capabilities of a programmable relay using WI-FI</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>