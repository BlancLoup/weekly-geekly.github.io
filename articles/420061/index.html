<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Evaluate processes in a development team based on objective data.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Software development is considered to be a poorly measurable process, and it seems that in order to effectively manage it, you need a special flair. A...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Evaluate processes in a development team based on objective data.</h1><div class="post__text post__text-html js-mediator-article">  Software development is considered to be a poorly measurable process, and it seems that in order to effectively manage it, you need a special flair.  And if the intuition with emotional intelligence is not developed very well, then the time will inevitably shift, the quality of the product will sink and the speed of delivery will fall. <br><img src="https://habrastorage.org/webt/66/pb/ou/66pbouievmzonjbz4gybvjyitxw.jpeg"><br>  <b>Sergey Semenov</b> believes that this happens mainly for two reasons. <br><br><ul><li>  There are no tools and standards for evaluating the work of programmers.  Managers have to resort to a subjective assessment, which in turn leads to errors. </li><li>  The automatic control of the processes in the team is not used.  Without proper control, the processes in the development teams cease to perform their functions, as they begin to be partially or simply ignored. </li></ul><br>  And offers an approach to assessing and controlling processes based on objective data. <br><br>  Below is the video and text version of the report by Sergey, who, according to the audience voting, took second place on <a href="http://teamleadconf.ru/">Saint TeamLead Conf</a> . <br><a name="habracut"></a><br><iframe width="560" height="315" src="https://www.youtube.com/embed/-yDLzoX4re4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <strong><em>About the speaker:</em></strong> <em>Sergey Semenov ( <a href="https://habr.com/users/sss0791/" class="user_link">sss0791</a></em> <em>)</em> <em>has worked in IT for 9 years, was a developer, team leader, product manager, now the CEO of GitLean.</em>  <em>GitLean is an analytical product for managers, technical directors, and team leaders who are designed to make objective management decisions.</em>  <em>Most of the examples in this story are based not only on personal experience, but also on the experience of client companies with a development staff of 6 to 200 people.</em> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      About the <a href="https://habr.com/company/oleg-bunin/blog/417411/">evaluation of the developers,</a> we have already with my colleague Alexander Kiselyov told in February at the previous TeamLead Conf.  I will not dwell on this in detail, but I will refer to the article on some metrics.  Today we will talk about the processes and how to control and measure them. <br><br><h2>  Data sources <br></h2><br>  If we are talking about measurements, it would be good to understand where to get the data.  First of all, we have: <br><br><ul><li>  <strong>Git</strong> with code info; </li><li>  <strong>Jira</strong> or any other task tracker with task information; </li><li>  <strong>GitHub</strong> , Bitbucket, Gitlab with code review information. </li></ul><br>  In addition, there is such a cool mechanism, as the collection of various subjective assessments.  I will make a reservation that it should be used systematically if we want to rely on this data. <br><img src="https://habrastorage.org/webt/cf/z4/hp/cfz4hpfcqpaeeqtofr0dt5pqpus.jpeg"><br><br>  Of course, there is dirt and pain waiting for you in the data - there's nothing you can do about it, but that's not so scary.  The most annoying thing is that the data on the work of your processes in these sources often may simply not be.  This may be because the processes were built in such a way that they do not leave any artifacts in the data. <br><br>  <strong>The first rule</strong> we recommend to follow when designing and building processes is to make them so that they leave artifacts in the data.  We need to build not just Agile, but make it measurable ( <strong>Measurable Agile</strong> ). <br><br>  I'll tell you a scary story that we met at one of the clients who came to us with a request to improve the quality of the product.  For you to understand the scale - about 30-40 bugs from production flew on a team of 15 developers a week.  We began to understand the causes, and found that 30% of the tasks do not fall into the status of "testing".  At first, we thought it was just a data error, or testers did not update the status of the task.  But it turned out that really 30% of tasks are simply not tested.  Once there was a problem in the infrastructure, due to which 1-2 puzzles in the iteration did not get into testing.  Then everyone forgot about this problem, testers stopped talking about it, and over time it turned into 30%.  In the end, this led to more global problems. <br><br>  Therefore, the first important metric for any process is that it leaves data.  Be sure to follow this. <br><img src="https://habrastorage.org/webt/rz/5i/i1/rz5ii1pj2i1h_7_sjr3gnzhi0ms.jpeg"><br><br>  Sometimes, for the sake of measurability, you have to sacrifice some of the principles of Agile and, for example, somewhere prefer to write oral communication. <br><br>  Due date practice proved to be very good. We implemented it in several teams in order to improve predictability.  Its essence is as follows: when a developer takes a task and drags it into ‚Äúin progress‚Äù, he must deliver due date when the task is either released or ready for release.  This practice teaches the developer to be a conditional micro project manager of his own tasks, that is, to take into account external dependencies and understand that the task is ready only when the client can use its result. <br><br>  In order for learning to occur, after the due date, the developer needs to go to Jira and put a new due date and leave comments in a specially defined form, why this happened.  It would seem, why do we need such a bureaucracy.  But in fact, after two weeks of this practice, we unload all such comments from Jira with a simple script and conduct a retrospective with this texture.  It turns out a bunch of insights about why deadlines fail.  Very cool work, I recommend to use. <br><br><h3>  Approach from problems <br></h3><br>  In the measurement of processes, we confess the following approach: we must proceed from the problems.  We imagine some ideal practices and processes, and then we will be creative, in what ways they may not work. <br><br>  <strong>It is necessary to monitor the violation of processes</strong> , and not how we follow some kind of practice.  Processes often do not work, not because people maliciously violate them, but because the developer and manager do not have enough control and memory to follow them all.  By tracking violations of the rules, we can automatically remind people about what needs to be done, and we get automatic controls. <br><br>  To understand what processes and practices need to be implemented, you need to understand why it should be done in the development team, what the business needs from development.  Everyone understands that you need not so much: <br><br><ul><li>  that the product is delivered for an adequate predictable time; </li><li>  that the product was of proper quality, not necessarily perfect; </li><li>  make it all fast enough. </li></ul><br>  That is, <strong>predictability, quality and speed</strong> are important.  Therefore, we will look at all the problems and metrics with regard to how they affect predictability and quality.  We‚Äôre almost not going to discuss speed, because of the nearly 50 teams we worked with in one way or another, only two could work with speed.  In order to increase speed, you need to be able to measure it, and so that it is at least a little predictable, and this is predictability and quality. <br><br>  In addition to predictability and quality, we introduce such a direction as a <strong>discipline</strong> .  We will call discipline everything that ensures the basic functioning of processes and data collection, on the basis of which the analysis of problems with predictability and quality is carried out. <br><img src="https://habrastorage.org/webt/sm/oq/gp/smoqgpexbfnhzuwa6jz7leysdiq.jpeg"><br><br>  Ideally, we want to build the following workflow: so that we have automatic data collection;  from this data, we could build metrics;  using metrics to find problems;  signal problems directly to the developer, team leader or team.  Then everyone will be able to respond to them in a timely manner and cope with the problems found.  I‚Äôll say right away that it‚Äôs not always possible to reach clear signals.  Sometimes metrics will remain just metrics that will have to be analyzed, look at values, trends, and so on.  Even with the data there will sometimes be a problem, sometimes they cannot be collected automatically and you have to do it with your hands (I will separately clarify such cases). <br><br><img src="https://habrastorage.org/webt/zw/wm/zp/zwwmzpjooufdihy43odchlwfixw.jpeg"><br><br>  Next we look at 4 stages of feature life: <br><br><ul><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/420061/">planning</a> <br></li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/420061/">development</a> </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/420061/">code review</a> </li><li>  <a href="https://habr.com/ru/company/oleg-bunin/blog/420061/">testing</a> </li></ul><br>  And we will analyze what problems with discipline, predictability and quality can be at each of these stages. <br><a name="planning"></a><br><h2>  Problems with discipline at the planning stage <br></h2><br>  There is a lot of information, but I pay attention to the most important points.  They may seem simple enough, but they are faced with a very large number of commands. <br><img src="https://habrastorage.org/webt/b5/fs/ov/b5fsov3ytrgjnluj19m1l_1nqeq.jpeg"><br><br>  The first problem that often arises during planning is a trite <strong>organizational problem</strong> ‚Äî not everyone who should be there is present at the planning meeting. <br><br>  <em>Example: the</em> team complains that the tester is testing something wrong.  It turns out that testers in this team never go to planning at all.  Or instead of sitting and planning something, the team frantically searches for a place to sit, because it has forgotten to book a meeting. <br><br>  You do not need to configure metrics and signals, just please make sure that you do not have these problems.  The rally is marked in the calendar, everyone is invited to it, the venue is occupied.  No matter how funny it may sound, they face it in different teams. <br><br>  Now we will discuss situations in which signals and metrics are needed.  At the planning stage, most of the signals that I will talk about should be sent to the team about an hour after the end of the planning rally, so as not to distract the team in the process, but so that the focus remains. <br><br>  The first disciplinary problem is <strong>that tasks have no description,</strong> or they are poorly described.  It is controlled by elementary.  There is a format to which the tasks must correspond - check if this is so.  For example, we follow that acceptance criteria are set, or for frontend tasks there is a link to the layout.  You also need to keep track of the components placed, because the description format is often tied to the component.  For the backend task, one description is relevant, for a frontend one, another. <br><img src="https://habrastorage.org/webt/bm/8b/dg/bm8bdgvhsrrp_fhytcr-nlcuunw.jpeg"><br><br>  The next frequent problem is that <strong>priorities are spoken orally or not at all and are not reflected in the data</strong> .  As a result, by the end of the iteration, it turns out that the most important tasks were never completed.  You need to ensure that the team uses priorities and uses them adequately.  If a team has 90% of tasks in an iteration having a high priority, it is all the same that there are no priorities at all. <br><br>  We try to arrive at this distribution: 20% high priority tasks (it‚Äôs impossible not to release);  60% - medium priority;  20% - low priority (not scary, if not released).  We hang all the signals. <br><img src="https://habrastorage.org/webt/te/dl/cr/tedlcrd-7hx0rym5gvv21kxfug4.jpeg"><br><br>  The last problem with the discipline, which happens at the planning stage - <strong>there</strong> is <strong>not enough data</strong> , including for subsequent metrics.  The basic ones are: the tasks have no ratings (a signal should be made) or the types of tasks are inadequate.  That is, bugs get started as tasks, and tasks of technical duty are not tracked at all.  Unfortunately, it is impossible to automatically control the second type of problems.  We recommend just once a couple of months, especially if you are a CTO and you have several teams, to look through the backlog and make sure that people get bugs as bugs, story as stori, technical debt tasks as technical debts. <br><br><h2>  Predictability problems at the planning stage <br></h2><br>  We turn to problems with predictability. <br><img src="https://habrastorage.org/webt/2y/yo/sl/2yyoslllpnojcyieprrebrsnf9k.jpeg"><br><br>  The basic problem is <strong>that we do not fall within the time limits and estimates</strong> , we <strong>estimate</strong> it incorrectly.  Unfortunately, there is no way to find a magic signal or metric that will solve this problem.  The only way is to encourage the team to learn better, to sort through examples of the causes of errors with one or another assessment.  And this learning process can be facilitated with the help of automatic tools. <br><br>  The first thing that can be done is to deal with obviously problem tasks with a high estimate of execution time.  We hang the SLA and control that all tasks are fairly well decomposed.  We recommend a limit of two days maximum for execution to begin with, and then you can go to one-day. <br><br>  The next item can facilitate the collection of artifacts on which it will be possible to conduct training and disassemble with the team why there was an error with the assessment.  We recommend using the practice Due date for this.  She has proven herself very cool. <br><br>  Another way is a metric called Churn code within a task.  Its essence is that we look at what percentage of the code in the framework of the task was written, but did not live up to the release (for more details, see the last <a href="https://habr.com/company/oleg-bunin/blog/417411/">report</a> ).  This metric shows how well the tasks are thought out.  Accordingly, it would be nice to pay attention to the tasks with Churn jumps and to understand them that we did not take into account and why we made a mistake in the assessment. <br><img src="https://habrastorage.org/webt/s-/wp/l3/s-wpl3duc2ouynubynumjcltzxe.jpeg"><br><br>  The next story is standard: the team planned something, filled the sprint, but in the end <strong>did not at all what it had planned</strong> .  It is possible to tune the signals for stuffing, changing priorities, but for the majority of the teams with which we did this, they were irrelevant.  Often these are legal operations by the product manager to throw something into the sprint, change the priority, so there will be a lot of false positives. <br><br>  What can be done here?  Calculate fairly standard basic metrics: the closability of the initial sprint skopa, the number of stuffing in the sprint, the closability of the stuffing itself, the change of priorities, to see the structure of stuffing.  After that, estimate how many tasks and bugs you usually throw into the iteration.  Further, using a signal to control what you <strong>are laying out this quota at the planning stage</strong> . <br><br><h2>  Quality problems at the planning stage <br></h2><br>  The first problem: the <strong>team does not think out the functionality of the released features</strong> .  I will talk about quality in a general sense - the problem with quality is if the client says that it is.  This may be some kind of product nedodumki, and there may be technical things. <br><br>  Regarding the product mismatch, a metric such as <strong>3-week churn works well</strong> , revealing that 3 weeks after the release of the churn task is above the norm.  The essence is simple: the task was released, and then within three weeks a rather high percentage of its code was deleted.  Apparently, the task was not well implemented.  We catch such cases and sort them out with the team. <br><img src="https://habrastorage.org/webt/mf/xr/hw/mfxrhwteflgdh9gxfqwezbbthje.jpeg"><br><br>  The second metric is needed for teams that have problems with bugs, crashes and quality.  We propose to build a <strong>graph of the balance of bugs and crashes:</strong> how many bugs are there right now, how many flew yesterday, how many did yesterday.  You can hang such a <strong>Real Time Monitor</strong> right in front of the team so that it sees it every day.  This is a great emphasis on the quality problems of the team.  We did this with two teams, and they really began to think through the tasks better. <br><img src="https://habrastorage.org/webt/qb/-5/hh/qb-5hhsoscrt88wma6t_8l3wxpq.jpeg"><br><br>  The next very standard problem is <strong>that the team has no time for technical debt</strong> .  This story is easily monitored, if you observe the work with the types, that is, the technical debt tasks are evaluated and set up in Jira as technical debt tasks.  We can calculate what time distribution quota was given to the team for technical debt during the quarter.  If we agreed with the business that it is 20%, and spent only 10%, this can be taken into account and take more time to the technical debt in the next quarter. <br><a name="development"></a><br><h2>  Problems with discipline at the development stage <br></h2><br>  We now turn to the development stage.  What problems can there be with discipline? <br><br>  Unfortunately, it happens that <strong>developers do nothing</strong> or we cannot understand whether they do anything.  Track it easily for two banal signs: <br><br><ul><li>  commit frequency - at least once a day; </li><li>  at least one active task in Jira. </li></ul><br>  If not, then it‚Äôs not a fact that you need to beat the developer‚Äôs hands, but you need to know about it. <br><br>  The second problem, which can knock down even the most powerful people and the brain, even a very tough developer, is <strong>constant processing</strong> .  It would be nice if you, as a tmilid, know about what a person is recycling: writing code or doing a review code during off-hours. <br><br>  <strong>Different rules for working with Git may</strong> also <strong>be violated</strong> .  The first thing we urge to follow all the commands is to specify the task prefixes from the tracker in the commit messages, because only in this case can we link the task and the code to it.  It‚Äôs better not even to build signals, but to directly configure git hook.  For any additional git-rules that you have, for example, you cannot commit in master, we also configure git hooks. <br><br>  The same applies to the agreed practitioners.  At the design stage, there are many practices that a developer must follow.  For example, in the case of Due date there will be three signals: <br><br><ul><li>  tasks for which due date is not set; </li><li>  tasks that have overdue due date; </li><li>  Tasks that have a due date been changed, but have no comments. </li></ul><br>  Signals are tuned to everything.  For any other practice, you can also set up such things. <br><br><h2>  Problems with predictability at the design stage <br></h2><br>  A lot of things can go wrong in the forecasts during the development phase. <br><br>  The task may just hang in development for a long time.  We have already tried to solve this problem at the planning stage - to decompose the tasks rather small.  Unfortunately, this does not always help, and <strong>there are tasks that hang</strong> .  To begin with, we recommend simply setting the SLA to ‚Äúin progress‚Äù status, so that there is a signal that this SLA is violated.  This will not allow us to start releasing tasks faster right now, but this will again allow us to collect the invoice, react to it and discuss with the team what happened, why the task has been hanging for a long time. <br><br>  Predictability may suffer if <strong>there are too many tasks on one developer</strong> .  The number of parallel tasks that the developer does is preferably checked by code, not by Jira, because Jira does not always reflect the relevant information.  We are all human, and if we do many parallel tasks, the risk that something goes wrong somewhere increases. <br><img src="https://habrastorage.org/webt/9d/fe/tg/9dfetgnpjyairw_fb-p8jx1irka.jpeg"><br><br>  A developer may have some problems about which he does not speak, but which are easy to identify on the basis of data.  For example, yesterday the developer had little code activity.  This does not necessarily mean that there is a problem, but you, as a team leader, can come up and find out.  Perhaps he is stuck and he needs help, but he hesitates to ask her. <br><br>  Another example is the developer, on the contrary, some big task, which is growing and expanding along the code.  This can also be identified and possibly decomposed, so that in the end there are no problems with the code review or testing stages. <br><br>  It makes sense to adjust the signal and to the fact that during the work on the task the code is repeatedly rewritten.  Perhaps it is constantly changing requirements, or the developer does not know which architectural solution to choose.  On the data it is easy to detect and discuss with the developer. <br><br><h2>  Quality problems at the design stage <br></h2><br>  Development directly affects quality.  The question is how to understand which of the developers most influences the decline in quality. <br><img src="https://habrastorage.org/webt/am/47/oz/am47ozo5s2quoyjv0xsiybiek1e.jpeg"><br><br>  We suggest doing this as follows.  It is possible to calculate the <strong>criterion of the ‚Äúimportance‚Äù of the developer</strong> : we take all the tasks that were in the tracker for three months;  among all the tasks we find ‚Äúbug‚Äù tasks;  We look at the code of these tasks of the type "bug";  We look, the code of what tasks fixed this bug fix.  Accordingly, we can understand the correlation of tasks, in which later defects were discovered, to all the tasks that the developer did - this will be the ‚Äúcriterion of importance‚Äù. <br><br>  If we supplement this story with statistics <strong>on returns from testing</strong> , that is, the proportion of developer tasks that were returned to testing for refinement, then it will be possible to assess which developer has the most quality problems.  As a result, we will understand by whom it is necessary to fine-tune the processes of code review and testing, whose code must be carefully reviewed and whose tasks should be given to more corrosive testers. <br><br>  The next problem, which may be with the quality at the development stage, is that we <strong>write hard-to-maintain code</strong> , such a ‚Äúlayered‚Äù architecture.  I will not dwell here in detail, I described it in detail last time.  There is a metric called <strong>Legacy Refactoring</strong> , which just shows how much time is spent on embedding a new code into an existing one, how much old code is removed and changing when writing a new one. <br><br>  Probably one of the most important criteria when assessing quality at the development stage is the <strong>SLA control for high-priority bugs</strong> .  I hope you follow this already.  If not, I recommend starting it, because it is often one of the most important indicators for a business: the high-priority and critical bugs development team undertakes to close at a certain time. <br><br>  The last <strong>thing</strong> you often come across is <strong>no autotest</strong> .  First, they need to be written.  Secondly, you need to monitor that the coating is kept at a certain level and does not fall below the threshold.  Many people write autotests, but forget to follow the coverage. <br><a name="code_review"></a><br><h2>  Problems with discipline at the code review stage <br></h2><br>  We proceed to the Code review stage.  What problems can there be with discipline?  Let's start with probably the most stupid reason - forgotten pull requests.  First, the author may simply not assign a reviewer for the pull request, which will be forgotten as a result.  Or, for example, they forgot to move the ticket to the ‚Äúin review‚Äù status, and the developers check which tasks need to be reviewed in Jira.  We must not forget to follow this, for which we set up simple signals.  If you have practice, which should be more than 2-3 reviewers per task, then this is also easily controlled with a simple signal. <br><img src="https://habrastorage.org/webt/vc/g6/_n/vcg6_nocjml2yhmouzlrgqsieqw.jpeg"><br><br>  The next story that the reviewer uncovers a task cannot quickly understand to which task the pull request relates, he is too lazy to ask and he postpones it.  Here we also make a signal - we make sure that in the pull request there is always a link to the ticket in Jira and the reviewer can easily read it. <br><img src="https://habrastorage.org/webt/xd/ac/-z/xdac-zwwq5fzmqwe1azrct4z_xu.jpeg"><br><br>  The next problem, which, unfortunately, cannot be excluded.  There are always huge pull requests in which a lot is done.  Accordingly, the reviewer opens them, looks at them and thinks: ‚ÄúNo, I'd rather check it later, something is too much here.‚Äù  In this case, the author can help the reviewer with onboarding, and we can control this process.  Large pull requests must have a good clear description that matches a specific format, and this format is different from the ticket in Jira. <br><br>  The second kind of practice for greater pull request, which can also be monitored, is when the author himself in advance in the code puts comments in those places where something needs to be discussed, where there is some non-obvious solution, thereby inviting the reviewer to discussion.  Signals are also easily tuned to this. <br><br>  Further, the problem that we also encounter very often, - the author says that he simply does not know when he can begin to correct everything, because he does not know whether the review is complete.  For this, elementary disciplinary practice is being introduced: the reviewer must at the end of the review be sure to unsubscribe with a special comment that "I have finished, you can fix."  Accordingly, you can configure automatic notifications about this to the author. <br><img src="https://habrastorage.org/webt/va/o6/xp/vao6xpbuim5eeawjsjpjb7ud8y0.jpeg"><br><br>  Please configure linter.  In half of the teams we work with, for some reason linter is not configured, and they themselves are engaged in such syntax code review and for some reason they are doing the work with which the machine will cope much better. <br><br><h2>  Problems with predictability in the code review stage <br></h2><br>  If the <strong>tasks continue to hang</strong> , we recommend that you configure the SLA that the task either waits for fixes for a long time, or it waits for a long time to review.  Accordingly, be sure to ping both the author and the reviewer. <br><br>  If SLA does not help, I recommend to introduce into practice the morning ‚Äú <strong>code-review hour</strong> ‚Äù or the evening one ‚Äî how convenient.  This is the time when the whole team sits down and is engaged in a purely code review.  The implementation of this metric is very easy to monitor by shifting the activity time in a pull request to the desired hour. <br><img src="https://habrastorage.org/webt/3k/xs/bs/3kxsbshnge4ye-wh_mnzma754pc.jpeg"><br><br>  It happens that there are <strong>people overloaded with code review</strong> , and this is also not very good.  For example, in one of the teams, the CTO stood at the very beginnings of the system, wrote it all up, and it just so happened that it was always the main reviewer.  All developers are constantly hung on him the task of code review.  At some point, everything came to the fact that in a team of 6 people more than 50% of the code-review hung on it and continued to accumulate and accumulate.  ,   ,         50%,   CTO    .       ,   CTO      -   ,           100%. <br><br>  ,       ‚Äî  ,   -  .    : <br><br><ul><li>  ,         ; </li><li>     -; </li><li>    ,     . </li></ul><br>    ,      -. <br><br><h2>      - <br></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">First of all, the problem may be in just a very superficial code review. In order to monitor this, there are two good metrics. You can measure the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">activity of the reviewer</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> as the number of comments for every 100 lines of code. Someone reviews every 10 lines of code, while others scroll through entire screens and leave 1-2 comments. Of course, not all comments are equally useful. Therefore, you can refine this metric by measuring the </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">influence of the reviewer</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - the percentage of comments that pointed to a line of code that was later changed as part of the review. Thus, we understand who is the most corrosive, and most effective in the sense that his comments often lead to code changes.</font></font><br><img src="https://habrastorage.org/webt/6e/nz/cr/6enzcr1prlbw9ig1jd_s49dpaiy.jpeg"><br><br>  ,      ,      , ,      . <br><br>   ‚Äî <strong>      </strong>          ,    -  ,   .   ‚Äî  churn   -, ..     pull request  ,   . <br><br> ,  <strong>  - </strong> ,  ,   ,     . ,         commit,       ,      -. <br><img src="https://habrastorage.org/webt/mz/ax/40/mzax40kyqd0grz4mmowweyxdoca.jpeg"><br><br>           ,     - ( pull request  ),            - .        ,       commit,   .       ,   . <br><a name="testing"></a><br><h2>       <br></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We proceed to the testing phase and problems with discipline at this stage. </font><font style="vertical-align: inherit;">The most frequent we face is that there is no information about testers in Jira. </font><font style="vertical-align: inherit;">That people save on licenses and do not add testers in Jira. </font><font style="vertical-align: inherit;">That tasks that simply do not fall into the status of "testing". </font><font style="vertical-align: inherit;">That we can not determine the return of the task for revision on the task-tracker. </font><font style="vertical-align: inherit;">We recommend setting up signals for all of this and watching for data to be accumulated, otherwise it will be extremely difficult to say something about the tester.</font></font><br><img src="https://habrastorage.org/webt/gs/jk/7s/gsjk7suwl3y_c27_jvdynifshmu.jpeg"><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Predictability issues at the testing stage </font></font><br></h2><br>          <strong>SLA        </strong> .     SLA   ,               . <br><br>    -,       ,   ,   ‚Äî   .       .          ,          ,  , <strong>     </strong> . <br><img src="https://habrastorage.org/webt/rz/hl/y_/rzhly_cj91od5sdw1qsa8cqloko.jpeg"><br><br>  pipeline test- ‚Äî  , ,  ,      .    build'  ,    ,     ‚Äî      ,   .  ,      1-2          ,  ,      .       ,     . <br><br><h2>       <br></h2><br>    ‚Äî     .   ,        .     ,    ,      ¬´¬ª ,   ,   ,       ,     ,   . <br><img src="https://habrastorage.org/webt/oc/0m/nx/oc0mnxau8fvl5n_ejie_0oo8gk0.jpeg"><br><br>     ,      ,     ,      <strong> </strong> .         . ,  ,   ,         ,    .  ,   ,   ,    .    :   ,    ,    ,      . <br><img src="https://habrastorage.org/webt/hb/gv/ix/hbgvixb5l9isgweuol-ddftimig.jpeg"><br><br>     ,      ¬´¬ª   ,     ,      . , ,    ,    . <br><br><img src="https://habrastorage.org/webt/_3/6m/ik/_36mikes0bfn4r2_fj9cupfv5hk.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Another story that affects the quality of the testing phase is such a constant </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ping-pong between testing and development</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">The tester simply returns the task to the developer, and he, in turn, without changing anything, returns it back to the tester. </font><font style="vertical-align: inherit;">You can look at it either as a metric, or set up a signal for such tasks and look closely at what is happening there and if there are any problems.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Metrics Methodology </font></font><br></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We talked about metrics, and now the question is how to work with all this? </font><font style="vertical-align: inherit;">I told only the most basic things, but even there are quite a lot of them. </font><font style="vertical-align: inherit;">What to do with all this and how to use it? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We recommend that you automate this process to the maximum and deliver all signals to the team using a bot in messengers. </font><font style="vertical-align: inherit;">We tried different communication channels: both e-mail and dashboard does not work well. </font><font style="vertical-align: inherit;">Bot has proven itself best. </font><font style="vertical-align: inherit;">You can write the bot yourself, you can get OpenSource from someone, you can buy from us.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The point here is very simple: the team responds to signals from the bot much more calmly than to the manager, who points to problems. </font><font style="vertical-align: inherit;">If possible, deliver most signals directly to the developer first, then to the team if the developer does not respond, for example, within one or two days.</font></font><br><img src="https://habrastorage.org/webt/0o/nd/fa/0ondfaj9cnbx_ohyttocofnet_w.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">No need to try to build all the signals at once. Most of them simply will not work, because you will not have data, because of the banal problems with discipline. Therefore, we first establish discipline and set up signals for disciplinary practices. According to the experience of the teams with whom we communicated, it took a year and a half to simply build up the discipline in the development team without automation. With automation, with the help of constant signals, the team begins to work in a disciplined manner in about a couple of months, that is, much faster.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Any signals that you make public, or send directly to the developer, in any case, you can not just take and turn on. First you need to coordinate this with the developer, speak with him and with the team. It is advisable to put in writing all the thresholds in the Team Agreement, the reasons why you are doing this, what the next steps will be, and so on. </font></font><br><img src="https://habrastorage.org/webt/qi/4f/do/qi4fdouol2ehhi5pnpxjckznhjg.jpeg"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It should be borne in mind that all processes have exceptions, and take this into account at the design stage. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We do not build a concentration camp for developers</font></font></strong> ,     ,  .    ,      .      - ,    ,      5 ,    ¬´no-tracking¬ª,      . ,  ,      ¬´no-tracking¬ª ,    ,     ,   ,   .      ¬´no-tracking¬ª  , ,  ,  ,     ,   ,   ,     ,     . <br><img src="https://habrastorage.org/webt/kh/zg/xp/khzgxpoiicraynvpzxsue9hlgye.jpeg"><br><br> <strong>    </strong> .       -   ‚Äî       .   - ,      ‚Äî    - ,   ,     . <br><br>         ,  <strong>  </strong> .     (  ),    ,    .          ,    .          .    ,   ,          .         <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">bring closer the moment when the team becomes autonomous</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and you can still go to Bali.</font></font><br><br>  <strong>findings</strong> <br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Collect data. </font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Build processes in such a way that your data is collected. </font><font style="vertical-align: inherit;">Even if you do not want to build metrics and signals now, you can do a cool, retrospective analysis in the future if you now start collecting them. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatically monitor processes. </font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When designing processes, always think about how you can hack them, and how you can recognize these hacks by data. </font></font><br><br> <strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When the signals are not enough for several weeks - you are well done!</font></font></strong>     ,   ,    ,    ,      - ,  - ,      .    , ,     ‚Äî    ,    ,      ,    :) <br><br><blockquote>       <a href="http://teamleadconf.ru/moscow/2019/">TeamLead Conf</a> .        <a href="https://conf.ontico.ru/lectures/propose%3Fconference%3Dtl2019-moscow"><b>Call for Papers</b></a> <b> </b> . <br><br>     ?    <a href="http://eepurl.com/bPteUf"></a>  ,          <a href="https://conf.ontico.ru/conference/join/tl2019-moscow.html"> </a>  . </blockquote></div><p>Source: <a href="https://habr.com/ru/post/420061/">https://habr.com/ru/post/420061/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420051/index.html">[DotNetBook] Span, Memory and ReadOnlyMemory</a></li>
<li><a href="../420053/index.html">Veeam Academy for C # Developers: New Season</a></li>
<li><a href="../420055/index.html">Theory and practice of backups with Borg</a></li>
<li><a href="../420057/index.html">8 rules of a successful freelancer</a></li>
<li><a href="../420059/index.html">Now I am a team leader, but why is it so bad for me? Practical advice</a></li>
<li><a href="../420063/index.html">"Holy" Timlid and his followers</a></li>
<li><a href="../420065/index.html">Communications as a performance zone</a></li>
<li><a href="../420067/index.html">From the closed caste to Servant Leadership: the evolution of the team leader on Booking.com</a></li>
<li><a href="../420069/index.html">Richard Hamming: Chapter 20. Modeling - III</a></li>
<li><a href="../420071/index.html">Different prices for warehouses in multistore. Revision 1C-Bitrix</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>