<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Is it possible to train with reinforcements agent for trading in the stock market? R language implementation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Let's create a prototype of a learning agent with reinforcements (RL) who will master the skill of trading. 

 Given that the implementation of the pr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Is it possible to train with reinforcements agent for trading in the stock market? R language implementation</h1><div class="post__text post__text-html js-mediator-article">  Let's create a prototype of a learning agent with reinforcements (RL) who will master the skill of trading. <br><br>  Given that the implementation of the prototype works in the R language, I urge R users and programmers to come closer to the ideas presented in this material. <br><br>  This is a translation of my English-language article: <a href="https://medium.com/%40alexeybnk/can-reinforcement-learning-trade-stock-implementation-in-r-8cd54d13165c">Can Reinforcement Learning Trade Stock?</a>  <a href="https://medium.com/%40alexeybnk/can-reinforcement-learning-trade-stock-implementation-in-r-8cd54d13165c">Implementation in R.</a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <i>I want to warn code hunters that in this note there is only a neural network code adapted for R.</i> <br><br>  If I did not distinguish myself in good Russian, point out the mistakes (the text was prepared with the help of an automatic translator). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/97b/b88/e48/97bb88e4896efcd9f281cfb1b72830cf.png" alt="image"><br><a name="habracut"></a><br><h3>  Introduction to the problem </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/986/550/1f7/9865501f78bdeb575a3017a0c2466d54.png" alt="image"><br><br>  I advise you to start the dive into the topic with this article: <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">DeepMind</a> <br><br>  It will introduce you to the idea of ‚Äã‚Äãusing the Deep Q-Network (DQN) to approximate the value function, which are crucial in Markov decision-making processes. <br><br>  I also recommend delving into math using the preprint of this book by Richard S. Sutton and Andrew J. Barto: <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Reinforcement Learning</a> <br><br>  Below I will present an extended version of the original DQN, which includes more ideas that help the algorithm to quickly and efficiently converge, namely: <br><br>  <b>Deep Double Dueling Noisy NN neural networks</b> with a priority selection from the experience playback buffer. <br><br>  What makes this approach better than classic DQN? <br><br><ul><li>  Dual: there are two networks, one of which is trained, and the other estimates the following values ‚Äã‚Äãof Q </li><li>  Dueling: there are neurons that clearly appreciate the value and benefits </li><li>  Noisy: there are noise matrices applied to the weights of the intermediate layers, where the mean and standard deviations are learning weights </li><li>  Sampling priority: batches of observations from the playback buffer contain examples, due to which previous training of functions led to large residues that can be stored in an auxiliary array. </li></ul><br><h4>  Well, what about the trade done by the DQN agent?  This is an interesting topic as such. </h4><br>  There are reasons why this is interesting: <br><br><ul><li>  Absolute freedom of choice of representations of the state, actions, awards and architecture NN.  You can enrich the entry space with anything you consider worth trying, from news to other stocks and indices. </li><li>  Compliance of the trading logic with the reinforcement learning logic is that: the agent performs discrete (or continuous) actions, is rarely rewarded (after closing the transaction or expiration of the period), the environment is partially observable and may contain information on the next steps, trading is an episodic game. </li><li>  You can compare the results of DQN with several references, such as indices and technical trading systems. </li><li>  The agent can continuously learn new information and, thus, adapt to the changing rules of the game. </li></ul><br>  In order not to stretch the material, look at the code of this NN, which I want to share, since this is one of the mysterious parts of the whole project. <br><br><h4>  R-code for value neural network using Keras to build our agent RL </h4><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># configure critic NN ------------ library('keras') library('R6') learning_rate &lt;- 1e-3 state_names_length &lt;- 12 # just for example a_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { x - k_mean(x, axis = 2, keepdims = T) } ) ) a_normalize_layer &lt;- function(object) { create_layer(a_CustomLayer, object, list(name = 'a_normalize_layer')) } v_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , public = list( call = function(x, mask = NULL) { k_concatenate(list(x, x, x), axis = 2) } , compute_output_shape = function(input_shape) { output_shape = input_shape output_shape[[2]] &lt;- input_shape[[2]] * 3L output_shape } ) ) v_normalize_layer &lt;- function(object) { create_layer(v_CustomLayer, object, list(name = 'v_normalize_layer')) } noise_CustomLayer &lt;- R6::R6Class( "CustomLayer" , inherit = KerasLayer , lock_objects = FALSE , public = list( initialize = function(output_dim) { self$output_dim &lt;- output_dim } , build = function(input_shape) { self$input_dim &lt;- input_shape[[2]] sqr_inputs &lt;- self$input_dim ** (1/2) self$sigma_initializer &lt;- initializer_constant(.5 / sqr_inputs) self$mu_initializer &lt;- initializer_random_uniform(minval = (-1 / sqr_inputs), maxval = (1 / sqr_inputs)) self$mu_weight &lt;- self$add_weight( name = 'mu_weight', shape = list(self$input_dim, self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_weight &lt;- self$add_weight( name = 'sigma_weight', shape = list(self$input_dim, self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) self$mu_bias &lt;- self$add_weight( name = 'mu_bias', shape = list(self$output_dim), initializer = self$mu_initializer, trainable = TRUE ) self$sigma_bias &lt;- self$add_weight( name = 'sigma_bias', shape = list(self$output_dim), initializer = self$sigma_initializer, trainable = TRUE ) } , call = function(x, mask = NULL) { #sample from noise distribution e_i = k_random_normal(shape = list(self$input_dim, self$output_dim)) e_j = k_random_normal(shape = list(self$output_dim)) #We use the factorized Gaussian noise variant from Section 3 of Fortunato et al. eW = k_sign(e_i) * (k_sqrt(k_abs(e_i))) * k_sign(e_j) * (k_sqrt(k_abs(e_j))) eB = k_sign(e_j) * (k_abs(e_j) ** (1/2)) #See section 3 of Fortunato et al. noise_injected_weights = k_dot(x, self$mu_weight + (self$sigma_weight * eW)) noise_injected_bias = self$mu_bias + (self$sigma_bias * eB) output = k_bias_add(noise_injected_weights, noise_injected_bias) output } , compute_output_shape = function(input_shape) { output_shape &lt;- input_shape output_shape[[2]] &lt;- self$output_dim output_shape } ) ) noise_add_layer &lt;- function(object, output_dim) { create_layer( noise_CustomLayer , object , list( name = 'noise_add_layer' , output_dim = as.integer(output_dim) , trainable = T ) ) } critic_input &lt;- layer_input( shape = c(as.integer(state_names_length)) , name = 'critic_input' ) common_layer_dense_1 &lt;- layer_dense( units = 20 , activation = "tanh" ) critic_layer_dense_v_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) critic_layer_dense_v_2 &lt;- layer_dense( units = 5 , activation = "tanh" ) critic_layer_dense_v_3 &lt;- layer_dense( units = 1 , name = 'critic_layer_dense_v_3' ) critic_layer_dense_a_1 &lt;- layer_dense( units = 10 , activation = "tanh" ) # critic_layer_dense_a_2 &lt;- layer_dense( # units = 5 # , activation = "tanh" # ) critic_layer_dense_a_3 &lt;- layer_dense( units = length(acts) , name = 'critic_layer_dense_a_3' ) critic_model_v &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_v_1 %&gt;% critic_layer_dense_v_2 %&gt;% critic_layer_dense_v_3 %&gt;% v_normalize_layer critic_model_a &lt;- critic_input %&gt;% common_layer_dense_1 %&gt;% critic_layer_dense_a_1 %&gt;% #critic_layer_dense_a_2 %&gt;% noise_add_layer(output_dim = 5) %&gt;% critic_layer_dense_a_3 %&gt;% a_normalize_layer critic_output &lt;- layer_add( list( critic_model_v , critic_model_a ) , name = 'critic_output' ) critic_model_1 &lt;- keras_model( inputs = critic_input , outputs = critic_output ) critic_optimizer = optimizer_adam(lr = learning_rate) keras::compile( critic_model_1 , optimizer = critic_optimizer , loss = 'mse' , metrics = 'mse' ) train.x &lt;- rnorm(state_names_length * 10) train.x &lt;- array(train.x, dim = c(10, state_names_length)) predict(critic_model_1, train.x) layer_name &lt;- 'noise_add_layer' intermediate_layer_model &lt;- keras_model(inputs = critic_model_1$input, outputs = get_layer(critic_model_1, layer_name)$output) predict(intermediate_layer_model, train.x)[1,] critic_model_2 &lt;- critic_model_1</span></span></code> </pre> <br></div></div><br>  I used this source to adapt the Python code for the noise part of the network: <a href="https://github.com/jakegrigsby/keras-rl">github repo</a> <br><br>  This neural network looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3dc/20c/e1e/3dc20ce1eedec47b871ef59ab7911ef3.png" alt="image"><br><br>  Recall that in the duel architecture, we use equality (equation 1): <br><br>  Q = A '+ V, where <br><br>  A '= A - avg (A); <br><br>  Q = state-action value; <br><br>  V = state value; <br><br>  A = advantage. <br><br>  Other variables in the code speak for themselves.  In addition, this architecture is only good for a specific task, so do not take it for granted. <br><br>  The rest of the code is likely to be quite template for publication, and it will be interesting for the programmer to write it yourself. <br><br>  And now - experiments.  Testing the work of the agent was carried out in a sandbox, far from the realities of trading in a live market, with a real broker. <br><br><h3>  Phase I </h3><br>  We run our agent vs synthetic dataset.  Our transaction value is 0.5: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb2/f15/3b6/fb2f153b6c4e14ff1dc19d61a524f5e8.png" alt="image"><br><br>  The result is excellent.  The maximum average episodic reward in this experiment <br>  should be 1.5. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/417/0a0/a84/4170a0a842560a2a2fd3112d601dba54.jpg" alt="image"><br><br>  We see: the loss of the critic (the so-called network of values ‚Äã‚Äãin the approach of the actor-critic), the average remuneration for the episode, the cumulative reward, the sample of the last rewards. <br><br><h3>  Phase II </h3><br>  We train our agent to an arbitrarily chosen exchange symbol, which demonstrates interesting behavior: a smooth start, fast growth in the middle and a dreary end.  In our training set about 4300 days.  Transaction cost is set at 0.1 USD (purposefully low);  the reward is USD Profit / loss after closing a deal to buy / sell 1.0 shares. <br><br>  Source: <a href="https://finance.yahoo.com/quote/algn%3Fltr%3D1">finance.yahoo.com/quote/algn?ltr=1</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6d/22c/540/d6d22c540514afc7261f549bea8c74ac.png" alt="image"><br><br>  <i>NASDAQ: ALGN</i> <br><br>  After configuring some parameters (leaving the NN architecture the same), we arrived at the following result: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/612/008/50c/61200850c8a8c0b7963bbee12ca1a2bd.jpg" alt="image"><br><br>  It turned out not bad, because in the end the agent learned to make a profit by pressing the three buttons on his console. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d8d/d86/afb/d8dd86afb84b4f4625012a5c3ba9dc4e.png" alt="image"><br><br>  <i>red marker = sell, green marker = buy, gray marker = do nothing.</i> <br><br>  Note that at its peak, the average reward for an episode exceeded the realistic value of the transaction, which can be encountered in real trading. <br><br>  It is a pity that stocks are falling like crazy because of bad news ... <br><br><h3>  Concluding remarks </h3><br>  Trading with RL is not only difficult, but also useful.  When your robot does it better than you, it's time to spend personal time to get an education and health. <br><br>  I hope it was an interesting journey for you.  If you like this story, wave your hand.  If there is a lot of interest, I can continue and show you how policy gradient methods work using the R language and the Keras API. <br><br>  I also want to thank my friends who are passionate about neural networks for their advice. <br><br>  If you have any questions, I am always here. </div><p>Source: <a href="https://habr.com/ru/post/433182/">https://habr.com/ru/post/433182/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../433170/index.html">How did we fail to remake the architecture of the company</a></li>
<li><a href="../433172/index.html">Screwing multiplayer to the mobile game "Make a word from a word" on iOS and Android, written in C ++</a></li>
<li><a href="../433176/index.html">Docker Remote API with certificate authentication with revocation checking</a></li>
<li><a href="../433178/index.html">How we recovered a corrupted .wav file</a></li>
<li><a href="../433180/index.html">Solving data type problems in Ruby or Make data reliable again</a></li>
<li><a href="../433184/index.html">ASP.NET Core 2.2 has been released. What's new? (2 of 3)</a></li>
<li><a href="../433186/index.html">It is not enough to count polygons to optimize 3D models.</a></li>
<li><a href="../433188/index.html">The State Duma submitted a bill on the autonomous work of the RuNet</a></li>
<li><a href="../433192/index.html">Kubernetes: an amazingly affordable solution for personal projects</a></li>
<li><a href="../433194/index.html">Night light with scheduled shutdown</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>