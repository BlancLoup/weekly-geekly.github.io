<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating a bot for participation in the AI ‚Äã‚Äãmini cup 2018 based on a recurrent neural network (part 2)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is a continuation of the first part of the article. 


 In the first part of the article, the author spoke about the conditions of the competitio...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating a bot for participation in the AI ‚Äã‚Äãmini cup 2018 based on a recurrent neural network (part 2)</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/of/tw/ke/oftwke-wbh5-w_nlsm_p5rmhano.jpeg"><br>  <a href="https://habr.com/post/417311/">This is a continuation of the first part of the article.</a> </p><br><p>  In the first part of the article, the author spoke about the conditions of the competition for the game Agario on mail.ru, the structure of the game world, and partly about the device of the bot.  Partially, because they affected only the device of input sensors and commands at the exit from the neural network (further in pictures and text there will be an abbreviation NN).  So try to open the black box and understand how everything is arranged there. </p><a name="habracut"></a><br><p>  Here is the first picture: </p><br><p><img src="https://habrastorage.org/webt/v2/pb/_s/v2pb_sha05gqjtqcqp9bv5mnm0o.jpeg"></p><br><p>  It schematically depicts what should cause a bored smile to my reader, saying, again, in first grade, they have seen it many times in <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D1%2581%25D0%25BA%25D1%2583%25D1%2581%25D1%2581%25D1%2582%25D0%25B2%25D0%25B5%25D0%25BD%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD">various sources</a> .  But we want to practically apply this picture to the control of the bot, so after an Important Note, let us take a closer look at it. </p><br><p>  <strong>Important note:</strong> there are a large number of ready-made solutions (frameworks) for working with neural networks: </p><br><p><img src="https://habrastorage.org/webt/jt/il/97/jtil97rhtusvazj6iared1q_hnc.png"></p><br><p>  All these packages solve the main tasks for the developer of neural networks: the construction and training of NN or the search for "optimal" weights.  And the main method of this search is the <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">Back Propagation Error (eng backpropagation) method</a> .  They invented it in the 70s of the last century, as indicated by the article on the above link, during this time, like the bottom of a ship, it was overgrown with various improvements, but the essence is the same: finding weights with a database of examples for training and it is highly desirable that of these examples contained a ready-made response in the form of a neural network output.  I can argue the reader.  that self-learning networks of various classes and principles have already been invented, but everything is not smooth there, as far as I understand.  Of course, there are plans to explore this zoo in more detail, but I think I will find like-minded people in the fact that a bicycle made without any special drawings, even the most curved, is closer to the creator‚Äôs heart than the conveyor clone of a perfect bicycle. <br>  Understanding that the game server most likely will not have these libraries and the computational power allocated by the organizers in the form of 1 processor core is obviously not enough for a heavy framework, the author went by creating his own bike.  An important note is over. </p><br><p>  Let us return to the picture depicting perhaps the simplest possible neural networks with a hidden layer (also known as the hidden leer or hidden layer).  Now the author himself has closely looked at the picture with the ideas on this simple example to reveal to the reader the depths of artificial neural networks.  When everything is simplified to a primitive, it is easier to understand the essence.  The bottom line is that a hidden layer of a hidden layer has nothing to add.  And most likely it is not even a neural network, in the textbooks, the simplest NN is a network with two inputs.  So here we are, as it were, the discoverers of the simplest of the simplest networks. </p><br><p>  Let's try to describe this neural network (pseudocode): <br>  We introduce the network topology as an array, where each element corresponds to a layer and the number of neurons in it: </p><br><p><code>int array Topology= { 1, 1, 1}</code> <br>  We also need a float array of weights of the W neural network, considering our network as a ‚Äúforward neural networks, FF or FFNN‚Äù neural network, where each neuron of the current layer is connected to each neuron of the next layer, we obtain the dimension of the array W [number of layers , the number of neurons in the layer, the number of neurons in the layer].  Not quite the optimal encoding, but given the hot breath of the GPU somewhere very close in the text, it is understandable. <br>  The <code>CalculateSize</code> short procedure for counting the number of <code>neuroncount</code> neurons and the number of their connections in a <code>neuroncount</code> neural network, I think will explain the nature of these connections better than the author: </p><br><pre> <code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">void</span></span> CalculateSize(<span class="hljs-keyword"><span class="hljs-keyword">array</span></span> <span class="hljs-type"><span class="hljs-type">int</span></span> Topology, <span class="hljs-type"><span class="hljs-type">int</span></span> neuroncount, <span class="hljs-type"><span class="hljs-type">int</span></span> dendritecount) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i : Topology) // i         neuroncount += i; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt;Topology.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>, i &lt; Topology[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, i++) //   <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-type"><span class="hljs-type">int</span></span> j = <span class="hljs-number"><span class="hljs-number">0</span></span>, j &lt; Topology[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>], j++) //   dendritecount++; }</code> </pre> <br><p>  My reader, the one who already knows all this, the author came to this opinion in the first article, of course, he wondered why in the third nested cycle Topology [layer1 + 1] instead of Topology [layer1], which gives the neuron more than in the network topology .  And I will not answer.  It is also useful for the reader to set tasks at home. </p><br><p>  We are almost a step away from building a working neural network.  It remains to add the function of summing signals at the input of the neuron and its activation.  There are many activation functions, but those closest to the nature of the neuron are Sigmoid and Tangensoid <em>(probably it is better to call it this, although this name is not particularly used in the literature, the maximum of the tangensoid, but this is the name of the graph, but what is the graph if not a reflection of the function?)</em> </p><br><p>  So we have the neuron activation functions (in the picture they are present, in its lower part) </p><br><pre> <code class="hljs go">float Sigmoid(float x) { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (float)(<span class="hljs-number"><span class="hljs-number">1.0f</span></span> / (<span class="hljs-number"><span class="hljs-number">1.0f</span></span> + expf(-x))); }</code> </pre> <br><p>  Sigmoid returns values ‚Äã‚Äãfrom 0 to 1. </p><br><pre> <code class="hljs cs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">float</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Tanh</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> x</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &lt; <span class="hljs-number"><span class="hljs-number">-10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">-1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (x &gt; <span class="hljs-number"><span class="hljs-number">10.0f</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1.0f</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">float</span></span>)(tanhf(x)); }</code> </pre> <br><p>  Tangensoid returns values ‚Äã‚Äãfrom -1 to 1. </p><br><p>  The main idea of ‚Äã‚Äãthe signal passing through the neural network is a wave: a signal is sent to the input neurons-&gt; through neural connections; the signal passes to the second layer-&gt; the neurons of the second layer sum up the signals that have come to them, modified by interneuron scales-&gt; added through the additional weight bias (bias) -&gt; use the activation-&gt; function and go to the next layer (read the first cycle of the example from the layers), that is, from the very beginning, this repetition of the chain will be the neurons of the next layer.  In simplification, it is not even necessary to store the values ‚Äã‚Äãof the neurons of the entire network; you only need to store the weights NN and the values ‚Äã‚Äãof the neurons of the active layer. </p><br><p>  Once again, we give a signal to the input NN, the wave ran through the layers and remove the obtained value on the output layer. </p><br><p>  Here, it is possible to programmatically solve the reader's taste with the help of recursion or just a triple cycle as the author has, in order to speed up the calculations it is not necessary to fence objects in the form of neurons and their connections and other OOP.  Again, this is connected with the sensation of close GPU calculations, and on graphics processors, due to their nature of mass parallelism, OOP is a little stuck, this is relative to the languages ‚Äã‚Äãc # and c ++. </p><br><p>  Next, I propose that the reader independently go the way of building in the neural network code, with his reader having a voluntary desire, the absence of which is quite understandable and familiar to the author, as <a href="https://habr.com/post/313216/">for examples of building NN</a> from scratch, there are a large number of examples in the network, therefore it will be difficult to go astray as straightforward as a neural network of direct distribution in the above picture. </p><br><p>  But where the new pictures exclaim the reader, who has not yet departed from the previous passage, and will be right, in childhood the author determined the value of the book from illustrations to it.  Here you are: </p><br><p><img src="https://habrastorage.org/webt/03/jl/dw/03jldwzryoeaxscxxtfi3deakie.jpeg"></p><br><p>  In the picture we see a recurrent neuron and the NN constructed from such neurons is called recurrent or RNN.  This neural network has a short-term memory and is chosen by the author for the bot as the most promising from the point of view of adaptation to the game process.  Of course, the author built a direct distribution neural network, but in the process of searching for an "effective" solution, he switched to RNN. </p><br><p>  The recurrent neuron has an additional state C, which is formed after the first passage of the signal through the neuron, Tick + 0 on the time scale.  In simple terms, this is a copy of the output signal of the neuron.  In the second step, read Tick + 1 (since the network operates at the frequency of the gaming bot and server), the value C returns to the input of the neural layer through additional weights and thus participates in the formation of the signal, but by Tick + 1 time. </p><br><p>  <em>Note: <a href="https://arxiv.org/abs/1807.01281">in research groups' work on controlling NN gaming bots</a> , there is a tendency to use two rhythms for a neural network, one rhythm is the frequency of a game Tic, the second rhythm, for example, is twice as slow as the first.</em>  <em>Different parts of NN operate at different frequencies, which gives a different vision of the game situation inside the NN, thereby increasing its flexibility.</em> </p><br><p>  To build the RNN in the bot code, we introduce into the topology an additional array, where each element corresponds to a layer and the number of neural states in it: </p><br><p> <code>int array TopologyNN= { numberofSensors, 16, 8, 4}</code> <br> <code>int array TopologyRNN= { 0, 16, 0, 0 }</code> </p> <br><p>  From the above topology, it is clear that the second layer is recurrent, since it contains neural states.  We also introduce additional weights in the form of a WRR float array, the same dimension as the W array. </p><br><p>  The counting of connections in our neural network will change slightly: </p><br><pre> <code class="hljs matlab"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> &lt; TopologyNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> &lt; TopologyNN[layer + <span class="hljs-number"><span class="hljs-number">1</span></span>] , <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int layer = <span class="hljs-number"><span class="hljs-number">0</span></span>, layer &lt; TopologyRNN.Length - <span class="hljs-number"><span class="hljs-number">1</span></span>, layer++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">i</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>&lt; TopologyRNN[layer] + <span class="hljs-number"><span class="hljs-number">1</span></span> , <span class="hljs-built_in"><span class="hljs-built_in">i</span></span>++) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (int <span class="hljs-built_in"><span class="hljs-built_in">j</span></span> = <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>&lt; TopologyRNN[layer], <span class="hljs-built_in"><span class="hljs-built_in">j</span></span>++) dendritecount++;</code> </pre> <br><p>  The author will attach the general code of the recurrent neural network at the end of this article, but here the main thing is to understand the principle: passing the wave through the layers in the case of recurrent NN does not fundamentally change anything, only one more term is added to the neuron activation function.  This is the term of the state of neurons on the previous Tick multiplied by the weight of neural communication. </p><br><p>  We will consider that they have refreshed the theory and practice of neural networks, but the author clearly realizes that he did not bring the reader closer to understanding how to train this simple structure of neural connections to make any decisions in the game process.  We don‚Äôt have libraries with examples for teaching NN.  In the Internet groups of developers of bots, the opinion sounded: give us a log file in the form of coordinates of bots and other game information for the formation of a library of examples.  But the author unfortunately could not figure out how to use this log file for learning NN.  I will be glad to discuss this in the comments to the article.  Therefore, the only available for the author's understanding of the method of learning, or rather the finding of "effective" neural weights (neural links), was the genetic algorithm. </p><br><p>  Prepared a picture of the principles of the genetic algorithm: </p><br><p><img src="https://habrastorage.org/webt/-3/ex/ls/-3exlspxldh64avmkbi_3vuewou.jpeg"></p><br><p>  So the <strong>genetic algorithm</strong> . </p><br><p>  The author will try not to delve <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BD%25D0%25B5%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B8%25D0%25B9_%25D0%25B0%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC">into the theory of this process</a> , but to recall only the minimum necessary to continue the full reading of the article. <br>  In the genetic algorithm, the main working body is the gene (DNA is the name of the molecule).  In our case, the genome is a sequential set of genes or a one-dimensional array of float long ... </p><br><p>  At the initial stage of working with only the built neural network, it is necessary to initialize it.  Initialization is the assignment of random values ‚Äã‚Äãfrom -1 to 1 to the neural weights. The author mentioned that the range of values ‚Äã‚Äãfrom -1 to 1 is too <a href="https://ru.wikipedia.org/wiki/%25D0%25AD%25D0%25BA%25D1%2581%25D1%2582%25D1%2580%25D0%25B5%25D0%25BC%25D1%2583%25D0%25BC">extreme</a> and the trained networks have weights in a smaller range, for example, from -0.5 to 0.5, and that the initial range of values ‚Äã‚Äãshould be different. from -1 to 1. But we will go classic by collecting all the difficulties in one gate and take the widest possible segment of the initial random variables as the basis for the initialization of the neural network. </p><br><p>  Now there will be a <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B8%25D0%25B5%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F">bijection</a> .  We assume that the length (size) of a bot's genome will be equal to the total length of the neural network arrays TopologyNN.Length + TopologyRNN.Length it‚Äôs not for nothing that the author spent the reader‚Äôs time on the neural link counting procedure. </p><br><p>  <em>Note: As the reader has already noted for himself, we transfer only the weights of the neural network to the genotype, the structure of connections, activation functions, and the state of neurons are not transferred.</em>  <em>For a genetic algorithm, only neural connections are sufficient, which suggests that they are the information carriers.</em>  <em>There are developments where the genetic algorithm also changes the structure of connections in the neural network and is fairly easy to implement.</em>  <em>Here the author leaves space for creativity to the reader, although he himself will think about it with interest: you need to understand to use two independent genomes and two fitness functions (simply two independent genetic algorithms) or you can all under one genome and algorithm.</em> </p><br><p>  And since we initialized NN with random variables, we thereby initialized the genome.  The reverse process is also possible: initialization of the genotype with random variables and its subsequent copying into neural weights.  The usual is the second option.  Since the genetic algorithm in the program often exists separately from the entity itself and is associated with it only by the genome data and the value of the fitness function ... Stop, stop, the reader will say, the population clearly shows in the picture and not a word about a single genome. </p><br><p>  Well, add pictures to the oven of the reader's mind: </p><br><p><img src="https://habrastorage.org/webt/h5/dz/ho/h5dzhojhlf6b1xqsol30qo63mdo.jpeg"></p><br><p>  Since the author drew the pictures before writing the text of the article, they support the text, but do not follow the letter to the letter of the current narration. </p><br><p>  From the drawn information it follows that the main working body of the genetic algorithm is the population of <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B5%25D0%25BD%25D0%25BE%25D0%25BC">genomes</a> .  This is a bit contrary to what was previously said by the author, but how to do in the real world without small contradictions.  Yesterday the sun rotated around the earth, and today the author talks about a neural network inside a software bot.  No wonder he remembered the furnace of reason. <br>  I trust the reader to understand the question about the contradictions of the world.  The bot world is quite self-sufficient for the article. </p><br><p>  But what the author has already done, to this part of the article, is to form a population of bots. <br>  Let's look at it from the program side: </p><br><p>  There is a bot (it can be an object in the OOP, a structure, although it is probably also an object or just an array of data).  Inside, the bot contains information about its coordinates, speed, mass, and other information useful in the gameplay, but the main thing for us now is that it contains a link to its own genotype or the genotype itself, depending on the implementation.  Then you can go in different ways, limit yourself to arrays of the neural network weights, or enter an additional array of genotypes, as it will be convenient for the reader to imagine in your imagination.  At the first stages, the author selected arrays of neural weights and genotypes in the program.  Then he refused to duplicate information and limited himself to the weights of the neural network. </p><br><p>  Following the logic of the story you need to tell that the population of bots is an array of the above bots.  What is the game cycle ... Stop again, what game cycle?  the developers politely provided only one place for Bot to board the game simulation program on the server or a maximum of four bots in the local simulator.  And if you remember the topology of the neural network chosen by the author: </p><br><p><img src="https://habrastorage.org/webt/vh/1d/xq/vh1dxqmwqoejzszyh3zpfyykve4.jpeg"></p><br><p>  And suppose, to simplify the narration, that the genotype contains about 1000 neural connections, by the way, in the simulator, the genotypes look like this (the red color is the negative value of the gene, the green is a positive value, each line is a separate genome): </p><br><p><img src="https://habrastorage.org/webt/ri/h0/s-/rih0s-ss3gaflldpts95rm9yo4u.jpeg"></p><br><p>  <em>Note to the photo: over time, the pattern changes towards the dominance of one of the solutions; vertical bars are common genes of genotypes.</em> </p><br><p>  So, we have 1000 genes in the genotype and a maximum of four bots in the simulator program of the game world from the organizers of the competition.  How many times it is necessary to run the simulation of the battle of bots in order to bust, even the cleverest, in the search to get closer to the "effective" <br>  genotype, read the "effective" combination of neural connections, provided that each neural connection varies from -1 to 1 in increments, and which step?  initialization was random float, which is 15 decimal places.  Step is not yet clear to us.  On the number of variants of neural weights, the author assumes that this is an infinite number, when choosing a certain step size, probably a finite number, but in any case, these numbers are much more than 4 places in the simulator, even considering the sequential launch from the queue of bots plus simultaneous parallel launch of official simulators, up to 10 on one computer (for fans of vintage programming: computers). </p><br><p><img src="https://habrastorage.org/webt/it/-8/if/it-8ifszotccnx4wguexdtditpm.jpeg"></p><br><p>  I hope the pictures help the reader. </p><br><p>  Here you need to pause and talk about the architecture of the software solution.  Since the solution in the form of a separate software bot downloaded to the competition site is no longer suitable.  It was necessary to divide the bot playing according to the rules of the competition within the framework of the ecosystem of the organizers and the program trying to find the configuration of the neural network for it.  The diagram below is taken from the conference presentation, but generally reflects the real picture. </p><br><p><img src="https://habrastorage.org/webt/-i/hb/ph/-ihbph2b0hlv3lfzxze7ihmu3-q.jpeg"></p><br><p>  Remembered the bearded anecdote: </p><br><p>  <em>Large organization.</em> <em><br></em>  <em>Time 18.00, all employees work as one.</em>  <em>Suddenly one of the employees turns off the computer, gets dressed and leaves.</em> <em><br></em>  <em>All escorted him with a surprised look.</em> <em><br></em>  <em>The next day.</em>  <em>At 18.00 the same employee turns off the computer and leaves.</em>  <em>All continue to work and begin to whisper with displeasure.</em> <em><br></em>  <em>The next day.</em>  <em>At 18.00 the same employee turns off the computer ...</em> <em><br></em>  <em>A colleague approaches him:</em> <em><br></em>  <em>- As you are not ashamed, we are working, the end of the quarter, so many reports, we also want to go home on time, and you are such a one-man ...</em> <em><br></em>  <em>-Guys, yes, I'm generally on vacation!</em> </p><br><p>  ‚Ä¶ to be continued. </p><br><p>  Yes, I almost forgot to attach the code for the RNN calculation procedure, it is valid and written independently, so maybe there are errors in it.  To enhance, I will bring it as it is, it is in c ++ with reference to CUDA (library for calculating on the GPU). </p><br><p>  Note: multidimensional arrays do not get along well on GPUs, there are of course textures and matrix calculations, but it is recommended to use one-dimensional arrays. </p><br><p>  An example of an array [i, j] of dimension M through j becomes an array of the form [i * M + j]. </p><br><div class="spoiler">  <b class="spoiler_title">RNN calculation procedure source code</b> <div class="spoiler_text"><pre> <code class="hljs perl">__global_<span class="hljs-number"><span class="hljs-number">_</span></span> void cudaRNN(Bot *bot, argumentsRNN *RNN, ConstantStruct *Const, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> *Topology, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> *TopologyRNN, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> numElements, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> gameTick) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> tid = blockIdx.x * blockDim.x + threadIdx.x; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> threadN = gridDim.x * blockDim.x; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> TopologySize = Const-&gt;TopologySize; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pos</span></span> = tid; <span class="hljs-keyword"><span class="hljs-keyword">pos</span></span> &lt; numElements; <span class="hljs-keyword"><span class="hljs-keyword">pos</span></span> += threadN) { const <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ii = <span class="hljs-keyword"><span class="hljs-keyword">pos</span></span>; const <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> iiA = <span class="hljs-keyword"><span class="hljs-keyword">pos</span></span>*Const-&gt;ArrayDim; <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ArrayDim = Const-&gt;ArrayDim; const <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> iiAT = ii*TopologySize*ArrayDim; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (bot[<span class="hljs-keyword"><span class="hljs-keyword">pos</span></span>].TTF != <span class="hljs-number"><span class="hljs-number">0</span></span> &amp;&amp; bot[<span class="hljs-keyword"><span class="hljs-keyword">pos</span></span>].Mass&gt;<span class="hljs-number"><span class="hljs-number">0</span></span>) { RNN-&gt;outputs[iiA + Topology[<span class="hljs-number"><span class="hljs-number">0</span></span>]] = <span class="hljs-number"><span class="hljs-number">1</span></span>.f; <span class="hljs-regexp"><span class="hljs-regexp">//bias</span></span> <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> neuroncount7 = Topology[<span class="hljs-number"><span class="hljs-number">0</span></span>]; neuroncount7++; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> layer1 = <span class="hljs-number"><span class="hljs-number">0</span></span>; layer1 &lt; TopologySize - <span class="hljs-number"><span class="hljs-number">1</span></span>; layer1++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> j4 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j4 &lt; Topology[layer1 + <span class="hljs-number"><span class="hljs-number">1</span></span>]; j4++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i5 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i5 &lt; Topology[layer1] + <span class="hljs-number"><span class="hljs-number">1</span></span>; i5++) { RNN-&gt;sums[iiA + j4] = RNN-&gt;sums[iiA + j4] + RNN-&gt;outputs[iiA + i5] * RNN-&gt;NNweights[((ii*TopologySize + layer1)*ArrayDim + i5)*ArrayDim + j4]; } } <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (TopologyRNN[layer1] &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> j14 = <span class="hljs-number"><span class="hljs-number">0</span></span>; j14 &lt; Topology[layer1]; j14++) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> i15 = <span class="hljs-number"><span class="hljs-number">0</span></span>; i15 &lt; Topology[layer1]; i15++) { RNN-&gt;sumsContext[iiA + j14] = RNN-&gt;sumsContext[iiA + j14] + RNN-&gt;neuronContext[iiAT + ArrayDim * layer1 + i15] * RNN-&gt;MNweights[((ii*TopologySize + layer1)*ArrayDim + i15)*ArrayDim + j14]; } RNN-&gt;sumsContext[iiA + j14] = RNN-&gt;sumsContext[iiA + j14] + <span class="hljs-number"><span class="hljs-number">1.0</span></span>f* RNN-&gt;MNweights[((ii*TopologySize + layer1)*ArrayDim + Topology[layer1])*ArrayDim + j14]; <span class="hljs-regexp"><span class="hljs-regexp">//bias</span></span>=<span class="hljs-number"><span class="hljs-number">1</span></span> } <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> t = <span class="hljs-number"><span class="hljs-number">0</span></span>; t &lt; Topology[layer1 + <span class="hljs-number"><span class="hljs-number">1</span></span>]; t++) { RNN-&gt;outputs[iiA + t] = Tanh(RNN-&gt;sums[iiA + t] + RNN-&gt;sumsContext[iiA + t]); RNN-&gt;neuronContext[iiAT + ArrayDim * layer1 + t] = RNN-&gt;outputs[iiA + t]; } //SoftMax /* double sum = <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt;ArrayDim; ++k) sum += <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(RNN-&gt;outputs[iiA + k]); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span> k = <span class="hljs-number"><span class="hljs-number">0</span></span>; k &lt; ArrayDim; ++k) RNN-&gt;outputs[iiA + k] = <span class="hljs-keyword"><span class="hljs-keyword">exp</span></span>(RNN-&gt;outputs[iiA + k]) / sum; *<span class="hljs-regexp"><span class="hljs-regexp">/ } else { for (int i1 = 0; i1 &lt; Topology[layer1 + 1]; i1++) { RNN-&gt;outputs[iiA + i1] = Sigmoid(RNN-&gt;sums[iiA + i1]); /</span></span><span class="hljs-regexp"><span class="hljs-regexp">/sigma } } if (layer1 + 1 != TopologySize - 1) { RNN-&gt;outputs[iiA + Topology[layer1 + 1]] = 1.f; } for (int i2 = 0; i2 &lt; ArrayDim; i2++) { RNN-&gt;sums[iiA + i2] = 0.f; RNN-&gt;sumsContext[iiA + i2] = 0.f; } } } } }</span></span></code> </pre> </div></div></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/417657/">https://habr.com/ru/post/417657/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417647/index.html">Belarus has a draft law on the protection of personal data - what is ‚Äúinside‚Äù?</a></li>
<li><a href="../417649/index.html">OpenAI has overcome significant limitations in Dota 2 AI games.</a></li>
<li><a href="../417651/index.html">What should the reader do for you to read more?</a></li>
<li><a href="../417653/index.html">Open lesson "Basic database concepts"</a></li>
<li><a href="../417655/index.html">Background: State Corporation "Roscosmos" and its work</a></li>
<li><a href="../417659/index.html">Neuropoet and other future stars</a></li>
<li><a href="../417661/index.html">Aubrey de Gray is visiting Joe Rogan</a></li>
<li><a href="../417665/index.html">English grammar as mathematics. Where to start for those who did not work out</a></li>
<li><a href="../417667/index.html">AI. Tactical Barrier Tracker</a></li>
<li><a href="../417671/index.html">New ABAP programming language features on SAP webinars</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>