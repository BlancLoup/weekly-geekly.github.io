<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Understanding Q-learning, the problem of "walking on a rock"</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I bring to your attention the translation of the article ‚ÄúUnderstanding Q-Learning, the Cliff Walking problem‚Äù by Lucas Vazquez . 




 In t...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Understanding Q-learning, the problem of "walking on a rock"</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  I bring to your attention the translation of the article <a href="https://medium.com/init27-labs/understanding-q-learning-the-cliff-walking-problem-80198921abbc">‚ÄúUnderstanding Q-Learning, the Cliff Walking problem‚Äù</a> by <a href="https://medium.com/%40lgvaz">Lucas Vazquez</a> . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ez/dl/ol/ezdlol2oj19smcejxtdlt4aon68.jpeg"></div><br><p>  In the <a href="https://medium.com/init27-labs/reinforcement-learning-rl-for-the-intimates-eef924cd5ee4">last post,</a> we presented the problem of "Walking on a rock" and stopped at a terrible algorithm that did not make sense.  This time we will reveal the secrets of this gray box and see that it is not so bad at all. </p><br><h3>  Summary </h3><br><p>  We concluded that by maximizing the amount of future rewards, we also find the fastest way to the goal, so our goal now is to find a way to do it! </p><br><a name="habracut"></a><br><h3>  Introduction to Q-Learning </h3><br><ul><li>  Let's start by building a table that measures how well a particular action will perform in any state (we can measure it with a simple scalar value, so the larger the value, the better the action) </li><li>  This table will have one row for each state and one column for each action.  In our world, the grid has 48 (4 to Y to 12 to X) states and 4 actions are allowed (up, down, left, right), so the table will be 48 x 4. </li><li>  The values ‚Äã‚Äãstored in this table are called ‚ÄúQ-values‚Äù. </li><li>  These are estimates of the amount of future awards.  In other words, they estimate how much more rewards we can receive until the end of the game, being in state S and performing action A. </li><li>  We initialize the table with random values ‚Äã‚Äã(or some constant, for example, all zeros). </li></ul><br><p>  The optimal ‚ÄúQ-table‚Äù has values ‚Äã‚Äãthat allow us to take the best actions in each state, ultimately giving us the best way to win.  The problem is solved, cheers, masters of robots :). </p><br><img src="https://habrastorage.org/webt/5a/ii/wl/5aiiwljmx4igtrsrhrc3qoymoge.png"><br>  <i>Q-values ‚Äã‚Äãof the first five states.</i> <br><br><h4>  Q-learning </h4><br><ul><li>  Q-learning is an algorithm that ‚Äúlearns‚Äù these values. </li><li>  At every step we get more information about the world. </li><li>  This information is used to update the values ‚Äã‚Äãin the table. </li></ul><br><p>  For example, suppose we are one step away from the goal (square [2, 11]), and if we decide to go down, we will receive a reward of 0 instead of -1. <br>  We can use this information to update the value of the state-action pair in our table, and the next time we visit it, we will already know that going down gives us a reward of 0. </p><br><img src="https://habrastorage.org/webt/7a/iq/u3/7aiqu3ttrz1qnypctrqvlgyd93e.png"><br><p>  Now we can spread this information even further!  Since we now know the path to the target from the square [2, 11], any action that leads us to the square [2, 11] will also be good, so we update the Q-value of the square that leads us to [2, 11] , to be closer to 0. </p><br><p>  <b>And that, ladies and gentlemen, is the essence of Q-learning!</b> </p><br><p>  Please note that every time we reach a goal, we increase our ‚Äúmap‚Äù of how to achieve the goal by one square, so after a sufficient number of iterations we will have a complete map that will show us how to get to the goal from each state. </p><br><img src="https://habrastorage.org/webt/mj/q0/sh/mjq0shtkn3u37zlhdpptbh2oppe.gif"><br>  <i>The path is generated by taking the best actions in each state.</i>  <i>Green tonality represents the value of a better action, more saturated tonalities represent higher values.</i>  <i>The text represents the value for each action (up, down, right, left).</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Bellman equation </h3><br><p>  Before we talk about code, let's talk about mathematics: the basic concept of Q-learning, the Bellman equation. </p><br><img src="https://habrastorage.org/webt/i2/_u/gx/i2_ugxlinshqmsyzkawlbmirxri.png"><br><br><ul><li>  First, let's forget Œ≥ in this equation. </li><li>  The equation states that the value of Q for a certain state-action pair must be the reward obtained when moving to a new state (by performing this action), added to the value of the best action in the next state. </li></ul><br><p>  <b>In other words, we disseminate information about the values ‚Äã‚Äãof actions one step at a time!</b> </p><br><p>  But we can decide that receiving an award right now is more valuable than receiving an award in the future, and therefore we have Œ≥, a number from 0 to 1 (usually from 0.9 to 0.99), which is multiplied by the award in the future, devaluing future rewards. </p><br><p>  So, given Œ≥ = 0.9 and applying this to some states of our world (grid), we have: </p><br><img src="https://habrastorage.org/webt/e7/yp/gi/e7ypginhzc-cetmrcbqa6ar3byg.png"><br><br><p>  We can compare these values ‚Äã‚Äãwith those given above in the GIF and see that they are the same. </p><br><br><h3>  Implementation </h3><br><p>  Now that we have an intuitive understanding of how Q-learning works, we can start thinking about realizing all of this, and we will use the Q-learning pseudo-code from <a href="http://incompleteideas.net/book/bookdraft2017nov5.pdf">Sutton‚Äôs book</a> as a guide. </p><br><img src="https://habrastorage.org/webt/wf/6x/fi/wf6xfiyazgu0echvfsw8d9-oly4.png"><br>  <i>Pseudocode from Sutton's book.</i> <br><br><p>  Code: </p><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Initialize Q arbitrarily, in this case a table full of zeros q_values = np.zeros((num_states, num_actions)) # Iterate over 500 episodes for _ in range(500): state = env.reset() done = False # While episode is not over while not done: # Choose action action = egreedy_policy(q_values, state, epsilon=0.1) # Do the action next_state, reward, done = env.step(action) # Update q_values td_target = reward + gamma * np.max(q_values[next_state]) td_error = td_target - q_values[state][action] q_values[state][action] += learning_rate * td_error # Update state state = next_state</span></span></code> </pre> <br><ul><li>  First, we say: ‚ÄúFor all states and actions, we initialize Q (s, a) arbitrarily,‚Äù which means that we can create our table of Q-values ‚Äã‚Äãwith any values ‚Äã‚Äãthat we like, they can be random, they can to be some kind of constant doesn't matter.  We see that in row 2 we create a table full of zeros. </li></ul><br><p>  <b>We also say: ‚ÄúThe value of Q for the final states is zero‚Äù, we cannot take any actions in the final states, so we consider the value for all actions in this state to be zero.</b> </p><br><ul><li>  For each episode, we have to ‚Äúinitialize S‚Äù, this is just a fancy way of saying ‚Äúreload the game‚Äù, in our case it means putting the player to the starting position;  in our world there is a method that does just that, and we are calling it in line 6. </li><li>  For each time step (each time we need to act) we must choose an action derived from Q. </li></ul><br><p>  Remember, I said ‚Äúwe are taking actions that have the greatest value in every condition? </p><br><p>  When we do this, we use our Q-values ‚Äã‚Äãto create a policy;  in this case, it will be a greedy policy, because we always take actions that, in our opinion, are best in every state, therefore it is said that we act greedily. </p><br><br><h3>  Whisk </h3><br><p>  But there is a problem with this approach: imagine that we are in a maze that has two rewards, one of which is +1 and the other is +100 (and every time we find one of them, the game ends).  Since we always take actions that we consider to be the best, we will be stuck with the first award found, always returning to it, therefore, if we first recognize the +1 award, we will miss the big +100 award. </p><br><br><h3>  Decision </h3><br><p>  We need to make sure that we have studied our world enough (this is an amazingly difficult task).  This is where Œµ comes into play.  Œµ in the greedy algorithm means that we have to act greedily, BUT do random actions as a percentage of Œµ in time, thus, with an infinite number of attempts, we must investigate all the states. </p><br><p>  The action is chosen in accordance with this strategy in line 12, with epsilon = 0.1, which means that we are exploring the world 10% of the time.  The implementation of the policy is as follows: </p><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">egreedy_policy</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(q_values, state, epsilon=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment"># Get a random number from a uniform distribution between 0 and 1, # if the number is lower than epsilon choose a random action if np.random.random() &lt; epsilon: return np.random.choice(4) # Else choose the action with the highest value else: return np.argmax(q_values[state])</span></span></code> </pre><br><p>  In line 14 in the first listing, we call the step method to perform the action, the world returns us the next state, reward, and information about the end of the game. </p><br><p>  Let's go back to the math: </p><br><p>  We have a long equation, let's think about it: </p><br><img src="https://habrastorage.org/webt/9v/bn/ws/9vbnws8g-1gclwefuvtpjv_yqpm.png"><br><br><p>  If we take Œ± = 1: </p><br><img src="https://habrastorage.org/webt/7r/aw/er/7rawertkpcilxbfzrorhpygtrok.png"><br><br><p>  What exactly coincides with the Bellman equation, which we saw a few paragraphs ago!  So we already know that this is the string responsible for disseminating information about the values ‚Äã‚Äãof the states. </p><br><p>  But usually Œ± (mostly known as learning speed) is much less than 1, its main goal is to avoid big changes in one update, so instead of flying to a goal, we slowly approach it.  In our tabular approach, setting Œ± = 1 does not cause any problems, but when working with neural networks (more on this in the next articles) everything can easily get out of control. </p><br><p>  Looking at the code, we see that in line 16 in the first listing we defined td_target, this is the value we should get closer to, but instead of going directly to this value in line 17, we calculate td_error, we will use this value in conjunction with the speed learning to move slowly towards the goal. </p><br><p>  <b>Remember that this equation is the essence of Q-Learning.</b> </p><br><p>  Now we just need to update our state, and everything is ready, this is line 20. We repeat this process until we reach the end of the episode, dying in the rocks or reaching the goal. </p><br><br><h3>  Conclusion </h3><br><p>  Now we intuitively understand and know how to code Q-Learning (at least the tabular version), be sure to check all the code used for this post, available on <a href="https://github.com/lgvaz/blog/blob/master/rl_intro.ipynb">GitHub</a> . </p><br><p>  Visualization of the learning process test: </p><br><iframe width="560" height="315" src="https://www.youtube.com/embed/Vto8n9C7DSQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><p>  Note that all actions begin with a value greater than its final value; this is a trick to stimulate the exploration of the world. </p></div><p>Source: <a href="https://habr.com/ru/post/443240/">https://habr.com/ru/post/443240/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../443222/index.html">How Protonmail is blocked in Russia</a></li>
<li><a href="../443224/index.html">Mat elephant and horse</a></li>
<li><a href="../443234/index.html">What engineers in Apple and Intel are doing in the office: career guidance online course on modern microelectronics for schoolchildren</a></li>
<li><a href="../443236/index.html">We demystify convolutional neural networks</a></li>
<li><a href="../443238/index.html">How not to turn into a dragonfly if you have a lot of different databases</a></li>
<li><a href="../443242/index.html">Quarkus - supersonic subatomic Java. Framework Overview</a></li>
<li><a href="../443244/index.html">Analysis of tasks. Binpoisk_1</a></li>
<li><a href="../443246/index.html">How we reinvented the IP PBX Askozia, after the project was sold and closed by the developer</a></li>
<li><a href="../443248/index.html">PRP and HSR seamless redundancy protocols</a></li>
<li><a href="../443250/index.html">Homemade garbage collector for OpenJDK</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>