<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Simulation of a neural network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Dear habocoobschestvuyu, decided to share with you my experience in the study of the neural network of the Boltzmann Machine, made in the student year...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Simulation of a neural network</h1><div class="post__text post__text-html js-mediator-article"> Dear habocoobschestvuyu, decided to share with you my experience in the study of the neural network of the Boltzmann Machine, made in the student year. <br><br>  In Russia there was very little information on this topic.  Even the head of our department could not help me with the material.  The benefit of our university was in a single international base, and there was an opportunity to take advantage of international experience.  In particular, most of it was found in the literature of the University of Oxford.  In fact, this article is a collection of information from various sources, reinterpreted and presented in a fairly clear language, I think.  I hope someone will be interested.  Once it forced me not to sleep at night. <br>  So let's get started. <br><a name="habracut"></a><br>  <b>Hopfield network</b> <br><br>  Among the various configurations of artificial neural networks (NS) there are such, when classifying them according to the principle of learning, strictly speaking, neither learning with a teacher [1] nor learning without a teacher [2] is suitable.  In such networks, the weights of synapses are calculated only once before the start of the network operation based on information about the data being processed, and all network training is reduced to just this calculation.  On the one hand, the presentation of a priori information can be regarded as a teacher‚Äôs help, but on the other hand, the network actually just remembers the samples before real data arrives at its entrance and cannot change its behavior, therefore, talking about a link of feedback with the ‚Äúworld "(Teacher) is not necessary.  Of the networks with similar logic, the most well-known is the Hopfield network, which is commonly used to organize associative memory.  That is what we consider. <br>  The block diagram of the Hopfield network is shown in the figure below.  It consists of a single layer of neurons, the number of which is simultaneously the number of inputs and outputs of the network.  Each neuron is connected by synapses with all other neurons, and also has one input synapse, through which the signal is input.  Output signals, as usual, are formed on axons. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/781/61c/41b/78161c41be1e5b2a44e1b09311a8eb4f.png" alt="image"><br><br>  The problem solved by this network as an associative memory, as a rule, is formulated as follows.  A certain set of binary signals (images, sound digitization, other data describing certain objects or characteristics of processes), which are considered exemplary, is known.  The network must be able to select (‚Äúremember‚Äù by partial information) the corresponding sample (if any) or ‚Äúgive a conclusion‚Äù that the input data do not correspond to any of the samples from an arbitrary non-ideal signal applied to its input.  In general, any signal can be described by the vector X = {xi: i = 0 ... n-1}, n is the number of neurons in the network and the dimension of the input and output vectors.  Each element xi is either +1 or -1.  Denote the vector describing the k-th pattern by Xk, and its components, respectively, xik, k = 0 ... m-1, m is the number of samples.  When the network recognizes (or ‚Äúremembers‚Äù) a sample based on the data presented to it, its outputs will contain it, that is, Y = Xk, where Y is the vector of output network values: Y = {yi: i = 0 ,. ..n-1}.  Otherwise, the output vector does not coincide with any exemplary. <br>  If, for example, the signals are some images, then by displaying the data from the network output graphically, you can see a picture that completely coincides with one of the exemplary (if successful) or ‚Äúfree improvisation‚Äù of the network (in case of failure). <br><br>  At the network initialization stage, the weights of synapses are set as follows [3] [4]: <br><img src="https://habrastorage.org/getpro/habr/post_images/c39/691/95b/c3969195bbc30e035ca8a7c524c56f79.jpg" alt="image">  (one) <br>  Here, i and j are indices of, respectively, presynaptic and postsynaptic neurons;  xik, xjk are the i-th and j-th elements of the k-th sample vector. <br>  The network operation algorithm is as follows (p is the iteration number): <br>  1. An unknown signal is sent to the network inputs.  In fact, it is entered directly by setting axon values: <br>  Yi (0) = xi, i = 0 ... n-1 (2) <br>  therefore, the designation on the network diagram of the input synapses in an explicit form is purely conditional.  Zero in the bracket to the right of yi means zero iteration in the network loop. <br>  2. Calculate the new state of neurons. <br><img src="https://habrastorage.org/getpro/habr/post_images/d0c/67f/a3f/d0c67fa3f0b5b087352b1b905c9c94f5.jpg" alt="image">  (3) <br>  and new values ‚Äã‚Äãof axons <br><img src="https://habrastorage.org/getpro/habr/post_images/a91/7f9/3a2/a917f93a2992cc7192d7a128f295c438.jpg" alt="image">  (four) <br><img src="https://habrastorage.org/getpro/habr/post_images/5ae/0b5/5aa/5ae0b55aacddda0d8b500943b5f6e3f4.jpg" alt="image"><br><br>  Fig.  4 Activation Functions <br>  where f - activation function in the form of a jump, shown in Figure (4) <br><br>  3. Check whether the output values ‚Äã‚Äãof axons changed during the last iteration.  If yes, go to step 2, otherwise (if the outputs are stabilized), end.  At the same time, the output vector is a sample that is best combined with the input data. <br>  As mentioned above, sometimes the network can not conduct recognition and outputs a non-existent image.  This is due to the problem of limited network capabilities.  For the Hopfield network, the number of memorized images m should not exceed a value approximately equal to 0.15 ‚Ä¢ n.  In addition, if the two images A and B are very similar, they will probably cause cross associations in the network, that is, presentation of the vector A to the network inputs will result in the appearance of vector B at its outputs and vice versa. <br>  Also for a complete understanding of the functioning of the network it is necessary to mention the Hebbian rule. <br>  The learning rule for the Hopfield network is based on research by Donald Hebb (D.Hebb, 1949), who suggested that the synaptic connection connecting the two neurons will increase if both neurons in the process of learning are consistently excited or inhibited.  A simple algorithm that implements such a learning mechanism is called the Hebbian rule.  Consider it in detail. <br><br>  Let a training sample of images be given xa, a = 1..p.  It is required to construct the process of obtaining the matrix of connections W, such that the corresponding neural network will have the images of the training sample as stationary states (the values ‚Äã‚Äãof the thresholds of neurons T are usually set equal to zero). <br>  In the case of a single training image, Hebb's rule leads to the required matrix: <br>  Wij = Œæi * Œæj (5) <br>  Let us show that the state S = x is stationary for the Hopfield network with the indicated matrix.  Indeed, for any pair of neurons i and j, the energy of their interaction in the state x reaches its minimum possible value. <br>  Eij = - (1/2) xi xj xi xj = -1/2.  (6) <br>  In this case,  - total energy is equal to E = - (1/2) N2, which corresponds to the global minimum. <br>  An iterative process can be used to memorize other images: <br>  Wij (a) = Wij (a-1) + Œæi (a) + Œæj (a), W (0) = 0, a = 1..p (7) <br>  which leads to a full matrix of connections in the form of Hebb: <br><img src="https://habrastorage.org/getpro/habr/post_images/548/273/809/548273809a6bf55d749f0e30b8fae26e.jpg" alt="image">  (eight) <br>  The stability of the totality of images is not as obvious as in the case of a single image.  A number of studies show that a neural network trained according to Hebb's rule can, on average, with a large network size N, store no more than p ¬ª0.14 N different images.  Resilience can be shown for a set of orthogonal patterns when <br><img src="https://habrastorage.org/getpro/habr/post_images/f49/13a/7db/f4913a7db9280bc22911c3f3fee13bf7.jpg" alt="image">  (9) <br>  In this case, for each state xa, the product of the total input of the ith neuron hi by the amount of its activity Si = xia is positive, therefore the state xa itself is a state of attraction (stable attractor): <br><img src="https://habrastorage.org/getpro/habr/post_images/f30/da9/a43/f30da9a43274b91391dce365224e2b5b.jpg" alt="image">  (ten) <br>  Thus, the Hebba rule ensures the stability of the Hopfield network on a given set of relatively few orthogonal patterns. <br><br>  <b>Synchronous and asynchronous activation option</b> <br><br>  Consider the behavior of the network in time with a fixed matrix of weights.  Let at the moment t = 0 we betrayed the state vector some value.  There are two options for the future process of the network: synchronous and asynchronous.  The synchronous version of the work is that all neurons simultaneously change their state at time t + 1 according to the state of the network at time t.  In the case of asynchronous operation at time t + 1, only one neuron changes its state, at the time t + 2 some other neuron changes according to the network state at time t + 1, etc.  (each time a neuron is chosen randomly).  In any case, over time, the network somehow changes its state.  It is argued that under the conditions imposed by us on the matrix of weights, the network will come to a stationary state after some finite time, that is.  It is also stated that this stationary state S is achieved and is the same regardless of the synchronism of the work of neurons. <br><br>  <b>Boltzmann Machines (Probabilistic Network)</b> <br><br>  One of the major drawbacks of the Hopfield network is the tendency for the output signal to ‚Äústabilize‚Äù at a local rather than a global minimum.  It is desirable that the network should find deep minima more often than shallow ones, and that the relative probability of a network moving to one of two different minima depends only on the ratio of their depths.  This would allow controlling the probabilities of obtaining specific output vectors by changing the profile of the energy surface of the system by modifying the link weights. <br>  The idea of ‚Äã‚Äãusing ‚Äúthermal noise‚Äù to get out of local minima and increase the likelihood of falling into deeper minima belongs to S. Kirpatriku.  Based on this idea, an annealing simulation algorithm has been developed. <br>  We introduce some parameter t - an analogue of the level of thermal noise.  Then the probability of activity of a certain neuron k is determined on the basis of the Boltzmann probability function: <br>  k = 1 / (1 + exp (-k / t)) (11) <br>  where t is the level of thermal noise in the network;  EC - the sum of the weights of the connections of the k-th neuron with all currently active neurons.  The curve of the change in the probability of the activity of the kth neuron is shown in Figure 5. As t decreases, the fluctuations in the activity of the neuron decrease;  when t = 0, the curve becomes threshold. <br><img src="https://habrastorage.org/getpro/habr/post_images/baa/cb5/d75/baacb5d75e4839af5771cc1abca55e2b.jpg" alt="image"><br>  The change in the probability of neuron activity depending on the parameter t. <br>  Statisticians call such networks random Markov fields.  The network using an annealing simulation algorithm for training is called the Boltzmann machine in honor of the Austrian physicist L. Boltzmann, one of the creators of statistical mechanics.  We formulate the problem of learning a probabilistic network, in which the probability of a neuron's activity is calculated by formula (11).  Let for each set of possible input vectors it is required to obtain with a certain probability all admissible output vectors.  In most cases, this probability is close to zero.  The procedure for learning the Boltzmann machine comes down to performing the following alternating steps: <br>  1) Submit the input vector to the network input and fix the output (as in the training procedure with the teacher).  Provide networks with the opportunity to get closer to the state of thermal equilibrium: <br>  a) assign the state of each neuron with probability pk (see formula (7.2)) to value one (active neuron) and with probability 1-pk - zero (not active neuron); <br>  b) reduce the parameter t, go to a). <br>  In accordance with the Hebbian rule, to increase the weight of the connection between active neurons by Œ¥.  Repeat these steps for all pairs of training sample vectors. <br>  2) Submit to the input network input vector without fixing the output vector.  Repeat a), b).  Reduce the weight of the connection between active neurons by Œ¥. <br>  The resulting change in the weight of the connection between two randomly taken neurons at a certain learning step will be proportional to the difference between the probabilities of the activity of these neurons at the 1st and 2nd steps.  When repeating steps 1 and 2, this difference tends to zero. <br>  The second way of learning - learning without a teacher - is a preliminary calculation of the coefficients in the matrix of weights.  These calculations are based on a preliminary knowledge of the conditions of the problem being solved. <br><br>  Friends, this is all I could remember and find in my storerooms. <br>  In the next article I want to describe the already applied aspect of this issue, namely the development of a program using the Boltzmann Machine algorithm.  The decision will be on the task of a salesman. <br>  Thank you all for your attention. <br>  Really looking forward to comments and questions. </div><p>Source: <a href="https://habr.com/ru/post/188368/">https://habr.com/ru/post/188368/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../188354/index.html">A brief introduction to GNU autoconf</a></li>
<li><a href="../188358/index.html">‚ÄúBoys - to the left, girls - to the right‚Äù, or add the ‚ÄúGender‚Äù field in the Oracle database</a></li>
<li><a href="../188360/index.html">Justification of the need to purchase SSD for developers</a></li>
<li><a href="../188362/index.html">DLNA server for home and family</a></li>
<li><a href="../188364/index.html">Search for the shortest path in the transport graph (concept) + source</a></li>
<li><a href="../188370/index.html">Selection of mnemonic quotes for car and phone numbers</a></li>
<li><a href="../188372/index.html">Basic principles of game development</a></li>
<li><a href="../188374/index.html">The story of a bad start - the option "All myself"</a></li>
<li><a href="../188376/index.html">Emacs as an IDE for Python</a></li>
<li><a href="../188380/index.html">Creating a full-fledged video hosting with their own hands (nginx + php5-fpm + ffmpeg + cumulusclips)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>