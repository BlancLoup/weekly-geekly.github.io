<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>VMware ESXi 5.X and NetApp ONTAP 8: Tuning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuing the topic of host optimization for interoperating with NetApp FAS storage systems, this article will be devoted to optimizing VMWare ESXi p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>VMware ESXi 5.X and NetApp ONTAP 8: Tuning</h1><div class="post__text post__text-html js-mediator-article">  Continuing the topic of host optimization for interoperating with NetApp <abbr title="Fabric attached storage">FAS</abbr> <abbr title="Storage System">storage</abbr> systems, this article will be devoted to optimizing VMWare ESXi performance, previous articles were devoted to tuning <a href="http://habrahabr.ru/post/245357/">Linux</a> and <a href="http://habrahabr.ru/post/243153/">Windows OS</a> in <abbr title="Storage Area Network">SAN</abbr> environment.  NetApp has been working closely with VMware for a long time, which can be confirmed by the fact that the sensational <a href="http://community.netapp.com/t5/Tech-OnTap-Articles/NetApp-Unlocks-the-Power-of-VMware-VVOLs/ta-p/86939">vVOL</a> technology <a href="http://community.netapp.com/t5/Tech-OnTap-Articles/NetApp-Unlocks-the-Power-of-VMware-VVOLs/ta-p/86939">was implemented as</a> one of the first in the release of Clustered Data ONTAP 8.2.1 (August 2014), while vSphere 6.0 has not even been released yet.  Therefore, NetApp <abbr title="Fabric attached storage">FAS</abbr> storage systems are extremely popular in this environment.  The <a href="https://habr.com/ru/post/247833/">Disk Alignment part</a> will be useful not only for NetApp owners. <br><br>  VMWare ESXi settings can be divided into the following parts: <br><br><ul><li>  Hypervisor Optimization </li><li>  Guest <abbr title="Operating system">OS</abbr> Optimization ( <abbr title="Guest Operation System">GOS</abbr> ) </li><li>  Optimal <abbr title="Storage Area Network">SAN</abbr> Settings ( <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> and <abbr title="Internet Small Computer System Interface">iSCSI</abbr> ) </li><li>  <abbr title="Network area storage">NAS</abbr> Settings ( <abbr title="Network file system">NFS</abbr> ) </li><li>  Check compatibility of equipment, firmware and <abbr title="Software">software</abbr> </li></ul><br><img src="https://habrastorage.org/files/cf7/a0d/398/cf7a0d3981cf470292069a67453b0264.jpg"><br>  To search for a bottleneck, a sequential exception technique is usually performed.  I suggest first thing to start with the <abbr title="Storage System">storage system</abbr> .  And move on to the <a href="http://habrahabr.ru/post/243045/">storage system</a> -&gt; Network ( <a href="http://habrahabr.ru/post/243119/">Ethernet</a> / FC) -&gt; Host ( <a href="http://habrahabr.ru/post/243153/">Windows</a> / <a href="http://habrahabr.ru/post/245357/">Linux</a> / <b>VMware ESXi 5.X</b> and <a href="https://habrahabr.ru/post/314778/">ESXi 6.X</a> ) -&gt; Application. <br><a name="habracut"></a><br>  There are a couple of basic documents you need to rely on when configuring VMware + NetApp: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="http://www.netapp.com/us/media/tr-4068.pdf">TR-4068: VMware vSphere 5 on NetApp Clustered Data ONTAP</a> <br>  <a href="http://netwell.ru/download/documents/techlibrary/16%2520-%2520%25D0%2598%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5%2520NFS%2520%25D0%25B2%2520VMware.pdf">TR 3839: Using NFS in VMware (7-Mode)</a> <br>  <a href="http://netwell.ru/download/documents/techlibrary/24%2520-%2520%25D0%25A0%25D1%2583%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25BE%25D0%25B4%25D1%2581%25D1%2582%25D0%25B2%25D0%25BE%2520%25D0%25BF%25D0%25BE%2520%25D0%25BD%25D0%25B0%25D0%25B8%25D0%25BB%25D1%2583%25D1%2587%25D1%2588%25D0%25B8%25D0%25BC%2520%25D1%2581%25D0%25BF%25D0%25BE%25D1%2581%25D0%25BE%25D0%25B1%25D0%25B0%25D0%25BC%2520%25D0%25B8%25D1%2581%25D0%25BF%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D1%258F%2520%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%2520NetApp%2520%2520%25D1%2581%2520VMware%2520vSphere.pdf">TR-3749: A Guide to Best Practices for Using NetApp Systems with VMware vSphere (7-Mode)</a> <br>  <a href="http://netwell.ru/download/documents/techlibrary/12%2520-%2520Ethernet%2520%25D0%25B4%25D0%25BB%25D1%258F%2520%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%2520%25D1%2585%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F-%2520%25D0%25BD%25D0%25B0%25D0%25B8%25D0%25BB%25D1%2583%25D1%2587%25D1%2588%25D0%25B8%25D0%25B5%2520%25D0%25BC%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4%25D1%258B..pdf">TR-3802: Ethernet for Storage: Best Practices (7-Mode)</a> <br><br><h4>  <a href="https://habr.com/ru/post/247833/">Hypervisor</a> </h4><a name="Hypervisor"></a><br>  You don‚Äôt have to give the guest <abbr title="Operating system">OS</abbr> all server resources; first, the hypervisor needs to leave at least 4GB of <abbr title="RAM">RAM</abbr> , and secondly, the opposite effect is sometimes observed when adding guest <abbr title="Operating system">OS</abbr> resources, this needs to be selected empirically. <br><br><h5>  <a href="https://habr.com/ru/post/247833/">Swap</a> </h5><a name="SWAP"></a><br>  I will carry out this section in a <a href="http://habrahabr.ru/post/247631/">separate post</a> . <br><br><h4>  <a href="https://habr.com/ru/post/247833/">Guest OS</a> </h4><a name="GOS"></a><br>  Tuning settings is needed for two purposes: <br><ul><li>  Optimization of guest <abbr title="Operating system">OS work</abbr> speed </li><li>  Normal work in <abbr title="High availability">HA</abbr> pair, with the failure of one controller (takeover) and the resumption of its work (takeover) </li></ul><br><br><h5>  <a href="https://habr.com/ru/post/247833/">Disk alignment</a> </h5><a name="alignment"></a><br>  To optimize performance, you may need <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1010881">to eliminate <i>disk misalignment</i></a> .  <i>Misalignment</i> can be obtained in two cases: <br><ol><li>  due to incorrectly chosen geometry of the moon when it was created in the <abbr title="Storage System">storage system</abbr> .  Such an error can be created only in the <abbr title="Storage Area Network">SAN</abbr> environment. </li><li>  inside virtual disks of virtual machines.  It can be both in <abbr title="Storage Area Network">SAN</abbr> and in <abbr title="Network area storage">NAS</abbr> environment </li></ol><br>  Let's look at these cases. <br>  First, consider fully aligned blocks on the <abbr title="Virtual Machine File System">VMFS</abbr> datastor and storage boundaries. <br><img src="https://habrastorage.org/files/fd2/1ca/a6c/fd21caa6c6c140f4b057cc71b35d5d98.png"><br><br>  The first case is when there is a <abbr title="Virtual Machine File System">VMFS</abbr> datastor <i>misalignment</i> regarding storage.  To eliminate the first type of problem, you must create a moon with the correct geometry and move the virtual machines there. <br><img src="https://habrastorage.org/files/0f3/119/65a/0f311965a6f043c18d01f67f60f4a109.png"><br><br>  The second situation, with displaced file system partitions within the guest <abbr title="Operating system">OS</abbr> with respect to the <abbr title="Write Anywhere File Layout">WAFL</abbr> file structure, can be obtained in older Linux distributions and Windows 2003 and older.  As a rule, this is due to the non-optimal allocation of the MBR partition table or to the machines that were converted from physical to virtual.  You can check this in <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1013644%26actp%3Dsearch%26viewlocale%3Den_US%26searchid%3D1419848156220"><abbr title="Operating system">the</abbr> Windows guest <abbr title="Operating system">OS</abbr></a> using the <i>dmdiag.exe -v</i> utility (the value of the Rel Sec field must be a multiple of 4KB per <abbr title="Write Anywhere File Layout">WAFL</abbr> ).  More on <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1010803%26actp%3Dsearch%26viewlocale%3Den_US%26searchid%3D1419848156220">diagnosing misalignment for windows</a> machines.  The location of the file system on the disk can also be verified using the <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1011402">mbralign</a> utility <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1011402">for the ESXi host</a> included in <a href="http://mysupport.netapp.com/NOW/download/software/sanhost_esx/ESX/">NetApp Host Utilities version 5.x</a> and <a href="https://habr.com/ru/post/247833/">VSC</a> .  For details on how to eliminate such situations are described in the <a href="http://media.netapp.com/documents/tr-3747.pdf">TR-3747 Best Practices for File Alignment in Virtual Environments</a> . <br><img src="https://habrastorage.org/files/301/a14/7a5/301a147a5c5a4c939a2bb0518d842db9.png"><br><br>  And of course, you can get <i>misalignment</i> on two levels at once: both at the <abbr title="Virtual Machine File System">VMFS</abbr> datastor level and at the guest <abbr title="Operating system">OS</abbr> file system level.  Learn more about <a href="http://habrahabr.ru/post/243045/">finding misalignment from the NetApp FAS repository</a> . <br><img src="https://habrastorage.org/files/f91/fb5/e5f/f91fb5e5f634490499a1f03d47bae443.png"><br><br>  Example for VMFS3 file system.  In the newly created VMFS5 (not an upgrade from VMFS3), the block is 1MB in size with 8KB sub-blocks. <br><br><h5>  <a href="https://habr.com/ru/post/247833/">takeover / giveback</a> </h5><a name="takeover-giveback"></a><br>  To complete the takeover / giveback in the <abbr title="High availability">HA</abbr> pair, you need to configure the correct guest <abbr title="Operating system">OS</abbr> timeouts: <br><table><tbody><tr><th>  OS </th><th>  OS Tuning for SAN: ESXi 3.x / 4.x and Data ONTAP 7.3 / 8.0 (SAN) </th><th>  Updated Guest OS Tuning for SAN: ESXi 5 and later, or Data ONTAP 8.1 and later (SAN) <br></th></tr><tr><td>  Windows </td><td>  disk timeout = 190 </td><td>  disk timeout = 60 </td></tr><tr><td>  Linux </td><td>  disk timeout = 190 </td><td>  disk timeout = 60 </td></tr><tr><td>  Solaris </td><td>  disk timeout = 190;  busy retry = 300;  not ready retry = 300;  reset retry = 30;  max.  throttle = 32;  min.  throttle = 8 </td><td>  disk timeout = 60;  busy retry = 300;  not ready retry = 300;  reset retry = 30;  max.  throttle = 32;  min.  throttle = 8;  corrected VID / PID specification </td></tr></tbody></table><br>  The <abbr title="Operating system">OS</abbr> default values ‚Äã‚Äãfor <abbr title="Network file system">NFS are</abbr> satisfactory, and the settings for the guest <abbr title="Operating system">OS</abbr> do not need to be changed. <br><br>  These values ‚Äã‚Äãare set manually or using scripts available in the <a href="https://habr.com/ru/post/247833/">VSC</a> . <br>  <b>Windows</b> : Set the value of the disk access delay 60 seconds using the registry (set in seconds, in hexadecimal form). <br><pre><code class="dos hljs">[HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\services\Disk] "TimeOutValue"=dword:<span class="hljs-number"><span class="hljs-number">0000003</span></span>c</code> </pre> <br>  <b>Linux</b> : Set the disk access delay value to 60 seconds by creating a udev rule (specified in seconds, in hexadecimal form). <br><pre> <code class="bash hljs">DRIVERS==<span class="hljs-string"><span class="hljs-string">"sd"</span></span>, SYSFS{TYPE}==<span class="hljs-string"><span class="hljs-string">"0|7|14"</span></span>, RUN+=<span class="hljs-string"><span class="hljs-string">"/bin/sh -c 'echo 60 &gt; /sys$</span><span class="hljs-variable"><span class="hljs-string"><span class="hljs-variable">$DEVPATH</span></span></span><span class="hljs-string">/timeout'"</span></span></code> </pre>  (Linux distributions may have a different location for setting udev rules).  VMware Tools for Linux guest <abbr title="Operating system">OS</abbr> automatically sets the udev rule with a delay value for the virtual disk equal to 180 seconds.  You can run the <i>grep command</i> for ‚ÄúVMware‚Äù vendor ID in the folder with udev rules to find the script that sets this value and change it if necessary.  Remember to check this value. <br><br>  <b>Solaris</b> : Set the value of 60 sec delay (specified in seconds, in hexadecimal form) for a disk in the <i>/ etc / system</i> file: <pre> <code class="bash hljs"><span class="hljs-built_in"><span class="hljs-built_in">set</span></span> sd:sd_io_time=0x3c</code> </pre> <br>  Additional settings can be made to the <i>/kernel/drv/sd.conf</i> file: <br>  Solaris 10.0 GA - Solaris 10u6: <br><pre> <code class="bash hljs">sd-config-list=<span class="hljs-string"><span class="hljs-string">"NETAPP LUN"</span></span>,<span class="hljs-string"><span class="hljs-string">"netapp-sd-config"</span></span>, <span class="hljs-string"><span class="hljs-string">"VMware Virtual"</span></span>,<span class="hljs-string"><span class="hljs-string">"netapp-sd-config"</span></span>; netapp-sd-config=1,0x9c01,32,0,0,0,0,0,0,0,0,0,300,300,30,0,0,8,0,0;</code> </pre><br>  Solaris 10u7 and newer and Solaris 11: <br><pre> <code class="bash hljs">sd-config-list= <span class="hljs-string"><span class="hljs-string">"NETAPP LUN"</span></span>,<span class="hljs-string"><span class="hljs-string">"physical-block-size:4096,retries-busy:300,retries-timeout:16,retries-notready:300,retries-reset:30,throttle-max:32,throttle-min:8"</span></span>, <span class="hljs-string"><span class="hljs-string">"VMware Virtual"</span></span>,<span class="hljs-string"><span class="hljs-string">"physical-block-size:4096,retries-busy:300,retries-timeout:16,retries-notready:300,retries-reset:30,throttle-max:32,throttle-min:8"</span></span>;</code> </pre><br>  Please note: there are two spaces between vendor ID NETAPP and ID LUN, as well as between the words "VMware" and "Virtual" in the config above. <br><br><h4>  <a href="https://habr.com/ru/post/247833/">FC / FCoE Switch Zoning Settings</a> </h4><a name="zoning"></a><br>  Learn more about <a href="http://habrahabr.ru/post/260107/">zoning</a> recommendations <a href="http://habrahabr.ru/post/260107/">for NetApp in pictures</a> . <br><br><h5>  <a href="https://habr.com/ru/post/247833/">ALUA</a> </h5><a name="ALUA"></a><br>  For NetApp <abbr title="Fabric attached storage">FAS</abbr> systems with 7-Mode, <abbr title="Asymmetric Logical Unit Access">ALUA is</abbr> recommended for <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> .  And for Systems NetApp <abbr title="Fabric attached storage">FAS</abbr> with <abbr title="Clustered Data ONTAP">cDOT</abbr> <abbr title="Asymmetric Logical Unit Access">ALUA is</abbr> recommended for use for all block protocols: <abbr title="Internet Small Computer System Interface">iSCSI</abbr> / <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> . <br>  ESXi will determine if <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> is <abbr title="Asymmetric Logical Unit Access">enabled</abbr> .  If <abbr title="Asymmetric Logical Unit Access">ALUA is</abbr> enabled, the <i>Storage Array Type plug-in</i> will be <i>VMW_SATP_ALUA</i> , if <abbr title="Asymmetric Logical Unit Access">ALUA is</abbr> disabled, it is recommended to use the <i>Fixed</i> + path balancing policy you must manually specify the optimal / preferred paths.  If <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> is used, the algorithm <i>Most Recently Used</i> or <i>Round Robin</i> is allowed to use - any. <br><img src="https://habrastorage.org/files/909/b95/264/909b952646e847279f7771e65a555492.png"><br><br>  <i>Round Robin</i> will be more productive if there are more than one path to the controller.  In the case of using Microsoft Cluster + <abbr title="RAW Device Mapping">RDM</abbr> disks, the <i>Most Recently Used</i> balancing mechanism is recommended. <br><br>  Below is a table of recommended load balancing settings.  Learn more about <a href="http://habrahabr.ru/post/243045/">NetApp FAS, ALUA logic and load balancing for block protocols</a> . <br><table><tbody><tr><th>  Mode </th><th>  <abbr title="Asymmetric Logical Unit Access">ALUA</abbr> </th><th>  Protocol </th><th>  ESXi Policy <br></th><th>  ESXi Path Balancing <br></th></tr><tr><td>  7-Mode 7.x / 8.x </td><td>  Enabled </td><td>  <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> </td><td>  VMW_SATP_ALUA </td><td>  Most Recently Used or Round Robin <br></td></tr><tr><td>  7-Mode 7.x / 8.x </td><td>  Disabled </td><td>  <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> </td><td>  AA SATP </td><td>  Fixed PSP (choose optimal paths) <br></td></tr><tr><td>  7-Mode 7.x / 8.x </td><td>  Disabled </td><td>  <abbr title="Internet Small Computer System Interface">iSCSI</abbr> </td><td>  AA SATP </td><td>  Round Robin PSP </td></tr><tr><td>  cDOT 8.x </td><td>  Enabled </td><td>  <abbr title="Fiber channel">FC</abbr> / <abbr title="Fiber channel over ethernet">FCoE</abbr> / <abbr title="Internet Small Computer System Interface">iSCSI</abbr> </td><td>  VMW_SATP_ALUA </td><td>  Most Recently Used or Round Robin </td></tr></tbody></table><br><br><div class="spoiler">  <b class="spoiler_title">Check the applicable policy to the checked moon / datastore</b> <div class="spoiler_text"><pre> <code class="bash hljs">~ <span class="hljs-comment"><span class="hljs-comment"># esxcli storage nmp device list naa.60a980004434766d452445797451376b Device Display Name: NETAPP Fibre Channel Disk (naa.60a980004434766d452445797451376b) Storage Array Type: VMW_SATP_ALUA Storage Array Type Device Config: {implicit_support=on;explicit_support=off; explicit_allow=on;alua_followover=on;{TPG_id=1,TPG_state=ANO}{TPG_id=0,TPG_state=AO}} Path Selection Policy: VMW_PSP_RR Path Selection Policy Device Config: {policy=rr,iops=1000,bytes=10485760,useANO=0; lastPathIndex=0: NumIOsPending=0,numBytesPending=0} Path Selection Policy Device Custom Config: Working Paths: vmhba2:C0:T6:L119, vmhba1:C0:T7:L119 Is Local SAS Device: false Is USB: false Is Boot USB Device: false</span></span></code> </pre><br></div></div><br><br>  Make sure that the SATP policy applied to your moon has the reset_on_attempted_reserve option enabled: <br><div class="spoiler">  <b class="spoiler_title">SATP policy</b> <div class="spoiler_text"><pre> <code class="bash hljs">~ <span class="hljs-comment"><span class="hljs-comment"># esxcli storage nmp satp rule list Name Device Vendor Model Driver Transport Options Rule Group Claim Options Default PSP PSP Options Description ------------------- ------ ------- ---------------- ------ --------- -------------------------- ---------- ----------------------------------- ----------- ----------- -------------------------------------------------------------------------- VMW_SATP_ALUA LSI INF-01-00 reset_on_attempted_reserve system tpgs_on VMW_PSP_MRU NetApp E-Series arrays with ALUA support VMW_SATP_ALUA NETAPP reset_on_attempted_reserve system tpgs_on VMW_PSP_RR NetApp arrays with ALUA support</span></span></code> </pre><br></div></div><br><br><h4>  <a href="https://habr.com/ru/post/247833/">ESXi host settings</a> </h4><a name="ESXI_configurations"></a><br>  For an ESXi host to work optimally, it is necessary to install the recommended options for it. <br><table><tbody><tr><th>  Parameter </th><th>  Protocol (s) </th><th>  ESXi 4.x with DataONTAP 8.1.x </th><th>  ESXi 5.x with DataONTAP 7.3 / 8.x </th></tr><tr><td>  Net.TcpipHeapSize </td><td>  iSCSI / NFS </td><td>  thirty </td><td>  32 </td></tr><tr><td>  Net.TcpipHeapMax </td><td>  iSCSI / NFS </td><td>  120 </td><td>  512 (For vSphere 5.0 / 5.1 set 128) </td></tr><tr><td>  NFS.MaxVolumes </td><td>  Nfs </td><td>  64 </td><td>  256 </td></tr><tr><td>  NFS41.MaxVolumes </td><td>  NFS 4.1 </td><td colspan="2">  - </td></tr><tr><td>  NFS.HeartbeatMaxFailures </td><td>  Nfs </td><td colspan="2">  ten </td></tr><tr><td>  NFS.HeartbeatFrequency </td><td>  Nfs </td><td colspan="2">  12 </td></tr><tr><td>  NFS.HeartbeatTimeout </td><td>  Nfs </td><td colspan="2">  five </td></tr><tr><td>  NFS.MaxQueueDepth </td><td>  Nfs </td><td>  - </td><td>  64 </td></tr><tr><td>  Disk.QFullSampleSize </td><td>  iSCSI / FC / FCoE </td><td colspan="2">  32 (for <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1013944">5.1 is configured on each LUNe</a> ) </td></tr><tr><td>  Disk.QFullThreshold </td><td>  iSCSI / FC / FCoE </td><td colspan="2">  8 (for <a href="https://kb.netapp.com/support/index%3Fpage%3Dcontent%26id%3D1013944">5.1 is configured on each LUNe</a> ) </td></tr></tbody></table><br>  There are several ways to do this: <br><br><ul><li>  Using the Command Line Interface (CLI) on ESXi 5.x hosts. </li><li>  Using vSphere Client / vCenter Server. </li><li>  Using the Remote CLI tool from VMware. </li><li>  Using the VMware Management Appliance (VMA). </li><li>  By adopting the Host Profile unwinding it from an already configured ESXi 5.x to other hosts. </li></ul><br><div class="spoiler">  <b class="spoiler_title">An example of setting advanced parameters from ESX 4.x CLI</b> <div class="spoiler_text">  The esxcfg-advcfg utility used in these examples is located in the / usr / sbin folder for the ESXi host. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -s 30 /Net/TcpipHeapSize #esxcfg-advcfg -s 120 /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -s 64 /NFS/MaxVolumes #esxcfg-advcfg -s 10 /NFS/HeartbeatMaxFailures #esxcfg-advcfg -s 12 /NFS/HeartbeatFrequency #esxcfg-advcfg -s 5 /NFS/HeartbeatTimeout #  iSCSI/FC/FCoE #esxcfg-advcfg -s 32 /Disk/QFullSampleSize #esxcfg-advcfg -s 8 /Disk/QFullThreshold</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Checking advanced settings from ESX 4.x CLI</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -g /Net/TcpipHeapSize #esxcfg-advcfg -g /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -g /NFS/MaxVolumes #esxcfg-advcfg -g /NFS/HeartbeatMaxFailures #esxcfg-advcfg -g /NFS/HeartbeatFrequency #esxcfg-advcfg -g /NFS/HeartbeatTimeout #  iSCSI/FC/FCoE #esxcfg-advcfg -g /Disk/QFullSampleSize #esxcfg-advcfg -g /Disk/QFullThreshold</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">An example of setting advanced parameters from ESX 5.x CLI</b> <div class="spoiler_text">  The esxcfg-advcfg utility used in these examples is located in the / usr / sbin folder for the ESXi host. <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -s 32 /Net/TcpipHeapSize #For vSphere 5.0/5.1: #esxcfg-advcfg -s 128 /Net/TcpipHeapMax #For vSphere 5.5: #esxcfg-advcfg -s 512 /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -s 256 /NFS/MaxVolumes #esxcfg-advcfg -s 10 /NFS/HeartbeatMaxFailures #esxcfg-advcfg -s 12 /NFS/HeartbeatFrequency #esxcfg-advcfg -s 5 /NFS/HeartbeatTimeout #esxcfg-advcfg -s 64 /NFS/MaxQueueDepth #  iSCSI/FC/FCoE #esxcfg-advcfg -s 32 /Disk/QFullSampleSize #esxcfg-advcfg -s 8 /Disk/QFullThreshold</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Checking advanced settings from ESX 5.x CLI</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -g /Net/TcpipHeapSize #esxcfg-advcfg -g /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -g /NFS/MaxVolumes #esxcfg-advcfg -g /NFS/HeartbeatMaxFailures #esxcfg-advcfg -g /NFS/HeartbeatFrequency #esxcfg-advcfg -g /NFS/HeartbeatTimeout #esxcfg-advcfg -g /NFS/MaxQueueDepth #  iSCSI/FC/FCoE #esxcfg-advcfg -g /Disk/QFullSampleSize #esxcfg-advcfg -g /Disk/QFullThreshold</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">An example of setting advanced parameters from ESX 5.1 CLI</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -s 32 /Net/TcpipHeapSize #esxcfg-advcfg -s 128 /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -s 256 /NFS/MaxVolumes #esxcfg-advcfg -s 10 /NFS/HeartbeatMaxFailures #esxcfg-advcfg -s 12 /NFS/HeartbeatFrequency #esxcfg-advcfg -s 5 /NFS/HeartbeatTimeout #esxcfg-advcfg -s 64 /NFS/MaxQueueDepth #  iSCSI/FC/FCoE # esxcli storage core device set --device naa.60a98000486e5334524a6c4f63624558 --queue-full-sample-size 32 --queue-full-threshold 8</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Checking advanced settings from ESX 5.1 CLI</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#  iSCSI/NFS #esxcfg-advcfg -g /Net/TcpipHeapSize #esxcfg-advcfg -g /Net/TcpipHeapMax #  NFS #esxcfg-advcfg -g /NFS/MaxVolumes #esxcfg-advcfg -g /NFS/HeartbeatMaxFailures #esxcfg-advcfg -g /NFS/HeartbeatFrequency #esxcfg-advcfg -g /NFS/HeartbeatTimeout #esxcfg-advcfg -g /NFS/MaxQueueDepth #  iSCSI/FC/FCoE # esxcli storage core device list</span></span></code> </pre><br></div></div><br><h5>  <a href="https://habr.com/ru/post/247833/">HBA</a> </h5><a name="HBA"></a><br>  NetApp typically recommends using "default values" for <abbr title="Host bus adapter">HBAs</abbr> set by the adapter manufacturer for <abbr title="Fabric attached storage">FAS</abbr> systems with an ESXi host.  If they have been changed, you must return them to the factory settings.  Check out relevant best practices.  For example, if we are talking about DB2 virtualization in a VMware environment on NetApp, then it is recommended ( <a href="http://www.netapp.com/us/media/wp-7109.pdf">see page 21</a> ) to increase the queue length to 64 on ESXi (as written in <a href="http://kb.vmware.com/selfservice/microsites/search.do%3Flanguage%3Den_US%26cmd%3DdisplayKC%26externalId%3D1267">Vmware KB 1267</a> ). <br><div class="spoiler">  <b class="spoiler_title">Qlogic HBA setup example on ESXi</b> <div class="spoiler_text"><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#    Qlogic  ESXi 5.5  6.0 # esxcli system module list | grep qln #    Qlogic  ESXi 5.1  5.0 # esxcli system module list | grep qla #    Qlogic  ESXi 5.5  6.0 # esxcli system module parameters set -p qlfxmaxqdepth=64 -m qlnativefc #    Qlogic  ESXi 5.1  5.0 # esxcli system module parameters set -p ql2xmaxqdepth=64 -m qla2xxx</span></span></code> </pre><br></div></div><br><br><h5>  <a href="https://habr.com/ru/post/247833/">VSC</a> </h5><h5><a name="VSC"></a><br>  The NetApp <abbr title="Virtual Storage Console">VSC</abbr> plugin (is free <abbr title="Software">software</abbr> ) sets the recommended settings on the ESXi host and <abbr title="Host bus adapter">HBA</abbr> adapter: queue, delay, and others.  The plugin itself integrates into vCenter.  Saves time and eliminates the human factor during the test when configuring parameters on an ESXi host to work more effectively with NetApp.  Allows you to perform basic operations to manage storage from vCenter, necessary for the administrator of virtualized environments.  Access rights to the repository using <abbr title="Virtual Storage Console">VSC</abbr> can be flexibly configured for multiple users using <abbr title="Role-based Access Control">RBAC</abbr> . <br><img src="https://habrastorage.org/getpro/habr/post_images/77b/964/cc9/77b964cc93e4687bcd8cf8aa1d0fea38.jpg" alt="image"><br><br>  A version is available for both the ‚Äúfat‚Äù (old) client and the new web client. <br><img src="https://habrastorage.org/files/c3c/167/000/c3c1670007284425a8ce2b528d7a052a.png"><br><br></h5><h4>  <a href="https://habr.com/ru/post/247833/">Ethernet</a> </h4><a name="Ethernet"></a><br><h5>  <a href="https://habr.com/ru/post/247833/">Jumbo frames</a> </h5><a name="Jumbo_frames"></a><br>  If <abbr title="Internet Small Computer System Interface">iSCSI</abbr> is used, it is highly recommended to use Jumbo Frames on Ethernet with a speed higher than or equal to 1Gb.  Read more in the article about <a href="http://habrahabr.ru/post/243119/">Ethernet with NetApp FAS</a> . <br><br><h5>  <a href="https://habr.com/ru/post/247833/">ESXi &amp; MTU9000</a> </h5><h5><a name="ESXi_and_MTU9000"></a><br>  Remember to create the right network adapter - VMware recommends using VMXNEE3.  Starting with ESXi 5.0, VMXNET3 supports Jumbo Frames.  The E1000e network adapter supports speed of 1GB networks and MTU 9000 - it is installed for all created VMs by default (except Linux).  Standard virtual network adapter type "Flexible" supports MTU 1500. <a href="http://kb.vmware.com/selfservice/microsites/search.do%3Flanguage%3Den_US%26cmd%3DdisplayKC%26externalId%3D1001805">More.</a> <br><img src="https://habrastorage.org/files/191/315/538/191315538e34466890b04cde344e1194.png"><br><br>  Also, do not forget that the <i>port group</i> installed for the virtual network adapter of your virtual machine must be connected to a virtual switch with the MTU 9000 setting set for the entire switch. <br><img src="https://habrastorage.org/files/f13/575/a43/f13575a43a2941d48af0e860f79e0633.png"><br><br></h5><h4>  <a href="https://habr.com/ru/post/247833/">NAS and VAAI</a> </h4><a name="NAS"></a><br>  NetApp <abbr title="Fabric attached storage">FAS</abbr> systems support VMware <abbr title="vStorage APIs for Array Integration">VAAI</abbr> primitives by downloading some of the routine data management tasks on a datastore from host to storage, where it is more logical to do this.  In a <abbr title="Storage Area Network">SAN</abbr> environment with ESXi 4.1+ and higher with NetApp <abbr title="Fabric attached storage">FAS</abbr> Data ONTAP 8.0 and above, <abbr title="vStorage APIs for Array Integration">VAAI is</abbr> automatically supported and does not require any manipulations.  For the <abbr title="Network area storage">NAS</abbr> environment, NetApp has released a plugin that allows you to perform similar optimization for the <abbr title="Network file system">NFS</abbr> protocol.  This requires the installation of a <i>NetAppNFSVAAI</i> kernel <i>module</i> for each ESXi host.  <a href="https://habr.com/ru/post/247833/">VSC</a> can install the <abbr title="Network file system">NFS</abbr> <abbr title="vStorage APIs for Array Integration">VAAI</abbr> plugin automatically from vCenter. <br><br><h5>  <a href="https://habr.com/ru/post/247833/">VASA</a> </h5><a name="VASA"></a><br>  <abbr title="VMware APIs for Storage Awareness">VASA</abbr> is free <abbr title="Software">software</abbr> that allows vCenter through the <abbr title="Application program interface">API</abbr> to learn about the storage capabilities and more intelligently use them.  <abbr title="VMware APIs for Storage Awareness">VASA</abbr> integrates into <abbr title="Virtual Storage Console">VSC</abbr> and allows you to create datastore profiles with specific storage capabilities via a <abbr title="Graphical User Interface">GUI</abbr> interface (for example, presence / absence of Thing Provitioning, disk type: <abbr title="Serial Attached SCSI">SAS</abbr> / <abbr title="Serial ATA">SATA</abbr> / <abbr title="Solid state drive">SSD</abbr> , availability of a second-level cache, etc.) and include notifications on reaching level (for example, occupancy or load).  Starting from version 6.0, <abbr title="VMware APIs for Storage Awareness">VASA</abbr> is a mandatory component of <abbr title="Virtual Storage Console">VSC</abbr> and is an important part of the VMware 6 <abbr title="Virtual volume">vVOL paradigm</abbr> . <br><br><h6>  Space Reservation - UNMAP </h6><br>  Starting with ESXi 5.1, the release of released blocks from a thin moon (datastor) is supported.  This is configured by default on ESXi 5.1, disabled by default on all other ESXi versions 5.X &amp; 6.X (requires manual start), for ESXi 6.X it works automatically with vVOL, and on the ONTAP side this functionality is always the default is off, to <a href="https://habrahabr.ru/post/271959/">enable it, you need to run several uncomplicated commands on the storage</a> . <br><br><h4>  <a href="http://habrahabr.ru/post/154205/">Compatibility</a> </h4><a name="compatibility"></a><br>  <a href="http://habrahabr.ru/post/154205/">Make</a> extensive <a href="http://habrahabr.ru/post/154205/">use of the compatibility matrix</a> in your practice to reduce potential problems in the <abbr title="Data processing center">data center</abbr> infrastructure.  For troubleshooting, contact <abbr title="Knowledge base">KB</abbr> <a href="https://kb.netapp.com/">NetApp</a> and <a href="http://kb.vmware.com/">VMware</a> . <br><br>  I am sure that over time I will have something to add to this article on optimizing the ESXi host, so look here from time to time. <br><br>  <b>I ask to send messages on errors in the text to the <abbr title="Private message">LAN</abbr> .</b> <br>  <b>Notes and additions on the contrary please in the comments</b> </div><p>Source: <a href="https://habr.com/ru/post/247833/">https://habr.com/ru/post/247833/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../247823/index.html">Free CCNA Cisco video course available on one of the YouTube channels</a></li>
<li><a href="../247825/index.html">Google opened domain registration for US residents</a></li>
<li><a href="../247827/index.html">How to make e-mail newsletter 5 times easier and 5 times faster</a></li>
<li><a href="../247829/index.html">Consulo: Code Coverage, Unity3D and other changes</a></li>
<li><a href="../247831/index.html">Kevin Hale: the subtleties of working with user experience (part 1)</a></li>
<li><a href="../247837/index.html">It is finished! io.js Version 1.0.1 (Beta stability) released!</a></li>
<li><a href="../247839/index.html">Intel RealSense Hands-On Lab Workshop. From idea to project - 6 hours</a></li>
<li><a href="../247841/index.html">The App Store breaks records, Rovio experiments with genres, 9GAG takes on games - and other news of the week for a mobile developer</a></li>
<li><a href="../247843/index.html">Dictionary implementation in Python 2.7</a></li>
<li><a href="../247847/index.html">Simple site parsing with SlimerJS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>