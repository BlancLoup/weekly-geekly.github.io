<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Solving Hola Javascript Challenge with LSTM</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Inspired by the recent Hola Javascript Challenge . We will not package the algorithm in 64kb, but we will get accurate accuracy. 

 This implies that ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Solving Hola Javascript Challenge with LSTM</h1><div class="post__text post__text-html js-mediator-article">  Inspired by the recent <a href="https://github.com/hola/challenge_word_classifier">Hola Javascript Challenge</a> .  We will not package the algorithm in 64kb, but we will get accurate accuracy. <br><a name="habracut"></a><br>  This implies that the reader has a general idea of ‚Äã‚Äãhow the LSTM works.  If not, then you should read <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">this</a> wonderful tutorial. <br><br>  The task of chellendzh is to develop an algorithm that fits with all the accompanying 64kb data that can distinguish a word belonging to this dictionary from randomly generated garbage sequences. <br><br>  It seemed to me that such data is a wonderful playground for deep learning, especially for studying recurrent neural networks.  Let us treat words as abstract sequences of characters and try to force the neural network to extract some internal rules from them that define them.  We will not try to add our model to 64 kilobytes, but we will get significantly more interesting accuracy - 0.92+. <br>  Let's look at the source data in a little more detail: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <a href="https://cdn.rawgit.com/hola/challenge_word_classifier/1945d565fd929118c88fbceadfe4818d138e3b5f/words.txt">Vocabulary</a> <br>  <a href="https://hola.org/challenges/word_classifier/testcase/1">Test case example</a> <br><br>  Let us reformulate our problem as follows: we solve the problem of binary classification of sequences, and all sequences of the class true will be included in the training set.  Yes, the data in the test set may intersect with the training, but only for positive examples.  Negative examples will be unique. <br><br><h3>  Data processing </h3><br>  The first stage is data processing.  With positive examples, everything is simple - all the ‚Äúgood‚Äù words are in the dictionary.  It is only necessary to collect a lot of negative examples.  To build them, you can use hola test cases, you can use their own generator (you can find it <a href="https://github.com/hola/challenge_word_classifier/tree/master/tests">here</a> ), or write your own.  I collected negative examples for learning by contacting the API. <br><br>  Before compiling a dataset for learning, let's first study the vocabulary of good examples for a bit and filter out the ‚Äúextra‚Äù.  The main thing that interests us is the maximum length of the sequence, I want to minimize it (within reasonable limits, of course). <br><br>  Construct the distribution of the number of words in length: <br><br><img src="http://i.imgur.com/fl3m0QD.png?2" alt="image"><br><br>  Yes, the vast majority of good words are shorter than 21 characters. <br><br>  The longest word in dataset is shocking: Llanfairpwllgwyngyllgogerychwyrndrobwllllantysiliogogogoch's (try to read it out loud the first time!).  I don‚Äôt think that we will lose much, if we refuse to consider such cadavers.  We cut good examples to a maximum length of 22 characters.  Negative examples then, naturally, we will also consider only the same length. <br><br><h3>  Filtered data </h3><br>  <a href="https://drive.google.com/open%3Fid%3D0B6vZMotwvIlqaWdSN1Q0NWpjbG8">Good words</a> - 600+ words, maximum length - 22 <br>  <a href="https://drive.google.com/open%3Fid%3D0B6vZMotwvIlqdmo0bnZhc1c0d00">Bad words</a> - 3 + kk <u>unique</u> words, maximum length - 22. <br><br>  Bad words collected from the stock - a little more than 3 million unique sequences - this is really quite a lot. <br><br><h3>  Learning neural network </h3><br>  Next, the Bidirectional LSTM implementation on <a href="http://keras.io/">Keras</a> will be used with <a href="https://www.tensorflow.org/">TensorFlow</a> as the backend. <br><br>  The evolution of the network architecture during the experiments looked like this: <br><br>  <b>First ass</b> .  The simplest, basic LSTM.  Input sequences are embedded in batch_size * MAX_LENGTH * hidden_neurons tensors and passed through LSTM cells.  The output of LSTM is collected in a dense layer on 2 outputs, one for each class, the final class is calculated accordingly.  We train on the whole dictionary of good words, and on twice the number of bad ones (this proportion allows to correct the errors of the first / second kind). <br><br><pre><code class="python hljs">model = Sequential() model.add(Embedding(len(char_list), <span class="hljs-number"><span class="hljs-number">500</span></span>, input_length=MAX_LENGTH, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">64</span></span>, init=<span class="hljs-string"><span class="hljs-string">'glorot_uniform'</span></span>, inner_init=<span class="hljs-string"><span class="hljs-string">'orthogonal'</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, inner_activation=<span class="hljs-string"><span class="hljs-string">'hard_sigmoid'</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)) model.add(Dense(len(labels_list))) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre> <br>  This approach allows to achieve approximately 0.80 accuracy in each of the classes.  0.80 - quite a bit, from such a complex model, we would like to pull more. <br><br>  <b>The second attempt</b> .  Let's try to increase accuracy by changing the learning rate in the learning process.  Keras has a convenient callback mechanism that works well for this.  We will be very clumsy to chop lr 10 times, if at the end of an era the network does not begin to show the results better on the validation set.  Possible implementations of this method are discussed <a href="https://github.com/fchollet/keras/issues/898">here</a> .  The code offered by jiumem almost works.  The results of its minor modification for correct work with TensorFlow under the spoiler: <br><br><div class="spoiler">  <b class="spoiler_title">LR Annealing Callback for Keras + TF</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">LrReducer</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(Callback)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, patience=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0</span></span></span></span><span class="hljs-function"><span class="hljs-params">, reduce_rate=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">0.1</span></span></span></span><span class="hljs-function"><span class="hljs-params">, reduce_nb=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">, verbose=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> super(Callback, self).__init__() self.patience = patience self.wait = <span class="hljs-number"><span class="hljs-number">0</span></span> self.best_score = float(<span class="hljs-string"><span class="hljs-string">"inf"</span></span>) self.reduce_rate = reduce_rate self.current_reduce_nb = <span class="hljs-number"><span class="hljs-number">0</span></span> self.reduce_nb = reduce_nb self.verbose = verbose <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">on_epoch_end</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, epoch, logs={})</span></span></span><span class="hljs-function">:</span></span> current_score = logs[<span class="hljs-string"><span class="hljs-string">'val_loss'</span></span>] print(<span class="hljs-string"><span class="hljs-string">"cur score:"</span></span>, current_score, <span class="hljs-string"><span class="hljs-string">"current best:"</span></span>, self.best_score) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> current_score &lt;= self.best_score: self.best_score = current_score self.wait = <span class="hljs-number"><span class="hljs-number">0</span></span> self.current_reduce_nb = <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.verbose &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">'---current best val accuracy: %.3f'</span></span> % current_score) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.wait &gt;= self.patience: self.current_reduce_nb += <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.current_reduce_nb &lt;= self.reduce_nb: lr = self.model.optimizer.lr.initialized_value() self.model.optimizer.lr.assign(lr*self.reduce_rate) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self.verbose &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>: print(<span class="hljs-string"><span class="hljs-string">"Epoch %d: early stopping"</span></span> % (epoch)) self.model.stop_training = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> self.wait += <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre><br></div></div><br>  The use of this callback with the architecture proposed earlier allows to raise the classification accuracy to 0.83.  Not impressive. <br><br>  <b>The third attempt, the final</b> Stumbled upon the mention of an interesting architecture - Bidirectional LSTM.  In fact, we go around the sequence two times by two networks, one reads from right to left, the other - vice versa.  Similar constructions have been used successfully in the field of speech recognition.  Let's build a network of the same size, 500 cells, but bidirectional. <br><br><div class="spoiler">  <b class="spoiler_title">Bidirectional LSTM in Keras</b> <div class="spoiler_text"><pre> <code class="python hljs"> hidden_units = <span class="hljs-number"><span class="hljs-number">500</span></span> left = Sequential() left.add(Embedding(len(char_list), hidden_units, input_length=MAX_LENGTH, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) left.add(LSTM(output_dim=hidden_units, init=<span class="hljs-string"><span class="hljs-string">'uniform'</span></span>, inner_init=<span class="hljs-string"><span class="hljs-string">'uniform'</span></span>, forget_bias_init=<span class="hljs-string"><span class="hljs-string">'one'</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, inner_activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) right = Sequential() right.add(Embedding(len(char_list), hidden_units, input_length=MAX_LENGTH, mask_zero=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) right.add(LSTM(output_dim=hidden_units, init=<span class="hljs-string"><span class="hljs-string">'uniform'</span></span>, inner_init=<span class="hljs-string"><span class="hljs-string">'uniform'</span></span>, forget_bias_init=<span class="hljs-string"><span class="hljs-string">'one'</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'tanh'</span></span>, inner_activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>, go_backwards=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)) model = Sequential() model.add(Merge([left, right], mode=<span class="hljs-string"><span class="hljs-string">'sum'</span></span>)) model.add(Dense(len(labels_list))) model.add(Activation(<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>))</code> </pre><br></div></div><br>  We will also use lr annealing, on the same data. <br><br>  This approach made it possible to stably obtain an accuracy of 0.91‚Äì0.93 on validation data composed of randomly selected 200k words, half of which are positive examples from the source dictionary, and the rest are unique, randomly selected negative examples that were not encountered in the training set.  This result is already quite good, and so far it has not been possible to improve it steadily, without increasing the learning time. <br><br>  You can try to incite a trained model on all the data collected (yes, this is not entirely correct, since the model could see most of the negative examples during training).  Let's do this and build a beautiful confusion matrix: <br><br><img src="http://i.imgur.com/GRZ1y06.png?1" alt="image"><br><br><h3>  Studying time </h3><br>  A small lyrical digression: everything was considered on the GTX960.  The last network begins to converge in the 13th era, one era is considered to be almost 50 minutes. <br><br><h3>  Conclusion </h3><br>  Received accuracy above 90% in a fairly reasonable time.  Yes, such a model cannot be put into 64 kilobytes, but, nevertheless, this is quite an interesting demonstration of the ability of LSTM to capture the internal structure in sequences, or even its absence. </div><p>Source: <a href="https://habr.com/ru/post/303914/">https://habr.com/ru/post/303914/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../303904/index.html">Apei Gaming - to be a project! (Part 2)</a></li>
<li><a href="../303906/index.html">Verification of documents: what is missing office applications "for complete happiness"</a></li>
<li><a href="../303908/index.html">See the reports of the virtual forum ‚ÄúData. Technology. SQL Server 2016 ¬ª</a></li>
<li><a href="../303910/index.html">83% of the latest web browser features are completely unnecessary.</a></li>
<li><a href="../303912/index.html">Creating a library search by a young programmer - what is it?</a></li>
<li><a href="../303916/index.html">How to build perfect graphics in vROps</a></li>
<li><a href="../303918/index.html">Survey: how strictly do you follow the standards and best practices in the frontend?</a></li>
<li><a href="../303922/index.html">1C.Drop.1 uses 1C to execute malicious code</a></li>
<li><a href="../303924/index.html">Azure Service Fabric: First Steps</a></li>
<li><a href="../303926/index.html">The structure of the Android project - an alternative way</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>