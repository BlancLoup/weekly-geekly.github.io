<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Benchmark of Google's new tensor processor for deep learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Each Cloud TPU device consists of four ‚ÄúTPUv2 chips‚Äù. The chip has 16 GB of memory and two cores, each core with two units for multiplying matrices. T...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Benchmark of Google's new tensor processor for deep learning</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/sk/g7/tn/skg7tnocn-2djcjjf46ntcpzele.jpeg"><br>  <i><font color="gray">Each Cloud TPU device consists of four ‚ÄúTPUv2 chips‚Äù.</font></i>  <i><font color="gray">The chip has 16 GB of memory and two cores, each core with two units for multiplying matrices.</font></i>  <i><font color="gray">Together, the two cores issue 45 TFLOPS, a total of 180 TFLOPS and 64 GB of memory per TPU</font></i> <br><br>  Most of us provide in-depth training on the Nvidia GPU.  Currently there are almost no alternatives.  Google Tensor Processor (Tensor Processing Unit, TPU) is a specially designed chip for in-depth training that should make a difference. <br><br>  Nine months after the initial announcement two weeks ago, Google finally <a href="https://cloudplatform.googleblog.com/2018/02/Cloud-TPU-machine-learning-accelerators-now-available-in-beta.html">released TPUv2</a> and opened access to the first beta testers on the Google Cloud platform.  At <a href="https://riseml.com/">RiseML, we</a> took the opportunity and drove a couple of quick benchmarks.  We want to share our experience and preliminary results. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      For a long time we have been waiting for the emergence of competition in the market for equipment for in-depth training.  It must break the monopoly of Nvidia and determine what the future depth learning infrastructure will look like. <br><a name="habracut"></a><br>  Keep in mind that TPU is still in the early beta, as Google clearly and universally reminds - so some of the ratings discussed may change in the future. <br><br><h1>  TPU in the google cloud </h1><br>  While the first generation of TPUv1 chips focused on speeding up data output, the current second generation is primarily focused on speeding up training.  At the heart of TPUv2 is the <a href="https://en.wikipedia.org/wiki/Systolic_array">systolic array</a> responsible for the multiplication of matrices that are actively used in depth learning.  According to Jeff Dean's <a href="http://learningsys.org/nips17/assets/slides/dean-nips17.pdf">slides</a> , each Cloud TPU device consists of four ‚ÄúTPUv2 chips‚Äù.  The chip has 16 GB of memory and two cores, each core with two units for multiplying matrices.  Together, the two cores issue 45 TFLOPS, a total of 180 TFLOPS and 64 GB of memory per TPU.  For comparison, the current generation Nvidia V100 has only 125 TFLOPS and 16 GB of memory. <br><br>  To use tensor processors on the Google Cloud platform, you need to run Cloud TPU (after receiving a quota for it).  It is not necessary (and possible) to assign a Cloud TPU to a specific virtual machine instance.  Instead, the TPU from the instance is accessed over the network.  Each Cloud TPU is assigned a name and an IP address, which should be indicated in the TensorFlow code. <br><br> <a href=""><img src="https://habrastorage.org/webt/9b/ah/he/9bahhe3y3hdi5unu0u2nhdq4dau.png"></a> <br>  <i>Creating a new Cloud TPU.</i>  <i>Please note that he has an IP address.</i>  <i><a href="">Animated gif</a></i> <br><br>  TPUs are supported only in TensorFlow version 1.6, which is still in release candidate status.  In addition, no drivers are needed for the VM, since all the necessary code is included in TensorFlow.  The code for execution on TPU is optimized and compiled by the JIT compiler <a href="https://www.tensorflow.org/performance/xla/">XLA</a> , also included in TensorFlow. <br><br>  To use TPU effectively, the code must be based on high-level abstractions of the <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a> class.  Then go to the class <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/tpu/TPUEstimator">TPUEstimator</a> , which performs many of the necessary tasks for the effective use of TPU.  For example, configures data queues for the TPU and parallelizes the calculations between the cores.  There is definitely a way to do without using the TPUEstimator, but we are not aware of such examples or documentation yet. <br><br>  When everything is set up, run your TensorFlow code as usual.  TPU will be detected when loading, the calculation schedule will be compiled and will be transferred there.  Interestingly, TPU can also directly read and write to the cloud storage control points and summaries.  To do this, you need to allow entry to the cloud storage in your Cloud TPU account. <br><br><h1>  Benchmarks </h1><br>  Of course, the most interesting thing is the real performance of tensor processors.  The TensorFlow repository on GitHub has a <a href="https://github.com/tensorflow/tpu">set of tested and optimized TPU models</a> .  Below are the results of experiments with <a href="https://github.com/tensorflow/tpu/tree/master/models/official/resnet">ResNet</a> and <a href="https://github.com/tensorflow/tpu/tree/master/models/experimental/inception">Inception</a> .  We also wanted to see how a model that is not optimized for TPU is being calculated, so we adapted the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/text_classification_character_rnn.py">model to classify text</a> on the long short-term memory architecture (LSTM) to run on TPU.  In fact, Google recommends using larger models (see the <a href="https://cloud.google.com/tpu/docs/tpus">‚ÄúWhen to use TPU‚Äù section</a> ).  We have a smaller model, so it‚Äôs especially interesting to see if TPU gives any advantage. <br><br>  For all models, we compared the learning speed on a single Cloud TPU with a single Nvidia P100 and V100 graphics processor.  It should be noted that a full-fledged comparison should include a comparison of the final quality and convergence of models, and not just throughput.  Our experiments are only superficial first benchmarks, and leave a detailed analysis for the future. <br><br>  Tests for TPU and P100 were launched on n1-standard-16 instances of the Google Cloud platform (16 Intel Haswell virtual CPUs, 60 GB memory).  For the V100 graphics processor, <i>p3.2xlarge</i> instances <i>on AWS were used</i> (8 virtual CPUs, 60 GB of memory).  All systems under Ubuntu 16.04.  For TPU, install TensorFlow 1.6.0-rc1 from the PyPi repository.  Tests for GPUs were launched from <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a> containers with TensorFlow 1.5 images ( <i>tensorflow: 1.5.0-gpu-py3</i> ), including support for CUDA 9.0 and cuDNN 7.0. <br><br><h1>  TPU-optimized models </h1><br>  Let's look first at the performance of models that are officially optimized for TPU.  The following shows the performance by the number of processed images per second. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f33/38f/e8c/f3338fe8c8913c79ab635641d1fcda1a.png"><br>  <i><font color="gray">Package sizes: 1024 on TPU and 128 on GPU.</font></i>  <i><font color="gray">For the latter, they took the implementation from <a href="https://github.com/tensorflow/benchmarks">the</a> TensorFlow <a href="https://github.com/tensorflow/benchmarks">benchmarks</a> repository.</font></i>  <i><font color="gray">As training data, simulate Google‚Äôs ImageNet dataset in cloud storage (for TPU) and on local disks (for GPU)</font></i> <br><br>  On the ResNet-50, a single Cloud TPU tensor processor (8 cores and 64 GB of RAM) was about <b>8.4 times faster</b> than one P100, and about 5.1 times faster than a V100.  For InceptionV3, the performance difference is almost the same (~ 8.4 and ~ 4.8, respectively).  On calculations with lower accuracy (fp16), the V100 adds significantly to speed. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d0/b9f/7dd/1d0b9f7dd45a09a55fe84d9aecf9770a.png"><br><br>  It is clear that in addition to speed you need to take into account the price.  The table shows the performance, normalized by price with every second billing.  TPU still clearly wins. <br><br><h1>  Custom models LSTM </h1><br>  Our <a href="https://gist.github.com/elmarhaussmann/6a2804a334baff9aed79aa7f1ed4b038">custom model</a> is a bidirectional LSTM for text classification with 1024 hidden units.  LSTM are the main building blocks in modern neural networks, so this is a good addition for official machine vision models. <br><br>  <a href="https://github.com/tensorflow/tensorflow/blob/9054c9b2ac303cbd1538166d0821f389cbc75894/tensorflow/examples/learn/text_classification_character_rnn.py">The original code</a> has already used the Estimator framework, so it is very easy to adapt it for TPUEstimator.  Although there is one big caveat: on TPU <b>we could not achieve the convergence of the model</b> , although the same model (packet size, etc.) on the GPU worked fine.  We think this is due to some bug that will be fixed - either in our code (if you find it, let us know!) Or in TensorFlow. <br><br>  It turned out that TPU provides an even greater increase in performance on the LSTM model (21402 samples / s): <b>~ 12.9 times faster</b> than P100 (1658 samples / s) and ~ 7.7 times faster than V100 (2778 samples /with)!  Given that the model is relatively small and has not been optimized in any way, this is a very promising result.  But until the bug is fixed, we will consider these results preliminary. <br><br><h1>  Conclusion </h1><br>  On the tested models, TPU performed very well both in terms of performance and in terms of money saving compared to the latest generations of GPUs.  This is contrary to <a href="https://www.forbes.com/sites/moorinsights/2018/02/13/google-announces-expensive-cloud-tpu-availability/">previous estimates</a> . <br><br><img src="https://habrastorage.org/webt/gs/qt/ex/gsqtexrjzd9sizvkhsnc4saq_dk.png"><br>  <i><font color="gray">The results of previous benchmarks.</font></i>  <i><font color="gray">Source: <a href="https://www.forbes.com/sites/moorinsights/2018/02/13/google-announces-expensive-cloud-tpu-availability/">Forbes</a></font></i> <br><br>  Although Google is <a href="https://cloud.google.com/tpu/docs/tpus">promoting</a> TPU as the optimal solution for scaling large models, our preliminary results on a small model have proved very promising.  In general, the experience of using TPU and adapting the TensorFlow code is already quite good for the beta version. <br><br>  We believe that when TPU becomes available to a wider audience, it can be a real alternative to the Nvidia GPU. </div><p>Source: <a href="https://habr.com/ru/post/350042/">https://habr.com/ru/post/350042/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../350030/index.html">Searchless method for calculating controller settings using Python</a></li>
<li><a href="../350032/index.html">Additional factors for evaluating spam activity IP / Email addresses in the Anti-Spam / Anti-Fraud API</a></li>
<li><a href="../350034/index.html">Service Workers. Web Push and where they live</a></li>
<li><a href="../350038/index.html">Rake when moving to a virtual platform with physical hardware</a></li>
<li><a href="../350040/index.html">Call Tracking Mango Office: under the hood of the service</a></li>
<li><a href="../350044/index.html">Announcement of Google Summer of Code 2018 for the project radare2</a></li>
<li><a href="../350046/index.html">Hidden JS-mining in the browser</a></li>
<li><a href="../350048/index.html">Let's Encrypt postponed issuance of wildcard certificates due to security issues.</a></li>
<li><a href="../350050/index.html">CodeFest Frontend: React, Javascript and Best Practices</a></li>
<li><a href="../350052/index.html">Designer, close Sketch! Knowledge of UX / UI is not a guarantee of a successful career</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>