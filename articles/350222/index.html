<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Getting command parameters from a human phrase</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Although I was able to deal with the classification of the intent, there remained a more difficult task - to extract additional parameters from the ph...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Getting command parameters from a human phrase</h1><div class="post__text post__text-html js-mediator-article">  Although I was <a href="https://habrahabr.ru/post/348224/">able to deal</a> with the classification of the intent, there remained a more difficult task - to extract additional parameters from the phrase.  I know that this is done using tags.  Once I have successfully applied <a href="https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html">sequence_tagging</a> , but I‚Äôm not very happy to keep a dictionary of vector representations of words larger than 6 gigabytes. <br><a name="habracut"></a><br><h4>  Attempt zero </h4><br>  I found <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">an example of Kegher‚Äôs implementation of tegger</a> and, in the best traditions of my experiments, began to copy pieces of code from there <a href="https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html">without thinking</a> .  In the example, the neural network processes the input string as a sequence of characters, not dividing it into words.  But further down the text is an example using the Embedding layer.  And since I learned to use hashing_trick, then I felt a keen desire to use this skill. <br><br>  What I have done has been learning much slower than the classifier.  I included a debugging output in Keras, and, thoughtfully looking at the slowly appearing lines, I noticed the Loss value.  It did not particularly decrease, and at the same time it seemed to me to be rather large.  And accuracy while it was small.  I was too lazy to sit and wait for the result, so I remembered one of the recommendations of Andrew Ng - to try my neural network on a smaller set of training data.  By looking at the dependence of Loss on the number of examples, it is possible to evaluate whether it is worth expecting good results. <br><br>  Therefore, I stopped the training, generated a new set of training data - 10 times less than the previous one - and started the training again.  And almost immediately received the same Loss and the same Accuracy.  It turns out that the increase in the number of training examples will not be better. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I nevertheless waited for the end of training (about an hour, despite the fact that the classifier studied in a few seconds) and decided to try it out.  And I realized that I had to copy more, because in the case of seq2seq for training and for real work, different models are needed.  I checked the code a little more and decided to stop and think about what to do next. <br><br>  Before me was the choice - to take the finished example again, but without amateur, or to take the <a href="https://github.com/farizrahman4u/seq2seq">ready seq2seq</a> , or to return to the tool that I had already worked with - the sequence tagger on NERModel.  It is true that without GloVe. <br><br>  I decided to try all three in the reverse order. <br><br><h4>  NER model from sequence tagging </h4><br>  The desire to edit the existing code disappeared as soon as I looked inside.  So I went the other way - to pull out of sequence tagging different classes and methods, take gensim.models.Word2Vec and feed it all.  After an hour of attempts, I was able to make training data sets, but I could not replace the dictionary.  I looked at the mistake that came from somewhere in the depths of numpy, and refused this idea. <br><br>  Made a <a href="https://github.com/aragaer/human2pa/commit/195d7b2564757d318eb2e031888d6ca07c1a8bf9">commit</a> , just in case not lost. <br><br><h4>  Seq2Seq </h4><br>  The documentation on Seq2Seq describes only how to prepare it, but not how to use it.  I had to find <a href="https://github.com/nicolas-ivanov/debug_seq2seq">an example</a> and try again to adjust to my own.  Another couple of hours of experiments and the result - the accuracy in the learning process is consistently equal to 0.83.  Regardless of the size of the training data.  So again I confused something somewhere. <br><br>  Here in the example I didn‚Äôt really like that, first, the manual splitting of the training data into pieces takes place, and second, the embedding is done manually.  In the end, I screwed into one Keras-model Embedding first, then Seq2Seq, and prepared the data in one large piece. <br><br><div class="spoiler">  <b class="spoiler_title">it turned out beautiful</b> <div class="spoiler_text"><pre><code class="python hljs">model = Sequential() model.add(Embedding(<span class="hljs-number"><span class="hljs-number">256</span></span>, TOKEN_REPRESENTATION_SIZE, input_length=INPUT_SEQUENCE_LENGTH)) model.add(SimpleSeq2Seq(input_dim=TOKEN_REPRESENTATION_SIZE, input_length=INPUT_SEQUENCE_LENGTH, hidden_dim=HIDDEN_LAYER_DIMENSION, output_dim=output_dim, output_length=ANSWER_MAX_TOKEN_LENGTH, depth=<span class="hljs-number"><span class="hljs-number">1</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'rmsprop'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> </div></div><br>  But beauty did not save - the behavior of the network has not changed. <br><br>  <a href="https://github.com/aragaer/human2pa/commit/18c199cfe377b54bc2578346a3b7b1b07fabe8d8">Another commit</a> , turn to the third option. <br><br><h4>  Manual seq2seq </h4><br>  At first I honestly copied everything and tried to run as is.  The input is simply a sequence of characters of the original phrase, the output should be a sequence of characters that can be spaced apart by spaces and get a list of tags.  Accuracy seems to be good.  Because the neural network quickly learned that if it began to spell out a tag, then it would write to the end without errors.  But the tags themselves didn‚Äôt fit the desired result at all. <br><br>  We make a small change - the result should not be a sequence of characters, but a sequence of tags, recruited from the final list.  Accuracy fell immediately - because now it has become honestly clear that the network is not coping. <br><br>  Nevertheless, I brought the network training to the end and looked at exactly what it gives out.  Because a stable result of 20% probably means something.  As it turned out, the network found a way not to strain too much: <br><br><pre> <code class="hljs vbscript">please, remind <span class="hljs-keyword"><span class="hljs-keyword">me</span></span> tomorrow <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> buy stuff O</code> </pre> <br>  That is, it pretends that there is only one word in the phrase that does not contain any data (in the sense of those that the classifier has not eaten yet).  We look at the training data ... indeed, about 20% of the phrases are just that - yes, no, part of ping (that is, all sorts of hello) and part of acknowledge (all sorts of thanks). <br><br>  We begin to put the network stick in the wheel.  I cut the number of yes / no 4 times, ping / acknowledge 2 times and add all the garbage in one word, but containing data.  At this stage, I decided that I don‚Äôt need to explicitly bind to the class in tags, so for example <code>B-makiuchi-count</code> turned into just <code>B-count</code> .  And the new ‚Äúrubbish‚Äù was just numbers with the <code>B-count</code> class, ‚Äútime‚Äù in the form of ‚Äú4:30 AM‚Äù with the expected <code>B-time</code> tag, indicating a date like ‚Äúnow‚Äù, ‚Äútoday‚Äù and ‚Äútomorrow‚Äù with the tag <code>B-when</code> <br><br>  Still not working.  The network no longer gives an unambiguous answer ‚ÄúO and that's it‚Äù, but at the same time accuracy remains at 18%, and the answers are completely inadequate. <br><br><pre> <code class="hljs kotlin">not yet expected [<span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">actual</span></span> [<span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'B-what'</span></span>] what <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> the weather outside? expected [<span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">actual</span></span> [<span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'O'</span></span>, <span class="hljs-string"><span class="hljs-string">'B-what'</span></span>]</code> </pre><br>  While a dead end. <br><br><h4>  Interlude - Comprehension </h4><br>  The lack of results is also a result.  I even had a superficial, but an understanding of what exactly happens when I construct models in Keras.  I learned how to save them, load them and even train them as needed.  But at the same time, I did not achieve what I wanted - the translation of "human" speech into "the language of the bot."  Clues I no longer left. <br><br>  And then I started writing an article.  Previous article  In its original version, everything ended at this place - I have a classifier, but no tagger.  After some deliberation, I abandoned this idea and left only about a more or less successful classifier and mentioned problems with the tagger. <br><br>  The calculation was justified - I received a link to the <a href="https://github.com/RasaHQ/rasa_nlu">Rasa NLU</a> .  At first glance, it looked like something very appropriate. <br><br><h4>  Rasa nlu </h4><br>  For several days I did not return to my experiments.  Then he sat down and fastened the Rasa NLU with his experimental scripts in an hour and a little.  This is not to say that it was very difficult. <br><br><div class="spoiler">  <b class="spoiler_title">code</b> <div class="spoiler_text"><div class="spoiler">  <b class="spoiler_title">make_sample</b> <div class="spoiler_text"><pre> <code class="python hljs">tag_var_re = re.compile(<span class="hljs-string"><span class="hljs-string">r'data-([az-]+)\((.*?)\)|(\S+)'</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">make_sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(rs, cls, *args, **kwargs)</span></span></span><span class="hljs-function">:</span></span> tokens = [cls] + list(args) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k, v <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> kwargs.items(): tokens.append(k) tokens.append(v) result = rs.reply(<span class="hljs-string"><span class="hljs-string">''</span></span>, <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(map(str, tokens))).strip() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> result == <span class="hljs-string"><span class="hljs-string">'[ERR: No Reply Matched]'</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">raise</span></span> Exception(<span class="hljs-string"><span class="hljs-string">"failed to generate string for {}"</span></span>.format(tokens)) cmd, en, rasa_entities = cls, [], [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> tag, value, just_word <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> tag_var_re.findall(result): <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> just_word: en.append(just_word) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: _, tag = tag.split(<span class="hljs-string"><span class="hljs-string">'-'</span></span>, maxsplit=<span class="hljs-number"><span class="hljs-number">1</span></span>) words = value.split() start = len(<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(en)) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> en: start += <span class="hljs-number"><span class="hljs-number">1</span></span> en.extend(words) end = len(<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(en)) rasa_entities.append({<span class="hljs-string"><span class="hljs-string">"start"</span></span>: start, <span class="hljs-string"><span class="hljs-string">"end"</span></span>: end, <span class="hljs-string"><span class="hljs-string">"value"</span></span>: value, <span class="hljs-string"><span class="hljs-string">"entity"</span></span>: tag}) <span class="hljs-keyword"><span class="hljs-keyword">assert</span></span> <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(en)[start:end] == value <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> cmd, en, rasa_entities</code> </pre> </div></div>  After this, save the training data is not difficult: <pre> <code class="python hljs"> rasa_examples = [] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> e, p, r <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> zip(en, pa, rasa): sample = {<span class="hljs-string"><span class="hljs-string">"text"</span></span>: <span class="hljs-string"><span class="hljs-string">' '</span></span>.join(e), <span class="hljs-string"><span class="hljs-string">"intent"</span></span>: p} <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> r: sample[<span class="hljs-string"><span class="hljs-string">"entities"</span></span>] = r rasa_examples.append(sample) <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> open(os.path.join(data_dir, <span class="hljs-string"><span class="hljs-string">"rasa_train.js"</span></span>), <span class="hljs-string"><span class="hljs-string">"w"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> rf: json.dump({<span class="hljs-string"><span class="hljs-string">"rasa_nlu_data"</span></span>: {<span class="hljs-string"><span class="hljs-string">"common_examples"</span></span>: rasa_examples, <span class="hljs-string"><span class="hljs-string">"regex_features"</span></span>: [], <span class="hljs-string"><span class="hljs-string">"entity_synonims"</span></span>: []}}, rf)</code> </pre>  The most difficult thing in creating a model is the correct config. <pre> <code class="python hljs"> training_data = load_data(os.path.join(data_dir, <span class="hljs-string"><span class="hljs-string">"rasa_train.js"</span></span>)) config = RasaNLUConfig() config.pipeline = registry.registered_pipeline_templates[<span class="hljs-string"><span class="hljs-string">"spacy_sklearn"</span></span>] config.max_training_processes = <span class="hljs-number"><span class="hljs-number">4</span></span> trainer = Trainer(config) trainer.train(training_data) model_dir = trainer.persist(os.path.join(data_dir, <span class="hljs-string"><span class="hljs-string">"rasa"</span></span>))</code> </pre>  And the most difficult to use is to find it. <pre> <code class="python hljs"> config = RasaNLUConfig() config.pipeline = registry.registered_pipeline_templates[<span class="hljs-string"><span class="hljs-string">"spacy_sklearn"</span></span>] config.max_training_processes = <span class="hljs-number"><span class="hljs-number">4</span></span> model_dir = glob.glob(data_dir+<span class="hljs-string"><span class="hljs-string">"/rasa/default/model_*"</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>] interpreter = Interpreter.load(model_dir, config)</code> </pre> <pre> <code class="python hljs"> parsed = interpreter.parse(line) result = [parsed[<span class="hljs-string"><span class="hljs-string">'intent_ranking'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-string"><span class="hljs-string">'name'</span></span>]] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> entity <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> parsed[<span class="hljs-string"><span class="hljs-string">'entities'</span></span>]: result.append(entity[<span class="hljs-string"><span class="hljs-string">'entity'</span></span>]+<span class="hljs-string"><span class="hljs-string">':'</span></span>) result.append(<span class="hljs-string"><span class="hljs-string">'"'</span></span>+entity[<span class="hljs-string"><span class="hljs-string">'value'</span></span>]+<span class="hljs-string"><span class="hljs-string">'"'</span></span>) print(<span class="hljs-string"><span class="hljs-string">' '</span></span>.join(result))</code> </pre> </div></div> <code>please, find me some pictures of japanese warriors <br> find what: "japanese warriors" <br> remind me to have a breakfast now, sweetie <br> remind action: "have a breakfast" when: "now" what: "sweetie"</code> <br>  ... although there is still something to work on. <br><br>  Of the shortcomings - the learning process is completely silent.  Surely it is included somewhere.  However, all the training took about three minutes.  Still for work spacy after all the model for initial language is required.  But it weighs significantly less than GloVe - for English it is less than 300 megabytes.  True for the Russian language model is not yet - and the ultimate goal of my experiments should work with the Russian.  It will be necessary to look at the other pipeline, available in Rasa. <br><br>  All code is available <a href="https://github.com/aragaer/human2pa/tree/test_4">in github</a> . </div><p>Source: <a href="https://habr.com/ru/post/350222/">https://habr.com/ru/post/350222/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../350208/index.html">Programming Languages ‚Äã‚Äãfor a Quantum Computer</a></li>
<li><a href="../350210/index.html">Play with ormats</a></li>
<li><a href="../350212/index.html">Will robots inherit the earth?</a></li>
<li><a href="../350214/index.html">Security Week 6: ‚Äúenchanted letter‚Äù threatens apple-growers, with the world of Captcha - Monero, mining now in Word</a></li>
<li><a href="../350218/index.html">Russian storage systems from the company StoreQuant</a></li>
<li><a href="../350224/index.html">Webpack 4 Legato released</a></li>
<li><a href="../350228/index.html">About threat modeling</a></li>
<li><a href="../350230/index.html">We summarize the animation tables in iOS applications</a></li>
<li><a href="../350232/index.html">Oh, those modal windows or why I loved the render functions in VueJs</a></li>
<li><a href="../350234/index.html">SAML authorization bypass</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>