<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Accelerate std :: shared_mutex 10 times</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, we will examine in detail the atomic operations and memory barriers of C ++ 11 and the assembler instructions generated by them on x8...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Accelerate std :: shared_mutex 10 times</h1><div class="post__text post__text-html js-mediator-article">  In this article, we will examine in detail the atomic operations and memory barriers of C ++ 11 and the assembler instructions generated by them on x86_64 processors. <br><br>  Next, we show how to speed up the work of the <b>contfree_safe_ptr &lt;std :: map&gt;</b> to the level of complex and optimized <b>lock-free</b> data structures with similar functionality to std :: map &lt;&gt;, for example: SkipListMap and BronsonAVLTreeMap from the libCDS library (Concurrent Data Structures library): <a href="https://github.com/khizmax/libcds">github. com / khizmax / libcds</a> <br><br>  And we will be able to get such multithreaded performance for any of your natively safe class T used as contfree_safe_ptr &lt;T&gt;.  We are interested in optimizations that increase productivity by ~ 1000%, so we will not pay attention to weak and dubious optimizations. <br><a name="habracut"></a><br>  Three related articles: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  <a href="https://habrahabr.ru/post/328348/">Making any object thread safe</a> </li><li>  Accelerate std :: shared_mutex 10 times </li><li>  <a href="https://habrahabr.ru/post/328374/">Thread safe std :: map with lock-free map performance</a> </li></ol><br>  ‚Üí <a href="https://www.codeproject.com/Articles/1183423/We-make-a-std-shared-mutex-times-faster">My article in English</a> <br>  ‚Üí <a href="https://github.com/AlexeyAB/object_threadsafe">Examples and tests from all three articles</a> <br><br><h2>  <b>High performance lock-based data structures</b> </h2><br>  contfree_safe_ptr &lt;T&gt; is the safe_ptr &lt;T class, contention_free_shared_mutex&gt;, where contention_free_shared_mutex is its own optimized shared-mutex.  And safe_ptr is a thread safe pointer from the previous article. <br><br>  In order, we show how to implement our own high-performance <b>contention-free shared-mutex</b> , almost non-conflicting on readings.  We implement our own active spinlock and recursive-spinlock locks to block rows (rows) on update operations.  Create RAII-blocking pointers to avoid the cost of multiple blocking.  We present the results of performance tests. <br><br>  And as a bonus ‚ÄúJust for fun‚Äù, we will show how to implement our own class of simplified partitioned type partitioned_map, even more optimized for multithreading, consisting of several std :: map, by analogy with the partition table from the DBMS, when the boundaries of each section are initially known. <br><br><h2>  <b>Basics of atomic operations</b> </h2><br>  Consider the basics of multithreading, atomicity and memory barriers.  If we change the same data from multiple threads, i.e.  if we run the thread_func () function simultaneously in multiple threads: <br><br><pre><code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">int</span></span> a; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">thread_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ a = a + <span class="hljs-number"><span class="hljs-number">1</span></span>; } <span class="hljs-comment"><span class="hljs-comment">// wrong: data-race</span></span></code> </pre> <br>  Then each thread calling the thread_func () function adds 1 to the usual shared variable int a;  Such code in general will not be thread safe, since  compound operations (RMW - read-modify-write) on ordinary variables consist of a set of small operations between which another thread can change data.  The operation a = a + 1;  consists of at least three mini-operations: <br><br><ol><li>  Load the value of the variable "a" in the register processor </li><li>  Add 1 to the value in the register </li><li>  Write the value of the register back to the variable "a" </li></ol><br>  For example, if int a = 0;  and 2 threads perform the operation a = a + 1;  then the result should be a = 2;  But the following can happen - step by step: <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">int</span></span> a = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-comment"><span class="hljs-comment">// register1 = ?, register2 = ?, a = 0 Thread 1: register1 = a; // register1 = 0, register2 = ?, a = 0 Thread 2: register2 = a; // register1 = 0, register2 = 0, a = 0 Thread 1: register1++; // register1 = 1, register2 = 0, a = 0 Thread 2: register2++; // register1 = 1, register2 = 1, a = 0 Thread 1: a = register1; // register1 = 1, register2 = 1, a = 1 Thread 2: a = register2; // register1 = 1, register2 = 1, a = 1</span></span></code> </pre> <br>  Two threads added to the same global variable +1, but in the end the variable a = 1 - this problem is called Data-Races. <br><br>  To avoid this problem, there are 3 ways: <br><br><ol><li>  Using atomic instructions over atomic variables - but there is one drawback, the number of atomic functions is very small - therefore it is difficult to implement complex logic using them: <a href="http://en.cppreference.com/w/cpp/atomic/atomic">en.cppreference.com/w/cpp/atomic/atomic</a> </li><li>  Develop your own complex lock-free algorithms for each new container. </li><li>  Use locks (std :: mutex, std :: shared_timed_mutex, spinlock ...) - they allow for locked code in turn one by one thread, so data-races problems do not occur and we can use arbitrarily complex logic using any normal thread-unsafe objects. </li></ol><br>  For std :: atomic &lt;int&gt; a;  if initially a = 0;  and 2 threads perform the operation a + = 1;  then the result will always be a = 2; <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt; a; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">thread_func</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ a += <span class="hljs-number"><span class="hljs-number">1</span></span>; } <span class="hljs-comment"><span class="hljs-comment">// correct: no data-race</span></span></code> </pre> <br>  The following will always occur on x86_64 processors (step by step): <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt; a = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-comment"><span class="hljs-comment">// register1 = ?, register2 = ?, a = 0 Thread 1: register1 = a; // register1 = 0, register2 = ?, a = 0 Thread 1: register1++; // register1 = 1, register2 = ?, a = 0 Thread 1: a = register1; // register1 = 1, register2 = ?, a = 1 Thread 2: register2 = a; // register1 = 1, register2 = 1, a = 1 Thread 2: register2++; // register1 = 1, register2 = 2, a = 1 Thread 2: a = register2; // register1 = 1, register2 = 2, a = 2</span></span></code> </pre> <br>  On processors with LL / SC support, for example, on ARM or PowerPC, other steps will occur, but with the same result a = 2. <br><br>  Atomic variables are introduced in C ++ 11 standard: <a href="http://en.cppreference.com/w/cpp/atomic/atomic">en.cppreference.com/w/cpp/atomic/atomic</a> <br><br>  The member functions of the std :: atomic &lt;&gt; class template are always atomic. <br>  We give 3 fragments of the correct code of identical meaning: <br><br>  1. Example: <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt; a; a = <span class="hljs-number"><span class="hljs-number">20</span></span>; a += <span class="hljs-number"><span class="hljs-number">50</span></span>;</code> </pre> <br>  2. This is identical to example 1: <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt; a; a.store( <span class="hljs-number"><span class="hljs-number">20</span></span> ); a.fetch_add( <span class="hljs-number"><span class="hljs-number">50</span></span> );</code> </pre> <br>  3. And this is identical to example 1: <br><br><pre> <code class="cpp hljs"><span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic&lt;<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>&gt; a; a.store( <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_seq_cst ); a.fetch_add( <span class="hljs-number"><span class="hljs-number">50</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_seq_cst );</code> </pre> <br>  This means that for std :: atomic: <br><br><ul><li>  <b>load ()</b> and <b>store ()</b> are the same as <b>operator T</b> and <b>operator =</b> and </li><li>  <b>fetch_add ()</b> and <b>fetch_sub ()</b> are the same as <b>operator + =</b> and <b>operator- =</b> </li><li>  Sequential Consistency (std :: memory_order_seq_cst) is the default memory barrier (strictest and most reliable, but also the slowest relative to others). </li></ul><br>  Note the <b>std :: atomic</b> and <b>volatile</b> differences in C ++ 11: <a href="http://www.drdobbs.com/parallel/volatile-vs-volatile/212701484">www.drdobbs.com/parallel/volatile-vs-volatile/212701484</a> <br><br>  There are 5 main differences: <br><br><ol><li>  <b>Optimizations</b> : For std :: atomic &lt;T&gt; a;  there are two possible optimizations that are impossible for volatile T a; <br>  ‚Ä¢ Merger optimization: a = 10;  a = 20;  can be replaced by a compiler with a = 20; <br>  ‚Ä¢ Optimization of constant replacement: a = 1;  local = a;  can be replaced with a = 1 compiler;  local = 1; </li><li>  <b>Reordering</b> : Operations on std :: atomic &lt;T&gt; a;  may limit the reordering around itself for operations with ordinary variables and operations with other atomic variables in accordance with the memory barrier used std :: memory_order_ ... In contrast, volatile T a;  does not affect the order of ordinary variables (non-atomic / non-volatile), but calls to all volatile variables always maintain strict mutual order, i.e.  The order of execution of any two volatile operations cannot be changed by the compiler, but not by the processor. </li><li>  <b>Spilling</b> : Memory barriers std :: memory_order_release, std :: memory_order_acq_rel, std :: memory_order_seq_cst specified for operations on std :: atomic &lt;T&gt; a;  initiate spilling of all ordinary variables until the execution of an atomic operation.  Those.  These barriers unload normal variables from processor registers to RAM / cache, unless the compiler can guarantee 100% that this local variable cannot be used in other threads. </li><li>  <b>Atomicity / alignment</b> : Operations on std :: atomic &lt;T&gt; a;  visible to other streams, either completely or not at all.  For integral types T, this is achieved by aligning the location of the atomic variables in the memory by the compiler ‚Äî at least the variable must lie in one cache line, so the atomic variable can be changed or read by just one CPU operation.  Conversely, the compiler does not guarantee the alignment of volatile variables and the atomic nature of operations on them.  Volatile variables are typically used to access device memory or <a href="https://en.wikipedia.org/wiki/Volatile_(computer_programming)">in other cases</a> , but not to exchange data between threads.  The device driver API returns a pointer to volatile variables, and if necessary, this API provides alignment. </li><li>  <b>Atomicity of RMW operations</b> (read-modify-write): Operations on std :: atomic &lt;T&gt; a;  such as (++, -, + =, - =, * =, / =, CAS, exchange) are executed atomically, i.e.  if two threads perform an operation ++ a;  then this variable is guaranteed to be increased by 2. This is achieved by blocking the cache lines (x86_64) or by marking the absence of changes in the cache lines on processors supporting LL / SC (ARM, PowerPC) for the entire duration of the RMW operation.  Volatile variables do not provide atomicity for compound RMW operations. </li></ol><br><br>  There is one general rule for std :: atomic and volatile variables: each read or write operation always accesses the memory / cache, i.e.  values ‚Äã‚Äãare never cached in processor registers. <br><br>  Also note that for ordinary (ordinary) variables and objects (non-atomic / non-volatile) - any optimizations are possible and any reordering of independent instructions relative to each other by the compiler or processor. <br><br>  Recall that the write operations into memory over atomic variables with memory barriers std :: memory_order_release, std :: memory_order_acq_rel and std :: memory_order_seq_cst guarantee spilling (writing from registers to memory) of all non-atomic / non-volatile variables that are present moment in processor registers: <a href="https://en.wikipedia.org/wiki/Register_allocation">en.wikipedia.org/wiki/Register_allocation#Spilling</a> <br><br><h2>  <b>Re-order instructions</b> </h2><br>  The compiler and processor change the order of instructions to optimize the program and improve performance. <br><br>  Here is an example of the optimizations that the GCC compiler and the x86_64 processor do: <a href="https://godbolt.org/g/n91hpt">godbolt.org/g/n91hpt</a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f31/f71/4e6/f31f714e62fad1a8f2046cffab9e2296.jpg" alt="image"><br><br>  Full size picture: <br><br>  <a href="">hsto.org/files/3d2/18b/c7b/3d218bc7b3584f82820f92077096d7a0.jpg</a> <br><br>  What are the optimization on the picture: <br><br><ol><li>  <b>Reordering with the GCC 7.0 compiler:</b> <br><br>  ‚Ä¢ Swaps the entry in memory b = 5;  and loading from memory into the register int tmp_c = c ;.  This allows you to request the value ‚Äúc‚Äù as early as possible, and while the processor is waiting for this long operation, the processor‚Äôs pipeline allows you to simultaneously start the operation b = 5;  These 2 operations are independent of each other. <br>  ‚Ä¢ Combines loading from memory into register int tmp_a = a;  and the addition operation tmp_c = tmp_c + tmp_a;  - as a result, instead of two instructions, you get one add eax, a [rip] </li><li>  <b>Reordering the x86_64 processor:</b> <br>  The processor can interchange the actual write to memory mov b [rip], 5 and reading from memory combined with addition add eax, a [rip]. <br>  By initiating the entry into memory with the instruction mov b [rip], 5, the following occurs: first, the value 5 and the address b [rip] are placed in the hardware store-buffer queue, the cache lines containing the address b [rip] in all processor cores are expected to become invalid and receive from response, then CPU-Core-0 sets the status ‚ÄúeXclusive‚Äù for the cache line containing b [rip], and only after that the actual value 5 from the Store-buffer is written to this cache line at b [rip].  Learn more about x86_64 cache coherence protocols MOESI / MESIF - changes in which are visible to all cores instantly: <a href="https://en.wikipedia.org/wiki/MESIF_protocol">en.wikipedia.org/wiki/MESIF_protocol</a> <br><br>  In order not to wait all this time - immediately after 5 is placed in the Store-Buffre, without waiting for the actual write to the cache, we can start executing the following instructions: reading from memory or operations with registers.  This is what the x86_64 processor does. <br><br>  Intel 64 and IA-32 Architects Software Developer's Manual Volume 3 (3A, 3B, 3C &amp; 3D): System Programming Guide: <a href="https://www-ssl.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-system-programming-manual-325384.pdf">www-ssl.intel.com/content/dam/www/public/us/en/documents/manuals/ 64-ia-32-architectures-software-developer-system-programming-manual-325384.pdf</a> <br><blockquote>  8.2.3.4 Loads May Be Reordered with Ear Stores to Different Locations <br>  The Intel-64 Memory-Ordering Model.  However, loads are not reordered with the same location. </blockquote></li></ol><br>  Processors of the x86_64 family have a strong memory model.  And processors with weak memory models, for example, PowerPC and ARM v7 / v8 can perform even more reorders. <br>  Let us give some examples of possible reordering of a record in the memory of ordinary <b>ordinary-</b> variables, <b>volatile-</b> variables and <b>atomic-</b> variables. <br>  Reordering ordinary variables: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/644/14e/f9d/64414ef9db4b116d1200e366df6b7aee.jpg" alt="image"><br><br>  This code with ordinary variables can be reordered by the compiler or processor, since  within one flow its meaning does not change.  But within the framework of multiple threads, such reordering may affect the logic of the program. <br><br>  If 2 variables are volatile, then the following reorders are possible.  The compiler cannot reorder operations on volatile variables during compilation, but the compiler allows the processor to perform this reordering at runtime. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1c0/db4/8fa/1c0db48faea9a487237a09b009de2d3e.jpg" alt="image"><br><br>  To prevent all or only some of these reorders - atomic operations exist (recall that by default, atomic operations use the strictest memory barrier <b>std :: memory_order_seq_cst</b> ): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/630/a55/769/630a55769ff98887a9ae924b36816c11.jpg" alt="image"><br><br>  Another thread can see the variable changes in memory in exactly this changed order. <br>  If we do not specify a memory barrier for an atomic operation, the std :: memory_order_seq_cst barrier is used by default, and no atomic or non-atomic operations can be reordered with such operations (but there are exceptions - which we will look at next). <br><br>  In the above case, we first write to the normal variables a and b, and then to the atomic variables a_at and b_at, and this order cannot be changed.  Also, a b_at write to memory cannot occur earlier than a_at write to a memory.  But writing to variables a and b can be reordered relative to each other. <br><br>  When we say ‚Äúthey can be reordered,‚Äù that means they can, but not necessarily.  It depends on how you decide to optimize the C ++ code - the compiler at compile time or the CPU at run time. <br><br>  Below we will look at weaker memory barriers, which allow instructions to be reordered in permitted directions - this allows the compiler and processor to better optimize the code and improve performance more. <br><br><h2>  <b>Barriers to reordering memory accesses</b> </h2><br>  The memory model of the C ++ 11 standard provides us with 6 types of memory barriers that correspond to the capabilities of modern CPUs for extraordinary execution of operations (speculative execution), using them we do not completely prohibit reordering, but only in the necessary directions.  This leaves the compiler and processor to optimize the code as much as possible.  And the forbidden reordering directions allow us to preserve the correctness of our code.  <a href="http://en.cppreference.com/w/cpp/atomic/memory_order">en.cppreference.com/w/cpp/atomic/memory_order</a> <br><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">enum</span></span> memory_order { memory_order_relaxed, memory_order_consume, memory_order_acquire, memory_order_release, memory_order_acq_rel, memory_order_seq_cst };</code> </pre> <br><ul><li>  <b>memory_order_consume.</b>  Immediately, we note that we practically will not use the memory_order_consume barrier, since  in the standard there are doubts about the expediency of its use - a quote from the standard: <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf">www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf</a> <br><blockquote>  ¬ß29.3 <br>  (1.3) - memory_order_consume: a load operation  [Note: <b>Prefer memory_order_acquire</b> , which provides more guarantees than memory_order_consume.  Implement better than that of memory_order_acquire.  Specification revisions are under consideration.  - end note] </blockquote></li><li>  <b>memory_order_acq_rel.</b>  Note also that the memory_order_acq_rel barrier is used only for atomic compound RMW operations (Read-Modify-Write) such as: compare_exchange_weak () / _ strong (), exchange (), fetch_ (add, sub, and, or, xor) or corresponding operator <a href="http://en.cppreference.com/w/cpp/atomic/atomic">names</a> : <a href="http://en.cppreference.com/w/cpp/atomic/atomic">en.cppreference.com/w/cpp/atomic/atomic</a> </li></ul><br>  The remaining 4 memory barriers can be used for any operations, with the exception of: acquire - not used for store (), and release - not used for load (). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/64c/422/198/64c422198a2c76f6814aa24315c13bed.jpg" alt="image"><br><br>  Depending on the memory barrier chosen, the compiler and the processor are prohibited from moving executable code relative to the barrier in different directions. <br><br>  Now we will show what exactly the given arrows denote - we will show what can change places and what cannot: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ca/3f8/9b6/6ca3f89b6ec08634131df0c499ad0f2d.jpg" alt="image"><br><br>  In order for 2 instructions to be swapped, it is necessary that the barriers of both of these instructions allow such an exchange.  Since  ‚ÄúOther any code‚Äù are ordinary non-atomic variables that have no barriers, then they allow any changes in the order.  In the lower-left example of <b>Relaxed-Release-Relaxed</b> , as you can see, the ability to change the order of the same memory barriers depends on the sequence in which they are placed. <br><br>  Let's take a look at what these memory barriers mean and what advantages they give us using as an example the implementation of the simplest ‚Äúspin-lock‚Äù lock, which requires the most common semantics of the Acquire-Release reordering.  Spin-lock is a lock by use similar to std :: mutex. <br><br>  To begin with, we implement the spin-lock concept directly in the body of our program.  And then we implement a separate class spin-lock.  To implement locks (mutex, spinlock ...), you must use Acquire-Release semantics, <b>C ++ 11 Standard ¬ß 1.10.1</b> (3): <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf">www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606 .pdf</a> <br><blockquote>  ... For example, it‚Äôs not <b>necessary to complete the</b> operation.  Correspondingly, it‚Äôs the same mutex.  In the <u>case of a formal operation,</u> </blockquote><br>  The basic meaning of Acquire-Release semantics is that: thread-2 ‚ÄúThread-2‚Äù after performing the <b>flag.load</b> operation <b>(std :: memory_order_acquire)</b> should see all changes of any variables / structures / classes (even non-atomic) that were made thread-1 ‚ÄúThread-1‚Äù before it <b>performs flag.store</b> operation <b>(0, std :: memory_order_release)</b> . <br><br>  The basic meaning of locks (mutex, spinlock ...) is to create a piece of code that can be executed only by one thread at the same time, i.e.  cannot be executed by two or more threads in parallel.  This section of code is called the critical section.  Inside it, you can use any ordinary code, including without std :: atomic. <br><br>  Memory barriers (memory fences) - prevent the program from being optimized by the compiler, so that no operation from the critical section goes beyond its limits. <br><br>  The thread that first captures the lock executes this block of code, while the remaining threads wait in a loop (possibly temporarily falling asleep).  When the first thread releases the lock, the processor decides which next one of the waiting threads will capture it.  And so on. <br>  We give 2 identical in the sense of the example: <br><br><ol><li>  using std :: atomic_flag: [19] <a href="http://coliru.stacked-crooked.com/a/1ec9a0c2b10ce864">coliru.stacked-crooked.com/a/1ec9a0c2b10ce864</a> </li><li>  using std :: atomic &lt;int&gt;: [20] <a href="http://coliru.stacked-crooked.com/a/03c019596b65199a">coliru.stacked-crooked.com/a/03c019596b65199a</a> </li></ol><br>  Example-1 is preferable, so we will schematically show the meaning of using memory barriers ‚Äî the atomic operations are shown in solid blue: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a5c/089/f12/a5c089f12996e73d320da1e7fec478da.jpg" alt="image"><br>  [19] <a href="http://coliru.stacked-crooked.com/a/1ec9a0c2b10ce864">coliru.stacked-crooked.com/a/1ec9a0c2b10ce864</a> <br>  The meaning of the barriers is very simple - the compiler optimizer is forbidden to move instructions from the critical section to the outside: <br><br><ul><li>  No instruction located after memory_order_ <b>acquire</b> can be executed before it. </li><li>  No instruction before memory_order_ <b>release</b> can be executed later. </li></ul><br>  Any other changes in the order of execution of independent instructions can be performed by the compiler (compile-time) or processor (run-time) - in order to optimize performance. <br><br>  For example, the string <b>int new_shared_value = shared_value;</b>  can be executed before <b>lock_flag.clear (std :: memory_order_release);</b>  .  Such reordering is acceptable and does not create data-races, since  All code that refers to data common to multiple threads is always enclosed within two acquire and release barriers.  And outside there is a code that only works with data local to the stream - and no matter in what order it will be executed.  Local dependencies for a stream are always saved as well, as it happens with single-threaded execution, therefore <b>int new_shared_value = shared_value;</b>  cannot be executed earlier than <b>shared_value + = 25;</b> <br><br>  So what is prohibited, and what acquire-release barriers for the critical section allow: <br><br><ul><li>  Any operation inside the critical section is <b>prohibited</b> to be performed outside the critical section. </li><li>  Any operation outside the critical section is <b>allowed</b> to be executed inside the critical section (if there are no additional memory barriers). </li></ul><br>  What specific actions does the compiler perform on std :: memory_order: <br><br><ul><li>  <b>1, 6</b> : The compiler generates acquire-barrier Assembler instructions for the load operation and a release barrier for the store operation if these barriers are necessary for a given processor architecture </li><li>  <b>2</b> : The compiler cancels the previous caching of variables in the processor registers in order to reload the values ‚Äã‚Äãof these variables changed by another thread - after the load (acquire) operation </li><li>  <b>5</b> : The compiler saves the values ‚Äã‚Äãof all variables from the processor registers to memory so that they become visible to other threads, i.e.  performs spilling ( <a href="https://en.wikipedia.org/wiki/Register_allocation">link</a> ) to store (release) </li><li>  <b>3, 4</b> : The compiler prevents the optimizer from changing the order of instructions in forbidden directions - indicated by red arrows </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/96b/a79/c0b/96ba79c0bcc392831a41e64d39377108.jpg" alt="image"><br><br>  And now let's see what happens if we use the Relaxed-Release semantics instead of Acquire-Release: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/251/b6e/de7/251b6ede7186788cb1f5f0367ddd8896.jpg" alt="image"><br><ul><li>  Left.  In the case of <b>Acquire-Release,</b> everything is executed correctly. </li><li>  On right.  In the case of <b>Relaxed-Release</b> , the algorithm will work incorrectly, since  part of the code inside the critical section protected by the lock can be moved outside by the compiler or processor.  Then there will be a problem Data Races - many threads even before blocking start simultaneously working with data, on which non-atomic operations are performed - we can get an erroneous result. </li></ul><br>  Note that it is usually impossible to implement all the logic on general data only with the help of atomic operations, since there are very few of them and they are rather slow if there are a lot of them.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Therefore, it is simpler and faster: in one atomic operation, set the ‚Äúclosed‚Äù flag, perform all non-atomic operations on the common data for the streams, and set the ‚Äúopen‚Äù flag. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We show schematically this process in time: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/589/a28/6f4/589a286f40175b4d847eaccda08857d8.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For example, two threads start the execution of the add_to_shared () function.</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thread-1 comes a little earlier, and one atomic instruction test_and_set () performs 2 operations at once: check if lock_flag == false, then set it to true (i.e., block spin-lock), and return false. </font><font style="vertical-align: inherit;">Therefore, the expression while (lock_flag.test_and_set ()); </font><font style="vertical-align: inherit;">right there the code of the critical section is completed and starts to be executed.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stream-2 at this moment also starts to execute this atomic instruction. Test_and_set (): checks if lock_flag == false, then sets the value to true, otherwise it does not change anything and returns the current value to true. </font><font style="vertical-align: inherit;">Therefore, the expression while (lock_flag.test_and_set ()); </font><font style="vertical-align: inherit;">will be executed until while (lock_flag);</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stream-1 performs the addition operation shared_value + = 25; </font><font style="vertical-align: inherit;">and then the atomic operation sets the value of lock_flag = false (i.e., unlocks spin-lock).</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thread 2 finally waiting for the condition lock_flag == false assigns lock_flag = true, returns false and ends the loop. </font><font style="vertical-align: inherit;">It then performs the addition of shared_value + = 25; </font><font style="vertical-align: inherit;">and assigns lock_flag = false (unlocks spin-lock).</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> At the beginning of this chapter, we gave 2 examples: </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">using std :: atomic_flag and test_and_set (): [21] </font></font><a href="http://coliru.stacked-crooked.com/a/1ec9a0c2b10ce864"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/1ec9a0c2b10ce864</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">using std :: atomic &lt;int&gt; and compare_exchange_weak (): [22] </font></font><a href="http://coliru.stacked-crooked.com/a/03c019596b65199a"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/03c019596b65199a</font></font></a> </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learn more about these operations by reference: </font></font><a href="http://en.cppreference.com/w/cpp/atomic/atomic"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en.cppreference.com/w/cpp/atomic/atomic</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let's look at how the assembler code for x86_64 generated by the GCC compiler for these two examples differs: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b8/88f/52d/2b888f52db4b8ebe319a5237c7cc5e6a.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To make it convenient to use, you can combine it into one class:</font></font><br><br><pre> <code class="cpp hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">spinlock_t</span></span></span><span class="hljs-class"> {</span></span> <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::atomic_flag lock_flag; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span>: <span class="hljs-keyword"><span class="hljs-keyword">spinlock_t</span></span>() { lock_flag.clear(); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">bool</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">try_lock</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> !lock_flag.test_and_set(<span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_acquire); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">lock</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">size_t</span></span> i = <span class="hljs-number"><span class="hljs-number">0</span></span>; !try_lock(); ++i) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (i % <span class="hljs-number"><span class="hljs-number">100</span></span> == <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::this_thread::yield(); } <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">unlock</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> </span></span>{ lock_flag.clear(<span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_release); } };</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">An example of using the class </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">spinlock_t</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> the following link: [23] </font></font><a href="http://coliru.stacked-crooked.com/a/92b8b9a89115f080"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/92b8b9a89115f080</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ensure that you never can understand what kind of barrier you use and what it will be compiled on x86_64, I will give the following table: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/64a/803/708/64a80370811061535c03f0f2d4ee8ba3.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The picture can be viewed in full size on the link: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hsto.org/files/4f8/7b4/1b6/4f87b41b64a54549afca679af1028f84.jpg</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The following is necessary to know only if you are writing code in x86_64 assembler, when the compiler cannot interchange assembly instructions for optimization:</font></font><br><br><ul><li> <b>seq_cst</b> .   (Clang  MSVC)  GCC ‚Äì    Store   Sequential Consistency,  : a.store(val, memory_order_seq_cst); ‚Äî    Clang  MSVC   [LOCK] XCHG reg, [addr],   CPU-Store-buffer  ,    MFENCE.  GCC       MOV [addr], reg  MFENCE. </li><li> <b>RMW (CAS, ADD‚Ä¶) always seq_cst</b> .  Since   RMW (Read-Modify-Write)   x86_64   LOCK,   Store-Buffer,      Sequential-Consistency    .  memory_order  RMW   ,     memory_order_acq_rel. </li><li> <b>LOAD(acquire), STORE(release)</b> .  ,  x86_64  4   (relaxed, consume, acquire, release)     ‚Äì ..  x86_64   acquire-release  ‚Äì       - MESIF(Intel) / MOESI (AMD).               Write Back (    ,   Un-cacheable  Write Combined ‚Äì      Mapped Memory Area from Device ‚Äì      Acquire-semantic). </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As we know nowhere and never can dependent operations be reordered, for example: (Read-X, Write-X) or (Write-X, Read-X) </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Slides from the Herb Sutter performance for x86_64: </font></font><a href="https://onedrive.live.com/view.aspx%3Fresid%3D4E86B0CF20EF15AD!24884%26app%3DWordPdf%26authkey%3D!AMtj_EflYn2507c"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://onedrive.live. com / view.aspx? resid = 4E86B0CF20EF15AD! 24884 &amp; app = WordPdf &amp; authkey =! AMtj_EflYn2507c</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Ä¢ On x86_64, any can not be reordered:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Read-X - Read-Y </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Read-X - Write-Y </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Write-X - Write-Y </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ On x86_64, any can be reordered: Write-X &lt;-&gt; Read-Y. </font><font style="vertical-align: inherit;">To prevent this, the std :: memory_order_seq_cst barrier is used, which can generate 4 variants of code on x86_64 depending on the compiler:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load: MOV (from memory) store: MOV (to memory), </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MFENCE</font></font></b> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load: MOV (from memory) store: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LOCK</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> XCHG (to memory)</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MFENCE</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> + MOV (from memory) store: MOV (to memory)</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">load: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LOCK</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> XADD (0, from memory) store: MOV (to memory)</font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summary table of the memory barriers to processor instructions for architectures (x86_64, PowerPC, ARMv7, ARMv8, AArch64, Itanium) at: </font></font><a href="http://www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">www.cl.cam.ac.uk/~pes20/cpp/cpp0xmappings.html</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> View the actual assembly code for different compilers You can follow the links. </font><font style="vertical-align: inherit;">And also you can choose other architectures: ARM, ARM64, AVR, PowerPC. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">GCC 6.1 (x86_64):</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Load / Store: </font></font><a href="https://godbolt.org/g/xq9ulH"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/xq9ulH</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RMW (fetch_add): </font></font><a href="https://godbolt.org/g/dp1zZ0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/dp1zZ0</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CAS (compare_exchange_weak () / compare_exchange_strong ()): </font></font><a href="https://godbolt.org/g/OMuXmz"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/OMuXmz</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">std :: atomic_flag :: test_and_set (): </font></font><a href="https://godbolt.org/g/7ksl0J"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/7ksl0J</font></font></a> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Clang 3.8 (x86_64): </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Load / Store: </font></font><a href="https://godbolt.org/g/gfpeZW"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/gfpeZW</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">RMW (fetch_add): </font></font><a href="https://godbolt.org/g/afoIQW"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/afoIQW</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CAS (compare_exchange_weak () / compare_exchange_strong ()): </font></font><a href="https://godbolt.org/g/kYXzK6"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/kYXzK6</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">std :: atomic_flag :: test_and_set (): </font></font><a href="https://godbolt.org/g/RD5fJG"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/RD5fJG</font></font></a> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We will also briefly show in the table what effect various memory barriers have on the order relative to CAS (Compare-and-swap) instructions: </font></font><a href="http://en.cppreference.com/w/cpp/atomic/atomic/compare_exchange"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en.cppreference.com/w/cpp/atomic/atomic/compare_exchange</font></font></a> <br><br><img src="https://habrastorage.org/getpro/habr/post_images/429/470/04d/42947004d7a535f2f4a886e487b4a3dd.jpg" alt="image"><br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Examples of reordering memory accesses</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Now we will show more complex examples of reordering from 4 consecutive operations: LOAD, STORE, LOAD, STORE. </font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Blue rectangles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> are atomic operations.</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The dark blue rectangles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> inside the light blue (in examples 3 and 4) are parts of the composite atomic Read-Modify-Write (RMW) instructions consisting of several atomic operations</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">White rectangles</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> are common nonatomic operations.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us give 4 examples in each of which we will show the possible reordering of operations with ordinary variables around operations with atomic variables. </font><font style="vertical-align: inherit;">But only in examples 1 and 3 some reordering of atomic operations between themselves is also possible. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/292/a97/b00/292a97b005473eff1f160134f8c44381.jpg" alt="image"><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The 1st case</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is interesting because several critical sections can be fused into one.</font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The compiler cannot perform such reordering at compile time, but the compiler allows the processor to do this reordering at run time. Therefore, merging critical sections that are executed in different sequences in different threads cannot lead to deadlock, because the initial sequence of instructions will be visible to the processor. Consequently, the processor will try to enter the second critical section in advance, but if it fails, it will continue the execution of the first critical section and, after its full completion, will wait for the entrance to the second critical section. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The 3rd case</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is interesting in that parts of two compound atomic instructions can be reordered: STORE A &lt;-&gt; LOAD B.</font></font><br><br><ul><li>      ,      3-  ASM- (lwarx, add, stwcx),        LL/SC ( <a href="https://en.wikipedia.org/wiki/Load-link/store-conditional">wiki-link</a> ),    : <a href="https://godbolt.org/g/j8uw7n">godbolt.org/g/j8uw7n</a>         ,          . </li></ul><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The 2nd case</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is interesting because std :: memory_order_seq_cst is the strongest barrier and it would seem that it should prohibit any reordering of atomic or non-atomic operations around itself. </font><font style="vertical-align: inherit;">But seq_cst barriers have only one additional property compared to acquire / release ‚Äî only if both atomic operations have a seq_cst barrier, then the sequence of operations is STORE-A (seq_cst); </font><font style="vertical-align: inherit;">LOAD-B (seq_cst); </font><font style="vertical-align: inherit;">cannot be reordered. </font><font style="vertical-align: inherit;">Here are 2 quotes of the C ++ standard:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A strict unified reciprocal order of execution is preserved only for operations with a barrier memory_order_seq_cst, Standard C ++ 11 ¬ß 29.3 (3): </font></font><a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf</font></font></a> <br><blockquote> There <b>shall be a single total order S on all memory_order_seq_cst operations</b> , consistent with the ‚Äúhappens before‚Äù order and modification orders for all affected locations, such that each memory_order_seq_cst operation B that loads a value from an atomic object M observes one of the following values: ‚Ä¶ </blockquote></li><li>        memory_order_seq_cst      memory_order_seq_cst‚Äì   , Standard C++11 ¬ß 29.3 (8): <a href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf">www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/n4606.pdf</a> <br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">[Note: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">memory_order_seq_cst ensures sequential consistency only for a program that is</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> free of data races and </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">uses exclusively memory_order_seq_cst operations. </font><font style="vertical-align: inherit;">Any use of weaker ordering will</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ensure that extreme care is used. </font><font style="vertical-align: inherit;">In particular, memory_order_seq_cst fences ensure a total order only for the fences themselves. </font><font style="vertical-align: inherit;">Fences cannot be in general, can be used to restore sequential consistency for weaker ordering specifications. </font><font style="vertical-align: inherit;">- end note]</font></font></blockquote></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The allowed directions for reordering non ‚Äì seq_cst operations (atomic and non-atomic) around seq_cst operations are the same as for acquire / release: </font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a.load (memory_order_seq_cst)</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - guarantees for non-seq_cst operations the same order as </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">a.load (memory_order_acquire)</font></font></b> </li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b.store (memory_order_seq_cst)</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - guarantees for non-seq_cst-operations the same order as </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">b.store (memory_order_release)</font></font></b> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stricter order is possible, but not guaranteed. </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/e71/ca5/6f3/e71ca56f3a76e9a7ed549b3bf4066049.jpg" alt="image"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the case of seq_cst as well as for acquire and release: nothing going before STORE (seq_cst) can be done after it, and nothing going after LOAD (seq_cst) can be done before it. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But in the opposite direction reordering is possible. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now let us show what changes in the order of instructions the compilers allow for processors, using the example of GCC for x86_64 and PowerPC - code examples in C ++ and generated code in Assembler x86_64 and PowerPC. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The following order changes are possible around memory_order_seq_cst ‚Äì operations:</font></font><br><br><ol><li> <b> x86_64</b>      Store-Load.  Those.     : <br><blockquote> <b>STORE-C(release);</b> LOAD-B(seq_cst); ==&gt; LOAD-B(seq_cst); <b>STORE-C(release);</b> </blockquote>  because   x86_64,  MFENCE   STORE(seq_cst),   LOAD(seq_cst)     LOAD(release)  LOAD(relaxed): <a href="https://godbolt.org/g/BsLqas">godbolt.org/g/BsLqas</a> </li><li> <b> PowerPC</b>      Store-Load, Store-Store  .  Those.     : <br><blockquote> STORE-A(seq_cst); <b>STORE-C(relaxed); LOAD-C(relaxed);</b> ==&gt; <br> <b>LOAD-C(relaxed); STORE-C(relaxed);</b> STORE-A(seq_cst); </blockquote>  because<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">on the PowerPC architecture, for the seq_cst barrier, sync (hwsync) is added only up to STORE (seq_cst) and before LOAD (seq_cst), so all instructions that are between STORE (seq_cst) and LOAD (seq_cst) can be executed before STORE (seq_cst): </font></font><a href="https://godbolt.org/g/dm7tWd"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/dm7tWd</font></font></a> </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We show in more detail an example of 3 variables with semantics: seq_cst and relaxed. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/37e/03f/c6e/37e03fc6eee179dbfa17dd5735a8678c.jpg" alt="image"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What reordering allows the C ++ compiler to do</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++ to ASM for PowerPC: </font></font><a href="https://godbolt.org/g/mEM8T8"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/mEM8T8</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C ++ to ASM for x86_64: </font></font><a href="https://godbolt.org/g/lTNMJ2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">godbolt.org/g/lTNMJ2</font></font></a> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Why are such changes in order possible? </font><font style="vertical-align: inherit;">Because the C ++ compiler generates a assembler code that allows such reordering of x86_64 and PowerPC processors: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/077/0d6/ee0/0770d6ee0621cd122bc14d838092fbda.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What sort of reordering allows different CPUs to do in the absence of assembler barriers between instructions. </font><font style="vertical-align: inherit;">‚ÄúMemory Barriers: a Hardware View for Software Hackers‚Äù Paul E. McKenney June 7, 2010 - Table 5: </font></font><a href="http://www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">www.rdrop.com/users/paulmck/scalability/paper/whymb.2010.06.07c.pdf</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> There is another data sharing feature between threads, which manifests itself in the interaction of 4 threads or more. </font><font style="vertical-align: inherit;">If at least one of the following operations does not use the strictest barrier memory_order_seq_cst, then different threads can see the same changes in a different order.</font></font> For example: <br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If thread-1 changed some value first </font></font></li><li>  -2     </li><li>  -3      -2,       -1 </li><li>  -4       -1,       -2 </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This is possible due to the hardware features of the cache-coherent protocol, additional load / store buffers and the topology of the location of the cores in the processors. In this case, some two cores may see changes made by each other before they see changes made by other cores. For all threads to see changes in the same order, i.e. had a </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">single total order</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (C ++ 11 ¬ß 29.3 (3)) - it is necessary that all operations (LOAD, STORE, RMW) be performed with a memory barrier memory_order_seq_cst: </font></font><a href="http://en.cppreference.com/w/cpp/atomic/memory_order"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en.cppreference.com/w/cpp/atomic/memory_order</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the following example the program never complete with an assert error (z.load ()! = 0);</font></font> because<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in all operations, the strictest memory barrier is used memory_order_seq_cst: </font></font><a href="http://coliru.stacked-crooked.com/a/52726a5ad01f6529"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/52726a5ad01f6529</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In the figure, we will show what order changes are possible for Acquire-Release semantics and for Sequential semantics using the example of 4 streams:</font></font><br><br><ol><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Acuire-Release</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ It is possible to change the order of ordinary variables in permitted directions </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ It is possible to change the order of atomic variables with Acquire-Release-semantics</font></font></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sequential</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ It is possible to change the order of ordinary variables in allowed directions </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is impossible to</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> change the order of atomic variables with Sequential-semantics</font></font></li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/5d4/e08/f98/5d4e08f98ea0cf0581a72f67fe4f76aa.jpg" alt="image"><br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Active spin-lock and recursive-spin-lock locks</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us show how memory barriers are applied to atomic operations using the example of implementing own active locks: spin-lock and recursive-spin-lock. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, we will need these locks to block individual rows (row-lock) of a table instead of locking the entire table (table-lock) - this will increase the degree of parallelism and increase performance, because different threads will be able to work with different rows in parallel, without blocking the entire table. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The number of synchronization objects provided by the operating system may be limited. The number of rows in a table can be millions or billions, many operating systems do not allow creating so many mutexes. And the amount of spin-lock can be any, as far as RAM allows it - therefore, they can be used to block each individual line.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In fact, spinlock is + 1 byte for each row (row), or +17 bytes when using recursive-spin-lock (1 byte for the flag + 8 bytes for the recursion counter + 8 bytes for the thread number thread_id). </font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have already given an example of implementing and using spinlock: [24] </font></font><a href="http://coliru.stacked-crooked.com/a/92b8b9a89115f080"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/92b8b9a89115f080</font></font></a> </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will show an example of the implementation and use of recursive_spinlock: [25] </font></font><a href="http://coliru.stacked-crooked.com/a/eae6b7da971810a3"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/eae6b7da971810a3</font></font></a> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The main difference between recursive_spinlock and ordinary spinlock is that recursive_spinlock can be repeatedly blocked by the same thread, i.e. </font><font style="vertical-align: inherit;">supports recursive nested locks. </font><font style="vertical-align: inherit;">Similarly, std :: recursive_mutex and std :: mutex are different. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">An example of nested locks:</font></font><br><br><ul><li> spinlock_t ‚Äì    (deadlock) ‚Äì   : [26] <a href="http://coliru.stacked-crooked.com/a/d3b93315270fd367">coliru.stacked-crooked.com/a/d3b93315270fd367</a> </li><li> recursive_spinlock_t ‚Äì   ‚Äì   shared_value = 50: [27] <a href="http://coliru.stacked-crooked.com/a/875ad2754a007037">coliru.stacked-crooked.com/a/875ad2754a007037</a> </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's see how recursive_spinlock_t works. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you try to run this code in the MSVC 2013 compiler, you will get a very strong slowdown due to the std :: this_thread :: get_id () function: </font></font><a href="https://connect.microsoft.com/VisualStudio/feedback/details/1558211"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">connect.microsoft.com/VisualStudio/feedback/details/1558211</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We will modify the recursive_spinlock_t class to cached the thread-id in the variable __declspec (thread) is an analogue of thread_local from the standard C ++ 11. This example will show good performance in MSVC 2013: [28] </font></font><a href="http://coliru.stacked-crooked.com/a/3090a9778d02f6ea"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/3090a9778d02f6ea</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> This is a temporary fix for the old MSVC 2013, so we will not think about the beauty of this solution.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is believed that in most cases repeated (recursive) locking of a mutex is a design error, but in our case it may be a slow but working code. </font><font style="vertical-align: inherit;">Secondly, everyone is mistaken, and with nested recursive locks, recursive_spinlock_t will work much slower, and spinlock_t will hang forever - it's better for you to decide. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the case of using our thread safe, safe_ptr &lt;T&gt; is indicated - both examples are quite logical, but the second will only work with recursive_spinlock:</font></font><br><br><pre> <code class="cpp hljs">safe_int_spin_recursive-&gt;second++; <span class="hljs-comment"><span class="hljs-comment">// spinlock &amp; recursive_spinlock safe_int_spin_recursive-&gt;second = safe_int_spin-&gt;second + 1; // only recursive_spinlock</span></span></code> </pre> <br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implement your own high-performance shared-mutex</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As you know, new types of mutexes gradually appeared in C ++ standards: </font></font><a href="http://en.cppreference.com/w/cpp/thread"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">en.cppreference.com/w/cpp/thread</font></font></a> <br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C ++ 11: mutex, timed_mutex, recursive_mutex, recursive_timed_mutex </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C ++ 14: shared_timed_mutex </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> C ++ 17: shared_mutex </font></font></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/fa2/c18/178/fa2c181780b71d5339446fd51028474c.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">shared_mutex is a mutex that allows many threads to read the same data at the same time, if at this moment there are no threads that modify this data. shared_mutex did not appear immediately, because there was a debate about its performance compared with the usual std :: mutex. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The classic implementation of shared_mutex with a counter for the number of readers showed an advantage in speed only when many readers held the lock for a long time - i.e. when read for a long time. With short reads, shared_mutex only slowed down the program and complicated the code. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But are all shared_mutex implementations so slow with short reads?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The main reason for the slow performance of std :: shared_mutex and boost :: shared_mutex is the atomic reader count. Each reading thread increments the counter when blocking and decrementing it when unlocking. As a result, the threads constantly drive one cache line between the cores (namely, the exclusive-state (E) drive it). According to the logic of such an implementation, each reading thread counts how many readers are now, but this is absolutely not important for the reading thread, since it is only important for him that there is not a single writer. Moreover, since The increment and decrement are RMW operations, they always generate cleaning Store-Buffer (MFENCE x86_64) and at the level x86_64 asm actually correspond to the slowest semantics of the Sequential Consistency. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's try to solve this problem.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is a type of algorithms that is classified as write contention-free - when there is not a single memory cell in which more than one thread could write. And in a more general case, there is not a single cache line in which more than one stream could write. In order for our shared-mutex with only readers to be classified as write contention-free, it is necessary that readers do not interfere with each other - i.e. so that each reader wrote the flag (what he reads) in his own cell, and removed the flag in the same cell at the end of the reading - without RMW operations. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Write contention-free is the most productive guarantee, which is more productive than the wait-free and lock-free.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is possible that each cell is located in a separate cache line to prevent false-sharing, and it is possible that the cells are tightly packed 16 each in one cache line - the performance loss will depend on the CPU and the number of threads. </font><font style="vertical-align: inherit;">To eliminate false-sharing - each variable should be placed in a separate cache line, for this purpose, there is an alignas (std :: hardware_destructive_interference_size) in C ++ 17, and earlier you can use a processor-dependent solution char tmp [60]; </font><font style="vertical-align: inherit;">(on x86_64 the size of the cache line is 64 bytes):</font></font><br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//struct cont_free_flag_t { // alignas(std::hardware_destructive_interference_size) std::atomic&lt;int&gt; value; // C++17 // }; struct cont_free_flag_t { char tmp[60]; std::atomic&lt;int&gt; value; // tmp[] to avoid false sharing };</span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Before setting the flag, the reader checks if there is a writer - i.e. </font><font style="vertical-align: inherit;">Is there an exclusive lock?</font></font> And since<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">shared-mutex is used in cases where there are very few writers, then all used kernels can have a copy of this value in their cache-L1 in a shared-state (S), from where in 3 cycles they will receive the value of the writer's flag until it changes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For all writers, as usual, there is the same want_x_lock flag - it means that there is a writer at the moment. Threads writers install and remove it using RMW-operations.</font></font><br><br><pre> <code class="cpp hljs">lock(): <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>(!want_x_lock.compare_exchange_weak(flag, <span class="hljs-literal"><span class="hljs-literal">true</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_acquire))</code> </pre> <br><pre> <code class="cpp hljs">unlock(): want_x_lock.store(<span class="hljs-literal"><span class="hljs-literal">false</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">std</span></span>::memory_order_release);</code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But in order for readers not to interfere with each other, for this they need to write information about their shared locks in different memory cells. We will allocate an array for these locks, the size of which we will set to the template parameter, by default equal to 20. When you first call lock_shared (), the stream will be automatically registered - taking a certain place in this array. If there are more threads than the size of the array, the remaining threads will call an exclusive writer lock when calling lock_shared (). Threads are rarely created, and the time taken by the operating system to create them is so long that the time to register a new thread in our object will be negligible. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us make sure that there are no obvious mistakes - by examples we will show that everything works correctly, and then we schematically prove the correctness of our approach.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us give an example of such a quick shared blocking in which readers do not interfere with each other: [30] </font></font><a href="http://coliru.stacked-crooked.com/a/b78467b7a3885e5b"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/b78467b7a3885e5b</font></font></a> <br><br><ol><li> <u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During a shared lock, there can be no object changes. </font></font></u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This line of two recursive shared-lock shows this: assert (readonly_safe_map_string-&gt; at ("apple") == readonly_safe_map_string-&gt; at ("potato")); </font><font style="vertical-align: inherit;">- the values ‚Äã‚Äãof both lines should always be equal, since </font><font style="vertical-align: inherit;">we change 2 strings in std :: map under one eXclusive lock std :: lock_guard</font></font></li><li> <u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">While reading, we do call the lock_shared () function. </font></font></u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let's reduce the cycle to two iterations, remove the lines modifying the data, leaving only the first two inserts into std :: map in the main () function. </font><font style="vertical-align: inherit;">Now add the output of the letter S to the lock_shared () function, and the letter X to the lock () function. </font><font style="vertical-align: inherit;">We see that there are two inserts X first, and then only the letters S - it means that when reading a const-object we call shared_lock (): [31] </font></font><a href="http://coliru.stacked-crooked.com/a/515ba092a46135ae"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/515ba092a46135ae</font></font></a> </li><li> <u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">During the changes, we actually call the lock () function. </font></font></u><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will comment out the reading and leave only the operations for changing the array, now only the letters X are output: [32] </font></font><a href="http://coliru.stacked-crooked.com/a/882eb908b22c98d6"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/882eb908b22c98d6</font></font></a> </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The main task is to ensure that at the same time there can be only one of two states: </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Any number of threads successfully executed lock_shared (), while all threads attempting to perform lock () should go into standby </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> One of the threads successfully executed lock (), and all other threads trying to execute lock_shared () or lock () should go into wait </font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schematically, the table of compatible states looks like this. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae9/8bd/c13/ae98bdc1321409338f78734b40791a56.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consider the algorithm of our contention_free_shared_mutex for cases when 2 threads try to simultaneously perform operations:</font></font><br><br><ul><li> <b>T1-read &amp; T2-read:</b> -    lock_shared() ‚Äì      , ..           ,          - (want_x_lock == false).   ,   ,    ‚Äì   -  ,  CAS-: want_x_lock = true. </li><li> <b>T1-write &amp; T2-write:</b> -           (want_x_lock)      true,   CAS-: want_x_lock.compare_exchange_weak();   ,     recursive_spinlock_t,    . </li><li> <b>T1-read &amp; T2-write:</b> - T1      ,         (want_x_lock),    (true),    ,    (want_x_lock == false)     . </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The thread writer T2 sets the flag (want_x_lock = true), and then waits until all the thread readers clear the blocking flags from their cells. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The flow writer in our scheme takes precedence over the reader. And if they simultaneously set their own blocking flags, then the stream-reader by the next operation checks if there is a flow-writer (want_x_lock == true), and if there is, then the reader cancels its blocking. The thread writer sees that there are no more locks from readers and successfully completes the blocking function. The global order of these locks is maintained by the Sequential Consistency semantics (std :: memory_order_seq_cst). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schematically, the interaction of two threads (reader and writer) is as follows. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b2/3a3/23b/5b23a323b6a1c9e13ade90bbd82ed98b.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Full size:</font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">habrastorage.org/getpro/habr/post_images/5b2/3a3/23b/5b23a323b6a1c9e13ade90bbd82ed98b.jpg</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> In both functions, lock_shared () and lock (), for both operations 1. and 2. use </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">std :: memory_order_seq_cst</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> - i.e. </font><font style="vertical-align: inherit;">for these operations, a single order is guaranteed for all streams (single total order).</font></font><br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Automatic deregistration of stream in cont-free shared-mutex</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When a thread first accesses our lock, it is registered. And when this thread ends or the lock is removed, the registration should be canceled. </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But now let's see what happens if 20 threads work with our mutex, then these threads end and try to register new 20 threads, provided that the blocking array is equal to 20: [33] </font></font><a href="http://coliru.stacked-crooked.com/a/f1ac55beedd31966"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/ f1ac55beedd31966</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As you can see, the first 20 threads successfully registered, but the next 20 threads could not register (register_thread = -1) and had to use the writer's exclusive lock, despite the fact that the previous 20 threads had already left and no longer use the lock.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To solve this problem, we add an automatic deregistration of a stream when the stream is deleted. If the thread has worked with many such locks, then at the moment of the thread termination, the registration should be canceled in all such locks. And if at the moment of deleting a thread there are locks that are already deleted at the moment, then there should not be an error due to an attempt to cancel the registration in a nonexistent block. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example: [34] </font></font><a href="http://coliru.stacked-crooked.com/a/4172a6160ca33a0f"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/4172a6160ca33a0f</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As you can see, 20 threads first registered, and after deleting them and creating new 20 threads, they were also able to register with the same numbers register_thread = 0 - 19 (see the output (output) of the example).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will show that even if the threads worked with a lock, and then the lock was removed, then at the end of the threads there will not be an error due to an attempt to cancel registration in a nonexistent blocking object: [35] </font></font><a href="http://coliru.stacked-crooked.com/a/d2e5a4ba1cd787da"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/d2e5a4ba1cd787da</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> We set the timers to create 20 threads, execute reading using our lock and fall asleep for 500ms, but at this time the object contfree_safe_ptr containing our contention_free_shared_mutex lock was removed after 100ms, and only after that 20 threads woke up and ended. There was no error when completing them due to cancellation of registration in the remote object of blocking.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As a small addition, we will support MSVS2013 so that the owners of the old compiler can also see the example. </font><font style="vertical-align: inherit;">Add a simplified support for registering threads, but without the possibility of unregistering a stream </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We will show the final result in the form of an example in which all the above thoughts are taken into account. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Example: [36] </font></font><a href="http://coliru.stacked-crooked.com/a/0a1007765f13aa0d"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/0a1007765f13aa0d</font></font></a> <br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The correct functioning of the algorithm and the selected barriers</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Above, we conducted tests that showed no obvious errors. But in order to prove operability it is necessary to create a scheme of possible changes in the order of operations and possible states of variables. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Exclusive lock lock () / unlock () is almost as simple as in the case of recursive_spinlock_t, so we will not consider it in detail. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The competition of the reader stream for lock_shared () and the writer stream for lock () was discussed in detail above. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now the main task is to show that lock_shared () in all cases uses at least Acquire-semantic, and unlock_shared () in all cases uses at least Release-semantic. But this is not a requirement for recursive locking / unlocking. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/faf/4c1/75b/faf4c175b21d5e4b890a7045e72b7b79.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will show how these barriers are implemented in our code.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Barriers to </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">lock_shared ()</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> are shown </font><font style="vertical-align: inherit;">schematically with </font><font style="vertical-align: inherit;">red crossed out arrows showing directions in which order change is prohibited: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/bf0/a21/091/bf0a2109192641cac4f1890fe7dbc4f9.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Schematically barriers to </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">unlock_shared ()</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/baf/15d/688/baf15d688e3fae2968eb2fe1f06083fc.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Full size: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hsto.org/files/065/9ce/bd7/0659cebd72424256b6254c57d35c7d07.jpg</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The full size </font><a href=""><font style="vertical-align: inherit;">rasmat ras rasters</font></a><font style="vertical-align: inherit;"> transitions: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hsto.org/files/fa7/4d2/2b7/fa74d22b7cda4bf4b1015ee263cad9ee.jpg</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> present also a block diagram of this same function lock_shared () </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c16/eca/6a7/c16eca6a73904282017f80dbf17cee9a.jpg" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">picture in full size: </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">hsto.org/files/c3a/abd/4e7/c3aabd4e7cfa4e9db55b2843467d878f.jpg</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in oval blocks are marked strict sequence of operations:</font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> First, the operation is executed - in red </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Then the operation is performed - in purple </font></font></li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Green color indicates changes that can be executed in any order, because </font><font style="vertical-align: inherit;">these changes do not block our ‚Äúshared-mutex‚Äù, but only increase the recursion nesting counter ‚Äî these changes are important only for local use.</font></font> Those.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> these green operations are not the actual entry to the lock. </font></font><br><br>  Since<font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 2 conditions are fulfilled - it is considered that all necessary side-effects from multithreading are taken into account: </font></font><br><br><ol><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The moment of making a decision about entering a lock always has a semantics of no less than ‚Äúacquire‚Äù: </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ want_x_lock.load (std :: memory_order_seq_cst) </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚Ä¢ want_x_lock.compare_exchange_weak (flag, true, std :: memory_order_seq_cst)</font></font></li><li>   <b></b> (1-),     <b></b> (2-)      .            . </li></ol><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further, the correctness of the algorithm can be checked by simply comparing the results of these operations and their sequence with the logic of the algorithm. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All other functions of our contention_free_shared_mutex blocking are more obvious from the point of view of multi-threaded execution logic. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I also note that when re-blocking (recursively), atomic operations do not have to have a std :: memory_order_acquire barrier (as shown in the figure), it is enough to set std :: memory_order_relaxed, because this is not the actual entry to the lock - we are already blocked . But this does not add much speed and can complicate understanding.</font></font><br><br><h2>  <b>How to use</b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us show an example of using contention_free_shared_mutex &lt;&gt; in C ++ as a highly optimized shared_mutex. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Download this code for Linux (GCC 6.3.0) and Windows (MSVS 2015/13) at the following link: </font></font><a href="https://github.com/AlexeyAB/object_threadsafe/tree/master/contfree_shared_mutex"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/object_threadsafe/tree/master/contfree_shared_mutex</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> To compile this example on Clang ++ 3.8.0 for Linux you must change Makefile</font></font><br><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt; iostream &gt; #include &lt; thread &gt; #include &lt; vector &gt; #include "safe_ptr.h" template &lt; typename T &gt; void func(T &amp;s_m, int &amp;a, int &amp;b) { for (size_t i = 0; i &lt; 100000; ++i) { // x-lock for modification { s_m.lock(); a++; b++; s_m.unlock(); } // s-lock for reading { s_m.lock_shared(); assert(a == b); // will never happen s_m.unlock_shared(); } } } int main() { int a = 0; int b = 0; sf::contention_free_shared_mutex&lt; &gt; s_m; // 19 threads std::vector&lt; std::thread &gt; vec_thread(20); for (auto &amp;i : vec_thread) i = std::move(std::thread([&amp;]() { func(s_m, a, b); })); for (auto &amp;i : vec_thread) i.join(); std::cout &lt;&lt; "a = " &lt;&lt; a &lt;&lt; ", b = " &lt;&lt; b &lt;&lt; std::endl; getchar(); return 0; }</span></span></span></span></code> </pre> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This code in the online compiler: </font></font><a href="http://coliru.stacked-crooked.com/a/11c191b06aeb5fb6"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">coliru.stacked-crooked.com/a/11c191b06aeb5fb6</font></font></a> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> As you can see our sf :: contention_free_shared_mutex &lt;&gt; mutex is used in exactly the same way as the standard std :: shared_mutex.</font></font><br><br><h2> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Test: std :: shared_mutex vs contention_free_shared_mutex</font></font></b> </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let us give an example of testing on 16 streams for a single Intel Xeon E5-2660 v3 2.6 GHz server processor. </font><font style="vertical-align: inherit;">First of all, we are interested in blue and purple lines:</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> safe_ptr &lt;std :: map, std :: shared_mutex&gt; </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> contfree_safe_ptr &lt;std :: map&gt; </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The source code of the test: </font></font><a href="https://github.com/AlexeyAB/object_threadsafe/tree/master/bench_contfree"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/AlexeyAB/object_threadsafe/tree/master/bench_contfree</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Command line for starting: </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numactl --localalloc --cpunodebind = 0 ./benchmark 16</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If you only have one CPU on the motherboard, then run: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. / benchmark</font></font></b> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Performance of various locks for different ratios of read locks (shared-lock) and write locks (exclusive-lock).</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> % exclusive locks = (% of write operations) </font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> % shared locks = 100 - (% of write operations) </font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(in case of std :: mutex - exclusive-lock locking always works) </font></font><br><br><img src="https://hsto.org/files/3d8/1c1/bbd/3d81c1bbd945413d85eb65be18aa2b0f.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Performance (the bigger - the better), MOps - millions of operations per second</font></font><br><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With 0% of changes, our shared-mutex (as part of contfree_safe_ptr &lt;map&gt;) shows a performance of 34.60 Mops, which </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is 14 times faster</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> than the standard std :: shared_mutex (as part of safe_ptr &lt;map, std :: shared_mutex&gt;), which shows only 2.44 Mops.</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With 15% of changes, our shared-mutex (as part of contfree_safe_ptr &lt;map&gt;) shows a performance of 5.16 Mops, which </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is 7 times faster</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> than the standard std :: shared_mutex (as part of safe_ptr &lt;map, std :: shared_mutex&gt;), which shows only 0.74 Mops.</font></font></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Our contention-free shared-mutex lock works for any number of threads: for the first 36 threads, as contention-free, and for subsequent ones, as an exclusive-lock. As you can see from the graphs, even the ‚Äúexclusive-lock‚Äù std :: mutex works faster than std :: shared_mutex with 15% of changes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The number of threads 36 for contention-free is set by the template parameter and can be changed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Now we will show the median delay for different ratios of the type of locks: read (shared-lock) and write (exclusive-lock). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the test code main.cpp you need to set: const bool measure_latency = true; </font></font><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The command line to run: </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">numactl --localalloc --cpunodebind = 0 ./benchmark 16</font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> If you have only one CPU on the motherboard, then run: </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">./benchmark</font></font></b> <br><br><img src="https://hsto.org/files/3ae/e18/850/3aee188503b14363abb9fe0a069ecfa0.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Median-latency (the lower - the better), microseconds </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Thus, we created a shared lock in which readers do not interfere with each other during locking and unlocking, unlike std :: shared_timed_mutex and boost :: shared_mutex. But we have an additional allocation for each stream: 64 bytes in the array of locks + 24 bytes is occupied by the unregister_t structure for unregistering + the element pointing to this structure from hash_map. About 100 bytes per stream.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A deeper problem is the ability to scale. For example, if you have 8 CPUs (Intel Xeon Processor E7-8890 v4) with 24 cores (48 threads each HyperThreading), then there are 384 logical cores in total. Before writing, each thread writer must read 24,576 bytes (64 bytes from each of the 384 cores), but you can read them in parallel, this is certainly better than waiting for one cache line to go sequentially from each of the 384 streams to each, as in ordinary std :: shared_timed_mutex and boost :: shared_mutex (for any type of unique / shared locking). But parallelization per 1000 cores and more is usually implemented through a different approach, and not through a call to an atomic operation to process each message. All options discussed above: atomic operations, active locks, lock-free data structures ‚Äî all this is necessary for low latency (0,5 - 5 usec) individual messages.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">For high rates of operations per second, i.e. for high system performance and scalability across tens of thousands of logical cores use mass parallelism approaches, ‚Äúhide latency‚Äù and ‚Äúbatch processing‚Äù - batch processing, when messages are sorted (for map) or grouped (for hash_map) and merged with already existing sorted ones or in a grouped array of 50 - 500 usec. As a result, each operation has a delay of 10-100 times more, but these delays occur at the same time in a huge number of threads, resulting in the concealment of ‚Äúhide latency‚Äù delays due to the use of ‚ÄúFine-grained Temporal multithreading‚Äù.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we assume: each message has a delay of 100 times more, but messages are processed 10,000 times more. This is a unit of time 100 times more efficient. Such principles are applied when developing on a GPU. Perhaps in the following articles we will analyze this in more detail.</font></font><br><br><h2>  <b>Conclusion:</b> </h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We have developed our own ‚Äúshared-mutex‚Äù, which does not require that reader threads synchronize with each other, as is done in the standard std :: shared_mutex. </font><font style="vertical-align: inherit;">We strictly proved the correctness of our ‚Äúshared-mutex‚Äù work. </font><font style="vertical-align: inherit;">We also studied atomic operations, memory barriers, and allowed reordering directions in detail for maximum performance optimization. </font><font style="vertical-align: inherit;">Next, we will see how much we were able to increase the performance of multi-threaded programs, compared to highly optimized lock-free algorithms from the libCDS (Concurrent Data Structures library) library: </font></font><a href="https://github.com/khizmax/libcds"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">github.com/khizmax/libcds</font></font></a> </div><p>Source: <a href="https://habr.com/ru/post/328362/">https://habr.com/ru/post/328362/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../328352/index.html">Top 10 Mistakes Made by Django Developers</a></li>
<li><a href="../328354/index.html">Comparison of PVS-Studio C # and static analyzer built into Visual Studio based on the code of the CruiseControl.NET project</a></li>
<li><a href="../328356/index.html">As we did the third internship in iOS and Android development at Redmadrobot</a></li>
<li><a href="../328358/index.html">Fatty programs - we study the factor of memory part B</a></li>
<li><a href="../328360/index.html">Min-Long Chou: What I designed for Uncharted 4. Part 2</a></li>
<li><a href="../328364/index.html">Crystal High Performance Services, Newbie Introduction</a></li>
<li><a href="../328366/index.html">Build 2017: text translation. Day 2</a></li>
<li><a href="../328368/index.html">Distributed data structures (part 2, how it's done)</a></li>
<li><a href="../328370/index.html">Should I buy ECC memory?</a></li>
<li><a href="../328372/index.html">Metrics in machine learning tasks</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>