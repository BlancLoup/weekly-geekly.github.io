<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Visualization of latent semantic analysis results using Python tools</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Formulation of the problem 
 Semantic (semantic) text analysis is one of the key problems of both the theory of creating artificial intelligence syste...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Visualization of latent semantic analysis results using Python tools</h1><div class="post__text post__text-html js-mediator-article"><h3>  Formulation of the problem </h3><br>  Semantic (semantic) text analysis is one of the key problems of both the theory of creating artificial intelligence systems relating to the processing of natural language (Natural Language Processing, NLP) and computational linguistics.  The results of semantic analysis can be used to solve problems in such areas as, for example, psychiatry (to diagnose patients), political science (prediction of election results), trade (analysis of the demand for certain goods based on comments on this product), philology (analysis of copyright texts ), search engines, automatic translation systems.  Google search engine is fully built on semantic analysis. <br><br>  Visualization of the results of semantic analysis is an important stage of its implementation because it can provide quick and effective decision-making based on the results of the analysis. <br><br>  Analysis of publications in the network on latent semantic analysis (LSA) shows that the visualization of the analysis results is given only in two publications [1,2] in the form of a two coordinate graph of the semantic space with plotted coordinates of words and documents.  Such visualization does not allow to unambiguously identify groups of close documents and to assess the level of their semantic connection by the words belonging to the documents.  Although in my publication titled ‚ÄúComplete latent semantic analysis using Python tools‚Äù [1], an attempt was made to use cluster analysis of latent semantic analysis results, however, only cluster marks and centroid coordinates for groups of words and documents without visualization were identified. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Visualization of LSA results by cluster analysis </h3><br>  We present all the steps of the LSA, including visualization of the results by the clustering method: <br><br><ol><li>  Stop words are excluded from the analyzed documents.  These are words that occur in every text and do not carry a semantic load; they are, first of all, all unions, particles, prepositions, and many other words. </li><li>  From the analyzed documents it is necessary to filter the numbers, individual letters and punctuation marks. </li><li>  Delete the words that are found in all documents only once.  This does not affect the final result, but greatly simplifies mathematical calculations. </li><li>  With all the words from the documents should be carried out the operation of stemming - getting the basis of the word. </li><li>  Create a frequency matrix indexable lov.  In this matrix, the rows correspond to the indexed words, and the columns correspond to the documents.  Each cell of the matrix must indicate how many times the word occurs in the corresponding document. </li><li>  The resulting frequency matrix should be normalized.  The standard way to normalize the matrix TF-IDF [3]. </li><li>  The next step is a singular decomposition of the resulting matrix.  Singular decomposition [4];  - is a mathematical operation, decomposing the matrix into three components.  Those.  We represent the initial matrix M in the form: <br>  <b>M = U * W * V ^ t</b> <br>  where U and V ^ t are orthogonal matrices, and W is a diagonal matrix.  Moreover, the diagonal elements of the matrix W are ordered in decreasing order.  The diagonal elements of the matrix W are called singular numbers. </li><li>  Discard the last columns of the matrix U and the last rows of the matrix V ^ t, leaving only the first 2. These are the X, Y coordinates of each word for the U matrix and the X, Y coordinates for each document in the V ^ t matrix, respectively.  A decomposition of this type is called a two-dimensional singular decomposition. </li><li>  Discard the last columns of the matrix U and the last rows of the matrix V ^ t, leaving only the first 3. These are respectively the X, Y, Z coordinates of each word for the U matrix and the X, Y, Z coordinates for each document in the V ^ t matrix.  The decomposition of this type is called the three-dimensional singular decomposition. </li><li>  Preparation of source data in the form of nested lists of X, Y coordinates for two columns of the matrix U of words and two rows of the matrix of V ^ t documents. </li><li>  The construction of diagrams of Euclidean distances between the rows of two columns of the matrix U and the columns of two rows of the matrix V ^ t. </li><li>  Isolation of the number of clusters [5,6,7]. </li><li>  Construction of diagrams and dendrograms. </li><li>  Preparation of source data in the form of nested lists of coordinates X, Y, Z for three columns of the matrix U of words and three rows of the matrix V ^ t of documents. </li><li>  The allocation of the number of clusters. </li><li>  Construction of diagrams and dendrograms. </li><li>  Comparison of cluster analysis results for two-dimensional and three-dimensional singular decomposition of the normalized frequency matrix of words and documents. </li></ol><br>  To implement the above steps, a special program was developed in which the same test set of documents was used for comparison as in the publication [2]. <br><br><div class="spoiler">  <b class="spoiler_title">The program for visualizing the results of LSA</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#!/usr/bin/env python # -*- coding: utf-8 -*- import numpy from numpy import * import nltk import scipy from nltk.corpus import brown from nltk.stem import SnowballStemmer from scipy.spatial.distance import pdist from scipy.cluster import hierarchy import matplotlib.pyplot as plt stemmer = SnowballStemmer('russian') stopwords=nltk.corpus.stopwords.words('russian') #      docs =[ "      WikiLeaks",#  ‚Ññ 0 "      ,  ",#  ‚Ññ1 "      19 ",#  ‚Ññ2 "      Wikileaks  ",#  ‚Ññ3 "     ",#  ‚Ññ4 "      Wikileaks",#  ‚Ññ5 "         ",#  ‚Ññ6 "    WikiLeaks, ,  ",#  ‚Ññ7 "        "#  ‚Ññ8 ] word=nltk.word_tokenize((' ').join(docs))#      n=[stemmer.stem(w).lower() for w in word if len(w) &gt;1 and w.isalpha()]#      stopword=[stemmer.stem(w).lower() for w in stopwords]# - fdist=nltk.FreqDist(n) t=fdist.hapaxes()#       #    d={};c=[] for i in range(0,len(docs)): word=nltk.word_tokenize(docs[i]) word_stem=[stemmer.stem(w).lower() for w in word if len(w)&gt;1 and w.isalpha()] word_stop=[ w for w in word_stem if w not in stopword] words=[ w for w in word_stop if w not in t] for w in words: if w not in c: c.append(w) d[w]= [i] elif w in c: d[w]= d[w]+[i] a=len(c); b=len(docs) A = numpy.zeros([a,b]) c.sort() for i, k in enumerate(c): for j in d[k]: A[i,j] += 1 # TF-IDF    wpd = sum(A, axis=0) dpw= sum(asarray(A &gt; 0,'i'), axis=1) rows, cols = A.shape for i in range(rows): for j in range(cols): m=float(A[i,j])/wpd[j] n=log(float(cols) /dpw[i]) A[i,j] =round(n*m,2) #     U, S,Vt = numpy.linalg.svd(A) rows, cols = U.shape for j in range(0,cols): for i in range(0,rows): U[i,j]=round(U[i,j],4) print(' 2    U ') for i, row in enumerate(U): print(c[i], row[0:2]) res1=-1*U[:,0:1]; res2=-1*U[:,1:2] data_word=[] for i in range(0,len(c)):#         data_word.append([res1[i][0],res2[i][0]]) plt.figure() plt.subplot(221) dist = pdist(data_word, 'euclidean')#    ( ) plt.hist(dist, 500, color='green', alpha=0.5)#    Z = hierarchy.linkage(dist, method='average')#   plt.subplot(222) hierarchy.dendrogram(Z, labels=c, color_threshold=.25, leaf_font_size=8, count_sort=True,orientation='right') print(' 2    Vt ') rows, cols = Vt.shape for j in range(0,cols): for i in range(0,rows): Vt[i,j]=round(Vt[i,j],4) print(-1*Vt[0:2, :]) res3=(-1*Vt[0:1, :]);res4=(-1*Vt[1:2, :]) data_docs=[];name_docs=[] for i in range(0,len(docs)): name_docs.append(str(i)) data_docs.append([res3[0][i],res4[0][i]]) plt.subplot(223) dist = pdist(data_docs, 'euclidean') plt.hist(dist, 500, color='green', alpha=0.5) Z = hierarchy.linkage(dist, method='average') plt.subplot(224) hierarchy.dendrogram(Z, labels=name_docs, color_threshold=.25, leaf_font_size=8, count_sort=True) #plt.show() print(' 3    U ') for i, row in enumerate(U): print(c[i], row[0:3]) res1=-1*U[:,0:1]; res2=-1*U[:,1:2];res3=-1*U[:,2:3] data_word_xyz=[] for i in range(0,len(c)): data_word_xyz.append([res1[i][0],res2[i][0],res3[i][0]]) plt.figure() plt.subplot(221) dist = pdist(data_word_xyz, 'euclidean')#    ( ) plt.hist(dist, 500, color='green', alpha=0.5)#   Z = hierarchy.linkage(dist, method='average')#   plt.subplot(222) hierarchy.dendrogram(Z, labels=c, color_threshold=.25, leaf_font_size=8, count_sort=True,orientation='right') print(' 3    Vt ') rows, cols = Vt.shape for j in range(0,cols): for i in range(0,rows): Vt[i,j]=round(Vt[i,j],4) print(-1*Vt[0:3, :]) res3=(-1*Vt[0:1, :]);res4=(-1*Vt[1:2, :]);res5=(-1*Vt[2:3, :]) data_docs_xyz=[];name_docs_xyz=[] for i in range(0,len(docs)): name_docs_xyz.append(str(i)) data_docs_xyz.append([res3[0][i],res4[0][i],res5[0][i]]) plt.subplot(223) dist = pdist(data_docs_xyz, 'euclidean') plt.hist(dist, 500, color='green', alpha=0.5) Z = hierarchy.linkage(dist, method='average') plt.subplot(224) hierarchy.dendrogram(Z, labels=name_docs_xyz, color_threshold=.25, leaf_font_size=8, count_sort=True) plt.show()</span></span></code> </pre> <br>  The result of the program <br>  The first 2 columns of the orthogonal matrix U of words <br>  wikileaks [-0.0741 0.0991] <br>  arrest [-0.023 0.0592] <br>  British [-0.023 0.0592] <br>  handed [-0.0582 -0.5008] <br>  Nobel [-0.0582 -0.5008] <br>  founder [-0.0804 0.1143] <br>  polits [-0.0337 0.0846] <br>  Prem [-0.0582 -0.5008] <br>  proto [-0.5954 0.0695] <br>  countries [-0.3261 -0.169] <br>  court [-0.3965 0.1488] <br>  usa [-0.5954 0.0695] <br>  Ceremony [-0.055 -0.3875] <br>  The first 2 rows of the orthogonal matrix of Vt documents <br>  [[0.0513 0.6952 0.1338 0.045 0.0596 0.2102 0.6644 0.0426 0.0566] <br>  [-0.1038 -0.1495 0.5166 -0.0913 0.5742 -0.1371 0.0155 -0.0987 0.5773]] <br>  The first 3 columns of the orthogonal matrix U of words <br>  wikileaks [-0.0741 0.0991 -0.4372] <br>  arrest [-0.023 0.0592 -0.3241] <br>  British [-0.023 0.0592 -0.3241] <br>  handed [-0.0582 -0.5008 -0.1117] <br>  Nobel [-0.0582 -0.5008 -0.1117] <br>  founder [-0.0804 0.1143 -0.5185] <br>  police [-0.0337 0.0846 -0.4596] <br>  Prem [-0.0582 -0.5008 -0.1117] <br>  proto [-0.5954 0.0695 0.1414] <br>  countries [-0.3261 -0.169 0.0815] <br>  court [-0.3965 0.1488 -0.1678] <br>  United States [-0.5954 0.0695 0.1414] <br>  Ceremony [-0.055 -0.3875 -0.0802] <br>  The first 3 rows of the orthogonal matrix of Vt documents <br>  [[0.0513 0.6952 0.1338 0.045 0.0596 0.2102 0.6644 0.0426 0.0566] <br>  [-0.1038 -0.1495 0.5166 -0.0913 0.5742 -0.1371 0.0155 -0.0987 0.5773] <br>  [0.5299 -0.0625 0.0797 0.4675 0.1314 0.3714 -0.1979 0.5271 0.1347]] <br></div></div><br>  Diagrams and dendrograms of two-dimensional singular decomposition of the normalized frequency matrix of words and documents. <br><br><img src="https://habrastorage.org/web/73b/ab8/9d1/73bab89d14a140e48fbafa91003de195.JPG"><br><br>  Obtained a clear visualization of the proximity of documents (see docs) and the belonging of words to documents. <br><br>  Charts and dendrograms of the three-dimensional singular decomposition of the normalized frequency matrix of words and documents. <br><br><img src="https://habrastorage.org/web/32f/019/61b/32f01961bb8d4db1a915c616ff4c2730.png"><br><br>  The transition to the three-dimensional singular decomposition of the normalized frequency matrix of words and documents for the given set of documents (see docs) does not qualitatively change the result of the LSA analysis. <br><br><h3>  findings </h3><br>  The given LSA visualization technique, in my opinion, is a successful addition to the LSA itself and deserves the attention of developers. <br>  Thank you all for your attention! <br><br><h3>  Links </h3><br><ol><li>  <a href="https://habrahabr.ru/post/323516/">Complete latent semantic analysis with Python tools.</a> </li><li>  <a href="https://habrahabr.ru/post/197238/">Latent semantic analysis and search in Python.</a> </li><li>  <a href="https://ru.wikipedia.org/wiki/TF-IDF">TF-IDF-Wikipedia.</a> </li><li>  <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25BD%25D0%25B3%25D1%2583%25D0%25BB%25D1%258F%25D1%2580%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BB%25D0%25BE%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5">Singular decomposition - Wikipedia.</a> </li><li>  <a href="http://koldunov.net/%3Fp%3D357">Visualization of cluster analysis in Python (hcluster and matplotlib modules).</a> </li><li>  <a href="http://easydan.com/arts/2016/hierarchy-clustering/">Hierarchical cluster analysis in the Python programming language.</a> </li><li>  <a href="http://blog.esemi.ru/2012/08/k-means-python.html">Algorithm k-means [Cluster analysis and Python].</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/335668/">https://habr.com/ru/post/335668/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../335658/index.html">Coco Framework - blockchain on a large scale</a></li>
<li><a href="../335660/index.html">Detailed guide to creating and deploying chat on Tornado + Telegram</a></li>
<li><a href="../335662/index.html">Bitmap way to display tile cards</a></li>
<li><a href="../335664/index.html">"300 million books per kilometer": IBM extends the life of magnetic tape</a></li>
<li><a href="../335666/index.html">10 steps to solve problems in programming</a></li>
<li><a href="../335670/index.html">The future of contact centers: omni-channel and customer experience</a></li>
<li><a href="../335672/index.html">The book "Theory and practice of programming languages. Textbook for universities. 2nd ed. Standard 3rd generation "</a></li>
<li><a href="../335674/index.html">Construction of wireless networks of all sizes based on TP-Link equipment</a></li>
<li><a href="../335676/index.html">Bug Bounty: Earn from the mistakes of others</a></li>
<li><a href="../335678/index.html">Three strategies for testing Terraform</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>