<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ceph. Anatomy of a catastrophe</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Ceph is object storage designed to help build a failover cluster. And yet failures happen. Everyone who works with Ceph knows the legend about CloudMo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ceph. Anatomy of a catastrophe</h1><div class="post__text post__text-html js-mediator-article">  Ceph is object storage designed to help build a failover cluster.  And yet failures happen.  Everyone who works with Ceph knows the legend about CloudMouse or Rosreestr.  Unfortunately, sharing negative experiences is not accepted here, the reasons for failures are often silent, and do not allow future generations to learn from the mistakes of others. <br><br>  Well, let's set up a test, but close to real cluster and analyze the catastrophe by bone.  We measure all performance drawdowns, find memory leaks, and analyze the process of restoring maintenance.  And all this under the leadership of Artemy Kapitula, who, having spent almost a year studying the pitfalls, made the cluster performance not drop to zero with failure, and the latency did not jump to indecent values.  And I got a red graph, which is much better. <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>  Below you will find a video and text version of one of the best <a href="https://devopsconf.io/">DevOpsConf Russia</a> 2018 reports. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/_fWYUl2QsoI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><a name="habracut"></a><br>  <strong>About speaker:</strong> Artemy Kapitula, system architect RCNTEC.  The company offers solutions for IP telephony (collaboration, organization of a remote office, software-defined storage systems and power management / distribution systems).  The company mainly operates in the enterprise sector, and therefore is not very well known in the DevOps market.  Nevertheless, a certain experience has been gained with Ceph, which in many projects is used as a basic element of the storage infrastructure. <br><br>  <strong>Ceph is a software-defined repository with many software components.</strong> <br><img src="https://habrastorage.org/webt/dw/ow/hm/dwowhmqvjfugd0u-ljhhz3fy2ji.png"><br><br>  In the diagram: <br><br><ul><li>  The upper level is an internal cluster network over which the cluster itself communicates; </li><li>  The lower level is Ceph itself, a set of internal Ceph daemons (MON, MDS and OSD) that store data. </li></ul><br>  All data, as a rule, is replicated, I specifically identified three groups in the diagram, each with three OSDs, and in each of these groups there is usually one replica of data.  As a result, the data is stored in three copies. <br><br>  Above, the cluster network is the network through which Ceph clients access data.  Through it, clients communicate with the monitor, with MDS (who needs it) and with the OSD.  Each client works with each OSD and with each monitor independently.  Therefore, the <strong>system is devoid of a single point of failure</strong> , which is very pleasing. <br><br><h2>  Customers <br></h2><br>  ‚óè S3 clients <br><br>  S3 is an API for HTTP.  S3 clients work over HTTP and connect to Ceph Rados Gateway (RGW) components.  They almost always communicate with the component through a dedicated network.  In this network (I called it S3 network) only HTTP is used, exceptions are rare. <br><br>  ‚óè Hypervisor with virtual machines <br><br>  This group of clients is often used.  They work with monitors and OSD, from which they receive general information about the cluster status and data distribution.  For data, these clients directly go to the OSD daemons through the Cluster public network. <br><br>  ‚óè RBD clients <br><br>  There are also physical BR metals hosts, on which, as a rule, Linux.  They are RBD clients and access images stored within a Ceph cluster (virtual machine disk images). <br><br>  ‚óè CephFS clients <br><br>  The fourth group of clients, which not many people still have, but which is of increasing interest, are the clients of the CephFS cluster file system.  The CephFS cluster system can be mounted simultaneously from many nodes, and all nodes access the same data from each OSD.  That is, there are no Gateways per se (Samba, NFS, and others).  The problem is that such a client can only be Linux, with a fairly modern version. <br><img src="https://habrastorage.org/webt/fw/nm/xc/fwnmxcaiig0yy6tkofrljqri3ck.png"><br><br>  Our company works on the corporate market, and there ESXi, HyperV and others rule the ball.  Accordingly, the Ceph cluster, which is somehow used in the corporate sector, is required to support the relevant techniques.  We didn‚Äôt have enough of this at Ceph, so we had to refine and expand the Ceph cluster with our components, in fact, building up something more than Ceph, our platform for storing data. <br><br>  In addition, clients in the corporate sector are not on Linux, and for the most part, Windows, occasionally Mac OS, cannot go into the Ceph cluster themselves.  They have to run through some gateways, which in this case become narrow places. <br><br>  We had to add all these components, and we got a somewhat wider cluster. <br><img src="https://habrastorage.org/webt/p2/tg/j2/p2tgj2qtpzrzsnst5bophkclywi.png"><br><br>  We have two central components - a <strong>group of SCSI Gateways</strong> , which provide access to data in a Ceph cluster via FibreChannel or iSCSI.  These components are used to connect the HyperV and ESXi to the Ceph cluster.  PROXMOX clients still work in their native way - through RBD. <br><br>  We do not allow file clients directly to the cluster network; they are allocated several fault-tolerant gateways.  Each Gateway provides access to the file cluster system via NFS, AFP or SMB.  Accordingly, almost any client, be it Linux, FreeBSD, or not just a client, server (OS X, Windows), gets access to CephFS. <br><br>  In order to manage all of this, we had to actually develop our own ceph orchestra and all our components, which are numerous.  But to talk about it now makes no sense, because this is our development.  Most of them will probably be interested in the ‚Äúnaked‚Äù Ceph itself. <br><br>  Ceph is used in many places, and in some places failures occur periodically.  Surely everyone who works with Ceph knows the legend about CloudMouse.  This is a terrible urban legend, but everything is not as bad as it seems.  There is a new tale of Rosreestr.  Ceph was spinning everywhere, and his refusal was everywhere.  Somewhere it ended fatally, somewhere we managed to quickly eliminate the consequences. <br><br>  Unfortunately, it is not customary to share negative experiences with us, everyone is trying to hide the relevant information.  Foreign companies are a little more open, in particular, in DigitalOcean (a well-known provider that distributes virtuals), Ceph was also denied for almost a day, it was April 1 - a wonderful day!  Some of the reports they posted, below a brief log. <br><img src="https://habrastorage.org/webt/qo/sb/ds/qosbdsczlkvzh-zqsfvid86er5u.png"><br><br>  The problems started at 7 in the morning, at 11 they understood what was happening, and began to eliminate the failure.  To do this, they identified two teams: one for some reason ran through the servers and installed memory there, and the second for some reason manually started one server after another and carefully monitored all servers.  Why?  We are all used to everything turned on with one click. <br><br>  <em>What basically happens in a distributed system when it is effectively built and works almost at the limit of its abilities?</em> <br><br>  To answer this question, we need to see how the Ceph cluster works and how the failure occurs. <br><img src="https://habrastorage.org/webt/ln/ks/rd/lnksrda1mb-lmfbymyacym1f8aw.png"><br><br><h2>  Ceph crash scenario <br></h2><br>  At first the cluster is working fine, everything is going great.  Then something happens, after which the OSD daemons, where the data is stored, lose contact with the central components of the cluster (monitors).  At this point, a timeout occurs and the entire cluster gets a stake.  The cluster costs some time, until it realizes that something is wrong with it, and after that, it corrects its internal knowledge.  After that, customer service is restored to some extent, and the cluster is again operating in degraded mode.  And the funny thing is that it works faster than in normal mode - this is an amazing fact. <br><br>  Then we eliminate the failure.  Suppose we have lost power, the rack was cut down completely.  Electricians came running, everything was restored, power was supplied, the servers were turned on, and here <strong>the fun began</strong> . <br><br><blockquote>  Everyone is used to the fact that when the server fails, everything becomes bad, and when we turn on the server, everything becomes good.  Here everything is completely wrong. <br></blockquote><br>  The cluster practically stops, performs primary synchronization, and then begins a smooth, slow recovery, gradually leaving for normal mode. <br><img src="https://habrastorage.org/webt/ml/r_/i3/mlr_i3llw-lsdaybp4vbedxeuhi.png"><br><br>  Above the performance graph of the Ceph cluster as the failure progresses.  Please note that here the very intervals that we talked about are very clearly visible: <br><br><ul><li>  Normal operation up to about 70 seconds; </li><li>  Fail to a minute to about 130 seconds; </li><li>  The plateau, which is noticeably higher than normal operation, is the work of degraded clusters; </li><li>  Then we include the missing node - this is a training cluster, there are only 3 servers and 15 SSDs.  We start the server to work somewhere around 260 seconds. </li><li>  The server turned on, entered the cluster - IOPS fell. </li></ul><br>  Let's try to figure out what really happened there.  The first thing that interests us is a dip down at the very beginning of the chart. <br><br><h3>  OSD failure <br></h3><br>  Consider an example of a cluster with three racks, several nodes in each.  If the left rack fails, all OSD daemons (not hosts!) Ping themselves with Ceph messages at a certain interval.  If there is a loss of several messages, a message is sent to the monitor "I, OSD is such and such, I can not reach OSD such and such." <br><img src="https://habrastorage.org/webt/zh/1s/ge/zh1sge1ljlclxjmgfygxpyyyc8i.png"><br><br>  In this case, messages are usually grouped by hosts, that is, if two messages from different OSDs arrive on the same host, they are combined into one message.  Accordingly, if OSD 11 and OSD 12 reported that they could not reach OSD 1, it would be interpreted as Host 11 complained about OSD 1. When OSD 21 and OSD 22 reported, it is interpreted as Host 21 dissatisfied with OSD 1 After that, the monitor considers that OSD 1 is in the down state and notifies all about it (by changing the OSD map) of all cluster members, work continues in degraded mode. <br><img src="https://habrastorage.org/webt/uu/-c/1w/uu-c1wnwflbqk6ueyumhohtlvjy.png"><br><br>  So, here is our cluster and failed rack (Host 5 and Host 6).  Turn on Host 5 and Host 6, as the power has appeared, and ... <br><br><h3>  Ceph's internal behavior <br></h3><br>  And now the most interesting - we begin the <strong>initial synchronization of data</strong> .  Since there are many replicas, they must be synchronous and in the same version.  In the process of starting OSD: <br><br><ul><li>  OSD reads available versions, existing history (pg_log - to determine the current versions of objects). </li><li>  Then it determines which OSD the latest versions of the degraded objects (missing_loc) are on, and which are the laggards. </li><li>  Where backward versions are stored, you need to synchronize, and new versions can be used as reference ones for reading and writing data. </li></ul><br>  The story that is collected from all OSDs is used, and this story can be quite a lot;  the actual location of the set of objects in the cluster is determined, where the corresponding versions lie.  How many objects are in a cluster, so many records are obtained, if the cluster has been standing in degraded mode for a long time, then the story is big. <br><br>  <strong>For comparison: the</strong> typical size of an object when we work with an RBD image is 4 MB.  When we work in erasure coded - 1MB.  If we have a 10 TB disk, we get a million megabyte objects on the disk.  If we have 10 disks in the server, then there are already 10 million objects, if 32 disks (we build an effective cluster, we have a dense location), then 32 million objects that need to be kept in memory.  And in fact, information about each object is stored in several copies, because each copy says that in this place it lies in this version, and in this - in this. <br><br>  It turns out a huge amount of data that is placed in RAM: <br><br><ul><li>  the more objects, the greater the history of the missing_loc; </li><li>  the more PG - the more pg_log and OSD map; </li></ul><br>  Besides: <br><br><ul><li>  the larger disk size; </li><li>  the higher the density (the number of disks in each server); </li><li>  the higher the load on the cluster and the faster your cluster; </li><li>  the longer the OSD is down (in the Offline state); </li></ul><br>  in other words, the <strong>steeper the cluster we have built, and the longer the part of the cluster did not respond, the more RAM will be required at the start</strong> . <br><br><h2>  Extreme optimizations - the root of all evil <br></h2><br><blockquote>  <em>"... and to the bad boys and girls at night comes the black OOM and kills all the processes right and left"</em> <br><br>  City sysadmin legend <br></blockquote><br>  So, a lot of RAM is required, memory consumption is growing (we started right away in a third of the cluster) and the system can in theory go to SWAP, if you have created it, of course.  I think there are quite a few people who think that SWAP is bad and they don‚Äôt create it: ‚ÄúWhy?  We have a lot of memory! ‚ÄùBut this is the wrong approach. <br><br>  If the SWAP file is not created in advance, since it was decided that Linux will work more efficiently, then sooner or later it will happen out of memory killer (OOM-killer) one unlucky one.  We know what an optimistic location is - we request a memory, we are promised it, we say: ‚ÄúNow give it to us‚Äù, in response: ‚ÄúBut no!‚Äù - and out of memory killer. <br><br>  This is Linux full-time work, if it is not configured in the field of virtual memory. <br><br>  The process gets out of memory killer and falls out quickly and ruthlessly.  In this case, no other processes that he died, do not know.  He did not have time to notify anyone about anything, he was simply terminated. <br><br>  Then, of course, the process will restart - we have systemd, it also starts, if necessary, OSD, which have fallen.  Fallen OSD start, and ... a chain reaction begins. <br><img src="https://habrastorage.org/webt/9p/s8/4z/9ps84zkjtmuamxyllkcgffsgxkq.png"><br><br>  In our case, we started OSD 8 and OSD 9, they started to push everything, but no luck OSD 0 and OSD 5. They flew out of the memory killer and terminated them.  They restarted - read their data, began to synchronize and crush the rest.  Three more were unlucky (OSD 9, OSD 4 and OSD 7).  These three restarted, began to put pressure on the entire cluster, the next pack was unlucky. <br><br>  <strong>The cluster begins to fall apart literally before our eyes</strong> .  Degradation occurs very quickly, and this ‚Äúvery quickly‚Äù is usually expressed in minutes, maximum, in tens of minutes.  If you have 30 knots (10 knots in the rack), and knocked out the rack due to a power failure - after 6 minutes, half of the cluster lies. <br><br>  So, we get something like the following. <br><img src="https://habrastorage.org/webt/1b/hq/bu/1bhqburpjt74vwnpbgqn5ehdhh0.png"><br><br>  In almost every server we have a failed OSD.  And if it is on every server, that is, in every failure domain we have on the failed OSD, then <strong>most of our data is not available</strong> .  Any request is blocked - for writing, for reading - no difference.  Everything!  We got up. <br><br>  What to do in this situation?  More precisely, <strong>what should have been done</strong> ? <br><br>  <strong>Answer:</strong> Do not start the cluster at once, that is, the entire rack, but carefully raise one daemon. <br><br>  But we didn't know that.  We started right away and got what we got.  We have launched in this case one of the four daemon (8, 9, 10, 11), the memory consumption will increase by about 20%.  As a rule, we sustain such a jump.  Then the memory consumption starts to decrease, because some of the structures that were used to hold information about how the cluster has degraded, go away.  That is, part of the Placement Groups has returned to its normal state, and everything that is needed to maintain a degraded state is released ‚Äî <strong>in theory, it is released</strong> . <br><br>  Let's see an example.  The C code on the left and the right is almost identical, the only difference is in constants. <br><img src="https://habrastorage.org/webt/sy/1j/u0/sy1ju0rfqjg507jxvk_4wax9_o4.png"><br><br>  These two examples request a different amount of memory from the system: <br><br><ul><li>  left - 2048 pieces of 1 MB; </li><li>  right - 2097152 pieces of 1 KB. </li></ul><br>  Then both examples are waiting for us to photograph them in the top.  And after pressing ENTER, they release the memory - all but the last piece.  This is very important - the last piece remains.  And again they are waiting for us to photograph them. <br><br>  Below is what actually happened. <br><img src="https://habrastorage.org/webt/zx/ah/ug/zxahugrdasantcktho7dbu-tnes.png"><br><br><ul><li>  First, both processes started and ate the memory.  It looks like the truth - 2 GB of RSS. </li><li>  We press ENTER and we are surprised.  The first program, which allocated in large pieces, the memory returned.  But the second program did not return. </li></ul><br>  The answer, why did this happen, lies in the Linux malloc. <br><br>  If we request memory in large chunks, it is issued using the anonymous mmap mechanism, which is given to the address space of the processor, from which we are then cut into memory.  When we do free (), the memory is freed and the pages return to page cache (system). <br><br>  If we allocate memory in small pieces, we do sbrk ().  sbrk () shifts the pointer to the tail of the heap, in theory the offset tail can be returned back by returning memory pages to the system if memory is not used. <br><br>  Now look at the illustration.  We had many entries in the history of the location of degraded objects, and then came the user session - a long-lived object.  We synchronized and all unnecessary structures were gone, but the long-lived object remained, and we cannot move sbrk () back. <br><img src="https://habrastorage.org/webt/06/wf/eg/06wfegwyvu0ibae8xjlwizrwteo.png"><br><br>  We still have a large unused space that could be freed if we had SWAP.  But we are smart - we have disabled SWAP. <br><br>  Of course, then some part of the memory from the beginning of the heap will be used, but this is only a certain part, and a very substantial remainder will be kept occupied. <br><br>  What to do in this situation?  The answer is below. <br><br><h3>  Controlled launch <br></h3><br><ul><li>  We start one OSD demon. </li><li>  We are waiting for it to synchronize, check the memory budgets. </li><li>  If we understand that we will sustain the start of the next demon, we start the next one. </li><li>  If not, then quickly restart the daemon that occupied the most memory.  He was able to down for a short time, he doesn‚Äôt have much history, there are missing locs and other things, so he will eat less memory, the memory budget will increase slightly. </li><li>  We run over the cluster, control it, and gradually raise everything. </li><li>  Check whether you can proceed to the next OSD, go to it. </li></ul><br>  DigitalOcean actually did this: <br>  <em>Bidding for each team. "</em> <br><img src="https://habrastorage.org/webt/nr/yg/a1/nryga17av_ez5yj0mt3lm5grkk0.png"><br><br>  Let's return to our configuration and the current situation.  Now we have a collapsed cluster after a chain reaction out of memory killer.  We prohibit automatic restart of the OSD in the red domain, and one by one we start the nodes from the blue domains.  Because <strong>our first task is always to restore service</strong> , without understanding why this happened.  We will understand later when we restore service.  In operation is always the case. <br><br>  We bring the cluster to the target state in order to restore the service, and then we start to run one OSD after another according to our methodology.  We are looking at the first one, restarting others if necessary, in order to adjust the memory budget, the next one - 9, 10, 11 - and the cluster seems to be synchronized and ready to start the service. <br><br>  The problem is how to <strong>handle the recording in Ceph</strong> . <br><img src="https://habrastorage.org/webt/hl/rp/ek/hlrpekm0rvjgrjwl11zgdklwecc.png"><br><br>  We have 3 replicas: one master OSD and two slaves for it.  Let's clarify that the master / slave in each Placement Group has its own, but each has one master and two slaves. <br><br>  The write or read operation goes to master.  When reading, if master has the necessary version, he will give it to the client.  The recording is a bit more complicated; the recording must be repeated on all replicas.  Accordingly, when a client writes 64 KB in OSD 0, the same 64 KB in our example falls on OSD 5 and OSD 8. <br><br>  But the fact is that OSD 8 is very degraded here, since we restarted a lot of processes. <br><img src="https://habrastorage.org/webt/es/_z/fr/es_zfrsvdaq8a7f_rgn7hcakpi4.png"><br><br>  Since in Ceph any change is a transition from version to version, on OSD 0 and OSD 5 we will have a new version, on OSD 8 - the old one.  This means that in order to repeat the recording (to distribute 64 KB) we need to update the version on OSD 8 - and this is 4 MB (object size).  That is, we read 4 MB on OSD 0, send on OSD 8, it writes, comes in a synchronous state.  Now we have the same fresh versions everywhere, and only then do we record 64 Kb. <br><br>  Now the numbers will go - the most interesting. <br><img src="https://habrastorage.org/webt/ch/uc/l_/chucl_b0vhoi-jvuhl3xokm26qg.png"><br><br>  Test cluster performance: <br><br><ul><li>  A 4 KB write operation takes 1 ms, a performance of 1000 operations / second in 1 stream. </li><li>  The 4 MB operation (object size) takes 22 ms, the performance is 45 operations / second. </li></ul><br>  Consequently, when one of the three domains fails, the cluster is in a degraded state for some time, and half of the hot objects spread to different versions, then half of the write operations will begin with forced recovery. <br><br>  The compulsory recovery time is calculated approximately - write operations to the degraded object. <br><img src="https://habrastorage.org/webt/it/0p/34/it0p34kbqfs3u9hvyhmextflvqc.png"><br><br>  First we read 4 MB for 22 ms, we write 22 ms, and then 1 ms we write 4 Kb of data itself.  A total of 45 ms per one write operation to a degraded object on the SSD, when we had a nominal performance of 1 ms - <strong>a performance drop of 45 times</strong> . <br><br>  The greater the percentage of degraded objects we have, the more terrible things become. <br><br><h2>  Average service time <br></h2><br><br><ul><li>  When <strong>half of the objects have</strong> degraded, the average service time is (45 + 1) / 2 = <strong>23 ms.</strong> </li><li>  If <strong>75% of the objects are</strong> degraded, then (45 * 3 + 1) / 4 = <strong>34 ms</strong> . </li><li>  If 90% - (45 * 9 + 1) / 10 = 41 ms - 40 times slower than the normal mode. </li></ul><br>  This is the factory mechanism of Ceph, and nothing can be done with it.  If your cluster has partially been offline and at this time its other part has serviced customer requests, then after switching it on, there will be a dramatic, several dozen times, drop in performance on some operations. <br><br>  Now consider the results of Ceph performance tests in emergency mode on two graphs at once. <br><img src="https://habrastorage.org/webt/ng/jj/od/ngjjodzmfd4n6kes71g4n6pg7os.png"><br><br><ol><li>  The lower graph is familiar to us - this is cluster performance: normal mode, failure, failure detection, degraded mode, work in degraded mode. <br></li><li>  Above - latency.  Here, in fact, latency is even worse than we expected.  This cluster degraded by almost 100% during the test (I specifically held it longer so that the pictures were spectacular and the depth of the lesion reached you).  Latency from 60 ms due to overhead costs, which we did not take into account in the initial calculations. <br></li></ol><br><img src="https://habrastorage.org/webt/z3/pb/ob/z3pbobkev0bfszscnwgprpop3xe.png"><br><br>  The cluster will be restored in the course of normal work, and we will rest primarily on the network.  A network of 10 Gbps, that is, 1,200 Mb / s, which means 300 objects per second per server, regardless of how many disks there are.  There will be 10 SSD - it is still 300 objects per second, one disk - maybe there will still be 300 objects per second. <br><br><blockquote>  We built an efficient cluster, and got into a replication network. <br></blockquote><br>  In addition, there is still disk bandwidth.  Our drive in normal mode gives 900 MB / s (this is the average SSD).  Usually it serves about 2,500 operations at 128 Kbytes per second (as a rule, ESXi and HyperV align their requests under 128 Kbytes).  But if we enter into degraded, we rest on 225 objects per second.  And if we use the file store, and not the object store, then we also have a journal (double entry), then we generally get 110 operations per second, and everything becomes generally very, very sad. <br><br>  SSD issues 110 operations per second - a catastrophe! <br><br>  <strong>What we can do?</strong> <br><br>  <strong>Answer 1:</strong> You can only fight architecture - <b>to make more domains of failure</b> . <br><img src="https://habrastorage.org/webt/ls/ib/rh/lsibrhcfnucjiox9f8gzxbk1cc8.png"><br><br>  Here the columns from left to right: how many domains failed;  the percentage of degraded PG; <br>  average service time with the corresponding failure. <br><br>  If we refused: <br><br><ul><li>  One domain of three, then 45 ms is an optimistic estimate. </li><li>  One domain out of ten (according to the theory of probability, taking into account the mathematical expectation), turns out about 14 ms. </li><li>  One domain of twenty, then 8 ms (degraded approximately 10% PG). </li></ul><br>  That is, <strong>adding domains is effective, but expensive</strong> , since a domain of failure is designed for a power failure of servers, other equipment, and this is not always possible. <br><br>  <strong>Answer 2: The</strong> second option is to <b>reduce the size of the object</b> (order, objectsize) in the image. <br><br>  If we reduce the size of the object, then, for example, operations from 4 MB will become 2 or 1 MB.  Then everything will be much faster, but still much slower than the normal mode.  Wherein: <br><br><ul><li>  the replication time of the object is multiplied; </li><li>  proportionally decreases maintenance time (latency) on the cluster during recovery. </li></ul><br>  But you have to pay for everything: <br><br><ul><li>  the number of objects is multiplied; </li><li>  memory consumption increases by almost a factor; </li><li>  the most annoying thing is that it is impossible to resize an object for an already created image.  How it was created in 4 MB will remain so. </li></ul><br>  If we created it for maximum performance (32 MB object) - then we hit it very specifically right away! <br><br>  <strong>Answer 3:</strong> Another way is to <b>refine Ceph</b> . <br><br>  As part of the functional responsibilities, I, as a system architect and developer, climbed <strong>deep into Ceph</strong> .  In the course of the survey, we managed to force the cluster not to replicate for every sushi when writing to a degraded object, but at the same time to preserve the consistency of the cluster, that is, to truncate part of the transmitted data.  It turned out such an interesting picture. <br><img src="https://habrastorage.org/webt/c8/nr/1a/c8nr1akew1kjleodu5trq_ow3oy.png"><br><br>  On the top graph cluster performance, on the bottom - Latency.  Blue - the regular schedule, red - experimental.  Latency is actually growing at a minimum of 30%, just this is not visible on this scale, that is, everything is not so good. <br><br>  This code is not yet in the Community, as it is in a preproduction state.  It can not be included on the go, but it does not suit us.  When we bring this to the end, we will do it. <br><br><h1>  Conclusion <br></h1><br>  It took about a man-year for us to get this work schedule.  If you do not have the opportunity to invest so much effort, climb inside the Ceph and do something cardinal there, then this is what you can do. <br><br>  ‚óè <strong>It is useless to do something during an accident</strong> . <br>  During the accident you can not panic, you should be ready for it.  This means that <strong>it is imperative to conduct exercises</strong> .  Without this, all your theoretical research is useless.  Moreover, it is highly recommended to conduct exercises on approximately the same configurations as your production.  Because if you have little data in the teachings, then the memory problem that DigitalOcean encountered and we, you will not step on.  If there is a lot of data, then come, and you will not know what to do. <br><br>  In addition, if you have little data and a small load, then you will not see this hell of productivity.  Clients will come to you, they will start shouting: ‚ÄúNothing works for you!  What happened ?! ‚ÄùThey will jerk your tech support, tech support you, you will clutch at the head.  Everything will be sad, and we must be ready for this: to understand where we will spend, how long it will last for about down time. <br><br>  ‚óè <strong>You cannot delete cluster components (OSD).</strong> <br>  Every time you remove a seemingly slowing component, you lose some of the data ‚Äî some of the data that is redundant for now, but if something goes wrong elsewhere, it may be necessary.  <strong>Therefore, do not remove OSD cluster components ‚Äî monitors and others ‚Äî never on the go</strong> .  If you do this, you are your own vicious Buratino. <br><br>  ‚óè <strong>Design your cluster correctly.</strong> <br>  It should be at the design stage to minimize the number of inaccessible OSD in the case of planned work or unplanned situations.  <strong>Make more domains of failure if possible</strong> .<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If you can not, at least choose a hardware that can change disks without shutting down the server. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚óè </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Allocate enough RAM on the OSD nodes.</font></font></strong> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚óè </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not disable SWAP.</font></font></strong> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Behavior with SWAP is not only Ceph behavior, but generally general Linux behavior. You must be prepared for this and remember this. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚óè </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Maximize cluster replication network performance.</font></font></strong> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let it not be used in normal mode by 100%, and even by 10%. But, if an abnormal mode happens, every extra gigabit will make your life easier, and it is very essential. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">‚óè </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sometimes it makes sense to reduce the size of frequently modified RBD objects or reduce the size of an object in Rados Getway.</font></font></strong> <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But remember that reducing the size of the object will require additional RAM. </font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do not forget to add SWAP - no need to be afraid of it. </font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The fact that there is SWAP activity is not so bad, because, most likely, the system is bringing down something that is not used very actively.</font></font><br><br><blockquote><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This article is a transcript of one of the best DevOpsConf Russia reports. </font><font style="vertical-align: inherit;">Soon we will open the video and publish in the text version more interesting topics. </font><font style="vertical-align: inherit;">Subscribe here, on </font></font><a href="https://www.youtube.com/c/DevOpsChannel"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">youtube</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> or in the </font></font><a href="http://eepurl.com/bN_0E1"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">newsletter</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , if you want to not miss such useful materials and keep up to date with DevOps news.</font></font><br></blockquote></div><p>Source: <a href="https://habr.com/ru/post/431536/">https://habr.com/ru/post/431536/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431526/index.html">Pernicious influence: how the Stasi defended East Germany from video games</a></li>
<li><a href="../431528/index.html">Mysterious mathematical genius and writer are promoting the solution of the permutation problem.</a></li>
<li><a href="../431530/index.html">Open lesson "Android Material Design: review of updates"</a></li>
<li><a href="../431532/index.html">Memristors consisting of 2 nm thick parts</a></li>
<li><a href="../431534/index.html">Problem personalities among developers</a></li>
<li><a href="../431538/index.html">Case Rate & Goods and Mobio: Incremental increase of all indicators</a></li>
<li><a href="../431540/index.html">Packages and package managers for k8s</a></li>
<li><a href="../431546/index.html">Why do we say OK?</a></li>
<li><a href="../431548/index.html">Solar Dozor - what is behind the stars?</a></li>
<li><a href="../431550/index.html">Injection Molded Films (IMD): How It Works</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>