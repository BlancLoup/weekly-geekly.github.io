<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Bayesian neural network - because why not, damn it (part 1)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What I‚Äôll try to tell you now looks like real magic. 

 If you knew something about neural networks before this - forget it and don‚Äôt remember how ter...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Bayesian neural network - because why not, damn it (part 1)</h1><div class="post__text post__text-html js-mediator-article">  What I‚Äôll try to tell you now looks like real magic. <br><br>  If you knew something about neural networks before this - forget it and don‚Äôt remember how terrible a dream. <br>  If you didn‚Äôt know anything, it‚Äôs easier for you, halfway through. <br>  If you are on ‚Äúyou‚Äù with Bayesian statistics, read <a href="http://www.cs.toronto.edu/~graves/nips_2011.pdf">this</a> and <a href="http://arxiv.org/pdf/1505.05424v2.pdf">here this</a> article from Deepmind - ignore the previous two lines <s>and allow</s> me <s>to sign up for a consultation on one theological issue</s> . <br><br>  So, the magic: <br><img src="https://habrastorage.org/files/0ca/a5c/0fc/0caa5c0fc56d417abd902b030d9df394.png">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      On the left is a common and familiar neural network, in which each connection between a pair of neurons is given by some number (weight).  On the right is a neural network, the weights of which are represented not by numbers, but by <b>demonic clouds of probability</b> , oscillating whenever the devil plays dice with the universe.  That is what we want to end up with.  And if you, like me, are puzzled, shake your head and ask "but what for is all this necessary" - welcome under the cat. <br><br><a name="habracut"></a><br><br><h4>  One step back: linear regression </h4><br>  Let's start to take the world's simplest neural network, consisting of as much as one neuron.  Saw off his activation function and make spit out just the product of inputs by weights (w) plus b, and we get what is called a linear regression. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/82e/bdb/085/82ebdb08501240cc8e369b4bf178ee51.png"></div><br>  <i>At the entrance there are additionally several copies of the original X, raised to a power.</i>  <i>This is sometimes referred to as polynomial regression, although theoretically, the ‚Äúneuron‚Äù still makes a linear function.</i> <br><br>  Well, as in the classic puzzles, let's say we have some points, and we need to adjust the function to them.  Let's write some not very nice, but the easiest code in the world: <br><br><div class="spoiler">  <b class="spoiler_title">Spoiler header</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> seaborn <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> sns <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">add_powers</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data, n=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> result = [data] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">2</span></span>, n + <span class="hljs-number"><span class="hljs-number">1</span></span>): poly = np.power(data, i) result.append(poly) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.vstack(result).T <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">generate_data</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">20</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> x = np.linspace(<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>) y = x * np.cos(x) max_y = np.max(y) y += np.random.random(y.shape) idx = np.random.choice(len(y), n) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> x[idx], y[idx], max_y <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> __name__ == <span class="hljs-string"><span class="hljs-string">'__main__'</span></span>: original_data, target, max_y = generate_data(n=<span class="hljs-number"><span class="hljs-number">10</span></span>) power = <span class="hljs-number"><span class="hljs-number">5</span></span> data = add_powers(original_data, power) max_vals = np.array([max_y ** i <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">1</span></span>, power + <span class="hljs-number"><span class="hljs-number">1</span></span>)]) data /= max_vals w = (np.random.random(power) - <span class="hljs-number"><span class="hljs-number">0.5</span></span>) * <span class="hljs-number"><span class="hljs-number">1.</span></span> b = <span class="hljs-number"><span class="hljs-number">-1.</span></span> learning_rate = <span class="hljs-number"><span class="hljs-number">0.1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> e <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">100000</span></span>): h = data.dot(w) + b w -= learning_rate * ((h - target)[:, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>] * data).mean(axis=<span class="hljs-number"><span class="hljs-number">0</span></span>) b -= learning_rate * (h - target).mean() plt.title(<span class="hljs-string"><span class="hljs-string">'Interpolating f(x) = x * cos(x)'</span></span>) x = add_powers(np.linspace(<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">100</span></span>), power) x /= max_vals y = x.dot(w) + b plt.plot(x[:, <span class="hljs-number"><span class="hljs-number">0</span></span>] * max_vals[<span class="hljs-number"><span class="hljs-number">0</span></span>], y, lw=<span class="hljs-number"><span class="hljs-number">3</span></span>, c=<span class="hljs-string"><span class="hljs-string">'g'</span></span>) plt.scatter(data[:, <span class="hljs-number"><span class="hljs-number">0</span></span>] * max_vals[<span class="hljs-number"><span class="hljs-number">0</span></span>], target, s=<span class="hljs-number"><span class="hljs-number">50</span></span>) plt.show()</code> </pre> <br></div></div><br><br>  And we get something like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/8de/b47/acc/8deb47acc06e49eaa8127fc6902a78ed.png"></div><br><br>  The function we are trying to push through the blue dots is right now given by a fifth-degree polynomial, and its coefficients are: <code>[0.5423 -3.2648 -16.5311 43.3645 25.6159 -51.2418]</code> .  What does this mean, just in case?  This means that we suspect some pattern in the data, and this pattern is best expressed as <img src="http://tex.s2cms.ru/svg/f%28x%29%20%3D%20-51.2418x%5E%7B5%7D%20%2B%2025.6159x%5E%7B4%7D%20%2B%2043.3645x%5E%7B3%7D%20-16.5311x%5E%7B2%7D%20-3.2648x%20%2B%200.5423">  .  Suppose we looked at the graph, made sure that the green line does not do anything particularly bad and fits well with the data, maybe we checked how it behaves on a separate (previously hidden) test sample, and we are all arranged.  Then these six digits are the almost-truth found by us, some "real" parameters governing the laws of nature (in this case, the law of the function <img src="http://tex.s2cms.ru/svg/x%20%5C%3A%20cos%28x%29">  But these are the details - let's assume that we are looking at some real and very important data, something like a temperature graph for estimating global warming). <br><br><h4>  Probabilistic interpretation </h4><br>  Okay, if we found the real parameters, why does the green line nevertheless pass through the blue points imperfectly?  The standard deviation for this line is still not zero (in fact, it is about 0.97).  Is this normal, or have we done something wrong?  There are two answers to this question: <br><br>  1. Our model is not steep enough and does not fully reflect the desired pattern.  We can ‚Äúenrich‚Äù it by adding more parameters ‚Äî that is, increase the degree of the polynomial.  For <img src="http://tex.s2cms.ru/svg/n">  dots are enough for us to take a polynomial of degree <img src="http://tex.s2cms.ru/svg/n-1">  so that it goes perfectly through all the points. <br><br><img src="https://habrastorage.org/files/b79/c50/c8f/b79c50c8fe1041d8b16ad52c5b013469.png"><br><br>  ... not exactly perfect, but why not?  It looks even a little nicer, in my opinion.  Let's leave as a working hypothesis. <br><br>  2. Our model is pretty cool, the <i>problem is in the data</i> .  There is some kind of noise in them, caused by the imperfection of our world (in this case, by the fact that I have treacherously added some randomness to the points, but let's imagine that these are some real data).  Even if this noise is not truly random, but the truth is caused by factors that we did not take into account when formulating the problem, we can <i>simulate</i> it as random - and assume that the oscillations caused by the variations of all these factors might well fall into a normal distribution. . <br><br>  I don‚Äôt know about you, but the second conclusion seems to me, if not even right, then at least a priority one.  In the end, we can always suspect that there will be some noise in our data that interferes with our plans and deviates points from the desired value.  We will first think about it, and then, if anything, you can twist the degree of the polynomial. <br><br>  So, we make the second conclusion, pronounce the word "noise" and immediately transfer to the theory of probability.  The most convenient way (in any case, to me) is to present it this way: our green line tries to ‚Äúshoot through‚Äù all the blue dots that play the role of targets, while it can ‚Äúmiss the mark‚Äù, and the size of the slip is precisely that noise.  Suppose that he is Gaussian (why not).  Then the target shot will look like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/2c7/a88/7ac/2c7a887ac78a4f0b919f4a72c009cf77.png"></div><br><br>  This is all the same regression graph, only with zoom.  The blue dot is the actual value from the dataset, the red dot is what the regression predicts.  Small deviations of blue from red are more likely (lie near the center of the Gaussian), large deviations are less likely.  It becomes clear that if we predict the ‚Äúwrong center‚Äù (we place the red dot somewhere far away), the deviation for the blue dot will become large and therefore unlikely. <br><br>  Translated into a slightly more generally accepted language, this probability is called <strong>likelihood</strong> and is written in our Gaussian case as <img src="http://tex.s2cms.ru/svg/P%28X%20%5Cmid%20%5Cmu%2C%20%5Csigma%29">  (Where <img src="http://tex.s2cms.ru/svg/%5Cmu">  and <img src="http://tex.s2cms.ru/svg/%5Csigma">  - the center and standard deviation of the Gaussian curve), and in general, as <img src="http://tex.s2cms.ru/svg/P%28X%20%5Cmid%20%5Ctheta%29">  where under <img src="http://tex.s2cms.ru/svg/%5Ctheta">  mean any parameters.  Its meaning is the same everywhere - ‚Äúthe probability that if the distribution of data is controlled by such and such parameters <img src="http://tex.s2cms.ru/svg/%5Ctheta">  , the result will be exactly <img src="http://tex.s2cms.ru/svg/X">  ". <br><br>  Now we can slightly reformulate the regression problem.  In terms of targets and credibility, they will sound something like this: shoot so that the shooter does not look like a complete loser, that is, so that his misses look like misses and lie within the bounds of error.  Adhering to the generally accepted statistical language, we need to <strong>maximize the likelihood</strong> . <br><br>  The desired probability itself, as we assumed, is given by a <a href="https://en.wikipedia.org/wiki/Normal_distribution">normal distribution</a> .  We write down the formula from Wikipedia and multiply the probabilities for each point between each other (hence the index <img src="http://tex.s2cms.ru/svg/i">  ): <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0AP%28x%20%5Cmid%20%5Cmu_%7Bi%7D%2C%20%5Csigma_%7Bi%7D%5E%7B2%7D%29%20%3D%20%5Cprod_%7Bi%7D%20%5Cfrac%7B1%7D%7B%5Csigma_%7Bi%7D%20%5Csqrt%7B2%5Cpi%7D%7D%20e%5E%7B-%5Cfrac%7B%28x_%7Bi%7D-%5Cmu_%7Bi%7D%29%5E%7B2%7D%7D%7B2%5Csigma_%7Bi%7D%5E%7B2%7D%7D%7D%0A%20"></div><br>  Oookey, looks a little scary already.  Let's make it a little easier, we fix <img src="http://tex.s2cms.ru/svg/%5Csigma_%7Bi%7D%5E%7B2%7D%3D1">  - then we can throw half the characters out of here.  In order not to spoil the expression of probability, let us separately indicate that we are interested in the maximum: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0Aargmax%20%5C%3A%20P%28x%20%5Cmid%20%5Cmu_%7Bi%7D%29%20%3D%20argmax%20%5C%3A%20%20%5Cprod_%7Bi%7D%20e%5E%7B-%28x_%7Bi%7D-%5Cmu_%7Bi%7D%29%5E%7B2%7D%7D%0A"></div><br>  Heei, it became much more fun.  Take from this thing logarithm.  The maximum point is not going anywhere, but the product will turn into a sum, and the exponent will become its indicator: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0Aargmax%20%5C%3A%20P%28x%20%5Cmid%20%5Cmu_%7Bi%7D%29%20%3D%20argmax%20%5C%3A%20%20%5Csum_%7Bi%7D%20-%28x_%7Bi%7D-%5Cmu_%7Bi%7D%29%5E%7B2%7D%0A"></div><br>  And finally, we‚Äôll remove the minus in the amount, replacing the maximum with the minimum: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0Aargmax%20%5C%3A%20P%28x%20%5Cmid%20%5Cmu_%7Bi%7D%29%20%3D%20argmin%20%5C%3A%20%20%5Csum_%7Bi%7D%20%28x_%7Bi%7D-%5Cmu_%7Bi%7D%29%5E%7B2%7D%0A"></div><br>  Hmm, somewhere I have already seen such an expression ... <br><br>  ... and it turns out that the maximum likelihood is reached in the same place where the minimum of the standard deviation of the regression is.  For some reason, such moments are always terribly disappointing - I was already sure that I would discover some new regression method, and we returned to where we started. <br><br>  On the other hand, now we know the answer to the question ‚Äúwhy do we use exactly standard deviation?‚Äù Previously, we mentally shrugged it off, but now we know that it corresponds to Gaussian noise.  Bonus material - if an absolute error is used to optimize the regression ( <img src="http://tex.s2cms.ru/svg/%7Cx_%7Bi%7D-%5Cmu_%7Bi%7D%7C">  ), then we get a noise <a href="https://en.wikipedia.org/wiki/Laplace_distribution">in Laplace</a> (so pointed in the center).  From the question "and one is better than the other" shamefully dodge. <br><br><h4>  Think bayes </h4><br>  Well, it was all fun, and quite common, in fact - we just re-invented <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">the maximum likelihood method</a> here at our leisure.  It is time to step a little deeper and to another level of sleep. <br><br>  ... do you think the universe has any <em>parameters</em> ? <br><br>  This is a serious question.  We still know from school physics that some things seem to have exactly - acceleration of free fall, say, 9.8 meters per second squared, and if we want to find out how things fall, or say, to model a computer game with physics, we you have to twist this number there (or how computer games do, hmm? maybe the Earth‚Äôs attraction is done according to Newton's law? or, hell, taking into account the effects of the theory of relativity?).  Or let's say space has three dimensions ( <em>macroscopic dimensions</em> , <a href="http://www.amazon.com/Just-Six-Numbers-Forces-Universe/dp/0465036732">Martin Reese</a> corrects me);  not 3.1 and not 2.8, but exactly 3. If we suddenly decided to build a machine learning algorithm that would decide to measure the dimension of space, we would estimate its work the higher, the closer it would seem to number three. <br><br>  On the other hand, is there any exact, fixed parameter for how much milk a baby should drink per month?  How many songs should a bird sing to find a mate?  In what proportion will the tumor decrease after the injection of substance X into the patient's body?  How much water flows through the stream in three minutes?  All these things, undoubtedly, have a certain regularity under them - the tumor will shrink, the child will drink milk, and the water will flow, but if we measure the exact values, they will constantly fluctuate, remaining at the same time in some interval.  Where, in the case of physics, the use of more accurate tools gives us more and more exact value of magnitude ( <em>‚Äúthe second is the time equal to 9,192,631,770 periods of radiation corresponding to the transition between two superfine levels of the ground state of the cesium-133 atom‚Äù</em> ) to measure the cancerous we would rather need a tumor in more experimental patients in order to delineate the upper and lower boundaries and be ready for different variants of the development of events. <br><br>  This is generally a question from somewhere rather from philosophy - and somewhere here lies the dividing line between <strong>Bayesian</strong> and <strong>Orthodox</strong> (or frequency) statistics.  If roughly (I‚Äôll fix it a bit further), then the orthodox approach will say that at some level there are still real parameters (we just haven‚Äôt gotten to it yet, and someday it turns out that tumor growth, say, depends on the combination several well-defined factors).  The Bayesian will say that there is no exact parameter - if not even at the level of the physical design of the universe, then at the level of the effects we observe - and that the parameters should be treated as random variables that can fluctuate in different directions. <br><br><div class="spoiler">  <b class="spoiler_title">In fact...</b> <div class="spoiler_text">  Both parties can now say that I am wrong, because there are a whole bunch of interpretations of this question.  I‚Äôm <a href="http://stats.stackexchange.com/questions/83731/would-a-bayesian-admit-that-there-is-one-fixed-parameter-value">referring</a> to <a href="http://stats.stackexchange.com/questions/83731/would-a-bayesian-admit-that-there-is-one-fixed-parameter-value">StackExchange</a> , from where I borrowed a few examples like milk and a tumor, and the more correct answer for Bayesian statistics would rather be ‚Äúwe don‚Äôt know how the world really works there, and perhaps there are fixed parameters, but in our interactions with With them we invariably operate with a certain degree of uncertainty, and we model this uncertainty with the help of probability. ‚Äù  Fuh.  Urgently need a way to a smaller level of sleep. <br></div></div><br><br><h4>  2560000 regressions </h4><br>  Let's apply the Bayesian ‚ÄúParameters are Random Variables‚Äù philosophy to the coefficients of our regression.  To do this, write off the Bayes theorem: <br><br><div style="text-align:center;"><img src="https://tex.s2cms.ru/svg/P(%5Ctheta%20%5Cmid%20X)%20%3D%20%5Cfrac%7BP(X%20%5Cmid%20%5Ctheta)P(%5Ctheta)%7D%7BP(X)%7D"></div><br>  Here <img src="http://tex.s2cms.ru/svg/%5Ctheta">  means ‚Äúparameters in general‚Äù;  in our case, these are already familiar polynomial coefficients. <img src="http://tex.s2cms.ru/svg/b%2C%20w_%7B1%7D...%20w_%7B5%7D">  .  And we already know at least one garbage from this formula - <img src="https://tex.s2cms.ru/svg/P(X%20%5Cmid%20%5Ctheta)">  , the very <strong>likelihood</strong> that we maximized with the section above (once again recall that it is considered as the value of the deviations of the dataset points from the regression curve or as the ‚Äúaverage target slip size‚Äù). <img src="http://tex.s2cms.ru/svg/P%28%5Ctheta%29">  is called a <strong>priori</strong> (prior) probability and reflects our initial assumption about how the distribution of parameters looks. <img src="http://tex.s2cms.ru/svg/P%28X%29">  in the domestic literature, in my opinion, is not specifically called (the total probability?), in others it can be found under the word <strong>evidence</strong> - this is the general probability to obtain such data as we have, for all possible values ‚Äã‚Äãof the parameters <img src="http://tex.s2cms.ru/svg/%5Ctheta">  (it is not very clear where to get it from).  And finally <img src="http://tex.s2cms.ru/svg/P%28%5Ctheta%20%5Cmid%20X%29">  - <strong>posterior</strong> probability, which literally means ‚Äúwhat do we think about the distribution of parameters after seeing the data‚Äù. <br><br>  ... in fact, there are even very un-very-complicated ways to express posterior for regression analytically - that is, to get a formula where only to substitute the values ‚Äã‚Äãfrom the dataset, and you get the correct answer.  But we will do everything in a razdolbaysky programming way - numerically, iteratively, and completely ineffective.  But you don‚Äôt have to read about any inverted distributions of Wishart and other terrible things that don‚Äôt go to school (although you‚Äôll have to do it anyway, so if you are for the hardcore way of knowing, then we <a href="https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression">‚Äôre welcome</a> ).  The logic is as follows: <br><br>  - we start with a state of complete ignorance, when we have only prior.  That is, we believe that the regression coefficients can be anything.  To limit the abyss of knowledge a bit, say "any from -10 to 10". <br>  - we see some piece of data (one point from dataset, say).  For every possible combination of parameters <img src="http://tex.s2cms.ru/svg/%5Ctheta">  we consider the likelihood - how likely it is that a shot at the target was made with such settings.  As you can guess in advance, most of the settings will be rather invalid, and the shot will take somewhere completely past, but obviously there will be several (more than one) options for which the hole in the target looks reasonable.  Likelihood, we believe, as we have already seen, according to Gauss, we simply substitute the prediction of regression instead of mu into the density formula for the normal distribution, and the value from the dataset instead of x. <br>  - we sum up all the likelihood values ‚Äã‚Äãcalculated at the previous step and we get <img src="http://tex.s2cms.ru/svg/P%28X%29">  (which is easily verified by writing <img src="http://tex.s2cms.ru/svg/%5Csum_%7B%5Ctheta%7D%20P%28X%20%5Cmid%20%5Ctheta%29%20%3D%20P%28X%29">  ).  Now we have everything we need to upgrade posterior. <br>  - having processed one target, we got some ideas about how the parameters might look;  now it becomes our new prior.  We get the next point from the dataset and start with the second step. <br><br>  It all looks pretty simple: <br><br><div class="spoiler">  <b class="spoiler_title">Spoiler header</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> itertools <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> product <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">likelihood</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x, mu, sigma=</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1.</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> np.exp(-np.power(x - mu, <span class="hljs-number"><span class="hljs-number">2.</span></span>) / (<span class="hljs-number"><span class="hljs-number">2</span></span> * np.power(sigma, <span class="hljs-number"><span class="hljs-number">2.</span></span>))) frequency = <span class="hljs-number"><span class="hljs-number">40</span></span> posterior = np.ones(frequency ** (power + <span class="hljs-number"><span class="hljs-number">1</span></span>)) / float(frequency) param_values = [np.linspace(<span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>, frequency) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> _ <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(power + <span class="hljs-number"><span class="hljs-number">1</span></span>)] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i, pt <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(data): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j, params <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> enumerate(product(*param_values)): b = params[<span class="hljs-number"><span class="hljs-number">0</span></span>] w = np.array(params[<span class="hljs-number"><span class="hljs-number">1</span></span>:]) h = pt.dot(w) + b like = likelihood(pt[<span class="hljs-number"><span class="hljs-number">0</span></span>], h) posterior[j] *= like posterior /= posterior.sum()</code> </pre><br></div></div><br><br>  ( <code>itertools.product</code> will help us to <code>itertools.product</code> through all possible combinations of coefficients) <br><br>  As a result, we get a joint distribution for our regression coefficients - all their possible values ‚Äã‚Äãwill be added to one.  It turns out that for each set of coefficients there will be some probability, and the higher it is, the more correctly and the chop they set. <br><br>  There is a question for a million - how to draw this garbage? <br><br>  I tried something like this: I broke the spectrum of probabilities into ten equal segments, and from each segment I drew on the chart some curves with the corresponding color (from black to white - the higher the probability of the curve).  There are obviously more ‚Äúlow-probability‚Äù curves than high-probability ones, so we had to limit their number to a random hundred or so that the pyplot would not finally die in torment, trying to draw ... how many curves, by the way?  I temporarily lowered the degree of the polynomial to the third, each coefficient can take values ‚Äã‚Äãfrom -10 to 10 in increments of 0.5 - so modest <img src="http://tex.s2cms.ru/svg/40%5E%7B3%2B1%7D%3D2560000">  regressions where there used to be one. <br><br>  Total, it looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/a19/7f5/b94/a197f5b94e024d63a2e6cca80d382f83.gif"></div><br><br>  And if you mark the data on the graph, then so (I apologize for the vivid green color): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/9cf/47c/dc5/9cf47cdc53634c069f504f6c889b81ae.png"></div><br><br>  Notice that the ‚Äúhotter‚Äù curves, of course, have a higher probability, but in the Bayesian interpretation this does not mean that these curves are ‚Äúcorrect‚Äù (that we should take them and throw away all the black ones).  A slightly more intuitive way to think about these curves, in my opinion, is to imagine that you are looking, as it were, from above on a normal distribution, and you see that there is a certain probability peak in the center, but <i>the</i> probability of being at the point itself The peak is still very small.  Only by <i>gathering together</i> some part of the distribution can one ‚Äúdial‚Äù enough probability. <br><br><h4>  There is no retraining, Neo </h4><br>  If you are still wondering why we need this thing, then here is a small bonus: the Bayesian regression is invulnerable to overfitting.  And I do not mean simply ‚Äústable‚Äù or ‚Äúreliable‚Äù as a <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">regression with regularization</a> - to a certain extent it <i>is invulnerable</i> to it. <br><br>  What is retraining, again?  This is when we ‚Äúover-fit‚Äù the model to the available data (for example, we select a curve that ideally passes through all points of the dataset, but shows poor predictions on new data).  Translated into the philosophical language of the previous section, this means that when we were looking for the ‚Äúcorrect‚Äù set of parameters, we found some kind of erroneous, incorrect.  This is impossible to do with the Bayesian approach, because there is simply no concept of a ‚Äúcorrect‚Äù set of parameters!  The strange black and yellow cone in the pictures above tells us: ‚ÄúYes, most likely, the following points will be somewhere in the middle along the white curves, but there may be no problems at the edge, and above, and below.‚Äù  The degree of a polynomial can be increased as much as necessary: ‚Äã‚Äãthe number of possible curves will increase with it, and each of them will drop the probability separately.  All the same, only those that lie close to the data in our orange-hot beam will ‚Äúsurvive‚Äù.  Do the fifth degree?  No problem, Houston: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0b4/bf4/9b0/0b4bf49b0f71461993af069287108337.png"></div><br>  <i>It looks a little more faded, but this is simply because I reduced the total number of curves ‚Äî otherwise my laptop wouldn't wake up.</i> <br><br>  In fact, in such a context, the concept of retraining <strong>does not exist at all</strong> .  It is impossible to incorrectly find a parameter, because a ‚Äúparameter‚Äù is not a thing that can be ‚Äúfound‚Äù; it is a hefty cloud of probability that becomes ‚Äúdense‚Äù only where the data lie next to it.  There is no retraining, ladies and gentlemen, we disagree.  Do not forget to inform all these important guys from machine learning that you can close the shop. <br><br><div class="spoiler">  <b class="spoiler_title">Just in case</b> <div class="spoiler_text">  This does not mean that Bayesian regression always gives correct predictions and cannot be wrong - of course it can.  Suppose the next point in the dataset will have x = 3 and y = -20 and falls far below the orange beam ‚Äî then our model will predict the wrong value.  The trick is that in this place you would need to train from scratch and correct the parameters, while the Bayesian one just needs to ‚Äúproapdeit‚Äù by putting a new point in the algorithm - and the beam will bend accordingly. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/b8e/6f1/904/b8e6f190468e43508536f1f2ca32a920.png"></div><br>  <i>Something like this.</i> <br><br>  In addition, the Bayesian regression, as if to say, is ‚Äúinert.‚Äù  Having a dozen points along the white curve and one outlier, she is likely to bend in his direction a little, but not completely.  And this, generally speaking, is good and right, because what if this is just another result of noise? <br></div></div><br><br><h4>  Why do ensembles work? </h4><br>  In machine learning there is such a very simple technique - when you have several models and they all work poorly, you can take and average (or even combine them) their predictions, and then they magically start to work well.  In the right way it is called <a href="https://en.wikipedia.org/wiki/Ensemble_learning">ensembles</a> , and they can be found almost anywhere, especially in competitions. <br><br>  And generally speaking, this is not an intuitive question: why do ensembles work at all?         :       ,       ,       .         (-)   ,    .   - : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/431/38b/995/43138b9959124366a98da4f39cfad04b.png"></div><br><br>   ‚Äî ¬´¬ª  (  ),   ‚Äî ¬´¬ª ( ).      ,   ‚Äî  .   ‚Äî      ,     ,     ,  ,       .      ;     ,   , <em> ,      </em> (        ‚Äî   <em></em>   ,   ). <br><br>  ,     -   ,        ,  ‚Äî    ‚Äî    .  ,      .            -10  10    0.5,     -,         . ,  ‚Ä¶ <img src="http://tex.s2cms.ru/svg/%5Cinfty%5E%7B3%2B1%7D"> ,  .   ‚Äî    <strong></strong>   (     ). -      ¬´ ¬ª       ,   - . <br><br> <em>¬´-- ?¬ª</em> <br><br><h4>      </h4><br>    : ,      posterior,      -   .   ,      ,   ,  .           ,       ( /)         10485760000000000 . <br><br>     <em>-</em> :    ,     (      ,    ),       . ¬´¬ª   ‚Äî   ;            (   ).      : ,     posterior  ,       (  ),        ‚Äî   ,       -  . <br><br>     -     .   ,  , ¬´    ¬ª;    ,      -  ,          .         <em> ,     </em> ,  ,   ,    (   ,     ). <br><br> ,     ,     ,      <s>      </s> .  ,      ,   ,   - .           ,     ,     . <br><br><h4>  ,   -  </h4><br>          :  <em></em>  ,       ¬´¬ª .     ,    ,   ,  ,    . <br><br>     <img src="http://tex.s2cms.ru/svg/argmax%20%5C%3A%20P%28%5Ctheta%20%5Cmid%20X%29%20%3D%20argmax%20%5C%3A%20P%28X%20%5Cmid%20%5Ctheta%29%20P%28%5Ctheta%29">  .   ,     : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/argmax%20%5C%3A%20P%28%5Ctheta%20%5Cmid%20X%29%20%3D%20argmax%20%5C%3A%20%5Clog%20P%28X%20%5Cmid%20%5Ctheta%29%20%2B%20%5Clog%20P%28%5Ctheta%29"></div><br>  :  <img src="http://tex.s2cms.ru/svg/argmax%20%5C%3A%20%5Clog%20P%28X%20%5Cmid%20%5Ctheta%29">         ( ),     .      (  prior)?  , ,  <img src="http://tex.s2cms.ru/svg/%5Ctheta">    ‚Äî    ,         ,     - ,   (..,    <img src="http://tex.s2cms.ru/svg/%5Cprod_%7Bj%7D%20P%28w_%7Bj%7D%29">  ).    ?   ,   : <img src="http://tex.s2cms.ru/svg/P%28w_%7Bj%7D%29%20%5Csim%20Uniform%28-10%2C%2010%29"> ,         (   <img src="http://tex.s2cms.ru/svg/P%28%5Ctheta%29">   ‚Äî    ,      ).  ,            0  -   <img src="http://tex.s2cms.ru/svg/%5Csigma">  .  Then: <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20argmax%20%5C%3A%20%5Clog%20P%28%5Ctheta%29%20%3D%20argmax%20%5C%3A%20%5Clog%20%5Cprod_%7Bj%7D%20P%28w_%7Bj%7D%29%0A"></div><br>   <img src="http://tex.s2cms.ru/svg/P%28w_%7Bj%7D%29">   .    ,   ,       ,   .     : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20argmax%20%5C%3A%20%5Clog%20P%28%5Ctheta%29%20%3D%20argmax%20%5C%3A%20%5Clog%20%5Cprod_%7Bj%7D%20%5Cfrac%7B1%7D%7B%5Csigma%20%5Csqrt%7B2%5Cpi%7D%7D%20e%5E%7B-%5Cfrac%7B%28w_%7Bj%7D-0%29%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%7D%0A"></div><br>    -      (    <img src="http://tex.s2cms.ru/svg/w_%7Bj%7D">  ).     : <br><br><div style="text-align:center;"><img src="http://tex.s2cms.ru/svg/%0A%20%20%20%20argmax%20%5C%3A%20%5Clog%20P%28%5Ctheta%29%20%3D%20argmax%20%5C%3A%20%5Csum_%7Bj%7D%20-%5Cfrac%7Bw_%7Bj%7D%5E%7B2%7D%7D%7B2%5Csigma%5E%7B2%7D%7D%20%3D%20argmin%20%5C%3A%20%5Csum_%7Bj%7D%20w_%7Bj%7D%5E%7B2%7D%0A"></div><br> ,    ,      .  Nothing like?   <a href="https://en.wikipedia.org/wiki/Regularization_%2528mathematics%2529"></a> ,  ,     ,     (   - - ,   ¬´,       ,     -¬ª,            ). <br><br>      <strong>maximum a posteriori</strong>  MAP-learning.    ¬´¬ª , MAP      ,     ‚Äî    ¬´¬ª.  ,   - ,          -  ¬´ ¬ª ‚Äî         ,      ,    . <br><br><h4>   </h4><br>   ,  <a href="https://habrahabr.ru/users/haqreu/" class="user_link">haqreu</a>   ,        ,        .  -     ,   ? </div><p>Source: <a href="https://habr.com/ru/post/276355/">https://habr.com/ru/post/276355/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../276343/index.html">BlueZ bracelet control</a></li>
<li><a href="../276345/index.html">Firmware security using the example of industrial switches Hirschmann and Phoenix Contact</a></li>
<li><a href="../276347/index.html">Does your antivirus catch password-protected archives?</a></li>
<li><a href="../276349/index.html">Foreign hackers hacked the database of the largest US police union</a></li>
<li><a href="../276353/index.html">DevConf 2015: video of PHP section reports</a></li>
<li><a href="../276357/index.html">As "Satellite" included "Children's mode"</a></li>
<li><a href="../276359/index.html">Perl5 plugin version 1.3 released for IntelliJ IDEA</a></li>
<li><a href="../276361/index.html">Census Analyzer 1.0: A New Data Analysis Tool</a></li>
<li><a href="../276363/index.html">How to sell an application that does not make a profit? And how much does it cost?</a></li>
<li><a href="../276365/index.html">How the Zeptolab Game Designer Challenge was created</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>