<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Analyzing Nginx Logs with Amazon Athena and Cube.js</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Usually, Nginx uses commercial products or ready-to-use open-source alternatives, such as Prometheus + Grafana, to monitor and analyze the performance...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Analyzing Nginx Logs with Amazon Athena and Cube.js</h1><div class="post__text post__text-html js-mediator-article"><p>  Usually, Nginx uses commercial products or ready-to-use open-source alternatives, such as Prometheus + Grafana, to monitor and analyze the performance.  This is a good option for monitoring or real-time analytics, but not very convenient for historical analysis.  On any popular resource, the amount of data from nginx logs is growing rapidly, and it is logical to use something more specialized to analyze a large amount of data. </p><br><p>  In this article I‚Äôll tell you how <a href="https://aws.amazon.com/athena/">Athena</a> can be used to analyze logs, taking Nginx as an example, and show you how to compile an analytical dashboard from this data using the open source <a href="">cube.js</a> framework.  Here is the complete solution architecture: </p><br><p><img src="https://habrastorage.org/webt/km/xo/3i/kmxo3izommuyzgajw20t6-aolcg.png" alt="Architecture"></p><br><p>  TL: DR; <br>  <a href="https://statsbotco.github.io/cubejs/nginx-analytics-dashboard/">Link to the finished dashboard</a> . </p><a name="habracut"></a><br><p>  We use <a href="https://www.fluentd.org/">Fluentd</a> to collect information, <a href="https://aws.amazon.com/kinesis/data-firehose/">AWS Kinesis Data Firehose</a> and <a href="https://aws.amazon.com/glue/">AWS Glue</a> for processing, and <a href="https://aws.amazon.com/s3/">AWS S3</a> for storage.  With this bundle you can store not only the nginx logs, but also other events, as well as logs of other services.  You can replace some parts with similar ones for your stack, for example, you can write logs to kinesis straight from nginx, bypassing fluentd, or use logstash to do this. </p><br><h2 id="sobiraem-logi-nginx">  We collect Nginx logs </h2><br><p>  By default, Nginx logs look something like this: </p><br><pre><code class="bash hljs">4/9/2019 12:58:17 PM1.1.1.1 - - [09/Apr/2019:09:58:17 +0000] <span class="hljs-string"><span class="hljs-string">"GET /sign-up HTTP/2.0"</span></span> 200 9168 <span class="hljs-string"><span class="hljs-string">"https://example.com/sign-in"</span></span> <span class="hljs-string"><span class="hljs-string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36"</span></span> <span class="hljs-string"><span class="hljs-string">"-"</span></span> 4/9/2019 12:58:17 PM1.1.1.1 - - [09/Apr/2019:09:58:17 +0000] <span class="hljs-string"><span class="hljs-string">"GET /sign-in HTTP/2.0"</span></span> 200 9168 <span class="hljs-string"><span class="hljs-string">"https://example.com/sign-up"</span></span> <span class="hljs-string"><span class="hljs-string">"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36"</span></span> <span class="hljs-string"><span class="hljs-string">"-"</span></span></code> </pre> <br><p>  They can be parsed, but it is much easier to fix the Nginx configuration so that it issues logs in JSON: </p><br><pre> <code class="plaintext hljs">log_format json_combined escape=json '{ "created_at": "$msec", ' '"remote_addr": "$remote_addr", ' '"remote_user": "$remote_user", ' '"request": "$request", ' '"status": $status, ' '"bytes_sent": $bytes_sent, ' '"request_length": $request_length, ' '"request_time": $request_time, ' '"http_referrer": "$http_referer", ' '"http_x_forwarded_for": "$http_x_forwarded_for", ' '"http_user_agent": "$http_user_agent" }'; access_log /var/log/nginx/access.log json_combined;</code> </pre> <br><h3 id="s3-dlya-hraneniya">  S3 storage </h3><br><p>  To store logs, we will use S3.  This allows you to store and analyze logs in one place, since Athena can work with data in S3 directly.  Further in the article I will tell you how to correctly add and process logs, but first we need a clean bake in S3, in which nothing else will be stored.  It is worthwhile to think in advance in which region you will create the bucket, because Athena is not available in all regions. </p><br><h3 id="sozdaem-shemu-v-konsoli-athena">  Create a schema in the console Athena </h3><br><p>  Create a table in Athena for logs.  It is needed both for writing and reading if you plan to use Kinesis Firehose.  Open the Athena console and create a table: </p><br><div class="spoiler">  <b class="spoiler_title">SQL table creation</b> <div class="spoiler_text"><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXTERNAL</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-string"><span class="hljs-string">`kinesis_logs_nginx`</span></span>( <span class="hljs-string"><span class="hljs-string">`created_at`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>, <span class="hljs-string"><span class="hljs-string">`remote_addr`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>, <span class="hljs-string"><span class="hljs-string">`remote_user`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>, <span class="hljs-string"><span class="hljs-string">`request`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>, <span class="hljs-string"><span class="hljs-string">`status`</span></span> <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-string"><span class="hljs-string">`bytes_sent`</span></span> <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-string"><span class="hljs-string">`request_length`</span></span> <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-string"><span class="hljs-string">`request_time`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>, <span class="hljs-string"><span class="hljs-string">`http_referrer`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>, <span class="hljs-string"><span class="hljs-string">`http_x_forwarded_for`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>, <span class="hljs-string"><span class="hljs-string">`http_user_agent`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">ROW</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span> SERDE <span class="hljs-string"><span class="hljs-string">'org.apache.hadoop.hive.ql.io.orc.OrcSerde'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">STORED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> INPUTFORMAT <span class="hljs-string"><span class="hljs-string">'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'</span></span> OUTPUTFORMAT <span class="hljs-string"><span class="hljs-string">'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'</span></span> LOCATION <span class="hljs-string"><span class="hljs-string">'s3://&lt;YOUR-S3-BUCKET&gt;'</span></span> TBLPROPERTIES (<span class="hljs-string"><span class="hljs-string">'has_encrypted_data'</span></span>=<span class="hljs-string"><span class="hljs-string">'false'</span></span>);</code> </pre> </div></div><br><h3 id="sozdaem-kinesis-firehose-stream">  Create Kinesis Firehose Stream </h3><br><p>  Kinesis Firehose will record the data received from Nginx in S3 in the selected format, breaking up the directories in the format YYYY / MM / DD / HH.  This is useful when reading data.  You can, of course, write directly to S3 from fluentd, but in this case you have to write JSON, which is inefficient due to the large file size.  Also, when using PrestoDB or Athena, JSON is the slowest data format.  So open the Kinesis Firehose console, click "Create delivery stream", select "direct PUT" in the "delivery" field: </p><br><p><img src="https://habrastorage.org/webt/xe/vb/mc/xevbmcpntl3aqkgg50lh844gxe4.png" alt="Kinesis Firehose Console 1"></p><br><p>  In the next tab, select "Record format conversion" - "Enabled" and select "Apache ORC" as the format for the record.  According to some <a href="https://www.slideshare.net/oom65/file-format-benchmarks-avro-json-orc-parquet">Owen O'Malley</a> research, this is the optimal format for PrestoDB and Athena.  As a schema, we specify the table we created above.  Please note that you can specify any S3 location in kinesis, only the scheme is used from the table.  But if you specify another S3 location, then you will not be able to read these records from this table. </p><br><p><img src="https://habrastorage.org/webt/vu/y1/ro/vuy1royrwm3pb3jle5nvclzskd0.png" alt="Kinesis Firehose Console 2"></p><br><p>  We select S3 for storage and the buck we created earlier.  Aws Glue Crawler, about which I will talk about a little later, does not know how to work with prefixes in the S3 bucket, so it is important to leave it empty. </p><br><p><img src="https://habrastorage.org/webt/jq/0u/bs/jq0ubs7jvqmehbfqaaycrmxryso.png" alt="Kinesis Firehose Console 3"></p><br><p>  The remaining options can be changed depending on your load, I usually use the default.  Note that S3 compression is not available, but ORC uses native compression by default. </p><br><h3 id="fluentd">  Fluentd </h3><br><p>  Now that we have configured to store and receive logs, we need to configure the send.  We will use <a href="https://www.fluentd.org/">Fluentd</a> because I love Ruby, but you can use Logstash or send logs to kinesis directly.  Fluentd server can be started in several ways, I will tell you about docker, because it is easy and convenient. </p><br><p>  First of all, we need the fluent.conf configuration file.  Create it and add source: </p><br><br><p>  <a href="https://habr.com/ru/users/type/" class="user_link">type</a> forward <br>  port 24224 <br>  bind 0.0.0.0 </p><br><br><p>  Now you can start the Fluentd server.  If you need a more advanced configuration, the <a href="https://hub.docker.com/r/fluent/fluentd/">Docker Hub</a> has a detailed guide, including how to build your image. </p><br><pre> <code class="bash hljs">$ docker run \ -d \ -p 24224:24224 \ -p 24224:24224/udp \ -v /data:/fluentd/<span class="hljs-built_in"><span class="hljs-built_in">log</span></span> \ -v &lt;PATH-TO-FLUENT-CONF&gt;:/fluentd/etc fluentd \ -c /fluentd/etc/fluent.conf fluent/fluentd:stable</code> </pre> <br><p>  This configuration uses the path <code>/fluentd/log</code> to cache the logs before sending.  You can do without it, but then when you restart, you can lose all cached by overwork.  Any port can also be used, 24224 is the default port of Fluentd. </p><br><p>  Now that we have Fluentd running, we can send Nginx logs there.  We usually run Nginx in a Docker container, in which case Docker has a native log driver for Fluentd: </p><br><pre> <code class="bash hljs">$ docker run \ --<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-driver=fluentd \ --<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-opt fluentd-address=&lt;FLUENTD-SERVER-ADDRESS&gt;\ --<span class="hljs-built_in"><span class="hljs-built_in">log</span></span>-opt tag=\<span class="hljs-string"><span class="hljs-string">"{{.Name}}\" \ -v /some/content:/usr/share/nginx/html:ro \ -d \ nginx</span></span></code> </pre> <br><p>  If you run Nginx differently, you can use log files, Fluentd has a <a href="https://docs.fluentd.org/v1.0/articles/in_tail">file tail plugin</a> . </p><br><p>  Add the parsing of logs configured above to the Fluent configuration: </p><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">filter</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">YOUR-NGINX-TAG.</span></span></span><span class="hljs-tag">*&gt;</span></span> @type parser key_name log emit_invalid_record_to_error false <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">parse</span></span></span><span class="hljs-tag">&gt;</span></span> @type json <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">parse</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">filter</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br><p>  And sending logs to Kinesis using the <a href="https://github.com/awslabs/aws-fluent-plugin-kinesis">kinesis firehose plugin</a> : </p><br><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">match</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">YOUR-NGINX-TAG.</span></span></span><span class="hljs-tag">*&gt;</span></span> @type kinesis_firehose region region delivery_stream_name <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">YOUR-KINESIS-STREAM-NAME</span></span></span><span class="hljs-tag">&gt;</span></span> aws_key_id <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">YOUR-AWS-KEY-ID</span></span></span><span class="hljs-tag">&gt;</span></span> aws_sec_key <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">YOUR_AWS-SEC_KEY</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">match</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br><h2 id="athena">  Athena </h2><br><p>  If you have configured everything correctly, after a while (by default, Kinesis records the data every 10 minutes) you should see the log files in S3.  In the "monitoring" menu of Kinesis Firehose, you can see how much data is recorded in S3, as well as errors.  Do not forget to allow write access to the S3 bakery for the Kinesis role.  If Kinesis could not parse something, it will add errors in the same bucket. </p><br><p>  Now you can see the data in Athena.  Let's find fresh requests for which we have given errors: </p><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> <span class="hljs-string"><span class="hljs-string">"db_name"</span></span>.<span class="hljs-string"><span class="hljs-string">"table_name"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">status</span></span> &gt; <span class="hljs-number"><span class="hljs-number">499</span></span> <span class="hljs-keyword"><span class="hljs-keyword">ORDER</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> created_at <span class="hljs-keyword"><span class="hljs-keyword">DESC</span></span> <span class="hljs-keyword"><span class="hljs-keyword">limit</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span>;</code> </pre> <br><h3 id="skanirovanie-vseh-zapisey-na-kazhdyy-zapros">  Scan all records for each request </h3><br><p>  Now our logs are processed and folded in S3 in ORC, compressed and ready for analysis.  Kinesis Firehose even laid them out in directories for every hour.  However, while the table is not partitioned, Athena will load data for the entire time for each query, with rare exceptions.  This is a big problem for two reasons: </p><br><ul><li>  The volume of data is constantly growing, slowing down requests; </li><li>  Athena is billed based on the size of the scanned data, with a minimum of 10 MB for each request. </li></ul><br><p>  To fix this, we use AWS Glue Crawler, which will scan the data in S3 and record the information about the partitions in the Glue Metastore.  This will allow us to use partitions as a filter when querying in Athena, and it will only scan the directories specified in the query. </p><br><h3 id="nastraivaem-amazon-glue-crawler">  Configuring Amazon Glue Crawler </h3><br><p>  Amazon Glue Crawler scans all the data in the S3 bucket and creates tables with partitions.  Create a Glue Crawler from the AWS Glue console and add a batch in which you store the data.  You can use one crawler for several buckets, in which case it will create tables in the specified database with names matching the bucket names.  If you plan to constantly use this data, do not forget to customize the launch schedule for the Crawler to suit your needs.  We use one Crawler for all tables, which runs every hour. </p><br><h3 id="particirovannye-tablicy">  Partial tables </h3><br><p>  After the first launch of the crawler, tables for each scanned bake should appear in the database specified in the settings.  Open the Athena console and find the table with Nginx logs.  Let's try to read something: </p><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> <span class="hljs-string"><span class="hljs-string">"default"</span></span>.<span class="hljs-string"><span class="hljs-string">"part_demo_kinesis_bucket"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span>( partition_0 = <span class="hljs-string"><span class="hljs-string">'2019'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> partition_1 = <span class="hljs-string"><span class="hljs-string">'04'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> partition_2 = <span class="hljs-string"><span class="hljs-string">'08'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AND</span></span> partition_3 = <span class="hljs-string"><span class="hljs-string">'06'</span></span> );</code> </pre> <br><p>  This query will select all entries received from 6 to 7 am on April 8, 2019.  But how much more efficient is it than just reading from a non-partitioned table?  Let's find out and select the same records by filtering them by timestamp: </p><br><p><img src="https://habrastorage.org/webt/mu/bg/me/mubgmef262bxyte5dsqsa1q_hag.png" alt="Request without partitions"></p><br><p>  3.59 seconds and 244.34 megabytes of data on dataset, in which there is only a week of logs.  Let's try to filter by partitions: </p><br><p><img src="https://habrastorage.org/webt/9-/n4/dc/9-n4dczjdvcrspm0zbypq-ul5cs.png" alt="Request with filter by partitions"></p><br><p>  Slightly faster, but the most important - only 1.23 megabytes of data!  It would be much cheaper if it were not for the minimum 10 megabytes per request in pricing.  But all the same it is much better, but on large datasets the difference will be much more impressive. </p><br><h2 id="sobiraem-deshbord-s-pomoschyu-cubejs">  Build a dashboard using Cube.js </h2><br><p>  To build a dashboard, we use the analytical framework Cube.js.  It has quite a lot of functions, but we are interested in two: the ability to automatically use filters by partitions and pre-aggregation of data.  It uses the <a href="https://cube.dev/docs/getting-started-cubejs-schema">data schema</a> , written in Javascript, to generate SQL and execute a database query.  We are only required to specify how to use the filter by partitions in the data schema. </p><br><p>  Create a new application Cube.js.  Since we are already using the AWS stack, it is logical to use Lambda for deployment.  You can use the express template to generate if you plan to host the Cube.js backend in Heroku or Docker.  The documentation describes other <a href="https://cube.dev/docs/deployment">ways of hosting</a> . </p><br><pre> <code class="bash hljs">$ npm install -g cubejs-cli $ cubejs create nginx-log-analytics -t serverless -d athena</code> </pre> <br><p>  To set up access to the database, cube.js uses environment variables.  The generator will create a .env file in which you can specify your keys for <a href="https://cube.dev/docs/connecting-to-the-database">Athena</a> . </p><br><p>  Now we need <a href="https://cube.dev/docs/getting-started-cubejs-schema">a data scheme</a> in which we indicate exactly how our logs are stored.  There you can also specify how to calculate the metrics for dashboards. </p><br><p>  In the <code>schema</code> directory, create a <code>Logs.js</code> file.  Here is an example of a data model for nginx: </p><br><div class="spoiler">  <b class="spoiler_title">Model code</b> <div class="spoiler_text"><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> partitionFilter = <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">from</span></span></span></span><span class="hljs-function"><span class="hljs-params">, to</span></span></span><span class="hljs-function">) =&gt;</span></span> <span class="hljs-string"><span class="hljs-string">` date(from_iso8601_timestamp(</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${</span></span><span class="hljs-keyword"><span class="hljs-string"><span class="hljs-subst"><span class="hljs-keyword">from</span></span></span></span><span class="hljs-string"><span class="hljs-subst">}</span></span></span><span class="hljs-string">)) &lt;= date_parse(partition_0 || partition_1 || partition_2, '%Y%m%d') AND date(from_iso8601_timestamp(</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${to}</span></span></span><span class="hljs-string">)) &gt;= date_parse(partition_0 || partition_1 || partition_2, '%Y%m%d') `</span></span> cube(<span class="hljs-string"><span class="hljs-string">`Logs`</span></span>, { <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">` select * from part_demo_kinesis_bucket WHERE </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${FILTER_PARAMS.Logs.createdAt.filter(partitionFilter)}</span></span></span><span class="hljs-string"> `</span></span>, <span class="hljs-attr"><span class="hljs-attr">measures</span></span>: { <span class="hljs-attr"><span class="hljs-attr">count</span></span>: { <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`count`</span></span>, }, <span class="hljs-attr"><span class="hljs-attr">errorCount</span></span>: { <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`count`</span></span>, <span class="hljs-attr"><span class="hljs-attr">filters</span></span>: [ { <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">`</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${CUBE.isError}</span></span></span><span class="hljs-string"> = 'Yes'`</span></span> } ] }, <span class="hljs-attr"><span class="hljs-attr">errorRate</span></span>: { <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`number`</span></span>, <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">`100.0 * </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${errorCount}</span></span></span><span class="hljs-string"> / </span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${count}</span></span></span><span class="hljs-string">`</span></span>, <span class="hljs-attr"><span class="hljs-attr">format</span></span>: <span class="hljs-string"><span class="hljs-string">`percent`</span></span> } }, <span class="hljs-attr"><span class="hljs-attr">dimensions</span></span>: { <span class="hljs-attr"><span class="hljs-attr">status</span></span>: { <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">`status`</span></span>, <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`number`</span></span> }, <span class="hljs-attr"><span class="hljs-attr">isError</span></span>: { <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`string`</span></span>, <span class="hljs-attr"><span class="hljs-attr">case</span></span>: { <span class="hljs-attr"><span class="hljs-attr">when</span></span>: [{ <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">`</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${CUBE}</span></span></span><span class="hljs-string">.status &gt;= 400`</span></span>, <span class="hljs-attr"><span class="hljs-attr">label</span></span>: <span class="hljs-string"><span class="hljs-string">`Yes`</span></span> }], <span class="hljs-attr"><span class="hljs-attr">else</span></span>: { <span class="hljs-attr"><span class="hljs-attr">label</span></span>: <span class="hljs-string"><span class="hljs-string">`No`</span></span> } } }, <span class="hljs-attr"><span class="hljs-attr">createdAt</span></span>: { <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: <span class="hljs-string"><span class="hljs-string">`from_unixtime(created_at)`</span></span>, <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`time`</span></span> } } });</code> </pre> </div></div><br><p>  Here we use the <a href="https://cube.dev/docs/cube">FILTER_PARAMS</a> variable to generate a SQL query with a filter by partitions. </p><br><p>  We also set the metrics and parameters that we want to display on the dashboard, and specify the pre-aggregation.  Cube.js will create additional tables with pre-aggregated data and will automatically update the data as it is received.  This not only speeds up requests, but also reduces the cost of using Athena. </p><br><p>  Add this information to the data schema file: </p><br><pre> <code class="javascript hljs">preAggregations: { <span class="hljs-attr"><span class="hljs-attr">main</span></span>: { <span class="hljs-attr"><span class="hljs-attr">type</span></span>: <span class="hljs-string"><span class="hljs-string">`rollup`</span></span>, <span class="hljs-attr"><span class="hljs-attr">measureReferences</span></span>: [count, errorCount], <span class="hljs-attr"><span class="hljs-attr">dimensionReferences</span></span>: [isError, status], <span class="hljs-attr"><span class="hljs-attr">timeDimensionReference</span></span>: createdAt, <span class="hljs-attr"><span class="hljs-attr">granularity</span></span>: <span class="hljs-string"><span class="hljs-string">`day`</span></span>, <span class="hljs-attr"><span class="hljs-attr">partitionGranularity</span></span>: <span class="hljs-string"><span class="hljs-string">`month`</span></span>, <span class="hljs-attr"><span class="hljs-attr">refreshKey</span></span>: { <span class="hljs-attr"><span class="hljs-attr">sql</span></span>: FILTER_PARAMS.Logs.createdAt.filter(<span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">from</span></span></span></span><span class="hljs-function"><span class="hljs-params">, to</span></span></span><span class="hljs-function">) =&gt;</span></span> <span class="hljs-string"><span class="hljs-string">`select CASE WHEN from_iso8601_timestamp(</span><span class="hljs-subst"><span class="hljs-string"><span class="hljs-subst">${to}</span></span></span><span class="hljs-string">) + interval '3' day &gt; now() THEN date_trunc('hour', now()) END`</span></span> ) } } }</code> </pre> <br><p>  We indicate in this model that it is necessary to pre-aggregate data for all used metrics, and to use partitioning by months.  <a href="https://cube.dev/docs/pre-aggregations">Partitioning pre-aggregations</a> can significantly speed up data collection and update. </p><br><p>  Now we can put together a dashboard! </p><br><p>  The Cube.js backend provides a <a href="https://cube.dev/docs/rest-api">REST API</a> and a set of client libraries for popular front-end frameworks.  We will use the React-version of the client to build a dashboard.  Cube.js provides only data, so we need a library for visualization - I like <a href="http://recharts.org/">recharts</a> , but you can use any. </p><br><p>  The Cube.js server accepts the request in <a href="https://cube.dev/docs/query-format">JSON format</a> , in which the required metrics are specified.  For example, to calculate how many errors gave Nginx by day, you need to send such a request: </p><br><pre> <code class="json hljs">{ <span class="hljs-attr"><span class="hljs-attr">"measures"</span></span>: [<span class="hljs-string"><span class="hljs-string">"Logs.errorCount"</span></span>], <span class="hljs-attr"><span class="hljs-attr">"timeDimensions"</span></span>: [ { <span class="hljs-attr"><span class="hljs-attr">"dimension"</span></span>: <span class="hljs-string"><span class="hljs-string">"Logs.createdAt"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"dateRange"</span></span>: [<span class="hljs-string"><span class="hljs-string">"2019-01-01"</span></span>, <span class="hljs-string"><span class="hljs-string">"2019-01-07"</span></span>], <span class="hljs-attr"><span class="hljs-attr">"granularity"</span></span>: <span class="hljs-string"><span class="hljs-string">"day"</span></span> } ] }</code> </pre> <br><p>  Install the Cube.js client and the React-component library via NPM: </p><br><pre> <code class="bash hljs">$ npm i --save @cubejs-client/core @cubejs-client/react</code> </pre> <br><p>  We import the <code>cubejs</code> and QueryRenderer components to upload data, and collect the dashboard: </p><br><div class="spoiler">  <b class="spoiler_title">Dashboard Code</b> <div class="spoiler_text"><pre> <code class="javascript hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> React <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'react'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> { LineChart, Line, XAxis, YAxis } <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'recharts'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cubejs <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@cubejs-client/core'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> { QueryRenderer } <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> <span class="hljs-string"><span class="hljs-string">'@cubejs-client/react'</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> cubejsApi = cubejs( <span class="hljs-string"><span class="hljs-string">'YOUR-CUBEJS-API-TOKEN'</span></span>, { <span class="hljs-attr"><span class="hljs-attr">apiUrl</span></span>: <span class="hljs-string"><span class="hljs-string">'http://localhost:4000/cubejs-api/v1'</span></span> }, ); <span class="hljs-keyword"><span class="hljs-keyword">export</span></span> <span class="hljs-keyword"><span class="hljs-keyword">default</span></span> () =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-function"><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"> &lt;QueryRenderer query={{ measures: [</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Logs.errorCount'</span></span></span></span><span class="hljs-function"><span class="hljs-params">], timeDimensions: [{ dimension: </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'Logs.createdAt'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, dateRange: [</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'2019-01-01'</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'2019-01-07'</span></span></span></span><span class="hljs-function"><span class="hljs-params">], granularity: </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">'day'</span></span></span></span><span class="hljs-function"><span class="hljs-params"> }] }} cubejsApi={cubejsApi} render={({ resultSet }</span></span></span><span class="hljs-function">) =&gt;</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!resultSet) { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-string"><span class="hljs-string">'Loading...'</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> ( &lt;LineChart data={resultSet.rawData()}&gt; &lt;XAxis dataKey="Logs.createdAt"/&gt; &lt;YAxis/&gt; &lt;Line type="monotone" dataKey="Logs.errorCount" stroke="#8884d8"/&gt; &lt;/LineChart&gt; ); }} /&gt; ) }</code> </pre> </div></div><br><p>  Dashboard sources are available on <a href="https://codesandbox.io/s/l5qyrzlw07%3Ffontsize%3D14">CodeSandbox</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/447886/">https://habr.com/ru/post/447886/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../447874/index.html">Chaos Information Entropy</a></li>
<li><a href="../447876/index.html">Everything is very bad or a new type of traffic interception</a></li>
<li><a href="../447880/index.html">Checking rdesktop and xrdp using PVS-Studio analyzer</a></li>
<li><a href="../447882/index.html">Network tools, or where to start pentester?</a></li>
<li><a href="../447884/index.html">We understand how 5G will work in the millimeter range on the street and in rooms</a></li>
<li><a href="../447890/index.html">Thank god i'm not a manager</a></li>
<li><a href="../447892/index.html">Two new PHDays contests: IDS bypass and factory hacking</a></li>
<li><a href="../447894/index.html">MODX Digest # 3 (March 25 - April 8, 2019)</a></li>
<li><a href="../447896/index.html">Photos from rough sketches: how exactly does the NVIDIA GauGAN neural network work?</a></li>
<li><a href="../447900/index.html">Close contacts of ADL degree</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>