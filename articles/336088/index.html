<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The future of web technologies: creating an intelligent chat bot that can hear and speak</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Nowadays, voice interfaces are omnipresent. First, more and more mobile phone users are using voice assistants such as Siri and Cortana. Secondly, dev...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The future of web technologies: creating an intelligent chat bot that can hear and speak</h1><div class="post__text post__text-html js-mediator-article">  Nowadays, voice interfaces are omnipresent.  First, more and more mobile phone users are using voice assistants such as Siri and Cortana.  Secondly, devices like Amazon Echo and <a href="https://www.smashingmagazine.com/2017/05/build-action-google-home-api-ai/">Google Home</a> are becoming a familiar element of the interior.  These systems are based on speech recognition software that allows users to communicate with machines using <a href="https://www.smashingmagazine.com/2017/05/designing-voice-experiences/">voice commands</a> .  Now the relay, in the guise of Web Speech API, goes to browsers. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/197/2de/470/1972de4702fcb6ad86becd3ab7e1a853.jpg" alt="image"></div><a name="habracut"></a><br>  During the development of a web application, we can rely on various graphical controls for organizing user interaction.  Web Speech API allows you to integrate into a person's natural ways of voice communication with a minimal visual interface.  We have at our disposal countless options for applying new technology, enriching the capabilities of programs.  In addition, the Web Speech API can make working with web applications more convenient for people with physical or cognitive disabilities or injuries.  Thus, the web space of the future may well become more sociable and accessible. <br><br>  The Web Speech API allows web sites and web applications not only to talk with the user, but also to perceive his speech.  Take a look at at least a few excellent <a href="https://www.smashingmagazine.com/2014/12/enhancing-ux-with-the-web-speech-api/">examples</a> of how this can be used to expand the possibilities of interaction of programs with a person. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Today we will talk about how to create an intelligent browser-based voice chat using the Web Speech API.  The program will listen to the user and respond to his replicas with a synthesized voice.  Since the Web Speech API is still experimental, the application will only work in <a href="http://caniuse.com/">browsers</a> that support this API. <br><br>  The features used in this material, both speech recognition and speech synthesis, are now supported only by Chromium-based browsers, including Chrome 25+ and Opera 27+, while Firefox, Edge and Safari only support speech synthesis. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/099/168/c9f/099168c9fcf700020ea6bd8daef18bfc.png"><br>  <i><font color="#999999">Support for speech synthesis and recognition in various browsers</font></i> <br><br>  <a href="https://vimeo.com/215612852">Here is a video</a> that shows how what we are going to create works in Chrome. <br><br>  Work on a web application consists of three main steps: <br><br><ol><li> Using the <code>SpeechRecognition</code> interface from the Web Speech API to organize the program's perception of the user's voice <br></li><li>  Transforming what the user said to text, and sending the request to a commercial natural language processing API, in our case API.AI. <br></li><li>  Getting a response from API.AI and voicing it using the <code>SpeechSynthesis</code> interface. <br></li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/bab/a93/91a/baba9391a5220d3568b761373c6ba970.png"><br>  <i><font color="#999999">Diagram of application interaction with the user and external services</font></i> <br><br>  <a href="https://github.com/girliemac/web-speech-ai">The full source code</a> for what we will create here can be found on GitHub. <br><br>  This tutorial is based on Node.js.  In order to successfully deal with it, you need to know JavaScript and have a basic understanding of Node.js.  Check if you have a Node installed and can get to work. <br><br><h2>  <font color="#3AC1EF">Preparing Node.js Applications</font> </h2><br>  To begin with we will prepare a framework of Node.js-applications.  Create a folder and place the following in it: <br><br><pre> <code class="hljs css">. ‚îú‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">index</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.js</span></span> ‚îú‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">public</span></span> ‚îÇ   ‚îú‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">css</span></span> ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">style</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.css</span></span> ‚îÇ   ‚îî‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">js</span></span> ‚îÇ       ‚îî‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">script</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.js</span></span> ‚îî‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">views</span></span>   ‚îî‚îÄ‚îÄ <span class="hljs-selector-tag"><span class="hljs-selector-tag">index</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.html</span></span></code> </pre> <br>  Then run this command to initialize the application: <br><br><pre> <code class="hljs swift">$ npm <span class="hljs-keyword"><span class="hljs-keyword">init</span></span> -f</code> </pre> <br>  The <code>-f</code> allows you to apply the default settings, but you can configure the application manually without using this switch.  During initialization, a <code>package.json</code> file will be created that contains basic information about the application. <br><br>  Now you need to install the dependencies required to build the program: <br><br><pre> <code class="hljs sql">$ npm <span class="hljs-keyword"><span class="hljs-keyword">install</span></span> express socket.io apiai <span class="hljs-comment"><span class="hljs-comment">--save</span></span></code> </pre> <br>  Using the <code>--save</code> flag <code>--save</code> automatically update the <code>package.json</code> file.  Dependency information will be added to it. <br><br>  We are going to use <a href="https://expressjs.com/">Express</a> , a server-side framework for Node.js applications that will run on a local server.  In order to organize bidirectional data exchange between the server and the client in real time, apply <a href="https://socket.io/">Socket.IO</a> .  In addition, we will install a tool to interact with the <a href="https://api.ai/">API.AI</a> natural language processing service.  This will allow us to create an intelligent chat bot capable of maintaining a conversation with a person. <br><br>  Socket.IO is a library that simplifies the use of WebSockets technology in Node.js.  After establishing a connection between the client and the server based on the sockets, the messages between them will be transmitted very quickly.  Namely, it will happen, on the client side, at the moment when the Web Speech API converts the user's phrase into a text message, and, on the server side, when the text of the artificial intelligence response comes from API.AI. <br><br>  Create an <code>index.js</code> file, prepare an instance of Express, and wait for connections. <br><br><pre> <code class="hljs coffeescript">const express = <span class="hljs-built_in"><span class="hljs-built_in">require</span></span>(<span class="hljs-string"><span class="hljs-string">'express'</span></span>); const app = express(); app.use(express.static(__dirname + <span class="hljs-string"><span class="hljs-string">'/views'</span></span>)); <span class="hljs-regexp"><span class="hljs-regexp">//</span></span> html app.use(express.static(__dirname + <span class="hljs-string"><span class="hljs-string">'/public'</span></span>)); <span class="hljs-regexp"><span class="hljs-regexp">//</span></span> js, css, images const server = app.listen(<span class="hljs-number"><span class="hljs-number">5000</span></span>); app.get(<span class="hljs-string"><span class="hljs-string">'/'</span></span>, <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(req, res)</span></span></span><span class="hljs-function"> =&gt;</span></span> { res.sendFile(<span class="hljs-string"><span class="hljs-string">'index.html'</span></span>); });</code> </pre> <br>  Now let's work on the client part of the system.  In the next step, we integrate the Web Speech API into the application interface. <br><br><h2>  <font color="#3AC1EF">Speech Recognition with SpeechRecognition</font> </h2><br>  The Web Speech API has a main controller interface called <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">SpeechRecognition</a> .  It allows you to convert to text what the user said into the microphone. <br><br>  The user interface of this application is very simple.  Its main element is a button, pressing which starts voice recognition.  Configure the <code>index.html</code> file, connect the file with client scripts ( <code>script.js</code> ) and the Socket.IO library, which we will use to exchange data with the server later: <br><br><pre> <code class="hljs xml"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">html</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">lang</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"en"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">head</span></span></span><span class="hljs-tag">&gt;</span></span>‚Ä¶<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">head</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">body</span></span></span><span class="hljs-tag">&gt;</span></span>   ‚Ä¶   <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">script</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">src</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"https://cdnjs.cloudflare.com/ajax/libs/socket.io/2.0.1/socket.io.js"</span></span></span><span class="hljs-tag">&gt;</span></span><span class="undefined"></span><span class="hljs-tag"><span class="undefined"></span><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">script</span></span></span><span class="hljs-tag">&gt;</span></span>   <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">script</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">src</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"js/script.js"</span></span></span><span class="hljs-tag">&gt;</span></span><span class="undefined"></span><span class="hljs-tag"><span class="undefined"></span><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">script</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">body</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">html</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br>  Add a button to the body of an HTML document: <br><br><pre> <code class="hljs xml"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">button</span></span></span><span class="hljs-tag">&gt;</span></span>Talk<span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">button</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre> <br>  To style a button, use the <code>style.css</code> file, which can be found in <a href="https://github.com/girliemac/web-speech-ai">the example code</a> . <br><br><h2>  <font color="#3AC1EF">Voice Capture Using JavaScript</font> </h2><br>  In <code>script.js</code> , for voice recognition, let's create an instance of <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">SpeechRecognition</a> voice recognition services controller interface from the Web Speech API: <br><br><pre> <code class="hljs javascript"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> SpeechRecognition = <span class="hljs-built_in"><span class="hljs-built_in">window</span></span>.SpeechRecognition || <span class="hljs-built_in"><span class="hljs-built_in">window</span></span>.webkitSpeechRecognition; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> recognition = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SpeechRecognition();</code> </pre> <br>  Here we use the names of objects with and without prefix, since Chrome now supports the API we need only in the version with the prefix <code>webkit</code> . <br><br>  In addition, we use ECMAScript6 syntax.  It includes support for the <code>const</code> keyword and switch functions.  All this is available in browsers that support both Speech APIs - <code>SpeechRecognition</code> and <code>SpeechSynthesis</code> . <br><br>  If desired, speech recognition can be customized by setting various <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechRecognition">properties of the</a> corresponding object.  We apply the following settings: <br><br><pre> <code class="hljs cs">recognition.lang = <span class="hljs-string"><span class="hljs-string">'en-US'</span></span>; recognition.interimResults = <span class="hljs-literal"><span class="hljs-literal">false</span></span>;</code> </pre> <br>  Then we need a link to the button.  We will listen to the click event that is used to start the speech recognition process. <br><br>  After running recognition, use the <code>result</code> event to get a textual representation of what the user said: <br><br><pre> <code class="hljs markdown">recognition.addEventListener('result', (e) =&gt; { let last = e.results.length - 1; let text = e.results[<span class="hljs-string"><span class="hljs-string">last</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">0</span></span>].transcript; console.log('Confidence: ' + e.results[<span class="hljs-string"><span class="hljs-string">0</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">0</span></span>].confidence); //     Socket.IO });</code> </pre> <br>  Here we are interested in the <code>SpeechRecognitionResultList</code> object, which contains the result of speech recognition.  Text can be extracted from the corresponding array.  In addition, as you can see, here we are outputting the <code>confidence</code> indicator for the received text in the console. <br><br>  Now we will use Socket.IO to transfer data to the server code. <br><br><h2>  <font color="#3AC1EF">Real-time data exchange using Socket.IO</font> </h2><br>  <a href="https://socket.io/">Socket.IO</a> is a library designed for real-time web applications.  It allows you to organize bidirectional data exchange between the client and server parts of the system.  We are going to use this library to transfer the textual representation of the recognized speech to the code running on the Node.js server, and then to send the server's response to the browser. <br><br>  You may be wondering why we do not use simple HTTP or AJAX requests.  Data could be sent to the server, for example, using a POST request.  However, we use WebSockets via Socket.IO, since this is the most successful solution for organizing bi-directional communication, especially for those cases when events are transmitted from the server to the browser.  Thanks to a persistent socket-based connection, we don‚Äôt need to reload the data in the browser or send AJAX requests often. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a2c/6d3/405/a2c6d3405cfbe0c0779067f2c820498c.png"><br>  <i><font color="#999999">Interaction between client, server, and third-party natural language processing service</font></i> <br><br>  On the client, in the <code>script.js</code> file, create an instance of Socket.IO: <br><br><pre> <code class="hljs objectivec"><span class="hljs-keyword"><span class="hljs-keyword">const</span></span> socket = io();</code> </pre> <br>  Then we add the following code to where we handle the <code>result</code> event from <code>SpeechRecognition</code> : <br><br><pre> <code class="hljs mel">socket.<span class="hljs-keyword"><span class="hljs-keyword">emit</span></span>(<span class="hljs-string"><span class="hljs-string">'chat message'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">text</span></span>);</code> </pre> <br>  We now turn to the server side of the application.  It should receive the text transmitted by the client and transfer it to a third-party service that implements the functions of artificial intelligence.  After receiving a response from the service, the server must transmit it to the client. <br><br><h2>  <font color="#3AC1EF">Getting a response from an external service</font> </h2><br>  Many platforms and services allow you to integrate an application with an artificial intelligence system using speech-to-text conversion and natural language processing.  Among them - <a href="https://www.ibm.com/watson/">IBM Watson</a> , <a href="https://www.luis.ai/">Microsoft LUIS</a> , and <a href="https://wit.ai/">Wit.ai.</a>  To quickly build a voice interface, we will use <a href="https://api.ai/">API.AI.</a>  Here you can get a free developer account and quickly set up the core of a simple chat bot using the web service interface and library for Node.js. <br><br>  After you have created an account, create an agent.  You can read about this at the beginning of the <a href="https://docs.api.ai/docs/get-started">getting</a> started <a href="https://docs.api.ai/docs/get-started">guide</a> .  Full customization involves creating entities and setting up links between phrases spoken by the user and actions performed by the system.  We will not do this by using the Small Talk preset.  This item should be selected in the left menu, and then, using the switch, enable the service. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a5/ec4/14f/8a5ec414fb3ca0c9e6f6aa1f9c2fc417.png"><br>  <i><font color="#999999">Configuring Small Talk in API.AI</font></i> <br><br>  Immediately, using the API.AI interface, configure the agent.  Go to the General Settings page by clicking the gear icon next to the agent name and get the API key.  You will need what is called here "client access token" - the service access token.  We will use this token in the Node.js SDK. <br><br><h2>  <font color="#3AC1EF">Using the SDK API.AI for Node.js</font> </h2><br>  We connect our Node.js application to API.AI using the appropriate SDK.  In the <code>index.js</code> file, <code>index.js</code> initialize API.AI using the access token. <br><br><pre> <code class="hljs lisp">const apiai = require('apiai')(<span class="hljs-name"><span class="hljs-name">APIAI_TOKEN</span></span>)<span class="hljs-comment"><span class="hljs-comment">;</span></span></code> </pre> <br>  If you are going to run the code only locally, you can simply enter your API token here.  Otherwise, it is better to store it in an environment variable.  There are many ways to set environment variables, I usually write the corresponding data to a <code>.env</code> file.  In the code that is laid out on GitHub, this file does not exist, since it is included in <code>.gitignore</code> .  If you want to know how these files are organized, take a <a href="https://github.com/girliemac/web-speech-ai/blob/master/.env_test">look at the</a> <code>.env-test</code> <a href="https://github.com/girliemac/web-speech-ai/blob/master/.env_test">file</a> . <br><br>  So, we, on the server, use Socket.IO to get the result of speech recognition from the browser.  As soon as the connection is established and the message is delivered to the server, we will use the API.AI capabilities to get the bot's response to the user's statement: <br><br><pre> <code class="hljs coffeescript">io.<span class="hljs-literal"><span class="hljs-literal">on</span></span>(<span class="hljs-string"><span class="hljs-string">'connection'</span></span>, function(socket) { socket.<span class="hljs-literal"><span class="hljs-literal">on</span></span>(<span class="hljs-string"><span class="hljs-string">'chat message'</span></span>, <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(text)</span></span></span><span class="hljs-function"> =&gt;</span></span> {   <span class="hljs-regexp"><span class="hljs-regexp">//</span></span>    API.AI   let apiaiReq = apiai.textRequest(text, {     sessionId: APIAI_SESSION_ID   });   apiaiReq.<span class="hljs-literal"><span class="hljs-literal">on</span></span>(<span class="hljs-string"><span class="hljs-string">'response'</span></span>, <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(response)</span></span></span><span class="hljs-function"> =&gt;</span></span> {     let aiText = response.result.fulfillment.speech;     socket.emit(<span class="hljs-string"><span class="hljs-string">'bot reply'</span></span>, aiText); <span class="hljs-regexp"><span class="hljs-regexp">//</span></span> Send the result back to the browser!   });   apiaiReq.<span class="hljs-literal"><span class="hljs-literal">on</span></span>(<span class="hljs-string"><span class="hljs-string">'error'</span></span>, <span class="hljs-function"><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(error)</span></span></span><span class="hljs-function"> =&gt;</span></span> {     <span class="hljs-built_in"><span class="hljs-built_in">console</span></span>.log(error);   });   apiaiReq.end(); }); });</code> </pre> <br>  When API.AI returns the result, we use the <code>socket.emit()</code> Socket.IO method to send data to the browser. <br><br><h2>  <font color="#3AC1EF">SpeechSynthesis - voice for bot</font> </h2><br>  In order to complete the work on the application, let's return to the client file <code>script.js</code> . <br><br>  Create a function for voice synthesis.  Here we will use the <code>SpeechSynthesis</code> controller <code>SpeechSynthesis</code> from the Web Speech API. <br><br>  The function accepts, as an argument, a string, after which the system utters the text from this string: <br><br><pre> <code class="hljs javascript"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">synthVoice</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">text</span></span></span><span class="hljs-function">) </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> synth = <span class="hljs-built_in"><span class="hljs-built_in">window</span></span>.speechSynthesis; <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> utterance = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SpeechSynthesisUtterance(); utterance.text = text; synth.speak(utterance); }</code> </pre> <br>  In this function, we first create a link to the API <code>window.speechSynthesis</code> API entry point.  You may notice that this time we do not use properties with prefixes.  The fact is that support for this API is broader than that of <code>SpeechRecognition</code> , and all browsers that support this API have already removed the prefix for <code>SpeechSynthesis</code> . <br><br>  Then, using the constructor, we create a new <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance">SpeechSynthesisUtterance ()</a> object.  Next, set the <code>text</code> property of the <code>utterance</code> object.  Exactly what we write in this property, and say the machine.  You can set other <a href="https://developer.mozilla.org/en-US/docs/Web/API/SpeechSynthesisUtterance">properties here</a> , for example - the <code>voice</code> property to select the type of voice. <br><br>  And finally, we use the <code>SpeechSynthesis.speak()</code> call, thanks to which the computer utters the phrase. <br><br>  Now we will get a response from the server, again using Socket.IO.  Once the message is received, call the above function: <br><br><pre> <code class="hljs matlab">socket.on(<span class="hljs-string"><span class="hljs-string">'bot reply'</span></span>, <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(replyText)</span></span></span><span class="hljs-function"> { </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">synthVoice</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(replyText)</span></span></span><span class="hljs-function">; });</span></span></code> </pre> <br>  Now you can chat with our intelligent chat bot! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a1e/676/b65/a1e676b651b0757606fff4121f9706cc.gif"><br><br><h2>  <font color="#3AC1EF">Chatting with a chat bot</font> </h2><br>  Please note that the first time you try to communicate with the program, the browser will ask you for permission to use the microphone.  As in the case of other APIs (Geolocation, Notification), the browser asks for permission before working with confidential data.  As a result, your voice will not be recorded without your knowledge. <br><br>  If you chat with this bot for a while, you will be bored with chatter, as the AI ‚Äã‚Äãused here is very simple.  However, bots created on the basis of API.AI, can be configured and trained.  You can make your bot smarter by reading the API.AI <a href="https://docs.api.ai/">documentation</a> and working on its intelligence. <br><br>  It should be noted that this guide covers only the main features of the new API, but it is, in fact, very flexible and supports many settings.  You can change the language of speech recognition and synthesis, the synthesized voice, including the accent (for example, American and British English), the pitch of the voice, and the speed at which the words are pronounced.  Here are some useful links to help you learn more about the Web Speech API: <br><br><ul><li>  <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API">Web Speech API</a> (Mozilla Developer Network) <br></li><li>  <a href="https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html">Web Speech API Specification</a> (W3C) <br></li><li>  <a href="https://docs.microsoft.com/en-us/microsoft-edge/dev-guide/multimedia/web-speech-api">Web Speech API: Speech Synthesis</a> (Microsoft) <br></li></ul><br>  Here are useful materials on Node.js and on the libraries we used here: <br><br><ul><li>  <a href="https://nodejs.org/en/docs/guides/">Node.js tutorial</a> . <br></li><li>  <a href="https://docs.npmjs.com/">Documentation for npm</a> . <br></li><li>  <a href="https://expressjs.com/en/starter/hello-world.html">"Hello World" on Express</a> . <br></li><li>  <a href="https://socket.io/get-started/">Getting started in Socket.IO</a> . <br></li></ul><br>  And finally, take a look at the various natural language processing tools and platforms for communicating with computers: <br><br><ul><li>  <a href="https://api.ai/">API.AI</a> (Google) <br></li><li>  <a href="https://wit.ai/">Wit.ai</a> (Facebook) <br></li><li>  <a href="https://www.luis.ai/">LUIS</a> (Microsoft) <br></li><li>  <a href="https://www.ibm.com/watson/">Watson</a> (IBM) <br></li><li>  <a href="https://aws.amazon.com/lex/">Lex</a> (Amazon) <br></li></ul><br><h2>  <font color="#3AC1EF">Results: a step into the future of the web</font> </h2><br>  We believe that Google, Facebook, Microsoft, IBM and Amazon are far from being in the list of developers of platforms for processing natural language.  Does this mean that the future of the web is uniquely behind voice interfaces?  No, it cannot be stated with complete confidence.  However, this, as well as the fact that such interfaces are quite popular on mobile platforms, suggests that they also have a place in web applications.  Voice technologies and artificial intelligence will complement the usual ways of working with web content, making the Internet more convenient and accessible. <br><br>  I hope you liked our story about voice web technologies and you created an interesting virtual interlocutor. <br><br>  Dear readers!  What methods of application of speech recognition and speech synthesis in web applications seem to you the most interesting and promising? </div><p>Source: <a href="https://habr.com/ru/post/336088/">https://habr.com/ru/post/336088/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../336078/index.html">A brief history of Connected Cars: what is considered ‚Äúconnected machines‚Äù and why the near future is not in drones?</a></li>
<li><a href="../336080/index.html">Creating a programming language using LLVM. Part 6: Language Extension: User Defined Operators</a></li>
<li><a href="../336082/index.html">Angular - Implementing secure queries to the GraphQL API via JWT tokens</a></li>
<li><a href="../336084/index.html">Zabbix 3.4 released</a></li>
<li><a href="../336086/index.html">How to build a storage system with rocket on a standard hardware? SDS RAIDIX hardware platform architecture</a></li>
<li><a href="../336090/index.html">Why learn Spark?</a></li>
<li><a href="../336092/index.html">I'm not guilty. He came</a></li>
<li><a href="../336094/index.html">How Yandex taught artificial intelligence to understand the meaning of documents</a></li>
<li><a href="../336096/index.html">Declarative web programming</a></li>
<li><a href="../336098/index.html">Dive into BerkeleyDB JE. Introduction to DPL API</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>