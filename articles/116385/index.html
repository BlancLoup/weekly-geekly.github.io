<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Classification and regression using decision trees</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 This article provides an overview of decision trees (Decision trees) and three main algorithms that use these trees to build classifica...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Classification and regression using decision trees</h1><div class="post__text post__text-html js-mediator-article"><h4>  Introduction </h4><br>  This article provides an overview of decision trees (Decision trees) and three main algorithms that use these trees to build classification and regression models.  In turn, it will be shown how the decision-making trees, initially focused on classification, are used for regression. <br><br><h4>  Decision Trees </h4><br>  <b>A decision tree</b> is a tree, in the leaves of which there are values ‚Äã‚Äãof the objective function, and in the remaining nodes there are transition conditions (for example, ‚ÄúFLOOR is MALE‚Äù) that determine which of the edges to go.  If, for a given observation, the condition is true, then a transition is made along the left edge, if false, on the right. <br><a name="habracut"></a><br><h5>  Classification </h5><br> <a title="Habraffe.ru" href=""><img src="https://habrastorage.org/storage/habraeffect/8f/6f/8f6f8d9158385ef9fa9ca941db6f79a4.png"></a> <br><br>  The image above shows an iris classification tree.  The classification goes into three classes (marked on the image - red, blue and green), and passes by the parameters: the length / thickness of the sepal (SepalLen, SepalWid) and the length / thickness of the petal (PetalLen, PetalWid).  As you can see, in each node it is worth belonging to a class (depending on which elements fall more into this node), the number of observations made there N, as well as the number of each class.  Also not in leaf vertices there is a condition of transition - to one of the children.  Accordingly, the sample is divided by these conditions.  As a result, this tree almost perfectly (6 out of 150 incorrectly) classified the initial data (namely, the initial ones - the ones on which it was trained). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Regression </h5><br>  If the classification in sheets are the resulting classes, with regression, there is some value of the objective function. <br> <a title="Habraffe.ru" href=""><img src="https://habrastorage.org/storage/habraeffect/70/89/70897233f8bc327f08a9b4215ccb437f.png"></a> <br><br>  In the above image, a regression tree is used to determine the price of land in the city of Boston in 1978, depending on the parameters RM - the number of rooms, LSTAT - the percentage of poor people and several other parameters (see [4] for more details).  Accordingly, here at each node we see the mean value (Avg) and standard deviation (STD) of the values ‚Äã‚Äãof the objective function of observations that fell at this vertex.  The total number of observations hit the node N. The result of the regression will be the value of the average (Avg), which node will get the observation. <br>  Thus, initially the classification tree can work for regression.  However, with this approach, a large tree size is usually required than with a classification in order to achieve good regression results. <br><br><h4>  Basic methods </h4><br>  Below are a few basic methods that use decision trees.  Their brief description, pros and cons. <br><br><h5>  CART </h5><br>  <b>CART</b> (Eng. Classification and regression trees - Classification and regression trees) was the first method invented in 1983 by four well-known scientists in the field of data analysis: Leo Breiman, Jerome Friedman, Richard Olshen and Stone [1]. <br>  The essence of this algorithm is the usual construction of a decision tree, no more and no less. <br>  At the first iteration, we build all possible (in a discrete sense) hyperplanes that would divide our space into two.  For each such partition of space, the number of observations in each of the subspaces of different classes is considered.  As a result, such a partition is chosen that maximally distinguishes the observation of one of the classes in one of the subspaces.  Accordingly, this partition will be our decision-making root, and the sheets in this iteration will be two partitions. <br>  At the next iterations, we take one worst (in the sense of the ratio of the number of observations of different classes) sheet and carry out the same operation to split it.  As a result, this sheet becomes a node with some sort of splitting, and two sheets. <br>  We continue to do this until we reach the limit on the number of nodes, or the total error ceases to improve from one iteration to another (the number of incorrectly classified observations by the whole tree).  However, the resulting tree will be ‚Äúretrained‚Äù (will be fitted to the training sample) and, accordingly, will not give normal results on other data.  In order to avoid ‚Äúretraining‚Äù, test samples are used (or cross-validation) and, accordingly, a reverse analysis is performed (the so-called pruning), when the tree is reduced depending on the result on the test sample. <br>  A relatively simple algorithm that results in a single decision tree.  Due to this, it is convenient for primary data analysis, for example, to check for connections between variables and another. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  Quick model building <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  Easy to interpret (because of the simplicity of the model, you can easily display the tree and follow all the nodes of the tree) <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Often converges on a local solution (for example, at the first step a hyperplane was chosen that maximally divides the space at this step, but at the same time it does not lead to an optimal solution) <br><br><h5>  RandomForest </h5><br>  <b>Random forest</b> (Random Forest) - a method invented after CART by one of the four - Leo Breiman in collaboration with Adele Cutler [2], which is based on the use of the committee (ensemble) decision trees. <br>  The essence of the algorithm is that at each iteration a random selection of variables is made, after which, at this new sample, the decision tree is built.  This is ‚Äúbagging‚Äù - a sample of a random two-thirds of the observations for training, and the remaining third is used to evaluate the result.  Such an operation is done hundreds or thousands of times.  The resulting model will be the result of ‚Äúvoting‚Äù a set of trees obtained in the simulation. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  High quality results, especially for data with a large number of variables and a small number of observations. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  The ability to parallelize <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  No test sample required. <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Each of the trees is huge, as a result, the model is huge <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Long model building, to achieve good results. <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Complex interpretation of the model (Hundreds or thousands of large trees are difficult to interpret) <br><br><h5>  Stochastic Gradient Boosting </h5><br>  <b>Stochastic Gradient Boosting</b> (Stochastic Gradient Addition) is a data analysis method presented by Jerome Friedman [3] in 1999, which represents a solution to a regression problem (to which classification can be reduced) by building a committee (ensemble) of ‚Äúweak‚Äù predictive decision trees. <br>  At the first iteration, a decision tree is bounded by the number of nodes.  After that, the difference between what the predicted tree multiplied by learnrate (the coefficient of ‚Äúweakness‚Äù of each tree) and the desired variable at this step is calculated. <br>  Yi + 1 = Yi-Yi * learnrate <br>  And already on this difference the following iteration is under construction.  This continues until the result ceases to improve.  Those.  at each step we try to correct the errors of the previous tree.  However, it is better to use verification data here (not participating in the simulation), since retraining is possible on the training data. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  High quality results, especially for data with a large number of observations and a small number of variables. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  Comparatively (with the previous method), the small size of the model, since each tree is limited to given sizes. <br><img src="https://habrastorage.org/getpro/habr/post_images/bad/31a/6b6/bad31a6b6345ea44e6332abfbf5c96f0.png" alt="+">  Comparatively (with the previous method) fast time building an optimal model <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Test sample required (or cross-validation) <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Inability to parallelize well <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Relatively weak tolerance for erroneous data and retraining <br><img src="https://habrastorage.org/getpro/habr/post_images/6f9/5fd/1ef/6f95fd1ef09cbe2ecc7aecd2057409b2.png" alt="-">  Difficult interpretation of the model (As in the Random forest) <br><br><h4>  Conclusion </h4><br>  As we have seen, each method has its pros and cons, and accordingly, depending on the task and the initial data, you can use one of the three methods to solve and get the desired result.  However, CART is more used in universities for teaching and research, when some clear descriptive basis is needed for a solution (as in the above example of analyzing the price of land in Boston).  And to solve industrial problems usually use one of his descendants - Random Forest or TreeNet. <br>  These methods can be found in many modern packages for data analysis: <br><ul><li>  <a href="http://salford-systems.com/">Salford Predictive Modeling Suite</a> </li><li>  <a href="http://www.spss.com/">Spss</a> </li><li>  <a href="http://www.sas.com/">SAS</a> </li><li>  <a href="http://www.r-project.org/">Data analysis packages for project R</a> </li></ul><br><h4>  Bibliography </h4><br><ol><li>  ‚ÄúClassification and Regression Trees‚Äù.  Breiman L., Friedman JH, Olshen R. A, Stone CJ </li><li>  ‚ÄúRandom Forests‚Äù.  Breiman L. </li><li>  ‚ÄúStochastic Gradient Boosting‚Äù.  Friedman jh </li><li>  <a href="http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html">http://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/116385/">https://habr.com/ru/post/116385/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../116380/index.html">Install video driver for SIS M671 / M672 family of video cards under Ubuntu 11.04</a></li>
<li><a href="../116381/index.html">Shatokua about testing</a></li>
<li><a href="../116382/index.html">Change or not change, that is the question</a></li>
<li><a href="../116383/index.html">Electronics on fingers</a></li>
<li><a href="../116384/index.html">Another look at caching on Drupal</a></li>
<li><a href="../116386/index.html">Management models of open source projects (Part 1)</a></li>
<li><a href="../116388/index.html">Resolving the password issue - once and for all</a></li>
<li><a href="../116390/index.html">Basic services and tariffs on the cybercrime market in the CIS countries</a></li>
<li><a href="../116391/index.html">ERP Implementation - The Perfect Storm</a></li>
<li><a href="../116393/index.html">Details about the breakthrough AI in Kinect</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>