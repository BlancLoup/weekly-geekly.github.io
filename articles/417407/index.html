<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Optimizing the rendering of scenes from the Disney cartoon "Moana". Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Walt Disney Animation Studios (WDAS) recently gave the rendering community an invaluable gift to the community by releasing a full scene description f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Optimizing the rendering of scenes from the Disney cartoon "Moana". Part 1</h1><div class="post__text post__text-html js-mediator-article">  Walt Disney Animation Studios (WDAS) recently gave the rendering community an invaluable gift to the community by releasing a <a href="https://www.disneyanimation.com/technology/datasets">full scene description for the island from <em>the Moana</em> cartoon</a> .  Geometry and textures for a single frame occupy more than 70 GB of disk space.  This is a terrific example of the degree of complexity that rendering systems have to deal with today;  never before have researchers and developers engaged in rendering outside film studios been able to work with such realistic scenes. <br><br>  Here is the result of rendering the scene using modern pbrt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/729/ae6/97a/729ae697af18f30723b71a7168807a3f.jpg"></div><br>  <i>The island of "Moana", rendered <a href="https://github.com/mmp/pbrt-v3">pbrt-v3</a> in the resolution of 2048x858 with 256 samples per pixel.</i>  <i>The total rendering time on a 12-core / 24-threaded Google Compute Engine instance with a frequency of 2 GHz with the latest version of pbrt-v3 was 1 h 44 min 45 s.</i> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      On the Disney side, it was a lot of work; it had to extract the scene from its own internal format and convert it into a regular one;  Special thanks to her for the time spent on packaging and preparing this data for widespread use.  I am confident that their work will be well rewarded in the future because researchers use this scene to study the problems of effectively rendering scenes of such a level of complexity. <br><a name="habracut"></a><br>  This scene has already taught me a lot and allowed me to improve the pbrt renderer, but before we get to this, I‚Äôll tell a short story to understand the context. <br><br><h2>  Hash which was not </h2><br>  Many years ago, while going through an internship in the Pixar rendering team, I learned a curious lesson: ‚Äúinteresting‚Äù things almost always appear when input data are transferred to the software system that is significantly different from what it was before.  Even in well-written and mature software systems, new types of input almost always lead to the detection of unknown defects in an existing implementation. <br><br>  I first learned this lesson during the production of <em>Toy Story 2</em> .  One day someone noticed that a surprisingly long time wasted in parsing RIB scene description files.  Someone else from the rendering team (I guess it was Craig Kolb) started the profiler and started to figure it out. <br><br>  It turned out that most of the parsing time was occupied by search operations in the hash table used for <a href="https://en.wikipedia.org/wiki/String_interning">string interning</a> .  The hash table had a rather small size, probably 256 elements, and when several values ‚Äã‚Äãwere hashed into one cell, it organized the chain.  After the first implementation of the hash table, a lot of time passed and now there were tens of thousands of objects in the scenes, so such a small table quickly filled up and became ineffective. <br><br>  The most expedient way was to simply increase the size of the table - all this happened at the height of the workflow, so there was no time for any sophisticated solution, for example, expanding the size of the table when it was filled.  We make a change in one line, reassemble the application, perform a quick test before the commit and ... no speed improvements occur.  The search for the hash table takes the same amount of time.  Awesome! <br><br>  After further study, it was found that the hash table function used was analogous to the following: <br><br><pre><code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">hash</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">const</span></span></span></span><span class="hljs-function"><span class="hljs-params"> </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">char</span></span></span></span><span class="hljs-function"><span class="hljs-params"> *str)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> str[<span class="hljs-number"><span class="hljs-number">0</span></span>]; }</code> </pre> <br>  (Forgive me, Pixar, if I revealed your top-secret RenderMan source code.) <br><br>  The ‚Äúhash‚Äù function was implemented back in the 1980s.  At that time, the programmer probably thought that the computational cost of checking the effect of all characters on the string on the hash value would be too high and not worth it.  (I think that if there were only a few objects in the scene and 256 elements in the hash table, then this was quite enough.) <br><br>  Another obsolete implementation also contributed: since the moment Pixar started creating its films, the names of objects in the scenes have grown quite a bit, for example, ‚ÄúBuzzLightyear / LeftArm / Hand / IndexFinger / Knuckle2‚Äù.  However, some initial stage of the pipeline used to store the names of objects a fixed-length buffer and shortened all long names, retaining only the end, and, if lucky, added an ellipsis at the beginning, making it clear that part of the name was lost: "... year / LeftArm / Hand / IndexFinger / Knuckle2 ". <br><br>  Subsequently, all the names of objects that the renderer saw had this form, the hash function hashed them all into one memory fragment as ".", And the hash table was actually a large linked list.  Good old days.  At least, having understood, we rather quickly corrected this error. <br><br><h2>  Intriguing innovation </h2><br>  I remembered this lesson last year, when Heather Pritchet and Rasmus Tamstorf from WDAS contacted me and asked if I would be interested to check the possible rendering quality of the scene from <em>Moana</em> in <a href="http://pbrt.org/">pbrt</a> <sup>1</sup> .  Naturally, I agreed.  I was glad to help and I was wondering how everything will turn out. <br><br>  The naive optimist inside me hoped that there would be no huge surprises - after all, the first version of pbrt was released about 15 years ago, and many people used and studied its code for many years.  You can be sure that there will be no interference like the old hash function from RenderMan, right? <br><br>  Of course, the answer was no.  (And that is why I am writing this and several other posts.) Although I was a little disappointed that pbrt was not perfect out of the box, but I think that my experience with the scene from <em>Moana</em> was the first confirmation of the value of publishing this scene ;  pbrt has already become a better system due to the fact that I figured out the processing of this scene. <br><br><h3>  First renders </h3><br>  Having access to the scene, I immediately downloaded it (with my home Internet connection, it took several hours) and unpacked from tar, receiving 29 GB of pbrt files and 38 GB of <a href="http://ptex.us/">ptex</a> <sup>2</sup> texture maps.  I blithely tried to render the scene on my home system (with 16 GB of RAM and a 4-core CPU).  Returning after a while to the computer, I saw that it hung, all the RAM is full, and pbrt is still trying to complete the parsing of the scene description.  The OS was trying to cope with the task using virtual memory, but it seemed hopeless.  Having nailed the process, I had to wait about a minute before the system began to respond to my actions. <br><br>  The next attempt was the Google Compute Engine instance, which allows you to use more RAM (120 GB) and more CPUs (32 threads per 16 CPUs).  The good news was that pbrt was able to successfully render the scene (thanks to the work of Heather and Rasmus in translating it into pbrt format).  It was very exciting to see that pbrt can generate relatively good pixels for high-quality movie content, but the speed was not so exciting: 34 min 58 s only for parsing the scene description, and the system spent up to 70 GB of RAM during rendering. <br><br>  Yes, the disk contained 29 gigabytes of pbrt scene description files that needed to be parsed, so I did not expect the first stage to take a couple of seconds.  But spend half an hour before the rays begin to be traced?  This greatly complicates the work with the scene. <br><br>  On the other hand, such speed told us that something very badly smelling was probably happening in the code;  not just ‚Äúmatrix inversion can be done 10% faster‚Äù;  rather, something of the level of "oh, we go through a linked list of 100 thousand items."  I was optimistic and hoped that by understanding, I could speed up the process considerably. <br><br><h3>  Statistics don't help </h3><br>  The first place I started looking for clues was the pbrt dump statistics after rendering.  The main stages of the execution of pbrt are configured so that you can collect approximate data profiling by fixing operations with periodic interruptions in the rendering process.  Unfortunately, the statistics did not help us much: according to reports, from almost 35 minutes before the start of rendering, 4 minutes and 22 seconds was spent on building BVH, but there was no detail about the rest of the time. <br><br>  Building a BVH is the only significant computational task that is performed while the scene is parsing;  all the rest is essentially a de-serialization of descriptions of geometry and materials.  Knowing how much time was spent on creating BVH gave an understanding of how (not) the system was effective: the remaining time, namely about 30 minutes, spent on parsing 29 GB of data, that is, the speed was 16.5 MB / s.  Well-optimized JSON parsers, in fact performing the same task, work at a speed of 50-200 MB / s.  It is clear that there is still room for improvement. <br><br>  To better understand what wasted time, I launched pbrt with the Linux <a href="https://perf.wiki.kernel.org/index.php/Main_Page">perf</a> tool, which I had never used before.  But it seems that he coped with the task.  I instructed him to look for the DWARF characters to get the function names ( <code>--call-graph dwarf</code> ), and in order not to get the hundredgbyte trace files, I had to reduce the sample rate from 4000 to 100 samples per second ( <code>-F 100</code> ).  But with these parameters, everything went great, and I was pleasantly surprised that the <code>perf report</code> tool has an interface with nice curses. <br><br>  Here is what he could tell me after starting with pbrt: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a9e/d31/643/a9ed31643e22b9c5d0566c7b210f4067.png"></div><br>  <i>I was not joking when I talked about the ‚Äúinterface with nice curses‚Äù.</i> <br><br>  We see that more than half the time is spent on the mechanics of parsing: <code>yyparse()</code> is a <a href="https://www.gnu.org/software/bison/">bison</a> generated parser, and <code>yylex()</code> is a lexical analyzer (lexer) generated by <a href="https://github.com/westes/flex">flex</a> .  More than half the time in <code>yylex()</code> is spent on <code>strtod()</code> , which converts strings to double values.  We will <code>yyparse()</code> attack on <code>yyparse()</code> and <code>yylex()</code> until the third article in this series, but now we can understand that reducing the number of data thrown into the renderer can be a good idea. <br><br><h3>  From text to PLY </h3><br>  One way to spend less time parsing data in text form is to convert the data to a format that is parsed more efficiently.  Quite a large part of the 29 GB of these scene description files are triangle meshes, and pbrt already has native support for <a href="https://en.wikipedia.org/wiki/PLY_(file_format)">the PLY format</a> , which is an effective binary representation of polygonal meshes.  Also in pbrt there is a command line flag <code>--toply</code> , which parses the scene description file pbrt, converts all the found triangle meshes into PLY files and creates a new pbrt file that references these PLY files. <br><br>  The catch is that <a href="http://ptex.us/">ptex</a> textures are actively used in the Disney scene, which, in turn, require that <code>faceIndex</code> value is associated with each triangle, which determines from which face of the original subdivided mesh it is taken.  To transfer these values, it was enough just to <a href="https://github.com/mmp/pbrt-v3/commit/6870bc7750b0f32c0e3ffa569eae2e8e0f8c268d">add support for new fields in the PLY file</a> .  Further research revealed that if each mesh is converted ‚Äî even if there are only ten triangles in it ‚Äî into the PLY file, tens of thousands of small PLY files are created in the folder, and this creates its own performance problems;  We managed to get rid of this problem by adding to the implementation the <a href="https://github.com/mmp/pbrt-v3/commit/3dae5fa7c0ed0dbd4fe2104179a5f7b8a0ea08a3">ability to leave small meshes unchanged</a> . <br><br>  I wrote a <a href="http://pharr.org/matt/blog/images/toply.sh.txt">small command line script</a> to convert all <code>*_geometry.pbrt</code> files in a folder to use PLY for large meshes.  Note that it has hard-coded assumptions about the paths that need to be changed in order for the script to work elsewhere. <br><br><h3>  First speed boost </h3><br>  After converting all large meshes to PLY, the size of the scene description on the disk decreased from 29 to 22 GB: 16.9 GB of pbrt scene files and 5.1 GB of PLY binary files.  After the conversion, the total time of the first stage of the system decreased to 27 minutes 35 seconds, and the savings amounted to 7 minutes 23 seconds, that is, we accelerated 1.3 times <sup>3</sup> .  Processing a PLY file is much more efficient than processing a pbrt text file: just 40 seconds of the launch time was spent on parsing the PLY files, and we see that the PLY files were processed at a speed of about 130 MB / s, or about 8 times faster than the text format pbrt . <br><br>  It was a good easy win, but we still had a lot to do. <br><br>  <a href="http://pharr.org/matt/blog/2018/07/09/moana-island-pbrt-2.html">Next time</a> we will figure out where all of the memory is actually used, correct some errors here and achieve even more speed in the process. <br><br><h2>  Notes </h2><br><ol><li>  Now you need to understand more clearly the motivation for adding ptex support on my part and converting Disney BSDF to pbrt last year. </li><li>  All the time here and in subsequent posts is indicated for the WIP version (Work In Progress), with which I worked before the official release.  It looks like the final version is a bit more.  We will stick to the results that I recorded when working with the original scene, despite the fact that they do not quite match the results of the final version.  I suspect that the lessons from them can be learned the same. </li><li>  Notice that the speed increase essentially corresponds to what might have been expected with an approximately 50 percent reduction in the volume of the parsing data.  The amount of time we spend according to the profiler readings confirms our idea. </li></ol></div><p>Source: <a href="https://habr.com/ru/post/417407/">https://habr.com/ru/post/417407/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../417395/index.html">End-to-end testing: what, why, why</a></li>
<li><a href="../417397/index.html">What programming language to learn in 2018 and why?</a></li>
<li><a href="../417399/index.html">Welcome aboard: we introduce new developers to the team</a></li>
<li><a href="../417401/index.html">Finally, we choose a budget multimeter with good functionality.</a></li>
<li><a href="../417405/index.html">Autoencoders and Strong AI</a></li>
<li><a href="../417409/index.html">How to tmlidu survive in a scalable scram and keep control over the quality of the code</a></li>
<li><a href="../417411/index.html">Evaluate the developer based on objective data.</a></li>
<li><a href="../417413/index.html">Whether the problems of Timblids differ in St. Petersburg, find out on Saint TeamLead Conf</a></li>
<li><a href="../417415/index.html">3D printing lessons. Printing parts with different layer thickness from 3Dtool</a></li>
<li><a href="../417419/index.html">[Yekaterinburg, Announcement] Alice Visiting Kontur - Hackathon to create skills for voice assistants</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>