<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of the second day of Data Science Weekend 2018. Data Engineering, ETL, search services and more</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A few days ago, we published a review of the first day of Data Science Weekend 2018 , which took place on March 2-3 at Attic Rambler & Co. Having stud...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of the second day of Data Science Weekend 2018. Data Engineering, ETL, search services and more</h1><div class="post__text post__text-html js-mediator-article">  A few days ago, we published a review of the first day of <a href="http://dswknd2018.datascienceweek.com/">Data Science Weekend 2018</a> , which took place on March 2-3 at Attic Rambler &amp; Co.  Having studied the practice of using machine learning algorithms, we now turn to the review of the second day of the conference, during which the speakers talked about the use of various tools, the engineer‚Äôs date for the needs of data platforms, ETL, search services and many other things. <br><br><img src="https://habrastorage.org/webt/qk/bh/a8/qkbha8tclhjnmv0hilugbjz87oo.jpeg"><br><a name="habracut"></a><br><h3>  GridGain </h3><br>  The second day of the DSWknd2018 was opened by Yuri Babak, the head of development of the machine learning module for the <a href="https://ignite.apache.org/">Apache Ignite</a> platform at GridGain, describing how the company coped with the optimization of distributed machine learning on a cluster. <br><br>  Usually, speaking of the optimization of the machine learning process, they primarily mean the optimization of the trainings themselves.  We decided to go on the other side and solve the problem that the data are usually stored separately from the system where they are processed and trained. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Consequently, we decided to focus on the problem of long ETL (after all, it is pointless to compete in machine learning performance with Tensorflow), as a result of which we managed to train distributed models on the entire cluster that we have.  The result is a new approach in the field of ML, using a non-parallel, namely, distributed learning system. <br><br>  We succeeded in achieving this goal due to the fact that we developed a new machine learning module in Apache Ignite, which relies on the functionality that already exists in it (streaming, native persistence, and more).  I would like to tell you about two features of the resulting module: distributed key-value storage and collocated computing. <br><br><ul><li>  <b>Key-value repository.</b>  First, we needed distributed replicated caches, which generally have the following structure: we have a cache with some data, it is ‚Äúspread‚Äù across the cluster, and Apache Ignite cares about how to balance the data so that distributed across the cluster.  The diagram below shows the situation when we have 3 copies of data in each partition, and even if the cluster falls and only one of the 4 nodes remains, we still have all the data intact. </li></ul><br><img src="https://habrastorage.org/webt/7h/eb/3g/7heb3gz1y7alp-vqhxh1jvdhewq.png"><br><br><ul><li>  <b>Collocated computing.</b>  This is essentially our implementation of the MapReduce concept, when we are not trying to aggregate data in one place and make calculations, but, on the contrary, ‚Äúdeliver‚Äù the calculations to our data, thereby minimizing the load on the cluster.  The scheme is as follows: in the <i>Map</i> step, we send our task only to those nodes where there is the necessary data, then it is executed locally on each specific node, so that at the <i>Reduce</i> step we can aggregate the results and send them somewhere else. </li></ul><br><img src="https://habrastorage.org/webt/w8/gc/mz/w8gcmzbdevlggrbgmhgygwyc_ik.png"><br><br>  More information about Apache Ignite can be found <a href="https://ignite.apache.org/">here</a> , <a href="https://github.com/apache/ignite">here</a> and <a href="https://apacheignite.readme.io/docs">here</a> . <br><br><h3>  Raiffeisenbank </h3><br>  After that, Alexey Kuznetsov and Mikhail Setkin, graduates of our <a href="https://goo.gl/wf3MHF">Data Engineer</a> programs and <a href="https://goo.gl/o1rj9t">Big</a> <a href="https://goo.gl/wf3MHF">Data</a> <a href="https://goo.gl/o1rj9t">Specialist</a> , shared their experience in building a Real-Time Decision Platform (RTDP) based on the <a href="https://hortonworks.com/products/data-platforms/hdp/">Hortonworks Data Platform</a> and <a href="https://hortonworks.com/webinar/introducing-hortonworks-dataflow/">Data Flow</a> . <br><br>  Any organization has data that characterizes events that can be displayed on a timeline and somehow be used to make real-time decisions.  For example, you can imagine a scenario when a bank customer failed to pay by card 2 times, then he called the call-center, went to the Internet bank, left a request, etc.  Of course, we would like to receive some signals, based on these events, in order to promptly offer the client some services, special offers or help with his problem. <br><br>  Each RTDM system is based on <b>streaming</b> , which works in such a way that there are sources from which we can shoot events and put on the data bus, and then pick up from there, and we cannot connect events from different sources and somehow aggregate them. <br><br>  The next level of abstraction over streaming is Complex Events Processing (CEP), which differs from simple streaming in that there are many sources that we try to process at the same time, that is, we see events together, the join events of these events appear, we can somehow aggregate them on the fly, etc. <br><br><img src="https://habrastorage.org/webt/ed/yh/ca/edyhcavqgrjitmhhet3t514rapw.png"><br><br>  The last element of abstraction is the RTDM system itself, the main difference of which from CEP is that it has a set of pre-configured solutions and actions that can be taken online: a call from the call center in the case of an application for consultation in the Internet bank, SMS from special offer in case of replenishment of the account for a large amount and other actions. <br><br>  How to implement this system?  You can go to vendors and in 99% of cases this is standard practice in the field.  On the other hand, we have a team of data engineers who can do everything themselves using open-source solutions.  The main drawback of most of them is the lack of a user interface. <br><br>  However, we managed to find a suitable platform for us - the choice fell on HortonWorks Data Flow 3.0, the new version of which has just what we need - Streaming Analytics Manager (SAM), where the graphical interface was implemented, and considering that HortonWorks was in production, we took the path of least resistance. <br><br>  Let us turn to the architecture of our RTDM solution.  Data from the sources come to the data bus, where they are aggregated, and then using HDF they are collected and put into Kafka.  Next comes SAM, where, using the user interface, the user launches the campaign for execution, the <i>JAR file is</i> then compiled and sent to Apache Storm. <br><br><img src="https://habrastorage.org/webt/qq/wt/ha/qqwthavwbkvvhpucyfah-slqphq.png"><br><br>  The central element of the entire system is SAM, thanks to which all this has become possible.  Here is what its interface looks like: <br><br><img src="https://habrastorage.org/webt/sa/ap/j9/saapj9o9tt-7xjslen5o7ydtvcc.png"><br><br>  SAM gives the user great opportunities: there is a choice of data sources, a set of processors with which he processes the data flow, a group of filters, a branching, the choice of an action for a particular client, whether it is an absolutely personalized offer or a common action for a certain group of people. <br><br><h3>  Lamoda </h3><br>  Our Data Science Weekend was in full swing and next in line was Igor Mosyagin, another <a href="https://goo.gl/wf3MHF">Data Engineer</a> graduate and R &amp; D developer at Lamoda.  Igor spoke about how they optimized the search tips on the site and tried to make friends with <a href="http://lucene.apache.org/solr/">Apache Solr</a> , <a href="https://golang.org/">Golang</a> and <a href="https://airflow.apache.org/">Airflow</a> . <br><br>  On our site, as well as on many others, there is a search field where people enter something and along the way there are hints with which we try to predict what the user needs.  At that time, we already had some ready-made solution, but it did not quite suit us.  For example, the system did not respond if the user confused the layout. <br><br>  Apache Solr is the center of the whole system, we used it in the old solution, but now we decided to implement Airflow, which I learned about using the Data Engineer program.  As a result, the request that comes from the user gets into our service, written in Go, which prepares it and sends it to Solr, and then receives the answer and returns it to the user.  At the same time, Airflow is launched regularly, at some predetermined time, which climbs into the database and, if required, starts importing data into Solr.  The main thing in all of this is the fact that 50 ms is passed from the user's request to the reply, of which the lion's share ‚Äî 40 ms ‚Äî is a request to Solr and receiving a response from it. <br><br><img src="https://habrastorage.org/webt/u4/sa/18/u4sa18c8_3uk8yjuyha6jvanfmq.png"><br><br>  Generally speaking, Apache Solr is such a big ‚Äúcolossus‚Äù with good documentation, it has a lot of sadzhesterov that work according to different logic: they can return an answer only by exact coincidence, or there are options when finding a line far from the beginning of the word and t .d  In total, there are 7 or 8 variants of sadzhester, but we used only 3, since the rest in most cases worked out very slowly. <br><br>  It is also important to update the weights from time to time, as the frequency of requests changes.  For example, if this month one of the most popular requests are winter boots, then, of course, this must be taken into account. <br><br><h3>  Aligned research group </h3><br>  Next came the turn of Nikolai Markov, who is the Senior Data Engineer in the Aligned Research Group, and also lectures on our <a href="https://goo.gl/o1rj9t">Big</a> <a href="https://goo.gl/wf3MHF">Data</a> <a href="https://goo.gl/o1rj9t">Specialist</a> and <a href="https://goo.gl/wf3MHF">Data Engineer</a> programs.  Nikolay spoke about the advantages and disadvantages of the Hadoop ecosystem, and why analyzing and processing data on the command line can be a good alternative. <br><br>  If you look at Hadoop from a modern engineering point of view, you can find not only a lot of advantages, but also a number of drawbacks.  For example, MapReduce is a general-purpose paradigm.  In fact, it all boils down to the fact that you spend a lot of time on transferring your algorithm to MapReduce, so that something counts there, and you may have made a lot of errors in the process.  Sometimes you need a lot of MapReduce to do another thing, and it turns out that instead of writing business logic, you spend time on MapReduce. <br><br><img src="https://habrastorage.org/webt/ew/jx/-i/ewjx-ioscnh3w7bk7bo57slyqgo.jpeg"><br><br>  The advantage of Hadoop is, of course, Python support.  He's good at everything, I write on it and recommend it to everyone, but the problem is that when we write in Python under Hadoop, we need a lot of engineering support to make it all work in production: all analytical packages (Pandas, Numpy, and .d.) must stand on the end nodes, all this must be deployed automatically.  As a result, it turns out that we either adapt to a specific vendor, which allows its versions to be installed there, or we need a configuration management system, which will be engaged in deployment. <br><br>  Naturally, one of the main drawbacks of Hadoop and at the same time the main reason for the appearance of Spark is that Hadoop always writes the results to disk, reads from it and writes there too.  In fact, even if we scatter this process on many nodes, it will still work at the speed of the disk (average). <br><br>  To solve the problem of speed, you can, of course, by scaling Hadoop, that is, simply ‚Äúthrowing money‚Äù.  However, there are more effective alternatives.  One of them is the analysis and processing of data <b>on the command line</b> .  It is really possible to solve serious analytical problems in it, and it will be several times faster than on Hadoop.  The only negative is presented below: <br><br><img src="https://habrastorage.org/webt/xn/0l/be/xn0lbey3of8x78cftsarlab5o8e.png"><br><br>  This may seem unreadable to some, but this thing works an order of magnitude faster than a Python script, so it‚Äôs great if you have an engineer who can write such things on the command line. <br><br>  Also, I do not quite understand why in companies, your business logic must necessarily be tied to a relational database, because nothing prevents you from taking some kind of non-relational database in the modern form (the same MongoDB).  You should not make excuses for having a bunch of join-s there and it‚Äôs impossible to do without SQL.  To date, the database is very much, and you can choose yourself which one is closer to you. <br><br>  If you still can not do without SQL, then you can try <a href="https://prestodb.io/">Presto</a> - this is an extensible engine for distributed work with many data sources at once.  That is, you can write a plugin for your data source and in fact extract everything you want with SQL.  In principle, Hive has the same logic, but it is tied to the Hadoop infrastructure, and Presto is an independent development.  The advantage is integration with <a href="https://zeppelin.apache.org/">Apache Zeppelin</a> - this is such a beautiful front-end, where you can write SQL queries and immediately get graphs. <br><br><h3>  Rambler &amp; Co </h3><br>  Alexander Shorin, an instructor at the <a href="https://goo.gl/wf3MHF">Data Engineer</a> program and a Python senior development engineer at Rambler &amp; Co, was honored to finish our productive weekend weekend.  This time the focus was on the engineering part of the project. <br><br>  The original pipeline looks like this: <br><br><img src="https://habrastorage.org/webt/i1/k_/pd/i1k_pdagr0gjkikoohd-vjndwe8.png"><br><br>  The cameras transfer photos to WebDAV, then the task from Airflow pulls out new photos and sends them to the API, which forms all of this into separate tasks, and then uploads them to RabbitMQ.  From the "rabbit" the workers take these tasks, do some transformations with them and send the results back. <br><br>  How can we scale this process from a technical point of view?  How many more cars do we need to handle the whole stream of photos?  To answer this question, take a profiler.  We decided to take <a href="https://pyflame.readthedocs.io/en/latest/">UF</a> 's PyFlame, which is actually a wrapper over Ptrace that clings to the Linux process, looks at what it does, and records what and how many times it has been. <br><br>  We launched a test dataset consisting of 472 photos, and it was calculated in 293 seconds.  Is it a lot or a little?  The PyFlame report looks like this in the following beautiful way: <br><br><img src="https://habrastorage.org/webt/et/kf/2k/etkf2klm1bdmy4dvt_v4pjzksva.png"><br><br>  Here we see the ‚Äúgorge‚Äù of loading models, there is a ‚Äúvalley‚Äù of segmentation and other interesting things.  On this report, it is clear that our code is slowing down, since a huge bar in the center of the image refers to it. <br><br>  In fact, it turned out that we needed to change only one line in the Jupyter laptop in order to optimize the segmentation: the process duration fell from 293 seconds to 223 seconds.  We also switched from PIL, which didn‚Äôt scold just lazy for sluggishness, to OpenCV, thanks to which the total running time decreased by another 20 seconds.  Finally, the use of Pillow-SIMD, which Alexander Karpinsky described in his speech at the Piter Py # 4 conference, for image processing made it possible to reduce the task execution time to 183 seconds.  True to PyFlame, this only slightly affected: <br><br><img src="https://habrastorage.org/webt/f9/_c/an/f9_canvv9aqibdnyrs4otrow7rg.png"><br><br>  As you can see, <a href="http://pytorch.org/">PyTorch</a> is still <a href="http://pytorch.org/">standing</a> out <a href="http://pytorch.org/">here</a> , so we will kick it.  what can we do with him?  In PyTorch, when sending data to a video card, they first undergo preprocessing, and then they are <a href="https://github.com/facebook/dataloader">thrown</a> into <a href="https://github.com/facebook/dataloader">DataLoader</a> . <br><br>  Having studied the principles of DataLoader, we saw that it raises the workers, processes the data, and then kills the workers.  The question arises: why do we constantly raise and kill workers, if we have few people in the cinema and the processing of a photo takes about a second?  Why raise and kill two processes every second if it is inefficient? <br><br>  DataLoader was optimized due to its modification: now it does not kill the workers and does not use them if there are less than 24 people in the hall (the number was taken more or less from the ceiling).  At the same time, such optimization did not give a significant increase in processing speed, however, the average utilization of CPU decreased from ~ 600% to ~ 200%, that is, 3 times. <br><br>  Finally, other improvements include facilitating the implementation of <i>Conv2d</i> , removing excess lambda from the neural network model, and converting <i>Image</i> to <i>np.array</i> for <i>ToTensor</i> . <br><br><hr><br>  Finally, some more feedback about our conference from speakers and listeners: <br><br>  ‚ÄúA very comfortable get-together where you can talk on the sidelines about the industry and catch speakers with questions in general.  As a speaker, I note the professionalism of the organizers, it is clear that everything is done so that both speakers and listeners are as comfortable as possible. ‚Äù- <i>Igor Mosyagin, speaker, R &amp; D developer, Lamoda.</i> <br><br>  ‚ÄúI liked it very much.  Friendly audience, smart questions. ‚Äù- <i>Mikhail Setkin, speaker, Product Manager, Raiffeisenbank.</i> <i><br></i> <br>  Full speeches of all speakers can be viewed on our Facebook <a href="https://www.facebook.com/newprolab/">page</a> .  See you soon at our other events! </div><p>Source: <a href="https://habr.com/ru/post/352010/">https://habr.com/ru/post/352010/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../352000/index.html">Digital events in Moscow from March 26 to April 1</a></li>
<li><a href="../352002/index.html">A manager from Amazon about layoffs in the US and programmer performance evaluation</a></li>
<li><a href="../352004/index.html">What is a digital handwritten signature (CRP)</a></li>
<li><a href="../352006/index.html">Key Application Metrics - Mobile Landmarks 2018 Report</a></li>
<li><a href="../352008/index.html">Top 10 errors in C ++ projects for 2017</a></li>
<li><a href="../352012/index.html">Resizing images on the site</a></li>
<li><a href="../352014/index.html">How to crack a picture and (not) get BTC</a></li>
<li><a href="../352016/index.html">Transparency and trust</a></li>
<li><a href="../352020/index.html">Work with notifications about events of IOT objects and GPS trackers</a></li>
<li><a href="../352022/index.html">Hacking Team is back in business: ESET has discovered new spyware samples of the company</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>