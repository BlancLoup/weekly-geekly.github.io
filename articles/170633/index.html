<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Bayesian analysis in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This post is a logical continuation of my first post on Bayesian methods, which can be found here . 
 I would like to talk in detail about how to carr...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Bayesian analysis in Python</h1><div class="post__text post__text-html js-mediator-article">  This post is a logical continuation of my first post on Bayesian methods, which can be found <a href="http://habrahabr.ru/post/170545/">here</a> . <br>  I would like to talk in detail about how to carry out the analysis in practice. <br><a name="habracut"></a><br>  As I already mentioned, the most popular tool used for Bayesian analysis is the <a href="http://www.r-project.org/">R</a> language with <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> and / or <a href="http://www.openbugs.info/w/">BUGS</a> packages.  For an ordinary user, the difference in packages is in cross-platform JAGS (first-hand experience has been made of the conflict between Ubuntu and BUGS), as well as the fact that in JAGS you can create your own functions and distributions (though infrequently, if ever, arises).  By the way, an excellent and convenient IDE for R is, for example, <a href="http://www.rstudio.com/">RStudio</a> . <br>  But in this post I will write about an alternative to R - Python with the <a href="http://pymc-devs.github.com/pymc/">pymc</a> module. <br>  As a convenient IDE for Python, I can offer <a href="http://code.google.com/p/spyderlib/">spyder</a> . <br>  I prefer Python because, firstly, I see no reason to study a language that is not so common as R just to find some kind of problem related to statistics.  Secondly, from my point of view, Python with its modules is not inferior to R in simplicity, clarity and functionality. <br><br>  I propose to solve a simple problem of finding the coefficients of linear data dependence.  A similar problem of optimization of parameters is quite common in the most diverse areas of knowledge, so I would say that it is very revealing.  At the end we will be able to compare the result of the Bayesian analysis and the least squares method <br><br><h4>  Software installation </h4><br>  First of all, we need to install <a href="http://www.python.org/download/">Python</a> (for those who have not yet done so).  I did not use Python 3.3, but with 2.7 everything works fine. <br>  Then you need to install all the necessary modules for Python: <a href="http://www.scipy.org/NumPy">numpy</a> , <a href="http://matplotlib.org/">Matplotlib</a> . <br>  If you wish, you can also install additional modules: <a href="http://www.scipy.org/">scipy</a> , <a href="http://www.pytables.org/moin">pyTables</a> , <a href="http://code.google.com/p/pydot/">pydot</a> , <a href="http://ipython.org/">IPython</a> and <a href="https://nose.readthedocs.org/en/latest/">nose</a> . <br>  All of these settings (except for Python itself) are easier to do through <a href="https://pypi.python.org/pypi/setuptools">setuptools</a> . <br>  And now you can install the actual <a href="https://github.com/pymc-devs/pymc">pymc</a> (you can also install it via setuptools). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      A detailed guide on pymc can be found <a href="http://pymc-devs.github.com/pymc/">here</a> . <br><br><h4>  Task statement </h4><br>  We have data obtained during a hypothetical experiment, which linearly depend on a certain value of x.  Data arrives with noise whose dispersion is unknown.  It is necessary to find the coefficients of linear dependence. <br><br><h4>  Decision </h4><br>  First, we import the modules that we have installed and which we will need: <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pymc</code> </pre> <br>  Then we need to get our hypothetical linear data. <br>  To do this, we determine how many points we want to have (in this case, 20), they are evenly distributed on the interval [0, 10], and set the real coefficients of a linear relationship.  Next, we apply noise to the Gaussian data: <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#Generate data with noise number_points = 20 true_coefficients = [10.4, 5.5] x = numpy.linspace(0, 10, number_points) noise = numpy.random.normal(size = number_points) data = true_coefficients[0]*x + true_coefficients[1] + noise</span></span></code> </pre><br>  So, we have the data, and now we need to think about how we want to conduct an analysis. <br>  First, we know (or assume) that our gaussian noise means that our likelihood function will be gaussian.  It has two parameters: mean and variance.  Since the noise average is zero, the mean for the likelihood function will be set by the model value (and the model is linear, so there are two parameters).  While the variance is unknown to us, therefore, it will be another parameter. <br>  As a result, we have three parameters and a Gaussian likelihood function. <br>  We do not know anything about the values ‚Äã‚Äãof the parameters, so we assume a priori them to be uniformly distributed with arbitrary boundaries (these boundaries can be moved as far as desired). <br>  When specifying an a priori distribution, we must specify a label by which we will learn about the a posteriori values ‚Äã‚Äãof the parameters (the first argument), as well as indicate the boundaries of the distribution (the second and third arguments).  All of the above arguments are required (there are additional arguments that can be found in the documentation). <br><br><pre> <code class="python hljs">sigma = pymc.Uniform(<span class="hljs-string"><span class="hljs-string">'sigma'</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">100.</span></span>) a = pymc.Uniform(<span class="hljs-string"><span class="hljs-string">'a'</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">20.</span></span>) b = pymc.Uniform(<span class="hljs-string"><span class="hljs-string">'b'</span></span>, <span class="hljs-number"><span class="hljs-number">0.</span></span>, <span class="hljs-number"><span class="hljs-number">20.</span></span>)</code> </pre><br>  Now we need to set our model.  In pymc there are two of the most frequently used classes: deterministic and stochastic.  If, given the input data, it is possible to uniquely determine the value (s) that the model (s) returns, then this is a deterministic model.  In our case, given the coefficients of linear dependence for any point, we can uniquely determine the result, respectively, this is a deterministic model: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@pymc.deterministic(plot=False) def linear_fit(a=a, b=b, x=x): return a*x + b</span></span></code> </pre><br>  And finally, we set the likelihood function, in which the mean is the value of the model, sigma is a parameter with a given prior distribution, and data is our experimental data: <br><br><pre> <code class="python hljs">y = pymc.Normal(<span class="hljs-string"><span class="hljs-string">'y'</span></span>, mu=linear_fit, tau=<span class="hljs-number"><span class="hljs-number">1.0</span></span>/sigma**<span class="hljs-number"><span class="hljs-number">2</span></span>, value=data, observed=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br>  So, the whole model.py file looks like this: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pymc <span class="hljs-comment"><span class="hljs-comment">#Generate data with noise number_points = 20 true_coefficients = [10.4, 5.5] x = numpy.linspace(0, 10, number_points) noise = numpy.random.normal(size = number_points) data = true_coefficients[0]*x + true_coefficients[1] + noise #PRIORs: #as sigma is unknown then we define it as a parameter: sigma = pymc.Uniform('sigma', 0., 100.) #fitting the line y = a*x+b, hence the coefficient are parameters: a = pymc.Uniform('a', 0., 20.) b = pymc.Uniform('b', 0., 20.) #define the model: if a, b and x are given the return value is determined, hence the model is deterministic: @pymc.deterministic(plot=False) def linear_fit(a=a, b=b, x=x): return a*x + b #LIKELIHOOD #normal likelihood with observed data (with noise), model value and sigma y = pymc.Normal('y', mu=linear_fit, tau=1.0/sigma**2, value=data, observed=True)</span></span></code> </pre><br><br>  Now I propose to make a small theoretical digression and discuss what pymc does after all. <br><br>  From the point of view of mathematicians, we need to solve the following problem: <br>  p (a, b, sigma | Data) = p (Data | a, b, sigma) * p (a, b, sigma) / p (Data) <br><br>  Since  a, b and sigma are independent, then we can rewrite the equation as follows: <br>  p (a, b, sigma | Data) = p (Data | a, b, sigma) * p (a) * p (b) * p (sigma) / p (Data) <br><br>  On paper, the task looks very simple, but when we solve it numerically (we must also think that we want to solve any problem of this class numerically, and not only with certain types of probabilities), then difficulties arise. <br><br>  p (Data) is, as discussed in my previous <a href="http://habrahabr.ru/post/170545/">post</a> , a constant. <br>  p (Data | a, b, sigma) is definitely given to us (that is, with known a, b and sigma, we can uniquely calculate the probabilities for our available data) <br>  a here, instead of p (a), p (b) and p (sigma), we, in fact, have only pseudo-random variable generators distributed according to the law we specified. <br>  How to get a posteriori distribution from all this?  That's right, generate (sample) a, b and sigma, and then read p (Data | a, b, sigma).  As a result, we get a chain of values, which is a sample of the posterior distribution.  But this raises the question of how we can make this sample correctly.  If our a posteriori distribution has several modes ("hills"), then how can we generate a sample covering all the modes.  That is, the task is how to effectively make a sample that would ‚Äúcover‚Äù the entire distribution in the least amount of iterations.  There are several algorithms for this, the most used of which are <a href="http://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo">MCMC</a> (Markov chain Monte Carlo).  <a href="http://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BF%25D1%258C_%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B0">The Markov chain</a> is such a sequence of random events in which each element depends on the previous one, but does not depend on the previous one.  I will not describe the algorithm itself (this may be the topic of a separate post), but just note that pymc implements this algorithm and as a result gives the Markov chain, which is a sample of the a posteriori distribution.  Generally speaking, if we do not want the chain to be Markov, then we just need to ‚Äúthin out‚Äù it, i.e.  take, for example, every second element. <br>  So, we create a second file, let's call it run_model.py, in which we will generate a Markov chain.  The model.py and run_model.py files must be in the same folder, otherwise the code should be added to the run_model.py file: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sys <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> path path.append(<span class="hljs-string"><span class="hljs-string">"/////model.py/"</span></span>)</code> </pre><br>  First we import some modules that will be useful to us: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> polyfit <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> figure, plot, show, legend <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pymc <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model</code> </pre><br>  polyfit implements the least squares method ‚Äî we will compare the Bayesian analysis with it. <br>  figure, plot, show, legend are needed in order to build a final schedule. <br>  model is, in fact, our model. <br><br>  Then we create the MCMC object and run the sample: <br><br><pre> <code class="python hljs">D = pymc.MCMC(model, db = <span class="hljs-string"><span class="hljs-string">'pickle'</span></span>) D.sample(iter = <span class="hljs-number"><span class="hljs-number">10000</span></span>, burn = <span class="hljs-number"><span class="hljs-number">1000</span></span>)</code> </pre><br>  D.sample takes two arguments (in fact, you can specify more) - the number of iterations and burn-in (let's call it the ‚Äúwarm-up period‚Äù).  The warm-up period is the number of first iterations that are clipped.  The fact is that MCMC initially depends on the starting point (this is a property), so we need to cut off this period of dependence. <br><br>  This completes our analysis. <br>  Now we have an object D, in which the sample is located, and which has various methods (functions) allowing to calculate the parameters of this sample (average, most probable value, variance, etc.). <br><br>  In order to compare the results, we make the analysis of the method of least squares: <br><br><pre> <code class="python hljs">chisq_result = polyfit(model.x, model.data, <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br>  Now we print all the results: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"\n\nResult of chi-square result: a= %f, b= %f"</span></span> % (chisq_result[<span class="hljs-number"><span class="hljs-number">0</span></span>], chisq_result[<span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"\nResult of Bayesian analysis: a= %f, b= %f"</span></span> % (Davalue, Dbvalue) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> <span class="hljs-string"><span class="hljs-string">"\nThe real coefficients are: a= %f, b= %f\n"</span></span> %(model.true_coefficients[<span class="hljs-number"><span class="hljs-number">0</span></span>], model.true_coefficients[<span class="hljs-number"><span class="hljs-number">1</span></span>])</code> </pre><br>  We build standard for pymc graphics: <br><br><pre> <code class="python hljs">pymc.Matplot.plot(D)</code> </pre><br>  And finally, we build our final schedule: <br><br><pre> <code class="python hljs">figure() plot(model.x, model.data, marker=<span class="hljs-string"><span class="hljs-string">'+'</span></span>, linestyle=<span class="hljs-string"><span class="hljs-string">''</span></span>) plot(model.x, Davalue * model.x + Dbvalue, color=<span class="hljs-string"><span class="hljs-string">'g'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'Bayes'</span></span>) plot(model.x, chisq_result[<span class="hljs-number"><span class="hljs-number">0</span></span>] * model.x + chisq_result[<span class="hljs-number"><span class="hljs-number">1</span></span>], color=<span class="hljs-string"><span class="hljs-string">'r'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'Chi-squared'</span></span>) plot(model.x, model.true_coefficients[<span class="hljs-number"><span class="hljs-number">0</span></span>] * model.x + model.true_coefficients[<span class="hljs-number"><span class="hljs-number">1</span></span>], color=<span class="hljs-string"><span class="hljs-string">'k'</span></span>, label=<span class="hljs-string"><span class="hljs-string">'Data'</span></span>) legend() show()</code> </pre><br>  Here is the full content of the run_model.py file: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> polyfit <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> figure, plot, show, legend <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pymc <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> model <span class="hljs-comment"><span class="hljs-comment">#Define MCMC: D = pymc.MCMC(model, db = 'pickle') #Sample MCMC: 10000 iterations, burn-in period is 1000 D.sample(iter = 10000, burn = 1000) #compute chi-squared fitting for comparison: chisq_result = polyfit(model.x, model.data, 1) #print the results: print "\n\nResult of chi-square result: a= %f, b= %f" % (chisq_result[0], chisq_result[1]) print "\nResult of Bayesian analysis: a= %f, b= %f" % (Davalue, Dbvalue) print "\nThe real coefficients are: a= %f, b= %f\n" %(model.true_coefficients[0], model.true_coefficients[1]) #plot graphs from MCMC: pymc.Matplot.plot(D) #plot noised data, true line and two fitted lines (bayes and chi-squared): figure() plot(model.x, model.data, marker='+', linestyle='') plot(model.x, Davalue * model.x + Dbvalue, color='g', label='Bayes') plot(model.x, chisq_result[0] * model.x + chisq_result[1], color='r', label='Chi-squared') plot(model.x, model.true_coefficients[0] * model.x + model.true_coefficients[1], color='k', label='Data') legend() show()</span></span></code> </pre><br>  In the terminal we see the following answer: <br><br>  Result of chi-square result: a = 10.321533, b = 6.307100 <br><br>  Result of Bayesian analysis: a = 10.366272, b = 6.068982 <br><br>  The real coefficients are: a = 10.400000, b = 5.500000 <br><br>  I note that since we are dealing with a random process, the values ‚Äã‚Äãyou see in yourself may differ from the above (except for the last line). <br><br>  And in the folder with the file run_model.py we will see the following graphics. <br>  For parameter a: <br><img src="https://habrastorage.org/getpro/habr/post_images/9a9/894/f1c/9a9894f1ce48d6d7c97477ee1d76816d.png" alt="image"><br>  For parameter b: <br><img src="https://habrastorage.org/getpro/habr/post_images/bc1/175/c77/bc1175c77c454acb5fdd710a26b6f120.png" alt="image"><br>  For the sigma parameter: <br><img src="https://habrastorage.org/getpro/habr/post_images/067/901/ac3/067901ac3a67b276bfc1c8de3490f9a9.png" alt="image"><br>  On the right, we see a histogram of the posterior distribution, and the two pictures on the left belong to the Markov chain. <br>  I will not focus on them now.  Let me just say that the bottom graph is the autocorrelation graph (you can read more <a href="http://pymc-devs.github.com/pymc/modelchecking.html">here</a> ).  It gives an idea of ‚Äã‚Äãthe convergence of MCMC. <br>  And the top graph shows the sample trace.  That is, it shows how the sample took place over time.  The average of this trace is the average of the sample (compare the vertical axis in this graph with the horizontal axis in the histogram on the right). <br><br>  In conclusion, I will talk about one more interesting option. <br>  If you still put the pydot module and include the following line in the run_model.py file: <br><br><pre> <code class="python hljs">pymc.graph.dag(D).write_png(<span class="hljs-string"><span class="hljs-string">'dag.png'</span></span>)</code> </pre><br>  He will create the following image in the folder with the file run_model.py: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4a4/9fe/97b/4a49fe97b8661865a1439f77b400ec6d.png" alt="image"><br><br>  This is a direct acyclic graph representing our model.  White ellipses show stochastic nodes (these are a, b and sigma), triangles are deterministic nodes, and a darkened ellipse includes our pseudo-experimental data. <br>  That is, we see that the values ‚Äã‚Äãa and b come into our model (linear_fit), which itself is a deterministic node, and then go to the likelihood function y.  Sigma is first set by the stochastic node, but since the parameter in the likelihood function is not sigma, but tau = 1 / sigma ^ 2, the stochastic value sigma is first squared (the upper triangle on the right), and then it is considered tau.  And tau already enters the likelihood function, as well as our generated data. <br>  I think that this graph is very useful both for explaining the model and for self-checking of logic. <br>  models. </div><p>Source: <a href="https://habr.com/ru/post/170633/">https://habr.com/ru/post/170633/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../170615/index.html">50 examples of basic design of applications for Windows 8</a></li>
<li><a href="../170619/index.html">Natural language processing: the missing tool</a></li>
<li><a href="../170621/index.html">Dimension Elite 3D Printer Review</a></li>
<li><a href="../170623/index.html">Is there a future for component architecture?</a></li>
<li><a href="../170627/index.html">CoffeeScript 1.5.0 allows you to write comments in Markdown format</a></li>
<li><a href="../170635/index.html">How much does it cost to get to the top Apple App Store?</a></li>
<li><a href="../170637/index.html">We are testing integration with external services.</a></li>
<li><a href="../170639/index.html">Yandex.Money distributes gifts</a></li>
<li><a href="../170641/index.html">Announced the official date for the presentation of the Samsung Galaxy S IV</a></li>
<li><a href="../170647/index.html">Surfingbird as a tool to attract new users</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>