<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The success story of Yandex.Mail with PostgreSQL</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Vladimir Borodin (on dev1ant Habr√©), the system administrator for the storage management group at Yandex.Mail, introduces the difficulties of migratin...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The success story of Yandex.Mail with PostgreSQL</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/b7a/093/789/b7a093789fd441559d829b6a53f7e508.jpg"><br><br>  <em><a href="https://simply.name/ru/">Vladimir Borodin</a> (on <a href="https://habrahabr.ru/users/dev1ant/" class="user_link">dev1ant</a> Habr√©), the system administrator for the storage management group at Yandex.Mail, introduces the difficulties of migrating a large project from Oracle Database to PostgreSQL.</em>  <em>This is a transcript of the report from the <a href="http://www.highload.ru/">HighLoad ++</a> 2016 conference.</em> <br><br>  Hello!  My name is Vova, today I will talk about the Yandex.Mail database. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First a few facts that will matter in the future.  Yandex.Mail is a rather old service: it was launched in 2000, and therefore we have accumulated a lot of legacy.  We have - as is customary and fashionable to say - quite a highload-service itself, more than 10 million users per day, some hundreds of millions in total.  More than 200 thousand requests per second at the peak arrive to us in the backend.  We add more than 150 million emails per day that have passed checks for spam and viruses.  The total volume of letters for all 16 years is more than 20 petabytes. <br><br>  What are we talking about?  How we transferred metadata from Oracle to PostgreSQL.  The metadata there is not petabytes - there are a little more than three hundred terabytes.  More than 250 thousand requests per second fly into the bases.  It should be borne in mind that these are small OLTP requests, mostly reading (80%). <br><br>  This is not our first attempt to get rid of Oracle.  At the beginning of zero was an attempt to move to MySQL, it failed.  In 2007 or 2008 there was an attempt to write something of their own, it also failed.  In both cases, there was a failure not so much for technical reasons, as for organizational reasons. <br><a name="habracut"></a><br>  What is metadata?  Here they are highlighted with arrows.  These are folders, which are some kind of hierarchy with counters, tags (also, in fact, lists with counters), collectors, threads and, of course, letters. <br><br><img src="https://habrastorage.org/files/a56/42e/3d8/a5642e3d83ab4672b2a8d1ad7b34f08e.png"><br><br>  We do not store the letters themselves in metabase, the body of the letters are in a separate repository.  In meta database we store envelopes.  Envelopes are some of the email headers: from whom, to whom, the subject of the letter, the date and the like.  We store information about attachments and conversations. <br><br><img src="https://habrastorage.org/files/4ad/ae2/806/4adae2806a7046b295c95ab8a19d074c.png"><br><br><h1>  Back to 2012 </h1><br>  This was all in Oracle.  We had a lot of logic in the database itself.  Oraklovye bases were the most efficient hardware for recycling: we put a lot of data on the shard, more than 10 terabytes.  Relatively speaking, with 30 cores, we have a normal working load average of 100. This is not when everything is bad, but under normal operation. <br><br>  Baz was not enough, so much was done by hand, without automation.  There were a lot of manual operations.  To save money, we divided the bases into ‚Äúwarm‚Äù (75%) and ‚Äúcold‚Äù (25%).  ‚ÄúWarm‚Äù is for active users, they are with SSD.  ‚ÄúCold‚Äù - for inactive users, with SATA. <br><br>  Sharding and resiliency is an important topic in Yandex.  Sharding is because you don‚Äôt cram everything into one shard, but fault tolerance is because we regularly take and turn off one of our data centers to see that everything works. <br><br>  How was this implemented?  We have an internal BlackBox service (black box).  When one request arrives on one of our backends, the backend exchanges authentication data - login, password, cookie, token or something like that.  He goes with this in BlackBox, which, if successful, returns him the user ID and the name of the shard. <br><br><img src="https://habrastorage.org/files/8ed/686/ad6/8ed686ad65434195b3f36b56093de926.png"><br><br>  Then the backend fed this shard name to the OCCI driver, further inside this driver all the fault tolerance logic was implemented.  That is, roughly speaking, in a special file /etc/tnsnames.ora a shardname and a list of hosts that enter this shard are stored, it is served.  Oracle itself decided which master was one of them, who made a replica, who was alive, who was dead, etc. So, the sharding was implemented by means of an external service, and fault tolerance by means of an Oracle driver. <br><br>  Most of the backends were written in C ++.  In order not to produce ‚Äúbicycles‚Äù, for a long time they had a common abstraction macs meta access.  This is just an abstraction for going to the base.  Almost all the time she had one macs_ora implementation for circulation directly in Oracle.  At the very bottom, of course, is OCCI.  There was also a small dbpool layer that implemented the connection pool. <br><br><img src="https://habrastorage.org/files/779/19a/ca1/77919aca1fc5475882f98367b08859ad.png"><br><br>  This is how it was once long conceived, designed and implemented.  Over time, abstractions have leaked, backends have begun to use methods from the macs_ora implementation, and even worse from dbpool.  There were Java and any other backends that could not use this library.  All this noodles then had to painfully rake. <br><br>  Oracle is a great database, but there were problems with it.  For example, laying out PL / SQL code is a pain, because there is a library cache.  If the database is under load, then you cannot simply take and update the function code that is currently being used by some sessions. <br><br>  The remaining problems are associated not so much with Oracle, as with the approach we used.  Again: a lot of manual operations.  Switching masters, pouring in new databases, launching user migrations - everything was done by hand, because there were few bases. <br><br>  From the development point of view, there is a disadvantage that the [C ++] plus driver has only a synchronous interface.  That is, a normal asynchronous backend cannot be written on top.  This caused some pain in the design.  The second pain in the development was caused by the fact that raising the test base is problematic.  First, because with hands, and second, because it is money. <br><br>  No matter what anyone says, Oracle has support.  Although support for enterprise-companies is often far from ideal.  But the main reason for the transition is money.  Oracle is expensive. <br><br><h1>  Chronology </h1><br>  In October 2012, more than 4 years ago, it was decided that we should get rid of Oracle.  There were no words PostgreSQL, did not sound any technical details - it was a purely political decision: to get rid of, period of 3 years. <br><br>  After six months, we began the first experiments.  What have been spent these six months, I can tell you a little later.  These six months have been important. <br><br>  In April 2013, we experimented with PostgreSQL.  Then there was a very fashionable trend for all sorts of NoSQL solutions, and we tried many different things.  We remembered that we have all the metadata already stored in the search backend by mail, and maybe you can use it.  This solution was also tried. <br><br>  The first successful experiment was with collectors, I <a href="https://simply.name/ru/video-pg-meetup-yandex.html">talked</a> about it <a href="https://simply.name/ru/video-pg-meetup-yandex.html">at a meeting in Yandex at 2014</a> . <br><br>  We took a small piece (2 terabytes) of fairly loaded (40 thousand queries per second) mail metadata and took them from Oracle to PostgreSQL.  That piece which is not so connected with the main metadata.  We did it and we liked it.  We decided that PostgreSQL is our choice. <br><br>  Next, we wrote down a prototype of the mail scheme for PostgreSQL and began to add the entire stream of letters to it.  We did it asynchronously: we folded all 150 million letters a day in PostgreSQL.  If the laying failed, then we would not care.  It was a pure experiment, he did not touch the production. <br><br>  This allowed us to test the initial hypotheses with the scheme.  When there is data that is not a pity to throw out - it is very convenient.  I made some kind of scheme, stuffed letters into it, saw that it was not working, dropped everything and started anew.  Excellent data that can be dropped, we love these. <br><br>  Also, thanks to this, it turned out to some extent to carry out load testing right under the live load, and not by some kind of synthetics, not on individual stands.  So it turned out to make an initial estimate of the iron, which is needed for PostgreSQL.  And of course, experience.  The main goal of the previous experiment and prototype is experience. <br><br>  Then the main work began.  Development took about a year of calendar time.  Long before it ended, we moved our mailboxes from Oracle to PostgreSQL.  We always understood that there would never be such a thing that we would show everyone for one night ‚Äúsorry, technical work‚Äù, transfer 300 terabytes and start working on PostgreSQL.  It does not happen.  We would definitely break down, roll back, and everything would be bad.  We understood that there would be a rather long period of time when some of the boxes would live in Oracle, and some in PostgreSQL, there would be a slow migration. <br><br>  In the summer of 2015, we moved our boxes.  The ‚ÄúMail‚Äù team, which writes it, tests it, admins, and so on, transferred its boxes.  This has greatly accelerated development.  The abstract Vasya suffers, or you suffer, but you can correct it - these are two different things. <br><br>  Even before we added and implemented all the features, we began to carry inactive users.  We call inactive such a user, to whom the mail arrives, we fold the letters, but he does not read them: neither the web, nor the mobile, nor IMAP - they are not interested in him.  There are such users, unfortunately.  We began to carry such inactive users when, for example, IMAP was not yet fully implemented, or half the pens in the mobile application did not work. <br><br>  But this is not because we are so bold and decided to break all the boxes, but because we had a plan B in the form of a reverse transfer, and it helped us a lot.  There was even automation.  If we moved a user, and he suddenly tried, for example, to log in to the web interface, he woke up and became active - we transferred him back to Oracle, so as not to break any features for him.  This allowed us to fix a bunch of bugs in the transfer code. <br><br>  Then followed the migration.  Here are some interesting facts.  We spent 10 man-years in order to rewrite all our noodles, which we have accumulated over 12-15 years. <br><br>  The migration itself took place very quickly.  This is a 4 month schedule.  Each line is the percentage of the load that the service renders from PostgreSQL.  Split into services: IMAP, web, POP3, tab, mobile and so on. <br><br> <a href=""><img src="https://habrastorage.org/files/3fd/7ce/54a/3fd7ce54a5d542df8e0ebc7d0f85a379.png"></a> <br><br>  Unfortunately, the abyss can not be jumped by 95%.  We could not move all of them by April, because the registration remained at Oracle, this is a rather complicated mechanism.  It turned out that we registered new users in Oracle and immediately transferred them to PostgreSQL at night.  In May, we filed the registration, and in July, we had already extinguished all the Oracle databases. <br><br><img src="https://habrastorage.org/files/af7/bdf/a07/af7bdfa071a54b8b9224c5b4309046b6.png"><br><br><h1>  Major changes </h1><br>  In our abstraction, another macs_pg implementation appeared, and we unraveled all the noodles.  All those leaked abstractions had to be carefully rewritten.  At the bottom of her libpq, we made a small layer apq, where the connection pool, timeouts, error handling are implemented, and everything is asynchronous. <br><br><img src="https://habrastorage.org/files/b0d/07e/ca9/b0d07eca9214452a81a152f1d9afc577.png"><br><br>  Sharding and fault tolerance are all the same.  The backend receives authentication data from the user, exchanges them in BlackBox for a user ID and shard name.  If the name of the shard has the letter pg, then it makes another request to the new service, which we call Sharpei.  The backend transmits there the identifier of this user and the mode in which he wants to get the base.  For example, ‚ÄúI want a master‚Äù, ‚ÄúI want a synchronous replica‚Äù or ‚ÄúI want the nearest host‚Äù.  Sharpei returns him connection strings.  Next, the backend opens the connection, holds it and uses it. <br><br>  To know the information, who is the master, who is the replica, who is alive, who is dead, who is behind, who is not, Sharpei goes to the final bases once a second and asks for their statuses.  At this point, a component appeared that took on both functions: sharding and fault tolerance. <br><br><img src="https://habrastorage.org/files/8b8/3ef/062/8b83ef06247c470699239b5c470aa240.png"><br><br>  In terms of iron, we made a few changes.  Because Oracle is licensed by processor cores, we had to scale vertically.  On one processor core, we crammed a lot of memory, a lot of SSD-drives.  There was a small number of databases with a small number of processor cores, but with huge arrays of memory and disks.  We have always had strictly one replica for resiliency, because all subsequent ones are money. <br><br>  In PostgreSQL, we changed the approach.  We began to make smaller bases and two replicas in each shard.  This allowed us to wrap up reading loads on replicas.  That is, in Oracle everything was serviced from the wizard, and in PostgreSQL - three machines instead of two smaller ones, and we wrapped the reading in PostgreSQL.  In the case of Oracle, we scaled vertically; in the case of PostgreSQL, we scaled horizontally. <br><br><img src="https://habrastorage.org/files/d6e/3e3/b4c/d6e3e3b4c34b4ed6937b802204f74d23.png"><br><br>  In addition to the ‚Äúwarm‚Äù and ‚Äúcold‚Äù bases, there appeared also ‚Äúhot‚Äù ones.  Why?  Because we suddenly discovered that 2% of active users create 50% of the load.  There are some bad users who are raping us.  We made separate bases for them.  They are not much different from warm ones, there are also SSDs there, but they are smaller by one processor core, because the processor is more actively used there. <br><br>  Of course, we wrote down the automation of the transfer of users between shards.  For example, if a user is inactive, now lives in a satashnoy [with SATA drive] base and suddenly started using IMAP, we will transfer it to the ‚Äúwarm‚Äù base.  Or if he doesn't move in a warm base for half a year, then we will transfer him to the ‚Äúcold‚Äù one. <br><br>  Moving old emails of active users from SSD to SATA is what we really want to do, but we cannot yet.  If you are an active user, you live on an SSD and you have 10 million emails, they are all on an SSD, which is not very efficient.  But so far in PostgreSQL there is no normal partitioning. <br><br>  We changed all identifiers.  In the case of Oracle, they were all globally unique.  We had a separate base where it was written that in this shard such ranges, in this - such.  Of course, we had a facac when, by virtue of an error, identifiers crossed, and about half of them were tied to their uniqueness.  It was painful. <br><br>  In the case of PostgreSQL, we decided to switch to a new scheme, when our identifiers are unique within a single user.  If earlier the letter identifier was unique to the letter, now the uid mid pair is unique.  In all the tablets we have the first field of uid, everything is prefixed to them, it is a part for now. <br><br>  Besides the fact that it is less space, there is another unobvious plus.  Since all these identifiers are taken from sequences, we have less competition for the last page of the index.  In Oracle, we used reverse indexes to solve this problem.  In the case of PostgreSQL, since the inserts go to different pages of the index, we use the usual B-Tree, and we have range scans, all the data of one user in the index are next to each other.  It is very convenient. <br><br>  We have introduced revisions for all objects.  This made it possible to read from the replicas, firstly, the intact data, and secondly, the incremental updates for IMAP, mobile.  That is, the answer to the question ‚Äúwhat has changed in this folder since such a revision‚Äù has been greatly simplified. <br><br>  In PostgreSQL, everything is fine with arrays, composites.  We made a part of the data denormalization.  Here is one example: <br><br><img src="https://habrastorage.org/files/6d8/47b/821/6d847b82141c461083a7fcdf61c498f3.png"><br><br>  This is our main mail.box sign.  It contains a line for each letter.  Her primary key is a pair of uid mid.  There is also an array of lids tags, because there can be more than one tag on one letter.  At the same time, there is a task to answer the question ‚Äúgive me all the letters with such a tag‚Äù.  Obviously, this requires some kind of index.  If we build a B-Tree index by array, then it will not answer such a question.  To do this, we use a clever gin functional index across the uid and lids fields.  It allows us to answer the question "give me all the letters of such and such a user with such and such labels or with such and such a label." <br><br><h3>  Stored logic </h3><br><ul><li>  Since there was a lot of pain with stored logic from Oracle, we decided that PostgreSQL would not have stored logic at all, no.  But in the course of our experiments and prototypes, we realized that <strong>PL / pgSQL is very good</strong> .  It does not suffer from a problem with the library cache, and we did not find other very critical problems. </li><li>  At the same time, the <strong>amount of logic was greatly reduced</strong> , leaving only the one that is needed for the logical integrity of the data.  For example, if you put a letter, then increase the counter in the plate with folders. </li><li>  Since there is no undo, the <strong>cost of the error has become much higher</strong> .  In undo, we climbed a couple of times after laying out a bad code, about that my colleague Alexander <a href="https://events.yandex.ru/lib/talks/4057/">did a separate report on our mitap</a> . </li><li>  Due to the lack of library cache it is much <strong>easier to deploy</strong> .  We ride a couple of times a week instead of quarterly, as it was before. </li></ul><br><h3>  Service approach </h3><br><ul><li>  Since we changed the hardware and began to scale horizontally, we changed the approach to serving the bases.  Bases we now rule <strong>SaltStack</strong> .  The most important feature of his killer feature for us is the opportunity to see a detailed diff between what is now on the base and what we expect from it.  If the observed suits, the person presses the "roll out" button, and it rolls. </li><li>  We now change the schema and code <strong>through migrations</strong> .  We had a <a href="https://events.yandex.ru/lib/talks/4055/">separate report about it</a> . </li><li>  We left the manual service, everything that was possible was <strong>automated</strong> .  Switching masters, transferring users, pouring new shards, and so on - all this is by a button and very simple. </li><li>  Since deploying a new database is one button, we have received <strong>representative test environments</strong> for development.  Each developer in the database, two, as he wants - it is very convenient. </li></ul><br><h1>  Problems </h1><br>  Such things never pass smoothly. <br><br>  This is a list of threads in the community with problems that we could not solve ourselves. <br><br><ul><li>  Problem with ExclusiveLock on inserts </li><li>  Checkpoint distribution </li><li>  Shared_buffers </li><li>  Hanging startup process on the replica after vacuuming on master </li><li>  Replication slots and isolation levels </li><li>  Segfault in BackendIdGetTransactions </li></ul><br>  That is, we went to the community, and they helped us.  It was a test of what to do when you do not have enterprise support: there is a community, and it works.  And this is very cool.  Of course, much more problems we decided on our own. <br><br>  For example, we had a very popular joke: ‚ÄúThe autovacuum is to blame for any incomprehensible situation‚Äù.  We also solved these problems. <br><br>  We really lacked a way to diagnose PostgreSQL.  The guys from Postgres Pro filed us a wait-interface.  I already <a href="https://simply.name/ru/slides-pgday2015.html">told about it on PG Day in 2015 to St. Petersburg</a> .  There you can read how it works.  With the help of the guys from Postgres Pro and EnterpriseDB, it was included in kernel 9.6.  Not all, but some of these developments were included in 9.6.  Further this functionality will be improved.  In 9.6 columns appeared that allow much better understand what is happening in the database. <br><br>  Surprise.  We encountered a problem with backups.  We have a recovery window of 7 days, that is, we should be able to recover at any time in the past for the last 7 days.  In Oracle, the size of the space for all backups and archivogs was about the size of the database.  Base 15 terabytes - and its backup for 7 days takes 15 terabytes. <br><br>  In PostgreSQL, we use barman, and in it, for backups, you need space at least 5 times larger than the size of the base.  Because WAL is compressed, but there are no backups, there are File-level increments that don‚Äôt really work, in general everything is single-threaded and very slow.  If we were backing up as is these 300 terabytes of meta-data, we would need about 2 petabytes of backups.  Let me remind you, the entire repository of "Mail" - 20 petabytes.  That is, 10% we would have to cut off just under the backups of meta-databases for the last 7 days, which is a pretty bad plan. <br><br>  We didn‚Äôt think of anything better and patched the barman, <a href="https://github.com/2ndquadrant-it/barman/issues/21">here‚Äôs a pull request</a> .  Almost a year has passed, as we ask them to file this killer feature, and they are asking for money from us to keep it up.  Very arrogant guys.  My colleague Eugene, who wrote down all this, <a href="http://pgday.ru/ru/2016/papers/80">talked about this on PGday in 2016</a> .  It really shakes backups much better, speeds them up, there are honest increments. <br><br>  According to the experience of the experiment, the prototype, and other databases that appeared on PostgreSQL by then, we expected a bunch of rakes during the transfer.  And they were not there.  There were many problems, but they were not related to PostgreSQL, which was surprising to us.  It was full of problems with the data, because in 10 years a lot of legacy has accumulated.  Suddenly, it was discovered that in some databases the data is encoded in KOI8-R, or other strange things.  Of course, there were errors in the transfer logic, so the data also had to be repaired. <br><br><h1>  Completion </h1><br>  There are things that we really miss in PostgreSQL. <br><br>  For example, <strong>partitioning</strong> to move old data from SSD to SATA.  We lack a good built-in <strong>recovery manager</strong> to not use the batman fork, because it probably will never reach the core barman.  We are already tired: we have been kicking them for almost a year, but they are not in a hurry.  It seems that this should be not aside from PostgreSQL, but in the kernel. <br><br>  We will develop the <strong>wait-interface</strong> .  I think in the 10th version a <strong>quourum commit</strong> will happen, there is a patch in good condition.  We also really want a <strong>normal work with the disk</strong> .  In terms of disk I / O, PostgreSQL strongly loses Oracle. <br><br>  What is the result?  If you take into account the raid replicas, then we have more than 1 petabyte in PostgreSQL.  Recently, I thought there were a little over 500 billion lines.  There flies 250 thousand requests per second.  In total, it took us 3 calendar years, but we spent more than 10 man-years.  That is the effort of the whole team is quite impressive. <br><br>  What did we get?  It has become faster deployed, despite the fact that the databases have become much larger, and the number of DBA has decreased.  The DBA for this project is now smaller than when Oracle was. <br><br>  Whether we wanted to or not, we had to refactor all the backend code.  All that legacy, which accumulated over the years, was cut out.  Our code is cleaner now, and this is very good. <br><br>  There is no tar without spoons.  We now have 3 times more hardware for PostgreSQL, but this is nothing compared to the cost of Oracle.  So far we have not had major fakapov. <br><br>  A quick note from me.  In the "Mail" we use a lot of open source libraries, projects and ready-made solutions.  To the three chairs on which we sat tight, which we have almost everywhere - Linux, nginx, postfix - was added PostgreSQL.  Now we use it for many bases in other projects.  We liked him.  Fourth - a good, reliable chair.  I think this is a success story. <br><br>  That's all I wanted to say.  Thank! <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/pe_dwL38_o8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <em><a href="http://www.highload.ru/2016/abstracts/2422.html">Vladimir Borodin - The success story of Yandex.Mail with PostgreSQL</a></em> </div><p>Source: <a href="https://habr.com/ru/post/321756/">https://habr.com/ru/post/321756/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../321744/index.html">As I made the fastest resize of images. Part 0</a></li>
<li><a href="../321746/index.html">jQuery UI FadeSlide 4.0</a></li>
<li><a href="../321748/index.html">JavaScript start performance</a></li>
<li><a href="../321750/index.html">Foreign GPS monitoring services</a></li>
<li><a href="../321754/index.html">How to ‚Äúpunch‚Äù a person on the Internet: using Google operators and logic</a></li>
<li><a href="../321758/index.html">Introducing web single sign-on (Web SSO) and identity federation</a></li>
<li><a href="../321760/index.html">Do not trust SUDO, she can let you down</a></li>
<li><a href="../321764/index.html">About the organization of Telephony in the Sales Department and integration with CRM</a></li>
<li><a href="../321766/index.html">Content filtering by state order. The case of educational blocking RosKomSvoboda</a></li>
<li><a href="../321768/index.html">Changing the contents of Web.config at runtime when debugging in Visual Studio and IISExpress</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>