<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Free VMware vSphere Storage Appliance replacement based on DRBD</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="VMware recently announced new products in the vSphere 5 line, and we were very interested in what is a VMware vSphere Storage Appliance? 

 In short, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Free VMware vSphere Storage Appliance replacement based on DRBD</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage1/a106a426/2e9914a1/bc4c1c5a/35f4c37a.jpg" align="left">  VMware recently announced new products in the vSphere 5 line, and we were very interested in what is a VMware vSphere Storage Appliance? <br><br>  In short, the bottom line is the ability to build a fault-tolerant virtual infrastructure without an external storage system.  For the implementation, two or three virtual machines are installed (one for each host), which replicate the free space of the ESXi server disk subsystem and provide it as shared storage to all the same ESXi hosts.  Detailed in Russian storage appliance is described <a href="http://www.vmgu.ru/news/vmware-vsphere-storage-appliance-vsa">here</a> . <br><br>  An interesting idea, but the price bites - around $ 6K.  In addition, if you think about performance, is it possible that there will be a drawdown in the speed of the disk array?  Approaching the question from the other side, you can think of many other ways of organizing external storage.  For example, you can create external storage from almost any hardware with the required number of disks and installed Openfiler, FreeNAS, Nexenta, Open-E software ‚Äî these software products have the ability to replicate between systems. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This approach is practiced by many companies that do not have the opportunity to purchase an expensive storage system of a famous manufacturer that would provide sufficient performance and reliability.  As a rule, such systems are equipped with two controllers, a redundant power supply system, high-speed drives and so on ... <br><a name="habracut"></a><br>  But back to the beginning and look at the scheme that VMware offers: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/storage1/929b7181/21b1757c/267a464c/92771e30.gif"></div> <br><br>  What do we see?  3 ESXi hosts with virtual machines deployed on them, one for each host.  Machines are collected in a cluster and give us the internal drives as external. <br><br>  The idea of ‚Äã‚Äãcollecting such a solution from one of the available tools has been in the air for a long time, but it did not find any justification.  And here VMware itself gave an impetus to try everything in a test environment. <br><br>  Solutions for building fault-tolerant storage - a bunch, for example, based on Openfiler + DRBD + Heartbeat.  But at the heart of all these decisions lies the idea of ‚Äã‚Äãbuilding an external repository.  Why not try to do something similar, but based on virtual machines? <br><br>  As a foundation, take 2 virtual machines with OS Ubuntu, Ubuntu documentation on building failover iSCSI-target and try to make your Appliance. <br><br>  Partitioning disks on both cluster nodes: <br><br> <code>/dev/sda1 - 10 GB / (primary' ext3, Bootable flag: on) <br> /dev/sda5 - 1 GB swap (logical) <br> <br> /dev/sdb1 - 1 GB (primary) DRBD meta-.  . <br> /dev/sdc1 - 1 GB (primary) DRBD ,      iSCSI.  . <br> /dev/sdd1 - 50 GB (primary) DRBD   iSCSI-target.</code> <br> <br>  The size of the disk sdd1 is selected for an example.  Actually, all the remaining free space on the local storage of the ESXi host is taken. <br><br>  ISCSI network: <br> <code>iSCSI server1: node1.demo.local IP address: 10.11.55.55 <br> iSCSI server2: node2.demo.local IP address: 10.11.55.56 <br> iSCSI Virtual IP address 10.11.55.50</code> <br> <br>  Private network: <br> <code>iSCSI server1: node1-private IP address: 192.168.22.11 <br> iSCSI server2: node2-private IP address: 192.168.22.12</code> <br> <br>  / etc / network / interfaces: <br><br>  For node1: <br> <code>auto eth0 <br> iface eth0 inet static <br> address 10.11.55.55 <br> netmask 255.0.0.0 <br> gateway 10.0.0.1 <br> <br> auto eth1 <br> iface eth1 inet static <br> address 192.168.22.11 <br> netmask 255.255.255.0</code> <br> <br>  For node2: <br> <code>auto eth0 <br> iface eth0 inet static <br> address 10.11.55.56 <br> netmask 255.0.0.0 <br> gateway 10.0.0.1 <br> <br> auto eth1 <br> iface eth1 inet static <br> address 192.168.22.12 <br> netmask 255.255.255.0</code> <br> <br>  The / etc / hosts file for both nodes: <br><br> <code>127.0.0.1 localhost <br> 10.11.55.55 node1.demo.local node1 <br> 10.11.55.56 node2.demo.local node2 <br> 192.168.22.11 node1-private <br> 192.168.22.12 node2-private</code> <br> <br>  Package installation: <br> <code>apt-get -y install ntp ssh drbd8-utils heartbeat jfsutils</code> <br> <br>  Reboot the servers. <br><br>  We change the owners of files and permissions to them: <br> <code>chgrp haclient /sbin/drbdsetup <br> chmod ox /sbin/drbdsetup <br> chmod u+s /sbin/drbdsetup <br> chgrp haclient /sbin/drbdmeta <br> chmod ox /sbin/drbdmeta <br> chmod u+s /sbin/drbdmeta</code> <br> <br>  Use /etc/drbd.conf to describe the configuration.  We define 2 resources: <br>  1. DRBD device that will contain iSCSI configuration files; <br>  2. DRBD device that will become our iSCSI target. <br><br>  For node1: <br><br> <code>/etc/drbd.conf: <br> <br> resource iscsi.config { <br> protocol C; <br> <br> handlers { <br> pri-on-incon-degr "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> pri-lost-after-sb "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> local-io-error "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> outdate-peer "/usr/lib/heartbeat/drbd-peer-outdater -t 5"; <br> } <br> <br> startup { <br> degr-wfc-timeout 120; <br> } <br> <br> disk { <br> on-io-error detach; <br> } <br> <br> net { <br> cram-hmac-alg sha1; <br> shared-secret "password"; <br> after-sb-0pri disconnect; <br> after-sb-1pri disconnect; <br> after-sb-2pri disconnect; <br> rr-conflict disconnect; <br> } <br> <br> syncer { <br> rate 100M; <br> verify-alg sha1; <br> al-extents 257; <br> } <br> <br> on node1 { <br> device /dev/drbd0; <br> disk /dev/sdc1; <br> address 192.168.22.11:7788; <br> meta-disk /dev/sdb1[0]; <br> } <br> <br> on node2 { <br> device /dev/drbd0; <br> disk /dev/sdc1; <br> address 192.168.22.12:7788; <br> meta-disk /dev/sdb1[0]; <br> } <br> } <br> <br> resource iscsi.target.0 { <br> protocol C; <br> <br> handlers { <br> pri-on-incon-degr "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> pri-lost-after-sb "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> local-io-error "echo o &gt; /proc/sysrq-trigger ; halt -f"; <br> outdate-peer "/usr/lib/heartbeat/drbd-peer-outdater -t 5"; <br> } <br> <br> startup { <br> degr-wfc-timeout 120; <br> } <br> <br> disk { <br> on-io-error detach; <br> } <br> <br> net { <br> cram-hmac-alg sha1; <br> shared-secret "password"; <br> after-sb-0pri disconnect; <br> after-sb-1pri disconnect; <br> after-sb-2pri disconnect; <br> rr-conflict disconnect; <br> } <br> <br> syncer { <br> rate 100M; <br> verify-alg sha1; <br> al-extents 257; <br> } <br> <br> on node1 { <br> device /dev/drbd1; <br> disk /dev/sdd1; <br> address 192.168.22.11:7789; <br> meta-disk /dev/sdb1[1]; <br> } <br> <br> on node2 { <br> device /dev/drbd1; <br> disk /dev/sdd1; <br> address 192.168.22.12:7789; <br> meta-disk /dev/sdb1[1]; <br> } <br> }</code> <br> <br>  Copy the configuration to the second node: <br> <code>scp /etc/drbd.conf root@10.11.55.56:/etc/</code> <br> <br>  We initialize disks with meta-data on both servers: <br> <code>[node1]dd if=/dev/zero of=/dev/sd1 <br> [node1]dd if=/dev/zero of=/dev/sdd1 <br> [node1]drbdadm create-md iscsi.config <br> [node1]drbdadm create-md iscsi.target.0 <br> <br> [node2]dd if=/dev/zero of=/dev/sd1 <br> [node2]dd if=/dev/zero of=/dev/sdd1 <br> [node2]drbdadm create-md iscsi.config <br> [node2]drbdadm create-md iscsi.target.0</code> <br> <br>  We start drbd: <br> <code>[node1]/etc/init.d/drbd start <br> [node2]/etc/init.d/drbd start</code> <br> <br>  Now you need to decide which server will be primary and which secondary to perform synchronization between disks.  Assume that the primary (Primary) will be node1. <br>  Run the command on the first node: <br> <code>[node1]drbdadm -- --overwrite-data-of-peer primary iscsi.config</code> <br> <br>  Command output <br> <code>cat /proc/drbd: <br> <br> version: 8.3.9 (api:88/proto:86-95) <br> srcversion: CF228D42875CF3A43F2945A <br> 0: cs:Connected ro: <font color="red">Primary/Secondary ds:UpToDate/UpToDate</font> C r----- <br> ns:1048542 nr:0 dw:0 dr:1048747 al:0 bm:64 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 <br> 1: cs:Connected ro:Secondary/Secondary ds:Inconsistent/Inconsistent C r----- <br> ns:0 nr:0 dw:0 dr:0 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:52428768</code> <br> <br>  Format and mount the / dev / drbd0 partition: <br> <code>[node1]mkfs.ext3 /dev/drbd0 <br> [node1]mkdir -p /srv/data <br> [node1]mount /dev/drbd0 /srv/data</code> <br> <br>  Create a file on the first node and then switch the second to Primary mode: <br> <code>[node1]dd if=/dev/zero of=/srv/data/test.zeros bs=1M count=100</code> <br> <br>  For node1: <br> <code>[node1]umount /srv/data <br> [node1]drbdadm secondary iscsi.config</code> <br> <br>  For node2: <br> <code>[node2]mkdir -p /srv/data <br> [node2]drbdadm primary iscsi.config <br> [node2]mount /dev/drbd0 /srv/data</code> <br> <br>  On the second node, a file of 100 MB in size will be visible. <br> <code>ls ‚Äìl /srv/data</code> <br> <br>  Delete it and again switch to the first node: <br><br>  On node2: <br> <code>[node2]rm /srv/data/test.zeros <br> [node2]umount /srv/data <br> [node2]drbdadm secondary iscsi.config</code> <br> <br>  On node1: <br> <code>[node1]drbdadm primary iscsi.config <br> [node1]mount /dev/drbd0 /srv/data</code> <br> <br>  Run the command ls / srv / data.  If there is no data on the partition, then the replication was successful. <br><br>  Go to the installation of iSCSI-target.  Select the first node as Primary and perform partition synchronization: <br> <code>[node1]drbdadm -- --overwrite-data-of-peer primary iscsi.target.0 <br> <br> cat /proc/drbd <br> <br> version: 8.3.9 (api:88/proto:86-95) <br> srcversion: CF228D42875CF3A43F2945A <br> 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- <br> ns:135933 nr:96 dw:136029 dr:834 al:39 bm:8 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 <br> 1: cs:SyncSource ro: <font color="red">Primary/Secondary ds:UpToDate/Inconsistent</font> C r----- <br> ns:1012864 nr:0 dw:0 dr:1021261 al:0 bm:61 lo:1 pe:4 ua:64 ap:0 ep:1 wo:f oos:51416288 <br> [&gt;....................] sync'ed: 2.0% (50208/51196)M <br> finish: 0:08:27 speed: 101,248 (101,248) K/sec</code> <br> <br>  Wait for sync ... <br><br> <code>cat /proc/drbd <br> <br> version: 8.3.9 (api:88/proto:86-95) <br> srcversion: CF228D42875CF3A43F2945A <br> 0: cs:Connected ro:Primary/Secondary ds:UpToDate/UpToDate C r----- <br> ns:135933 nr:96 dw:136029 dr:834 al:39 bm:8 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0 <br> 1: cs:Connected ro: <font color="red">Primary/Secondary ds:UpToDate/UpToDate</font> C r----- <br> ns:52428766 nr:0 dw:0 dr:52428971 al:0 bm:3200 lo:0 pe:0 ua:0 ap:0 ep:1 wo:f oos:0</code> <br> <br>  Install the iscsitarget package on both nodes: <br> <code>[node1]apt-get -y install iscsitarget <br> [node2]apt-get -y install iscsitarget</code> <br> <br>  Enable the option to run iscsi as a service: <br> <code>[node1]sed -is/false/true/ /etc/default/iscsitarget <br> [node2]sed -is/false/true/ /etc/default/iscsitarget</code> <br> <br>  Delete entries from all scripts: <br> <code>[node1]update-rc.d -f iscsitarget remove <br> [node2]update-rc.d -f iscsitarget remove</code> <br> <br>  Moving config files to drbd partition: <br> <code>[node1]mkdir /srv/data/iscsi <br> [node1] mv /etc/iet/ietd.conf /srv/data/iscsi <br> [node1]ln -s /srv/data/iscsi/ietd.conf /etc/iet/ietd.conf <br> [node2]rm /etc/iet/ietd.conf <br> [node2]ln -s /srv/data/iscsi/ietd.conf /etc/iet/ietd.conf</code> <br> <br>  We describe the iSCSI-target in the /srv/data/iscsi/ietd.conf file: <br><br> <code>Target iqn.2011-08.local.demo:storage.disk.0 <br> # IncomingUser geekshlby secret - ,      <br> # OutgoingUser geekshlby password <br> Lun 0 Path=/dev/drbd1,Type=blockio <br> Alias disk0 <br> MaxConnections 1 <br> InitialR2T Yes <br> ImmediateData No <br> MaxRecvDataSegmentLength 8192 <br> MaxXmitDataSegmentLength 8192 <br> MaxBurstLength 262144 <br> FirstBurstLength 65536 <br> DefaultTime2Wait 2 <br> DefaultTime2Retain 20 <br> MaxOutstandingR2T 8 <br> DataPDUInOrder Yes <br> DataSequenceInOrder Yes <br> ErrorRecoveryLevel 0 <br> HeaderDigest CRC32C,None <br> DataDigest CRC32C,None <br> Wthreads 8</code> <br> <br>  Now you need to configure heartbeat to control the iSCSI-target virtual IP address in case of node failure. <br><br>  We describe the cluster in the /etc/heartbeat/ha.cf file: <br><br> <code>logfacility local0 <br> keepalive 2 <br> deadtime 30 <br> warntime 10 <br> initdead 120 <br> bcast eth0 <br> bcast eth1 <br> node node1 <br> node node2</code> <br> <br>  Authentication mechanism <br> <code>/etc/heartbeat/authkeys: <br> <br> auth 2 <br> 2 sha1 NoOneKnowsIt</code> <br> <br>  Change the permissions on the / etc / heartbeat / authkeys file: <br> <code>chmod 600 /etc/heartbeat/authkeys</code> <br> <br>  We describe the cluster resources in the / etc / heartbeat / haresources file ‚Äî the main node, virtual IP, file systems, and services that will be launched: <br><br> <code>/etc/heartbeat/haresources <br> <br> node1 drbddisk::iscsi.config Filesystem::/dev/drbd0::/srv/data::ext3 <br> node1 IPaddr::10.11.55.50/8/eth0 drbddisk::iscsi.target.0 iscsitarget</code> <br> <br>  Copy the configuration to the second node: <br> <code>[node1]scp /etc/heartbeat/ha.cf root@10.11.55.56:/etc/heartbeat/ <br> [node1]scp /etc/heartbeat/authkeys root@10.11.55.56:/etc/heartbeat/ <br> [node1]scp /etc/heartbeat/haresources root@10.11.55.56:/etc/heartbeat/</code> <br> <br>  Unmount / srv / data, make the first node as Secondary. <br>  Starting a heartbeat <br> <code>[node1]/etc/init.d/heartbeat start</code> <br> <br>  Reboot both servers. <br><br> <code>[node1]/etc/init.d/drbd start <br> [node2]/etc/init.d/drbd start <br> <br> [node1]drbdadm secondary iscsi.config -  <br> [node1]drbdadm secondary iscsi.target.0 -  <br> <br> [node2]drbdadm primary iscsi.config <br> [node2]drbdadm primary iscsi.target.0 <br> <br> [node1]cat /proc/drbd <br> <br> [node1]/etc/init.d/heartbeat start</code> <br> <br>  After the heartbeat starts, we transfer the first node to the primary mode, the second to the secondary mode (otherwise, it will not start). <br><br> <code>[node2]drbdadm secondary iscsi.config <br> [node2]drbdadm secondary iscsi.target.0 <br> <br> [node1]drbdadm primary iscsi.config <br> [node1]drbdadm primary iscsi.target.0</code> <br> <br>  We look tail ‚Äìf / var / log / syslog <br>  We wait‚Ä¶ <br>  Some time later‚Ä¶ <br><br> <code>Aug 26 08:32:14 node1 harc[11878]: info: Running /etc/ha.d//rc.d/ip-request-resp ip-request-resp <br> Aug 26 08:32:14 node1 ip-request-resp[11878]: received ip-request-resp IPaddr::10.11.55.50/8/eth0 OK yes <br> Aug 26 08:32:14 node1 ResourceManager[11899]: info: Acquiring resource group: node1 IPaddr::10.11.55.50/8/eth0 drbddisk::iscsi.target.0 iscsitarget <br> Aug 26 08:32:14 node1 IPaddr[11926]: INFO: Resource is stopped <br> Aug 26 08:32:14 node1 ResourceManager[11899]: info: Running /etc/ha.d/resource.d/IPaddr 10.11.55.50/8/eth0 start <br> Aug 26 08:32:14 node1 IPaddr[12006]: INFO: Using calculated netmask for 10.11.55.50: 255.0.0.0 <br> Aug 26 08:32:14 node1 IPaddr[12006]: INFO: eval ifconfig eth0:0 10.11.55.50 netmask 255.0.0.0 broadcast 10.255.255.255 <br> Aug 26 08:32:14 node1 avahi-daemon[477]: Registering new address record for 10.11.55.50 on eth0.IPv4. <br> Aug 26 08:32:14 node1 IPaddr[11982]: INFO: Success <br> Aug 26 08:32:15 node1 ResourceManager[11899]: info: Running /etc/init.d/iscsitarget start <br> Aug 26 08:32:15 node1 kernel: [ 5402.722552] iSCSI Enterprise Target Software - version 1.4.20.2 <br> Aug 26 08:32:15 node1 kernel: [ 5402.723978] iscsi_trgt: Registered io type fileio <br> Aug 26 08:32:15 node1 kernel: [ 5402.724057] iscsi_trgt: Registered io type blockio <br> Aug 26 08:32:15 node1 kernel: [ 5402.724061] iscsi_trgt: Registered io type nullio <br> Aug 26 08:32:15 node1 heartbeat: [12129]: debug: notify_world: setting SIGCHLD Handler to SIG_DFL <br> Aug 26 08:32:15 node1 harc[12129]: info: Running /etc/ha.d//rc.d/ip-request-resp ip-request-resp <br> Aug 26 08:32:15 node1 ip-request-resp[12129]: received ip-request-resp IPaddr::10.11.55.50/8/eth0 OK yes <br> Aug 26 08:32:15 node1 ResourceManager[12155]: info: Acquiring resource group: node1 IPaddr::10.11.55.50/8/eth0 drbddisk::iscsi.target.0 iscsitarget <br> Aug 26 08:32:15 node1 IPaddr[12186]: INFO: Running OK <br> Aug 26 08:33:08 node1 ntpd[1634]: Listen normally on 11 eth0:0 10.11.55.50 UDP 123 <br> Aug 26 08:33:08 node1 ntpd[1634]: new interface(s) found: waking up resolver</code> <br> <br> <code>ifconfig <br> eth0 Link encap:Ethernet HWaddr 00:50:56:20:f9:6c <br> inet addr:10.11.55.55 Bcast:10.255.255.255 Mask:255.0.0.0 <br> inet6 addr: fe80::20c:29ff:fe20:f96c/64 Scope:Link <br> UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 <br> RX packets:3622 errors:0 dropped:0 overruns:0 frame:0 <br> TX packets:8081 errors:0 dropped:0 overruns:0 carrier:0 <br> collisions:0 txqueuelen:1000 <br> RX bytes:302472 (302.4 KB) TX bytes:6943622 (6.9 MB) <br> Interrupt:19 Base address:0x2000 <br> <br> <font color="red">eth0:0 Link encap:Ethernet HWaddr 00:50:56:20:f9:6c <br> inet addr:10.11.55.50 Bcast:10.255.255.255 Mask:255.0.0.0 <br> UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 <br> Interrupt:19 Base address:0x2000</font> <br> <br> eth1 Link encap:Ethernet HWaddr 00:50:56:20:f9:76 <br> inet addr:192.168.22.11 Bcast:192.168.22.255 Mask:255.255.255.0 <br> inet6 addr: fe80::20c:29ff:fe20:f976/64 Scope:Link <br> UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 <br> RX packets:1765 errors:0 dropped:0 overruns:0 frame:0 <br> TX packets:3064 errors:0 dropped:0 overruns:0 carrier:0 <br> collisions:0 txqueuelen:1000 <br> RX bytes:171179 (171.1 KB) TX bytes:492567 (492.5 KB) <br> Interrupt:19 Base address:0x2080</code> <br> <br>  We connect the resulting iSCSI-target to both ESX (i) hosts.  After both hosts have seen the storage system, we assemble the HA cluster.  Although there is no space left for creating virtual machines on the hosts themselves, this place is now represented as virtual storage.  If any of the nodes fail, the virtual machine on the second node will switch to Primary mode and will continue to work as an iSCSI target. <br><br>  Using hdparm, I measured the speed of the disk in a virtual machine installed on the target: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/storage1/b2e35253/d235ff26/7fa3660b/72e3169e.gif"></div><br><br>  Naturally, for a serious production-systems such storage system is not suitable.  But if there are no high-loaded virtual machines or it is necessary to test the possibility of building an HA cluster, then this method of providing shared storage has the right to life. <br><br>  After reading this material, many may say that it is ‚Äúwrong‚Äù, ‚Äúthere will be a performance drawdown‚Äù, ‚Äúthe possibility of failure of both nodes‚Äù, etc.  Yes!  Maybe it will be so, but then, for some reason, VMware has released its Storage Appliance? <br><br>  PS: By the way, who is too lazy to shovel everything manually, there is a Management Console for setting up a DRBD cluster: <a href="http://www.drbd.org/mc/screenshot-gallery/">http://www.drbd.org/mc/screenshot-gallery/</a> . <br><br>  <a href="https://habrahabr.ru/users/madbug/" class="user_link">madbug</a> <br>  Senior Systems Engineer <a href="http://www.depocomputers.ru/%3Futm_source%3Dhabrahabr%26utm_medium%3Dtopic_171011%26utm_campaign%3Dhabrahabr">DEPO Computers</a> </div><p>Source: <a href="https://habr.com/ru/post/130573/">https://habr.com/ru/post/130573/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../130566/index.html">Who created, who updated or write their embedded application on django</a></li>
<li><a href="../130567/index.html">MS SQL 2008, margin notes. A lot of random numbers</a></li>
<li><a href="../130569/index.html">How to build an entry-level home media system? Choosing a TV, player, audio</a></li>
<li><a href="../130570/index.html">GitLab: open source version of Github</a></li>
<li><a href="../130572/index.html">Point-rating technology assessment of knowledge</a></li>
<li><a href="../130574/index.html">TDD on the example of UrlBuilder</a></li>
<li><a href="../130577/index.html">CAP-theorem simple, accessible language</a></li>
<li><a href="../130578/index.html">FPGA. The first steps</a></li>
<li><a href="../130579/index.html">5th meeting of the DEFCON-Russia group</a></li>
<li><a href="../130580/index.html">Bare Metal Deployment - how to look at one of the most interesting innovations of System Center Virtual Machine Manager 2012 RC closer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>