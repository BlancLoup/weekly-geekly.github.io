<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The book "Deep learning. Immersion in the world of neural networks "</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habrozhiteli! Recently, we published the first Russian book about deep learning from Sergey Nikolenko, Arthur Kadurin and Ekaterina Arkhangelskaya...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The book "Deep learning. Immersion in the world of neural networks "</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habrahabr.ru/company/piter/blog/346358/"><img src="https://habrastorage.org/webt/tf/1x/nx/tf1xnxujxtd6ahnec8relkcdmrc.jpeg" align="left" alt="image"></a>  Hi, Habrozhiteli!  Recently, we published the first Russian book about deep learning from Sergey Nikolenko, Arthur Kadurin and Ekaterina Arkhangelskaya.  Maximum explanations, minimum code, serious material about machine learning and fascinating presentation.  Now we will consider the section ‚ÄúComputation Graph and Differentiation on It‚Äù in which the fundamental concept for the implementation of learning algorithms for neural networks is introduced. <br><br>  If we manage to present a complex function as a composition of simpler ones, then we can effectively calculate its derivative with respect to any variable, which is what is required for the gradient descent.  The most convenient representation in the form of a composition is the representation in the form of a calculation graph.  A computation graph is a graph whose nodes are functions (usually fairly simple, taken from a predetermined set), and the edges associate functions with their arguments. <br><a name="habracut"></a><br>  It is easier to see with your own eyes than to formally define;  look at pic  2.7, where three calculation graphs are shown for the same function: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/j8/iu/ky/j8iuky9rrodoib8xuk_6jtl_jb4.png" alt="image"></div><br>  In fig.  2.7, and the graph turned out to be quite straightforward, because we allowed the use of the unary function ‚Äúsquaring‚Äù as vertices. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/xk/u4/nw/xku4nwrecnkulawzztljr_9ecss.png" alt="image"></div><br>  And in fig.  2.7, b, the graph is a bit more cunning: we have replaced the obvious squaring with ordinary multiplication.  However, it didn‚Äôt greatly increase in size because of the calculation graph in fig.  2.7, b it is allowed to reuse the results of previous calculations several times and it is enough to simply submit the same x two times to the input of the multiplication function to calculate its square.  For comparison, we drew in Fig.  2.7, in the graph of the same function, but without reuse;  now it becomes a tree, but in it one has to repeat entire large subtrees, from which several paths to the root of the tree could go out at once. <br><br>  In neural networks, the functions from which neurons are derived are usually used as basic, elementary functions of a computation graph.  For example, the scalar product of vectors <img src="https://habrastorage.org/webt/jc/q1/s4/jcq1s448zcs-_ek00vjndvzcrbu.png" alt="image">  enough to build any, even the most complex neural network composed of neurons with the function <img src="https://habrastorage.org/webt/va/by/by/vabybyrexl7zbriyoijt-28et3w.png" alt="image">  One could, by the way, express the scalar product through addition and multiplication, but it is also not necessary to ‚Äúcrush‚Äù too: elementary can be any function for which we can easily calculate its own and its derivative with any argument.  In particular, any functions of neuron activation, which we will discuss in detail in section 3.3, are perfect. <br><br>  So, we realized that many mathematical functions, even with very complex behavior, can be represented as a calculation graph, where the nodes have elementary functions, from which, like bricks, we get a complex composition, which we want to calculate.  Actually, using such a graph, even with a not too rich set of elementary functions, it is possible to approximate any function arbitrarily accurately;  we'll talk about this a little later. <br><br>  Now back to our main subject - machine learning.  As we already know, the goal of machine learning is to choose a model (most often we mean the weight of a model specified in a parametric form) so that it best describes the data.  The ‚Äúbest of all‚Äù here, as a rule, refers to the optimization of a certain error function.  Usually it consists of the actual error on the training sample (likelihood function) and regularizers (a priori distribution), but now we just need to assume that there is some rather complicated function that is given to us from above, and we want to minimize it.  As we already know, one of the most simple and universal methods for optimizing complex functions is the gradient descent.  We also recently found out that one of the most simple and universal methods to define a complex function is the calculation graph. <br><br>  It turns out that the gradient descent and the calculation graph are literally made for each other!  As you were probably taught at school (the school program generally contains a lot of unexpectedly deep and interesting things), to calculate the derivative of the composition of functions (at school it was probably called a derivative of a complex function, as if the word ‚Äúcomplex‚Äù does not have enough values), it is enough to be able to calculate the derivatives of its components: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ke/rp/wb/kerpwbz3dfsnfvqhuidelbfbtey.png" alt="image"></div><br><br>  The rule is applied to each component separately, and the result is <br>  again quite expected: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/es/9l/dx/es9ldxsx5pgw5z4wu_efzjqhyui.png" alt="image"></div><br>  Such a matrix of partial derivatives is called the Jacobi matrix, and its determinant is <br>  Jacobian;  they will meet again and again.  Now we can calculate the derivatives and gradients of any composition of functions, including vector functions, and for this we need only to be able to calculate the derivatives of each component.  For a graph, all this de facto boils down to a simple, but very powerful idea: if we know the graph of computations and know how to take the derivative at each node, this is enough to take the derivative of the entire complex function that the graph sets! <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/co/ug/s8/cougs8ct54uw09n9sv_ezbj1mqw.png" alt="image"></div><br>  Let's first analyze this with an example: consider the same calculation graph that was shown in Fig.  2.7.  In fig.  2.8, and the elementary functions making the graph are shown;  we marked each node of the graph with a new letter, from a to f, and wrote out the partial derivatives of each node at each input. <br><br>  Now you can calculate the partial derivative <img src="https://habrastorage.org/webt/pt/tn/ha/pttnhagp55wlh2kt1d_lqypftee.png" alt="image">  as shown in fig.  2.8, b: we begin to calculate the derivatives from the sources of the graph and use the formula for differentiating the composition to calculate the next derivative.  For example: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ao/kh/3c/aokh3cijenp8itdiw7bpysusjnw.png" alt="image"></div><br>  But you can go in the opposite direction, as shown in Fig.  2.8 in.  In this case, we start from the source, where the private derivative is always <img src="https://habrastorage.org/webt/sa/do/gj/sadogjatsu4o582bra7-wxhe9im.png" alt="image">  and then we expand the nodes in the reverse order according to the formula for differentiating a complex function.  The formula will apply here in the same way;  for example, at the lowest node we get: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/hb/21/ze/hb21zexrhyka6hpdqr1xhqkrqum.png" alt="image"></div><br>  Thus, it is possible to apply the formula of differentiation of a composition on a graph or from sources to drains, obtaining partial derivatives of each node with the same variable <img src="https://habrastorage.org/webt/0a/5o/5d/0a5o5d6km7qwo9s44zbljgtpzbk.png" alt="image">  either from effluent to source, receiving partial derivatives of effluent at all intermediate nodes <img src="https://habrastorage.org/webt/sv/iu/o8/sviuo8xo0o79webwwmitiuh9vhw.png" alt="image">  Of course, in practice for machine learning we need the second option rather than the first: the error function is usually the same, and we need its partial derivatives for many variables at once, especially for all weights that we want to use for gradient descent. <br><br>  In general, the algorithm is as follows: Suppose we are given a certain directed acyclic calculation graph G = (V, E), whose vertices are functions <img src="https://habrastorage.org/webt/b6/cv/ac/b6cvac_qlmpmpkecuo7yytz8jc0.png" alt="image">  , moreover, part of the vertices corresponds to the input variables x1, ..., xn and has no incoming edges, one vertex has no outgoing edges and corresponds to the function f (the whole graph calculates this function), and the edges show the dependencies between the functions in the nodes.  Then we already know how to get the function f standing in the ‚Äúlast‚Äù vertex of the graph: to do this, it is enough to move along the edges and calculate each function in topological order. <br><br>  And to learn the partial derivatives of this function, it is enough to move in the opposite direction.  If we are interested in partial derivatives of functions <img src="https://habrastorage.org/webt/ow/m4/tq/owm4tqbgzcvztdmkbca53oqt7u4.png" alt="image">  then in full accordance with the formulas above, we can calculate <img src="https://habrastorage.org/webt/kp/dv/7n/kpdv7nlsmfs1-9em1ttql-h4zlq.png" alt="image">  for each node <img src="https://habrastorage.org/webt/jy/q5/nh/jyq5nhjoeitopplm2gwrjxivfkm.png" alt="image">  in this way: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/my/g3/j7/myg3j7y3dlz6xow53ein-2g2hb8.png" alt="image"></div><br>  This approach is called the backpropagation algorithm (backpropagation, backprop, bprop), because the partial derivatives are calculated in the opposite direction to the edges of the calculation graph.  And the algorithm for calculating the function itself or the derivative with respect to one variable, as in Fig.  2.8, b, is called forward propagation algorithm (fprop). <br><br>  And the last important note: note that for all the time while we were discussing graphs of calculations, differentials, gradients, and the like, we, actually, never seriously mentioned neural networks!  Indeed, the method of calculating derivatives / gradients according to the computation graph is in no way connected with neural networks.  This is useful to keep in mind, especially in practical matters, to which we will proceed in the next section.  The fact is that the Theano and TensorFlow libraries, which we will discuss below and where most of the deep learning is done, are, generally speaking, libraries for automatic differentiation, and not for neural network training.  All that they do is allow you to set the graph of calculations and damn efficiently, with parallelization and transfer to video cards, they calculate the gradient according to this graph. <br><br>  Of course, ‚Äúon top‚Äù of these libraries you can implement the library itself with the standard designs of neural networks, and this is also what people constantly do (we will consider Keras below), but it is important not to forget the basic idea of ‚Äã‚Äãautomatic differentiation.  It can be much more flexible and rich than just a set of standard neural structures, and it may happen that you will be extremely successful in using TensorFlow not at all to train neural networks. <br><br>  ¬ªMore information about the book can be found on <a href="https://www.piter.com/product/glubokoe-obuchenie">the publisher's website.</a> <br>  ¬ª <a href="http://storage.piter.com/upload/contents/978549602536/978549602536_X.pdf">Table of Contents</a> <br>  ¬ª <a href="http://storage.piter.com/upload/contents/978549602536/978549602536_p.pdf">Excerpt</a> <br><br>  For Habrozhiteley a discount of 20% for the coupon - <b>Deep learning</b> </div><p>Source: <a href="https://habr.com/ru/post/346358/">https://habr.com/ru/post/346358/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346346/index.html">Flask Mega-Tutorial, Part 5: Custom Logins (Edition 2018)</a></li>
<li><a href="../346348/index.html">Flask Mega-Tutorial, Part 6: Profile page and avatars (edition 2018)</a></li>
<li><a href="../346350/index.html">As a researcher hacked his own computer and became convinced of the reality of the most serious vulnerability in the history of processors</a></li>
<li><a href="../346352/index.html">Useful materials on SAN</a></li>
<li><a href="../346354/index.html">Touch or not: What are the cash registers of stores today?</a></li>
<li><a href="../346360/index.html">Attention! Users of free Legacy G Suite (ex. Google Apps) accounts under the threat of blocking</a></li>
<li><a href="../346362/index.html">Developers advise: a selection of popular books on programming, languages, algorithms</a></li>
<li><a href="../346364/index.html">JPoint 2017 is a conference that could. Review of the best reports in the public domain</a></li>
<li><a href="../346366/index.html">Continuation of the story about the models. Complicated cases</a></li>
<li><a href="../346368/index.html">3CX server integration with Bitrix24</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>