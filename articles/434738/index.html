<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learning with reinforcement in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello colleagues! 



 In the last publication of the outgoing year, we would like to mention Reinforcement Learning, a topic on which we are already ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learning with reinforcement in Python</h1><div class="post__text post__text-html js-mediator-article">  Hello colleagues! <br><br><img src="https://habrastorage.org/webt/8s/-m/om/8s-mommciij8mkqdkjm62glthy4.jpeg"><br><br>  In the last publication of the outgoing year, we would like to mention Reinforcement Learning, a topic on which we are already translating a <a href="https://www.amazon.com/Deep-Reinforcement-Learning-Hands-Q-networks/dp/1788834240/">book</a> . 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Judge for yourself: there was an elementary article with Medium, which sets out the context of the problem, describes the simplest algorithm with implementation in Python.  The article has several gifs.  And the motivation, reward and choice of the right strategy on the road to success are things that will be extremely useful in the coming year to each of us. <br><br>  Enjoy reading! <br><a name="habracut"></a><br>  Reinforcement training is a type of machine learning in which an agent learns to act in the environment, performing actions and thereby gaining intuition, and then observes the results of his actions.  In this article, I will tell you how to understand and formulate a reinforcement training task, and then solve it in Python. <br><br><br>  Recently, we have become accustomed to the fact that computers play games against humans - either as bots in multiplayer games, or as rivals in one-on-one games: for example, in Dota2, PUB-G, Mario.  The research company <a href="https://deepmind.com/">Deepmind</a> made a stir in the news when in 2016 their AlphaGo program in 2016 defeated South Korea's go champion.  If you are an avid gamer, you could hear about the Dota 2 OpenAI Five top five matches, where cars fought against people and overcame the best Dota2 players in several matches.  (If you are interested in the details, the algorithm is analyzed in detail <a href="https://blog.openai.com/openai-five/">here</a> and it is examined how the machines played). <br><br><img src="https://habrastorage.org/webt/da/l7/3j/dal73jd7dacspz0co83f6zuano0.png"><br><br>  The latest version of OpenAI Five <a href="https://blog.openai.com/openai-five-benchmark/">takes Roshan</a> . <br><br>  So let's start with the central question.  Why do we need reinforcement training?  Is it used only in games, or is it applicable in realistic scenarios for solving applied problems?  If for the first time you read about learning with reinforcement, then you simply cannot imagine the answer to these questions.  After all, reinforcement learning is one of the most widely used and rapidly developing technologies in the field of artificial intelligence. <br>  Here are a number of subject areas in which reinforced learning systems are particularly in demand: <br><br><ol><li>  Unmanned vehicles </li><li>  Game industry </li><li>  Robotics </li><li>  Recommender systems </li><li>  Advertising and marketing </li></ol><br>  <b>An overview and origin of training with reinforcements</b> <br><br>  So, how did the phenomenon of learning with reinforcement form when we have so many methods of machine and deep learning at our disposal?  "He was invented by Rich Sutton and Andrew Barto, the supervisor of Rich who helped him prepare the PhD."  The paradigm took shape for the first time in the 1980s and was then archaic.  Subsequently, Rich believed that she had a great future, and she would eventually receive recognition. <br><br>  Reinforcement learning supports automation in the environment where it is implemented.  Some also operate both machine and deep learning - they are strategically arranged differently, but both paradigms support automation.  So why did reinforcement training come about? <br><br>  It is very similar to the natural learning process, in which the process / model operates and receives feedback on how it manages to cope with the task: good and not. <br><br>  Machine and deep learning are also learning options, however, they are more focused on identifying patterns in the available data.  In reinforcement training, on the other hand, such experience is gained through trial and error;  the system gradually finds the right options or a global optimum.  A significant added benefit of learning with reinforcement is that in this case it is not necessary to provide an extensive set of training data, as in training with a teacher.  Enough will be a few small fragments. <br><br>  <b>The concept of learning with reinforcement</b> <br><br>  Imagine teaching your cats new tricks;  but, unfortunately, cats do not understand the human language, so you can‚Äôt take it and tell them what you are going to play with.  Therefore, you will act differently: imitate the situation, and the cat in response will try to react in one way or another.  If the cat responded the way you wanted, then you pour milk on it.  Do you understand what will happen next?  Once again in a similar situation, the cat will once again perform the desired action, and with even greater enthusiasm, hoping that it will feed even better.  This is how learning takes place on a positive example;  but, if you try to ‚Äúeducate‚Äù a cat with negative stimuli, for example, look at it strictly and frown, it usually does not train in such situations. <br><br>  Similarly, reinforcement training works.  We tell the machine some input and actions, and then reward the machine depending on the output.  Our ultimate goal is maximizing rewards.  Now let's look at how to reformulate the above problem in terms of reinforced learning. <br><br><ul><li>  The cat acts as an ‚Äúagent‚Äù exposed to the ‚Äúenvironment‚Äù. </li><li>  The environment is a home or play area, depending on what you are teaching the cat. </li><li>  Situations arising from training are called ‚Äústates‚Äù.  In the case of a cat, examples of states are when the cat "runs" or "crawls under the bed." </li><li>  Agents react by performing actions and moving from one ‚Äústate‚Äù to another. </li><li>  After changing the state, the agent receives a ‚Äúreward‚Äù or ‚Äúfine‚Äù depending on the action he has taken. </li><li>  ‚ÄúStrategy‚Äù is a method of choosing an action to get the best results. </li></ul><br>  Now, having understood what reinforcement learning is, let's talk in detail about the origins and evolution of reinforcement learning and deep learning with reinforcement, we will discuss how this paradigm allows us to solve problems that are very difficult to train with or without a teacher, and also note the following a curious fact: at present, Google search engine is optimized with the use of reinforcement learning algorithms. <br><br>  <b>Introduction to reinforcement learning terminology</b> <br><br>  Agent and Environment play key roles in the reinforcement learning algorithm.  The environment is the world in which the Agent has to survive.  In addition, the Agent receives back-up signals (reward) from the Environment: this is a number that characterizes how good or bad the current state of the world can be considered.  The goal of the Agent is to maximize the total remuneration, the so-called ‚Äúwinnings‚Äù.  Before writing our first algorithms for reinforcement learning, you need to understand the following terminology. <br><br><img src="https://habrastorage.org/webt/6j/vx/cp/6jvxcpcpr52v252wa9eze1mehx4.gif"><br><br><ol><li>  <b>States</b> : A state is a complete description of the world in which not a single piece of information characterizing this world has been omitted.  This can be a position, fixed or dynamic.  As a rule, such states are written in the form of arrays, matrices or tensors of higher order. </li><li>  <b>Action</b> : The action usually depends on environmental conditions, and the agent will take different actions in different environments.  The set of valid agent actions is recorded in a space called an ‚Äúaction space‚Äù.  As a rule, the number of actions in space, of course. </li><li>  <b>Wednesday</b> : This is the place where the agent exists and interacts with.  For different environments, different types of rewards, strategies, etc. are used. </li><li>  <b>Reward</b> and <b>win</b> : It is necessary to constantly monitor the reward function R when training with reinforcements.  It is critical when setting up an algorithm, optimizing it, and also when you stop learning.  It depends on the current state of the world, the action just taken and the next state of the world. </li><li>  <b>Strategies</b> : Strategy is the rule according to which the agent chooses the next action.  A set of strategies is also referred to as an agent ‚Äúbrain‚Äù. </li></ol><br><img src="https://habrastorage.org/webt/ur/lb/u-/urlbu-ifbred1iqfkhvqkv7bqds.png"><br><br>  Now, having become familiar with the terminology of learning with reinforcement, let's solve the problem using the appropriate algorithms.  Before this, it is necessary to understand how to formulate such a task, and in solving this problem, rely on the terminology of training with reinforcement. <br><br>  <b>Taxi solution</b> <br><br>  So, we proceed to solving the problem with the use of supporting algorithms. <br>  Suppose we have a training zone for unmanned taxi, which we train to deliver passengers to the parking at four different points ( <code>R,G,Y,B</code> ).  Before this, you need to understand and set the environment in which we start programming in Python.  If you are just starting to learn Python, I recommend <a href="https://towardsdatascience.com/python-programming-in-15-min-part-1-3ad2d773834c">this article</a> . <br><br>  The environment for solving the problem with a taxi can be configured using the <a href="https://openai.com/">Gym</a> from the company OpenAI - this is one of the most popular libraries for solving problems with reinforcement training.  Well, before using the gym, you need to install it on your machine, and for this, the Python package manager called pip is convenient.  Below is the installation command. <br><br> <code>pip install gym</code> <br> <br>  Next, let's see how our environment will be displayed.  All models and the interface for this task are already configured in the gym and named for <code>Taxi-V2</code> .  The following code snippet is used to display this environment. <br><br>  ‚ÄúWe have 4 locations (indicated by different letters);  our task is to catch the passenger at one point and drop him off at another.  We get +20 points for a successful disembarkation of a passenger and lose 1 point for each step spent on it.  A penalty of 10 points is also provided for each <a href="https://gym.openai.com/envs/Taxi-v2/">unintentional embarkation</a> and disembarkation of a passenger. ‚Äù(Source: <a href="https://gym.openai.com/envs/Taxi-v2/">gym.openai.com/envs/Taxi-v2</a> ) <br><br>  Here is the conclusion we will see in our console: <br><br><img src="https://habrastorage.org/webt/n8/xj/lw/n8xjlwkbla6unwn2k6tworczc8o.png"><br><br>  Taxi V2 ENV <br><br>  Great, <code>env</code> is the heart of the OpenAi Gym, a unified environment interface.  The following are env methods that will be very useful to us: <br><br>  <code>env.reset</code> : resets the environment and returns a random initial state. <br>  <code>env.step(action)</code> : Promotes the development of the environment one step in time. <br>  <code>env.step(action)</code> : returns the following variables <br><br><ul><li>  <code>observation</code> : Observation of the environment. </li><li>  <code>reward</code> : <code>reward</code> whether your action was helpful </li><li>  <code>done</code> : Indicates whether we were able to correctly pick up and disembark the passenger, also referred to as ‚Äúone episode‚Äù. </li><li>  <code>info</code> : Additional information, such as performance and latency, for debugging purposes. </li><li>  <code>env.render</code> : Displays a single frame of the medium (useful for rendering) </li></ul><br>  So, having considered the environment, let's try to understand the task more deeply.  Taxi - the only car in this parking.  Parking can be divided into a <code>5x5</code> grid, where we get 25 possible taxi positions.  These 25 values ‚Äã‚Äãare one of the elements of our state space.  Please note: at the moment our taxi is located at coordinates (3, 1). <br><br>  There are 4 points in the environment where passengers can disembark: it‚Äôs: <code>R, G, Y, B</code> or <code>[(0,0), (0,4), (4,0), (4,3)]</code> in coordinates ( horizontally; vertically) if it were possible to interpret the above medium in Cartesian coordinates.  If we also take into account one more (1) passenger state: inside the taxi, then we can take all combinations of the locations of passengers and their destinations to calculate the total number of states in our environment for taxi training: we have four (4) destinations and five (4+ 1) passenger locations. <br><br>  So, in our environment for a taxi there are 5 √ó 5 √ó 5 √ó 4 = 500 possible states.  The agent deals with one of 500 states and takes action.  In our case, the options are as follows: moving in one direction or another, or the decision to pick up / drop off the passenger.  In other words, we have six possible actions at our disposal: <br>  pickup, drop, north, east, south, west (The last four values ‚Äã‚Äãare directions in which a taxi can move.) <br><br>  This is an <code>action space</code> : a collection of all the actions that our agent can take in a given state. <br><br>  As is clear from the illustration above, taxis cannot perform certain actions in certain situations (walls interfere).  In the code describing the environment, we simply assign a penalty of -1 for each hit in the wall, and a taxi, when faced with a wall.  Thus, such penalties will accumulate, so the taxi will try not to crash into the walls. <br><br>  Table of rewards: When creating a ‚Äútaxi‚Äù environment, a primary table of rewards is also created called P. It can be considered a matrix, where the number of states corresponds to the number of rows, and the number of actions to the number of columns.  That is, it‚Äôs about the <code>states √ó actions</code> matrix. <br><br>  Since absolutely all states are recorded in this matrix, you can view the default values ‚Äã‚Äãof the awards assigned to the state that we chose to illustrate: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym &gt;&gt;&gt; env = gym.make(<span class="hljs-string"><span class="hljs-string">"Taxi-v2"</span></span>).env &gt;&gt;&gt; env.P[<span class="hljs-number"><span class="hljs-number">328</span></span>] {<span class="hljs-number"><span class="hljs-number">0</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">433</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">1</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">233</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">2</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">353</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">3</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">4</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)], <span class="hljs-number"><span class="hljs-number">5</span></span>: [(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">333</span></span>, <span class="hljs-number"><span class="hljs-number">-10</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)] }</code> </pre> <br>  The structure of this dictionary is: <code>{action: [(probability, nextstate, reward, done)]}</code> . <br><br><ul><li>  Values ‚Äã‚Äã0‚Äì5 correspond to the actions (south, north, east, west, pickup, dropoff) that a taxi can perform in the current state shown in the illustration. </li><li>  done allows you to judge when we successfully disembarked the passenger at the desired point. </li></ul><br>  To solve this problem without any reinforcement training, you can set a target state, sample the spaces, and then, if you can reach the target state in a certain number of iterations, assume that this moment corresponds to the maximum reward.  In other states, the value of remuneration is either nearing the maximum if the program acts correctly (approaches the goal) or accumulates penalties if it makes mistakes.  Moreover, the value of the penalty can reach no lower than -10. <br><br>  Let's write the code to solve this problem without learning with reinforcement. <br>  Since we have a P-table with default remuneration values ‚Äã‚Äãfor each state, we can try to navigate our taxi simply on the basis of this table. <br><br>  Create an infinite loop that goes through until the passenger gets to the destination (one episode), or, in other words, until the reward rate reaches 20. The <code>env.action_space.sample()</code> method automatically selects a random action from the set of all available actions. .  Consider what happens: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> time <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sleep <span class="hljs-comment"><span class="hljs-comment">#  thr env env = gym.make("Taxi-v2").env env.s = 328 #     ,   , epochs = 0 penalties, reward = 0, 0 frames = [] done = False while not done: action = env.action_space.sample() state, reward, done, info = env.step(action) if reward == -10: penalties += 1 #         frames.append({ 'frame': env.render(mode='ansi'), 'state': state, 'action': action, 'reward': reward } ) epochs += 1 print("Timesteps taken: {}".format(epochs)) print("Penalties incurred: {}".format(penalties)) #    , ,  def frames(frames): for i, frame in enumerate(frames): clear_output(wait=True) print(frame['frame'].getvalue()) print(f"Timestep: {i + 1}") print(f"State: {frame['state']}") print(f"Action: {frame['action']}") print(f"Reward: {frame['reward']}") sleep(.1) frames(frames)</span></span></code> </pre><br>  Conclusion: <br><br><img src="https://habrastorage.org/webt/b3/kd/z_/b3kdz_kejhninocffgc2_3ytr_u.gif"><br><br>  credits: OpenAI <br><br>  The problem is solved, but not optimized, or this algorithm will not work in all cases.  We need a suitable interacting agent so that the number of iterations spent by the machine / algorithm on solving the problem remains minimal.  Here we are helped by the Q-learning algorithm, the implementation of which we will consider in the next section. <br><br>  <b>Introduction to Q-learning</b> <br><br>  Below is the most popular and one of the most simple algorithms for learning with reinforcement.  The medium rewards the agent for gradual learning and for making the most optimal step in a particular state.  In the implementation discussed above, we had a table of rewards ‚ÄúP‚Äù, according to which our agent will study.  Based on the reward table, he chooses the next action depending on how useful it is, and then updates another value, called a Q-value.  As a result, a new table is created, called a Q-table, displayed on a combination (Status, Action).  If the Q-values ‚Äã‚Äãare better, then we get more optimized rewards. <br><br>  For example, if a taxi is in a state where the passenger is at the same point as the taxi, it is extremely likely that the Q-value for the ‚Äúpick up‚Äù action is higher than for other actions, for example, ‚Äúdisembark the passenger‚Äù or ‚Äúgo north ". <br>  Q-values ‚Äã‚Äãare initialized with random values, and as the agent interacts with the environment and receives various rewards by performing certain actions, the Q-values ‚Äã‚Äãare updated according to the following equation: <br><br><img src="https://habrastorage.org/webt/ed/fv/br/edfvbr7xz2terdw8meeftimstx0.png"><br><br>  This raises the question: how to initialize Q-values ‚Äã‚Äãand how to calculate them.  As actions are performed, Q values ‚Äã‚Äãare performed in this equation. <br><br>  Here Alpha and Gamma are the parameters of the algorithm for Q-learning.  Alpha is the pace of learning, and gamma is the discounting factor.  Both values ‚Äã‚Äãcan range from 0 to 1 and sometimes equal to one.  Gamma can be equal to zero, and alpha - can not, because the value of losses during the update must be compensated (the pace of learning is positive).  The alpha value here is the same as when training with a teacher.  Gamma determines how important we want to give the rewards that await us in the future. <br><br>  This algorithm is summarized below: <br><br><ul><li>  Step 1: we initialize the Q-table, filling it with zeros, and for Q-values ‚Äã‚Äãwe set arbitrary constants. </li><li>  Step 2: Now let the agent react to the environment and try different actions.  For each state change, select one of all actions possible in a given state (S). </li><li>  Step 3: We proceed to the next state (S ') according to the results of the previous action (a). </li><li>  Step 4: For all possible actions from state (S '), choose the one with the highest Q-value. </li><li>  Step 5: Update the values ‚Äã‚Äãof the Q-table according to the above equation. </li><li>  Step 6: Turn the next state into the current one. </li><li>  Step 7: If the target state is reached, end the process and then repeat. </li></ul><br>  <b>Q-learning in Python</b> <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> random <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> IPython.display <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> clear_output <span class="hljs-comment"><span class="hljs-comment">#  Taxi-V2 Env env = gym.make("Taxi-v2").env #    q_table = np.zeros([env.observation_space.n, env.action_space.n]) #  alpha = 0.1 gamma = 0.6 epsilon = 0.1 all_epochs = [] all_penalties = [] for i in range(1, 100001): state = env.reset() #   epochs, penalties, reward, = 0, 0, 0 done = False while not done: if random.uniform(0, 1) &lt; epsilon: #    action = env.action_space.sample() else: #    action = np.argmax(q_table[state]) next_state, reward, done, info = env.step(action) old_value = q_table[state, action] next_max = np.max(q_table[next_state]) #    new_value = (1 - alpha) * old_value + alpha * \ (reward + gamma * next_max) q_table[state, action] = new_value if reward == -10: penalties += 1 state = next_state epochs += 1 if i % 100 == 0: clear_output(wait=True) print("Episode: {i}") print("Training finished.")</span></span></code> </pre><br>  Great, now all your values ‚Äã‚Äãwill be stored in the variable <code>q_table</code> . <br><br>  So, your model is trained in environmental conditions, and now knows how to more accurately select passengers.  And you got acquainted with the phenomenon of learning with reinforcement, and you can program the algorithm to solve the new problem. <br><br>  Other reinforcement learning techniques: <br><br><ul><li>  Markov decision processes (MDP) and Bellman equations </li><li>  Dynamic programming: model-based RL, iteration over strategies, and iteration over values </li><li>  Deep Q-learning </li><li>  Gradient Descent Strategies </li><li>  SARSA </li></ul><br>  The code for this exercise is located at: <br><br>  vihar / python-reinforcement-learning </div><p>Source: <a href="https://habr.com/ru/post/434738/">https://habr.com/ru/post/434738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../434728/index.html">People and processes: why is it not suitable for every company?</a></li>
<li><a href="../434730/index.html">In-memory databases: application, scaling and important additions</a></li>
<li><a href="../434732/index.html">Life at 6200 DPI. HyperX Pulsefire Core Review</a></li>
<li><a href="../434734/index.html">Fourier transform. The fast and the furious</a></li>
<li><a href="../434736/index.html">Using the Mikrotik log database to suppress brute force</a></li>
<li><a href="../434740/index.html">The neural network was taught to detect solar panels on satellite images and predict their level of distribution.</a></li>
<li><a href="../434742/index.html">Part 2: Using Cypress UDB PSoC Controllers to Reduce the Number of Interrupts in a 3D Printer</a></li>
<li><a href="../434744/index.html">Samsung SSD 860 QVO 1 TB and 4 TB: the first consumer SATA QLC (part 2)</a></li>
<li><a href="../434746/index.html">BLE under the microscope 4</a></li>
<li><a href="../434748/index.html">Mail.ru plans to buy music licensing partner for $ 100 million</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>