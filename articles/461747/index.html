<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we implemented ML in an application with nearly 50 million users. Sberbank Experience</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, Habr! My name is Nikolai, and I am engaged in the construction and implementation of machine learning models in Sberbank. Today I‚Äôll talk about...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we implemented ML in an application with nearly 50 million users. Sberbank Experience</h1><div class="post__text post__text-html js-mediator-article">  Hello, Habr!  My name is Nikolai, and I am engaged in the construction and implementation of machine learning models in Sberbank.  Today I‚Äôll talk about developing a recommendation system for payments and transfers in the application on your smartphones. <br><br> <a href="https://habr.com/ru/company/sberbank/blog/461747/"><img src="https://habrastorage.org/getpro/habr/post_images/358/23c/0f8/35823c0f8e513f268f58de291b8504c7.png"></a> <br>  <i>Design of the main screen of the mobile application with recommendations</i> <br><br>  We had 2 hundred thousand possible payment options, 55 million customers, 5 different banking sources, half a developer column and a mountain of banking activity, algorithms and all that, all colors, as well as a liter of random seeds, a box of hyperparameters, half a liter of correction factors and two dozens of libraries.  Not that all this was necessary in the work, but since he began to improve the life of clients, then go in your hobby to the end.  Under the cut is the story of the battle for UX, the correct formulation of the problem, the fight against the dimensionality of data, the contribution to open-source and our results. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><h2>  Formulation of the problem </h2><br>  As the development and scaling up, the Sberbank Online application is gaining useful features and additional functionality.  In particular, in the application you can transfer money or pay for services of various organizations. <br><br>  ‚ÄúWe carefully looked at all the user paths inside the application and realized that many of them can be significantly reduced.  To do this, we decided to personalize the main screen in several stages.  First, we tried to remove from the screen what the client does not use, starting with bank cards.  Then they brought to the fore those actions that the client had already performed before and for which he could go into the application right now.  Now the list of actions includes payments to organizations and transfers to contacts, then the list of such actions will be expanded, ‚Äùsaid my colleague Sergey Komarov, who is developing the functionality from the point of view of the client in the Sberbank Online team.  It is necessary to build a model that would fill the designated slots in the "Actions" widgets (figure above) with personal recommendations of payments and transfers instead of simple rules. <br><br><h2>  Decision </h2><br>  We in the team decomposed the task into two parts: <br><br><ul><li>  the recommendation of the repetition of operations to pay for services or transfer funds (block ‚ÄúRecommended operations‚Äù) <br></li><li>  recommendation of examples of search requests for payment for services not previously used by this client (block ‚ÄúSearch Examples‚Äù) <br></li></ul><br>  We decided to test the functionality first in the search tab: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8fe/450/0ee/8fe4500ee0d612b4210a7237a643850b.png"></div><br>  <i>Recommended Search Screen Design</i> <br><br><h2>  Recommended Operations </h2><br><h3>  Scoring optimization </h3><br>  If we set the subtask as a recommendation to repeat operations, this allows us to get rid of the calculation and evaluation of trillions of combinations of all possible operations for all customers and focus on a much more limited number of them.  If out of the whole set of operations available to our clients, the hypothetical client with the YYY hash used only gas and parking payments, then we will evaluate the probability of repeating only these operations for this client: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/33f/d0e/415/33fd0e4159a1143ff54c6834c0cd964d.png"><br>  <i>An example of reducing the dimension of data for scoring</i> <br><br><h3>  Preparing the dataset </h3><br>  The sample is a transactional observation, enriched with factors of client demography, financial aggregates and various frequency characteristics of a particular operation. <br><br>  The target variable in this case is binary and reflects the fact of the event on the day following the day the factors are calculated.  Thus, iteratively moving the day of calculating the factors and setting the flag of the target variable, we multiply and mark the same operations and mark them differently depending on the position relative to this day. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/929/618/c8a/929618c8a1fbcf3b2385300bf9669ae3.png"></div><br>  <i>Observation Scheme</i> <br><br>  Calculating the slice on 03/17/2019 for the client ‚ÄúYYY‚Äù, we get two observations: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/78b/e70/741/78be707412f6a3fee92dcb5d62413de9.png"></div><br>  <i>An example of the observations for a dataset</i> <br><br>  ‚ÄúFeature 1‚Äù can mean, for example, the balance on all cards of the client, ‚ÄúFeature 2‚Äù - the presence of this type of operation in the last week. <br><br>  We take the same transactions, but form observations for training on a different date: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/901/c8e/618/901c8e6188d792bb9c77fc207f7d95d1.png"></div><br>  <i>Observation Scheme</i> <br><br>  We get observations for a dataset with other values ‚Äã‚Äãof both features and the target variable: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e99/a4f/060/e99a4f0602c8c127f47aecee38f02292.png"><br>  <i>An example of the observations for a dataset</i> <br><br>  In the examples above, for clarity, the actual values ‚Äã‚Äãof the factors are given, but in fact, the values ‚Äã‚Äãare processed by an automatic algorithm: the results of the <a href="https://support.sas.com/resources/papers/proceedings13/095-2013.pdf">WOE conversion</a> are fed to the input of the model.  It allows you to bring the variables to a monotonic relationship with the target variable and at the same time get rid of the effects of outliers.  For example, we have the factor ‚ÄúNumber of cards‚Äù and some distribution of the target variable: <br><br><img src="https://habrastorage.org/webt/4z/vz/xt/4zvzxtz04cy0corijvcyz4zfhf0.png"><br><br>  <i>WOE conversion example</i> <br><br>  The WOE transformation allows us to transform a nonlinear dependence into at least a monotonic one.  Each value of the analyzed factor is associated with its own WOE value and thus a new factor is formed, and the original one is removed from the dataset: <br><br><img src="https://habrastorage.org/webt/9j/os/o4/9joso4mjqdpris64sxtrb3ji34k.png"><br>  <i>The effect of the WOE transform on the relationship with the target variable</i> <br><br>  The dictionary for converting variable values ‚Äã‚Äãto WOE is saved and used later for scoring.  That is, if we need to calculate the probabilities for tomorrow, we create a dataset as in the tables with examples of observations above, convert the necessary variables into WOE with the saved code, and apply the model on these data. <br><br><h3>  Training </h3><br>  The choice of method was strictly limited - interpretability.  Therefore, to comply with the deadlines, it was decided to postpone the explanations using the same <a href="https://github.com/slundberg/shap">SHAP</a> in the second half of the problem and test relatively simple methods: regression and shallow neurons.  The tool was SAS Miner, a software for preprocessing, analyzing, and building models on various data in an interactive form, which saves a lot of time on writing code. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1ae/80d/b3c/1ae80db3c2741d5c70a2cbc9e48dd0ff.png"><br>  <i>SAS Miner Interface</i> <br><br><h3>  Quality control </h3><br>  Comparison of the GINI metric on an out-of-time sample showed that the neural network copes with the task best: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d24/553/e79/d24553e79a193415b2fa94c6e3b0d987.png"></div><br>  <i>Comparative table of quality models and frequency rules</i> <br><br>  The model has two exit points.  The recommendations in the form of widget cards on the main screen include operations whose forecast is above a certain threshold (see the first picture in the post).  The border is selected based on a balance of quality and coverage, which in such an architecture is half of all operations performed.  Top-4 operations are sent to the ‚Äúrecommended operations‚Äù block of the search screen (see the second picture). <br><br><h2>  Search Examples </h2><br>  Turning to the second part of the task, we return to the problem of a huge number of possible payment options for the services of providers that need to be evaluated and sorted within each client ‚Äî trillions of pairs.  In addition to this, we have implicit data, that is, there is no information about the assessment of payments made, or why the client did not make any payments.  Therefore, to begin with, it was decided to test various methods of expanding the matrix of payments from customers to providers: ALS and FM. <br><br><h3>  ALS </h3><br>  ALS (Alternating Least Squares) or Alternating Least Squares - in collaborative filtering, one of the methods for solving the problem of factorization of the interaction matrix.  We will present our transactional data on payment of services in the form of a matrix in which columns are unique identifiers of all providers' services, and rows are unique customers.  In the cells we place the number of operations of specific clients with specific providers for a certain period of time: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/430/fcd/db7/430fcddb7494ed4ba54411d0423b5b16.png"><br>  <i>Matrix decomposition principle</i> <br><br>  The meaning of the method is that we create two such matrices of lower dimension, the multiplication of which gives the closest result to the original large matrix in the filled cells.  The model learns to create a hidden factorial description for customers and for providers.  An implementation of the method in the <a href="https://github.com/benfred/implicit">implicit</a> library was used.  Training takes place according to the following algorithm: <br><br><ol><li>  The matrix of clients and providers with hidden factors is initialized.  Their number is the hyperparameter of the model. <br></li><li>  The matrix of hidden factors of providers is fixed and the derivative of the loss function for the correction of the client matrix is ‚Äã‚Äãconsidered.  The author used an interesting <a href="https://www.benfrederickson.com/fast-implicit-matrix-factorization/">method of conjugate gradients</a> , which allows you to greatly accelerate this step. <br></li><li>  The previous step is repeated similarly for the matrix of hidden factors of customers. </li><li>  Steps 2-3 alternate until the algorithm converges. </li></ol><br><h3>  Training </h3><br>  Transactional data was transformed into a matrix of interactions with a degree of sparseness of ~ 99% with great unevenness among providers.  To separate the data into train and validation samples, we randomly masked the proportion of filled cells: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d60/d96/282/d60d96282aac1b7425ee0ea597253d66.png"><br>  <i>Data Sharing Example</i> <br><br>  Transactions were taken as a test for the time interval following the training, and laid out in a matrix of the same format - it turned out-of-time. <br><br><h3>  Training </h3><br>  The model has several hyperparameters that can be adjusted to improve quality: <br><br><ul><li>  Alpha is the coefficient by which the matrix is ‚Äã‚Äãweighted, adjusting the degree of confidence ( <a href="http://yifanhu.net/PUB/cf.pdf">C_iu</a> ) that the given service of the provider really ‚Äúlikes‚Äù the client. <br></li><li>  The number of factors in the hidden matrices of clients and providers is the number of columns and rows, respectively. <br></li><li>  Regularization coefficient L2 Œª. <br></li><li>  The number of iterations of the method. <br></li></ul><br>  We used the <a href="https://github.com/hyperopt/hyperopt">hyperopt</a> library, which allows us to evaluate the effect of hyperparameters on the quality metric using the <a href="http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html">TPE</a> method and select their optimal value.  The algorithm starts with a cold start and makes several estimates of the quality metric depending on the values ‚Äã‚Äãof the analyzed hyperparameters.  Then, in essence, he tries to select a set of hyperparameter values ‚Äã‚Äãthat is more likely to give a good value for the quality metric.  The results are saved in a dictionary from which you can build a graph and visually evaluate the result of the optimizer (blue is better): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa2/175/64a/fa217564a3f448a81abbbfad1d27f7a1.png"></div><br>  <i>The graph of the dependence of the quality metric on the combination of hyperparameters</i> <br><br>  The graph shows that the values ‚Äã‚Äãof the hyperparameters strongly affect the quality of the model.  Since it is necessary to apply ranges for each of them to the input of the method, the graph can further determine whether it makes sense to expand the value space or not.  For example, in our task it is clear that it makes sense to test large values ‚Äã‚Äãfor the number of factors.  In the future, this really improved the model. <br><br><h3>  Quality assessment metric and complexity </h3><br>  How to evaluate the quality of the model?  One of the most commonly used metrics for recommender systems where order is important is <a href="http://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html">MAP @ k</a> or Mean Average Precision at K. This metric estimates the accuracy of the model on K recommendations taking into account the order of the items in the list of these recommendations on average for all clients. <br><br>  Unfortunately, a quality assessment operation even on a sample took several hours.  Having rolled up our sleeves, we started profiling the mean_average_pecision_at_k () function with the line_profiler library.  The task was further complicated by the fact that the function used cython code and had to be correctly <a href="https://cython.readthedocs.io/en/latest/src/tutorial/profiling_tutorial.html">taken into account</a> , otherwise the necessary statistics were simply not collected.  As a result, we again faced the problem of the dimensionality of our data.  To calculate this metric, you need to get some estimates of each service from all possible for each client and select the top-K personal recommendations by sorting from the resulting array.  Even considering the use of partial sorting of numpy.argpartition () with O (n) complexity, sorting the grades turned out to be the longest step, stretching the quality score to the clock.  Since numpy.argpartition () did not use all the kernels of our server, it was decided to improve the algorithm by rewriting this part in C ++ and OpenMP via cython.  A briefly new algorithm is as follows: <br><br><ol><li>  Data is cut into batches by customers. <br></li><li>  An empty matrix and pointers to memory are initialized. <br></li><li>  Batches strings by pointers are sorted in two ways: by the partial_sort function and then sort by the C ++ algorithm library. <br></li><li>  The results are written to the cells of the empty matrix in parallel. <br></li><li>  Data is returned in python. <br></li></ol><br>  This allowed us to speed up the calculation of recommendations several times.  The revision has been <a href="https://github.com/benfred/implicit/pull/179">added</a> to the official repository. <br><br><h2>  OOT Results Analysis </h2><br>  And now it's time to evaluate the quality of the model.  Why do we need out-of-time sampling?  If we look at the distribution of operations by providers, we will see the following picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/345/5d2/3d4/3455d23d4093172fbbf416d9719e14fa.png"></div><br>  <i>Distribution of popularity of service providers</i> <br><br>  There is an imbalance.  This leads to the fact that the model is trying to recommend popular services.  Back to the picture above: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6ef/0fd/b08/6ef0fdb084bd7495ad12821c42cd70c2.png"><br><br>  The problem is that if you check the accuracy of the model by masking the same matrix, as is advised almost everywhere, then for most clients (marginal examples: ‚ÄúW‚Äù, ‚ÄúE‚Äù and ‚ÄúI‚Äù) the quality of the validation forecasts (we will pretend that she did not participate in the selection of hyperparameters) will be high if these are the most popular providers.  As a result, we get a false confidence in the strength of the model.  Therefore, we acted as follows: <br><br><ol><li>  Formed estimates of providers by model. <br></li><li>  The existing client-service pairs were excluded from the ratings (see the figure below) and OOT-matrices. <br></li><li>  Formed from the remaining ratings of the top-K recommendations and rated MAP @ k on the remaining OOT. <br></li></ol><br><img src="https://habrastorage.org/getpro/habr/post_images/ee7/c06/f4b/ee7c06f4b68beda0b4f0e22e3c8163c1.png"><br>  <i>The logic of preparing the matrix for generating forecasts</i> <br><br>  As a base line, we compiled a list of providers, sorted by popularity, and multiplied it by all customers, again excluding the existing client-service pairs.  It turned out to be sad and not at all what we expected and saw on the train \ validation samples: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ee2/a94/050/ee2a94050abe17d305bdcffeb895f2d0.png"></div><br>  <i>Comparison chart of model quality and baseline</i> <br><br>  Stop!  We have client factors and parameters of providers.  We get factorization machines. <br><br><h3>  FM </h3><br>  Factorization machines (factorization machine) - a learning algorithm with a teacher, designed to find relationships between factors that describe interacting entities, which are presented in the form of sparse matrices.  We used the implementation of FM from the <a href="https://github.com/lyst/lightfm">LightFM</a> library. <br><br><h3>  Data format </h3><br>  In addition to the normalized interaction matrix, the <a href="https://arxiv.org/pdf/1507.08439.pdf">method</a> uses two additional datasets with factors for customers and for the services of providers in the form of one-hot-encoded matrices connected to single ones: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/61b/fa6/c8a/61bfa6c8ab458cfb9b747a37a28e0e32.png"><br>  <i>The logic of preparing the matrix for generating forecasts</i> <br><br><h3>  Quality control </h3><br>  The quality of the FM model on our data turned out to be lower than ALS: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/48b/457/003/48b457003013f435b77ba2d9ecc4cb25.png"></div><br>  <i>Comparative table of quality models and baseline</i> <br><br><h3>  Change Model Architecture - Boosting </h3><br>  It was decided to come in from the other side.  Remembering the distribution of the popularity of services, we identified 300 of them, transactions for which cover 80% of all operations, and trained a classifier on them.  Here, the data represents aggregates of customer transactions enriched with client features: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d25/47d/718/d2547d718a7199fba96be2d0f653bf9a.png"><br>  <i>Transaction aggregation scheme</i> <br><br>  Why only client-side, you ask?  Because in this case, to prepare recommendations, it will be enough for us to have one line per client.  Applying the model to it, we get the output vector of probabilities for all classes, from which it is easy to choose top-K recommendations.  If we add the features of provider services to the training set, then at the stage of applying the model we will have to either prepare 300 lines for each client - one for each provider service with features that describe them, or build another model for pre-sorting scoring candidates . <br><br>  Adding features to clients from ALS did not increase our data, since we already took into account transactional activity - for example, in sections of the MCC or categories in the style of "gamer" or "theater".  In this format, we managed to get good results: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/eed/66a/389/eed66a389e4b545fa18a59c6c4ac123a.png"></div><br>  <i>Comparative table of quality models and baseline</i> <br><br><h3>  Regional filter </h3><br>  Despite the high quality of the model, one more problem remains in this approach.  Since the architecture of the data and model does not imply the use of features of the services of providers, the model does not fully take into account geography and may recommend that people pay for the service of a local provider from another region.  To minimize this risk, we have developed a small filter to pre-cut options before entering recommendations.  An easy fleur of recursion is thrown on the algorithm: <br><br><ol><li>  We collect information about the client‚Äôs region from bank profiles and other internal sources. <br></li><li>  We select the main regions of presence for each provider. <br></li><li>  We clarify / fill in the information about the client‚Äôs region by the regions of the providers that he uses. <br></li></ol><br>  After these manipulations using <a href="https://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BD%25D0%25B4%25D0%25B5%25D0%25BA%25D1%2581_%25D0%25A5%25D0%25B5%25D1%2580%25D1%2584%25D0%25B8%25D0%25BD%25D0%25B4%25D0%25B0%25D0%25BB%25D1%258F">the Herfindahl index, we</a> separate the regional providers, which are represented in a limited set of regions, from the federal ones: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/620/15f/090/62015f090d3bdcc34d468f7b36ffef90.png"></div><br>  <i>Separation of providers by presence in the regions</i> <br><br>  We form a mask with acceptable regional providers for customers and exclude unnecessary items from model predictions before creating a list of recommendations. <br><br><h2>  Conclusion </h2><br>  We have developed two models that together form a complete set of recommendations on payments and transfers.  It was possible to reduce the client path for half the recurring operations to one click.  In future plans to improve the model of ‚Äúrecommended operations‚Äù using feedback data (cards can be hidden, etc.), which will reduce the threshold for selecting recommendations and increase coverage.  It is also planned to expand the coverage of recommended payments in the model of ‚Äúsearch examples‚Äù and develop an algorithm for scoring optimization for it. <br><br>  We have gone through the thorny path of building a recommender system of payments and transfers.  On the way, we got bumps and gained experience in decomposing and simplifying such tasks, correctly evaluating such systems, applicability of methods, optimal work with large amounts of data, and significantly expanded our understanding of the specifics of such tasks.  Along the way, I managed to contribute to the open-source, which we ourselves use.  I wish you interesting tasks, realistic baselines and single F1.  Thanks for attention! </div><p>Source: <a href="https://habr.com/ru/post/461747/">https://habr.com/ru/post/461747/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../461737/index.html">We collect the environment for modern TDD on JavaScript + VS code</a></li>
<li><a href="../461739/index.html">Backend United 4: Okroshka. Incidents</a></li>
<li><a href="../461741/index.html">Hierarchical clustering of categorical data in R</a></li>
<li><a href="../461743/index.html">Security Week 31: VLC vulnerability and broken phone</a></li>
<li><a href="../461745/index.html">Russian black market prices for breaking through personal data (plus a response to Tinkoff Bank's answer)</a></li>
<li><a href="../461751/index.html">Designer's contribution to mobile app development</a></li>
<li><a href="../461753/index.html">InterSystems IRIS Global Transactions</a></li>
<li><a href="../461755/index.html">The Psychology of Sound Vision. Svetlana Lebedeva told how people are taught a new way of perceiving</a></li>
<li><a href="../461757/index.html">Organization of UAT testing for business users in Jira</a></li>
<li><a href="../461759/index.html">Listening to Encrypted VoIP Communications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>