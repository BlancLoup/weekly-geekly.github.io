<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Tornado vs Aiohttp: a journey into the wilds of asynchronous frameworks</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! I am Dima, and I have been sitting on Python for quite a long time. Today I want to show you the differences between two asynchronous framework...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Tornado vs Aiohttp: a journey into the wilds of asynchronous frameworks</h1><div class="post__text post__text-html js-mediator-article">  Hello!  I am Dima, and I have been sitting on Python for quite a long time.  Today I want to show you the differences between two asynchronous frameworks - Tornado and Aiohttp.  I'll tell you the story of the choice between frameworks in our project, the difference between Tortado and Tornado and AsyncIO, show benchmarks and give some useful tips on how to get into the wilds of frameworks and successfully get out. <br><br><img src="https://habrastorage.org/webt/df/uz/jm/dfuzjmbmzyoqjfd87asllltamtk.png"><br><a name="habracut"></a><br>  As you know, Avito is a fairly large ad service.  We have a lot of data and workload, 35 million users every month and 45 million active ads daily.  I work as a technical recommendation team.  My team writes microservices, now we have about twenty of them.  All this is filled with the top load - like 5k RPS. <br><br><h2>  Choosing an asynchronous framework </h2><br>  First, I will tell you how we got to where we are now.  In 2015, we had to choose an asynchronous framework, because we knew: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  that you have to make many requests to other microservices: http, json, rpc; </li><li>  that it will be necessary to collect data all the time from different sources: Redis, Postgres, MongoDB. </li></ul><br>  Thus, we have a lot of network tasks, and the application is mainly busy with I / O.  The actual version of python at that time - 3.4, async and await did not yet appear.  Aiohttp was also in version 0.x.  Asynchronous Tornado from Facebook appeared in 2010.  For him, written a lot of drivers to the databases that we need.  On the benchmarks Tornado showed stable results.  Then we stopped our choice on this framework. <br><br>  Three years later we understood a lot. <br><br>  First, there was Python 3.5 with async / await mechanics.  We figured out what is the difference between yield and yield from and how Tornado fits in with await (spoiler: not very good). <br>  Secondly, we are faced with strange performance problems when there is a large amount of coruntine in the scheduler, even when the CPU is not fully occupied. <br>  Thirdly, we found that when performing a large number of http requests to other Tornado services, it is necessary to be friendly with the asynchronous dns resolver, it does not respect the timeouts for establishing a connection and sending a request, which we specify.  And in general, the best way to do http requests in Tornado is curl, which is rather strange in itself. <br><br>  In his <a href="https://www.youtube.com/watch%3Fv%3D5NrnBu1vcKo">report on PyCon Russia 2018,</a> Andrey Svetlov said: ‚ÄúIf you want to write some kind of asynchronous web application, please just write async, await.  Event loop, probably, you will not need any soon.  Do not climb into the wilds of frameworks, so as not to get confused.  Do not use low-level primitives, and everything will be fine with you ... ".  Over the past three years, we, unfortunately, had to quite often get into the inside of the Tornado, learn from there a lot of interesting things and see giant trainings for 30-40 calls. <br><br><h2>  Yield vs yield from </h2><br>  One of the biggest problems to understand in an asynchronous python is the difference between yield from and yield. <br><br>  <a href="https://groups.google.com/forum/">Guido Van Rossum wrote</a> more about this.  I enclose a translation with small cuts. <br><blockquote>  I was asked several times why PEP 3156 insists on using yield-from instead of yield, which eliminates the possibility of backporting in Python 3.2 or even 2.7. <br>  (...) <br>  whenever you want a future result, you use yield. <br>  This is implemented as follows.  The function containing yield is (obviously) a generator, so there must be some iteration code.  Let's call it a scheduler.  In fact, the scheduler does not ‚Äúiterate‚Äù in the classical sense (with for-loop);  instead, it maintains two collections of the future. <br><br>  I will call the first collection an ‚Äúexecutable‚Äù sequence.  These are the future whose results are available.  While this list is not empty, the scheduler selects one item and takes one iteration step.  This step calls the generator method .send () with the result from the future (which may be data that has just been read from the socket);  in the generator, this result appears as the return value of the yield expression.  When send () returns a result or completes, the scheduler analyzes the result (which can be StopIteration, another exception, or some object). <br>  (If you're confused, you probably should read about how generators work, in particular, the .send () method. Perhaps PEP 342 is a good starting point). <br><br>  (...) <br><br>  The second collection of the future, supported by the scheduler, consists of the future, which are still awaiting I / O.  They are somehow transmitted to the select / poll / etc shell.  which gives a callback when the file descriptor is ready for I / O.  The callback actually performs the I / O operation requested by the future, sets the resulting future value to the result of the I / O operation, and moves the future to the execution queue. <br><br>  (...) <br><br>  Now we have reached the most interesting.  Suppose you are writing a complex protocol.  Inside your protocol, you read the bytes from the socket using the recv () method.  These bytes fall into the buffer.  The recv () method is wrapped in an async shell, which sets up I / O and returns the future, which is executed when I / O completes, as I explained above.  Now suppose that some other part of your code wants to read data from the buffer one line at a time.  Suppose you used the readline () method.  If the buffer size is larger than the average length of the line, your readline () method can simply get the next line from the buffer without blocking;  but sometimes the buffer does not contain the whole line, and readline () in turn calls recv () on the socket. <br><br>  Question: should readline () return the future or not?  It would not be very good if it sometimes returned a byte string, and sometimes future, forcing the caller to perform type checking and conditional yield.  Therefore, the answer is that readline () should always return future.  When readline () is called, it checks the buffer, and if it finds at least a whole line there, it creates the future, sets the result to the future line taken from the buffer, and returns the future.  If the buffer does not have a whole line, it initiates I / O and waits for it, and when I / O is completed, it starts again. <br><br>  (...) <br><br>  But now we are creating many future that do not require blocking I / O, but still force the call to the scheduler - because readline () returns the future, the caller is required to yield, and that means calling the scheduler. <br>  The scheduler can transfer control directly to the coruntine if he sees that a future that is already completed is displayed, or can return the future to the execution queue.  The latter will slow down the work greatly (provided that there is more than one executable coroutine), since not only waiting at the end of the queue is required, but the locality of the memory (if it exists at all) is also likely to be lost. <br><br>  (...) <br><br>  The net effect of all this is that the authors of coroutines need to know about the yield future, and therefore there is a greater psychological barrier to reorganizing complex code into more readable corutins ‚Äî much stronger than the existing resistance, because the function calls in Python are rather slow.  And I remember from a conversation with Glyph that speed is important in a typical asynchronous I / O structure. <br>  Now let's compare this to yield-from. <br><br>  (...) <br><br>  You may have heard that ‚Äúyield from S‚Äù is roughly equivalent to ‚Äúfor i in S: yield i‚Äù.  In the simplest case, this is true, but for understanding korutin this is not enough.  Consider the following (don't think about async I / O yet): <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">driver</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(g)</span></span></span><span class="hljs-function">:</span></span> print(next(g)) g.send(<span class="hljs-number"><span class="hljs-number">42</span></span>) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> val = <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> <span class="hljs-string"><span class="hljs-string">'okay'</span></span> print(val) driver(gen1())</code> </pre> <br>  This code will print two lines containing "okay" and "42" (and then will give an unprocessed StopIteration, which you can suppress by adding yield at the end of gen1).  You can see this code in action at pythontutor.com via the <a href="http://goo.gl/Q5qpz">link</a> . <br><br>  Now consider the following: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gen1() driver(gen2())</code> </pre><br>  It works the <a href="http://goo.gl/rvOR1">same way</a> .  Now think.  How it works?  Here, a simple yield-from extension in for-loop cannot be used, since in this case the code would give out None.  <a href="http://goo.gl/R19Jv">(Try)</a> .  Yield-from acts as a ‚Äútransparent channel‚Äù between driver and gen1.  That is, when gen1 gives the value ‚Äúokay‚Äù, it leaves gen2, through yield-from, to the driver, and when the driver sends the value 42 back to gen2, this value is returned back through yield-from to gen1 again (where it becomes the result of yield ). <br><br>  The same thing would happen if the driver gave an error to the generator: the error passes through the yield-from to the internal generator that processes it.  For example: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">throwing_driver</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(g)</span></span></span><span class="hljs-function">:</span></span> print(next(g)) g.throw(RuntimeError(<span class="hljs-string"><span class="hljs-string">'booh'</span></span>)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen1</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: val = <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> <span class="hljs-string"><span class="hljs-string">'okay'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">except</span></span> RuntimeError <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> exc: print(exc) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: print(val) <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> throwing_driver(gen1())</code> </pre><br>  The code will issue "okay" and "bah", as well as the following code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">gen2</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">yield</span></span> <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> gen1() <span class="hljs-comment"><span class="hljs-comment"># unchanged throwing_driver(gen2())</span></span></code> </pre> <br>  (See here: <a href="http://goo.gl/8tnjk">goo.gl/8tnjk</a> ) <br><br>  Now I would like to introduce simple (ASCII) graphics to be able to talk about this kind of code.  I use [f1 -&gt; f2 -&gt; ... -&gt; fN) to represent the stack with f1 at the bottom (oldest call frame) and fN at the top (newest call frame), where each item in the list is a generator, and -&gt; yield-from .  The first example, driver (gen1 ()), does not have a yield-from, but has a gen1 generator, so it looks like this: <br><br><pre> <code class="python hljs">[ gen1 )</code> </pre> <br>  In the second example, gen2 calls gen1 using yield-from, so it looks like this: <br><br><pre> <code class="python hljs">[ gen2 -&gt; gen1 )</code> </pre> <br>  I use the mathematical designation of the half-open interval [...) to show that another frame can be added to the right when the right-most generator uses yield-from to call another generator, while the left ending is more or less fixed.  The left ending is what the driver sees (i.e., the scheduler). <br><br>  Now I'm ready to go back to the readline () example.  We can rewrite readline () as a generator that calls read (), another generator, using yield-from;  the latter in turn calls recv (), which performs the actual I / O from the socket.  On the left we have an application that we also view as a generator that calls readline (), again using yield-from.  The scheme is as follows: <br><br><pre> <code class="python hljs">[ app -&gt; readline -&gt; read -&gt; recv )</code> </pre> <br>  Now the recv () generator sets the I / O, binds it to the future and sends it to the scheduler using * yield * (not yield-from!).  The future passes to the left along both arrows from the yield-from planner (located to the left of the "[").  Note that the scheduler does not know that it contains a stack of generators;  all he knows is that he contains the leftmost generator and that he has just issued a future.  When I / O is complete, the scheduler sets the result to the future and sends it back to the generator;  the result moves to the right along both yiled-from arrows to the recv generator, which receives the bytes that it wanted to read from the socket as a result of the yield. <br><br>  In other words, the yield-from-based framework scheduler handles I / O operations just like the yield-based framework scheduler I described earlier.  * But: * it does not need to worry about optimization when the future is already completed, since the scheduler does not participate at all in the transfer of control between readline () and read () or between read () and recv (), and back.  Therefore, the scheduler is not involved at all when app () calls readline (), and readline () can satisfy the request from the buffer (without calling read ()) - the interaction between app () and readline () is completely processed by the byte-code interpreter in this case. Python.  The scheduler can be simpler, and the number of futures created and managed by the scheduler is smaller, because there are no futures that are created and destroyed with each call to the coroutine.  The only futures that are still needed are those that are actual I / O, for example, created recv (). <br><br>  If you have read this far, you deserve a reward.  I have omitted many details of the implementation, but the above illustration essentially reflects the picture. <br><br>  One more thing I would like to point out.  * You can * make part of the code use yield-from and the other part yield.  But yield requires that every link in the chain have a future, and not just a quortin.  Since there are several advantages to using yield-from, I want the user not to remember when to use yield, and when yield-from, it‚Äôs easier to always use yield-from.  A simple solution even allows recv () to use yield-from to transfer future I / O to the scheduler: the __iter__ method is actually a generator that issues a future. <br><br>  (...) <br><br>  And something else.  What value does yield-from return?  It turns out that this is the return value * of the external * generator. <br><br>  (...) <br><br>  Thus, although the arrows link the left and right frames to * yielding *, they also transfer the usual return values ‚Äã‚Äãin the usual way, one stack frame at a time.  Exceptions are moved in the same way;  Of course, every level requires try / except to catch them. <br></blockquote>  It turns out that yield from is almost the same as await. <br><br><h2>  yield from vs async </h2><br><table><tbody><tr><td><p>  def coro () ^ </p><p>  y = yield from a </p></td><td>  async def async_coro (): <p>  y = await a </p></td></tr><tr><td>  0 load_global </td><td>  0 load_global </td></tr><tr><td>  2 get_yield_from_iter </td><td><p>  2 get_awaitable </p></td></tr><tr><td>  4 load_const </td><td><p>  4 load_const </p></td></tr><tr><td>  6 yield_from </td><td>  6 yield_from </td></tr><tr><td>  8 store_fast </td><td><p>  8 store_fast </p></td></tr><tr><td>  10 load_const </td><td>  10 load_const <br></td></tr><tr><td>  12 return_value </td><td>  12 return_value </td></tr></tbody></table><br><br>  The two old and new school korutin have only one minor difference - get yield from iter vs get awaitable. <br><br>  What is this all about?  Tornado uses simple yield.  Prior to version 5, it connects all of this call chain through yield, which is poorly compatible with the new cool yield from / await paradigm. <br><br><h2>  The simplest asynchronous benchmark </h2><br>  It is difficult to find a really good framework, choosing it only according to the synthetic tests.  In real life, a lot of things can go wrong. <br><br>  I took Aiohttp version 3.4.4, Tornado 5.1.1, uvloop 0.11, took the server processor Intel Xeon, CPU E5 v4, 3.6 GHz, and with Python 3.6.5 I started checking the web servers for competitiveness. <br><br>  A typical problem that we solve with the help of microservices, and which works on asynchronous, looks like this.  We will receive requests.  For each of them we will do one request to some microservice, receive data from there, then go to another two or three microservice, also asynchronously, then write the data somewhere to the database and return the result.  It turns out many moments where we will wait. <br><br>  We perform a simpler operation.  Turn on the server, make him sleep 50 ms.  Create a corutin and complete it.  We will not have a very large RPS (it may not coincide by an order of magnitude with what is seen in fully synthetic benchmarks) with an acceptable delay due to the fact that a lot of coroutines will simultaneously rotate in a competitive server. <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@tornado.gen.coroutine def old_school_work(): yield tornado.gen.sleep(SLEEP_TIME) async def work(): await tornado.gen.sleep(SLEEP_TIME)</span></span></code> </pre> <br>  Load - GET http requests.  Duration - 300s, 1s - warmup, 5 repetitions of the load. <br><br><img src="https://habrastorage.org/webt/ep/mq/fw/epmqfwh6bv_vyfu8tymtelvohos.png"><br><br>  <i>Results on service response time percentiles.</i> <br><br><div class="spoiler">  <b class="spoiler_title">What are percentiles?</b> <div class="spoiler_text">  You have some big set of numbers.  95th percentile X means that 95% of the values ‚Äã‚Äãin this sample are less than X. With a probability of 5%, your number will be more than X. <br></div></div><br>  We see that Aiohttp on such a simple test did a great job on 1000 RPS.  All while without <a href="https://uvloop.readthedocs.io/">uvloop</a> . <br><br>  Compare Tornado with the old and new async schools.  Authors are strongly advised to use async.  We can make sure that they are really much faster. <br><br>  At 1200 RPS, the Tornado, even with the new school korutinami, is already beginning to give up, and the Tornado with the old school korutinami has completely blown away.  If we sleep 50 ms, and microservice is responsible for 80 ms, it does not go into any gate at all. <br><br>  The new Tornado school for the 1500 RPS completely surrendered, and Aiohttp is still far from the 3000 RPS limit.  The most interesting is yet to come. <br><br><h2>  Pyflame, profiling a working microservice </h2><br>  Let's see what is happening at this moment with the processor. <br><br><img src="https://habrastorage.org/webt/mw/-6/c-/mw-6c-vzw_kk-flygqeig93qohe.png"><br><br>  When we figured out how asynchronous Python microservices work in production, they tried to understand what was going on.  In most cases, the problem was with the CPU or with descriptors.  There is a great profiling tool created in Uber, the <a href="https://github.com/uber/pyflame">Pyflame</a> profiler, which is based on the ptrace system call. <br><br>  We start some service in the container and start throwing a combat load on it.  Often, this is not a very trivial task - to create just such a load that is in combat, because it often happens that you run synthetic tests on load testing, you look, and everything works fine.  You are pushing a combat load onto it, and now microservice starts to blunt. <br><br>  During operation, this profiler does snapshots of the call stack for us.  You can not change the service at all, just run pyflame next to it.  It will collect the stack trace once in a while, and then does a cool visualization.  This profiler gives very little overhead, especially when compared with cProfile.  Pyflame also supports multithreaded programs.  We ran this thing right in the market, and the performance was not degraded. <br><br><img src="https://habrastorage.org/webt/pk/2s/lu/pk2slutzgqe-szo4mbefnk_zoqe.png"><br><br>  Here on the X axis is the amount of time, the number of calls when the stack frame was in the list of all the stack of Python frames.  This is the approximate amount of CPU time that we spent in this particular stack frame. <br><br>  As you can see, most of the time in aiohttp is spent on idle.  Great: this is what we want from an asynchronous service, so that it will deal with network calls most of the time.  In this case, the stack depth is about 15 frames. <br><br>  In Tornado (the second picture) with the same load, much less time is spent on idle and the stack depth in this case is about 30 frames. <br><br>  Here is a <a href="http://bit.ly/2CpLcUG">link to svg</a> , you can twist it yourself. <br><br><h2>  More sophisticated asynchronous benchmark </h2><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">async</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">work</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     await asyncio.sleep(SLEEP_TIME) class HardWorkHandler(tornado.web.RequestHandler): timeout_time = datetime.timedelta(seconds=SLEEP_TIME / 2) async def get(self): await work() #     await tornado.gen.multi([work(), work()]) #     try: await tornado.gen.with_timeout(self.timeout_time, work()) except tornado.util.TimeoutError: #     pass</span></span></code> </pre><br>  You should expect a runtime of 125 ms. <br><br><img src="https://habrastorage.org/webt/i2/u5/3d/i2u53d-v1qhpmgkqiifa6q8rita.png"><br><br>  Tornado with uvloop holds up better.  But Aiohttp uvloop helps much more. Aiohttp      2300-2400 RPS,   uvloop    .   ,         . <br><br><h2>  Results </h2><br>    ,      . <br><br><ul><li> -,     ,      .    Aiohttp     2,5 ,  Tornado. </li><li>  . Uvloop      Aiohttp (,  Tornado). </li><li>    Pyflame,        . </li><li>        yield from (await)  yield. </li></ul><br>        (  )     Aiohttp c Tornado    Python  . <br><br><ul><li>     CPU     2 . </li><li>     http-. </li><li>     2  5 . </li></ul><br>  <a href="http://bit.ly/2FvU2UX">  </a> .  ,  .  Thank you all for your attention.  ,    . </div><p>Source: <a href="https://habr.com/ru/post/435532/">https://habr.com/ru/post/435532/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../435520/index.html">We write our programming language, part 3: Translator Architecture. Parsing language structures and mathematical expressions</a></li>
<li><a href="../435522/index.html">Snapshots of events in Axonframework 3, improving performance</a></li>
<li><a href="../435526/index.html">Adventures with a home Kubernetes cluster</a></li>
<li><a href="../435528/index.html">5 reasons for success: why Amazon has become the most expensive company in the world</a></li>
<li><a href="../435530/index.html">Paid Subscriptions - Dependency of Auto Connection on Mobile</a></li>
<li><a href="../435534/index.html">Data Science: entry level books</a></li>
<li><a href="../435536/index.html">Humanoid robots: the benefits and problems of anthropomorphic mechanisms</a></li>
<li><a href="../435538/index.html">In 2018, in Germany, more "green" energy was received than electricity from the burning of coal</a></li>
<li><a href="../435540/index.html">New keywords in java</a></li>
<li><a href="../435542/index.html">Game development and diploma defense or ‚ÄúHow I killed two birds with one stone the first pancake‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>