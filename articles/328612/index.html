<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Your bot for a few hours, or talk about beer with the machine</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The topic of improving the interaction between machines and humans is more relevant than ever. There were technical possibilities for the transition f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Your bot for a few hours, or talk about beer with the machine</h1><div class="post__text post__text-html js-mediator-article">  The topic of improving the interaction between machines and humans is more relevant than ever.  There were technical possibilities for the transition from the model of "100 clicks" to the paradigm of "say what you want."  Yes, I mean various bots, which for several years have been developed by all and sundry.  For example, many large companies, not only technological, but also retail, logistics, banks are currently conducting active Research &amp; Design in this area. <br><br>  A simple example, for example, how does the process of choosing goods in any online store take place?  A bunch of lists, categories in which I dig and choose something.  It suck's.  Or, for example, entering the Internet bank, I come across different menus, if I want to make a transfer, then I have to select the corresponding items in the menu and enter a bunch of data, if I want to see the list of transactions, then again, I have to strain brain and index finger.  It would be much easier and more convenient to go to the page, and just say: ‚ÄúI want to buy a liter of milk and half a liter of vodka‚Äù, or simply ask the bank: ‚ÄúWhat about the money?‚Äù. <br><br>  The list of professions facing extinction in a fairly close perspective, are added: tellers, call center operators, and many others.  And with a simple example, which took me 7 hours to implement, I will show how you can simply make the integration of speech recognition, and identify entities, using the example of open Wit.Ai (Google Speech API integration is also included). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/web/f1c/f84/d32/f1cf84d327f444cd8023382c4b313463.jpg"><br><a name="habracut"></a><br>  There are many APIs for speech recognition, some of them are paid, some are open for developing their systems.  For example, the Google Speech API offers 60 minutes of recognition per month for free, if this limit is exceeded, a fee is charged at a rate of 0.006 cents per minute, in 15 second increments.  On the contrary, Wit.ai positions itself as an open API for developers, but whether they will have the same level of service, for example, if for example the number of calls to the service grows to hundreds of thousands, or even millions per month, remains a question. <br><br>  A couple of weeks ago, we had seminars on Data Science and Artificial Intelligence in Tartu, and many speakers addressed, one way or another, the topic of human-machine interaction in a language that people can understand.  And on the following, after the events, the weekend, I decided to implement speech recognition using public services.  And of course, I would like the bot to understand that I want to drink beer, and what kind of beer it is.  Dark or light, as well as possible and would understand the varieties. <br><br>  In general, the task is to write on the client side what he said, send to the server, do some transformations, make a call to a third-party API, and get the result. <br>  At first, I planned to go to the Google Speech API to get a transcript of the audio file, and then I wanted to send a string of text to Wit.AI to get a set of entities and intentions. <br><br>  The choice for backend was trivial for me, it's Spring Boot.  And not only because the Java stack is native to me, but also because I wanted to create a small service that will serve as an intermediary between third-party APIs and the client.  Additional functionality can be implemented by introducing an additional service. <br><br>  I posted the source code on <a href="https://github.com/alextavgen/SpeechBotExample/tree/master/speechbotpoc">github</a> .  I deployed a working application on Heroku - <a href="https://speechbotpoc.herokuapp.com/">https://speechbotpoc.herokuapp.com/</a> .  Before use, allow use of the microphone.  Next, click on the microphone icon and say something, then click the icon again.  What you said will <s>be used against you</s> will be sent to recognition, and after a while, you will receive the result in the panel on the left.  I turned off the choice of translation language, since this example can be used by several people, in order to avoid race condition, the choice of recognition language is disabled. <br><br><img src="https://habrastorage.org/web/da6/501/400/da650140091c4a73ae285823ba2843ec.png"><br><br>  So, for starters, we create an empty project using Spring Initializr or Spring Boot CLI, as you like, I chose Spring Initializr.  The required dependencies are Spring MVC, Lombok (if you don‚Äôt want to write a lot of generic code), Log4j.  We import the created project framework into a preferred IDE (someone else uses Eclipse?). <br><br>  First, we need to record the audio file on the client side.  HTML5 provides all the features for this, (MediaRecorder interface), but there is an excellent implementation from Matt Diamond, which is distributed under the MIT license, and I decided to take it, because Matt developed a good visualization on the client side as well.  Most of the time, in fact, was occupied not by the development of the server part, but by the implementation on the client side.  I did not use AngularJS or ReactJS, because I was interested in general integration, and my choice was jQuery, cheap and cheerful. <br><br>  As for the server part, at first I wanted to use the Google Speech API for the initial transcription of an audio file into text, and since this API requires transcoding of recorded speech to Base64, then on the client side, after receiving the audio data, I transcoded to Base64, and then sent to the server. <br><br>  In our frame we create a controller that will receive our audio file. <br><br><pre><code class="hljs kotlin"><span class="hljs-meta"><span class="hljs-meta">@RestController</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">ReceiveAudioController</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-meta"><span class="hljs-meta">@Autowired</span></span> <span class="hljs-meta"><span class="hljs-meta">@Setter</span></span> <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> WitAiService service; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> static <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> Logger logger = LogManager.getLogger(ReceiveAudioController.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); <span class="hljs-meta"><span class="hljs-meta">@RequestMapping(value = </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"/save"</span></span></span><span class="hljs-meta">, method = RequestMethod.POST)</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-meta"><span class="hljs-meta">@ResponseBody</span></span> String saveAudio(<span class="hljs-meta"><span class="hljs-meta">@RequestBody</span></span> Audio <span class="hljs-keyword"><span class="hljs-keyword">data</span></span>, HttpServletRequest request) throws IOException { logger.info(<span class="hljs-string"><span class="hljs-string">"Request from:"</span></span> + request.getRemoteAddr()); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> service.processAudio(<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>); } }</code> </pre> <br>  Everything is quite simple, we take the data and transfer it to the service, which does all the further work. <br><br>  WitAiService is also quite simple. <br><br><pre> <code class="hljs pgsql">@Data @Component @ConfigurationProperties(prefix="witai") <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> WitAiService { private static final Logger logger = LogManager.getLogger(WitAiService.<span class="hljs-keyword"><span class="hljs-keyword">class</span></span>); private String url; private String key; private String <span class="hljs-keyword"><span class="hljs-keyword">version</span></span>; private String command; private String charset; <span class="hljs-built_in"><span class="hljs-built_in">public</span></span> String processAudio(Audio audio) throws IOException { URLConnection <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span> = getUrlConnection(); OutputStream outputStream = <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>.getOutputStream(); byte[] decoded; decoded = Base64.getDecoder().decode(audio.getContent()); outputStream.<span class="hljs-keyword"><span class="hljs-keyword">write</span></span>(decoded); BufferedReader response = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> BufferedReader(<span class="hljs-built_in"><span class="hljs-built_in">new</span></span> InputStreamReader(<span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>.getInputStream())); StringBuilder sb = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> StringBuilder(); String <span class="hljs-type"><span class="hljs-type">line</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">while</span></span>((<span class="hljs-type"><span class="hljs-type">line</span></span> = response.readLine()) != <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>) { sb.append(<span class="hljs-type"><span class="hljs-type">line</span></span>); } logger.<span class="hljs-keyword"><span class="hljs-keyword">info</span></span>("Received from Wit.ai: " + sb.toString()); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> sb.toString(); } private URLConnection getUrlConnection() { String query = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; try { query = String.format("v=%s", URLEncoder.encode(<span class="hljs-keyword"><span class="hljs-keyword">version</span></span>, charset)); logger.<span class="hljs-keyword"><span class="hljs-keyword">info</span></span>("Query string for wit.ai: " + query); } catch (UnsupportedEncodingException e) { e.printStackTrace(); } URLConnection <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">null</span></span>; try { <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span> = <span class="hljs-built_in"><span class="hljs-built_in">new</span></span> URL(url + "?" + query).openConnection(); } catch (IOException e) { e.printStackTrace(); } <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>.setRequestProperty ("Authorization","Bearer " + key); <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>.setRequestProperty("Content-Type", "audio/wav"); <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>.setDoOutput(<span class="hljs-keyword"><span class="hljs-keyword">true</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">connection</span></span>; } }</code> </pre><br>  All necessary parameters, such as a key and a token for Wit.AI, are taken from the application.properties file.  (yes, I opened tokens for my application).  If you want to register your application on Wit.AI, then you will need to change the tokens and App.ID in the settings files.  Registration on Wit.AI is quite simple, the token can be obtained in the Settings section. <br><br>  Spring Boot tightens settings using annotations.  The <a href="https://habrahabr.ru/users/data/" class="user_link">Data</a> and <a href="https://habrahabr.ru/users/setter/" class="user_link">Setter</a> annotations are <a href="https://projectlombok.org/">Lombok</a> project annotations that help avoid writing template code, in the form of setters, getters, default constructors, and so on. <br><br>  I don‚Äôt include the model files and the auxiliary converter here; all this can be viewed in the source code. <br><br>  If you open the Developer Console in Chrome, you will see that after you have written what has been said, a REST call is made to the service, where the audio data encoded in Base64 is transmitted to the server in the content field.  I had some problems with Google billing (billing is registered to an old credit card that is closed and I haven‚Äôt yet issued a new one), and after I wrote the interaction with the Google Speech API, I decided to send the recognition directly to Wit.AI.  But Wit.AI accepts the data in streaming form as it is, and therefore the server translates from Base64 back to wav format. <br><br><pre> <code class="hljs pgsql"> byte[] decoded; decoded = Base64.getDecoder().decode(audio.getContent()); outputStream.<span class="hljs-keyword"><span class="hljs-keyword">write</span></span>(decoded);</code> </pre><br>  Small points regarding recognition.  Pay attention to the microphone level, because if it is very sensitive, you will get a cut off sound that will negatively affect the quality of recognition. <br><br>  Now that we have gotten the recognition of what we have said, let's learn to understand the application, what we specifically want.  In the <b>Understanding</b> Wit.AI section (To do this, you need to register your application on Wit.AI, that is, be registered, it is easy.) We can specify the so-called intent, or intentions.  It looks quite logical, we write a line, for example, ‚ÄúLet's go drink beer‚Äù and select the word we need, in this case ‚ÄúBeer‚Äù, and click on <b>Add a new entity</b> , then we select the <b>intent</b> and create an intention "Beer".  Go ahead, we want to understand whether we want to drink beer, and then we create a new entity, ‚ÄúDrink‚Äù or ‚ÄúDrink‚Äù.  Further, it is recommended to introduce some more examples, like: ‚ÄúMaybe for a beer today‚Äù, ‚ÄúLet's drink a beer tomorrow‚Äù, etc.  Then the system will more and more accurately highlight the <b>intent</b> - Beer. <br><br>  Now let's say we want to understand what kind of beer we want to drink, light or dark.  In the same way, we introduce a new phrase, ‚ÄúLet's drink dark beer tomorrow,‚Äù and on the word ‚Äúdark‚Äù we should again click on <b>Add a new entity</b> , but further, we should not use the existing one, but create our own essence, in my application it is and is called <b>beer_type</b> .  Then we repeat for light beer, just choose the already created entity <b>beer_type</b> .  As a result, the system begins to understand that I want to drink beer, and what kind of beer specifically.  It is also possible to set all of the above not manually, but automatically, using the WEST.AI REST interface.  Categories are easy to translate into essence in batch mode. <br><br>  Try to suggest in the example to drink dark or light beer, you will see that the system returns an object in which there is both a beer intention and a beer_type - dark or light. <br><br>  This example is a little toy, BUT ... Adding entities can be done through the REST interface, so you can quite easily transfer the product catalog to the bot essence, there are many context entities in Wit.AI, such as time / date, location, and so on.  That is, you can get information from contextual words such as today or tomorrow (date), here (place), etc.  Detailed documentation is <a href="https://wit.ai/docs">here</a> . <br><br>  By itself, the code is quite simple, and everything is clear from it.  The logic of integration with other services is the same.  After receiving the transcription, you can transfer this string to other services, for example, write a service that will add the desired goods to the basket, or simply transfer the string to the translation for the neuroservice ( <a href="http://xn--neurotlge-v7a.ee/">http://xn--neurotlge-v7a.ee/#</a> while from Estonian to English and back, but you can train the model in other languages).  That is, this example can serve as a small brick to build a more complex flow of interactions.  For example, I can, for example, order a beer at home, combining this example with a food ordering service, receiving their token or cookie.  Or, on the contrary, send friends suggestions about beer by messenger.  Uses a huge amount of options <br><br>  Possible ways of use: online stores, online banks, transfer systems, etc. <br><br>  If you have questions, you can contact. </div><p>Source: <a href="https://habr.com/ru/post/328612/">https://habr.com/ru/post/328612/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../328602/index.html">Digest: IaaS provider operation, SSL certificates, data center and our ‚ÄúFriday format‚Äù</a></li>
<li><a href="../328604/index.html">Migrating users and their privileges to MySQL</a></li>
<li><a href="../328606/index.html">Analysis of Wana Decrypt0r 2.0 cipher</a></li>
<li><a href="../328608/index.html">Python PID Controller Model</a></li>
<li><a href="../328610/index.html">Can Laravel be used for large enterprise solutions?</a></li>
<li><a href="../328616/index.html">Who needs calls when there are chats and bots?</a></li>
<li><a href="../328620/index.html">Soft Mocks for Go! (redefinition of functions and methods in runtime)</a></li>
<li><a href="../328624/index.html">Elements of functional programming in C ++: mapping compositions</a></li>
<li><a href="../328628/index.html">How to make it happen</a></li>
<li><a href="../328630/index.html">Product design digest, April 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>