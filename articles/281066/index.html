<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Metric recommendation system imhonet.ru</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The purpose of this story is to share ways to solve the problem that the authors worked on when developing the imhonet.ru reference service. Since the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Metric recommendation system imhonet.ru</h1><div class="post__text post__text-html js-mediator-article">  The purpose of this story is to share ways to solve the problem that the authors worked on when developing the <a href="http://imhonet.ru/">imhonet.ru</a> reference service.  Since the problem is not purely scientific and technical, but rather located at the intersection of technology and business and may be useful to a wider audience than a regular technical report, we chose just such a presentation format for our work - we tried to write the story in as simple a language as possible.  The first part of the story is devoted to a fairly detailed justification of how to correctly measure the quality of the workings of the recommender system algorithms.  And at the end illustratively listed several examples in which we carried out these measurements to solve specific problems. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f55/fd2/413/f55fd2413adf2653519c6437b476b237.png"><br><a name="habracut"></a><br><h1>  Problem metrics for recommender </h1><br>  Any recommendation system helps to solve a specific business problem.  And the result is measured in ways that are understandable for the business problem - the number of visitors, sales, CTR, etc.  However, the quality of the recommender algorithm in this way is too difficult to measure - it will depend on a huge variety of conditions, among which the recommender algorithm itself may be the tenth thing. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d77/d75/596/d77d75596755e590d2c0332fe7618cc0.png"></div><br>  So it turns out that formal numerical criteria for recommender algorithms need to be invented by their developers in isolation from the business task.  As a result, the work devoted to recommender systems is full of various useful numerical metrics, but sometimes quite tricky in relation to the problems to be solved. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The optimal approach to be discussed is to try to construct a metric directly for a business problem.  It is clear that this is easier said than done, so the discussion below will deal with a specific example: how this is done for the reference service of imhonet.ru movies.  Although we tested this approach on the films, but it is rather distracted, as can be seen by replacing the film word in the whole next story with something else. <br><br><h1>  Service product and user feedback </h1><br>  Service product - personal lists of recommended films.  The feedback is fairly accurate, and therefore valuable - direct ratings to these films: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c0d/a5b/e7b/c0da5be7b154eced762e60a7665ffb44.png"></div><br>  It makes sense to use such a good feedback to the fullest.  Estimates, for the sake of convenience in terminology, we will sometimes be called "signals of satisfaction." <br><br>  Why in the task we are talking about the list of recommendations, and, for example, not about one element?  Therefore, why the search engine gives a list of links: it does not know the full context of the request.  If he knew the full context (for example, the contents of the user's head), then, probably, the only result would make sense, otherwise the part of the work still remains for the author of the request. <br><br><h1>  Recommendations and evaluations </h1><br>  We compare the feedback of users - ratings, with the lists of items that we recommended: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f1c/a12/f39/f1ca12f3934f6e1906a7b25b52e61903.png" width="750"></div><br>  We will need to consider the following situations: <br><br><ul><li> <b>Hit</b> - the film was recommended, and there is a positive rating </li><li>  <b>Mishit</b> - recommended movie, but negative </li><li>  <b>Recommended but not rated</b> - the film was recommended, but no evaluation </li><li>  <b>Rated but not recommended</b> - there is a positive assessment of the film, but it was not recommended. </li></ul><br>  Go through them one by one. <br><br>  <b>Hit</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b2b/66b/49d/b2b66b49d95e4a51ea83db1c8daa3797.png" width="750"></div><br>  If a film was recommended to a person, and there is a positive signal from him, it means that we hit exactly.  It is precisely such cases that interest us, so let's call them successes and maximize their number: <br><br>  <b>Mishit</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/230/e50/dd5/230e50dd57cee46e953c30bd46061df1.png" width="750"></div><br>  The film that was recommended and received a negative signal is an example of the cases that we should avoid.  It is even possible that avoiding such cases is more important than maximizing the number of cases of success. <br><br>  This is a bit unexpected, but practice has shown that such cases can simply be neglected - the probability of the coincidence of negative signals with elements from the list of recommendations, which any imputed recommender system gives, turns out to be too small to significantly affect the value of the metric.  Therefore, it is not necessary to specifically consider this case.  And this means that the metric will be insensitive to negative evaluations.  More precisely, a negative assessment will be equivalent to simply the lack of assessment by the user (this will be completely clear a little later). <br><br>  <b>Recommended but not rated</b> <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/309/f87/2be/309f872be4fcf0d052065ea4c0637d16.png" width="750"></div><br>  If the film was recommended, but we do not have a positive signal, then it seems that it means nothing.  You never know why it happened - a person could not watch the movie or not appreciate it.  However, from the point of view of the entire audience, the entire mass of signals, these are cases for which the probability that a film for some reason does not deserve a high mark for a person is definitely higher than for cases of sheer success.  Therefore, it would be possible to consider this incident also negative, but it is very difficult to estimate how much this probability is.  That is, we cannot determine the degree of negativity.  As a result, it makes sense to assume that simply nothing happened and not to take into account the case explicitly.  In practice, it turns out that the majority of such cases are - firstly, we should always advise some excess (that is, take the <b>N <sub>rec</sub></b> value with a margin), secondly, most people, as a rule, do not mark every viewed film (t .e., we see only a part of their assessments). <br><br>  <b>Rated but not recommended</b> <br>  The opposite case - the film was not recommended, but there is a positive signal, that is, the adviser missed the opportunity to increase the number of successes.  While such opportunities are found, you can improve the result: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a19/405/31e/a1940531e44a7c8e29afac4fc1b81bed.png" width="750"></div><br>  If the recommendations contain all positive signals, the accuracy of the system is maximum - there is nothing more to improve: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/683/987/b93/683987b9375fb6238f8ab7ec0857b57a.png" width="750"></div><br>  So, in our scheme, it turns out that improving the quality of recommendations is an increase in the number of successes. <img src="https://habrastorage.org/getpro/habr/post_images/abe/c78/833/abec78833b74aaeb38b8e1085aaf1401.png" height="30">  in the list of recommendations (the size of which, <b>N <sub>rec is</sub></b> fixed). <br><br><h1>  Metric precision </h1><br>  If instead of just the number of successes we use the value precision (p), i.e.  we will divide <img src="https://habrastorage.org/getpro/habr/post_images/abe/c78/833/abec78833b74aaeb38b8e1085aaf1401.png" height="30">  - the number of successes by <b>N <sub>rec</sub></b> , then, essentially, nothing will change: instead of the number of successes, we will maximize the value, which differs from it only by dividing by a constant: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/495/1f9/305/4951f9305940876153b30ebd806e0e6a.png" height="60"></div><br>  But, as we will see later, this division will allow to take into account in the metric a very significant aspect of the task - the dependence of the metric on the order of recommendations in the list.  In addition, the value of <i><b>p</b></i> has clear logic that can be described in a probabilistic language.  Suppose that a person consumes our product, namely, it looks through all the elements of the list of recommendations.  Then <i><b>p</b></i> means for him the probability to find in this list a suitable element - one that will satisfy him in a relative future.  Denote <i><b>precision</b></i> for user <i><b>u</b></i> as <i><b>p <sub>u</sub></b></i> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/65f/28e/7ea/65f28e7ea83c16a7c9b0a8f91aec987f.png" height="60"></div><br>  It is appropriate to generalize this formula to our entire audience (or measured sample) of users ( <i><b>Users</b></i> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/02a/d56/326/02ad56326a9da96880ecd88c2a75ab22.png" height="60"></div><br>  Everyone looks at their own list and chooses what they need, and <b>P <sub>Nrec</sub></b> shows the average probability of success for all cases.  Value <img src="https://habrastorage.org/getpro/habr/post_images/abe/c78/833/abec78833b74aaeb38b8e1085aaf1401.png" height="30">  on the right side, this is now the total number of successes in the entire sample. <br><br><h1>  Discounting </h1><br>  So far, we have evaluated the list of elements as a whole, but we know perfectly well that its beginning is more important than the tail.  Anyway, if the list is not too short.  The metric that takes this into account, and therefore depends on the order of the elements in the list, is called discounted.  The beginning of the list is more important than the tail, due to the uneven distribution of users' attention - people almost always look at the first element, look at the first and the second, and so on, more rarely.  This means that for a proper discounting, we need some behavioral model.  And the data with which we can support this model (train). <br><br>  We imagine an average user who scans the elements in turn, starting with the first, and then, at a certain moment, stops doing it: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/076/3f3/092/0763f3092075dd9dbc74f13fdcbb610e.png"></div><br>  We can not identify a specific user, and do not want.  Sometimes the same person will want to see a list of 2 items, and sometimes out of 20. <br><br>  It would be nice to get the average probability of the transition from each element to the next, but we have enough data, even a little easier. <br><br>  Suppose that we have the probability that an arbitrary person will scan a list of long <i><b>N</b></i> elements: <i><b>w <sub>N</sub></b></i> for any reasonable <i><b>N.</b></i> Then we can average the values ‚Äã‚Äãof <i><b>P <sub>N</sub></b></i> by the formula of total probability, meaning by <i><b>P <sub>N the</sub></b></i> average value of the probability of success for that part of our audience which looked through a list of length <i><b>N.</b></i>  In our terminology, it will look like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/411/8c3/8f8/4118c38f8f3490e9d406ff6fa657bf03.png" height="60"></div><br>  It turns out that, in contrast to <i><b>precision</b></i> , the value of <i><b>AUC</b></i> estimates the average probability of success for personal recommendation lists in more realistic conditions ‚Äî when lists are viewed by living people whose attention is unevenly distributed.  Note that to get the value of <i><b>AUC</b></i> , we used the dependence of the value of <b>P <sub>Nrec</sub></b> ( <i><b>precision</b></i> ) on <b>N <sub>rec</sub></b> - the size of the list of recommendations, which we fixed a little earlier. <br><br><h1>  What is AUC? </h1><br>  The term <i><b>AUC</b></i> denotes, among other things, the integral <i><b>precision by recall</b></i> , which is sometimes used as a quality metric of classifiers. <br><br>  The summation we made is in some sense an analogue of such a metric: with different values ‚Äã‚Äãof <b>N <sub>rec,</sub></b> we model different values ‚Äã‚Äãof <i><b>Recall</b></i> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b6c/ec1/2f4/b6cec12f4e24be386d0d2499e078d678.png"><br><br>  This is a (already classic) example of constructing ROC curves in <i><b><abbr title="Jesse Davis and Mark Goadrich. The relationship between precision recall and roc curves. In ICML '06: Proceedings of the 23rd International Conference on Machine Learning, pages 233-240, 2006.">precision-recall</abbr></b></i> coordinates. <br><br><h1>  How to quickly assess the value of the scales wN? </h1><br>  There is a simple and common model for estimating the values ‚Äã‚Äãof <i><b>w <sub>N</sub></b></i> , which makes it possible not to process the entire probability distribution of transitions, but gives a qualitatively correct behavior of the values ‚Äã‚Äãof <i><b>w <sub>N</sub></b></i> : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a60/2e6/250/a602e62506e56a718febc9272f4e332c.png"></div><br>  The only parameter of the <i><b>Q</b></i> model is the probability that an arbitrary user will go to the next page of the list (it is easy to get it from the web logs of pagination).  Suppose that the pages contain <i><b>m</b></i> elements each, and users with the same probability <i><b>p</b></i> go to the next element or, in other words, with probability <i><b>(1-p)</b></i> on each element go away.  Then <i><b>p is</b></i> easily obtained from the relation <i><b>Q = p <sup>m</sup></b></i> .  (If we assume that the first element will be scanned with probability 1.) Then, we need the probability <i><b>w <sub>N</sub></b></i> that a person will look at <i><b>N</b></i> elements, and then leave, it is easy to calculate: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/4ce/164/c57/4ce164c575155bdc495f39728dfc656d.png" height="50"></div><br>  In our experiment, the pages consisted of 50 elements, and the value of <i><b>Q</b></i> was equal to <i><b>0.1</b></i> .  The dependence of <i><b>w <sub>N</sub></b></i> on <i><b>N</b></i> at the same time looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e96/4eb/871/e964eb871f8eb28ab4f16e5c38c8fd1f.png"></div><br><h1>  The value of evaluation and "satisfaction" </h1><br>  So far, we have used only the fact of a positive signal and ignored the fact that we have its value.  It is clear that ignoring this information would be unfair.  When the scale of the collection of assessments is chosen well, the magnitude of the signal should be, on average, proportional to human satisfaction: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/127/a28/177/127a281779e65f7cb0209d2ed53b78c2.png"></div><br>  Following this logic, replace in the current metric: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e54/426/bc9/e54426bc9dd2c3b1819a57431d97335e.png" height="80"></div><br>  ... counter success <img src="https://habrastorage.org/getpro/habr/post_images/abe/c78/833/abec78833b74aaeb38b8e1085aaf1401.png" height="30">  by (more informative) amount of positive points: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/e4a/7cd/feb/e4a7cdfeba8f8584d7b911a22684fcff.png" height="80"></div><br>  <b>S <sub>Nrec</sub></b> - many successful evaluations, that is, those positive evaluations that are counted in <img src="https://habrastorage.org/getpro/habr/post_images/abe/c78/833/abec78833b74aaeb38b8e1085aaf1401.png" height="30">  .  The positive part of our scale begins with 6 points, so we (just for convenience) subtract 5 from all <i><b>r</b></i> estimates. <br><br>  Why can this be done?  Because you can imagine the role of positive points in the metric a bit differently: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/139/6a4/e6b/1396a4e6b5429408f49d910f8c50a488.png" height="150"></div><br>  This is the average positive score for the audience that will see a list of long <b>N <sub>rec</sub></b> .  And since we assumed that lists of different lengths are viewed by approximately the same people, an increase in the metric will depend only on the ability of the algorithm to increase the average score for the top of the lists at the expense of the bottom for all users. <br><br>  The product of the probability of success by the average score in case of success can be called the total amount of satisfaction that the recommendatory algorithm can achieve.  That is, now the metric, although it loses its purely probabilistic interpretation, is better suited to our goals.  (Note that the numerical value of the <i><b>AUC <sup>r</sup></b></i> can be any - in fact, this is the specific number of positive points in the feedback, which depends on the sample size, audience activity, selected behavioral model of users, etc.) <br><br>  Suppose that two recommender algorithms generated top lists that differ slightly in the order of elements.  It turned out that in the first case, the actual user ratings were distributed like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/318/de8/9c8/318de89c842592e7f4d3daad72ac031b.png" height="30"></div><br>  and in the second: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c72/9fa/7aa/c729fa7aac1ac6d164994981a1476894.png" height="30"></div><br>  Simultaneous accounting and estimation values ‚Äã‚Äãand discounting allow <i><b>AUC <sup>r</sup></b></i> to distinguish them reasonably and quantitatively, that is, to find out that the second algorithm is better and say how much. <br><br><h1>  Long tail and short head </h1><br>  Movies, like other media products, have a funny feature: the distribution of elements in popularity is extremely uneven, there are quite a few films that are known and appreciated by a large number of people - this is a <i><b>short head</b></i> , and the vast majority are known to relatively few - this is a <i><b>long tail</b></i> . <br><br>  For example, the distribution of imhonet estimates looks like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6b8/84f/0b9/6b884f0b960f0c2a06f2d449d774d8e4.png"></div><br>  This distribution is so steep in the <i><b>short head</b></i> part that, for any summation of signals, films from it dominate, giving more than 80% of the contribution to the metric.  This causes its numerical instability: the disappearance or appearance in the top of the recommendations of just a few objects from the <i><b>short head</b></i> can dramatically change the value of the metric. <br><br>  Ahead of your curiosity, we note that the <i><b>short-head</b></i> feature of the graph can be straightened in log-log coordinates: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f5e/bb4/38b/f5ebb438b50f51d04adfc51b678a0d16.png"></div><br>  Such coordinates are used, for example, to describe the distribution of the frequency of words in natural languages. <br><br>  Recall that the purpose of recommender systems is effective personalization.  She is associated with skill accurately <br>  pick items from <i><b>long tail</b></i> .  The elements from the <i><b>short head</b></i> are mostly known to everyone, that is, they almost do not need to be included in the recommendations: <br><img src="https://habrastorage.org/getpro/habr/post_images/41e/e6f/f57/41ee6ff571e57df3344b522cf717103a.png"><br><br>  Therefore, a sensible solution is to simply reduce the weight of the <i><b>short head</b></i> in the metric.  How to do it correctly?  We start from the metric problem itself - numerical instability will necessarily lead to a decrease in sensitivity.  We simulate test cases that the metric must distinguish and achieve its maximum sensitivity in them.  As a point of reference, we take a situation where we do not know anything about the users in our sample.  Then all the personal lists of recommendations will turn out the same and will correspond to some movie ratings.  To build this movie rating, we use the simple probability that the film will be liked, calculated on the entire rating base.  For example, it may be the probability that an arbitrary score of a film is more than 5 points: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/0b0/390/1c0/0b03901c06787e4b53caa8c5ebf51b4f.png" height="60"></div><br>  (For calculations, we, of course, will correct this probability by the Laplace rule. And we can take into account the magnitudes of the estimates.) <br><br>  Fortunately, in addition to ratings, we know about the users from our sample various additional information that they indicate in the questionnaire: gender, age, likes fiction or vice versa, and much more.  For example, we know the floor.  Then we can build instead of one general two different recommendation lists.  We will do this using the Bayesian probability formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/223/2ac/ba7/2232acba768731590ae50ff321a2a62f.png" height="60"></div><br>  Here the probability for our point of reference <i><b>P (r&gt; 5)</b></i> works as a priori.  Since two different recommendation lists are better than one, we can rightly expect an increase in the value of the metric: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a4b/0d9/c88/a4b0d9c88263e02cdd629b7e5131734f.png"><br><br>  It is more convenient to assess the relative increase: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9f7/609/6ab/9f76096abfa69f8c906cbbaa4398d097.png" height="60"></div><br>  An increase in the metric should also occur when using any other information about users that is relevant to film preferences.  The greater the increase, the more sensitive it is to this information.  Since we are not interested in a specific case about gender, in order not to retrain on it, we will average the <i><b>AUC</b></i> increase over the many ways we can divide the audience.  In our experiment, user answers were used in the questionnaire for 40 different questions. <br><br>  And now let's write out what the optimization problem looks like, which finds the necessary metric with optimal sensitivity.  Recall that the basic idea is to reduce the weight for elements from a <i><b>short head</b></i> .  The function responsible for the <i><b>short head</b></i> penalties for elements is denoted by <i><b>œÉ</b></i> .  We need such a function <i><b>œÉ</b></i> , which provides the maximum sensitivity: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/503/734/aa4/503734aa445c9495c0d5493ddc618955.png" height="80"></div><br>  Here <i><b>G</b></i> is the set of ways to segment an audience with the corresponding calculations of recommendations for segments. <br><br>  In a simple experiment that turned out to be effective, we used the step function as <i><b>œÉ</b></i> (sigmoid approximation), which simply zeroes the weights for the <i><b>short head</b></i> : <br><img src="https://habrastorage.org/getpro/habr/post_images/0f5/834/b85/0f5834b85684bcff6636b750a9f05dbd.png"><br><br>  This means that the optimization task: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/503/734/aa4/503734aa445c9495c0d5493ddc618955.png" height="60"></div><br>  need to be solved for the only parameter <i><b>œÉ</b></i> that determines the position of the step in the list of films sorted by the number of ratings.  For example, here‚Äôs what happened with our optimization: <br><img src="https://habrastorage.org/getpro/habr/post_images/ca5/670/ee6/ca5670ee67bdafb62c2bd4926c287a3d.png"><br><br>  On the left side - we penalize too many elements.  On the right - no one is fined.  In the optimal position (40 films fined), the sensitivity of the metric is improved by ~ 20%. <br><br><h1>  Summary: calculation formula </h1><br>  We present the final form of the metric, which takes into account all the above logic.  <i><b>AUC <sup>r</sup></b></i> metric which: <br><br><ul><li>  measures the quality of personal recommendation lists according to user evaluations made after the fact, as the probability that the recommendation will be successful ( <i><b>precision</b></i> ); </li><li>  sensitive to the order of the elements in the lists or, more precisely, discounted according to a simple behavioral model - viewing the recommendations of the user in turn and from top to bottom; </li><li>  takes into account the values ‚Äã‚Äãof positive assessments, therefore, can measure not only the <i>concentration of success</i> , but also the <i>amount of satisfaction</i> ; </li><li>  correctly handles the problem of <i><b>short head / long tail -</b></i> finishes the elements of <i><b>short head</b></i> in such a way as to optimize the sensitivity; </li></ul><br>  can be written as follows: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/65e/a7a/c54/65ea7ac543be9757543ab94d6facfb73.png" height="60"></div><br><ul><li>  <b>N <sub>rec</sub></b> - length of recommendation lists <br></li><li>  <b>z</b> is the limit for adding <i><b>precision</b></i> values.  For long lists, the <i><b>precision</b></i> values ‚Äã‚Äãare very small, as well as the <b>WN <sub>rec</sub></b> multiplier, so a large enough <b>z</b> does not affect the metric, and it‚Äôs easy to choose </li><li>  <b>| Users |</b>  - how many users are in the sample, </li><li>  <b>WN <sub>rec</sub></b> - an estimate of the probability that a random user will view a list of exactly <b>N <sub>rec</sub></b> length, <br></li><li>  <b>S <sub>Nrec</sub></b> - many successful ratings if the recommendation lists are of length <b>N <sub>rec</sub></b> , </li><li>  <b>œÉ (i)</b> is the value of the penalty function ( <b>short-head</b> ) for the element <b>i</b> , </li><li>  <b>r <sub>ui</sub></b> - the score that element <b>i</b> received from user <b>u</b> . </li></ul><br>  We note here another interesting point.  All users with their ratings are included in the formula for <i><b>AUC <sup>r</sup></b></i> equally.  However, we know that sometimes people can use the rating scale in different ways - some can give a lot of high points, others, on the contrary, rarely give high points.  This means that if we use the immediate values ‚Äã‚Äãof the estimates of <i><b>r</b></i> in the metric, the contribution from the first users will be higher due to the reduced contribution from the second.  Output - enter a normalized scale in which the estimates made by different types of users will be comparable to each other.  In practice, however, it turned out that when we work with a large sample, this effect can be neglected.  The main reason is that the type of use of the scale does not correlate with the taste of the person, so averaging over the audience, which occurs when calculating <i><b>AUC <sup>r</sup></b></i> , has the same effect as using a normalized scale. <br><br><h1>  Some examples from practice </h1><br>  Here are some practical examples of using metrics.  We used machine learning models of the recommendation system imhonet.ru, the details of which we omit here - we simply show the results of solving problems in which the target metric was <i><b>AUC <sup>r</sup></b></i> .  A particular problem for recommender systems is cold start tasks.  We have them of two types: <br><br><ul><li>  Newbie  We need to quickly begin to give him accurate recommendations. </li><li>  New  We need to quickly begin to accurately recommend the item to users. </li></ul><br>  The more people put the estimates, the more accurate his lists of recommendations.  Let us compare this dependence (red line) with the value of the metric, which gives the overall rating ( <i><b>AUC <sub>0</sub></b></i> , black line), and with the level when we give recommendations using only <b>gender and age</b> (orange line): <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/365/81f/be2/36581fbe2f3bd1b3ee939a450f4423fb.png"></div><br>  Information about gender and age gives about the same as 5 ratings. <br><br>  <b>Questionnaire.</b>  For an ordinary person, it is not always easy to formulate your preferences immediately in the form of assessments of the elements, so for beginners we offer to answer simple questions, such as ‚ÄúDo you like anime?‚Äù, ‚ÄúDo you like action?‚Äù.  For this purpose, we have brought several hundred such questions.  Let's see what their use can give: <br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/973/677/0cb/9736770cb800e95fd31e55a386754545.png"></div><br>  The questions are not so informative, but they are still beneficial, since it is much easier to answer than to put in numerical estimates.  For example, 10 ratings are equivalent to 25 responses. <br><br>  <b>Profile recommendations from another section.</b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It so happens that a person has already composed his profile from the ratings of the elements of another section. </font><font style="vertical-align: inherit;">If there is a connection between preferences in different sections, you can try to use it. </font><font style="vertical-align: inherit;">In our example, according to the evaluation profiles of art books, we are trying to make cinema recommendations:</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/67f/bd3/7a4/67fbd37a45ce22bfbcd7fb566d87d63a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It turns out worse than the use of questions, but you can fix a certain effect. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">New items </font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is very, very necessary to be able to start recommending new products before users start evaluating them. </font><font style="vertical-align: inherit;">It is clear that this can be done only on the basis of outgoing data about the film. </font><font style="vertical-align: inherit;">In our recommendation system, the properties of films that have a key influence on recommendations turned out to be:</font></font><br><br><ul><li>  Genres </li><li>  Directors </li><li>  Actors </li><li>  Screenwriters </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> This metadata allows you to start recommending a movie as if ‚Äúgood‚Äù users (not newbies) have already given him about 27 ratings: </font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/f47/59f/4e7/f4759f4e7d8c08c89aa0c092b4e0d50a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> That's not bad. </font></font><br><br>  Thanks for attention! </div><p>Source: <a href="https://habr.com/ru/post/281066/">https://habr.com/ru/post/281066/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../281056/index.html">RCNTEC launches third version of auth.as:token for iOS</a></li>
<li><a href="../281058/index.html">Kemerovo Hackathon, or "as the first sticker appeared on my laptop"</a></li>
<li><a href="../281060/index.html">Overview of the Central Bank: ATMs are less interested in fraudsters</a></li>
<li><a href="../281062/index.html">Release DBMS InterSystems Cach√© 2016.1</a></li>
<li><a href="../281064/index.html">Microsoft ‚ô• Open Source - the opening of a site dedicated to the history of friendship</a></li>
<li><a href="../281068/index.html">OpenStack implementation and where to go next?</a></li>
<li><a href="../281070/index.html">Process Monitor or how to make the software work as user</a></li>
<li><a href="../281072/index.html">Secure cleaning of private data</a></li>
<li><a href="../281074/index.html">What's new in the Azure cloud after Build? Announcement Handbook</a></li>
<li><a href="../281076/index.html">Computer, no pain - Vulnerability Checklist</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>