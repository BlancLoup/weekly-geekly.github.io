<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Depth cameras are a quiet revolution (when robots see) Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the first part of this text, we looked at depth cameras based on structural light and measuring round-trip light delays, which mainly use infrared ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Depth cameras are a quiet revolution (when robots see) Part 2</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/uy/lx/0t/uylx0tpbzxlqa5hnxqzjnriruoy.png"><br><br>  In the <a href="https://habr.com/ru/post/457524/">first part of</a> this text, we looked at depth cameras based on structural light and measuring round-trip light delays, which mainly use infrared lights.  They work well in premises at distances from 10 centimeters to 10 meters, and most importantly - they are very cheap.  Hence the massive wave of their current use in smartphones.  But ... As soon as we go outside, the sun even through the clouds illuminates the infrared illumination and their work deteriorates dramatically. <br><br>  As Steve Blank says ( <a href="https://www.youtube.com/watch%3Fv%3Da-J_SwmMJyo">on another occasion</a> , however): ‚ÄúIf you want success, leave the building.‚Äù  Below we will discuss the depth cameras that work outdoors.  Today, this topic is strongly moved by autonomous cars, but, as we will see, not only. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/659/8b4/4b2/6598b44b2d0a420dd83a434753cc6ffb.png"><br>  <i>Source: <a href="https://medium.com/syncedreview/innoviz-envisions-mass-produced-self-driving-cars-with-solid-state-lidar-448f44992f6c">Innoviz Envisions Mass Produced Self-Driving Cars With Solid State LiDAR</a></i> <br><br>  So, the depth cameras, i.e.  devices shooting video, each pixel of which is the distance to the object of the scene, working in sunlight! <br><br>  Who cares - welcome under the cat! <br><a name="habracut"></a><br>  Let's start with the eternal classics ... <br><br><h1>  Method 3: Depth from Stereo Camera + </h1><br>  The construction of the depth map from stereo is well known and has been used for <a href="https://scholar.google.ru/scholar%3Fq%3Dstereo%2Bdisparity%2Bmap%26hl%3Dru%26as_sdt%3D0%252C5%26as_ylo%3D%26as_yhi%3D1980">more than 40 years</a> .  Below is an example of a $ 450 compact camera that can be used to control gestures, along with professional photography or with VR helmets: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/594/0ab/3b0/5940ab3b0cbc72369d80083d8d32be57.png"><br>  <i><a href="https://www.stereolabs.com/">A source</a></i> <br><br>  The main advantage of such cameras is that sunlight not only does not interfere with them, but vice versa, makes their results better, and as a result, the active use of such cameras for all kinds of street cases, for example, here‚Äôs a great example of how to take a three-minute model of a three-year old fort: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/AFH2yN3rM78" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i><a href="https://www.youtube.com/watch%3Fv%3DAFH2yN3rM78">ZED outdoor camera example</a></i> <br><br>  Considerable money in the translation of the construction of depth from stereo to a new level was infused, of course, by the topic of autonomous cars.  Of the 5 considered methods of building video depth, only two - this and the next (stereo and plenoptic) do not interfere with the sun and do not interfere with neighboring cars.  At the same time, plenopticism is several times more expensive and less accurate at large distances.  It can take shape anyway, it is difficult to build forecasts, but in this case it is worth agreeing with Ilon Mask - the stereo of all 5 ways has the best prospects.  And the current results are quite encouraging: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/12e/cec/e3f/12ecece3f83fc654f64023e1391c38bb.png"><br>  <i>Source: <a href="https://www.youtube.com/watch%3Fv%3DoJt3Ln8H03s">Large-Scale Direct SLAM with Stereo Cameras</a></i> <br><br>  But it is interesting that, it seems, not unmanned vehicles (of which only a little is being produced so far) will have an even stronger influence on the development of the depth-building theme from stereo, where already the stereo depth map is being built, namely ... That's right!  Smartphones! <br><br>  About three years ago, the ‚Äútwo-eyed‚Äù smartphones boomed, in which literally all brands were noted, for the quality of photos taken with one camera and the quality of photos taken with two differed dramatically, but in terms of increasing the price of a smartphone, it was not so significant: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ab7/6b4/b3e/ab76b4b3e9696443e5191d293b59e663.png"><br><br>  Moreover, last year the process actively went even further: ‚ÄúDo you have 2 cameras in your smartphone?  Sucks!  I have <s>three</s> four !!! ": <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48f/905/554/48f905554fab15bcc6ce668c0c5f9ef7.png"><br>  <i>Source: <a href="https://www.samsung.com/global/galaxy/galaxy-a9/">Samsung Galaxy A8 &amp; A9</a></i> <br><br>  Sony's six-eyed future were mentioned in the first part.  In general, multi-eyed smartphones are gaining popularity among manufacturers. <br><br>  The fundamental reasons for this phenomenon are simple: <br><br><ul><li>  The resolution of the camera phones is growing, and the lens size is small.  As a result, despite numerous tricks, the noise level increases and the quality drops, especially when shooting in the dark. <br></li><li>  At the same time, in the second chamber we can remove the so-called <a href="https://ru.wikipedia.org/wiki/%25D0%25A4%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580_%25D0%2591%25D0%25B0%25D0%25B9%25D0%25B5%25D1%2580%25D0%25B0">Bayer filter</a> from the matrix, i.e.  One camera will be black and white, and the second color.  This increases the black and white sensitivity by about 3 times.  Those.  The sensitivity of 2 cameras grows conditionally not 2, but 4 times (!).  There are numerous nuances, but such an increase in sensitivity is really clearly seen by eye. <br></li><li>  Among other things, when a stereo pair appears, we have the opportunity to programmatically change the depth of field, i.e.  Blur the background, from which many photos significantly benefit (we wrote about it in the second half <a href="https://habr.com/ru/post/440652/">here</a> ).  This option of new smartphone models quickly became extremely popular. <br></li><li> With an increase in the number of cameras, it also becomes possible to use other lenses - more <a href="https://ru.wikipedia.org/wiki/%25D0%25A8%25D0%25B8%25D1%2580%25D0%25BE%25D0%25BA%25D0%25BE%25D1%2583%25D0%25B3%25D0%25BE%25D0%25BB%25D1%258C%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BE%25D0%25B1%25D1%258A%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D0%25B2">wide-angle</a> (short-focus), and, conversely, long-focus lenses, which make it possible to noticeably improve quality when ‚Äúapproaching‚Äù objects. <br></li><li>  Interestingly, the increase in the number of cameras brings us closer to the subject of a rarefied light field, whose mass of its features and advantages, however, is a separate story. <br></li><li>  Note also that the increase in the number of cameras allows you to increase the resolution resolution <a href="https://habr.com/ru/post/439766/">recovery</a> methods. <br></li></ul><br>  In general, there are so many advantages that when people become aware of them, they start to wonder why at least 2 cameras have not been installed for a long time. <br><br>  And here it turns out that not everything is so simple.  In order to meaningfully use additional cameras to improve the image (and not just switch to a camera with a different lens), we must build a so-called disparity map, which is directly converted into a depth map.  And this is a very nontrivial task, the solution to which the power of smartphones came just.  And even now, depth maps often have a rather dubious quality.  That is, you still need to live up to an exact comparison of the ‚Äúpixel in the right image to the pixel on the left‚Äù.  For example, here are real examples of depth maps for the iPhone: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/221/1f2/7d2/2211f27d2bfb2dc01da93559e8d1d8d1.png"><br>  <i>Source: <a href="">iPhone XS &amp; XR depth maps comparison</a></i> <br><br>  Even on the eye, massive problems are clearly visible on the background, on the borders, I am silent about <a href="http://videomatting.com/">translucent</a> hair.  From here - numerous problems that arise both when transferring color from a black and white camera to a color one, and during further processing. <br><br>  For example, here‚Äôs a pretty good example from the Apple‚Äôs ‚ÄúCreating Photo and Video Effects Using Depth‚Äù talk: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/83c/38f/efc/83c38fefc85d9e1ddef931b594e20469.png"><br>  <i>Source: <a href="https://devstreaming-cdn.apple.com/videos/wwdc/2018/503rfgg72ckqxj2fi/503/503_creating_photo_and_video_effects_using_depth.pdf%3Fdl%3D1">Creating Photo and Video Effects ‚Ä¢ Using Depth, Apple, WWDC18</a></i> <br><br>  You can clearly see how the depth is buggy under the hair, and indeed on any more or less uniform background, but more importantly, the collar went to the background on the right matting card (that is, it will be blurred).  Another problem is the real resolution of the depth and the matting map is significantly lower than the image resolution, which also affects the quality during processing: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/848/dcb/5cf/848dcb5cf62ea5991a85b23160fc1ce9.png"><br><br>  However, all this - the problem of growth.  If 4 years ago there was no talk of any serious effects on the video, the phone simply didn‚Äôt ‚Äúpull‚Äù them, then today the processing of video with depth is shown on top serial phones (it was in the same Apple presentation): <br><br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/481/5fe/6f8/4815fe6f8f9e7fd837a4e410df01a0da.png"><br>  <i>Source: <a href="https://devstreaming-cdn.apple.com/videos/wwdc/2018/503rfgg72ckqxj2fi/503/503_creating_photo_and_video_effects_using_depth.pdf%3Fdl%3D1">Creating Photo and Video Effects ‚Ä¢ Using Depth, Apple, WWDC18</a></i> <br><br>  In general, the topic of multi-cell phones, and, as a result, the theme of receiving depth from stereo on cell phones - conquers the masses without a fight: <br><br><img src="https://habrastorage.org/webt/t2/ba/sz/t2basz3pj6tskae0fqox5ol3x-q.png"><br>  <i>Source: "found in these your internet"</i> <br><br>  Key findings: <br><br><ul><li>  The depth of the stereo - in terms of the cost of equipment - the cheapest way to get the depth, because the cameras are now cheap and continue to fall rapidly.  The difficulty is that further processing is much more resource-intensive than for other methods. <br></li><li>  Mobile phones can not increase the diameter of the lens, while the resolution is growing rapidly.  As a result, the use of two or more cameras allows you to significantly improve the quality of photos, reduce noise in low light conditions, increase resolution.  Since today the mobile phone is often chosen for the quality of the camera, this is a very significant plus.  Building a depth map comes as an inconspicuous side bonus. <br></li><li>  The main disadvantages of building depth from stereo: <br><ul><li>  As soon as the texture disappears or simply becomes less contrasting, the noise in the depth increases sharply, as a result, on flat monochromatic objects the depth is often poorly used (serious errors are possible). <br></li><li>  Also, the depth is poorly defined on thin and small-sized objects (‚Äúcut off‚Äù fingers, or even hands, collapsed poles, etc.) <br></li></ul></li><li>  As soon as the power of iron allows you to build a depth map for video, the depth on smartphones will give a powerful impetus to the development of AR (at some point the quality of AR applications on all new phone models, including budget ones, will suddenly become noticeably higher and a new wave will go) .  Totally unexpected! <br></li></ul><br>  The next way is less trivial and well-known, but very cool.  Meet <br><br><h1>  Method 4: Light Field Camera Depth </h1><br>  The topic of plenopticians (from the Latin plenus is full and optikos is visual) or light fields are still relatively poorly known to the broad masses, although the professionals began to study it very closely.  At many top conferences, separate sections have been allocated to articles on Light Field (the author was once amazed at the number of Asian researchers at the IEEE International Conference on Multimedia and Expo who are closely involved in this topic). <br><br>  Google Trends say that the US, Australia are leading in terms of interest in Light Field, followed by Singapore and Korea.  Great Britain.  Russia is in 32nd place ... We will correct the gap with India and South Africa: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/651/993/87b/65199387b858541acc6ff99a129c2fe3.png"><br>  <i>Source: <a href="https://trends.google.com/trends/explore%3Fdate%3Dall%26q%3DLight%2520Field">Google Trends</a></i> <br><br>  Your humble servant some time ago did a <a href="https://habr.com/ru/post/440652/">detailed article on Habr√©</a> with a detailed description of how it works and what it gives, so let's go over briefly. <br><br>  The main idea is to try to fix at each point not just light, but a two-dimensional array of light rays, making each frame four-dimensional.  In practice, this is done using a microlens array: <br><br> <a href=""><img src="https://habrastorage.org/getpro/habr/post_images/90f/203/fb4/90f203fb49d7e464638a4f164e96fb26.gif"></a> <br>  <i>Source: <a href="">plenoptic.info (it is recommended to click and see in full resolution)</a></i> <br><br>  As a result, we have a <a href="https://habr.com/ru/post/440652/">lot of new features</a> , but the resolution is seriously falling.  Having solved a lot of complex technical problems, they drastically raised the resolution at <a href="https://en.wikipedia.org/wiki/Lytro">Lytro</a> (bought by Google), where in Lytro Cinema the resolution of the camera's sensor was brought to 755 megapixels of RAW data, and which looked cumbersome, like the first television cameras: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3ec/631/027/3ec6310271829f9272a4d3ef41462bde.png"><br>  <i>Source: <a href="https://www.hollywoodreporter.com/behind-screen/nab-lytro-cinema-light-field-885841">NAB: New Chandelier Light</a></i> <br><br>  Interestingly, even professionals regularly misinterpret the drop in resolution of plenoptic cameras, because they underestimate how well <a href="https://habr.com/ru/post/439766/">Super Resolution</a> algorithms can be worked out for them, really superbly restoring many of the details on the micro-shifts of the image in the light field (pay attention to the blurry needles and moving swings in the background) : <br><br><img width="200%" src="https://habrastorage.org/getpro/habr/post_images/bc9/6cd/e55/bc96cde55e276f4420757de5359467b1.gif"><br>  <i>Source: <a href="http://www.tgeorgiev.net/Superres.pdf">Naive, Smart and Super Resolution plenoptic frame recovery</a> from the Adobe Technical Report "Superresolution with Plenoptic Camera 2.0"</i> <br><br>  All of this would have been relatively theoretical interest, if Google in Pixel 2 had not implemented plenoptic, having <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">covered 2 pixels with a lens</a> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/852/c36/209/852c36209a7528f576e20dc37e0ef1f3.png"><br>  <i>Source: <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">AI Google Blog</a></i> <br><br>  As a result, a microstereopara formed, which made it possible to MEASURE the depth to which Google, faithful to new traditions, added neural networks, and it turned out generally wonderful: <br><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/429/442/99e/42944299e72b7eb720f3babc5a757b79.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/da3/fe1/3ea/da3fe13eacb3cd5b0a558db1d843b686.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/c6f/a27/c4b/c6fa27c4bac8b9c5cb122e0c69ab814d.png"><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/2ba/e97/e67/2bae97e67ae10cd150ef28ccdc6cc458.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/88a/159/34f/88a15934f1192deb0da889a630fed232.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/d7d/eab/811/d7deab81164e7e886b42a95173c58641.png"><br><br>  More depth examples in full resolution <a href="https://photos.google.com/share/AF1QipMzvQX22sGl5ESe-dN-bW7wl783MqawktwouIkw9MXoQJQLqEb5uE_A2tQjSrbzBg%3Fkey%3DemtnUkN1eEJfVzczdkFXOElMMzE2cDlNM3hQN2tR">in a special gallery</a> . <br><br>  Interestingly, the depth is stored by Google (like Huawei, and others) in the image itself, so you can extract it from there and see: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d63/958/b09/d63958b09a9584752a90f014adfadc24.png"><br>  <i>Source: <a href="https://www.makeuseof.com/tag/secret-features-googles-new-camera-app/">Google's New Secret Camera App.</a></i> <br><br>  And then you can turn the photo into three-dimensional: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/48b/3a5/e71/48b3a5e7156a14dcf60c600472154f6c.gif"><br>  <i>Source: <a href="https://www.makeuseof.com/tag/secret-features-googles-new-camera-app/">Google's New Secret Camera App.</a></i> <br><br>  You can experiment with this on the website <a href="http://depthy.me/">http://depthy.me</a> where you can upload your photo.  Interestingly, the site <a href="https://github.com/panrafal/depthy">is available in source code</a> , i.e.  You can improve the processing of depth, since there are many possibilities for this, now the simplest processing algorithms are used there. <br><br>  Key points: <br><br><ul><li>  At one of the Google conferences, it sounded that perhaps the lens would cover not 2, but 4 pixels.  This will reduce the direct resolution of the sensor, but it will drastically improve the depth map.  Firstly, due to the appearance of stereo pairs in two perpendicular directions, and secondly, due to the fact that the stereo base will increase by a factor of 1.4 (two diagonals).  This includes a marked improvement in depth accuracy at a distance. <br></li><li>  In itself plenoptic (it is also a calculated photo), it allows: <br><ul><li>  ‚ÄúHonestly‚Äù changing the focus and depth of field after shooting is the most well-known ability of plenoptic sensors. <br></li><li>  Calculate the shape of the diaphragm. <br></li><li>  Calculate scene lighting. <br></li><li>  Multiple shift point shooting, including getting a stereo (or multi-angle frame) with a single lens. <br></li><li>  Calculate the resolution, because using the Super Resolution algorithms that are heavy in computational complexity, you can actually restore the frame. <br></li><li>  Calculate a transparency map for translucent borders. <br></li><li>  And finally, build a depth map, which is important today. <br></li></ul></li><li>  Potentially, <b>when the main camera of the phone can simultaneously build a real-time quality depth map in parallel with the survey, this will create a revolution</b> .  This depends largely on the computing power on board (this is what is preventing us from doing better and better resolution of the depth map in real time).  This is most interesting for AR, of course, but there will be many opportunities to change photos. <br></li></ul><br>  And finally, we come to the last in this review method of measuring depth. <br><br><h1>  Method 5: Cameras on lidar technology </h1><br>  Generally, <a href="https://market.yandex.ru/catalog--lazernye-dalnomery/61099/list%3Fonstock%3D1%26local-offers-first%3D0">laser rangefinders are</a> firmly established in our lives, are inexpensive and give high accuracy.  The first lidars (from LIDaR - <a href="https://ru.wikipedia.org/wiki/%25D0%259B%25D0%25B8%25D0%25B4%25D0%25B0%25D1%2580">Light Identification Detection and Ranging</a> ), built as bundles of similar devices rotating around a horizontal axis, were first used by the military, then tested in the autopilot of the machines.  They performed quite well there, which caused a strong surge in investment in the region.  Initially, the lidars rotated, giving a similar pattern several times a second: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ed4/984/265/ed49842653721c8d7e6dae51c290166d.png"><br>  <i>Source: <a href="https://news.voyage.auto/an-introduction-to-lidar-the-key-self-driving-car-sensor-a7e405590cff">LIDAR: The Key Self-Driving Car Sensor</a></i> <br><br>  It was inconvenient, unreliable due to moving parts and rather expensive.  <a href="https://tonyseba.com/biography/">Tony Seb</a> in his lectures presents interesting data on the rate of decline in the cost of lidars.  If for the first stand-alone Google machines, the lidars cost 70 thousand dollars (for example, the specialized <a href="https://velodynelidar.com/hdl-64e.html">HDL-64E</a> used on the first machines cost 75 thousand): <br><img src="https://habrastorage.org/webt/w7/n4/4x/w7n44xwcg5gmcyi8v_dxc-yefd4.png"><br>  <i>Source: This and the Next <a href="https://cdn.ymaws.com/www.bellevuechamber.org/resource/resmgr/docs/annualdinner2017_tonyseba.pdf">From Parking to Parks ‚ÄìBellevue &amp; the Disruption of Transportation</a></i> <br><br>  When mass production of new models of the next generation, they are threatening to reduce the price significantly less than $ 1000: <br><img src="https://habrastorage.org/getpro/habr/post_images/178/fb1/712/178fb1712f85c4534ff25d11e54ad098.png"><br><br>  One can argue about Tony‚Äôs example (the promise of a startup is not the final cost), but what is booming in this area, the rapid increase in production runs, the emergence of completely new products and the general decline in prices is indisputable.  A little later, in 2017, the forecast for the fall in prices was as follows (and the moment of truth will come when they will start to be put into cars in large quantities): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d75/c6c/5b0/d75c6c5b04e509f698a5d967776ff68c.png"><br>  <i>Source: <a href="https://semiengineering.com/lidar-completes-sensing-triumvirate/">LiDAR Completes Sensing Triumvirate</a></i> <br><br>  In particular, relatively recently, several manufacturers have acquired the so-called <a href="https://www.digitaltrends.com/cars/solid-state-lidar-for-self-driving-cars/">Solid State Lidar</a> , which in principle have no moving parts, which show a radically higher reliability, especially when shaken, <a href="https://www.allaboutcircuits.com/news/solid-state-lidar-faster-cheaper-better/">a lower cost</a> , etc.  I recommend to watch this video, where their device is explained in 84 seconds very clearly: <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/yQi7J0ehKAA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Source: <a href="https://www.youtube.com/watch%3Fv%3DyQi7J0ehKAA">Solid State Lidar Sensor</a></i> <br><br>  What is important for us is that Solid State Lidar gives a rectangular picture, i.e.  in essence, it starts working like a ‚Äúregular‚Äù depth camera: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1f6/90e/e65/1f690ee65f60f431037d6c9191896005.png"><br>  <i>Source: <a href="https://medium.com/syncedreview/innoviz-envisions-mass-produced-self-driving-cars-with-solid-state-lidar-448f44992f6c">Innoviz Envisions Mass Produced Self-Driving Cars With Solid State LiDAR</a></i> <br><br>  The example above gives a video of approximately 1024x256, 25 FPS, 12 bits per component.  Such lidars will be mounted under the hood grill (since the device heats well): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/18b/8dc/a13/18b8dca13a096a2337ad38b5728b26ea.png"><br>  <i>Source: <a href="https://www.autonews.com/awards/2018-finalist-magna-electronics-solid-state-lidar">Solid-State LiDAR Magna Electronics</a></i> <br><br>  As usual, the Chinese, who are today in the first place in the world for the production of electric cars and who obviously aim at the first place in the world in autonomous cars, are lighting up: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c59/b26/b00/c59b26b009b05ce5a872f3203c720a3d.png"><br>  <i>Source: <a href="https://www.electronicsweekly.com/uncategorised/528965-2018-06/">Alibaba, RoboSense launch unmanned vehicle using solid-state LIDAR</a></i> <br><br>  In particular, their experiments with non-square ‚Äúpixel‚Äù depths are interesting, if you do joint processing with an RGB camera, it is possible to raise the resolution and this is a rather interesting compromise (‚Äúsquareness‚Äù of pixels is important, in fact, only for humans): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7e9/13f/af2/7e913faf27197e1b893b56f13873e1d6.png"><br>  <i>Source: <a href="https://www.eejournal.com/article/mems-lidar-for-driverless-vehicles-takes-another-big-step/">MEMS Lidar for Driverless Vehicles Takes Another Big Step</a></i> <br><br>  Lidars are mounted in different schemes, depending on the cost of the kit and the power of the on-board system, which will need to process all this data.  The overall characteristics of the autopilot change accordingly.  As a result, more expensive cars will better tolerate dusty roads and it is easier to ‚Äúdodge‚Äù cars entering at intersections at the side, while cheap ones will only help reduce the number of (numerous) stupid accidents in traffic jams: <br><br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/00d/dd1/d4e/00ddd1d4e9f980216796306e4a62fe2e.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/025/548/1df/0255481dfb9f5dcd102a9a5979ca2fb8.png"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/7bf/842/4bf/7bf8424bf82edc861dc15434a03bfcbe.png"><br>  <i>Source: <a href="https://www.robosense.ai/rslidar/rs-lidar-M1">Description RoboSense RS-LiDAR-M1</a></i> <br><br>  Note that, apart from Solid-State, low prices promise a couple more directions in which lidars are developing.  Something to predict here is a thankless task, since too much will not depend on the potential engineering characteristics of the technology, but, for example, on patents.  Just Solid-State is already engaged in several companies, so the topic looks the most promising.  But not to say about the rest would be unfair: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3f9/3aa/265/3f93aa2653a9dde4393c007b925d07a2.png"><br>  <i>Source: <a href="https://driverless.wonderhowto.com/news/2017-lidar-bottleneck-is-causing-modern-day-gold-rush-0177282/">This 2017 LiDAR Bottleneck Is Causing a Modern-Day Gold Rush</a></i> <br><br>  If we talk about lidars as cameras, it‚Äôs worth mentioning another important feature that is essential when using solid-state lidars.  They by their nature work like cameras with an already forgotten <a href="https://en.wikipedia.org/wiki/Rolling_shutter">running shutter</a> , which makes noticeable distortions when shooting moving objects: <br><br><img width="66%" src="https://habrastorage.org/getpro/habr/post_images/b83/2e7/2a8/b832e72a8f8c67c65401da694df03c8e.gif"><br>  <i>Source: <a href="https://www.cameraiq.ru/support/faq/2916">What is the difference between the global (global) shutter and the running (rolling)</a></i> <br><br>  Considering that on the road, especially on the autobahn at a speed of 150 km / h, everything changes pretty quickly, this feature of the lidars will distort objects, including fast flying towards us ... Including in depth ... Note that the two previous methods There is no such problem. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9b9/15b/979/9b915b9799869f23dcf7b2b3e1f5f1ca.gif"><br>  <i>Source: <a href="">good animation on wikipedia, which shows the distortion of the car</a></i> <br><br>  This feature, coupled with low FPS, requires the adaptation of processing algorithms, but in any case, due to high accuracy, including at large distances, lidars have no special competitors. <br><br>  Interestingly, by their nature, lidars work very well on even areas and worse on borders, and stereo sensors do poorly on even areas and relatively well on borders.  In addition, the lidars give a relatively small FPS, and the cameras are much larger.  As a result, they essentially complement each other, which was used, among other things, in the Lytro Cinema cameras (in the photo above the camera there is an illuminated lens of plenoptics, giving up to 300 FPS, and below is <s>Malevich‚Äôs</s> black square lidar): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5c1/eb9/ede/5c1eb9edeca7d67133487e93290435cd.png"><br>  <i>Source: <a href="https://www.dpreview.com/news/6720444400/lytro-cinema-impresses-large-crowd-with-prototype-755mp-light-field-video-camera-at-nab">Lytro poised to forever change filmmaking: debuts Cinema prototype and short film at NAB</a></i> <br><br>  If we already combined two depth sensors in a movie camera, then in other devices (from smartphones to cars) we can expect the mass distribution of hybrid sensors that provide maximum quality. <br><br>  In a sense, lidars are still about investments that in the previous three years amounted to about 1.5 billion dollars (since this market is valued at <a href="https://www.autonomousvehicletech.com/articles/1768-lidar-market-to-reach-10-billion-by-2025">10 billion in 6 years</a> and the graph below clearly shows how the average transaction size grew): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e07/fd6/3ae/e07fd63aee838e9a5c8958b0527ac4e9.png"><br>  <i>Source: <a href="https://techcrunch.com/2019/03/09/transportation-weekly-waymo-unleashes-laser-bear-bird-spreads-its-wings-lyft-tightens-its-belt/">An interesting March review of trends in the lidar market from Techcrunch</a></i> <br><br>  The development cycle of an innovative product even in highly competitive markets is 1.5-2 years, so soon we will see extremely interesting products.  In the form of prototypes they already have. <br><br>  Key points: <br><br><ul><li>  Pluses lidar: <br><ul><li>  the highest accuracy, including the best among all methods at large distances, <br></li><li>  work best on flat surfaces on which the stereo stops working, <br></li><li>  almost do not shine the sun <br></li></ul></li><li>  Cons lidar: <br><ul><li>  high power consumption <br></li><li>  relatively low frame rate <br></li><li>  running shutter and the need to compensate for it during processing <br></li><li>  Lidars working alongside create interference to each other, which is not so easy to compensate. <br></li></ul></li><li>  And nevertheless, we are seeing how literally in 2 years a new type of depth cameras has appeared with excellent prospects and a huge potential market.  In the coming years, we can expect a serious decline in prices, including the emergence of small universal lidars for industrial robots. <br></li></ul><br><h1>  Processing video with depth </h1><br>  In a nutshell, consider why depth is not so easy to handle. <br><br>  Below is an example of raw depth data from a very nice line of <a href="https://www.intelrealsense.com/">Intel RealSense cameras</a> .  Fingers on this video can be relatively easy to see the work of the camera and processing algorithms: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6b2/319/b14/6b2319b143f48bdf0c57ffbe2acbc891.png"><br>  <i>Source: hereinafter - the author's materials</i> <br><br>  The typical problems of depth cameras are clearly visible: <br><br><ul><li>  The data is unstable in terms of values, the pixels are ‚Äúnoisy‚Äù in depth. <br></li><li>  Along the borders for a fairly large width, the data is also very noisy (for this reason, depth values ‚Äã‚Äãalong the borders are often simply masked so as not to interfere with operation). <br></li><li>  A lot of ‚Äúwalking‚Äù pixels can be seen around the head (apparently, proximity to the wall from behind + hair begins to influence). <br></li></ul><br>  But that is not all.  If we take a closer look at the data, it can be seen that in some places deeper data ‚Äúsee through‚Äù through closer ones: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a30/c5a/911/a30c5a911123f7a8f6ff8ebd8893163a.png"><br><br>  This is because for convenience of processing the image is reduced to an image from an RGB camera, and since the borders are not precisely determined, it becomes necessary to specially process such ‚Äúoverlaps‚Äù, otherwise problems arise, such as such ‚Äúholes‚Äù in depth on the arm: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/22f/1a3/5df/22f1a35dfdb61edcabe8a1675f5c855f.png"><br><br>  When processing, there are a lot of problems, for example: <br><br><ul><li>  "Flowing" depth from one object to another. <br></li><li>  The instability of the depth in time. <br></li><li>  Aliasing (the formation of a "ladder"), when the suppression of noise in depth leads to the discretization of depth values: <br></li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/e3d/7a4/c10/e3d7a4c10e13824f1d79815661e17773.png"><br><br>  Nevertheless, it is clear that the picture has improved greatly.  Note that slower algorithms can still significantly improve the quality. <br><br>  In general, video depth processing ( <a href="https://scholar.google.ru/scholar%3Fq%3Ddepth%2B3d%2B%2522video%2Bprocessing%2522">depth video processing</a> ) is a separate huge topic that not many people are engaged in so far, but which is rapidly gaining popularity.  The previous market, which she completely changed, is the conversion of films from 2D to 3D.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The result of manually building stereo on 2D video improved so quickly, showed so much more predictable results, caused so much less headache and less cost that in the 3D movie, after almost 100 years of shooting (!), Pretty quickly almost completely ceased to shoot and only started to convert . There was even a separate specialty - </font></font><a href="https://www.google.com/search%3Fq%3D%2522depth%2Bartist%2522"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">depth artist</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and the </font><font style="vertical-align: inherit;">old-school </font></font><a href="https://en.wikipedia.org/wiki/Stereographer"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">stereographers</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> were shocked (‚Äúka-ak ta-ak?‚Äù). And the key role in this revolution was played by the rapid improvement of video processing algorithms. Perhaps, somehow I will take the time to write about this separate series of articles, because my material is in large quantities. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Approximately the same can be expected in the near future for cameras depth robots, smartphones and autonomous cars.</font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Wake up! </font><font style="vertical-align: inherit;">Revolution is coming!</font></font></b> <br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Instead of conclusion </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A couple of years ago, your humble servant brought a comparison of different approaches to obtaining video with depth into a single tablet. </font><font style="vertical-align: inherit;">In general, during this time the situation has not changed. </font><font style="vertical-align: inherit;">The division here is rather arbitrary, since the manufacturers are well aware of the disadvantages of the technologies and try to compensate for them (sometimes very successfully). </font><font style="vertical-align: inherit;">Nevertheless, in general, it can be compared to different approaches: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/34d/491/11f/34d49111ff8005f743a89d9078864f1b.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: materials of the author</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Let </font><i><font style="vertical-align: inherit;">'s</font></i><font style="vertical-align: inherit;"> run through the table:</font></font><br><br><ul><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">According to the resolution, the</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> depth from the stereo is in the lead, but everything very much depends on the scene (if it is monotonous, the case is a pipe).</font></font><br></li><li> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In terms of accuracy</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , the lidars are out of competition, the situation is worse for plenopticians, by the way.</font></font><br></li><li> <b>  </b> ‚Äî ¬´¬ª    ToF  ,            . <br></li><li> <b> FPS</b>   ToF     ,   ,    (   300 fps).      . <br></li><li> <b>     </b>     . <br></li><li> <b>   </b> ‚Äî   ToF    . <br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is important to understand the situation in specific markets. For example, the plenoptic approach looks clearly lagging behind. However, if it turns out to be the best for AR (and this will become clear in the next 3-4 years), then it will take its rightful place in </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">every</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> phone and tablet. It is also clear that solid-state lidars look best of all (the ‚Äúgreenest‚Äù ones), but this is also the youngest technology, and these are their own risks, first of all patent (in the area of ‚Äã‚Äãa huge number of fresh patents that will not expire soon): </font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/94e/905/f90/94e905f90cdd1e525f148f983c557b00.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source : </font></font><a href="https://www.henrypatentfirm.com/blog/lidar-patent-landscape"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">LiDAR: An Overview of the Patent Landscape</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Nevertheless, we don‚Äôt forget - the market of depth sensors for smartphones is planned to be </font><a href="https://press.trendforce.com/node/view/3226.html"><font style="vertical-align: inherit;">worth $ 6 billion</font></a><font style="vertical-align: inherit;"> next year</font></font><a href="https://press.trendforce.com/node/view/3226.html"><font style="vertical-align: inherit;"></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">and less than 4 it can not be, because last year amounted to more than 3 billion. </font><font style="vertical-align: inherit;">This is a huge amount of money that will not only go to return on investment (the most successful investors), but also to develop new generations of sensors. </font><font style="vertical-align: inherit;">There is no similar growth of lidars yet, but by all indications it will go literally in the next 3 years. </font><font style="vertical-align: inherit;">And then it is the usual avalanche-like exponential process, which have already happened more than once. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The near future of depth cameras looks extremely interesting! </font></font><br><br> <b><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Carthage must be destroyed ...</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> All video until the end of the century will be three-dimensional! </font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stay tuned!</font></font><br><br>  <a href="https://habr.com/ru/post/457524/">Part 1</a> <br><br><div class="spoiler">  <b class="spoiler_title">Acknowledgments</b> <div class="spoiler_text">    : <br><ul><li>      . ..           , <br></li><li>  Google, Apple  Intel    ,       , <br></li><li>   ,      ,       , <br></li><li> , ,    ,            ,     ! <br></li></ul><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/458458/">https://habr.com/ru/post/458458/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458444/index.html">Internet for summer resident. Part 4. One SIM card is enough</a></li>
<li><a href="../458446/index.html">Hyperscale data centers: who builds them and how much they cost</a></li>
<li><a href="../45845/index.html">Three conversations about blog advertising</a></li>
<li><a href="../458450/index.html">Characteristics of quantum computers</a></li>
<li><a href="../458452/index.html">How to make office kitchen through a grocery approach</a></li>
<li><a href="../45846/index.html">Project Management with Unfuddle</a></li>
<li><a href="../458460/index.html">News from the world of OpenStreetMap ‚Ññ 466 (06/18/2019-24.06.2019)</a></li>
<li><a href="../458470/index.html">Texturing, or what you need to know to become an artist on surfaces. Part 2. Masks and Textures</a></li>
<li><a href="../458472/index.html">Rating of territories by the method of thermal potentials using open data</a></li>
<li><a href="../458474/index.html">The best reports from HighLoad ++ 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>