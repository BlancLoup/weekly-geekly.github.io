<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>#PostgreSQL. We accelerate the deployment seven times with the help of "multithreads"</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello! We at the GIS utilities project use PostgreSQL and recently faced the problem of long execution of SQL scripts due to the rapid increase in dat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>#PostgreSQL. We accelerate the deployment seven times with the help of "multithreads"</h1><div class="post__text post__text-html js-mediator-article">  Hello!  We at the GIS utilities project use PostgreSQL and recently faced the problem of long execution of SQL scripts due to the rapid increase in data in the database.  In February 2018, at PGConf, I told how we solved this problem.  Presentation slides are available <a href="http://pgconf.ru/2018/110657">on the conference website</a> .  I offer you the text of my speech. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/rg/ve/zt/rgveztkqdkc00bfkfb9m3_y21f8.jpeg" width="600"></div><br><a name="habracut"></a><br><h2>  Given </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cj/xf/5-/cjxf5-fcu37vt9i5m5m3mtbneai.jpeg" width="600"></div><br>  About GIS utilities has already been a detailed <a href="https://habrahabr.ru/company/lanit/blog/321476/">article</a> in the blog group LANIT Habr√©.  If in a nutshell, GIS utilities is Russia's first federal portal about all the information in the housing and utilities sector, which is launched in almost all regions (in 2019, Moscow, St. Petersburg and Sevastopol will join).  Over the past three months, more than 12 TB of data about houses, personal accounts, payment facts and a lot of other things have been loaded into the system, and now more than 24 TB are already in PostgreSQL. <br><br>  The project is architecturally divided into subsystems.  Each subsystem is allocated a separate database.  In total, there are about 60 such databases, they are located on 11 virtual servers.  Some subsystems are heavier than others, and their bases can take 3-6 terabytes in volume. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  MCC, we have a problem </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/0g/nb/wr/0gnbwrnkg75ual8ugxr1wgno3tq.jpeg" width="600"></div><br><br>  Now a little more talk about the problem.  I'll start from afar: we have application code and database migration code (by migration I mean transferring a database from one revision to another with all the necessary SQL scripts for this) are stored together in the version control system.  This is possible due to the use of Liquibase (for more information about Liquibase on the project, see Misha Balayan‚Äôs report on <a href="https://www.meetup.com/ru-RU/techguruday/%3F_cookie-check%3Danm4Cy64X9QDYNf1">TechGuruDay</a> in LANIT). <br><br>  Now let's imagine a release version.  When data is only a couple of terabytes or less and all tables are within a hundred gigabytes, changes (migrations) of any data or structure changes in any tables pass quickly (usually). <br><br>  And now let's imagine that we already have a couple of tens of terabytes of data and several tables of terabyte and more appeared (possibly divided into partitions).  In the new version, we need to migrate to one of these tables, or even worse all at once.  And while the time of maintenance work can not be increased.  And at the same time, the same migration should be done on test databases, where iron is weaker.  And at the same time, it is necessary to understand in advance how much all migrations will take in time.  This is where the problem begins. <br><br>  First, we tried <a href="https://www.postgresql.org/docs/9.6/static/populate.html">tips</a> from the official PostgreSQL documentation (removing indexes and FK before mass migration, re-creating tables from scratch, using copy, dynamically changing the config).  This had an effect, but we wanted to be even faster and more convenient (here, of course, it is a subjective matter - as it is convenient for anyone :‚Äì)).  As a result, we implemented parallel execution of mass migrations, which increased the speed on many cases by several times (and sometimes by an order of magnitude).  Although in fact several processes are launched in parallel, we have got the word ‚Äúmultithread‚Äù inside the command. <br><br><h2>  "Multithread" </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ii/fr/g6/iifrg6tzb-bzv8vdo7ogkgxy_4e.jpeg"></div><br>  The main idea of ‚Äã‚Äãthis approach is to divide a large table into disjoint ranges (for example, using the ntile function) and execute the SQL script not all data at once, but parallel across several ranges.  Each parallel process takes one range for itself, blocks it and starts executing a SQL script only for data from this range.  As soon as the script has completed, we again look for the unlocked and not yet processed range and repeat the operation.  It is important to choose the right key to separate.  This must be an indexed field with unique values.  If there is no such field, you can use the ctid service field. <br><br>  The first version of the "multithreads" was implemented using an auxiliary table with ranges and taking functions of the next range.  The required SQL script was substituted into an anonymous function and started in the required number of sessions, providing parallel execution. <br><br><div class="spoiler">  <b class="spoiler_title">Code example</b> <div class="spoiler_text"><pre><code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">--  UPDATE_INFO_STEPS    / --  ,    / CREATE TABLE UPDATE_INFO_STEPS ( BEGIN_GUID varchar(36), END_GUID varchar(36) NOT NULL, STEP_NO int, STATUS char(1), BEGIN_UPD timestamp, END_UPD timestamp, ROWS_UPDATED int, ROWS_UPDATED_TEXT varchar(30), DISCR varchar(10) ); ALTER TABLE UPDATE_INFO_STEPS ADD PRIMARY KEY(discr, step_no); --  FUNC_UPDATE_INFO_STEPS   . --  ""  ,   . CREATE OR REPLACE FUNCTION func_update_info_steps( pStep_no int, pDiscr varchar(10) ) RETURNS text AS $BODY$ DECLARE lResult text; BEGIN SELECT 'SUCCESS' INTO lResult FROM update_info_steps WHERE step_no = pStep_no AND discr = pDiscr AND status = 'N' FOR UPDATE NOWAIT; UPDATE UPDATE_INFO_STEPS SET status = 'A', begin_upd = now() WHERE step_no = pStep_no AND discr = pDiscr AND status = 'N'; return lResult; EXCEPTION WHEN lock_not_available THEN SELECT 'ERROR' INTO lResult; return lResult; END; $BODY$ LANGUAGE PLPGSQL VOLATILE; --   (1   1 ) --  1.      . DO LANGUAGE PLPGSQL $$ DECLARE --        l_count int := 10000; --   l_discr VARCHAR(10) := '&lt;discr&gt;'; BEGIN INSERT INTO UPDATE_INFO_STEPS ( BEGIN_GUID, END_GUID, STEP_NO, STATUS, DISCR ) SELECT min(guid) BEGIN_GUID, max(guid) END_GUID, RES2.STEP STEP_NO, 'N' :: char(1) STATUS, l_discr DISCR FROM ( SELECT guid, floor( (ROWNUM - 1) / l_count ) + 1 AS STEP FROM ( --    SELECT &lt;column&gt; AS GUID, --    row_number() over ( ORDER BY &lt;column&gt; ) AS ROWNUM FROM --      &lt;schema&gt;.&lt;table_name&gt; ORDER BY 1 -- ) RES1 ) RES2 GROUP BY RES2.step; END; $$; --  2.   ,   UPDATE. DO LANGUAGE PLPGSQL $$ DECLARE cur record; vCount int; vCount_text varchar(30); vCurStatus char(1); vCurUpdDate date; --   l_discr varchar(10) := '&lt;discr&gt;'; l_upd_res varchar(100); BEGIN FOR cur IN ( SELECT * FROM UPDATE_INFO_STEPS WHERE status = 'N' AND DISCR = l_discr ORDER BY step_no ) LOOP vCount := 0; --   ! SELECT result INTO l_upd_res FROM dblink( '&lt;parameters&gt;', 'SELECT FUNC_UPDATE_INFO_STEPS(' || cur.step_no || ',''' || l_discr || ''')' ) AS T (result text); IF l_upd_res = 'SUCCESS' THEN --  .      --    ,   . --   -   -- cur.begin_guid - cur.end_guid  dblink  " ". --   . SELECT dblink( '&lt;parameters&gt;', 'UPDATE FOO set level = 42 WHERE id BETWEEN ''' || cur.begin_guid || ''' AND ''' || cur.end_guid || '''' ) INTO vCount_text; --   . SELECT dblink( '&lt;parameters&gt;', 'update UPDATE_INFO_STEPS SET status = ''P'', end_upd = now(), rows_updated_text = ''' || vCount_text || ''' WHERE step_no = ' || cur.step_no || ' AND discr = ''' || l_discr || '''' ) INTO l_upd_res; END IF; END LOOP; END; $$; --  . SELECT SUM(CASE status WHEN 'P' THEN 1 ELSE 0 END) done, SUM(CASE status WHEN 'A' THEN 1 ELSE 0 END) processing, SUM(CASE status WHEN 'N' THEN 1 ELSE 0 END) LEFT_, round( SUM(CASE status WHEN 'P' THEN 1 ELSE 0 END):: numeric / COUNT(*)* 100 :: numeric, 2 ) done_proc FROM UPDATE_INFO_STEPS WHERE discr = '&lt;discr&gt;';</span></span></code> </pre> </div></div><br>  This approach, although it worked quickly, but required a very large number of actions by hand.  And if the deployment took place at 3 o'clock in the morning, the DBA had to catch the moment of the execution of the ‚Äúmulti-threaded‚Äù script in Liquibase (which performed it, in fact, in one process) and run some more processes to accelerate it. <br><br><h2>  ‚ÄúMultipoint 2.0‚Äù </h2><br><div style="text-align:center;"> <a href=""><img src="https://habrastorage.org/webt/_k/8z/vs/_k8zvswxowmvgu7yibu56-fzrxk.jpeg"></a> </div><br>  The previous version of the multithread was inconvenient to use.  Therefore, we made an application on Go that automates the process (it can be done in Python, for example, and in many other languages). <br><br>  First, we break the data in the variable table into ranges.  After that, we add information about the script to the auxiliary table of tasks - its name (a unique identifier, for example, the name of the task in Jira) and the number of simultaneously launched processes.  Then we add the text of the SQL migration divided into ranges into the auxiliary script table. <br><br><div class="spoiler">  <b class="spoiler_title">Code example</b> <div class="spoiler_text"><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">--      ,     --    (pg_parallel_task) --    (pg_parallel_task_statements). CREATE TABLE IF NOT EXISTS public.pg_parallel_task ( name text primary key, threads_count int not null DEFAULT 10, comment text ); COMMENT ON table public.pg_parallel_task IS '  '; COMMENT ON COLUMN public.pg_parallel_task.name IS ' '; COMMENT ON COLUMN public.pg_parallel_task.threads_count IS '   .   10'; COMMENT ON COLUMN public.pg_parallel_task.comment IS ''; CREATE TABLE IF NOT EXISTS public.pg_parallel_task_statements ( statement_id bigserial primary key, task_name text not null references public.pg_parallel_task (name), sql_statement text not null, status text not null check ( status in ( 'new', 'in progress', 'ok', 'error' ) ) DEFAULT 'new', start_time timestamp without time zone, elapsed_sec float(8), rows_affected bigint, err text ); COMMENT ON table public.pg_parallel_task_statements IS '  '; COMMENT ON COLUMN public.pg_parallel_task_statements.sql_statement IS '   '; COMMENT ON COLUMN public.pg_parallel_task_statements.status IS '   .   new|in progress|ok|error'; COMMENT ON COLUMN public.pg_parallel_task_statements.start_time IS '    '; COMMENT ON COLUMN public.pg_parallel_task_statements.elapsed_sec IS '  ,    '; COMMENT ON COLUMN public.pg_parallel_task_statements.rows_affected IS '  ,   '; COMMENT ON COLUMN public.pg_parallel_task_statements.err IS '  ,  . NULL,   .'; --   INSERT INTO PUBLIC.pg_parallel_task (NAME, threads_count) VALUES ('JIRA-001', 10); INSERT INTO PUBLIC.pg_parallel_task_statements (task_name, sql_statement) SELECT 'JIRA-001' task_name, FORMAT( 'UPDATE FOO SET level = 42 where id &gt;= ''%s'' and id &lt;= ''%s''', MIN(d.id), MAX(d.id) ) sql_statement FROM ( SELECT id, NTILE(10) OVER ( ORDER BY id ) part FROM foo ) d GROUP BY d.part; --   </span></span></code> </pre></div></div><br>  During the deployment, an application is invoked on Go, which reads the configuration of the task and the scripts for this task from the auxiliary tables and automatically runs the scripts with a specified number of parallel processes (workers).  After execution, control is transferred back to Liquibase. <br><br><div class="spoiler">  <b class="spoiler_title">Code</b> <div class="spoiler_text"><pre> <code class="sql hljs">&lt;changeSet id="JIRA-001" author="soldatov"&gt; &lt;executeCommand os="Linux, Mac OS X" executable="./pgpar.sh"&gt; &lt;arg value="testdatabase"/&gt;&lt;arg value="JIRA-001"/&gt; &lt;/executeCommand&gt; &lt;/changeSet&gt;</code> </pre> </div></div><br>  The application consists of three main abstractions: <br><br><ul><li>  task - loads the migration parameters, the number of processes and all ranges into memory, starts the ‚Äúmultithread‚Äù and raises the Web ‚Äì server to monitor the progress of the execution; </li><li>  statement - represents one range of the operation being performed, is also responsible for changing the status of the range execution, recording the range execution time, the number of lines in the range, etc .; </li><li>  worker is a single execution thread. </li></ul><br>  <u>In the task.do method</u> , a channel is created to which all statements statements are sent.  This channel starts the specified number of workers.  Inside workers, there is an infinite loop, it multiplexes on two channels: by which it gets the statements and executes them, and the empty channel as a signaling device?  that need to be completed.  As soon as the empty channel is closed, the worker will shut down - this happens when an error occurs in one of the workers.  Since  the channels in Go are a thread ‚Äì safe structure, then by closing one channel we can cancel all workers at once.  When the statement in the pipe ends, the worker will simply exit the loop and reduce the total counter for all workers.  Since the task always knows how many workers it works on, it simply waits for this counter to be reset and after that it completes itself. <br><br><h2>  Buns </h2><br> <a href=""><img src="https://habrastorage.org/webt/8v/h-/am/8vh-amlk95wkk0gbg63vbzsvkxu.png"></a> <br><br>  Due to such a multithread implementation, several interesting features appeared: <br><br><ul><li>  Integration with Liquibase (called with the executeCommand tag). </li><li>  a simple web interface that appears when you start a ‚Äúmulti-thread‚Äù and contains all the information about its progress. </li><li>  Progress bar (we know how much one range is processed, how many parallel processes are running and how many ranges are left to process - so we can calculate the completion time). </li><li>  Dynamic change of parallel processes (while we do it by hand, but in the future we want to automate). </li><li>  Logging information on the execution of multithreaded scripts for further analysis. </li><li>  You can perform blocking operations like update, blocking almost nothing (if you break the tablet into very small ranges, all scripts will be executed almost instantly). </li><li>  There is a wrapper for calling a ‚Äúmulti-threaded‚Äù directly from the database. </li></ul><br><h2>  No buns </h2><br>  The main disadvantage is the need to go full time through the plate to split it into ranges, if the text field, date or uid is used as the key.  If the key for splitting is a field with successively increasing dense values, then there is no such problem (we can specify all the ranges in advance simply by specifying the required step). <br><br><h2>  Accelerate seven times (test on pgbench table) </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/s0/4s/ve/s04svevukcydpdmbn2i8338nq60.jpeg" width="600"></div><br>  Finally, I will give an example of comparing the speed of an UPDATE operation of 500,000,000 rows without using a multi-threaded one and with it.  A simple UPDATE was performed for 49 minutes, while the ‚Äúmultithread‚Äù was completed in seven minutes. <br><br><div class="spoiler">  <b class="spoiler_title">Code example</b> <div class="spoiler_text"><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">count</span></span>(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pgbench_accounts; count <span class="hljs-comment"><span class="hljs-comment">------- 500000000 (1 row) SELECT pg_size_pretty(pg_total_relation_size('pgbench_accounts')); pg_size_pretty ---------------- 62 Gb (1 row) UPDATE pgbench_accounts SET abalance = 42; --   49  vacuum full analyze verbose pgbench_accounts; INSERT INTO public.pg_parallel_tASk (name, threads_count) values ('JIRA-002', 25); INSERT INTO public.pg_parallel_tASk_statements (tASk_name, sql_statement) SELECT 'JIRA-002' tASk_name, FORMAT('UPDATE pgbench_accounts SET abalance = 42 WHERE aid &gt;= ''%s'' AND aid &lt;= ''%s'';', MIN(d.aid), MAX(d.aid)) sql_statement FROM (SELECT aid, ntile(25) over (order by aid) part FROM pgbench_accounts) d GROUP BY d.part; --   10  --    ctid,               INSERT INTO public.pg_parallel_tASk_statements (tASk_name, sql_statement) SELECT 'JIRA-002-ctid' tASk_name, FORMAT('UPDATE pgbench_accounts SET abalance = 45 WHERE (ctid::text::point)[0]::text &gt; ''%s'' AND (ctid::text::point)[0]::text &lt;= ''%s'';', (d.min_ctid), (d.max_ctid)) sql_statement FROM ( WITH max_ctid AS ( SELECT MAX((ctid::text::point)[0]::int) FROM pgbench_accounts) SELECT generate_series - (SELECT max / 25 FROM max_ctid) AS min_ctid, generate_series AS max_ctid FROM generate_series((SELECT max / 25 FROM max_ctid), (SELECT max FROM max_ctid), (SELECT max / 25 FROM max_ctid))) d; --   9  ./pgpar-linux-amd64 jdbc:postgresql://localhost:5432 soldatov password testdatabase JIRA-002 --   7 </span></span></code> </pre> <br></div></div><br><h2>  PS You need it if: </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qo/pe/us/qopeustewmnxaghs1f6lf5zcwhm.jpeg" width="600"></div><br>  All tools are good for certain tasks, and here are a few for multithreading. <br><br><ul><li>  UPDATE tables&gt; 100,000 rows. </li><li>  UPDATE with complex logic that can be parallelized (for example, calling functions to evaluate something). </li><li>  UPDATE without locks.  By crushing into very small ranges and starting a small number of processes, instant processing of each range can be achieved.  Thus, the lock will also be almost instant. </li><li>  Parallel execution of changeSets in Liquibase (for example, VACUUM). </li><li>  Creating and filling data with new fields in the table. </li><li>  Sophisticated reports. </li></ul><br><div class="spoiler">  <b class="spoiler_title">Almost non-blocking UPDATE (50,000 ranges of 10,000 rows each)</b> <div class="spoiler_text"><pre> <code class="sql hljs">&lt;changeSet author="soldatov" id="JIRA-002-01"&gt; &lt;sql&gt; &lt;![CDATA[ <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> public.pg_parallel_task (<span class="hljs-keyword"><span class="hljs-keyword">name</span></span>, threads_count) <span class="hljs-keyword"><span class="hljs-keyword">VALUES</span></span> (<span class="hljs-string"><span class="hljs-string">'JIRA-002'</span></span>, <span class="hljs-number"><span class="hljs-number">5</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> public.pg_parallel_task_statements (task_name, sql_statement) <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-string"><span class="hljs-string">'JIRA-002'</span></span> task_name, <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span>( <span class="hljs-string"><span class="hljs-string">'UPDATE pgbench_accounts SET abalance = 42 WHERE filler IS NULL AND aid &gt;= ''%s'' AND aid &lt;= ''%s'';'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">MIN</span></span>(d.aid), <span class="hljs-keyword"><span class="hljs-keyword">MAX</span></span>(d.aid) ) sql_statement <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> aid, ntile(<span class="hljs-number"><span class="hljs-number">10000</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">over</span></span> ( <span class="hljs-keyword"><span class="hljs-keyword">order</span></span> <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> aid ) part <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pgbench_accounts <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> filler <span class="hljs-keyword"><span class="hljs-keyword">IS</span></span> <span class="hljs-literal"><span class="hljs-literal">NULL</span></span> ) d <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> d.part; ]]&gt; &lt;/sql&gt; &lt;/changeSet&gt; &lt;changeSet author="soldatov" id="JIRA-002-02"&gt; &lt;executeCommand os="Linux, Mac OS X" executable="./pgpar.sh"&gt; &lt;arg value="pgconfdb"/&gt;&lt;arg value="JIRA-002"/&gt; &lt;/executeCommand&gt; &lt;/changeSet&gt;</code> </pre><br><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Parallel changeSets in Liquibase</b> <div class="spoiler_text"><pre> <code class="sql hljs">&lt;changeSet author="soldatov" id="JIRA-003-01"&gt; &lt;sql&gt; &lt;![CDATA[ <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> pg_parallel_task (<span class="hljs-keyword"><span class="hljs-keyword">name</span></span>, threads_count) <span class="hljs-keyword"><span class="hljs-keyword">VALUES</span></span> (<span class="hljs-string"><span class="hljs-string">'JIRA-003'</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> pg_parallel_task_statements (task_name, sql_statement) <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-string"><span class="hljs-string">'JIRA-003'</span></span> task_name, <span class="hljs-string"><span class="hljs-string">'VACUUM FULL ANALYZE pgbench_accounts;'</span></span> sql_statement; <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> pg_parallel_task_statements (task_name, sql_statement) <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-string"><span class="hljs-string">'JIRA-003'</span></span> task_name, <span class="hljs-string"><span class="hljs-string">'VACUUM FULL ANALYZE pgbench_branches;'</span></span> sql_statement; ]]&gt; &lt;/sql&gt; &lt;/changeSet&gt; &lt;changeSet author="soldatov" id="JIRA-003-02"&gt; &lt;executeCommand os="Linux, Mac OS X" executable="./pgpar.sh"&gt; &lt;arg value="testdatabase"/&gt;&lt;arg value="JIRA-003"/&gt; &lt;/executeCommand&gt; &lt;/changeSet&gt;</code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Almost non-blocking filling of a new field of the table with data (50,000 ranges of 10,000 lines each) with the call of a "multi-threaded" function from the database</b> <div class="spoiler_text"><pre> <code class="sql hljs"><span class="hljs-comment"><span class="hljs-comment">-- SQL part ALTER TABLE pgbench_accounts ADD COLUMN account_number text; INSERT INTO public.pg_parallel_task (name, threads_count) VALUES ('JIRA-004', 5); INSERT INTO public.pg_parallel_task_statements (task_name, sql_statement) SELECT 'JIRA-004' task_name, FORMAT('UPDATE pgbench_accounts SET account_number = aid::text || filler WHERE aid &gt;= ''%s'' AND aid &lt;= ''%s'';', MIN(d.aid), MAX(d.aid)) sql_statement FROM (SELECT aid, ntile(50000) over (order by device_version_guid) part FROM pgbench_accounts) d GROUP BY d.part; SELECT * FROM func_run_parallel_task('testdatabase','JIRA-004');</span></span></code> </pre><br></div></div><br><div class="spoiler">  <b class="spoiler_title">By the way, we have a vacancy.</b> <div class="spoiler_text">  <a href="https://job.lanit.ru/vacancy/Pages/CH-45.aspx%3Futm_source%3Dhabr%26utm_medium%3Dpost-2018-03-20%26utm_campaign%3Ddks">Lead PostgreSQL / Oracle Developer</a> <br></div></div></div><p>Source: <a href="https://habr.com/ru/post/351160/">https://habr.com/ru/post/351160/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../351150/index.html">Web-based devices</a></li>
<li><a href="../351152/index.html">Immersing in templates and taming Windows GPO</a></li>
<li><a href="../351154/index.html">How to take the right person to the team - preparing for the interview</a></li>
<li><a href="../351156/index.html">No need to complicate! Or how can redpolitika help promote your solutions to users?</a></li>
<li><a href="../351158/index.html">Family tree inside git</a></li>
<li><a href="../351162/index.html">Alexey Ragozin on Java Mission Control on jug.msk.ru</a></li>
<li><a href="../351164/index.html">Welcome to the Whatever Hack hackathon March 16</a></li>
<li><a href="../351166/index.html">Guide to background work in Android. Part 3: Executors and EventBus</a></li>
<li><a href="../351168/index.html">Understanding redux-saga: From action generators to sagas</a></li>
<li><a href="../351170/index.html">How Red Hat killed its main product and became a multi-billion dollar corporation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>