<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Oracle RAC. General Description / Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Continuation of article about Real Application Cluster ( RAC ). Ending. 

 Beginning of the article 



 We believe that the cluster has risen and eve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Oracle RAC. General Description / Part 2</h1><div class="post__text post__text-html js-mediator-article">  Continuation of article about <strong>Real Application Cluster</strong> ( <strong>RAC</strong> ).  Ending. <br><br>  <a href="http://habrahabr.ru/blogs/oracle/72122/">Beginning of the article</a> <br><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  We believe that the cluster has risen and everything is spinning. </h4><br><h2>  The interaction of nodes.  Cache-fusion. </h2><br>  Many instances of the database, many disks.  Custom requests rushed ... here they are, the customers we were waiting for.  =) <br><br>  The bottleneck of any database is disk I / O.  Therefore, all databases try to access disks as rarely as possible using deferred recording.  In RAC, everything is the same as for a single-instance database: each node in RAM has an <strong>SGA</strong> ( <strong>System Global Area</strong> ) area, inside it there is a buffer cache ( <strong>database buffer cache</strong> ).  All blocks that were once read from the disk fall into this buffer and are stored there as long as possible.  But the cache is not infinite, therefore, to assess the importance of the stored block, <strong>TCA</strong> (Touch Count Algorithm) is used, which counts the number of calls to the blocks.  When you first hit the cache, the block is placed in its cold-end.  The more often the block is addressed, the closer it is to the hot-end.  If the block is "stale", it gradually loses its position in the cache and risks being replaced by another record.  Overwriting blocks starts with the least used.  The node cache is extremely important for the performance of the nodes, so in order to maintain high performance in a cluster, the cache needs to be shared (as you-know-who testified).  The blocks stored in the cluster node cache can have the role of <strong>local ones</strong> , i.e.  for his own use, but some will already have a <strong>global</strong> mark, which he, having gritted his <strike>teeth with</strike> disks, will be shared with other nodes of the cluster. <br><br>  The shared cache technology in a cluster is called <strong>Cache-fusion</strong> (cache synthesis).  CRS at each node spawns LMSn synchronous processes, their common name as a service is <strong>GCS</strong> ( <strong>Global Cache Service</strong> ).  These processes copy the blocks (global) read on this instance from the buffer cache to the instance that accessed them over the network, and are also responsible for rolling back unconfirmed transactions.  One copy of them can be up to 36 pieces (GCS_SERVER_PROCESSES).  It is usually recommended to have one LMSn for two cores, otherwise they spend too much resources.  Their coordination is the responsibility of the <strong>GES</strong> ( <strong>Global Enqueue Service</strong> ) service, which is represented on each node by the LMON and LMD processes.  LMON monitors the global resources of the entire cluster, calls for blocks to neighboring nodes, manages the recovery of GCS.  When a node is added or leaves the cluster, it initiates a reconfiguration of locks and resources.  LMD manages host resources, controls access to shared blocks and queues, is responsible for blocking requests to GCS, and manages the maintenance of the LMSn request queue.  LMD is also responsible for eliminating global deadlocks within multiple cluster nodes. <br><br><div style="text-align:center;"><img src="http://img203.imageshack.us/img203/9505/grd.jpg"></div><br><br>  But a special role in cluster resource coordination and cache unification is assigned to the <strong>GRD</strong> ( <strong>Global Resource Directory</strong> ) table, where it constantly records on which instance, which block (or its copy) is available, the mode in which the instance holds the block, the role of the block, SCN.  In the single-instance variant, SCN is just an incremental counter of changes made to the database.  In the same SCN cluster, you need to synchronize between nodes.  There are two SCN synchronization methods, depending on the value of the MAX_COMMIT_PROPOGATION_DELAY parameter, specified in milliseconds: <ol><li>  If&gt; 100, SCN are generated (incremented) on instances in parallel.  These SCNs are embedded in each message sent over interconnect, or at certain intervals Oracle polls instances of the current SCN values ‚Äã‚Äãfor mutual synchronization when idle.  Typically, the interval is up to 700 milliseconds.  It can be used for long-term transactions with "late" commit. </li><li>  If = 0 (default), then as soon as there is a COMMIT on any instance, it immediately sends the rest an alert about what number it has now.  The exchange takes place very often (up to fractions of seconds) and therefore a lot of traffic is generated via interconnect. </li></ol><br>  As a result, SCNs in RAC are regularly synchronized to the largest SCN known in the cluster.  SCN is required in order to record the changes in the blocks lined up in chronological order, which is necessary when restoring transactions (roll-forward). <br><br>  The GRD table is distributed between the cluster nodes.  Each node participates in the allocation of cluster resources, updating its part of the GRD.  Part of the GRD table relates to resources - objects: tables, indexes, etc.  It is constantly synchronized (updated) between nodes. <br>  When a node reads a block of data from disk, it becomes the master of this resource and makes the corresponding mark in its part of the GRD table.  The block is marked as local, because  the node is using it alone.  If this block was required by another node, then the GCS process will mark this block in the table as global (‚Äúpublished‚Äù for the cluster) and transfer it to the requested node. <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 3 </td><td>  shared </td><td>  local </td><td>  9996 </td><td>  0 </td></tr></tbody></table><br><br>  <em>Ok, let's bring 'em all together!</em>  In GRD at the master node, to coordinate the distribution of blocks between cluster instances, additional information is recorded with each block: <ul><li>  Data Block Address (DBA): physical block address </li><li>  Location: node on which this block is available. </li><li>  Resource mode: determined by who currently owns the block and what operation will be applied to it <ul><li>  null: node does not claim to change this block (select only) </li><li>  shared: block protected multiple read-only access on multiple nodes. </li><li>  exclusive: the node is about to change (or has already changed) this block.  Although at the same time the former (agreed) versions of the same block may be contained in the cluster, they cannot be changed. </li></ul></li><li>  Resource role: in which mode the block is stored in the cache of the node <ul><li>  local: when the node just read the block from the disk and has not shared it with anyone. </li><li>  global: when the node was initially read by this block, but after it was transferred to the node that requested it in a certain mode (mode).  Now the same block may be present on other nodes. </li></ul></li><li>  System Change Number (SCN): SCN in RAC when the block was changed </li><li>  Image: characterizes a copy of a block when several of them are simultaneously stored in the global cache of the entire cluster. <ul><li>  Past Image (PI): global dirty block (old version, after change), stored in the node's cache after the node has transferred it over the network to another.  The block is kept in memory until it or a later version is written to disk, which GCS will notify when the block is no longer needed. </li><li>  Current Image (XI): the current last copy of the block contained in the last node of the cluster in the query chain of this block. </li></ul></li></ul><br>  Some of the most important principles (single-instance) of the database remain true for RAC: <ul><li>  access the disk as rarely as possible, due to active work with the cache </li><li>  ensure <strong>consistency read</strong> ( <strong>CR</strong> ), <strong>read consistency</strong> , i.e.  data of an unconfirmed transaction no one will ever see in any (parallel) session </li></ul><br>  The difference of the RAC environment from the usual single-instance database is that, despite all the desire, locks are implemented <strong>not at the row level, but at the block level</strong> .  Those.  an instance can block the whole block (contents and other necessary data for someone). <br><br>  We describe several situations typical in the life of a cluster: <br><ul><li>  <strong>Read / read behavior (no transfer)</strong> . <br>  Let the data of table A be the first to consider the node number 4.  He is the master of this table and is responsible for the corresponding part in GRD. <ol><li>  Node 3 received a read request from table A. Node 3 in the cache does not have the necessary block.  From GRD, he learns that the master of table A is node number 4, and refers to it. </li><li>  Node 4 looks at GRD for the presence of the requested block.  If he was in his cache, he would just pass it.  But suppose that the necessary block was not there.  Node number 4 will send nodes number 3 independently read this block from the disk. </li><li>  Node 3 reads it from the disk itself, so far only for itself and with no one the block is divided (local), but later it can provide access to other nodes through an intermediary - the master of this table (shared). </li><li>  Node ‚Ññ3 reports to the master of table A with node ‚Ññ4, and he makes the corresponding entry in GRD (on node ‚Ññ4): <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 3 </td><td>  shared </td><td>  local </td><td>  9996 </td><td>  0 (xi) </td></tr></tbody></table></li></ol></li><li>  <strong>Read / read behavior (transfer)</strong> . <ol><li>  Suppose now that the request to read the same block fell on the node number 2.  Node ‚Ññ2 knows from its local copy of GRD that master node ‚Ññ4 is responsible for this resource (table A) and refers to it. </li><li>  Node 4 on its table GRD finds out that the block is now located on node 3 and sends him instructions to share the block with node 2 in read mode. </li><li>  The node number 3, having received such a command, sends a copy of the block to the node number 2.  In the message header, he also indicates that he is sharing a copy, and he also leaves a copy of this block. </li><li>  Node 2 receives a block and, through GCS, notifies master node 4 that it has received the necessary block.  Master updates GRD (node ‚Äã‚Äãnumber 4): <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 2 </td><td>  shared </td><td>  local </td><td>  9996 </td><td>  0 (xi) </td></tr><tr><td>  500 </td><td>  node number 3 </td><td>  shared </td><td>  local </td><td>  9996 </td><td>  0 (xi) </td></tr></tbody></table></li></ol></li><li>  <strong>Read / Write behavior</strong> . <br>  Finally, the user decided to make changes to table A regarding the block.  Let the user is connected to node # 1. <ol><li>  Node 1 sends a request for exclusive mode for master block to node 4. </li><li>  Node 4 sends a message to all nodes that hold the block, except for some one (say, node 3), so that they switch the block to a completely local mode (null mode, local role).  It ceases to be in the global cache, locks are removed from it (it can now be overwritten), and it is still stored in the cache only for consistent read requests. </li><li>  master requests one of the nodes (node ‚Äã‚Äãnumber 3), transfer this block in exclusive mode to node number 1. </li><li>  Node 3 sends the block to node 1, indicating that the block is transmitted in exclusive mode, and therefore its own will be removed from the global cache.  Node 3 removes locks from the block (it can now be overwritten), leaving it in the cache only for consistent read requests. </li><li> Requesting node number 1 finally gets its block, makes the necessary changes, assigns a new SCN.  Notifies the master node number 4 that the receiving unit and changed, additionally the message includes information that the node number 3 has closed this unit. </li><li>  Site number 4 confirms the deletion of the block for site number 3.  And now GRD on node # 4 contains: <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 1 </td><td>  shared </td><td>  local </td><td>  10010 </td><td>  0 (xi) </td></tr></tbody></table></li></ol></li><li>  <strong>Write / Write behavior</strong> . <br>  Suppose now that at the same time you need to make updates in a block in table A. A single node is always the master of a resource, and the GCS (Global Cache Service) acts on it consistently using the mechanisms of resource locks: <ol><li>  A request came for update to node # 2, which has already removed this block from the global cache.  The node again refers to the master node with a request for exclusive mode on the block. </li><li>  Node 4 requests the current block holder to issue an image (block image) to node 2. </li><li>  Node ‚Ññ1, after making sure that redo logs with applied changes to the block were flushed to disk (important: the transaction on it can still continue to be executed!), Transfers the image of its copy into the past image (after all, node ‚Ññ2 will insert into it changes), which he now can not modify.  And it sends the block in exclusive mode to the requesting node # 2, additionally including in the message that it now holds the PI block. </li><li>  Requesting node number 2 receives the block, makes the necessary changes, assigns a new SCN.  Notifies master node # 4 that the receiving unit is modified and, additionally, the message includes information that node 2 holds the block in exclusive mode, and block 1 contains its previous version (SCN = 10010). </li><li>  Node # 4 amends GRD: <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 1 </td><td>  shared </td><td>  global </td><td>  10010 </td><td>  1 (PI) </td></tr><tr><td>  500 </td><td>  node number 2 </td><td>  exclusive </td><td>  local </td><td>  10016 </td><td>  0 (xi) </td></tr></tbody></table></li></ol></li><li>  <strong>Write / Read behavior</strong> . <br>  The situation is different: the block was changed in one of the nodes and now the latest version is quite different from the one stored on disk. <ol><li>  A select request came from table A to node 3, it accesses the master resource. </li><li>  The master node from GRD finds out that the latest version of the block is contained at node 2 and sends a request to transfer the block to node 3. </li><li>  At node number 2, the block was received and stored in exclusive mode.  As soon as work with the block is terminated, node 2 transfers it to shared mode, marks its copy as past image (SCN = 10016) and transfers it to node 3. </li><li>  Node # 4 amends GRD: <br><br><table><tbody><tr><td>  DBA </td><td>  location </td><td>  mode </td><td>  role </td><td>  SCN </td><td>  PI / XI </td></tr><tr><td>  500 </td><td>  node number 1 </td><td>  shared </td><td>  local </td><td>  10010 </td><td>  1 (PI) </td></tr><tr><td>  500 </td><td>  node number 2 </td><td>  shared </td><td>  global </td><td>  10016 </td><td>  1 (PI) </td></tr><tr><td>  500 </td><td>  node number 3 </td><td>  shared </td><td>  global </td><td>  10016 </td><td>  0 (xi) </td></tr></tbody></table></li></ol></li><li>  <strong>Write-to-disk behavior</strong> . <br>  Suppose it is time to flush the data to disk.  It always comes, as it does not otgagyvay: <ul><li>  the amount of dirty data in the buffer will exceed a certain value (threshold).  You need to write these modified data in order to free the buffer, and you can add something new to it. </li><li>  when the buffer ends up in free space </li><li>  DBWR regularly flushes changed data from the buffer to disk (LOG_CHECKPOINT_TIMEOUT). </li><li>  By the way, this can be controlled through another parameter: fast_start_mttr_target = 3 sec (default), which determines the frequency of passing the checkpoint, i.e.  how often will the records accumulated in the redo log buffer and buffer cache be flushed?  This is to ensure that the recovery of a node's transactions (roll-forward), in the event of a failure, takes approximately 3 seconds. </li></ul><ol><li>  Suppose node number 1 with an obsolete block is about to be written to disk, it refers to master resource node number four, providing the SCN of its block. </li><li>  Node 4 learns from GRD that the latest version is held by node 2.  He refers to it to the node number 2 to write to the disk. </li><li>  As long as node two will write to disk, no one can change the block.  They will have to stand in line at the GRD.  This happens with shared shared blocks (outdated and local blocks can be thrown out and not taken into account). </li><li>  Node number 2 writes data to disk. </li><li>  Node 2 notifies the master that the data is written to disk.  His block goes to the local role (only he, this block will remain). </li><li>  Having received such a message, GCS sends a signal to all nodes to clear their PI, and if someone kept the block with exclusive mode switches to the local role. </li><li>  Now this block can be read from the disk, or it can access the node containing it in memory. </li></ol></li></ul><br><br>  Without the need for no recordings on the disk does not occur.  Always a copy of the block is stored on the site where it is most often used.  If a certain block is not yet in the global cache, then when requesting the master, it will ask the corresponding node to read the block from the disk and share it with the other nodes (as needed). <br><br>  Based on the above, it becomes clear that cache-fusion involves 2 scenarios: <ol><li>  Two nodes are involved: when the target node needed a block that was stored in the master cache. </li><li>  3 nodes participate: when master sends a request to an intermediate node, and that one sends a block to the node requested by it. </li></ol><br>  No matter how many nodes there are in a cluster, the number of hops (nodes participating in block sending) <strong>will never exceed 3</strong> .  This fact explains the ability of the RAC cluster to scale to a large number of nodes without pain. <br><br><h2>  Taking fire, need assistance!  Workload distribution. </h2><br>  The described Cache-fusion device provides the cluster with the ability to (automatically) respond to node loading.  Here‚Äôs how <strong>workload distribution</strong> or <strong>resource remastering</strong> occurs (redistribution of computing resources): <br>  If, say, through node No. 1, 1500 users access resource A, and approximately at the same time 100 users access the same resource A through node No. 2, then it is obvious that the first node has a greater number of requests, and will more often read with disk.  Thus, node 1 will be defined as master for requests to resource A, and GRD will be created and coordinated starting from node 1.  If node 2 needs the same resources, then in order to gain access to them it will have to coordinate its actions with GCS and GRD node No. 1, in order to obtain resources through interconnect. <br>  If the allocation of resources changes in favor of node 2, then processes No. 2 and No. 1 coordinate their actions through interconnect, and master A of resource A will become node No. 2, since  Now he will more often refer to the disk. <br>  This is called resource <strong>affinity</strong> , i.e.  resources will be allocated to the node on which there is more action to receive and block them.  Resource Affinity Policy coordinates the activities of the nodes so that resources are more accessible where it is more needed.  Here, briefly, and the entire <strong>workload distribution</strong> . <br><br>  Redistribution (remastering) also occurs when a node is added or leaves the cluster.  Oracle redistributes resources according to an algorithm called "lazy remastering" (lazy remastering), because  Oracle almost does not take active action on the redistribution of resources.  If a node falls, all that Oracle takes is to transfer the resources that belonged to the collapsed node to one of the remaining ones (less loaded).  After stabilization of the load, GCS and GES will re-automatically (automatically) redistribute resources (workload distribution) to those positions where they are more in demand.  A similar action occurs when a node is added: an approximately equal amount of resources is separated from the active nodes and assigned to the newcomer.  Then workload distribution will happen again. <br>  As a rule, to initiate a dynamic redistribution, the workload on a particular node should exceed the workload of the rest for more than 10 minutes. <br><br><h2>  Here the bullet flew, and ... huh?  Recovery. </h2><br>  Suddenly, a node did not respond to heartbeat, the CSSD process on the node that first discovered it ‚Äúsounds the alarm‚Äù and reports to the master node (if it is still connected, otherwise you will have to assume the responsibility of the master).  The master initiates the ‚Äúvoting‚Äù procedure on all nodes, the surviving cluster nodes begin to be marked on the voting disk.  If the missing node leaves no trace here, then the master begins the process of excluding the missing from the cluster.  The Redo log file will be read twice: once by redo records, and a second time (again) by undo records, to make the database available for queries as early as possible. <br><ol><li>  Part of the GRD table with the resources of the fallen node is ‚Äúfrozen‚Äù. </li><li>  A node that has not contacted it is marked as ‚Äúmissing‚Äù so that the remaining nodes do not contact it in vain via interconnect. </li><li>  The node that first detected the loss starts restoring the information that was processed on the disappeared node: <ul><li>  Lowers the rate of servicing your own transactions, throwing computing resources to recovery </li><li>  It accesses the shared file storage (datastorage), and on itself begins to apply online redo logs belonging to the missing node.  Taking into account the sequence number of SCN blocks, merge them with what is stored in the buffer, and rolls (roll-forward) in its cache.  In this case, the node skips those outdated block entries (PI), the later versions of which have already been flushed to disk.  If the read blocks in the cluster have the master of the corresponding resource, then the node reports the list of read blocks, and the master on these resources sets a lock so that the nodes do not access them (until they are restored). </li><li>  After that, by the second reading on the redo log, taking into account the undo of the record, it rolls back (roll-back) uncommitted transactions.  This happens using fast-recovery technology, i.e.  transaction rollback will be performed by a separate background process.  Oracle will return blocks blocked by incomplete transactions (uncommitted) in a consistent state (consistent), to the same values ‚Äã‚Äãas soon as a request for these blocks arrives.  Or they will be restored by this parallel background process by that time.  Thus, locks are already released in the cluster and new user requests can be executed. </li></ul></li><li>  The part of the GRD table belonging to the fallen node is thawed on the restoring node (now it is the master of the resource).  Thus, in a cluster, the state of processed transactions on the missing node is restored at the time of the crash. </li></ol><br><h4>  But while all these processes are happening, an impatient client has something to offer. </h4><br><br><h2>  As long as the knots save each other ... Failover. </h2><br>  <strong>Failover</strong> is the handling of a node crash situation in a cluster. <br>  It's time to mention another layer in a clustered environment - a public (public) network through which clients access the database.  On physical servers, preferably at least 2 network cards: <br><ol><li>  The first network card is assigned a static IP through which the host will exchange messages with its neighbors in the cluster (interconnect). </li><li>  The second network interface card is assigned a logical Virtual IP (virtual IP) through which clients will send requests to the cluster node. </li></ol><br><br>  <strong>Virtual IP</strong> ( <strong>VIP</strong> ) is a logical network address assigned to a host on the external network interface.  It provides the ability for CRS to quietly start, stop and transfer work with this VIP to another node.  A listener (process accepting connections) on each node will listen on its VIP.  As soon as a node becomes unavailable, its VIP picks up another node in the cluster, thus temporarily servicing its own and the requests of the fallen node. <br><br>  This is done in order to reduce customer downtime if the node on which his transaction was performed fell off.  After all, the client can wait for TCP timeout in a few minutes.  In this case, the VIP will immediately be picked up by another node, and further events may develop according to two scenarios, using <strong>TAF</strong> ( <strong>Transparent Application Failover</strong> ) technology: <br><ol><li>  <strong>Database VIPs</strong> : The client will connect via VIP, but will already connect to another node.  The temporarily replacing node will answer ‚Äúlogon failed‚Äù, despite the fact that VIP will be active, the necessary database instance will be absent.  And the client will immediately try again, but to another cluster instance / node from its list in the configuration. </li><li>  <strong>Application VIP</strong> : the same as before.  But only now on this VIP it will be possible to turn to the application, on whatever node it spins. </li></ol><br>  Database VIPs can only provide applications to their site, and if the application has migrated, they refuse.  Application VIP even after migration performs the functionality provided by the node (positive). <br><br>  If the node is restored and goes online, CRS will recognize this and will ask you to reset it offline in the replacement node and return the VIP address back to the owner.  VIP belongs to the CRS, and may not be transferred if it is a copy of the database that fails. <br><br>  It is important to note that with failover, only select queries are transferred, together with open cursors (returning results).  Transactions are not transferred (PL / SQL, temp tables, insert, update, delete), they will always need to be restarted. <br><br>  There are two ways to configure a TAF: <ul><li>  <strong>Connect-time failover and client load-balancing</strong> <br>  In this case, the client always randomly selects which cluster node to connect to from its network connection configuration list.  If the node performing the request fails, the client selects another cluster node by TAF and reconnects. <br></li><li>  <strong>Preconnect</strong> <br>  In this case, the client always connects to all nodes when a connection to the cluster is established, although the request will run only on one instance.  If the node fails, it simply transfers the request to another node.  Failover is faster, but consumes connection resources on all nodes in the cluster. <br></li></ul><br>  The client connection has retries and delay parameters.  They can be configured, how many times the client (silently) will try to reconnect if the node falls, and what delay to expose.  Perhaps the most interesting thing is that if a node falls, Oracle can notify the client about it through FAN (Fast Application Notification), which is part of ONS (Oracle Notification Services).  If the client uses a thick connection driver to Oracle, before calling the database, you can register the callback function (callback) to which the event will come, in the case of TAF (failover).  This can either be displayed as a ‚Äúslight hitch‚Äù on the user‚Äôs screen and can control the process of manually restarting the request. <br><br><h2>  Don't go there, go here ... Load-balancing. </h2><br><br>  When performing any operations, information related to query performance (like ‚Äúdebugging‚Äù), Oracle collects in <strong>AWR</strong> ( <strong>Automatic Workload Repository</strong> ).  It is stored in the SYSAUX tablespace.  Statistics collection starts every 60 minutes (default): I / O waits, wait events, CPU used per session, I / O rates on datafiles (which file is most often accessed). <br><br>  The need for load <strong>balancing</strong> (load distribution) across nodes in a cluster is determined by a set of criteria: by the number of physical connections to a node, by the processor load (CPU), by traffic.  It is a pity that you can‚Äôt load-balance by the average request execution time on nodes, but, as a rule, this is in some way related to the resources involved on the nodes, and therefore the remaining free resources. <br><br>  About <strong>client load-balancing</strong> was a little said above.  It simply allows the client to connect to a randomly selected cluster node from the list in the configuration.  To implement <strong>Server-side load-balancing, a</strong> separate process PMON (process monitor) collects information about the load of cluster nodes.  The frequency of updating this information depends on the workload of the cluster and can vary from about 1 minute to 10 minutes.  Based on this information, the Listener on the node to which the client connects will redirect it to the least loaded node. <br><br>  Oracle provides DBA to select the most relevant criteria for load balancing: <ul><li>  Based on elapsed-time (CLB_GOAL_SHORT): by the average query execution time on a node </li><li>  Based on number of sessions (CLB_GOAL_LONG): by the number of connections to a node </li></ul><br><br>  If the application has a connection pool, Oracle provides the <strong>Runtime Connection Load Balancing</strong> ( <strong>RCLB</strong> ) <strong>option</strong> .  Instead of the usual option, when we try to predict which of the nodes will be less loaded, and send a request there, we will use the notification mechanism (events) of the application about loading on the nodes.  And now the application itself will determine where to send the request, based on this data.  Notification occurs through ONS (Oracle Notification Service).  RCLB regularly receives data (feedback) from cluster nodes, and the connection pool will distribute connections to clients, relying on some relative number that reflects what percentage of connections each instance can make.  These metrics (average node load), which the RAC sends, each node builds itself in AWR.  Based on them, the required load advisory is formed and placed in the AQ (advanced querying) queue, from where data is sent via the ONS to the client. <br><br>  Notifications will be based on one of the mechanisms: <ul><li>  Focusing on time delays in query execution (GOAL_SERVICE_TIME) <br>  Time delays in processing previous requests are taken into account. </li><li>  Focusing on server load (GOAL_THROUGHPUT) <br>  CPU bandwidth is taken into account. </li></ul><br><br>  <a href="http://habrahabr.ru/blogs/oracle/72122/">Beginning of the article</a> <br><br></div><p>Source: <a href="https://habr.com/ru/post/72121/">https://habr.com/ru/post/72121/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../72112/index.html">A new tool for travel agents based on Microsoft Surface</a></li>
<li><a href="../72113/index.html">Solar battery for e-book</a></li>
<li><a href="../72117/index.html">Firefox Extensions - Working with Settings</a></li>
<li><a href="../72118/index.html">Browsers and Pixels</a></li>
<li><a href="../72119/index.html">12 hours without recharging!</a></li>
<li><a href="../72122/index.html">Oracle RAC. General Description / Part 1</a></li>
<li><a href="../72123/index.html">Broken iPod or what to do hard drive</a></li>
<li><a href="../72125/index.html">Twig, Smarty and Quicky Renderers for Yii</a></li>
<li><a href="../72126/index.html">DIY: KAP (Kite aerial photography)</a></li>
<li><a href="../72127/index.html">Create Rich Internet Applications with OpenLaszlo. Sample development framework for Vkontakte applications</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>