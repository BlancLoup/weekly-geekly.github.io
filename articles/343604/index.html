<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We teach the machine to understand the human genes</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is always pleasant to realize that the application of technology comes down not only to financial gain, there are also ideas that make the world a ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We teach the machine to understand the human genes</h1><div class="post__text post__text-html js-mediator-article">  It is always pleasant to realize that the application of technology comes down not only to financial gain, there are also ideas that make the world a better place.  We will tell about one of the projects with such an idea on this frosty Friday day.  You will learn about the solution, which made it possible to increase the accuracy of rapid blood analysis, by using machine learning algorithms to identify the links between micro-RNA and genes.  Also, it is worth noting that the methods described below can be used not only in biology. <br><br><img src="https://habrastorage.org/webt/7b/_u/cz/7b_uczlpr3gc0gv4pthfkhej768.png"><br><a name="habracut"></a><br><blockquote><h2>  Series of Digital Transformation articles </h2><br>  Technological articles: <br><br>  1. <a href="https://habrahabr.ru/company/microsoft/blog/341854/">Start</a> . <br>  2. <a href="https://habrahabr.ru/company/microsoft/blog/342364/">Blockchain in the bank</a> . <br>  3. We <a href="https://habrahabr.ru/company/microsoft/blog/343604/">learn the car to understand human genes</a> . <br>  4. <a href="https://habrahabr.ru/company/microsoft/blog/345452/">Machine learning and chocolates</a> . <br>  5. Loading ... 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      A series of interviews with Dmitry Zavalishin on <a href="https://www.youtube.com/channel/UCJuwCFeP6rhSUqk9gRMAwuw/feed">DZ Online</a> : <br><br>  1. <a href="https://habrahabr.ru/company/microsoft/blog/342596/">Alexander Lozhechkin from Microsoft: Do we need developers in the future?</a> <br>  2. <a href="https://habrahabr.ru/company/microsoft/blog/345662/">Alexey Kostarev from ‚ÄúRobot Vera‚Äù: How to replace HR with a robot?</a> <br>  3. <a href="https://habrahabr.ru/company/microsoft/blog/346870/">Fedor Ovchinnikov from Dodo Pizza: How to replace the restaurant director with a robot?</a> <br>  4. <a href="https://habrahabr.ru/company/microsoft/blog/348670/">Andrei Golub from ELSE Corp Srl: How to stop spending a lot of time on shopping trips?</a> </blockquote><br>  We recently entered into a partnership agreement with Miroculus, a young promising company in the field of medical research.  Miroculus is developing low-cost rapid blood test kits.  As part of this project, we focused on the problem of identifying links between micro-RNA and genes by analyzing scientific and medical documentation, finding a solution that can be applied in many other areas. <br><br><h2>  Problem </h2><br>  The task of the <a href="http://www.miroculus.com/">Miroculus</a> system is to identify the relationship of individual <a href="https://en.wikipedia.org/wiki/MicroRNA">micro-RNA</a> with certain genes or diseases.  Based on these data, a tool is being developed and constantly improved, with the help of which researchers will be able to quickly identify the links between micro-RNA, genes and types of diseases (for example, oncological). <br><br>  Although a lot of research is available in the medical literature regarding interdependencies between individual miRNAs, genes, and diseases, there is no single centralized database containing such information in an ordered structured form. <br><br>  There can be various types of dependencies between micro-RNA and genes; however, due to lack of data, the problem of extracting connections has been reduced to binary classification, the purpose of which is simply to determine the connection between micro-RNA and the gene. <br><br>  Detecting relationships between objects in unstructured text is called <a href="https://en.wikipedia.org/wiki/Relationship_extraction">relationship retrieval</a> . <br><br>  Strictly speaking, the task receives unstructured text input data and a group of objects, and then displays the resulting group of triads of the form ‚Äúfirst object, second object, type of connection‚Äù.  That is, this is a subtask within the larger task of <a href="https://en.wikipedia.org/wiki/Information_extraction">extracting information</a> . <br><br>  Since we are dealing with a binary classification, we need to create a classifier that receives a sentence and a pair of objects, and then displays the resulting score in the range from 0 to 1, reflecting the probability of a connection between these two objects. <br><br>  For example, you can pass the sentence "mir-335 adjusts BRCA1" and a pair of objects (mir-335, BRCA1) to the classifier, and the classifier will give the result "0.9". <br><br>  The source code for this project is available on the <a href="https://github.com/CatalystCode/corpus-to-graph-ml">page</a> . <br><br><h2>  Creating a dataset </h2><br>  We used the text of medical articles from two data sources: <a href="http://www.ncbi.nlm.nih.gov/pmc/">PMC</a> and <a href="http://www.ncbi.nlm.nih.gov/pubmed">PubMed</a> . <br><br>  The text of documents downloaded from the indicated sources was divided into sentences using the <a href="http://textblob.readthedocs.io/en/dev/">TextBlob</a> library. <br><br>  Each proposal was transferred to the <a href="http://gnat.sourceforge.net/">GNAT</a> Object Recognition Tool to extract the names of micro-RNA and the genes contained in the proposal. <br><br>  One of the most difficult tasks involved in retrieving relationships (or with basic machine learning tasks) is the availability of data with labels.  In our project, such data was not available.  Fortunately, we could use the ‚Äúremote monitoring‚Äù method. <br><br><h4>  Remote surveillance </h4><br>  The term "remote monitoring" was first introduced in the <a href="http://web.stanford.edu/~jurafsky/mintz.pdf">study</a> "Remote monitoring when extracting relationships without using data with tags" by Minz and co-authors.  The method of remote observation involves the creation of a set of data with labels based on a database of known links between objects and a database of articles in which these objects are mentioned. <br><br>  For each pair of objects and each link, link labels have been created in the database of objects for all sentences in the articles of the database where objects are mentioned. <br><br>  To generate negative patterns (no link), we arbitrarily selected sentences containing links that are not displayed in the link database.  It is worth noting that the main criticism of the method of remote observation is based on the possible inaccuracy of negative samples, since in some cases a false negative result can be obtained as a result of random sampling of data. <br><br><img src="https://habrastorage.org/webt/0g/zx/jh/0gzxjh2tn4lmmoliymbemolawlw.png"><br><br><h2>  Text conversion </h2><br>  After creating a learning set with tags, a link qualifier is created using the <a href="http://scikit-learn.org/">scikit-learn</a> Python library and several Python libraries based on natural language processing (NLP) technologies.  As an experiment, we tried to use several different distinctive features and classifiers. <br><br>  Before testing the methods and distinctive features in practice, we performed a text conversion consisting of the steps described below. <br><br><h4>  Object replacement </h4><br>  The idea is that the training model is not required in accordance with the name of a particular object, but it is necessary in accordance with the structure of the text. <br><br>  Example: <br><br><pre><code class="python hljs">miRNA<span class="hljs-number"><span class="hljs-number">-335</span></span> was found to regulate BRCA1</code> </pre> <br>  Converting to: <br><br><pre> <code class="python hljs">ENTITY1 was found to regulate ENTITY2</code> </pre> <br>  That is, we practically took all the pairs of objects from each sentence and for each of them replaced the objects we were looking for with placeholders.  At the same time, OBJECT1 always replaces micro-RNA, and OBJECT2 is always a gene.  We also used another special placeholder to mark the objects that are part of the proposal, but do not participate in the desired connection. <br><br>  So, for the following sentence: <br><br><pre> <code class="python hljs">High levels of expression of miRNA<span class="hljs-number"><span class="hljs-number">-335</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> miRNA<span class="hljs-number"><span class="hljs-number">-342</span></span> were found together <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> low levels of BRCA1</code> </pre> <br>  we got the following set of converted sentences: <br><br><pre> <code class="python hljs">High levels of expression of ENTITY1 <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> OTHER_ENTITY were found together <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> low levels of ENTITY2 High levels of expression of OTHER_ENTITY <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> ENTITY1 were found together <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> low levels of ENTITY2</code> </pre> <br>  At this stage, you can use the Python method string.replace () if you want to replace objects, as well as the methods itertools.combinations or itertools.product, if you want to see all the possible combinations. <br><br><h4>  Markup </h4><br>  Markup is the process of splitting a sequence of words into smaller segments.  In this case, we divide the sentences into words. <br>  For this we use the <a href="http://www.nltk.org/">nltk</a> library: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> nltk tokens = nltk.word_tokenize(sentence)</code> </pre> <br><h4>  Truncation </h4><br>  In accordance with the recommendations presented in the scientific literature, we truncated each sentence to a smaller segment containing two objects, words between objects and a few words before and after the object.  The purpose of this truncation is to remove those parts of the sentence that are not significant when extracting the relationship. <br><br>  We created a slice of the array of words, for which the previous step was marked, providing it with appropriate indices: <br><br><pre> <code class="python hljs">WINDOW_SIZE = <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-comment"><span class="hljs-comment"># make sure that we don't overflow but using the min and max methods FIRST_INDEX = max(tokens.index("ENTITY1") - WINDOW_SIZE , 0) SECOND_INDEX = min(sentence.index("ENTITY2") + WINDOW_SIZE, len(tokens)) trimmed_tokens = tokens[FIRST_INDEX : SECOND_INDEX]</span></span></code> </pre> <br><h4>  Normalization </h4><br>  We have normalized the sentences by simply converting all the letters to lowercase.  Although it is advisable to use the case creation method for solving such tasks as analyzing moods and emotions, it is not indicative in terms of extracting relationships.  We are interested in the information and structure of the text, and not the emphasis of individual words in the sentence. <br><br><h4>  Delete stop words / numbers </h4><br>  In this case, we use the standard process of removing stop words and numbers from a sentence.  Stop words are words with high frequency, for example, the prepositions "in," "to" and "to."  Since these words are found in sentences very often, they do not carry a semantic load about the relationships between objects in a sentence. <br><br>  For the same reason, we delete lexemes consisting only of numbers and short lexemes containing less than two characters. <br><br><h4>  Root extraction </h4><br>  <a href="https://en.wikipedia.org/wiki/Stemming">Root extraction</a> is the process of reducing a single word to the root. <br>  As a result, the volume of semantic space for the word is reduced, which allows you to focus on the actual meaning of the word. <br><br>  In practice, this step is not particularly effective in terms of improving accuracy.  For this reason, and also because of the relatively low level of productivity of this process (in terms of execution time), the selection of roots was not included in the final model. <br><br>  Normalization, deletion of words and selection of roots are performed by iterating the marked up and truncated sentences;  if necessary, normalization and deletion of words is performed. <br><br><pre> <code class="python hljs">cleaned_tokens = [] porter = nltk.PorterStemmer() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> trimmed_tokens: normalized = t.lower() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (normalized <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> nltk.corpus.stopwords.words(<span class="hljs-string"><span class="hljs-string">'english'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> normalized.isdigit() <span class="hljs-keyword"><span class="hljs-keyword">or</span></span> len(normalized) &lt; <span class="hljs-number"><span class="hljs-number">2</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">continue</span></span> stemmed = porter.stem(t) processed_tokens.append(stemmed)</code> </pre> <br><h2>  Submission of distinctive features </h2><br>  Upon completion of the transformations below, we decided to experiment with different types of distinctive features. <br><br>  We used three types of signs: a multiset of words, syntactic signs and a vector representation of words. <br><br><h4>  Multiset of words </h4><br>  <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">The word set multiset</a> (MS) <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">model</a> is a common method used in natural language processing (NLP) tasks for converting text into a numerical vector space. <br><br>  In the MC model, each word in the dictionary is assigned a unique numeric identifier.  Then each sentence is converted to a vector in the volume of the dictionary.  The position in the vector is expressed by the value ‚Äú1‚Äù in the event that the word with such an identifier is text, or the value ‚Äú0‚Äù otherwise.  As an alternative approach, you can customize the display in each element of the vector of the number of occurrences of a given word in the text.  An example can be found <a href="https://en.wikipedia.org/wiki/Bag-of-words_model">here</a> . <br><br>  However, the model described above does not take into account the order of the various words in a sentence, but only the occurrence of each individual word.  To include the word order in the model, we used the popular <a href="https://en.wikipedia.org/wiki/N-gram">N-gram</a> model, which evaluates a set of consecutive words of <b>N</b> words in length and treats each such set as a separate word. <br><br>  For more information about using N-grams in textual analysis, see the <a href="https://www.microsoft.com/developerblog/2015/11/29/feature-representation-for-text-analyses/">section</a> ‚ÄúRepresentation of distinctive features for textual analysis: one-gram, two-gram, trigram ... how much is all?‚Äù. <br><br>  Fortunately for us, MC models and N-grams are implemented in scikit-learn by means of the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> class. <br><br>  The following example shows the conversion of text to the MS 1/0 view using a trigram model. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> CountVectorizer vectorizer = CountVectorizer(analyzer = <span class="hljs-string"><span class="hljs-string">"word"</span></span>, binary = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, ngram_range=(<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)) <span class="hljs-comment"><span class="hljs-comment"># note that 'samples' should be a list/iterable of strings # so you might need to convert the processes tokens back to sentence # by using " ".join(...) data_features = vectorizer.fit_transform(samples)</span></span></code> </pre> <br><h4>  Syntactic features </h4><br>  We used two types of syntactic features: <a href="https://en.wikipedia.org/wiki/Part-of-speech_tagging">markers of speech</a> (PD) and <a href="https://en.wikipedia.org/wiki/Parse_tree">parse trees with dependencies</a> . <br><br>  We decided to use <a href="https://spacy.io/">spacy.io</a> to extract both PD markers and dependency graphs, since this technology is superior to existing Python libraries in terms of speed, and accuracy is comparable to other NLP systems. <br><br>  The following code snippet extracts the CR for the given sentence: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> spacy.en <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> English parser = English() parsed = parser(<span class="hljs-string"><span class="hljs-string">" "</span></span>.join(processed_tokens)) pos_tags = [s.pos_ <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> s <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> parsed]</code> </pre> <br>  After converting all the sentences, you can use the CountVectorizer class described above and the multiset model for PD markers to convert them into a numeric vector space. <br><br>  A similar method was used to process the distinguishing features of the parsing tree with the dependencies that were searched for between the two objects in each sentence, and they were also converted using the CountVectorizer class. <br><br><h4>  Vector representations of words </h4><br>  The method of vector representation of words has recently become very popular for solving problems related to NLP.  The essence of this method is to use neural models to transform words into a space of distinctive features so that similar words are represented by vectors at a small distance from each other. <br><br>  For more information about the vector representation of words, see the following blog <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">article</a> . <br><br>  We applied the approach described in the <a href="https://cs.stanford.edu/~quocle/paragraph_vector.pdf">Paragraph Vector</a> document: embedding sentences (or documents) into a high-dimensional space of distinctive features.  We used the Gensim implementation of the Doc2Vec library.  More information is available in this <a href="https://github.com/piskvorky/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb">tutorial</a> . <br><br>  Both the parameters and the size of the output vectors used correspond to the recommendations given in the Paragraph Vector document and the Gensim study guide. <br><br>  Notice that in addition to tagged data, we used a large set of untagged sentences in the Doc2Vec model to provide additional context for the model and to expand the language and features used to train the model. <br><br>  After creating the model, each of the proposals was represented by more than 200 dimensional vectors that can be used as input for the classifier. <br><br><h2>  Evaluation of classification models </h2><br>  Having completed the text conversion and extraction of distinctive features, you can proceed to the next step: the selection and evaluation of the classification model. <br><br>  For classification, a <a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a> algorithm was used.  We tried algorithms such as support vector machine and random forest, but the logistic regression showed the best results in terms of speed and accuracy. <br><br>  Before proceeding to assess the accuracy of this method, it is necessary to <a href="https://en.wikipedia.org/wiki/Test_set">divide the</a> data set into a training set and a test set.  To do this, simply use the train_test_split method: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.cross_validation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> train_test_split x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=<span class="hljs-number"><span class="hljs-number">0.25</span></span>)</code> </pre> <br>  This method divides the data set arbitrarily, with 75% of the data belonging to the training set, and 25% to the test set. <br><br>  To train the classifier based on logistic regression, we used the scikit-learn <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression</a> class.  To assess the performance of the classifier, we use the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html">classification_report</a> class, which prints data on <a href="https://en.wikipedia.org/wiki/Precision_and_recall">accuracy</a> , <a href="https://en.wikipedia.org/wiki/Precision_and_recall">return completeness</a> and <a href="https://en.wikipedia.org/wiki/F1_score">F1-Score</a> for classification. <br><br>  The following code snippet shows the logistic regression classifier training and the printout of the classification report: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.linear_model <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LogisticRegression <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.metrics <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> classification_report clf = linear_model.LogisticRegression(C=<span class="hljs-number"><span class="hljs-number">1e5</span></span>) clf.fit(x_train, y_train) y_pred = clf.predict(x_test) <span class="hljs-keyword"><span class="hljs-keyword">print</span></span> classification_report(y_test, y_pred)</code> </pre> <br>  An example of the result of executing the code fragment described above is as follows: <br><br><pre> <code class="python hljs"> precision recall f1-score support <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0.82</span></span> <span class="hljs-number"><span class="hljs-number">0.88</span></span> <span class="hljs-number"><span class="hljs-number">0.85</span></span> <span class="hljs-number"><span class="hljs-number">1415</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-number"><span class="hljs-number">0.89</span></span> <span class="hljs-number"><span class="hljs-number">0.83</span></span> <span class="hljs-number"><span class="hljs-number">0.86</span></span> <span class="hljs-number"><span class="hljs-number">1660</span></span> avg / total <span class="hljs-number"><span class="hljs-number">0.86</span></span> <span class="hljs-number"><span class="hljs-number">0.85</span></span> <span class="hljs-number"><span class="hljs-number">0.85</span></span> <span class="hljs-number"><span class="hljs-number">3075</span></span></code> </pre> <br>  Note that the C parameter (it indicates the degree of regularization) is chosen arbitrarily for this example, but it must be configured using a cross validation check, as shown below. <br><br><h2>  results </h2><br>  We combined all the above methods and techniques and compared various distinctive features and transformations to select the optimal model. <br><br>  We used the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html">LogisticRegressionCV</a> class to create a binary classifier with configurable parameters, and then analyzed another test suite to evaluate the performance of the model. <br>  Please note that for simple and convenient testing of various parameters of different distinguishing features, you can use the <a href="http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html">GridSearch</a> class. <br>  The following table shows the main results of the comparison of various distinguishing features. <br>  To check the accuracy of the model, the <a href="https://en.wikipedia.org/wiki/F1_score">F1-Score</a> scale was used, since it allows us to estimate both the accuracy and the completeness of the return to the model. <br><table><tbody><tr><th>  Opportunities </th><th>  F1-Score </th></tr><tr><td>  Single-trigram (word set) </td><td>  0.87 </td></tr><tr><td>  Single-trigrams (MC) and trigrams (markers CR) </td><td>  0.87 </td></tr><tr><td>  Single-trigram (MS) and Doc2Vec </td><td>  0.87 </td></tr><tr><td>  Odnograms (multiset of words) </td><td>  0.8 </td></tr><tr><td>  Dvigrams (multiset of words) </td><td>  0.85 </td></tr><tr><td>  Trigrams (multiset of words) </td><td>  0.83 </td></tr><tr><td>  Doc2Vec </td><td>  0.65 </td></tr><tr><td>  Trigrams (markers CR) </td><td>  0.62 </td></tr></tbody></table><br>  In general, it seems that when using a multiset of words in single-trigrams, maximum accuracy is ensured compared to other methods. <br><br>  Although the Doc2Vec model is distinguished by maximum performance in determining the similarity of words, it does not guarantee decent results in terms of extracting relationships. <br><br><h2>  Use cases </h2><br>  In this article, we looked at the method that was used to create a classifier for extracting relationships for processing links between micro-RNA and genes. <br><br>  Although the problems and patterns discussed in this article are related to the field of biology, the solution and methods studied can be applied in other areas to create relationship graphs based on unstructured text data. <br><br><hr><br>  We remind you that <a href="https://azure.microsoft.com/ru-ru/free/%3Fwt.mc_id%3DAID570629_QSG_BLOG_139069">you can try Azure for free</a> . <br><br>  <b>Minute advertising</b> .  If you want to try new technologies in your projects, but do not reach the hands, leave the application in the program <b><a href="https://www.tech-acceleration.com/">Tech Acceleration</a></b> from Microsoft.  Its main feature is that together with you we will select the required stack, we will help to realize the pilot and, if successful, we will spend maximum efforts so that the whole market will know about you. <br><br><img src="https://habrastorage.org/webt/59/e3/5d/59e35dcea0345205971169.jpeg" align="left" width="60">  <b>PS We</b> thank Kostya Kichinsky ( <a href="https://t.me/quantumquintum">Quantum Quintum</a> ) for the illustration of this article. </div><p>Source: <a href="https://habr.com/ru/post/343604/">https://habr.com/ru/post/343604/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343592/index.html">How we implemented DevOps: testing a production environment with Azure Web App</a></li>
<li><a href="../343594/index.html">LLVM testing</a></li>
<li><a href="../343596/index.html">Multi-channel attribution through Calltouch</a></li>
<li><a href="../343598/index.html">Why did I buy a Mac Mini (Late 2012) on the eve of 2018?</a></li>
<li><a href="../343600/index.html">Who and how much to keep pace with progress?</a></li>
<li><a href="../343610/index.html">PHP 7.2.0 released</a></li>
<li><a href="../343612/index.html">About creating payloads for different platforms using msfvenom</a></li>
<li><a href="../343614/index.html">Learn CSS Grid in 5 minutes</a></li>
<li><a href="../343616/index.html">Another Telegram-bot for video surveillance</a></li>
<li><a href="../343622/index.html">Eight C ++ 17 features that every developer should use</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>