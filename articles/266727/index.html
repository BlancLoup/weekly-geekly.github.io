<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About a Data Science Problem</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 



 As promised, I continue to publish articles in which I describe my experience after completing training in Data Science from the guys f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About a Data Science Problem</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br><img src="https://habrastorage.org/files/60f/d96/61c/60fd9661cf9944bf890193af5e035a5e.jpg"><br><br>  As promised, I continue to publish articles in which I describe my experience after completing training in Data Science from the guys from <a href="http://dscourse.mlclass.ru/">MLClass.ru</a> (by the way, who haven't had time yet - I <a href="http://dscourse.mlclass.ru/">recommend registering</a> ).  This time, using the example of the <a href="https://www.kaggle.com/c/digit-recognizer">Digit Recognizer</a> task, we will study the effect of the size of the training sample on the quality of the machine learning algorithm.  This is one of the very first and most basic questions that arise when building a predictive model. <br><a name="habracut"></a><br><h3>  Introduction </h3><br>  In the process of working on data analysis, there are situations when the size of the sample available for research is an obstacle.  I met this example while participating in a <b>Digit Recognizer</b> contest held on the <a href="http://www.kaggle.com/">Kaggle</a> website.  The object of the competition is the database of images of hand-written numbers - <b>The MNIST database of handwritten digits</b> .  Images were centered and brought to the same size.  A sample of 42,000 such numbers is proposed as a training sample.  Each digit is decomposed into a line of 784 signs, the value in each is its brightness. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First, load the full training sample into <b>R</b> <br><br><pre><code class="hljs lisp">library(<span class="hljs-name"><span class="hljs-name">readr</span></span>) require(<span class="hljs-name"><span class="hljs-name">magrittr</span></span>) require(<span class="hljs-name"><span class="hljs-name">dplyr</span></span>) require(<span class="hljs-name"><span class="hljs-name">caret</span></span>) data_train &lt;- read_csv(<span class="hljs-string"><span class="hljs-string">"train.csv"</span></span>)</code> </pre> <br>  Now, to get an idea of ‚Äã‚Äãthe data provided, let's draw the numbers in the usual form for the human eye. <br><br><pre> <code class="hljs pgsql">colors&lt;-c(<span class="hljs-string"><span class="hljs-string">'white'</span></span>,<span class="hljs-string"><span class="hljs-string">'black'</span></span>) cus_col&lt;-colorRampPalette(colors=colors) default_par &lt;- par() par(mfrow=c(<span class="hljs-number"><span class="hljs-number">6</span></span>,<span class="hljs-number"><span class="hljs-number">6</span></span>),pty=<span class="hljs-string"><span class="hljs-string">'s'</span></span>,mar=c(<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>),xaxt=<span class="hljs-string"><span class="hljs-string">'n'</span></span>,yaxt=<span class="hljs-string"><span class="hljs-string">'n'</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span>(i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">36</span></span>) { z&lt;-<span class="hljs-keyword"><span class="hljs-keyword">array</span></span>(<span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.matrix(data_train)[i,<span class="hljs-number"><span class="hljs-number">-1</span></span>],dim=c(<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">28</span></span>)) z&lt;-z[,<span class="hljs-number"><span class="hljs-number">28</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>] image(<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">28</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">28</span></span>,z,main=data_train[i,<span class="hljs-number"><span class="hljs-number">1</span></span>],col=cus_col(<span class="hljs-number"><span class="hljs-number">256</span></span>)) } par(default_par)</code> </pre><br><br><img src="https://habrastorage.org/files/3c1/7e4/bff/3c17e4bffb29429683a4d962b35c5d99.png"><br><br>  Then one could proceed to the construction of various models, the choice of parameters, etc.  But let's look at the data.  <b>42000</b> objects and <b>784</b> signs.  When I tried to build more complex models, such as <b>Random Forest</b> or <b>Support Vector Machine,</b> I received an error about the lack of memory, and not even a minute of learning from a full sample takes place.  One of the options for dealing with this is to use a significantly more powerful machine for computing, or creating clusters of several computers.  But in this paper I decided to investigate how the use of all the data provided for training part affects the quality of the model. <br><br><h3>  Learning curve theory </h3><br>  As a research tool, I use the <b>Learning Curve</b> or learning curve, which is a graph consisting of the dependence of the average model error on the data used for training and the dependence of the average error on the test data.  In theory, there are two main options that are obtained when building this graph. <br><br><img src="https://habrastorage.org/files/716/197/cf9/716197cf97534e8b8d55c2767f95c62c.png"><br><br>  The first option is when the model is under-trained or has a high <b>bias</b> .  The main symptom of this situation is a high average error for both training and test data.  In this case, attracting additional data will not improve the quality of the model.  The second option is when the model is retrained or has a large variability ( <b>High variance</b> ).  Visually, you can determine the presence of a large gap between the test and training curves and low training error.  Here, on the contrary, more data may lead to an improvement in the test error and, accordingly, to an improvement in the model. <br><br><h3>  Data processing </h3><br>  Divide the sample into training and test in the ratio of <b>60/40</b> <br><br><pre> <code class="hljs perl">data_train$label &lt;- as.factor(data_train$label) set.seed(<span class="hljs-number"><span class="hljs-number">111</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">split</span></span> &lt;- createDataPartition(data_train$label, p = <span class="hljs-number"><span class="hljs-number">0</span></span>.<span class="hljs-number"><span class="hljs-number">6</span></span>, list = FALSE) train &lt;- slice(data_train, <span class="hljs-keyword"><span class="hljs-keyword">split</span></span>) test &lt;- slice(data_train, -<span class="hljs-keyword"><span class="hljs-keyword">split</span></span>)</code> </pre><br>  If you look at the images of the figures given above, you can see that, because  they are centered, then at the edges there is a lot of space on which the figure itself never happens.  That is, in the data this feature will be expressed in features that have a constant value for all objects.  Firstly, such signs do not carry any information for the model and, secondly, for many models, with the exception of those based on trees, can lead to errors in learning.  Therefore, you can remove these attributes from the data. <br><br><pre> <code class="hljs mel">zero_var_col &lt;- nearZeroVar(train, saveMetrics = T) sum(zero_var_col$nzv) ## [<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">532</span></span> train_nzv &lt;- train[, !zero_var_col$nzv] test_nzv &lt;- test[, !zero_var_col$nzv]</code> </pre><br>  Such signs were <b>532</b> of <b>784</b> .  To check how this significant change affected the quality of the models, we will train a simple <b>CART</b> model (which should not be adversely affected by the presence of permanent features) on the data before and after the change.  As an estimate, the average percentage of error on the test data is given. <br><br><pre> <code class="hljs vhdl"><span class="hljs-keyword"><span class="hljs-keyword">library</span></span>(rpart) model_tree &lt;- rpart(<span class="hljs-keyword"><span class="hljs-keyword">label</span></span> ~ ., data = train, method=<span class="hljs-string"><span class="hljs-string">"class"</span></span> ) predict_data_test &lt;- predict(model_tree, newdata = test, <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = <span class="hljs-string"><span class="hljs-string">"class"</span></span>) sum(test$<span class="hljs-keyword"><span class="hljs-keyword">label</span></span> != predict_data_test)/nrow(test) ## [<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.383507</span></span> model_tree_nzv &lt;- rpart(<span class="hljs-keyword"><span class="hljs-keyword">label</span></span> ~ ., data = train_nzv, method=<span class="hljs-string"><span class="hljs-string">"class"</span></span> ) predict_data_test_nzv &lt;- predict(model_tree_nzv, newdata = test_nzv, <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = <span class="hljs-string"><span class="hljs-string">"class"</span></span>) sum(test_nzv$<span class="hljs-keyword"><span class="hljs-keyword">label</span></span> != predict_data_test_nzv)/nrow(test_nzv) ## [<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-number"><span class="hljs-number">0.3838642</span></span></code> </pre><br>  Since  changes have affected a hundredth of a percent, then you can further use data with remote signs <br><br><pre> <code class="hljs bash">train &lt;- train[, !zero_var_col<span class="hljs-variable"><span class="hljs-variable">$nzv</span></span>] <span class="hljs-built_in"><span class="hljs-built_in">test</span></span> &lt;- <span class="hljs-built_in"><span class="hljs-built_in">test</span></span>[, !zero_var_col<span class="hljs-variable"><span class="hljs-variable">$nzv</span></span>]</code> </pre><br><h3>  CART </h3><br>  Finally, we construct the learning curve itself.  A simple <b>CART</b> model was applied without changing the default parameters.  To obtain statistically significant results, each assessment was performed on each sample size value five times. <br><br><pre> <code class="hljs pgsql">learn_curve_data &lt;- data.frame(<span class="hljs-type"><span class="hljs-type">integer</span></span>(), <span class="hljs-type"><span class="hljs-type">double</span></span>(), <span class="hljs-type"><span class="hljs-type">double</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">5</span></span> ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> seq(<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2000</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> = <span class="hljs-number"><span class="hljs-number">200</span></span>)) { train_learn &lt;- train[sample(nrow(train), size = i),] test_learn &lt;- test[sample(nrow(test), size = i),] model_tree_learn &lt;- rpart(label ~ ., data = train_learn, <span class="hljs-keyword"><span class="hljs-keyword">method</span></span>="class" ) predict_train_learn &lt;- predict(model_tree_learn, <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = "class") error_rate_train_rpart &lt;- sum(train_learn$label != predict_train_learn)/i predict_test_learn &lt;- predict(model_tree_learn, newdata = test_learn, <span class="hljs-keyword"><span class="hljs-keyword">type</span></span> = "class") error_rate_test_rpart &lt;- sum(test_learn$label != predict_test_learn)/i learn_curve_data &lt;- rbind(learn_curve_data, c(i, error_rate_train_rpart, error_rate_test_rpart)) } }</code> </pre><br><br>  Averaging was performed using the <b>GAM</b> model. <br><br><pre> <code class="hljs lisp">colnames(<span class="hljs-name"><span class="hljs-name">learn_curve_data</span></span>) &lt;- c(<span class="hljs-string"><span class="hljs-string">"Size"</span></span>, <span class="hljs-string"><span class="hljs-string">"Train_Error_Rate"</span></span>, <span class="hljs-string"><span class="hljs-string">"Test_Error_Rate"</span></span>) library(<span class="hljs-name"><span class="hljs-name">reshape2</span></span>) library(<span class="hljs-name"><span class="hljs-name">ggplot2</span></span>) learn_curve_data_long &lt;- melt(<span class="hljs-name"><span class="hljs-name">learn_curve_data</span></span>, id = <span class="hljs-string"><span class="hljs-string">"Size"</span></span>) ggplot(<span class="hljs-name"><span class="hljs-name">data=learn_curve_data_long</span></span>, aes(<span class="hljs-name"><span class="hljs-name">x=Size</span></span>, y=value, colour=variable)) + geom_point() + stat_smooth(<span class="hljs-name"><span class="hljs-name">method</span></span> = <span class="hljs-string"><span class="hljs-string">"gam"</span></span>, formula = y ~ s(<span class="hljs-name"><span class="hljs-name">x</span></span>), size = <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><br><img src="https://habrastorage.org/files/f31/9e4/ddc/f319e4ddc2c44407940daf555d0d9e3b.png"><br><br>  What do we see? <br><br><ul><li>  The change in the average error percentage occurs monotonously, starting with 500 objects in the sample. </li><li>  The error for both training and test data is quite high. </li><li>  The gap between test and training data is small. </li><li>  Test error is not reduced. </li></ul><br>  To summarize, the <b>CART</b> model is clearly under-trained, i.e.  has a constant high offset.  An increase in the sample for training will not lead to an improvement in the quality of the prediction on the test data.  In order to improve the results of this model, it is necessary to improve the model itself, for example, by introducing additional significant features. <br><br><h3>  Random forest </h3><br>  Now, we will evaluate the Random Forest model.  Again, the model was applied "as is", no parameters were changed.  The initial sample size is changed to 100, since  A model cannot be built if there are substantially more features than objects. <br><br><pre> <code class="hljs erlang"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">library</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(randomForest)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">learn_curve_data</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">data</span></span></span><span class="hljs-function">.</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">frame</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(integer(), double(), double())</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">for</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(n in </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1</span></span></span></span><span class="hljs-function"><span class="hljs-params">:</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5</span></span></span></span><span class="hljs-function"><span class="hljs-params"> )</span></span></span><span class="hljs-function"> { </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">for</span></span></span><span class="hljs-function"> </span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(i in seq(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">100</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">5100</span></span></span></span><span class="hljs-function"><span class="hljs-params">, by = </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">1000</span></span></span></span><span class="hljs-function"><span class="hljs-params">))</span></span></span><span class="hljs-function"> { </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train_learn</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">train</span></span></span><span class="hljs-function">[</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(nrow(train), size = i)</span></span></span><span class="hljs-function">,] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test_learn</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">test</span></span></span><span class="hljs-function">[</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sample</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(nrow(test), size = i)</span></span></span><span class="hljs-function">,] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">model_learn</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">randomForest</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(label ~ ., data = train_learn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict_train_learn</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_learn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error_rate_train</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sum</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(train_learn$label != predict_train_learn)</span></span></span><span class="hljs-function">/</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">i</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict_test_learn</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">predict</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(model_learn, newdata = test_learn)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">error_rate_test</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sum</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(test_learn$label != predict_test_learn)</span></span></span><span class="hljs-function">/</span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">i</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">learn_curve_data</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">rbind</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(learn_curve_data, c(i, error_rate_train, error_rate_test))</span></span></span><span class="hljs-function"> } } </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">colnames</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(learn_curve_data)</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">c</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"Size"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"Train_Error_Rate"</span></span></span></span><span class="hljs-function"><span class="hljs-params">, </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"Test_Error_Rate"</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">learn_curve_data_long</span></span></span><span class="hljs-function"> &lt;- </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">melt</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(learn_curve_data, id = </span></span><span class="hljs-string"><span class="hljs-function"><span class="hljs-params"><span class="hljs-string">"Size"</span></span></span></span><span class="hljs-function"><span class="hljs-params">)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ggplot</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data=learn_curve_data_long, aes(x=Size, y=value, colour=variable))</span></span></span><span class="hljs-function"> + </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">geom_point</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function"> + </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">stat_smooth</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span></code> </pre><br><br><img src="https://habrastorage.org/files/d54/121/57a/d5412157a4274e60989d821ae0d05f42.png"><br><br>  Here we see a different situation. <br><br><ul><li>  The change in the average error rate also occurs monotonously. </li><li>  Test and training errors are small and continue to decrease. </li><li>  The gap between test and training data is small. </li></ul><br><br>  I believe that this graph shows a possible third option, i.e.  there is no retraining, because  there is no gap between the curves, but there is no obvious under-training.  I would say that with an increase in the sample, there will be a gradual decrease in the test and training errors until they reach the limit of internally inherent in the model and the improvement does not stop.  In this case, the schedule will be similar to the untrained.  Therefore, I think that increasing the sample size should lead, albeit to a small, but improving the quality of the model and, accordingly, it makes sense. <br><br><h3>  Support Vector Machine </h3><br>  Before proceeding with the study of the third model - <b>Support Vector Machine</b> , it is necessary to once again process the data.  We will carry out their standardization, since  This is necessary for the ‚Äúconvergence‚Äù of the algorithm. <br><br><pre> <code class="hljs lisp">library(<span class="hljs-string"><span class="hljs-string">"e1071"</span></span>) scale_model &lt;- preProcess(<span class="hljs-name"><span class="hljs-name">train</span></span>[, <span class="hljs-number"><span class="hljs-number">-1</span></span>], method = c(<span class="hljs-string"><span class="hljs-string">"center"</span></span>, <span class="hljs-string"><span class="hljs-string">"scale"</span></span>)) train_scale &lt;- predict(<span class="hljs-name"><span class="hljs-name">scale_model</span></span>, train[, <span class="hljs-number"><span class="hljs-number">-1</span></span>]) train_scale &lt;- cbind(<span class="hljs-name"><span class="hljs-name">train</span></span>[, <span class="hljs-number"><span class="hljs-number">1</span></span>], train_scale) test_scale &lt;- predict(<span class="hljs-name"><span class="hljs-name">scale_model</span></span>, test[, <span class="hljs-number"><span class="hljs-number">-1</span></span>]) test_scale &lt;- cbind(<span class="hljs-name"><span class="hljs-name">test</span></span>[, <span class="hljs-number"><span class="hljs-number">1</span></span>], test_scale)</code> </pre><br>  Now we will build a graph. <br><br><pre> <code class="hljs pgsql">learn_curve_data &lt;- data.frame(<span class="hljs-type"><span class="hljs-type">integer</span></span>(), <span class="hljs-type"><span class="hljs-type">double</span></span>(), <span class="hljs-type"><span class="hljs-type">double</span></span>()) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (n <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">5</span></span> ) { <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> seq(<span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">2010</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> = <span class="hljs-number"><span class="hljs-number">100</span></span>)) { train_learn &lt;- train_scale[sample(nrow(train_scale), size = i),] test_learn &lt;- test_scale[sample(nrow(test_scale), size = i),] model_learn &lt;- svm(label ~ ., data = train_learn, kernel = "radial", scale = F) predict_train_learn &lt;- predict(model_learn) error_rate_train &lt;- sum(train_learn$label != predict_train_learn)/i predict_test_learn &lt;- predict(model_learn, newdata = test_learn) error_rate_test &lt;- sum(test_learn$label != predict_test_learn)/i learn_curve_data &lt;- rbind(learn_curve_data, c(i, error_rate_train, error_rate_test)) } } colnames(learn_curve_data) &lt;- c("Size", "Train_Error_Rate", "Test_Error_Rate") learn_curve_data_long &lt;- melt(learn_curve_data, id = "Size") ggplot(data=learn_curve_data_long, aes(x=Size, y=<span class="hljs-keyword"><span class="hljs-keyword">value</span></span>, colour=variable)) + geom_point() + stat_smooth(<span class="hljs-keyword"><span class="hljs-keyword">method</span></span> = "gam", formula = y ~ s(x), size = <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><br><img src="https://habrastorage.org/files/614/5b0/96e/6145b096e9cd4dbea9019991d37210ae.png"><br><br><ul><li>  The training error is very small. </li><li>  There is a significant gap between the test and training curve, which monotonously decreases. </li><li>  The test error is quite small and continues to decrease. </li></ul><br><br>  I think that we are confronted with the second version of the theory, i.e.  the model is retrained or has high variability.  Based on this conclusion, we can confidently say that increasing the size of the training sample will lead to a significant improvement in the quality of the model. <br><br><h3>  findings </h3><br>  This work showed that the <b>Learning Curve</b> is a good tool in the data explorer‚Äôs arsenal both for evaluating the models used and for assessing the need to increase the sample of the data used. <br><br>  Next time I will talk about the application of the principal component method (PCA) to this problem. <br>  Stay in touch!) </div><p>Source: <a href="https://habr.com/ru/post/266727/">https://habr.com/ru/post/266727/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../266713/index.html">Design unification from the backend: JavaScript on the server</a></li>
<li><a href="../266715/index.html">Borrowing and time in Rust</a></li>
<li><a href="../266717/index.html">Study: Almost all popular firewalls miss XSS attacks</a></li>
<li><a href="../266719/index.html">Screentendo - Generate levels for Super Mario Bros based on screen content.</a></li>
<li><a href="../266721/index.html">Z-Desk - geometric constructions in space</a></li>
<li><a href="../266729/index.html">We log context of exceptions</a></li>
<li><a href="../266731/index.html">Run a mega-manual from Stackoverflow</a></li>
<li><a href="../266733/index.html">Why SMS is limited to 160 characters, and Twitter messages - 140 characters?</a></li>
<li><a href="../266735/index.html">Snaql. Raw SQL in Python projects</a></li>
<li><a href="../266743/index.html">Python 3.5; async / await</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>