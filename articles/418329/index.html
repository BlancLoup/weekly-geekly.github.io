<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Organization of safe testing in production. Part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this part of the article we will continue to consider various types of testing in production. Those who missed the first part can read it here . Th...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Organization of safe testing in production. Part 2</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/ht/ra/sh/htrashsucmi8c4a6nok7rihdoay.png"><br><br>  In this part of the article we will continue to consider various types of testing in production.  Those who missed the first part can read it <a href="http://habr.com/company/funcorp/blog/418081/">here</a> .  The rest - welcome under cat. <br><a name="habracut"></a><br><h2>  Production Testing: Release </h2><br>  Having tested the service after <i>deployment</i> , it must be prepared for <i>release</i> . <br>  It is important to note that at this stage the rollback of changes is possible only in situations of <i>sustained</i> failure, for example: <br><br><ul><li>  cycling service failure; </li><li>  a time-out for a significant number of upstream connections, causing a strong increase in error rates; </li><li>  unacceptable configuration changes, for example, the absence of a secret key in an environment variable that causes a failure in the service (environment variables are generally best avoided, but this is a topic for a separate discussion). </li></ul><br>  Careful testing at the <i>deployment</i> stage ideally allows minimizing or completely avoiding unpleasant surprises at the <i>release</i> stage.  However, there are a number of recommendations for the safe release of new code. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Canary Deployment </h3><br>  <i>Canary Deployment</i> is a partial <i>release of production</i> service.  As the basic health check progresses, small amounts of current production-production traffic are sent to the released parts.  The results of the service parts are monitored as the traffic is processed, the indicators are compared with the reference ones (not related to canary ones), and if they fall outside the acceptable threshold values, the previous state is rolled back.  Although this approach is usually applied when server software is released, <a href="http://cloudplatform.googleblog.com/2017/03/how-release-canaries-can-save-your-bacon-CRE-life-lessons.html">canary client software testing</a> is also becoming more common. <br><br>  Various factors influence how much traffic will be used for canary deployment.  In a number of companies, the released parts of the service first receive only internal user traffic (so-called dogfooding).  If no errors are observed, then a small part of the production-environment traffic is added, after which a full-scale deployment is performed.  Rollback to a previous state in case of unacceptable results of canary deployment is <a href="http://medium.com/netflix-techblog/deploying-the-netflix-api-79b6176cc3f0">recommended to be performed automatically</a> , and tools such as <a href="http://blog.spinnaker.io/can-i-push-that-building-safer-low-risk-deployments-with-spinnaker-a27290847ac4">Spinnaker</a> provide built-in support for automated analysis and rollback functions. <br><br>  There are some problems with canary testing, and <a href="http//dreynaud.fail/canaries-in-practice/">this article</a> presents a fairly comprehensive overview of them. <br><br><h3>  Monitoring </h3><br>  Monitoring is an absolutely necessary procedure at <i>every</i> stage of product deployment in production, but this function will be especially important at the <i>release</i> stage.  Monitoring is well suited for obtaining information about the overall level of system performance.  But monitoring everything in the world may not be the best solution.  <i>Effective</i> monitoring is performed pointwise, which reveals a small set of modes of sustained system failure or a basic set of indicators.  Examples of such failure modes are: <br><br><ul><li>  an increase in the frequency of errors; </li><li>  reducing the overall speed of processing requests in the entire service, at a specific endpoint, or, even worse, complete cessation of work; </li><li>  increase delay. </li></ul><br>  Observation of any of these sustained failure modes is the basis for an immediate rollback to a previous state or roll forward of new <i>released</i> software versions.  It is important to remember that monitoring at this stage is unlikely to be complete and revealing.  Many believe that the ideal number of monitored signals during monitoring is from 3 to 5, but <i>definitely</i> not more than 7‚Äì10.  The Facebook technical document on Kraken offers the following solution: <br><br>  <i>"The problem is solved with the help of a freely configurable component for monitoring, which is reported with two basic indicators (the 99th percentile of the response time of the web server and the frequency of fatal HTTP errors) that objectively describe the quality of interaction with users."</i> <br><br>  The set of system indicators and applications that are monitored during the release phase is best defined during system design. <br><br><h3>  Exception tracking </h3><br>  We are talking about tracking exceptions at the release stage, although it would seem that at the <i>deployment</i> stages and after the release it would be equally useful.  Exception tracking tools often do not guarantee the same thoroughness, accuracy, and mass coverage as some other system control tools, but they can still be very useful. <br><br>  Open source tools (for example, <a href="http://github.com/getsentry/sentry">Sentry</a> ) display extensive information about incoming requests and create stacks of trace data and local variables, which greatly simplifies the debugging process, usually consisting of viewing event logs.  Exception tracking is also useful when sorting and prioritizing problems that do not require a full rollback to a previous state (for example, a borderline case that causes an exception). <br><br><h3>  Traffic shaping </h3><br>  Traffic shaping (traffic redistribution) is not so much an independent form of testing, but a tool to support the canary approach and the phased release of a new code.  In essence, traffic shaping is provided by updating the load balancer configuration, which allows you to gradually redirect more and more traffic to the new <i>released</i> version. <br><br>  This method is also useful in the phased deployment of new software (separately from the usual deployment).  Consider an example.  In June 2016, imgix needed to deploy a fundamentally new infrastructure architecture.  After the first testing of the new infrastructure with the help of some amount of dark traffic, they started to deploy in production, initially redirecting about 1% of the production media to the new stack.  Then, over a period of several weeks, they increased the volume of data entering the new stack (solving incidentally arising problems) until it began to process 100% of the traffic. <br><br>  The popularity of the service mesh architecture has led to a new surge of interest in proxy servers.  As a result, both old (nginx, HAProxy) and new (Envoy, Conduit) proxy servers added support for new features in an attempt to outperform the competition.  It seems to me that the future, in which the redistribution of traffic from 0 to 100% at the stage of product release is carried out automatically, is not far off. <br><br><h2>  Production Testing: After Release </h2><br>  Testing after release is carried out in the form of a check performed <i>after a</i> successful <i>release of the</i> code.  At this stage, you can be sure that the code as a whole is correct, it is successfully <i>released</i> in production and processes the traffic properly.  Deployed code is directly or indirectly used in the real world, serving real customers or performing tasks that have a significant impact on the business. <br><br>  The goal of any testing at this stage is basically to check the performance of the system, taking into account possible various loads and traffic patterns.  The best way to do this is to collect documentary evidence about everything that happens in production, and use them both for debugging and for getting a complete picture of the system. <br><br><h3>  Feature Flagging, or Dark Start </h3><br>  <a href="http//code.flickr.net/2009/12/02/flipping-out/">The oldest publication</a> about the successful use of feature flags (function flags) that I managed to find was published almost ten years ago.  The <a href="http://featureflags.io/">featureflags.io</a> website provides the most comprehensive guide on this issue. <br><br>  <i>‚ÄúFeature flagging is a method used by developers for marking a new function with the help of if-then statements, which allows for more careful control of its release.</i>  <i>By marking a function with a flag and isolating it in this way, the developer is able to turn this function on and off regardless of the deployment status.</i>  <i>This makes it possible to effectively separate the release of a function from the deployment of a code. ‚Äù</i> <br><br>  By marking a new code with a flag, you can test its performance and performance in production as needed.  Feature flagging is one of the generally accepted types of testing in production, it is well known and often <a href="http://codeascraft.com/2011/02/04/how-does-etsy-manage-development-and-operations/">described</a> in <a href="http://zachholman.com/posts/deploying-software">various sources</a> .  Much less well known is the fact that this method can also be used in the process of testing the <a href="http://featureflags.io/feature-flags-database-migrations/">transfer of databases</a> or software for personal systems. <br><br>  What the authors of the articles rarely write about is the best methods for developing and applying flags of functions.  Uncontrolled use of flags can be a serious problem.  Lack of discipline in terms of removing unused flags after a specified period sometimes leads to the fact that it is necessary to conduct a full audit and remove obsolete flags accumulated over the months (if not years) of work. <br><br><h3>  A / B testing </h3><br>  <a href="http://en.wikipedia.org/wiki/A/B_testing">A / B testing is</a> often performed as part of experimental analysis and is not considered testing in production.  For this reason, A / B tests are not only widely (sometimes even in <a href="http://techcrunch.com/2014/06/29/ethics-in-a-data-driven-world/">dubious</a> ways) used, but also <a href="http://dl.acm.org/citation.cfm%3Fid%3D2926731">actively</a> <a href="http//fabijan.info/papers/ICSE17_TheEvolutionOfCE_preprint.pdf">studied</a> and <a href="http://hbr.org/2017/06/a-refresher-on-ab-testing">described</a> (including articles <a href="http://www.exp-platform.com/Documents/2016KDDMetricDevelopmentLessonsDengShi.pdf">that define an effective scorecard</a> for online experiments).  More rarely, A / B tests are used to test various hardware configurations or virtual machines.  They are often called ‚Äútuning‚Äù (for example, tuning a JVM), but they are not classified as typical A / B tests (although tuning can be considered as a type of A / B test performed with the same level of rigor when it comes to measurements) . <br><br><h3>  Logs, events, indicators and tracing </h3><br>  About the so-called ‚Äúthree whales of observability‚Äù - logs, indicators and distributed tracing <a href="http://medium.com/%40copyconstruct/monitoring-in-the-time-of-cloud-native-c87c7a5bfa3e">can be read here</a> . <br><br><h3>  Profiling </h3><br>  In some cases, in order to diagnose performance problems, it is necessary to use application profiling in production.  Depending on supported languages ‚Äã‚Äãand runtime environments, profiling can be a fairly simple procedure, which involves adding just one line of code to the application ( <code>import _ "net/http/pprof"</code> for the Go case).  On the other hand, it may require the use of a variety of tools or the testing of the process performed by the black box method and checking the results using tools such as <a href="http://brendangregg.com/flamegraphs.html">flamegraphs</a> . <br><br><h3>  Tee test </h3><br>  Many people consider such testing to be something like shadow data duplication, since in both cases the production-environment traffic is sent to non-production clusters or processes.  In my opinion, the difference is that the use of traffic for <i>testing</i> purposes is somewhat different from its use for <i>debugging</i> purposes. <br><br>  <a href="https//codeascraft.com/2015/04/06/experimenting-with-hhvm-at-etsy/">Etsy wrote in its blog</a> about using tee tests as a verification tool (this example really resembles shadow data duplication). <br>  <i>‚ÄúHere, <b>tee</b> can be taken as a command <a href="http://en.wikipedia.org/wiki/Tee_%2528command%2529">tee</a> on the command line.</i>  <i>We wrote an <a href="http-request-cloning-via-irules-part-1">iRule</a> rule based on the existing F5 load balancer to clone HTTP traffic sent to one of the pools and redirect it to another pool.</i>  <i>Thus, we were able to use the production traffic sent to our API cluster and send a copy of it to the HHVM experimental cluster, as well as to an isolated PHP cluster for comparison.</i> <i><br></i>  <i>This technique has been very effective.</i>  <i>It allowed us to compare the performance of two configurations using identical traffic profiles. ‚Äù</i> <br><br>  However, sometimes performing a tee-test based on production-environment traffic in a stand-alone system is required for <i>debugging</i> .  In such cases, the autonomous system can be modified to customize the display of additional diagnostic information or another compilation procedure (for example, using the stream cleaning tool), which greatly simplifies the troubleshooting process.  In such cases, tee-tests should be considered rather as <i>debugging tools</i> , rather than <i>verification</i> . <br><br>  In the past, in <a href="http://www.imgix.com/">imgix,</a> these types of debugging were relatively rare, but still used, especially if we were talking about problems with debugging sensitive applications. <br><br>  For example, below is an analytical description of one of these incidents that occurred in 2015.  Error 400 occurred so rarely that it was hardly seen when they tried to reproduce the problem.  She appeared literally in several cases out of a billion.  During the day they were quite a bit.  As a result, it turned out that it is simply impossible to reliably reproduce the problem, so it was necessary to perform debugging using work traffic in order to have a chance to track the occurrence of this error.  This is what my former colleague wrote about this: <br><br>  <i>‚ÄúI chose a library that was supposed to be internal, but eventually I had to create my own based on the library provided by the system.</i>  <i>In the version provided by the system, an error periodically occurred, which did not manifest itself in any way while the traffic volume was small.</i>  <i>However, the true problem was the truncated name in the title.</i> <i><br><br></i>  <i>Over the next two days, I studied in detail the problem associated with the increased frequency of false errors 400. The error manifested itself in a very small number of requests, and problems of this type are rather difficult to diagnose.</i>  <i>All this was similar to the notorious needle in a haystack: the problem occurred in one case per billion.</i> <i><br><br></i>  <i>The first step in locating the source of errors was to receive all the raw HTTP request source data that resulted in an incorrect response.</i>  <i>To perform a tee test of incoming traffic when connecting to a socket, I added the end point of the Unix domain socket to the render server.</i>  <i>The idea was to allow us to quickly and without special expenses turn on and off the flow of dark traffic and conduct testing directly on the developer's computer.</i>  <i>To avoid problems in production, it was necessary to break the connection if there was a problem with back-pressure.</i>  <i>Those.</i>  <i>if the duplicate did not cope with the task, it was turned off.</i>  <i>This socket was quite useful in a number of cases during development.</i>  <i>This time, however, we used it to collect incoming traffic on selected servers, hoping to receive a sufficient number of requests in order to identify a pattern that led to false errors 400. Using dsh and netcat, I was able to bring incoming traffic to a local file relatively easily .</i> <i><br><br></i>  <i>Most of the medium was spent on collecting this data.</i>  <i>As soon as we had enough data, I was able to use netcat to reproduce it on a local system, the configuration of which was changed to display a large amount of debugging information.</i>  <i>And everything went fine.</i>  <i>The next step is to play data at the highest possible speed.</i>  <i>In this case, the loop with the condition check sent the raw requests alternately.</i>  <i>After about two hours I managed to achieve the desired result.</i>  <i>The data in the logs showed no header!</i> <i><br><br></i>  <i>I use a red-black tree to transfer the headers.</i>  <i>Such structures consider comparability as identity, which in itself is very useful if there are special requirements for keys: in our case, the HTTP headers are not case sensitive.</i>  <i>At first we thought that the problem was in the leaf node of the library used.</i>  <i>The order of addition really influences the order of building the base tree, and balancing the red-black tree is a rather complicated process.</i>  <i>And although this situation was unlikely, it was not impossible.</i>  <i>I switched to another red-black wood implementation.</i>  <i>It was fixed a few years ago, so I decided to embed it directly in the source code in order to get exactly the version that was needed.</i>  <i>However, the assembly chose a different version, and since I was counting on a newer version, I ended up with incorrect behavior.</i> <i><br><br></i>  <i>Because of this, the visualization system gave 500 errors, which led to the interruption of the cycle.</i>  <i>That is why the error occurred only over time.</i>  <i>After cyclic processing of several assemblies, the traffic from them was redirected along a different route, which increased the scale of the problem on this server.</i>  <i>My assumption that the problem was in the library turned out to be wrong, and the reverse switching eliminated 500 errors.</i> <i><br><br></i>  <i>I returned to error 400: there was still a problem with the error, which took about two hours to find.</i>  <i>The change of the library obviously did not solve the problem, but I was sure that the chosen library was sufficiently reliable.</i>  <i>Not realizing the fallacy of choice, I did not change anything.</i>  <i>Having studied the situation in more detail, I realized that the correct value was stored in a one-character header (for example, ‚Äúh: 12345‚Äù).</i>  <i>It finally came to me that h is the end character of the Content-Length header.</i>  <i>After reviewing the data, I realized that the Content-Length header was empty.</i> <i><br><br></i>  <i>As a result, the whole thing was in the offset error by one when reading the headers.</i>  <i>The nginx / joyent HTTP analyzer creates partial data, and each time the partial header field was one character shorter than necessary, I sent a header without a value and subsequently received a single-character header field containing the correct value.</i>  <i>This is a rather rare combination, so its activation takes such a long time.</i>  <i>So I increased the volume of data collection with each appearance of a one-character header, applied the proposed correction, and successfully executed the script for several hours.</i> <i><br></i>  <i>Of course, there could have been any other pitfalls with the library malfunction mentioned, but both errors were eliminated. ‚Äù</i> <i><br></i> <br><br>  Engineers involved in the development of delay-sensitive applications need debugging using captured dynamic traffic, since errors often occur that cannot be reproduced during unit testing or detected using the monitoring tool (especially if there is a serious delay in logging). <br><br><h3>  Chaos Engineering Approach </h3><br>  <i>Chaos Engineering is an approach based on conducting experiments on a distributed system in order to confirm its ability to withstand the chaotic production environment.</i> <br><br>  The Chaos Engineering method, first made known by the <a href="http://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116">Chaos Monkey</a> tool from Netflix, has now become an independent discipline.  The term Chaos Engineering appeared quite recently, but testing by the method of introducing faults is a long-standing practice. <br><br>  The term "chaotic testing" refers to the following techniques: <br><br><ul><li>  shutdown of arbitrary nodes to determine how resistant the system is to their failure; </li><li>  introducing errors (for example, increasing the delay) in order to confirm that the system is processing them correctly; </li><li>  forced disruption of the network in order to determine the reaction of the service. </li></ul><br>  Most companies use an insufficiently complex and multi-layered operating environment to effectively conduct random testing.  It is important to emphasize that the introduction of faults in the system is best done after setting up basic fault tolerance functions.  <a href="http://www.gremlin.com/media/20171210%2520%25E2%2580%2593%2520Chaos%2520Engineering%2520White%2520Paper.pdf">This technical document</a> from <a href="http://www.gremlin.com/">Gremlin</a> contains a fairly complete description of the principles of chaotic testing, as well as instructions for preparing for this procedure. <br><br>  <i>‚ÄúEspecially important is the fact that Chaos Engineering is considered a scientific discipline.</i>  <i>Within this discipline, highly accurate engineering processes are applied.</i> <i><br></i>  <i>Chaos Engineering's task is <b>to inform</b> users of something new about system vulnerabilities by experimenting with it.</i>  <i>It is necessary to identify all the hidden problems that could arise in the production, even before they cause a major failure.</i>  <i>Only after that you will be able to effectively eliminate all weaknesses in the system and make it truly fault tolerant. ‚Äù</i> <br><br><h2>  Conclusion </h2><br>  The goal of production testing is not to completely <i>eliminate</i> all possible system failures.  <a href="http://twitter.com/allspaw">John Allspaw</a> says: <br>  <i>‚Äú <a href="http://queue.acm.org/detail.cfm%3Fid%3D2353017">We see that</a> systems are becoming increasingly resilient - and that's fine.</i>  <i>But it must be admitted: ‚Äúmore and more‚Äù is not equal to ‚Äúabsolutely.‚Äù</i>  <i>In any complex system, failure can occur (and will occur) in the most unpredictable way. ‚Äù</i> <i><br></i> <br>  Testing in production at first glance can seem like quite a challenge, going far beyond the competence of most engineering companies.  And although such testing is <i>not</i> an <i>easy</i> task, associated with some risks, if you perform it according to all the rules, it will help to ensure the reliability of complex distributed systems that are found everywhere today. </div><p>Source: <a href="https://habr.com/ru/post/418329/">https://habr.com/ru/post/418329/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../418319/index.html">LED lamps Gauss 2017-2018 year</a></li>
<li><a href="../418321/index.html">Photon random number generator: the most secure encryption?</a></li>
<li><a href="../418323/index.html">A little about the attributes of hosts, keyboard, code and SCS in the series "World of the Wild West¬ª (Westworld)</a></li>
<li><a href="../418325/index.html">Technical Director Qiwi namainil 500,000 bitcoins, that is 2.4% of the theoretically possible amount</a></li>
<li><a href="../418327/index.html">Shoemakers with boots: which smartphones experts choose for themselves</a></li>
<li><a href="../418331/index.html">Masters of servers and networks - with a holiday</a></li>
<li><a href="../418333/index.html">Uber robotic vehicles return to the roads, but people will drive them</a></li>
<li><a href="../418335/index.html">‚ÄúConfrontation‚Äù at Positive Hack Days 8: parsing chains of attacks</a></li>
<li><a href="../418337/index.html">Hot, summer DataGrip 2018.2</a></li>
<li><a href="../418339/index.html">"Manual" manipulator</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>