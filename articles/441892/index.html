<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating a Dataflow template for streaming data from Pub / Sub to BigQuery based on GCP using Apache Beam SDK and Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="At the moment I am engaged in the task of streaming (and transformation) of data. In some circles 
 such a process is known as ETL , i.e. extract, tra...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating a Dataflow template for streaming data from Pub / Sub to BigQuery based on GCP using Apache Beam SDK and Python</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/is/qf/7j/isqf7j7patfl6lgirlqfrfbz-k8.png" alt="image"></p><br><p>  At the moment I am engaged in the task of streaming (and transformation) of data.  In some circles <br>  such a process is known as <a href="https://ru.wikipedia.org/wiki/ETL">ETL</a> , i.e.  extract, transform and load information. </p><br><p>  The whole process involves the following <a href="https://cloud.google.com/gcp/">Google Cloud Platform</a> services: </p><br><ul><li>  <a href="https://cloud.google.com/pubsub/">Pub / Sub</a> - service for realtime data streaming </li><li>  <a href="https://cloud.google.com/dataflow/">Dataflow</a> is a data conversion service (can <br>  work in realtime and in batch mode) </li><li>  <a href="https://cloud.google.com/bigquery/">BigQuery</a> - a service for storing data in the form of tables <br>  (supports SQL) </li></ul><a name="habracut"></a><br><h5 id="0-tekuschee-polozhenie-del">  0. Current status </h5><br><p>  At the moment there is a working version of streaming on the above services, but in <br>  one of the <a href="">standard is</a> used as a template. </p><br><p>  The problem is that this template provides 1 to 1 data transfer, i.e.  on <br>  When entering the Pub / Sub, we have a JSON format string, the output is a BigQuery table with fields, <br>  which correspond to the keys of the objects at the top level of the input JSON. </p><br><h5 id="1-postanovka-zadachi">  1. Statement of the problem </h5><br><p> Create a dataflow template that would allow to get a table or tables at the output <br>  according to the given conditions.  For example, we want to create a separate table for each <br>  the value of a specific key input JSON.  It is necessary to take into account the fact that some <br>  Input JSON objects can contain nested JSON as a value.  is necessary <br>  be able to create BigQuery tables with <code>RECORD</code> fields to store <a href="https://cloud.google.com/bigquery/docs/nested-repeated">nested</a> <br>  data. </p><br><h5 id="2-podgotovka-k-resheniyu">  2. Preparation for the decision </h5><br><p>  To create a dataflow template, use the <a href="https://beam.apache.org/">Apache Beam</a> <a href="https://beam.apache.org/get-started/downloads/">SDK</a> , which, in turn, <br>  supports java and python as a programming language.  Need to say that <br>  Only Python version 2.7x is supported, which surprised me a bit.  Moreover, support <br>  Java is a bit wider, since  for Python, for example, some functionality is not available and more <br>  A modest list of embedded <a href="https://beam.apache.org/documentation/io/built-in/">connectors</a> .  By the way, you can <a href="https://beam.apache.org/documentation/io/developing-io-python/">write</a> your connectors. </p><br><p>  However, due to the fact that I am not familiar with Java, I used Python. </p><br><p>  Before starting to create a template, you must have the following: </p><br><ol><li>  JSON input format and it should not change over time </li><li>  BigQuery schema or schema tables to which the data will be streamed </li><li>  the number of tables to which the output data stream will be streamed </li></ol><br><p>  Note that after creating a template and running Dataflow Job on its base, these parameters can be <br>  change only by creating a new template. </p><br><p>  Let's say a few words about these restrictions.  They all assume that there is no possibility <br>  create a dynamic template that could accept any string as input, parse it <br>  according to internal logic and then populate dynamically created tables with dynamically <br>  created by the scheme.  It is very likely that this possibility exists, but within the framework of these <br>  I was not able to implement the tools to implement such a scheme.  As I understand it all <br>  pipeline is built before it is executed at runtime and because of this there is no possibility to change it to <br>  fly.  Maybe someone will share their decision. </p><br><h5 id="3-reshenie">  3. Decision </h5><br><p>  For a more complete understanding of the process, it is worthwhile to give a diagram of the so-called <a href="https://beam.apache.org/documentation/pipelines/design-your-pipeline/">pipeline.</a> <br>  from the Apache Beam documentation. </p><br><p><img src="https://habrastorage.org/webt/yq/yi/3z/yqyi3zjiwpmjf4i6qp7x4znqv2c.png" alt="image"></p><br><p>  In our case (we will use the division into several tables): </p><br><ul><li>  input - data comes from PubSub to Dataflow Job </li><li>  <a href="https://beam.apache.org/documentation/programming-guide/">Transform</a> # 1 - the data is converted from a string to the Python dictionary, we have <br>  <a href="https://beam.apache.org/documentation/programming-guide/">PCollection</a> # 1 </li><li>  Transform # 2 - the data is tagged, for further separation into separate tables, into <br>  output we have PCollection # 2 (actually PCollection tuple) </li><li>  Transform # 3 - PCollection # 2 data is written to tables using schemas. <br>  tables </li></ul><br><p>  In the process of writing my own template, I was actively inspired by <a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples">these</a> examples. </p><br><div class="spoiler">  <b class="spoiler_title">Template code with comments (left comments from previous authors as well):</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># coding=utf-8 from __future__ import absolute_import import logging import json import os import apache_beam as beam from apache_beam.pvalue import TaggedOutput from apache_beam.options.pipeline_options import PipelineOptions from apache_beam.options.pipeline_options import SetupOptions from apache_beam.options.pipeline_options import StandardOptions from apache_beam.io.gcp.bigquery import parse_table_schema_from_json #  GCP  gcp_project = '' #  Pub/Sub  topic_name = '' # Pub/Sub    'projects/_GCP_/topics/_' input_topic = 'projects/%s/topics/%s' % (gcp_project, topic_name) #  BigQuery  bq_dataset = 'segment_eu_test' #       schema_dir = './' class TransformToBigQuery(beam.DoFn): #          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #   yield body class TagDataWithReqType(beam.DoFn): #      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element def run(): #       _.json   schema_dir,  #         ()  schema_dct = {} for schema_file in os.listdir(schema_dir): filename_list = schema_file.split('.') if filename_list[-1] == 'json': with open('%s/%s' % (schema_dir, schema_file)) as f: schema_json = f.read() schema_dct[filename_list[0]] = json.dumps({'fields': json.loads(schema_json)}) # We use the save_main_session option because one or more DoFn's in this # workflow rely on global context (eg, a module imported at module level). pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = True pipeline_options.view_as(StandardOptions).streaming = True # Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic) # Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery()) # Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()). with_outputs(*schema_dct.keys(), main='default') # Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), ) # Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), ) result = p.run() result.wait_until_finish() if __name__ == '__main__': logging.getLogger().setLevel(logging.INFO) logger = logging.getLogger(__name__) run()</span></span></code> </pre> </div></div><br><p>  Now let's go through the code and give explanations, but first we should say that the main <br>  the difficulty in writing this template is to think in the category of "data flow", and <br>  not a specific message.  It is also necessary to understand that Pub / Sub handles messages and <br>  it is from them that we will receive information for tagging a stream. </p><br><pre> <code class="python hljs">pipeline_options = PipelineOptions() p = beam.Pipeline(options=pipeline_options) pipeline_options.view_as(SetupOptions).save_main_session = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span> pipeline_options.view_as(StandardOptions).streaming = <span class="hljs-keyword"><span class="hljs-keyword">True</span></span></code> </pre> <br><p>  Since  Apache Beam Pub / Sub IO connector is used only in streaming mode necessary <br>  add PipelineOptions () (although in fact the options are not used) otherwise the creation of a template <br>  falls with the exception.  Need to say about the options run the template.  They can be <br>  static and so-called "runtime".  <a href="https://cloud.google.com/dataflow/docs/guides/templates/creating-templates">Here is a</a> link to the documentation on this topic.  The options allow you to create a template without specifying the parameters in advance, but transferring them when you run the Dataflow Job from the template, but I never managed to implement it, probably due to the fact that this connector does not support <code>RuntimeValueProvider</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Read from PubSub into a PCollection. input_stream = p | beam.io.ReadFromPubSub(input_topic)</span></span></code> </pre> <br><p>  Everything is clear from the commentary, we read the stream from the topic.  It is worth adding that the flow can be taken <br>  both from the topic and from the subscription (subscription).  If the topic is specified as input, then <br>  a temporary subscription to this topic will be automatically created.  The syntax is also pretty <br>  clear, the input <code>beam.io.ReadFromPubSub(input_topic)</code> data stream is sent to our <br>  pipeline <code>p</code> . </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Transform stream to BigQuery IO format stream_bq = input_stream | 'transform to BigQuery' &gt;&gt; beam.ParDo(TransformToBigQuery())</span></span></code> </pre> <br><p>  Transform # 1 happens here and our input is converted from python string to <br>  python dict, and at the output we get PCollection # 1.  In the syntax appears <code>&gt;&gt;</code> .  On <br>  in fact, the text in quotes is the name of the stream (must be unique), as well as a comment, <br>  which will be added to the block on the graph in the GCP Dataflow web interface.  Consider more <br>  override class <code>TransformToBigQuery</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TransformToBigQuery</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#          ,   # BigQuery IO     python dict def process(self, element, *args, **kwargs): body = json.loads(element) #       ,      # python dict       ,     #  ,      python dict yield body</span></span></code> </pre> <br><p>  The <code>element</code> variable will contain one message from the PubSub subscription.  As seen from <br>  code, in our case it must be valid JSON.  In the class must be <br>  redefined <code>process</code> method in which the necessary conversions should be made <br>  the input line to get the output data that will fit the scheme <br>  tables in which this data will be loaded.  Since  our flow in this case is <br>  continuous, <code>unbounded</code> in terms of the Apache Beam, then you must return it using <br>  <code>yield</code> , not <code>return</code> , as for the final data stream.  In the case of a finite flow, you can <br>  (and need to) further customize <a href="https://beam.apache.org/documentation/programming-guide/"><code>windowing</code></a> and <a href="https://beam.apache.org/documentation/programming-guide/"><code>triggers</code></a> </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Tag stream by schema name tagged_stream = \ stream_bq \ | 'tag data by type' &gt;&gt; beam.ParDo(TagDataWithReqType()).with_outputs(*schema_dct.keys(), main='default')</span></span></code> </pre> <br><p>  This code directs PCollection # 1 to Transform # 2 where tagging will occur. <br>  (separation) data stream.  In the <code>schema_dct</code> variable in this case, the dictionary, where the key is the name of the schema file without an extension, this will be the tag and the value is the valid JSON schema. <br>  BigQuery tables for this tag.  It should be noted that the scheme should be transmitted precisely in <br>  as <code>{'fields': }</code> where <code></code> is the BigQuery table schema in the form of JSON (you can <br>  export from web interface). </p><br><p>  <code>main='default'</code> is the name of the tag of the thread that will go to <br>  all messages not subject to tagging conditions.  Consider the class <br>  <code>TagDataWithReqType</code> . </p><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">TagDataWithReqType</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">(beam.DoFn)</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#      , ..      #     ,       #  with_outputs + default def process(self, element, *args, **kwargs): req_type = element.get('_') types = ( 'type1', 'type2', 'type3', ) if req_type in types: yield TaggedOutput(req_type, element) else: yield element</span></span></code> </pre> <br><p>  Apparently, the <code>process</code> class is also redefined here.  <code>types</code> contains names <br>  tags and they must match the number and name with the number and names of the dictionary keys <br>  <code>schema_dct</code> .  Although the <code>process</code> method has the ability to take arguments, I still haven't <br>  was able to pass them.  The reason has not yet figured out. </p><br><p>  At the output we get a tuple of threads in the number of tags, namely the number of our <br>  predefined tags + default flow that could not be tagged. </p><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream unidentified data to default table tagged_stream.default | 'push to default table' &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.default' % ( gcp_project, bq_dataset, ), schema=parse_table_schema_from_json(schema_dct.get('default')), )</span></span></code> </pre> <br><p>  Transform # ... (in fact, it is not on the diagram, it is a "branch") - we write the default flow <br>  in the default table. </p><br><p>  <code>tagged_stream.default</code> - the stream is taken with the <code>default</code> tag, the alternative syntax is <code>tagged_stream['default']</code> </p><br><p>  <code>schema=parse_table_schema_from_json(schema_dct.get('default'))</code> - the schema is defined here <br>  tables.  Please note that the <code>default.json</code> file with a valid BigQuery table schema <br>  must be in the current <code>schema_dir = './'</code> directory. </p><br><p>  The stream will fall into a table named <code>default</code> . </p><br><p>  If a table with the same name (in this dataset of this project) does not exist, then it <br>  will be automatically created from the schema due to the default setting <br> <code>create_disposition=BigQueryDisposition.CREATE_IF_NEEDED</code> </p> <br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Stream data to BigQuery tables by number of schema names for name, schema in schema_dct.iteritems(): tagged_stream[name] | 'push to table %s' % name &gt;&gt; beam.io.WriteToBigQuery( '%s:%s.%s' % ( gcp_project, bq_dataset, name), schema=parse_table_schema_from_json(schema), )</span></span></code> </pre> <br><p>  Transform # 3, everything should be clear to those who read the article from the very beginning and owns <br>  python syntax.  We divide a tuple of threads in a cycle and write each stream to its table with <br>  its scheme.  It should be recalled that the stream name must be unique - <code>'%s:%s.%s' % (gcp_project, bq_dataset, name)</code> . </p><br><p>  Now it should be clear how this works and you can create a template.  For this you need <br>  execute in the console (do not forget to activate venv if available) or from the IDE: </p><br><pre> <code class="bash hljs">python _.py / --runner DataflowRunner / --project dreamdata-test / --staging_location gs://STORAGE_NAME/STAGING_DIR / --temp_location gs://STORAGE_NAME/TEMP_DIR / --template_location gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> </pre> <br><p>  At the same time, access to the Google Account should be organized, for example, through export <br>  the environment variable <code>GOOGLE_APPLICATION_CREDENTIALS</code> or in some other <a href="https://cloud.google.com/docs/authentication/getting-started">way</a> . </p><br><p>  A few words about <code>--runner</code> .  In this case, the <code>DataflowRunner</code> says that this code <br>  will run as a template for Dataflow Job.  You can still specify <br>  <code>DirectRunner</code> , it will be used by default with no <code>--runner</code> option and code <br>  will work as a dataflow job, but locally, which is very convenient for debugging. </p><br><p>  If there are no errors, then <code>gs://STORAGE_NAME/TEMPLATES_DIR/TEMPLATE_NAME</code> will be <br>  template created.  It is worth saying that <code>gs://STORAGE_NAME/STAGING_DIR</code> will also be written <br>  service files that are necessary for the successful work of the Datafow Job created on the base <br>  template and delete them is not necessary. </p><br><p>  Next, you need to create a dataflow job using this template, manually or by any <br>  in another way (CI for example). </p><br><h5 id="4-vyvody">  4. Conclusions </h5><br><p>  Thus it was possible to organize streaming stream from PubSub to BigQuery using <br>  necessary data transformations for further storage, conversion and <br>  data usage. </p><br><h2 id="osnovnye-ssylki">  Main links </h2><br><ul><li>  <a href="https://beam.apache.org/documentation/programming-guide/">Apache Beam SDK</a> </li><li>  <a href="https%253A%252F%252Fcloud.google.com%252Fdataflow%252F%26usg%3DAOvVaw0O3fmNpRXk31R0Qv1Jt6j8">Google dataflow</a> </li><li>  <a href="https://cloud.google.com/bigquery/">Google bigquery</a> </li><li>  <a href="https://medium.com/%40jamesmoore255/creating-a-template-for-the-python-cloud-dataflow-sdk-2fe36cc4167f">James Moore article on medium</a> </li><li>  <a href="https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples">Python code examples for Apache Beam</a> </li></ul><br><p>  In this article possible inaccuracies and even mistakes, I will be grateful for the constructive <br>  criticism.  In the end, I want to add that in fact, far from all are used here. <br>  Apache Beam SDK features, but there was no such goal. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/441892/">https://habr.com/ru/post/441892/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../441878/index.html">You as you like, and I did</a></li>
<li><a href="../441880/index.html">Russia is not ready for unmanned vehicles</a></li>
<li><a href="../441882/index.html">VMware NSX for the smallest. Part 3. Configuring DHCP</a></li>
<li><a href="../441886/index.html">Over the past 12 years, I have never shown a summary</a></li>
<li><a href="../441890/index.html">Everything you need to know about the iOS App Extensions</a></li>
<li><a href="../441898/index.html">Sketch + Node.js: generate icons for multiple platforms and brands</a></li>
<li><a href="../441900/index.html">Satya Nadella spoke about cooperation with the Pentagon</a></li>
<li><a href="../441902/index.html">How technology creates new realities</a></li>
<li><a href="../441904/index.html">Installing IPS-display in Thinkpad T430S</a></li>
<li><a href="../441908/index.html">Can digital medicine resist hackers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>