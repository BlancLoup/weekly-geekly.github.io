<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating chatbot-a using sockeye (MXNet) based on AWS EC2 and AWS DeepLearning AMI</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, the AWSDeepLearning team released a new framework - ‚Äúsockeye‚Äù, the purpose of which is to simplify the learning of seq2seq networks. Looking...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating chatbot-a using sockeye (MXNet) based on AWS EC2 and AWS DeepLearning AMI</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/5c3/f1a/d01/5c3f1ad015265b8c733ec697fd5eafa8.png" alt="AWS AI" align="right">  Recently, the AWSDeepLearning team released a new framework - ‚Äúsockeye‚Äù, the purpose of which is to simplify the learning of seq2seq networks.  Looking ahead - I did not even expect such simplicity.  So I decided to write a simple, fast and self-sufficient manual, which does not require a deep knowledge of the reader in the field of neural networks.  The only thing that is required for successful completion of all steps is to have some experience with: <br><br><ul><li>  AWS EC2; </li><li>  SSH; </li><li>  python; </li></ul><br>  If all these three things do not cause problems - I ask for cat. <br><a name="habracut"></a><br>  <i>Before continuing, I want to express special thanks to my <a href="https://www.patreon.com/b0noi">patrons</a> who support me.</i> <br><br>  As mentioned, the AWS DeepLearning team recently released a new framework, ‚Äúsockeye‚Äù.  Let me quote from the official site: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  ... the sockeye project, a sequence-to-sequence framework for Neural Machine Translation Based on MXNet.  It implements the well-known encoder-decoder architecture with attention. </blockquote><br>  Free translation: <br><br><blockquote>  Sockeye is a framework for teaching neural networks to machine translation, which is based on the well-known architecture encoder-decoder. </blockquote><br>  Despite the fact that this framework is officially developed for training networks, for machine translation, technically it can also be used to train a more general class of tasks for converting one sequence to another (seq2seq).  I have already touched on the topic of why machine translation and the creation of chat bots, as two tasks, have a lot of similarities and can be solved by similar methods, in one of the <a href="https://habrahabr.ru/post/317732/">past articles</a> .  So I will not repeat leaving the opportunity for inquisitive readers to follow the links, and in the meantime I will go directly to creating a chat bot. <br><br><h2>  Process description </h2><br>  In general, the process consists of the following steps: <br><br><ol><li>  Raise EC2 machine with GPU based on DeepLearning AMI </li><li>  Prepare EC2 machine for training </li><li>  Start learning </li><li>  Wait </li><li>  Profit </li></ol><br><h2>  We raise EC2 machine with GPU, based on AWS DeepLearning AMI </h2><br>  In this article we will use <a href="https://aws.amazon.com/amazon-ai/amis/">AWS DeepLearning AMI</a> , hereafter: DLAMI (by the way, if you don‚Äôt know what AMI is, I recommend reading the official documentation <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">here</a> ).  The main reasons for using this particular AMI: <br><br><ul><li>  it includes Nvidia CUDA drivers (at the time of this writing, version: 7.5); </li><li>  compiled with GPU support - MXNet; </li><li>  includes all (almost) utilities that we need, for example: git; </li><li>  can be used with, surprise-surprise, machines in which there is a GPU. </li></ul><br>  In order to quickly create the car we need from AMI, go to <a href="https://aws.amazon.com/marketplace/pp/B01M0AXXQB">the DLAMI page in the AWS Marketplace</a> .  Here you should pay attention to the following things: <br><br><h3>  1. AMI version </h3><br><img src="https://habrastorage.org/web/7e8/982/903/7e8982903a1146669f89c4f91fef850b.png"><br><br>  At the time of writing, ‚ÄúJun 2017‚Äù was the most recent version, so if you want your process to be consistency with the rest of this one, I recommend choosing it. <br><br><h3>  2. Region to create </h3><br><img src="https://habrastorage.org/web/780/943/b1a/780943b1a8cd4723bddf061d97ade797.png"><br>  Keep in mind that not all types of machines with GPU are available in all regions.  Actually, even if they are formally available, it is not always possible to create them.  For example, in 2016 during the NIPS conference with them was very problematic.  We need a machine like p2, plus, and at the time of this writing, DLAMI was available only in those regions where this same type was available: <br><br><img src="https://habrastorage.org/web/20f/e98/0a7/20fe980a78a643718c3dc308f7912ea9.png"><br><br><h3>  3. Select an instance type </h3><br><img src="https://habrastorage.org/web/eb2/532/4a8/eb25324a894c426bb06a3a4899c3fdad.png"><br>  p2.xlarge - is the cheapest machine that meets our requirements for GPU memory (you can of course try g2.2xlarge, but don‚Äôt say after all that you were not warned).  At the time of writing, the price for it was ~ $ 0.9 per hour.  But better check the price <a href="https://aws.amazon.com/ec2/pricing/on-demand">on the official website</a> . <br><br><h3>  4. VPC </h3><br><img src="https://habrastorage.org/web/bb4/db4/4bb/bb4db44bbbd84da783c0d9f841392df1.png"><br>  If you do not know what to do with it - do not touch. <br><br><h3>  5. Security group </h3><br><img src="https://habrastorage.org/web/d89/055/7a3/d890557a35cd421b913f6d1b4d0f1449.png"><br>  Just as with the VPC, do not know - do not touch.  However, if you want to use an existing group, then make sure that SSL is open there. <br><br><h3>  6. Key pair </h3><br><img src="https://habrastorage.org/web/3ae/c13/ae9/3aec13ae961e401984051fbc4b1add34.png"><br>  Let me assume that the reader has experience with SSH and there is an understanding of what it is. <br><br><h3>  7. Click Create! </h3><br><img src="https://habrastorage.org/web/17d/988/c41/17d988c41bde4b849b6983318d6080ec.png"><br><br>  Actually now we can create a car and connect to it. <br><br><h2>  Preparing for network training </h2><br>  Connect to the newly created machine.  Immediately after, it's time to start the screen.  And do not forget the fact that when connecting you need to use the login ubuntu: <br><br>  A few comments on this point: <br><br><ul><li>  I said ‚Äúscreen‚Äù because DLAMI does not contain tmux out of the box !!!  Yeah, sad. </li><li>  if you do not know what screen or tmux is - no problem, you can just continue reading, everything will work without problems.  However, it is better to still go and read about what kind of animals these are: <a href="https://en.wikipedia.org/wiki/Tmux">tmux</a> (my choice) and <a href="https://en.wikipedia.org/wiki/GNU_Screen">screen</a> . </li></ul><br><h3>  1. install sockeye </h3><br>  The first thing we need is to install sockeye.  With DLAMI, the installation process is very simple, just one command: <br><br><pre><code class="bash hljs">sudo pip3 install sockeye --no-deps</code> </pre> <br>  The important part here is that you need to use pip3, and not just pip, since the default pip from DLAMI uses Python 2, which in turn is not supported in sockeye.  It is also not necessary to establish any dependencies for they are all already installed. <br><br><h3>  2. Preparation of data (dialogues) for training </h3><br>  For training, we will use ‚ÄúCornell Movie Dialogs Corpus‚Äù (https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).  This, in fact, a huge body of dialogue from the movies.  For training, you need to "cook" it, in fact, I have already implemented a script that prepares the body and told in more detail about not earlier. <br><br>  Well, now let's get these same data for training and prepare: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd ~/src src# git clone https://github.com/b0noI/dialog_converter.git Cloning into 'dialog_converter'‚Ä¶ remote: Counting objects: 59, done. remote: Compressing objects: 100% (49/49), done. remote: Total 59 (delta 33), reused 20 (delta 9), pack-reused 0 Unpacking objects: 100% (59/59), done. Checking connectivity‚Ä¶ done. src# cd dialog_converter dialog_converter git:(master)# git checkout sockeye_chatbot Branch sockeye_chatbot set up to track remote branch sockeye_chatbot from origin. Switched to a new branch 'sockeye_chatbot' dialog_converter git:(sockeye_chatbot)# python converter.py dialog_converter git:(sockeye_chatbot)# ls LICENSE README.md converter.py movie_lines.txt train.a train.b test.a test.b</span></span></code> </pre><br>  A couple of things you should pay attention to: <br><br><ul><li>  src folder already exists, no need to create it; </li><li>  note the brunch: ‚Äúsockeye_chatbot‚Äù, in it I keep the code that is consistent with this article.  Use master at your own risk. </li></ul><br>  Now let's create a folder where we will conduct training and copy all the data there: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd ~ # mkdir training # cd training training# cp ~/src/dialog_converter/train.* . training# cp ~/src/dialog_converter/test.* .</span></span></code> </pre><br>  That's all, we are ready to start learning ... <br><br><h2>  Training </h2><br>  With sockeye, the learning process is very simple - you just need to run a single command: <br><br><pre> <code class="bash hljs">python3 -m sockeye.train --<span class="hljs-built_in"><span class="hljs-built_in">source</span></span> train.a --target train.b --validation-source train.a --validation-target train.b --output model</code> </pre><br>  <i><u>I know, I know, NEVER use the same data for training and validation.</u></i>  <i><u>However, my script at the moment does not quite correctly break the data into two groups and therefore better results (I am about subjective assessment) are obtained, if not strange, without a breakdown.</u></i> <i><u><br></u></i> <br><img src="https://habrastorage.org/web/d76/b60/421/d76b60421d9d473fbff2ba7dd1002d56.png"><br><br>  If you read the <a href="https://habrahabr.ru/post/317732/">last article,</a> you might have noticed that sockeye is trying to find the right configuration for your training for you, namely: <br><br><ul><li>  dictionary size; </li><li>  network settings; </li><li>  etc.‚Ä¶ </li></ul><br>  This is pretty good, as a configuration that is closer to optimum can lead to faster (read less expensive) training.  Although you still need to see exactly how sockeye searches for parameters and how much resources are spent on this process. <br><br>  Also, sockeye will determine exactly when the learning process is worth completing.  This will occur if the quality of the model has not improved on the data for validation for the last 8 control points. <br><br><h2>  We are waiting for the result </h2><br>  While you wait, you can see how MXNet chews GPU resources during training.  To do this, run this command in a new window: <br><br><pre> <code class="bash hljs">watch -n 0.5 nvidia-smi</code> </pre> <br>  See something like: <br><br><img src="https://habrastorage.org/web/512/3dc/070/5123dc070d364c478ddc4c150cb49f6c.png"><br><br>  By the way, technically, to start a conversation with a bot, you only need to wait for at least the first created control point.  When this happens you will see something like this: <br><br><img src="https://habrastorage.org/web/38c/541/c31/38c541c31bf54d45837cb5b2363b7035.png"><br><br>  Now you can start chatting: <br><br><h2>  Chatsy ... </h2><br>  This process does not require stopping the training, you just need to open a new window (or a new SSH connection) to go to the same folder where the training takes place and execute the command: <br><br><pre> <code class="bash hljs">python3 -m sockeye.translate --models model --use-cpu --checkpoints 0005</code> </pre> <br>  Several elements on which I want to emphasize: <br><br><ul><li>  python3 - naturally; </li><li>  model - the name of the folder in which the learning process stores the model, must match the name specified during training </li><li>  - use-cpu - without this, MXNet will try to use the GPU, which most likely will end in failure as the learning process still uses it. </li><li>  --checkpoints 0005 - checkpoint number, taken from the console output at the time of saving the checkpoint. </li></ul><br>  After running the sockeye command, it will read the input from STDIN and output the answer to STDOUT.  Here are some examples: <br><br><h3>  after an hour of training: </h3><br><img src="https://habrastorage.org/web/39b/9b5/baa/39b9b5baa3e4464ca02e34e11377de54.png"><br><br><h3>  after 2 hours of training </h3><br><img src="https://habrastorage.org/web/d69/27d/0a8/d6927d0a824e4f61b857e551c4f5b98d.png"><br><br><h3>  after 3 hours of training, it began to threaten me =) </h3><br><img src="https://habrastorage.org/web/93f/451/2a6/93f4512a64b04edc8cbacf2fb28db074.png"><br><br><h2>  Conclusion </h2><br>  As you can see with sockeye the learning process is very simple.  Actually, the most difficult, perhaps, is to raise the necessary machine and connect to it =) I'm still waiting for someone from the readers: <br><br><ul><li>  created a bot by the master Yoda (well, or Vader); </li><li>  created a bot from the universe Lord of The Rings; </li><li>  created a bot from the StarWars universe. </li></ul><br>  <i>PS: do not forget to download your model and nail the car after graduation.</i> <br><br><h2>  Trained model </h2><br>  <a href="">Here</a> you can download a trained model (coached only 4 hours).  It can be used on a local machine to play with a bot if you don't want to train yours. </div><p>Source: <a href="https://habr.com/ru/post/332654/">https://habr.com/ru/post/332654/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332640/index.html">How generic-they save us from packing</a></li>
<li><a href="../332642/index.html">Simple field validation</a></li>
<li><a href="../332644/index.html">Tandem office and mobile telephony. How we developed FMC</a></li>
<li><a href="../332646/index.html">Security Week 27: ExPetr = BlackEnergy, more than 90% of sites are insecure, in RCE-vulnerability closed in Linux</a></li>
<li><a href="../332652/index.html">Using LibVirt API, InfluxDB and Grafana to collect and visualize VM execution statistics</a></li>
<li><a href="../332656/index.html">What did the PR leaders do on the day of the meeting between Putin and Trump on the G20?</a></li>
<li><a href="../332658/index.html">Personal experience: as an IT specialist, move to work in the USA, relying only on himself</a></li>
<li><a href="../332660/index.html">Script for express recovery Excel files after damage</a></li>
<li><a href="../332662/index.html">Interview in SD podCast with Pavel Odintsov, author of FastNetMon, a tool for detecting and repelling DDoS attacks</a></li>
<li><a href="../332664/index.html">Automata programming. Part 3. State and transition diagram. Continuation</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>