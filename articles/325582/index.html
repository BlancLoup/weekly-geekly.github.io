<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Fail-safe processing of 10M OAuth tokens on Tarantool</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Many have already heard about the performance of the Tarantool DBMS, its capabilities and features. For example, he has a cool disk storage - Vinyl, i...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Fail-safe processing of 10M OAuth tokens on Tarantool</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/268/083/ffd/268083ffd2fe4328a5c0b6c62fa66d02.jpg" alt="image"></p><br><p>  Many have already heard about the performance of the Tarantool DBMS, its capabilities and features.  For example, he has a cool disk storage - Vinyl, in addition, he knows how to work with JSON-documents.  But in numerous publications bypass one important feature.  Usually, databases are considered simply as storage, but still the distinctive feature of Tarantool is the ability to write code inside and work very effectively with this data.  Under the cut, the story of how we built one system almost entirely inside Tarantool, written in collaboration with Igor <a href="https://habrahabr.ru/users/igorcoding/" class="user_link">igorcoding</a> Latkin. </p><a name="habracut"></a><br><p>  All of you have come across Mail.Ru Mail and you probably know that it is possible to configure the collection of letters from other mailboxes.  To do this, we do not need to ask the user for a username and password from a third-party service if OAuth is supported.  In this case, OAuth tokens are used for access.  In addition, Mail.Ru Group has many other projects that also need authorization through third-party services, and which need to issue OAuth ‚Äì tokens of users to work with this or that application.  We were engaged in the development of such a service for the storage and updating of tokens. </p><br><p>  Probably everyone knows what an OAuth ‚Äì token is.  Recall, most often this is a structure of three or four fields: </p><br><pre><code class="perl hljs">{ <span class="hljs-string"><span class="hljs-string">"token_type"</span></span> : <span class="hljs-string"><span class="hljs-string">"bearer"</span></span>, <span class="hljs-string"><span class="hljs-string">"access_token"</span></span> : <span class="hljs-string"><span class="hljs-string">"XXXXXX"</span></span>, <span class="hljs-string"><span class="hljs-string">"refresh_token"</span></span> : <span class="hljs-string"><span class="hljs-string">"YYYYYY"</span></span>, <span class="hljs-string"><span class="hljs-string">"expires_in"</span></span> : <span class="hljs-number"><span class="hljs-number">3600</span></span> }</code> </pre> <br><ul><li>  <strong>access_token</strong> , with which you can perform an action, get information about a user, download a list of his friends, and so on; </li><li>  <strong>refresh_token</strong> , with which you can get a new <code>access_token</code> , and as many as you like; </li><li>  <strong>expires_in</strong> - the timestamp of either the expiration of the token or another predefined time.  If you come with an expired access token, you will not have access to the resource. </li></ul><br><p>  Now consider the approximate architecture of the service.  Imagine that there are some frontends that just put tokens into our service and read them, and there is a separate entity called refresher.  The task of the refresher is to go to the OAuth provider for a new <code>access_token</code> at about the moment when it expires. </p><br><p><img src="https://habrastorage.org/files/059/f34/405/059f34405aa143ee9656158540a3a43d.png" alt="image"></p><br><p>  The structure of the base is also quite simple.  We have two database nodes (master and slave replicas).  The vertical bar is a conditional division of data centers.  In one data center there is a master with its frontend and a refresher, and in the other - a slave with its frontend and a refresher that goes to the master. </p><br><h2 id="kakie-voznikayut-slozhnosti">  What are the difficulties? </h2><br><p>  The main problem is the time (one hour) during which the token lives.  If you look at the project, the thought arises: ‚ÄúIs this highload scale - 10 million records that need to be updated within an hour?  If we divide one into another, see, we get an RPS of about 3000 ".  Problems begin at the moment when something stops refreshening, for example, some maintenance of the base, or if it falls, or if the machine falls - anything happens.  The fact is that if our service, master base, for some reason does not work for 15 minutes, we get 25% outage, i.e.  a quarter of our data is not valid, not updated, it can not be used.  If the service is 30 minutes, then without updating already half of the data.  Hour - no valid token.  Suppose the base was lying for an hour, we raised it, and all 10 million tokens need to be updated very quickly.  And this is no longer 3000 RPS, this is quite a high-load service. </p><br><p>  I must say that initially everything was handled normally, but two years after the launch, we added different logic, additional indexes, began to add secondary logic - in general, Tarantool ran out of processor.  It was unexpected, but after all any resource can be spent. </p><br><p>  For the first time, admins helped us.  We put the most powerful processor, which we found, it allowed us to grow for another six months, but during this time we had to somehow solve the problem.  We caught the eye of the new Tarantool (the system was written in the old Tarantool 1.5, which practically does not occur outside the Mail.Ru Group).  In Tarantool 1.6 at that time there was already master-master replication.  And the first thing that came to mind: let's put in three data centers on the copy of the database, between them run the master-master replication, and everything will be fine. </p><br><p><img src="https://habrastorage.org/files/22c/a56/09f/22ca5609fc4c41ae96cf6b1c3093bbb4.png" alt="image"></p><br><p>  Three masters, three data centers, three refresher, each working with his master.  We can drop one or two, everything seems to be working.  But what are the potential problems here?  The main problem is that we increase the number of requests to the OAuth-provider three times.  We refresh almost the same tokens, and as many times as we have replicas.  This is not the case.  The obvious solution: the nodes themselves must somehow decide which of them will be the current leader (i.e., to keep refreshing tokens from only one replica). </p><br><h2 id="vybor-lidera">  Leader Selection </h2><br><p>  There are several consensus algorithms.  The first one is Paxos.  Pretty complicated stuff.  We were not able to properly figure out how to make something simple based on it.  As a result, we stopped at Raft.  This is a very simple consensus algorithm, in which there is a choice of a leader with whom we can work until a new leader is selected when the connection is broken or for other reasons.  This is how we did it: </p><br><p><img src="https://habrastorage.org/files/f28/0a0/fbc/f280a0fbcde24463b91fe78863044722.png" alt="image"></p><br><p>  In Tarantool from the box there is neither Raft, nor Paxos.  But we take the finished module, which is in the delivery - net.box.  This module allows us to connect nodes with each other using a full mesh scheme: each node connects to all the rest.  And then everything is simple: on top of these connections we will implement the choice of the leader, which is described in Raft.  After that, each node begins to have the property: it is either a leader or a follower, or does not see either a leader or a follower. </p><br><p>  If you think it's hard to implement Raft, here‚Äôs an example of Lua code: </p><br><pre> <code class="lua hljs"><span class="hljs-keyword"><span class="hljs-keyword">local</span></span> r = self.pool.call(self.FUNC.request_vote, self.term, self.uuid) self._vote_count = self:count_votes(r) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self._vote_count &gt; self._nodes_count / <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.info(<span class="hljs-string"><span class="hljs-string">"[raft-srv] node %d won elections"</span></span>, self.id) self:_set_state(self.S.LEADER) self:_set_leader({ id=self.id, uuid=self.uuid }) self._vote_count = <span class="hljs-number"><span class="hljs-number">0</span></span> self:stop_election_timer() self:start_heartbeater() <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> <span class="hljs-built_in"><span class="hljs-built_in">log</span></span>.info(<span class="hljs-string"><span class="hljs-string">"[raft-srv] node %d lost elections"</span></span>, self.id) self:_set_state(self.S.IDLE) self:_set_leader(msgpack.NULL) self._vote_count = <span class="hljs-number"><span class="hljs-number">0</span></span> self:start_election_timer() <span class="hljs-keyword"><span class="hljs-keyword">end</span></span></code> </pre> <br><p>  Here we make requests to remote servers, to other replicas of Tarantool, we count the number of votes that we received from the node.  If a quorum has accumulated, we have been voted for, we become the leader and start heart-beat - we notify the other nodes that we are alive.  If we lose the election, we initiate it again.  After a while we can vote or be selected. </p><br><p>  After obtaining a quorum and determining a leader, we can direct our refresher to all nodes, but at the same time tell them to work only with the leader. </p><br><p>  So we get normal traffic.  Since the tasks are distributed by one node, one third will go to each refresher. Only here we can safely lose any of the masters - re-election will happen, the refressers will switch to another node.  But, as in any distributed system, problems arise with a quorum. </p><br><h2 id="zabroshennaya-noda">  "Abandoned" node </h2><br><p>  If connectivity between data centers is lost, then a mechanism is needed that makes the system continue to live, as well as a mechanism that should restore the integrity of the system.  This problem solves Raft. </p><br><p><img src="https://habrastorage.org/files/a1c/29e/820/a1c29e820aae4673b45d3823dd35af5d.png" alt="image"></p><br><p>  Suppose the Dataline data center is gone.  It turns out that the node standing there becomes ‚Äúabandoned‚Äù - it does not see other nodes.  The rest of the cluster sees that the node is lost, re-election takes place, the leader becomes a new node in the cluster, say, the top one.  And the system continues to work, because there is still a consensus between the nodes, because  more than half of the nodes see each other. </p><br><p>  The main question is: what happens to the refresher that is located in the departed data center when connectivity is lost?  The Raft specification does not have a separate name for such a node.  Usually, a node that does not have a quorum and no connection with a leader is inactive.  But he can still go to the network, independently update tokens.  Usually, tokens are updated in the connected mode, but can it be possible to update the tokens with a refresher that is connected to the ‚Äúabandoned‚Äù node?  Initially, it was not clear whether it makes sense to update tokens?  Will there be unnecessary update operations? </p><br><p>  We addressed this issue in the process of implementing the system.  The first thought is that we have a consensus, a quorum, and if we have lost someone from the quorum, we do not perform updates.  But then another idea appeared.  Let's look at the implementation of master-master in Tarantool.  Suppose at some point there are two nodes, both master.  There is a variable, the key X, the value of which is 1. Suppose that at the same time, until replication has reached this node, we simultaneously change this key to two different values: set 2 in one node and 3 in the other. exchange replication logs, that is, values.  From the point of view of consistency, such a master-master implementation is some kind of horror, forgive me for the Tarantool developers. </p><br><p><img src="https://habrastorage.org/files/6fb/136/004/6fb13600401743c8aa58c6a3935462b6.png" alt="image"></p><br><p>  If we need strict consistency - it does not work.  However, remember our OAuth token, which consists of two important parts: </p><br><ul><li>  refresh-token, which, conditionally, lives endlessly, </li><li>  and access-token - lives for one hour. </li></ul><br><p>  But at the same time, our refresher carries the refresh function, which from the refresh-token can always receive any number of access-tokens.  And they will all operate within an hour from the date of issue. </p><br><p>  Consider the scheme: two nodes worked normally with the leader, updated tokens, received the first access token.  He replicated, now everyone has this access token.  There was a gap, the follower became a ‚Äúabandoned‚Äù node, it does not have a quorum, it does not see either a leader or other followers.  At the same time, we allow our refresher to update tokens that live on the ‚Äúabandoned‚Äù node.  If there is no network, the circuit will not work.  But if this is a simple split - the network is broken, then that's okay, this part will work autonomously. </p><br><p>  After the connection is broken and the node joins, then either re-election will take place or data will be exchanged.  In this case, the second and third token are equally ‚Äúgood‚Äù. </p><br><p>  After the cluster is reunited, the next update procedure will be executed only on one node, and it will replicate.  That is, for some time our cluster is divided, and each updates in its own way, and after reunification, we return to normal consistent data.  This gives us the following: usually for the operation of the cluster, N / 2 + 1 active nodes are needed (in the case of three nodes, these are two).  In our case, at least one active node is enough for the system to work.  Out there will be exactly as many requests as needed. </p><br><p>  We talked about the problem of increasing the number of requests.  At the time of the split or downtime, we can afford to live at least one node.  We will update it, we will add the data.  If this is a marginal split, that is, all the nodes are separated and everyone has a network, then we will get the same tripling of the number of requests to the OAuth provider.  But due to the short duration of the event - this is not terrible, and we do not intend to constantly work in the separation mode.  Usually the system is in quorum, connectivity, and in general all nodes work. </p><br><h2 id="sharding">  Sharding </h2><br><p>  There was one problem - we rested against the ceiling on the CPU.  The obvious solution: sharding. </p><br><p><img src="https://habrastorage.org/files/42e/547/03c/42e54703c2a34a5a80a4c7ba8936d82b.png" alt="image"></p><br><p>  Suppose there are two shards - databases, each of them is replicated.  There is a certain function which some key comes to an input, on this key we can define, in what shard the data lies.  If we shard by e-mail, then some of the addresses are stored in one shard, some - in the other, and we always know where our data lies. </p><br><p>  There are two approaches to the implementation of sharding.  The first is the client.  We select the function of consistent hashing, which returns the number of the shard, for example, CRC32 key, Guava, Sumbur.  This feature is implemented in the same way on all clients.  This approach has an undoubted advantage: the DB does not know anything about sharding at all.  You raised the base, it works as standard, and sharding is somewhere on the side. </p><br><p>  But there is a serious drawback.  First, the clients are pretty fat.  If you want to make a new one, then you need to add the logic of sharding to it.  But the most terrible problem is that some clients can work according to one scheme, and others - according to another.  At the same time, the base itself does not know anything about the fact that you are in different ways. </p><br><p>  We chose a different approach - sharding inside the database.  In this case, its code becomes more complicated, but we can use simple clients.  Any client connecting to this database goes to any node, there is a function that calculates which node to contact and which one to transfer control to.  Clients are simpler - the base is more complicated, but at the same time it is fully responsible for its data.  In addition, the most difficult - it is Resarding.  When the database is responsible for its data, it is much easier to perform rewarding than when you have customers that you cannot update yet. </p><br><p>  How did we do it? </p><br><p><img src="https://habrastorage.org/files/bfc/234/a14/bfc234a140334cec870c071d90e9c46c.png" alt="image"></p><br><p>  Hexagons are Tarantools.  Take the top three nodes, call the shard number one.  We put exactly the same cluster and call it shard number two.  Connect all the nodes with each other.  What does this give?  First, we have Raft, where inside the top three servers we know who the leader is, who is the follower, what is the status of the cluster.  Thanks to the new connections of sharding, we now know the state of another shard.  We know perfectly well who is the leader in the second shard, who is the follower, and so on.  In general, we always know where to redirect the user who came to the first shard, if he needs not the first shard. </p><br><p>  Consider simple examples. </p><br><p>  Suppose a user requests a key that lies on the first shard.  He comes in a knot from the first shard.  Since  he knows who is the leader, the request is redirected to the leader, he in turn receives or writes the key, and the answer is then returned to the user. </p><br><p>  Suppose now that the user comes to the same node, and he needs a key, which is actually on the second shard.  The same: the first shard knows who is the leader in the second, goes to this node, receives or writes data, returns to the user. </p><br><p>  Very simple scheme, but with it there are difficulties.  The main problem: are there too many connections?  In the scheme, when each node is connected to each, it turns out 6 * 5 - 30 connections.  Put another shard - we get already 72 connections in the cluster.  Too much. </p><br><p>  We solve this problem in the following way: we simply put a couple of Tarantools, just call them no longer shards or bases, but proxies that will deal exclusively with sharding: calculate the key, find out who is the leader in a particular shard, but Raft clusters remain closed in themselves and will work only within the shard itself.  When a client comes to the proxy, it calculates which shard it needs, and if it needs a leader, we redirect to the leader.  If it doesn't matter who, we redirect to any node from this shard. </p><br><p><img src="https://habrastorage.org/files/217/227/c99/217227c9910948bdbc8577d968ddcfcf.png" alt="image"></p><br><p>  The complexity is obtained linear, depending on the number of nodes.  If we have three shards with three nodes in each, then we will get many times fewer connections. </p><br><p>  The proxy scheme is designed for further scaling when there are more than two shards.  With two shards, we have the same number of connections, but with an increase in the number of shards, we significantly save on connections.  The list of shards is stored in the Lua config, and in order to get a new list, simply reload the code and everything works. </p><br><p>  So, we started with a master-master, implemented Raft, screwed a sharding on it, then a proxy, rewrote it all.  It turned out a brick cluster.  Our scheme began to look pretty simple. <br>  Our front-ends remain, which only put or take tokens.  There are refressers who update tokens, take a refresh-token, give to the OAuth-provider, put a new access-token. </p><br><p>  We said that we still have a secondary logic, because of which the processor has run out of resources.  Let's move it to another cluster. </p><br><p>  Such secondary logic mainly includes address books.  If there is a user token, then it corresponds to the address book of this user.  And there are as many data on the number as there are tokens.  In order not to exhaust processor resources on one machine, obviously, the same cluster is needed, replicated again, and so on.  We put another pack of other refreshers for updating address books (this is a rarer task, therefore we do not update address books together with tokens). </p><br><p>  As a result, combining two such clusters, we got such a fairly simple architecture of the entire system: </p><br><p><img src="https://habrastorage.org/files/3a1/7f5/804/3a17f580438d47fb99bebd31853b70aa.png" alt="image"></p><br><h2 id="ochered-obnovleniya-tokenov">  Token Update Queue </h2><br><p>  Why do we turn around?  It was possible to take something standard.  The point is in our model update tokens.  After receiving the token lives one hour.  When it comes to the end of its time, it must be updated.  This is a deadline: the token must be updated before a certain time. </p><br><p><img src="https://habrastorage.org/files/5a5/012/c9d/5a5012c9dc91467989a19918276486eb.png" alt="image"></p><br><p>  Suppose an outage happened, short-term or not, but we have some volume of expired tokens.  If we update them, then some more will become obsolete.  We, of course, will catch up with everything, but it would be better to first update those that are about to die (after 60 seconds), and then update the remaining resources anyway with the remaining resources.  Last, we update the more distant horizon (5 minutes to death). </p><br><p>  To implement this logic on something third-party, you have to sweat.  In the case of Tarantool, this is implemented very simply.  Consider a simple scheme: there is a tuple, where Tarantool data is located, it has some kind of ID, which has a primary key.  And to make the queue that we need, we simply add two fields: status and time.  The status indicates the status of the token in the queue, the time - the same expire time, or some other. </p><br><p><img src="https://habrastorage.org/files/cfe/389/f11/cfe389f115704b1c855c39ae18c841cd.png" alt="image"></p><br><p>  Take the two main functions from the queue - <code>put</code> and <code>take</code> .  The task of <code>put</code> is to bring and put data.  They give us some kind of payload, <code>put</code> the status itself, <code>put</code> the time and put the data.  A new <code>tuple</code> appears. </p><br><p>  Comes in, looks at the index.  Creates an iterator, starts looking at it.  Selects waiting tasks (ready), checks whether it is time to take them, or they are outdated.  If there are no tasks, <code>take</code> goes into standby mode.  In addition to the built-in Lua in Tarantool, there are also primitives for synchronization between file servers - channels.  Any fayber can create a channel and say: "I'm waiting here."  Any other fayber can wake this channel and send messages to it. </p><br><p>  The function that is waiting for something - the release of tasks, the arrival of time or something else - creates a channel, marks it in a special way, puts it somewhere, and waits on it further.  If they bring us a token that needs urgent updating, say <code>put</code> , then it will send notify to this channel. </p><br><p>  Tarantool has one feature: if a token is accidentally released, someone took it at a refresh or just takes a task, you can monitor client connection breaks.  We remember in the session stash, to which connection which task was given.  We associate that such and such tasks were associated with this session.  Suppose the refresh process drops, the network is simply broken ‚Äî we don‚Äôt know whether it will refresh the token or not, whether it will be able to put it back or not.  Disconnect triggers here, which finds all tasks in a session and automatically releases them.             <code>put</code> ,      . </p><br><p>        ,   : </p><br><pre> <code class="lua hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">put</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(data)</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> t = box.space.queue:auto_increment({ <span class="hljs-string"><span class="hljs-string">'r'</span></span>, <span class="hljs-comment"><span class="hljs-comment">--[[ status ]]</span></span> util.<span class="hljs-built_in"><span class="hljs-built_in">time</span></span>(), <span class="hljs-comment"><span class="hljs-comment">--[[ time ]]</span></span> data <span class="hljs-comment"><span class="hljs-comment">--[[ any payload ]]</span></span> }) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">take</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(timeout)</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> start_time = util.<span class="hljs-built_in"><span class="hljs-built_in">time</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> q_ind = box.space.tokens.index.queue <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> _,t <span class="hljs-keyword"><span class="hljs-keyword">while</span></span> <span class="hljs-literal"><span class="hljs-literal">true</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> it = util.iter(q_ind, {<span class="hljs-string"><span class="hljs-string">'r'</span></span>}, { iterator = box.index.GE }) _,t = it() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> t[F.tokens.<span class="hljs-built_in"><span class="hljs-built_in">status</span></span>] ~= <span class="hljs-string"><span class="hljs-string">'t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> left = (start_time + timeout) - util.<span class="hljs-built_in"><span class="hljs-built_in">time</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> left &lt;= <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> t = q:wait(left) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-keyword"><span class="hljs-keyword">break</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> t = q:taken(t) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">queue:taken</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(task)</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> sid = box.session.id() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self._consumers[sid] == <span class="hljs-literal"><span class="hljs-literal">nil</span></span> <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> self._consumers[sid] = {} <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> k = task[self.f_id] <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> t = self:set_status(k, <span class="hljs-string"><span class="hljs-string">'t'</span></span>) self._consumers[sid][k] = { util.<span class="hljs-built_in"><span class="hljs-built_in">time</span></span>(), box.session.peer(sid), t } self._taken[k] = sid <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">on_disconnect</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> sid = box.session.id <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> now = util.<span class="hljs-built_in"><span class="hljs-built_in">time</span></span>() <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> self._consumers[sid] <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> consumers = self._consumers[sid] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> k,rec <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-built_in"><span class="hljs-built_in">pairs</span></span>(consumers) <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> <span class="hljs-built_in"><span class="hljs-built_in">time</span></span>, peer, task = <span class="hljs-built_in"><span class="hljs-built_in">unpack</span></span>(rec) <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> v = box.space[self.space].index[self.index_primary]:get({k}) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> v <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> v[self.f_status] == <span class="hljs-string"><span class="hljs-string">'t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">then</span></span> v = self:release(v[self.f_id]) <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> self._consumers[sid] = <span class="hljs-literal"><span class="hljs-literal">nil</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> <span class="hljs-keyword"><span class="hljs-keyword">end</span></span></code> </pre> <br><p> <code>Put</code>    <code>space</code> ,      ,     ,    FIFO-,    ,   . </p><br><p>  <code>take</code>  ,   :       .  <code>taken</code>    ,    ‚Äî  ,   .  <code>on_disconnect</code>     ,   ,  . </p><br><h2 id="vozmozhny-li-alternativy">   ? </h2><br><p>  Of course.     .               ,  ,   .        ,      .       ,         .        (      ).          .   ,        ,   , ,  ,      in memory,      . </p><br><p>         ,        ,   .  ‚Äî  7   tuple,    .          ,      . </p><br><h2 id="podvedyom-itog">   </h2><br><p>        outage,   .        . </p><br><p>        .     N <sup>2</sup>   ,       -:  ,        - .       :   Google, Microsoft,     -     OAuth-,       . </p><br><p>        ,   , ,   .   Tarantool .  Thank. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/325582/">https://habr.com/ru/post/325582/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../325570/index.html">How to test docker image for half a second</a></li>
<li><a href="../325572/index.html">Users convinced GitLab not to leave the cloud</a></li>
<li><a href="../325574/index.html">Announcement of mitc.ap Sync.NET # 4 in Kharkov</a></li>
<li><a href="../325576/index.html">Freedom of choice: freelance or large corporation</a></li>
<li><a href="../325580/index.html">UFOCTF 2017: we decompile Python in the King Arthur assignment (PPC600)</a></li>
<li><a href="../325586/index.html">Automation of printing in corporate systems or how to make friends with your ‚Äúbike‚Äù with a printer</a></li>
<li><a href="../325590/index.html">Recursive Moving Average Filter</a></li>
<li><a href="../325592/index.html">Azure-IaaS-Digest # 14 (March)</a></li>
<li><a href="../325596/index.html">This is Science: observing plant growth</a></li>
<li><a href="../325600/index.html">Serious errors in the code CryEngine V</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>