<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of data clustering algorithms</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Greetings 

 In my thesis, I conducted a review and comparative analysis of data clustering algorithms. I thought that already collected and developed...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of data clustering algorithms</h1><div class="post__text post__text-html js-mediator-article">  Greetings <br><br>  In my thesis, I conducted a review and comparative analysis of data clustering algorithms.  I thought that already collected and developed material may be interesting and useful to someone. <br>  <a href="http://habrahabr.ru/users/sashaeve/" class="user_link">Sashaeve</a> described the article <a href="http://habrahabr.ru/blogs/data_mining/67078/">‚ÄúClustering: k-means and c-means algorithms‚Äù about clustering</a> .  I will partly repeat the words of Alexander, partially add.  Also at the end of this article, those interested can read the materials on the links in the reference list. <br><br>  I also tried to bring a dry "diploma" style of presentation to a more journalistic. <br><a name="habracut"></a><br><h4>  Clustering concept </h4><br>  Clustering (or cluster analysis) is the task of breaking up a set of objects into groups called clusters.  Inside each group there should be ‚Äúsimilar‚Äù objects, and the objects of different groups should be as different as possible.  The main difference between clustering and classification is that the list of groups is not clearly defined and is determined during the operation of the algorithm. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The use of cluster analysis in general is reduced to the following steps: <br><ol><li>  Selecting a selection of objects for clustering. </li><li>  Determining the set of variables by which the objects in the sample will be evaluated.  If necessary, normalize the values ‚Äã‚Äãof variables. </li><li>  Calculating the values ‚Äã‚Äãof the similarity measure between objects. </li><li>  The use of cluster analysis to create groups of similar objects (clusters). </li><li>  Presentation of analysis results. </li></ol><br>  After receiving and analyzing the results, it is possible to adjust the selected metric and the clustering method to an optimal result. <br><br><h4>  Distance measures </h4><br>  So, how to determine the "similarity" of objects?  First you need to make a vector of characteristics for each object - as a rule, it is a set of numerical values, for example, height and weight of a person.  However, there are also algorithms that work with qualitative (so-called categorical) characteristics. <br><br>  After we have determined the vector of characteristics, it is possible to carry out a normalization so that all components give the same contribution when calculating the ‚Äúdistance‚Äù.  In the normalization process, all values ‚Äã‚Äãare reduced to a certain range, for example, [-1, -1] or [0, 1]. <br><br>  Finally, for each pair of objects, the ‚Äúdistance‚Äù between them is measured - the degree of similarity.  There are many metrics, here are just the main ones: <br><ol><li>  Euclidean distance <br>  The most common distance function.  It is a geometric distance in multidimensional space: <br><img src="https://habrastorage.org/getpro/habr/post_images/aa4/6e7/d7b/aa46e7d7b544dbaa221a43bb671fb43c.jpg"><br></li><li>  Square euclidean distance <br>  It is used to give more weight to more distant objects.  This distance is calculated as follows: <br><img src="https://habrastorage.org/getpro/habr/post_images/5dc/84a/1d6/5dc84a1d66392e9f11ba13be381ad036.jpg"><br></li><li>  City block distance (Manhattan distance) <br>  This distance is the average difference in coordinates.  In most cases, this measure of distance leads to the same results as for the usual Euclidean distance.  However, for this measure, the influence of individual large differences (emissions) is reduced (since they are not squared).  The formula for calculating the Manhattan distance is: <br><img src="https://habrastorage.org/getpro/habr/post_images/930/f68/0a2/930f680a27caed2bed04f6b09a70a785.jpg"><br></li><li>  Chebyshev distance <br>  This distance can be useful when you need to define two objects as ‚Äúdifferent‚Äù if they differ in any one coordinate.  Chebyshev distance is calculated by the formula: <br><img src="https://habrastorage.org/getpro/habr/post_images/ccd/79d/96b/ccd79d96ba5a93373b4daab5650ea7d2.jpg"><br></li><li>  Power distance <br>  It is used when it is necessary to increase or decrease the weight related to the dimension for which the corresponding objects are very different.  The power distance is calculated using the following formula: <br><img src="https://habrastorage.org/getpro/habr/post_images/4cf/36c/f90/4cf36cf908a6c2c27a0b87039d82d6e6.jpg">  , <br>  where r and p are user-defined parameters.  The parameter p is responsible for the gradual weighting of differences in individual coordinates, the parameter r is responsible for the progressive weighing of large distances between objects.  If both parameters - r and p - are equal to two, then this distance coincides with the Euclidean distance. </li></ol><br>  The choice of the metric lies entirely with the researcher, since the results of clusterization can differ significantly when using different measures. <br><br><h4>  Algorithm classification </h4><br>  For myself, I singled out two main classifications of clustering algorithms. <br><ol><li>  Hierarchical and flat. <br>  Hierarchical algorithms (also called taxonomy algorithms) do not build a single partitioning of a sample into disjoint clusters, but a system of nested partitions.  So  at the output we get a cluster tree, the root of which is the entire sample, and the leaves are the smallest clusters. <br>  Planar algorithms build one partitioning of objects into clusters. </li><li>  Clear and fuzzy. <br>  Clear (or non-intersecting) algorithms for each sample object are assigned a cluster number, i.e.  each object belongs to only one cluster.  Fuzzy (or intersecting) algorithms for each object associate a set of real values, indicating the degree of the object's relation to the clusters.  Those.  each object belongs to each cluster with a certain probability. </li></ol><br><h4>  Clustering </h4><br>  In the case of using hierarchical algorithms, the question arises how to combine the clusters among themselves, how to calculate the ‚Äúdistances‚Äù between them.  There are several metrics: <br><ol><li>  Single connection (nearest neighbor distance) <br>  In this method, the distance between two clusters is determined by the distance between the two closest objects (nearest neighbors) in different clusters.  The resulting clusters tend to chain together. <br></li><li>  Full communication (distance of the most distant neighbors) <br>  In this method, the distances between clusters are determined by the largest distance between any two objects in different clusters (that is, the most distant neighbors).  This method usually works very well when objects come from separate groups.  If the clusters are elongated or their natural type is ‚Äúchained,‚Äù then this method is unsuitable. <br></li><li>  Unweighted pairwise mean <br>  In this method, the distance between two different clusters is calculated as the average distance between all pairs of objects in them.  The method is effective when objects form different groups, but it works equally well in the case of extended (‚Äúchained‚Äù type) clusters. <br></li><li>  Weighted paired average <br>  The method is identical to the unweighted pairwise average method, except that the size of the corresponding clusters (that is, the number of objects contained in them) is used as a weighting factor in the calculations.  Therefore, this method should be used when unequal cluster sizes are assumed. <br></li><li>  Unweighted centroid method <br>  In this method, the distance between two clusters is defined as the distance between their centers of gravity. <br></li><li>  Weighted centroid method (median) <br>  This method is identical to the previous one, except that the calculations use weights to account for the difference between cluster sizes.  Therefore, if there are or are suspected significant differences in cluster sizes, this method is preferable to the previous one. <br></li></ol><br><h4>  Algorithm Overview </h4><br><h5>  Hierarchical clustering algorithms </h5><br>  Among the algorithms of hierarchical clustering, there are two main types: ascending and descending algorithms.  Top-down algorithms work on the ‚Äútop-down‚Äù principle: at the beginning, all objects are placed in one cluster, which is then divided into smaller and smaller clusters.  Ascending algorithms are more common, which at the beginning of the work place each object in a separate cluster, and then merge clusters into ever larger ones until all the objects in the sample are contained in one cluster.  Thus, a system of nested partitions is built.  The results of such algorithms are usually presented in the form of a tree - dendrograms.  A classic example of such a tree is the classification of animals and plants. <br><br>  To calculate the distances between clusters, more often everyone uses two distances: a single bond or a full bond (see an overview of the measures of the distances between clusters). <br><br>  The lack of hierarchical algorithms can include a system of complete partitions, which may be superfluous in the context of the problem being solved. <br><br><h5>  Quadratic error algorithms </h5><br>  The task of clustering can be viewed as the construction of an optimal partitioning of objects into groups.  At the same time, optimality can be defined as the requirement to minimize the root-mean-square split error: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d3/e4a/55c/1d3e4a55c117ab17dc9ee1b9ef50599f.jpg"><br><br>  where <i>c <sub>j</sub></i> is the ‚Äúcenter of mass‚Äù of the cluster <i>j</i> (a point with average values ‚Äã‚Äãof characteristics for a given cluster). <br><br>  The quadratic error algorithms are of the type of flat algorithms.  The most common algorithm in this category is the k-means method.  This algorithm builds a given number of clusters located as far as possible from each other.  The operation of the algorithm is divided into several stages: <br><ol><li>  Randomly select <i>k</i> points, which are the initial "centers of mass" of the clusters. </li><li>  Assign each object to a cluster with the nearest "center of mass". </li><li>  Recalculate the "centers of mass" clusters according to their current composition. </li><li>  If the criterion for stopping the algorithm is not satisfied, return to step 2. </li></ol><br>  As a criterion for stopping the operation of the algorithm, usually choose the minimum change in the root mean square error.  It is also possible to stop the operation of the algorithm, if at step 2 there were no objects that moved from cluster to cluster. <br><br>  The disadvantages of this algorithm include the need to specify the number of clusters for splitting. <br><br><h5>  Fuzzy algorithms </h5><br>  The most popular fuzzy clustering algorithm is the c-means algorithm.  It is a modification of the k-means method.  Algorithm steps: <br><ol><li>  Select the initial fuzzy partition of <i>n</i> objects into <i>k</i> clusters by choosing the membership matrix <i>U of</i> size <i>nxk</i> . </li><li>  Using the matrix U, find the value of the fuzzy error criterion: <br><img src="https://habrastorage.org/getpro/habr/post_images/21c/c5d/72b/21cc5d72b0c9bee8ad0476798eb4093c.jpg">  , <br>  where <i>c <sub>k</sub></i> is the ‚Äúcenter of mass‚Äù of a fuzzy cluster <i>k</i> : <br><img src="https://habrastorage.org/getpro/habr/post_images/5fb/811/641/5fb811641b4fa3c84ba2490811ff53ea.jpg">  . </li><li>  Regroup objects in order to reduce this value of the fuzzy error criterion. </li><li>  Return to Section 2 until the changes in the matrix <i>U</i> become insignificant. </li></ol><br>  This algorithm may not be suitable if the number of clusters is not known in advance, or it is necessary to clearly assign each object to one cluster. <br><br><h5>  Algorithms based on graph theory </h5><br>  The essence of such algorithms is that the sample of objects is represented as a graph <i>G = (V, E)</i> , whose vertices correspond to objects, and the edges have a weight equal to the ‚Äúdistance‚Äù between the objects.  The advantages of graph clustering algorithms are visibility, relative ease of implementation, and the ability to make various improvements based on geometrical considerations.  The main algorithms are the algorithm for selecting connected components, the algorithm for constructing the minimum spanning (spanning) tree and the layer-by-layer clustering algorithm. <br><br><h6>  Algorithm of selection of connected components </h6><br>  In the algorithm for selecting connected components, the input parameter <i>R</i> is specified and all edges for which ‚Äúdistances‚Äù are greater than <i>R</i> are removed in the graph.  Only the closest pairs of objects remain connected.  The meaning of the algorithm is to find a value of <i>R</i> that lies in the range of all ‚Äúdistances‚Äù at which the graph ‚Äúfalls apart‚Äù into several connected components.  The resulting components are clusters. <br><br>  For the selection of the parameter <i>R</i> , a histogram of distributions of pairwise distances is usually constructed.  In problems with a well-pronounced cluster data structure, there will be two peaks in the histogram ‚Äî one corresponds to intracluster distances, and the second to intercluster distances.  The parameter <i>R is</i> selected from the minimum zone between these peaks.  At the same time, managing the number of clusters using the distance threshold is rather difficult. <br><br><h6>  Algorithm minimum spanning tree </h6><br>  The minimum spanning tree algorithm first builds a minimal spanning tree on the graph, and then successively removes the edges with the greatest weight.  The figure shows the minimum spanning tree obtained for nine objects. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c0c/322/890/c0c32289017968ce5178d6345c304b3b.jpg"><br><br>  By removing the link labeled CD, with a length of 6 units (edge ‚Äã‚Äãwith maximum distance), we obtain two clusters: {A, B, C} and {D, E, F, G, H, I}.  The second cluster can be further divided into two more clusters by removing the edge EF, which has a length of 4.5 units. <br><br><h6>  Layered clustering </h6><br>  The layer-by-layer clustering algorithm is based on identifying connected components of a graph at some level of distance between objects (vertices).  The distance level is set by the distance threshold <i>c</i> .  For example, if the distance between objects <img src="https://habrastorage.org/getpro/habr/post_images/f67/e6a/707/f67e6a707dcac300ff90658e523b135f.jpg">  then <img src="https://habrastorage.org/getpro/habr/post_images/e91/fe1/65a/e91fe165a15d6fa1981cbb51f0e7fff3.jpg">  . <br><br>  The layer clustering algorithm generates a sequence of subgraphs of the graph <i>G</i> , which reflect the hierarchical relationships between clusters: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5ec/cd0/7cc/5eccd07cc4e56b227092cbe66501a67f.jpg">  , <br><br>  where <i>G <sup>t</sup> = (V, E <sup>t</sup> )</i> is a graph at the level of <i><sup>t</sup></i> , <br><img src="https://habrastorage.org/getpro/habr/post_images/35f/249/c62/35f249c626b8d0e118efc7f472beee48.jpg">  , <br>  <i>with <sup>t</sup></i> - t-th threshold of distance, <br>  m is the number of hierarchy levels, <br>  <i>G <sup>0</sup> = (V, o)</i> , o is the empty set of edges of the graph obtained when <i>t <sup>0</sup></i> = 1, <br>  <i>G <sup>m</sup> = G</i> , that is, the graph of objects without restrictions on the distance (the length of the edges of the graph), since <i>t <sup>m</sup></i> = 1. <br><br>  By changing the distance thresholds { <i>c <sup>0</sup> , ..., <sup>m</sup></i> }, where 0 = <i>c <sup>0</sup></i> &lt; <i>c <sup>1</sup></i> &lt;... &lt; <i>c <sup>m</sup></i> = 1, it is possible to control the depth of the hierarchy of the resulting clusters.  Thus, a layer-by-layer clustering algorithm is able to create both flat data partitioning and hierarchical. <br><br><h4>  Algorithm Comparison </h4><br>  Computational complexity of algorithms <br><table><tbody><tr><td>  <b>Clustering algorithm</b> </td><td>  <b>Computational complexity</b> </td></tr><tr><td>  Hierarchical </td><td>  O (n <sup>2</sup> ) </td></tr><tr><td>  k-medium </td><th rowspan="2">  O (nkl), where k is the number of clusters, l is the number of iterations </th></tr><tr><td>  c-medium </td></tr><tr><td>  Selection of connected components </td><td>  <i>depends on the algorithm</i> </td></tr><tr><td>  Minimum spanning tree </td><td>  O (n <sup>2</sup> log n) </td></tr><tr><td>  Layered clustering </td><td>  O (max (n, m)), where m &lt;n (n-1) / 2 </td></tr></tbody></table><br>  Comparative table of algorithms <br><table><tbody><tr><td>  <b>Clustering algorithm</b> </td><td>  <b>Cluster form</b> </td><td>  <b>Input data</b> </td><td>  <b>results</b> </td></tr><tr><td>  Hierarchical </td><td>  Arbitrary </td><td>  Number of clusters or distance threshold for truncating the hierarchy </td><td>  Binary Cluster Tree </td></tr><tr><td>  k-medium </td><td>  Hypersphere </td><td>  Number of clusters </td><td>  Cluster Centers </td></tr><tr><td>  c-medium </td><td>  Hypersphere </td><td>  The number of clusters, the degree of fuzziness </td><td>  Cluster centers, membership matrix </td></tr><tr><td>  Selection of connected components </td><td>  Arbitrary </td><td>  R threshold </td><td>  Cluster tree structure </td></tr><tr><td>  Minimum spanning tree </td><td>  Arbitrary </td><td>  Number of clusters or distance threshold for deleting edges </td><td>  Cluster tree structure </td></tr><tr><td>  Layered clustering </td><td>  Arbitrary </td><td>  Sequence of distance thresholds </td><td>  Tree structure of clusters with different levels of hierarchy </td></tr></tbody></table><br><h4>  Little application </h4><br>  In my work, I needed to select separate areas from the hierarchical structures (trees).  Those.  in fact, it was necessary to cut the original tree into several smaller trees.  Since an oriented tree is a special case of a graph, algorithms based on graph theory are naturally appropriate. <br><br>  In contrast to a fully connected graph, in an oriented tree, not all vertices are connected by edges, and the total number of edges is n ‚Äì 1, where n is the number of vertices.  Those.  applied to the nodes of the tree, the operation of the algorithm for selecting connected components will be simplified, since the removal of any number of edges will ‚Äúdecompose‚Äù the tree into connected components (individual trees).  The algorithm of the minimum spanning tree in this case will coincide with the algorithm for selecting connected components ‚Äî by removing the longest edges, the original tree is divided into several trees.  It is obvious that the construction phase of the minimal spanning tree itself is skipped. <br><br>  In the case of using other algorithms, they would have to separately consider the presence of connections between objects, which complicates the algorithm. <br><br>  Separately, I want to say that in order to achieve the best result, it is necessary to experiment with the choice of distance measures, and sometimes even change the algorithm.  There is no single solution. <br><br><h4>  Bibliography </h4><br>  1. Vorontsov K.V.  <a href="http://www.ccas.ru/voron/download/Clustering.pdf">Algorithms for clustering and multidimensional scaling</a> .  Lecture course.  Moscow State University, 2007. <br>  2. Jain A., Murty M., Flynn P. <a href="http://www.nd.edu/~flynn/papers/Jain-CSUR99.pdf">Data Clustering: A Review</a> .  // ACM Computing Surveys.  1999. Vol.  31, no.  3 <br>  3. Kotov A., Krasilnikov N. <a href="http://logic.pdmi.ras.ru/~yura/internet/02ia-seminar-note.pdf">Data clustering</a> .  2006 <br>  3. Mandel I. D. Cluster analysis.  - M .: Finance and Statistics, 1988. <br>  4. Applied statistics: classification and reduction of dimension.  / S.A.  Ayvazyan, V.M.  Buchstaber, I.S.  Enyukov, LD  Meshalkin - M .: Finance and Statistics, 1989. <br>  5. Information and analytical resource dedicated to machine learning, pattern recognition and data mining - <a href="http://www.machinelearning.ru/">www.machinelearning.ru</a> <br>  6. Chubukova I.A.  Course of lectures "Data Mining", Internet University of Information Technologies - <a href="http://www.intuit.ru/department/database/datamining/">www.intuit.ru/department/database/datamining</a> </div><p>Source: <a href="https://habr.com/ru/post/101338/">https://habr.com/ru/post/101338/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../101326/index.html">MS SQL 2008: A table type with columns of the form sql_variant (for now?) Is incompatible with ADO.NET</a></li>
<li><a href="../101327/index.html">Regular hosting vs VPS: what are the guarantees?</a></li>
<li><a href="../101328/index.html">Solazyme continues to develop algae fuel technology</a></li>
<li><a href="../101331/index.html">Verisign is no more</a></li>
<li><a href="../101336/index.html">We connect the domain and dynamic IP</a></li>
<li><a href="../101339/index.html">Call of Duty: Black Ops. Demonstration of multiplayer mode</a></li>
<li><a href="../101342/index.html">Database Connection Pool</a></li>
<li><a href="../101343/index.html">Small buns for the service Eventr.com</a></li>
<li><a href="../101344/index.html">Sight recovery</a></li>
<li><a href="../101345/index.html">Wordpress 3 domain mapping 2.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>