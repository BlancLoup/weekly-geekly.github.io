<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognizing eco-labels using Azure Custom Vision from a mobile app</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I want to talk about using the Custom Vision service to recognize photos of eco-labels from a mobile application. 


 The CustomVision...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognizing eco-labels using Azure Custom Vision from a mobile app</h1><div class="post__text post__text-html js-mediator-article"><p>  In this article I want to talk about using the Custom Vision service to recognize photos of eco-labels from a mobile application. </p><br><p>  The CustomVision service is part of the cloud-based Cognitive Services on the Azure platform. <br>  About what technologies we had to study, how to work with CustomVision, what it is and what it allows to achieve - further. </p><br><p><img src="https://habrastorage.org/webt/bc/7h/ox/bc7hoxzxf64einuj6yh30ftnuk8.png"></p><a name="habracut"></a><br><p>  The task of recognizing eco-labels appeared three years ago when my wife and I began to discuss the mobile application that her organization (environmental NGOs) wanted to do to spread information about eco-labels. </p><br><h3 id="chto-takoe-ekomarkirovka">  What is eco-labeling? </h3><br><p>  Eco-labeling is a certificate and a corresponding logo issued by certifying organizations that check products or services of a manufacturer‚Äôs supplier for compliance with certain criteria related to the life cycle of a product-service and focused on its environmental friendliness.  After certification, a manufacturer can place an eco-label logo on its products. </p><br><p>  Eco-labeling also includes a plastic mark with its composition for ease of processing and processing, and other similar signs. </p><br><p>  For example, here is a sign: </p><br><p><img src="https://habrastorage.org/webt/_q/b6/od/_qb6odkao8hmg469ujzuqsjyjoc.png"></p><br><h3 id="process-vybora-tehnologii-raspoznavaniya">  The process of selecting recognition technology </h3><br><p>  The two main features of the application should have been the search for stores with eco-products and the recognition of eco-labels.  If technologically everything is relatively simple with the search for stores, then with recognition it is not very.  The word is fashionable, but how to make it was not clear.  And I began to study the question. </p><br><p>  The logos of the markings are standardized and are ideal objects for recognition - he pointed the phone at the image on the packaging of the product, took a picture and the application shows what a sign it means and whether it should be trusted. </p><br><p>  I started thinking how to make recognition and analyze different options - I tried OpenCV with its recognition algorithms (Haar cascades, SWIFT, Template matching, etc.), but the recognition quality was not very good - no more than 70% with a training set of several dozen images . </p><br><p>  Probably, I misunderstood somewhere and did something wrong, but we also asked another acquaintance to investigate this topic and he also said that 70% at the cascades of Haar is the maximum on such a date. </p><br><p>  In parallel with this, materials about various frameworks of neural networks and the successful use of neural networks for solving such problems began to appear more often.  But everywhere there were glimpses of some horrific sizes of datasets (hundreds or thousands of images for each class), unfamiliar to me in Python, TensorFlow, the need for their own backend - all this was somewhat frightening. </p><br><p>  As a .NET developer, I looked at Accord.NET but I also did not quickly find something that would fit right away. </p><br><p>  At that time, we were busy finishing the application and launching it, and I postponed the proceedings with recognition. </p><br><p>  About a year ago, I came across an article describing Microsoft's early preview of Custom Vision, a service for classifying images in the cloud.  I tested it on 3 characters and I liked it - an understandable portal where you can both train and test a classifier without technical knowledge, learning a set of 100 images in 10-20 seconds, the quality of classification is above 90% even on 30 images of each character - then what do you need. </p><br><p>  I shared the find with my wife and we started making a less functional international version of the application, which does not contain information about products and stores, but is able to recognize eco-labels. </p><br><p>  Let's move on to the technical details of a running recognition application. </p><br><h3 id="custom-vision">  Custom vision </h3><br><p>  CV is part of Cognitive Services in Azure.  It can now be formalized and will be paid for with an Azure subscription, although it is still listed in the Preview. </p><br><p>  Accordingly, like any other Azure product, CognitiveServices are displayed and managed on the Azure portal. </p><br><p>  CV provides two REST APIs - one for training (Training), the other for recognition (Prediction).  In more detail, I will describe the interaction with Prediction further </p><br><p>  In addition to the Azure portal and API, CV users can access the customvision.ai portal, where you can easily and visually upload images, place labels on them, and see the images and recognition results that passed through the API. </p><br><p>  You can start using the customvision.ai portal and API without any binding to Azure ‚Äî a project is created for testing purposes even without Azure Subscription.  But if you want to make a project out of your test project in the future, then it is better to do it right away, otherwise we had to manually copy the pictures from the test project and re-mark it in production. </p><br><p>  In order to make a project in Azure, you need to register there and create a subscription.  This is relatively easy, problems can only be with the input and validation of data from a credit card - sometimes it happens. </p><br><p>  After registration, you need to create a ComputerVision instance through the Azure portal </p><br><p><img src="https://habrastorage.org/webt/jk/cf/vv/jkcfvvmhkp80zpptg1lpfinwxzi.png"></p><br><p>  After creating resources in Azure, they will be available in customvision.ai </p><br><p>  On the customvision.ai portal you can upload images and tag them with tags - there can be several tags for one image, but without selecting areas.  That is, the image belongs to several classes, but at this stage of development of the service it is impossible to select a specific fragment in the image and assign it to the class. </p><br><p>  After marking, you need to start training by pressing the Train button - the training of a model of 70 tags and 3 thousand images lasts about 30 seconds. </p><br><p>  The results of the training are stored in the essence of Iteration.  In fact, versioning is implemented through Iteration. </p><br><p>  Each Iteration can be used independently - that is, you can create an Iteration, test the result and delete it if it doesn‚Äôt fit or translate it into the default one and replace the current default Iteration and then all the recognitions from the applications will come to the model from this Iteration. </p><br><p>  The quality of the model is displayed as Precision and Recall (more <a href="http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html">here</a> ) for all classes at once, or separately. </p><br><p><img src="https://habrastorage.org/webt/j-/es/_m/j-es_mwi8onzwum9uc2a9k2p4y0.png"></p><br><p>  This is what a project looks like with already loaded and passed through images. </p><br><p><img src="https://habrastorage.org/webt/os/iq/pi/osiqpinmfbcxb8-zp8bunbr7lni.png"></p><br><p>  On the portal, you can run image recognition from a disk or from a URL using Quick Test and perform recognition testing on a single image. </p><br><p>  On the Predictions tab, you can see the results of all the latest recognitions - the percentages of belonging to tags are displayed directly in the picture. </p><br><p><img src="https://habrastorage.org/webt/zy/-f/-k/zy-f-kqcj7v4npxr4adnakn0724.png"></p><br><p>  The ability to see all the recognition results and add them to the training set with just a couple of mouse clicks helps a lot - anyone can do this without any knowledge of AI or programming. </p><br><h3 id="ispolzovanie-api">  API usage </h3><br><p>  Custom Vision Service has a very simple and intuitive REST API for learning and recognition. </p><br><p> In our application, only the recognition API is used and I will talk about its use. </p><br><p>  Url for recognition of this kind: </p><br><p>  <a href="https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/%257BYour">https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/{Your</a> project GUID} / image </p><br><p>  Where <br>  <strong>southcentralus **</strong> - Azure name of the region where the service is located.  While the service is available only in the South Central US region.  This does not mean that only there it can be used!  He just lives there - you can use it from anywhere, where the Internet is. <br>  <strong>{Your project GUID} **</strong> - your project identifier.  It can be viewed on the portal customvision.ai </p><br><p>  For recognition it is necessary to send the image via POST.  You can also send a publicly available image url and the service will download it yourself. </p><br><p>  In addition, you need to add the header "Prediction-Key‚Äù to the Headers in which to transfer one of the Access Key that will be issued upon registration - they are available both on the customvision.ai portal and on the Azure portal. </p><br><p>  The result contains the following field: </p><br><pre><code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"Predictions"</span></span>:[ {<span class="hljs-string"><span class="hljs-string">"TagId"</span></span>:<span class="hljs-string"><span class="hljs-string">"35ac2ad0-e3ef-4e60-b81f-052a1057a1ca"</span></span>,<span class="hljs-string"><span class="hljs-string">"Tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"dog"</span></span>,<span class="hljs-string"><span class="hljs-string">"Probability"</span></span>:<span class="hljs-number"><span class="hljs-number">0.102716163</span></span>}, {<span class="hljs-string"><span class="hljs-string">"TagId"</span></span>:<span class="hljs-string"><span class="hljs-string">"28e1a872-3776-434c-8cf0-b612dd1a953c"</span></span>,<span class="hljs-string"><span class="hljs-string">"Tag"</span></span>:<span class="hljs-string"><span class="hljs-string">"cat"</span></span>,<span class="hljs-string"><span class="hljs-string">"Probability"</span></span>:<span class="hljs-number"><span class="hljs-number">0.02037274</span></span>} ]</code> </pre> <br><p>  Where Probability indicates the likelihood that the image belongs to the specified tag (class). </p><br><p>  In C # it looks like this </p><br><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> client = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> HttpClient(); client.DefaultRequestHeaders.Add(<span class="hljs-string"><span class="hljs-string">"Prediction-Key"</span></span>, <span class="hljs-string"><span class="hljs-string">"{Acess key}"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> url = <span class="hljs-string"><span class="hljs-string">"https://southcentralus.api.cognitive.microsoft.com/customvision/v2.0/Prediction/{Your project GUID}/image"</span></span>; HttpResponseMessage response; List&lt;RecognitionResult&gt; recognitions = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> List&lt;RecognitionResult&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> content = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ByteArrayContent(imageBytes)) { content.Headers.ContentType = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MediaTypeHeaderValue (<span class="hljs-string"><span class="hljs-string">"application/octet-stream"</span></span>); response = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> client.PostAsync(url, content); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (response.IsSuccessStatusCode) { <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> strRes = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> response.Content.ReadAsStringAsync(); <span class="hljs-keyword"><span class="hljs-keyword">dynamic</span></span> res = (<span class="hljs-keyword"><span class="hljs-keyword">dynamic</span></span>) JsonConvert.DeserializeObject(strRes); <span class="hljs-keyword"><span class="hljs-keyword">foreach</span></span> (<span class="hljs-keyword"><span class="hljs-keyword">var</span></span> pr <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> res.predictions) { recognitions.Add( <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> RecognitionResult() { Tag = pr.tagName, RecognPercent = pr.probability }); } } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { Debug.WriteLine( <span class="hljs-string"><span class="hljs-string">"Non successful response. "</span></span> + response.ToString()); } }</code> </pre> <br><p>  As you can see - absolutely nothing complicated.  All the magic happens on the service side. </p><br><h3 id="prilozhenie-i-nekotorye-podobrannye-parametry">  Application and some selected options. </h3><br><p>  The application is quite simple and consists of a list of eco-labels, information about eco-labels, how they are divided and the scanner itself. </p><br><p>  The main part is written in Xamarin.Forms, but the scanner window works with the camera and had to be rendered as renders and implemented for each platform separately. </p><br><p>  The level when the application decides that the ecolabel is recognized exactly&gt; = 90% while almost all the images are recognized if they are of more or less acceptable quality and there are no other signs in the picture. <br>  This number was derived empirically - we started with 80, but realized that 90 reduces false positives.  And they happen quite a lot - many markings are similar and contain similar elements and the color scheme is shifted to green. </p><br><p>  For example, this is not the highest quality image is recognized correctly with an accuracy of 91% </p><br><p><img src="https://habrastorage.org/webt/uc/ru/cp/ucrucpzt4yiudf1ooaco4t7aska.jpeg"></p><br><p>  At the same time, this class was trained in 45 images. </p><br><p>  I hope the article was useful and will allow interested readers to look at new AI and ML tools. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/424379/">https://habr.com/ru/post/424379/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../424369/index.html">My favorite file in Chromium codebase</a></li>
<li><a href="../424371/index.html">Deploying vCloud Extender</a></li>
<li><a href="../424373/index.html">Where to work in IT, release 1: Voximplant</a></li>
<li><a href="../424375/index.html">Mayku FormBox Vacuum Former Review: Let Parts Reproduce</a></li>
<li><a href="../424377/index.html">Playme TIO Review: Top Mount Magnetic Video Recorder</a></li>
<li><a href="../424381/index.html">Hosting a game server in a professional data center</a></li>
<li><a href="../424383/index.html">A complete guide to the proper use of animation in the UX</a></li>
<li><a href="../424385/index.html">The Ultimate DJI GO 4 Tutorial: Main Screen and Camera Settings</a></li>
<li><a href="../424387/index.html">We invite you to the meeting GO.PITER</a></li>
<li><a href="../424389/index.html">A new science of peeking around the corner</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>