<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Depth cameras - silent revolution (when robots see) Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, I described how robots tomorrow will start MUCH better think (post about hardware acceleration of neural networks ). Today we analyze why ro...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Depth cameras - silent revolution (when robots see) Part 1</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/917/25a/9a4/91725a9a49451111a6d55d1015cc297c.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/f58/7fd/ebe/f587fdebeedd2f17fe6f4122a68dff9f.png"><br><br>  Recently, I described how robots tomorrow will start MUCH better think (post about <a href="https://habr.com/ru/post/455353/">hardware acceleration of neural networks</a> ).  Today we analyze why robots will soon be MUCH better to see.  In a number of situations it is much better than a person. <br><br>  It will be about depth cameras, which shoot a video, in each pixel of which not the color is stored, but the distance to the object at this point.  Such cameras have existed for more than 20 years, but in recent years their speed of development has grown many times and we can already speak about a revolution.  And multi-vector.  The rapid development is in the following areas: <br><ul><li>  <a href="https://en.wikipedia.org/wiki/Structured-light_3D_scanner">Structured Light cameras</a> , or structural light cameras, when there is a projector (often infrared) and a camera that captures the structural light of the projector; <br></li><li>  <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">Time of Flight cameras</a> , or cameras based on the measurement of the delay of the reflected light; <br></li><li>  <a href="https://docs.opencv.org/3.1.0/dd/d53/tutorial_py_depthmap.html">Depth from Stereo cameras</a> - the classic and perhaps the most well-known direction for constructing depth from a stereo; <br></li><li>  <a href="https://en.wikipedia.org/wiki/Light-field_camera">Light Field Camera</a> - they are light field cameras or plenoptic cameras, about which there was a separate <a href="https://habr.com/ru/post/440652/">detailed post</a> ; <br></li><li>  And, finally, <a href="https://en.wikipedia.org/wiki/Lidar">Lidar-</a> based cameras, especially the fresh <a href="https://www.google.com/search%3Fq%3DSolid%2BState%2BLidars">Solid State Lidars</a> , which work about 100 times as long as normal lidars and give out a familiar rectangular picture. <br></li></ul><br>  Who cares how it will look, as well as a comparison of different approaches and their current and tomorrow's application - welcome under the cat! <br><a name="habracut"></a><br>  So!  Let us examine the main directions of development of depth chambers or actually different principles of depth measurement.  With their pros and cons. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  Method 1: Structured Light Cameras </h1><br>  Let's start, perhaps, with one of the most simple, old and relatively cheap ways to measure the depth - structured light.  This method appeared essentially as soon as digital cameras appeared, i.e.  more than 40 years ago and greatly simplified a little later, with the advent of digital projectors. <br><br>  The basic idea is extremely simple.  Put a projector next to it, which creates, for example, horizontal (and then vertical) stripes and a camera next to it, which shoots a picture with stripes, as shown in this figure: <br><img src="https://habrastorage.org/getpro/habr/post_images/ed4/416/7c3/ed44167c3f6f9e1af2af2eda53f6ec97.png"><br>  <i>Source: <a href="https://www.instructables.com/id/Structured-Light-3D-Scanning/">Autodesk: Structured Light 3D Scanning</a></i> <br><br>  Since the camera and the projector are displaced relative to each other, the stripes will also be displaced in proportion to the distance to the object.  By measuring this offset we can calculate the distance to the object: <br><img src="https://habrastorage.org/webt/27/oo/xp/27ooxpjh4fgdijfazupjylg1ang.png">  <i>Source: <a href="http://www.vision-systems.com/sponsors/a-h/flir-integrated-imaging-solutions.html">http://www.vision-systems.com/</a></i> <br><br>  In fact, with the cheapest projector (and their price starts at 3,000 rubles) and a smartphone, you can measure the depth of static scenes in a dark room: <br><img src="https://habrastorage.org/getpro/habr/post_images/f41/e50/8b8/f41e508b881e7d1f1b36bbc0dfb9c179.png"><img src="https://habrastorage.org/getpro/habr/post_images/464/662/9ee/4646629ee8562fe3a76beaef82e4c284.png"><img src="https://habrastorage.org/getpro/habr/post_images/85a/493/ee0/85a493ee0b020b885518bfcddb2c9b1d.png"><br>  <i>Source: <a href="https://www.instructables.com/id/Structured-Light-3D-Scanning/">Autodesk: Structured Light 3D Scanning</a></i> <br><br>  It is clear that this will have to solve a whole bunch of tasks - it is the calibration of the projector, the calibration of the phone‚Äôs camera, the recognition of the fringe shift and so on, but all these tasks are fully within the power of even advanced high school students who study programming. <br><br>  This principle of measuring depth became most widely known when in 2010 Microsoft released the <a href="https://en.wikipedia.org/wiki/Kinect">MS Kinect</a> depth <a href="https://en.wikipedia.org/wiki/Kinect">sensor</a> for $ 150, which was revolutionaryly cheap at that time. <br><img src="https://habrastorage.org/getpro/habr/post_images/89c/3d5/8b2/89c3d58b24c16b18593782bd822a5bd2.png"><br>  <i>Source: <a href="http://sci-hub.se/10.1117/12.2053938">Partially Occluded Object Reconstruction using Multiple Kinect Sensors</a></i> <br><br>  With the addition of the actual depth measurement with an IR projector and an IR camera, Kinect also shot normal RGB video, had four microphones with noise reduction and could tune itself to a person in height, automatically tilting up or down, was built in right there data processing, which gave the finished depth map to the console immediately: <br><img src="https://habrastorage.org/getpro/habr/post_images/a48/49e/1a1/a4849e1a19f745aedb92800debafc6f7.png"><br>  <i>Source: <a href="https://www.semanticscholar.org/paper/Implementation-of-natural-user-interface-buttons-Ollila/bb4c451e29a3b2efcc9abc983518c90c0e37cf3c">Implementation of natural user interface buttons using Kinect</a></i> <br><br>  A total of about 35 million devices were sold, making Kinect the first mass depth camera in history.  And if we take into account that the depth chambers of course were before her, but they were usually sold by a maximum of hundreds and cost at least an order of magnitude more expensive - this was a revolution that provided large investments in this area. <br><br>  An important reason for the success was that by the time Microsoft launched the Xbox 360 there were already a few games that were actively using Kinect as a sensor.  Takeoff was swift: <br><img width="75%" src="https://habrastorage.org/getpro/habr/post_images/895/5be/676/8955be6766420326e6fc7c1ab2371385.png"><br><br>  Moreover, Kinect even managed to enter the Guinness Book of Records as the fastest selling gadget in history.  True, Apple soon pressed Microsoft out of this place, but nonetheless.  For a new experimental sensor that works in addition to the main device to become the fastest selling electronic device in history, this is simply a great achievement: <br><img src="https://habrastorage.org/getpro/habr/post_images/051/f2a/cf2/051f2acf275e677c10252fc38541ff82.png"><br><br>  While lecturing, I like to ask the audience where all these millions of shoppers come from?  Who were all these people? <br><br>  As a rule, no one guesses, but sometimes, especially if people are older and more experienced in the audience, they give the correct answer: the sales were driven by American parents, who enthusiastically saw that their children could play on the console and not sit on the couch while jumping in front of the TV.  It was a breakthrough !!!  Millions of moms and dads rushed to order a device for their children. <br><br>  Generally, when it comes to recognizing gestures, people usually naively believe that just having data from a 2D camera is enough.  After all, they have seen many beautiful demos!  The reality is much more harsh.  The accuracy of gesture recognition from the 2D video stream from the camera and the accuracy of gesture recognition from the camera depth differ by an order of magnitude.  From the depth camera, or rather from the RGB camera combined with the depth camera (the latter is important), gestures can be recognized much more accurately and cost-effectively (even if it is dark in the room) and this brought success to the first mass depth camera. <br><br>  About Kinect on Habr√© in his time wrote <a href="https://www.google.com/search%3Fq%3Dkinect%2Bsite%253Ahabr.com%2B-%25D0%25B7%25D0%25B0%25D0%25BA%25D0%25BB%25D0%25B0%25D0%25B4%25D0%25BA%25D0%25B8">a lot</a> , so very briefly how it works. <br><br>  An infrared projector gives a pseudo-random set of points in space, the displacement of which determines the depth in a given pixel: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/55f/c46/d72/55fc46d72e36087ac7522312d791af6f.png"><br>  <i>Source: <a href="https://www.semanticscholar.org/paper/Depth-Sensing-Planar-Structures%253A-Detection-of-O%2527Shaughnessy/2a003536afeaa53306c41ce2edb343cbd6e72fa4">Depth Sensing Planar Structures: Detection of Office Furniture Configurations</a></i> <br><br>  The resolution of the camera is stated as 640x480, but in reality there is somewhere around 320x240 with fairly strong filtering and the picture looks like real examples (that is, pretty scary): <br><img src="https://habrastorage.org/webt/a9/ni/up/a9niup8_g7i8a1x3n_0oaikpee0.png"><br>  <i>Source: <a href="http://sci-hub.se/10.1117/12.2053938">Partially Occluded Object Reconstruction using Multiple Kinect Sensors</a></i> <br><br>  The ‚Äúshadows‚Äù of objects are clearly visible, since the camera and the projector are spaced quite far away.  It can be seen that to predict the depth, shifts of several points of the projector are taken.  In addition, there is a (hard) filtering by direct neighbors, but still the depth map is quite noisy, especially at the borders.  This leads to quite noticeable noise on the surface of the resulting objects, which must be further and nontrivially smoothed: <br><img src="https://habrastorage.org/webt/ph/e-/dl/phe-dlp6dczaztxgy7q4p-tyxtg.png"><br>  <i>Source: <a href="https://research.dwi.ufl.edu/ufdw/j4k/">J4K Java Library for the Microsoft's Kinect SDK</a></i> <br><br>  Nevertheless, only $ 150 ( <a href="https://www.amazon.com/LilBit-Recognition-Camera-Microphone-Conference/dp/B06Y2WLNFN/">today is $ 69</a> , although <a href="https://www.amazon.com/Intel-RealSense-Depth-Camera-D415/dp/B07JVGRQZT/ref%3Dsr_1_4%3Fkeywords%3Ddepth%2Bcamera%26qid%3D1561197036%26s%3Dgateway%26sr%3D8-4">it‚Äôs better to get closer to $ 200</a> , of course) - and you ‚Äúsee‚Äù the depth!  Serial products <a href="https://www.amazon.com/s%3Fk%3Ddepth%2Bcamera%26ref%3Dnb_sb_noss">really a lot</a> . <br><br>  By the way, in February of this year, the new <a href="https://next.reality.news/news/microsofts-azure-kinect-standalone-depth-sensor-powers-major-augmented-reality-improvements-for-399-0194185/">Azure Kinect</a> was announced: <br><img src="https://habrastorage.org/webt/l6/0y/b2/l60yb2lcfeuufoe6q6jdd0e-jg0.png"><br>  <i>Source: <a href="https://www.neowin.net/news/microsoft-announces-azure-kinect-available-for-pre-order-now/">Microsoft announces Azure Kinect, available for pre-order now</a></i> <br><br>  Its deliveries to developers in the US and China should begin on June 27, i.e.  literally right now.  Of the possibilities, besides the noticeably better RGB resolution and better quality depth cameras (promising <a href="https://opdhsblobprod03.blob.core.windows.net/contents/503db294612a42b3b95420aaabac44cc/77342d6514e7dbbcf477614ed3a7acda%3Fsv%3D2015-04-05%26sr%3Db%26sig%3DFxpjsC3Njkwo26ppGk%252BFjrKpftp48oswIVvA%252FzrAh%252F0%253D%26st%3D2019-06-23T17%253A14%253A13Z%26se%3D2019-06-24T17%253A24%253A13Z%26sp%3Dr">1024x1024</a> at 15 FPS and 512x512 at 30 FPS and higher quality is <a href="https://youtu.be/aa8DzmvLxus%3Ft%3D51">clearly seen in the demo</a> , the ToF camera) supports the collaboration of several devices out of the box; sun, an error of less than 1 cm at a distance of 4 meters and 1-2 mm at a distance of less than 1 meter, which sounds EXTREMELY interesting, so we wait, we wait: <br><img src="https://habrastorage.org/getpro/habr/post_images/0d1/972/90b/0d197290bf6363f157fa9955070c4b5a.png"><br>  <i>Source: <a href="https://www.youtube.com/watch%3Fv%3DjJglCYFiodI">Introducing Azure Kinect DK</a></i> <br><br>  The next <b>mass</b> product, where the depth camera was implemented in structured light, was not a game console, but ... (drum roll) is correct - <a href="https://www.extremetech.com/mobile/255771-apple-iphone-x-truedepth-camera-works">iPhone X</a> ! <br><br>  Its Face ID technology is a typical depth camera with a typical infrared Dot projector and an infrared camera (by the way, now you understand why they are at the edges of the ‚Äúbangs‚Äù, spaced as far as possible from each other - this is a <a href="https://ru.wiktionary.org/wiki/%25D1%2581%25D1%2582%25D0%25B5%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D0%25B0%25D0%25B7%25D0%25B0">stereo base</a> ): <br><img src="https://habrastorage.org/getpro/habr/post_images/755/6e2/6de/7556e26dea5a132b997658d386d74e7f.png"><br><br>  The resolution of the depth map is even less than that of the Kinect - about 150x200.  It is clear that if you say: ‚ÄúOur resolution is about 150x200 pixels or 0.03 megapixels,‚Äù the people will say briefly and succinctly: ‚ÄúSucks!‚Äù.  And if you say <a href="https://www.apple.com/lae/iphone-xr/face-id/">"Dot projector: More than 30,000 invisible dots are projected onto your face,"</a> People will say: "Wow, 30 thousand invisible points, cool!"  Some blondes will ask if freckles appear from invisible points?  And the topic will go to the masses!  Therefore, the second option was far-sighted in advertising.  The resolution is not large for three reasons: firstly, the miniature requirements, secondly, energy consumption, and thirdly, prices. <br><br>  Nevertheless, this is another depth camera in a structured light, gone into a series of millions of copies and has already been repeated by other manufacturers of smartphones, <a href="https://www.systemplus.fr/reverse-costing-reports/the-huawei-mate-20-pros-3d-depth-sensing-system/">for example (surprise-surprise!) Huawei</a> (which bypassed Apple in sales of smartphones last year).  Only Huawei has a camera on the right and a projector on the left, but also, of course, on the edges of the ‚Äúbang‚Äù: <br><img src="https://habrastorage.org/webt/zt/2h/ab/zt2habvk5zl2pkyvmeerbzacjfq.png"><br>  <i>Source: <a href="https://www.xda-developers.com/huawei-mate-20-pro-update-second-face-unlock/">Huawei Mate 20 Pro unlock</a></i> <br><br>  In this case, 300,000 dots are announced, that is <a href="https://www.engadget.com/2017/11/28/huawei-claims-face-detection-system-can-beat-apple/">, 10 times more than that of Apple</a> , and the front camera is better, <s>and the font is larger</s> .  Whether there is an exaggeration regarding 300 thousand is hard to say, but Huawei demonstrates very good <a href="https://www.youtube.com/watch%3Fv%3DLo61GM_2BpY">3D scanning of objects with a front camera</a> .  Independent tests are <a href="https://www.gizmodo.co.uk/2018/11/huawei-mate-20-pro-toy-scanning/">more terrible</a> , but this is clearly the very beginning of the topic and the infancy of the technology of miniature energy-efficient depth cameras and camera announcements literally at the end of this year is already noticeably better in performance. <br><br>  At the same time, it is clear why the technology of identification of persons was used in phones.  First, now it is impossible to deceive the detector by showing a photo of a person (or video from a tablet).  Secondly, the face changes greatly when the lighting changes, but its shape does not, which makes it possible to identify a person more precisely with the data from the RGB camera: <br><img src="https://habrastorage.org/getpro/habr/post_images/5c9/12b/e86/5c912be86bf5a47aa02bcce67e48f148.png"><br>  <i>Source: <a href="http://www.ti.com/lit/wp/sloa190b/sloa190b.pdf">photo of the same person from TI materials</a></i> <br><br>  Obviously, the infrared sensor has inherent problems.  Firstly, the sun illuminates our relatively weak projector once or twice, so these cameras do not work on the street.  Even in the shade, if the white wall of the building is lit by the sun, you may have big problems with your Face ID.  The noise level in Kinect also rolls over, even when the sun is covered with clouds: <br><img src="https://habrastorage.org/getpro/habr/post_images/e5b/477/bb5/e5b477bb581588840cb6d1219b7899dd.png"><br>  <i>Source: this and the next two pictures</i> - <i><a href="https://youtu.be/OMDfQC0m4i4%3Ft%3D272">Basler AG materials</a></i> <br><br>  Another big problem is reflections and reflections.  Since infrared light is also reflected, it will be problematic to photograph an expensive stainless steel kettle, a varnished table or a glass ceiling with a Kinect: <br><img src="https://habrastorage.org/getpro/habr/post_images/872/913/e20/872913e207e550f6e7eb609510fd70f6.png"><br><br>  And finally, two cameras that shoot one object can interfere with each other.  Interestingly, in the case of a structured light, you can make the projector flicker and understand where our points are, and where not, but this is a separate and rather difficult story: <br><img src="https://habrastorage.org/getpro/habr/post_images/c9f/427/3e3/c9f4273e3bf9682c1b6869cbfaaf22b5.png"><br><br>  Now you know how to break FaceID ... <br><br>  However, for mobile devices, structured light looks like the most reasonable compromise for today: <br><img src="https://habrastorage.org/getpro/habr/post_images/cc8/77f/743/cc877f74308b460ca7cd306856ba637a.png"><br>  <i>Source: <a href="http://image-sensors-world.blogspot.com/2018/03/smartphone-companies-scrambling-to.html">Smartphone Companies Scrambling to Match Apple 3D Camera Performance and Cost</a></i> <br><br>  For structured light, the cheapness of a conventional sensor is such that its use in most cases is more than justified.  What brought to life a large number of startups operating by the formula: cheap sensor + complex software = quite an acceptable result. <br><br>  For example, our former graduate student <a href="http://clip-russia.ru/speaker/feduykov/">Maxim Fedyukov</a> , who has been involved in 3D reconstruction since 2004, created <a href="https://texel.graphics/ru/">Texel</a> , whose main product is a platform with 4 Kinect cameras and software that in 30 seconds turns a person into a potential monument.  Well, or a desktop statuette.  This is someone for how much money is enough.  Or you can send your 3D model photos cheaply and angrily (while for some reason the most requested case).  Now they send their platforms and software abroad from the UK to Australia: <br><iframe width="560" height="315" src="https://www.youtube.com/embed/VLaZ_jDuZ30" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  <i>Source: <a href="https://www.youtube.com/watch%3Fv%3DVLaZ_jDuZ30">Creating a 3D human model in 30 seconds</a></i> <br><br>  As a ballerina, I cannot stand beautifully, so I only thoughtfully look at the fin of a shark swimming by: <br><img src="https://habrastorage.org/getpro/habr/post_images/fca/b59/2b2/fcab592b290ca2a9838748bc3ac82ce2.gif"><br>  <i>Source: materials of the author</i> <br><br>  In general, a new type of sensor generated new art projects.  In winter, I saw a rather curious VR film, shot with Kinect.  Below is an interesting visualization of the dance, also made with Kinect (it looks like 4 cameras were used), and unlike the previous example, they didn‚Äôt struggle with noises, they rather added some amusing specifics: <br><img src="https://habrastorage.org/getpro/habr/post_images/cd3/072/e7f/cd3072e7ffef9e111604f9d6e83e640e.gif"><br>  <i>Source: <a href="https://www.pinterest.ru/pin/173951604333717281/">A Dance Performance Captured With A Kinect Sensor And Visualized With 3D Software</a></i> <br><br>  What trends can be observed in the area: <br><ul><li>  As is well known, digital sensors of modern cameras are sensitive to infrared radiation, so you have to use special <a href="https://en.wikipedia.org/wiki/Infrared_cut-off_filter">blocking filters</a> so that infrared noise does not spoil the picture (even the direction of <a href="http://lightroom.ru/photomaster/1567-nevidimyj-mir-osnovy-infrakrasnoj-semki-primery-foto.html">artistic imaging in the infrared range appeared</a> , including when the filter is removed from the sensor).  This means that huge amounts of money are invested in miniaturization, increasing resolution and cheapening sensors that can be used as infrared (with a <a href="https://www.google.com/search%3Fq%3Dir%2Bpass%2Bfilter">special filter</a> ). <br></li><li>  Similarly, algorithms for processing depth maps, including the methods of the so-called cross-filtering, when data from an RGB sensor and noisy data on depth make it possible to get very good video depth, are rapidly improving.  In this case, using neural network approaches, it becomes possible to drastically increase the speed of obtaining a good result. <br></li><li>  In this area, all the top companies work, especially manufacturers of smartphones. <br></li></ul><br>  Consequently: <br><ul><li>  You can expect a dramatic increase in the resolution and accuracy of shooting Structured Light depth cameras in the next 5 years. <br></li><li>  It will go (though slower) to reduce the power consumption of mobile sensors, which will simplify the use of next-generation sensors in smartphones, tablets and other mobile devices. <br></li></ul><br>  In any case, what we are seeing now is the infancy of technology.  The first mass products, which are just debugging the production and use of a new unusual type of data - video with depth. <br><br><h1>  Method 2: Time of Flight Camera </h1><br>  The next way to get the depth is more interesting.  It is based on measuring round-trip light delay (ToF - <a href="https://en.wikipedia.org/wiki/Time-of-flight_camera">Time-of-Flight</a> ).  As you know, the speed of modern processors is high, and the speed of light is small.  For one processor clock at 3 GHz, the light has time to fly just 10 centimeters.  Or 10 cycles per meter.  A lot of time, if someone was engaged in low-level optimization.  Accordingly, we install a pulsed light source and a special camera: <br><img src="https://habrastorage.org/getpro/habr/post_images/6e0/565/fc8/6e0565fc82326ab1f3487051f20ef58d.png"><br>  <i>Source: <a href="https://www.baslerweb.com/en/products/cameras/3d-cameras/time-of-flight-camera/">The Basler Time-of-Flight (ToF) Camera</a></i> <br><br>  In fact, we need to measure the delay in which the light returns to each point: <br><img src="https://habrastorage.org/getpro/habr/post_images/26b/fd8/bc8/26bfd8bc85003daa2b45d9dd11dfb31d.png"><img src="https://habrastorage.org/getpro/habr/post_images/4f6/7a5/13d/4f67a513d6cd9fb6e9370d680a28db79.png"><br>  <i>Source: <a href="https://www.baslerweb.com/en/products/cameras/3d-cameras/time-of-flight-camera/">The Basler Time-of-Flight (ToF) Camera</a></i> <br><br>  Or, if we have several sensors with different charge accumulation time, then, knowing the time shift relative to the source for each sensor and the flash brightness taken, we can calculate the shift and, accordingly, the distance to the object, and increasing the number of sensors - we increase the accuracy: <br><img src="https://habrastorage.org/getpro/habr/post_images/e78/c93/d70/e78c93d70d056347e6e963bb191f3016.png"><br><img src="https://habrastorage.org/getpro/habr/post_images/27f/26f/e8d/27f26fe8d30826c317bbc54687fbe169.png"><br>  <i>Source: <a href="http://www.ti.com/lit/wp/sloa190b/sloa190b.pdf">Larry Li "Time-of-Flight Camera - An Introduction"</a></i> <br><br>  The result is such a scheme of the camera with LED or, more rarely, laser ( <a href="https://en.wikipedia.org/wiki/Vertical-cavity_surface-emitting_laser">VCSEL</a> ) infrared illumination: <br><img src="https://habrastorage.org/getpro/habr/post_images/1b8/5cf/e74/1b85cfe7436630ed22abf6f9ec4c0555.png"><br>  <i>Source: a <a href="https://www.allaboutcircuits.com/technical-articles/how-do-time-of-flight-sensors-work-pmdtechnologies-tof-3D-camera/">very good job description of ToF on allaboutcircuits.com</a></i> <br><br>  The picture thus turns out to be quite low resolution (after all, we need to position several sensors next to each other with different polling times), but potentially with high FPS.  And the problems are mainly at the boundaries of objects (which is typical for all depth cameras).  But without the ‚Äúshadows‚Äù typical for structured light: <br><img src="https://habrastorage.org/getpro/habr/post_images/f25/aac/393/f25aac393d27eab254d2d2062f70aace.gif"><br>  <i>Source: <a href="https://youtu.be/OMDfQC0m4i4%3Ft%3D209">Basler AG video</a></i> <br><br>  In particular, it was this type of camera (ToF) that at one time actively tested Google in the <a href="https://en.wikipedia.org/wiki/Tango_(platform)">Google Tango</a> project, which is well presented in <a href="https://www.youtube.com/watch%3Fv%3DQe10ExwzCqk">this video</a> .  The point was simple - to combine the data of a gyroscope, an accelerometer, an RGB camera and a depth camera by building a three-dimensional scene in front of a smartphone: <br><img src="https://habrastorage.org/getpro/habr/post_images/0e5/48d/a33/0e548da33324491beed0083067daf629.png"><br>  <i>Source: <a href="https://gizmodo.com/hands-on-googles-project-tango-is-now-sized-for-smartp-1707811301">Google's Project Tango Is Now Sized for Smartphones</a></i> <br><br>  The project itself did not go (my opinion was due to the fact that it was somewhat ahead of its time), but it created important prerequisites for creating a wave of interest in AR - augmented reality - and, accordingly, developing sensors that can work with it.  Now all his achievements are poured into <a href="https://developers.google.com/ar/">ARCore</a> from Google. <br><br>  In general, the volume of the ToF camera market is growing by about 30% every 3 years, which is quite an exponential growth, and so few markets are growing so fast: <br><img src="https://habrastorage.org/getpro/habr/post_images/9ae/e45/a34/9aee45a343486d009e472897358a57c6.png"><br>  <i>Source: <a href="https://www.baslerweb.com/en/products/cameras/3d-cameras/time-of-flight-camera/an-overview-of-time-of-flight/">Potential of Time-of-Flight Cameras &amp; Market Penetration</a></i> <br><br>  A serious market driver today is the rapid (and also exponential) development of industrial robots, for which ToF cameras are the ideal solution.  For example, if you have a robot that is packing boxes, then with a conventional 2D camera, determining that you are starting to jam the cardboard is an extremely non-trivial task.  And for the ToF camera it is trivial to both ‚Äúsee‚Äù and process.  And very quickly.  As a result, we are seeing a <a href="https://www.google.com/search%3Fq%3Dindustrial%2Btof%2Bcamera%26source%3Dlnms%26tbm%3Disch">boom in industrial ToF cameras</a> : <br><img width="50%" src="https://habrastorage.org/webt/rg/7k/oh/rg7kohuwwuzhwji6zrqr19a54yy.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/843/1a9/ac6/8431a9ac68acfef82b2f9dc5e5579d32.png"><br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/c66/2c5/5c1/c662c55c1309b666ef460c5944289fbd.png"><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/5d7/514/fda/5d7514fda4a985c28cbc6695e1a3e27a.png"><br>  Naturally, this also leads to the appearance of home-made products using depth cameras.  Here, for example, a security camera with a night video unit and a ToF depth camera from the German <a href="https://pmdtec.com/mwc/">PMD Technologies</a> , which has been developing 3D cameras for <a href="https://en.wikipedia.org/wiki/PMD_Technologies">more than 20 years</a> : <br><img src="https://habrastorage.org/webt/dj/uz/_m/djuz_mmy6htracd_tgkn_clt698.png"><br>  <i>Source: <a href="https://www.businesswire.com/news/home/20180223005227/en/pmd%25E2%2580%2599s-3D-Time-of-Flight-Depth-Sensing-Brings-Magic">3D Time-of-Flight-Depth Sensing</a></i> <br><br>  Remember the invisibility cloak under which Harry Potter was hiding? <br><img width="50%" src="https://habrastorage.org/getpro/habr/post_images/4cc/5ac/d00/4cc5acd002df5032104a965cc01ec109.png"><br>  <i>Source: <a href="https://www.vanityfair.com/hollywood/2015/09/harry-potter-invisibility-cloak-jk-rowling">Harry Potter's Exact in Real Life</a></i> <br><br>  I am afraid that the German camera will detect it once or twice.  And it will be difficult to put a screen with a picture in front of such a camera (this is not a distracted guard to you): <br><img src="https://habrastorage.org/getpro/habr/post_images/af4/649/03f/af464903f5fdbda4fca2c1734f476ec5.png"><br>  <i>Source: <a href="https://youtu.be/4SVKtXom7vs%3Ft%3D67">Fragment of the film ‚ÄúMission: Impossible: Phantom Protocol‚Äù</a></i> <br><br>  It seems that the new Hogwarts magic will be needed for CCTV cameras to fool their ToF depth camera, which is able to shoot such a video in complete darkness: <br><img width="25%" src="https://habrastorage.org/getpro/habr/post_images/b34/f9f/884/b34f9f8844825387a528da8d630a69cb.gif"><br>  To pretend to be a wall, a screen, and other means of protecting oneself from the fact that a combined ToF + RGB camera detects a foreign object becomes technically more difficult. <br><br>  Another massive peaceful application of depth cameras is gesture recognition.  In the near future, we can expect televisions, consoles and robotic vacuum cleaners, which will be able to perceive not only voice commands as smart speakers, but also careless ‚Äúput away!‚Äù With a wave of the hand.  Then the remote control (aka laziness) to the smart TV will not be completely needed, and fiction will come to life.  As a result, what <a href="https://www.researchgate.net/publication/2530570_Hand_Gesture_Recognition_using_Multi-Scale_Colour_Features_Hierarchical_Models_and_Particle_Filtering/figures%3Flo%3D1%26utm_source%3Dgoogle%26utm_medium%3Dorganic">was fantastic in 2002</a> <a href="https://www.youtube.com/watch%3Fv%3DwDmosRnEfiw">became experimental in 2013</a> , and finally <a href="https://www.amazon.co.uk/singlecue-Gesture-Control-Entertainment-Devices-Black/dp/B016ICIBOU">serial in 2019</a> (people will not know what is inside the depth chamber, <s>what difference does this magic work?</s> ): <br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/9e6/bd8/2bd/9e6bd82bda38b0c8bb9cb939afee76a7.png"><img width="44%" src="https://habrastorage.org/getpro/habr/post_images/2a8/dcc/b9c/2a8dccb9cc86e1fe8ad57c613432e146.png"><br>  <i>Source: <a href="https://www.researchgate.net/publication/2530570_Hand_Gesture_Recognition_using_Multi-Scale_Colour_Features_Hierarchical_Models_and_Particle_Filtering/figures%3Flo%3D1%26utm_source%3Dgoogle%26utm_medium%3Dorganic">article</a> , <a href="https://www.youtube.com/watch%3Fv%3DwDmosRnEfiw">experiments</a> and <a href="https://www.amazon.co.uk/singlecue-Gesture-Control-Entertainment-Devices-Black/dp/B016ICIBOU">product</a></i> <br><br>  And the full range of applications is even wider, of course: <br><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/66d/fc8/23d/66dfc823dd2619f52003f3dbd92bbf34.gif"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/0c5/819/751/0c5819751f59b53a33f4b0d37efb1caf.gif"><img width="33%" src="https://habrastorage.org/getpro/habr/post_images/7b0/266/402/7b02664025b9c9c4fbab5b5825822a2b.gif"><br>  <i>Source: <a href="https://www.terabee.com/sensors-modules/3d-tof-cameras/">Terabee video depth sensors</a></i> <i>(by the way, what</i> <i><a href="https://www.terabee.com/sensors-modules/3d-tof-cameras/">kind of</a></i> <i><b>mice</b> do they run on the floor for 2 and 3 videos? See them? I'm kidding, it's dust in the air - the fee for the small size of the sensor and the proximity of the light source to the sensor)</i> <br><br>  By the way - in the famous "shops without cashiers" Amazon Go under the ceiling, too many cameras: <br><img src="https://habrastorage.org/webt/ca/k9/ds/cak9ds3gc_8-a9n2tgwxdjhvpg0.png"><br>  <i>Source: <a href="https://techcrunch.com/2018/01/21/inside-amazons-surveillance-powered-no-checkout-convenience-store/">Inside Amazon's surveillance-powered, no-checkout convenience store</a></i> <br><br>  Moreover, as <a href="https://techcrunch.com/2018/01/21/inside-amazons-surveillance-powered-no-checkout-convenience-store/">TechCrunch</a> writes: <i>‚ÄúThey‚Äôre augmented by separate <b>depth-sensing cameras</b> (using a <b>time-of-flight technique</b> , or so I understood from Kumar) that is, matte black.‚Äù</i> That is, the miracle of determining exactly which shelf yogurt was taken from is provided by the mysterious black matte ToF cameras (a good question, are they in the photo): <br><img src="https://habrastorage.org/webt/25/qj/g1/25qjg1s8_o5uyjmoyv9r-haadms.png"><br><br>  Unfortunately, it is often difficult to find direct information.  But there is an indirect.  For example, there was such a company <a href="https://en.wikipedia.org/wiki/Softkinetic">Softkinetic</a> , which since 2007 has been developing ToF cameras.  8 years later, they were <a href="https://techcrunch.com/2015/10/08/sony-buys-gesture-tracker-and-3d-sensor-maker-softkinetic/">bought by Sony</a> (which, by the way, is ready to conquer new markets under the brand <a href="https://www.sony-depthsensing.com/">Sony Depthsensing</a> ).<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">So one of the </font></font><a href="https://www.linkedin.com/in/laurent-guigues-77290b5/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">top</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Softkinetic </font><a href="https://www.linkedin.com/in/laurent-guigues-77290b5/"><font style="vertical-align: inherit;">employees</font></a><font style="vertical-align: inherit;"> is now working on Amazon Go. Such a coincidence! In a couple of years, when the technology is brought in and the main patents are filed, the details are likely to be disclosed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, as usual, the Chinese lit. Company </font></font><a href="https://www.pico-interactive.com/zense"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pico Zense</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , for example, introduced at CES 2019 a very impressive lineup of ToF cameras, including for outdoor use:</font></font><br><img src="https://habrastorage.org/getpro/habr/post_images/d8c/c1c/545/d8cc1c545595e42cdab55a1a790384c9.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">They promise revolution everywhere. Wagons will be loaded more tightly due to automated loading, ATMs will become safer, due to depth cameras in each, robot navigation will become simpler and more accurate, people (and, most importantly, children!) Will be much better in the stream, new fitness equipment will appear c the ability to control the correctness of the exercises without an instructor, and so on and so forth. Naturally, cheap Chinese cameras of the depth of the new generation are ready for all this magnificence. Take and build! </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interestingly, the latest serial Huawei P30 Pro has a ToF sensor next to the main cameras, i.e. the long-suffering Huawei knows better than Apple how to make frontal structured light sensors and, it seems, more successful than Google (Project Tango, which </font></font><a href="https://www.anandtech.com/show/12166/google-to-shut-down-project-tango-in-favor-of-arcore"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">was closed</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">) introduced next to the main cameras ToF camera: </font></font><br><img width="60%" src="https://habrastorage.org/getpro/habr/post_images/fc9/eaf/763/fc9eaf76396c966eeab065ca641e8543.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: </font></font><a href="https://arstechnica.com/gadgets/2019/03/new-huawei-phone-has-a-5x-optical-zoom-thanks-to-a-periscope-lens/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">review of the Huawei technology from Ars Technica at the end of March 2019</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Details of use, of course, are not disclosed, but in addition to accelerating the focusing (which is important for the three main cameras with different lenses), this sensor can be used to increase the quality of background blur for photos (imitation of small </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25BB%25D1%2583%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25B0_%25D1%2580%25D0%25B5%25D0%25B7%25D0%25BA%25D0%25BE_%25D0%25B8%25D0%25B7%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B6%25D0%25B0%25D0%25B5%25D0%25BC%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D1%2581%25D1%2582%25D0%25B2%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">DOF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It is also obvious that the next generation of depth sensors next to the main cameras will be used in AR applications, which will raise the accuracy of the AR from the current ‚Äúfun, but often buggy‚Äù to a massively working level. And, obviously, in the light of the Chinese success, the big question is how much Google will want to support in </font></font><a href="https://developers.google.com/ar/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">ARCore</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">revolutionary Chinese iron. </font><font style="vertical-align: inherit;">Patent wars can significantly slow technology entry into the market. </font><font style="vertical-align: inherit;">We will see the development of this dramatic story literally in the next two years.</font></font><br><br><h1><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Subtotals </font></font></h1><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">About 25 years ago, when the first automatic doors just appeared, I personally observed how quite respectable guys periodically accelerated before such doors. Will you have time to open or will you not have time? She is big, heavy, glass! Approximately the same thing I observed during the excursion of quite respectable professors at an automatic factory in China recently. They were a little behind the group to see what would happen if they got in the way of a robot peacefully carrying parts and playing a quiet pleasant tune. I, too, repent, could not resist ... You know, it stops! Maybe smoothly. Maybe as rooted to the spot. Depth sensors are working! </font></font><br><img src="https://habrastorage.org/webt/6z/h_/xm/6zh_xmrkvzfljpw0hkymlngojrs.png"><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Source: </font></font><a href="https://www.gettyimages.com/detail/news-photo/robot-center-right-transports-boxes-of-finished-products-as-news-photo/1083192424"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inside Huawei Technology's New Campus</font></font></a></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> The hotel also had cleaning robots that looked like this:</font></font><br><img src="https://habrastorage.org/webt/di/rm/bc/dirmbceths3wfdn1sbtswf9tduw.png"><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At the same time, they were mocked at them more than they were at the robots at the factory. </font><font style="vertical-align: inherit;">Not as tough as the </font></font><b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">inhuman</font></font></i></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in every sense of </font></font><a href="https://youtu.be/dKjCWfuvYxQ%3Ft%3D26"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Bosstown Dynamics</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , of course. </font><font style="vertical-align: inherit;">But I personally observed how they got up on the road, the robot tried to drive around a person, the person shifted, blocking the road ... A sort of cat and mouse. </font><font style="vertical-align: inherit;">In general, it seems that when unmanned vehicles appear on the roads, at first they will be cut more often than usual ... Eh, people-people ... Hmmm ... However, we digress. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summarizing the key points:</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Due to a different principle of operation, we can locate the light source in the ToF camera as close as possible to the sensor (even under the same lens). </font><font style="vertical-align: inherit;">In addition, many industrial models have LEDs located around the sensor. </font><font style="vertical-align: inherit;">As a result, the ‚Äúshadows‚Äù on the depth map are drastically reduced, or even in general, disappear.</font></font> Those.       ,     . <br></li><li>    ,  ,  ‚Äî         :   ,        .. ,      ,      . <br></li><li> , ToF   ¬´  ¬ª    RGB ,     ,         ToF  <a href="https://www.google.com/search%3Fq%3Dtof%2Bsensor%26source%3Dlnms%26tbm%3Dnws">    ()   </a> (       Samsung,  Google Pixel,  Sony Xperia...). <br></li><li>   Sony ,  2   8   (!!!)  ToF   (!), ..       : <img src="https://habrastorage.org/getpro/habr/post_images/5cb/319/b6a/5cb319b6ade6cec9232d201201d15a60.png"><br> <i>: <a href="https://www.gsmarena.com/hexacam_sony_phone_gets_camera_specs_revealed-news-37637.php">Hexa-cam Sony phone gets camera specs revealed</a></i> <br></li><li>  , <b>          !</b>      20%       (Structured Light + ToF).  ,   2017     Apple     ¬´30  ¬ª,    300    ,    : <br><img src="https://habrastorage.org/getpro/habr/post_images/463/6e5/971/4636e597115004d4a161719cfcafbe9d.png"><br> <i>: <a href="https://press.trendforce.com/node/view/3226.html">Limited Smartphone 3D Sensing Market Growth in 2019; Apple to be Key Promoter of Growth in 2020</a></i> <br></li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Do you still doubt the revolution going? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This was the first part! </font><font style="vertical-align: inherit;">A general comparison will be in the second. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the next series, wait:</font></font><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Method 3, classic: depth from stereo; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Method 4, newfangled: the depth of plenoptic; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Method 5, fast-growing: lidars, including Solid State Lidars; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Some problems of processing video with depth; </font></font><br></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> And finally, a brief comparison of all 5 methods and general conclusions. </font></font><br></li></ul><br><br> <b><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Carthage must be destroyed ...</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> All video until the end of the century will be three-dimensional! </font></font></b> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stay tuned! </font><font style="vertical-align: inherit;">(If there is enough time - by the end of the year I will describe new cameras, including tests of fresh Kinect.)</font></font><br><br>  <a href="https://habr.com/ru/post/458458/">Part 2</a> <br><br><div class="spoiler">  <b class="spoiler_title">Acknowledgments</b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> I would like to sincerely thank: </font></font><br><ul><li> Laboratory of Computer Graphics VMK MSU.  MV Lomonosov for his contribution to the development of computer graphics in Russia in general and work with depth cameras in particular, <br></li><li>  Microsoft, Apple, Huawei and Amazon for excellent products based on depth cameras, <br></li><li>  Texel for the development of high-tech Russian products with depth cameras, <br></li><li>  personally Konstantin Kozhemyakov, who did a lot to make this article better and clearer, <br></li><li>  and, finally, many thanks to Roman Kazantsev, Yevgeny Lyapustin, Yegor Sklyarov, Maxim Fedyukov, Nikolay Oplachko and Ivan Molodetsky for a large number of practical comments and edits that made this text much better! <br></li></ul><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/457524/">https://habr.com/ru/post/457524/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457514/index.html">Nevangary</a></li>
<li><a href="../457516/index.html">Writing a threat model</a></li>
<li><a href="../457518/index.html">Plasma Cash Chain as a solution to the blockchain scalability trilemma</a></li>
<li><a href="../457520/index.html">Creating a 3-level menu using the Htmlix framework - part 2 mobile menu version</a></li>
<li><a href="../457522/index.html">Raise your mailing service or use ready-made solutions? What I learned for 5 years of work in UniSender</a></li>
<li><a href="../457526/index.html">Technical media as a bazaar</a></li>
<li><a href="../45753/index.html">DARPA Binoculars beats the diffraction limit</a></li>
<li><a href="../457534/index.html">Certified versions - a rake that we choose</a></li>
<li><a href="../457538/index.html">How can you use interrupted Yandex.Cloud virtual machines and save on solving large-scale tasks</a></li>
<li><a href="../457540/index.html">Intel Optane DC Persistent Memory, a year later</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>