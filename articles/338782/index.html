<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Select and configure SDS Ceph</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello everyone, Dear readers and practitioners! 

 I felt all sorts of different and diverse Block / File Storage with SANs and was, in general, happy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Select and configure SDS Ceph</h1><div class="post__text post__text-html js-mediator-article">  Hello everyone, Dear readers and practitioners! <br><br>  I felt all sorts of different and diverse Block / File Storage with SANs and was, in general, happy, until the task appeared to understand - what is Object Storage?  And if there are already many decisions on the market, choose the one ... <br><a name="habracut"></a><br><h2>  Why is Object Storage? </h2><br>  Well, first of all, it is a storage system specifically designed for the storage of objects.  And this is precisely and above all Storage. <br><br>  Secondly, the system elegantly turns into <i>WORM</i> (write once read many) and back. <br>  Thirdly - separation of pools with data for different users is achieved, flexible setting of quotas for users, pool sizes, number of objects, number of buckets in pools.  In general, normal storage administration functionality. <br>  Fourth, fifth, etc.  Everyone who knows why he needs it, is sure to find more than one advantage.  And maybe disadvantages. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <b>Requirements for the selection of object storage:</b> <br><br>  - OS de facto standard - CentOS; <br>  - you need object storage, reserved between data centers; <br>  - if possible be completely free. <br>  Quite such meager and vague requirements. <br><br>  I started the search process with an overview of the available: <br><br><ul><li>  ScaleIO is cool, simple, but a step to the side is a license, and you want independence.  And on the move did not find the possibility of ensuring the resiliency of the configuration for data centers; </li><li>  OpenIO - apparently a beginner project and I did not find a description of the functionality I needed; </li><li>  Ceph - well, you can try.  It satisfies the initial meager requirements. </li></ul><br>  Having chosen Ceph, I embarked on the path of walking on a rake, of which there were enough, but they were also passed with optimism. <br><br>  <b>The first</b> thing that was done was <b>the choice of version</b> .  Last <b>Luminous</b> did not put, just had a release, but we are a serious company.  :) About <b>Hammer</b> had a lot of complaints.  I liked <b>Jewel</b> in a stable release.  I created a repository, zsinkal it with ceph.com, put the job in kroner and released it through nginx.  We will also need EPEL: <br><br> <code>#   Ceph-Jewel <br> /usr/bin/rsync -avz --delete --exclude='repo*' rsync://download.ceph.com/ceph/rpm-jewel/el7/SRPMS/ /var/www/html/repos/ceph/ceph-jewel/el7/SRPMS/ <br> /usr/bin/rsync -avz --delete --exclude='repo*' rsync://download.ceph.com/ceph/rpm-jewel/el7/noarch/ /var/www/html/repos/ceph/ceph-jewel/el7/noarch/ <br> /usr/bin/rsync -avz --delete --exclude='repo*' rsync://download.ceph.com/ceph/rpm-jewel/el7/x86_64/ /var/www/html/repos/ceph/ceph-jewel/el7/x86_64/ <br> #   EPEL7 <br> /usr/bin/rsync -avz --delete --exclude='repo*' rsync://mirror.yandex.ru/fedora-epel/7/x86_64/ /var/www/html/repos/epel/7/x86_64/ <br> /usr/bin/rsync -avz --delete --exclude='repo*' rsync://mirror.yandex.ru/fedora-epel/7/SRPMS/ /var/www/html/repos/epel/7/SRPMS/ <br> #   Ceph-Jewel <br> /usr/bin/createrepo --update /var/www/html/repos/ceph/ceph-jewel/el7/x86_64/ <br> /usr/bin/createrepo --update /var/www/html/repos/ceph/ceph-jewel/el7/SRPMS/ <br> /usr/bin/createrepo --update /var/www/html/repos/ceph/ceph-jewel/el7/noarch/ <br> #   EPEL7 <br> /usr/bin/createrepo --update /var/www/html/repos/epel/7/x86_64/ <br> /usr/bin/createrepo --update /var/www/html/repos/epel/7/SRPMS/</code> <br> <br>  and proceeded with further planning and installation. <br><br>  <b>First, it is necessary to draw the solution architecture</b> , even if it is a test one, but which can easily be scaled to a sale, to which we will strive.  I got the following: <br><br>  - three OSD nodes in each data center; <br>  - Three MON nodes on three sites (one each), which will provide the majority for the Ceph cluster;  (node ‚Äã‚Äãmonitors can be shared with other roles, but I chose to make them virtual and bring them to VMWare in general) <br>  - two RGW nodes (one in each data center), which provide API access to Object Storage using S3 or Swift protocol;  (RadosGW nodes can be co-ed with other roles, but I chose to make them virtual and also render them on VMWare) <br>  - node for deployment and centralized management;  (virtual server that rolls between data centers in VMWare) <br>  - node monitoring cluster and current / historical performance.  (the same story as with the Deploy node) <br><br>  <b>Plan networks</b> - I used the same network for the Ceph cluster "ecosystem".  For access to the deployment nodes, monitoring and RGW, two networks were forwarded to the nodes: <br><br>  - Ceph cluster network, for access to resources; <br>  - ‚Äúpublic‚Äù network, for access ‚Äúfrom outside‚Äù to these nodes. <br>  Official documentation recommends using different networks within the cluster for heartbeat and data movement between OSD nodes, although the same documentation states that using one network reduces latency ... I chose one network for the entire cluster. <br><br>  Cluster installation begins with a basic procedure: <b>preparing server-nodes with CentOS OS.</b> <br>  - we set up repositories, if they are local.  (for example, like mine) We also need an EPEL repository; <br>  - on all nodes in / etc / hosts we enter information about all nodes of the cluster.  If the infrastructure uses DHCP, then it is better to do bind for the addresses and fill in / etc / hosts anyway; <br>  - we configure ntp and we synchronize time, it is critical, for correct operation of Ceph; <br>  - create a user to manage a Ceph cluster, any name, the main thing is not the same name - ceph. <br><br>  For example: <br><br> <code>sudo useradd -d /home/cephadmin -m cephadmin <br> sudo passwd cephadmin <br> echo "cephadmin ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephadmin <br> chmod 0440 /etc/sudoers.d/cephadmin</code> <br> <br>  - install the ssh server on the deployment node, generate keys for the created user, copy the keys to all the nodes of the cluster, set in sudo with the option NOPASSWD.  We will need to get for the user a passwordless entry from the deployment node to all the nodes of the cluster; <br>  - we create a config file for the created user in the .ssh directory, describe all the nodes and set 600 permissions on this file; <br><br> <code>[cephadmin@ceph-deploy .ssh]$ cat config <br> Host ceph-cod1-osd-n1 <br> Hostname ceph-cod1-osd-n1 <br> User cephadmin <br> ................... <br> Host ceph-cod2-osd-n3 <br> Hostname ceph-cod2-osd-n3 <br> User cephadmin</code> <br> <br>  - open ports 6789 / tcp and 6800-7100 / tcp in firewalld, if you decide to leave it; <br>  - disable SELinux;  (although from the ceph version - Jewel with the installation, normal SE-policies roll in) <br>  - on the cluster management node, perform <i>yum install ceph-deploy</i> . <br><br><h2>  It seems everything is ready!  Go to the installation and configuration of the cluster itself. </h2><br>  In the home directory of our user create a directory where the cluster configs will be located.  It is better not to lose this folder, since  recover will be very problematic.  In the process of configuring the cluster and screwing different roles and services, this directory will be replenished with files and keys. <br><br>  Create the first MON node in our future cluster: <i>ceph-deploy new # our_name_MON_nody</i> .  The <b>ceph.conf</b> file has appeared in the directory created earlier, in which the cluster description will now be entered and its contents will be applied to the nodes we need. <br><br>  We install Ceph-Jewel itself on all nodes: <i>ceph-deploy install --release = jewel --no-adjust-repos # node1 # node2 ... # node_N</i> .  The key <i>--no-adjust-repos</i> must be used if the repository for installation is local and for the installation script to look for the path in the existing /etc/yum.repos.d/*.repo, and not try to register its repository.  With the Jewel version, the stable version is installed by default, unless otherwise indicated. <br><br>  After successful installation, initialize the cluster <i>ceph-deploy mon create-initial</i> <br><br>  When the cluster is initialized, the initial configuration is written to the ceph.conf file, including the <b>fsid</b> .  If this fsid is subsequently changed or lost by the cluster, this will lead to its ‚Äúcollapse‚Äù and, as a result, loss of information!  So, after having the initial configuration in ceph.conf, we boldly open it (by making a backup) and begin to edit and enter the values ‚Äã‚Äãthat we need.  When spilling on the nodes we need, we must specify the <i>--overwrite-conf</i> option.  Well, the approximate content of our config: <br><br> <code>[root@ceph-deploy ceph-cluster]# cat /home/cephadmin/ceph-cluster/ceph.conf <br> [global] <br> fsid = #-_ <br> mon_initial_members = ceph-cod1-mon-n1, ceph-cod1-mon-n2, ceph-cod2-mon-n1 <br> mon_host = ip-adress1,ip-adress2,ip-adress3 <br> auth_cluster_required = cephx <br> auth_service_required = cephx <br> auth_client_required = cephx <br> <br> #Choose reasonable numbers for number of replicas and placement groups. <br> osd pool default size = 2 # Write an object 2 times <br> osd pool default min size = 1 # Allow writing 1 copy in a degraded state <br> osd pool default pg num = 256 <br> osd pool default pgp num = 256 <br> <br> #Choose a reasonable crush leaf type <br> #0 for a 1-node cluster. <br> #1 for a multi node cluster in a single rack <br> #2 for a multi node, multi chassis cluster with multiple hosts in a chassis <br> #3 for a multi node cluster with hosts across racks, etc. <br> osd crush chooseleaf type = 1 <br> <br> [client.rgw.ceph-cod1-rgw-n1] <br> host = ceph-cod1-rgw-n1 <br> keyring = /var/lib/ceph/radosgw/ceph-rgw.ceph-cod1-rgw-n1/keyring <br> rgw socket path = /var/run/ceph/ceph.radosgw.ceph-cod1-rgw-n1.fastcgi.sock <br> log file = /var/log/ceph/client.radosgw.ceph-cod1-rgw-n1.log <br> rgw dns name = ceph-cod1-rgw-n1.**.*****.ru <br> rgw print continue = false <br> rgw frontends = ¬´civetweb port=8888¬ª <br> <br> [client.rgw.ceph-cod2-rgw-n1] <br> host = ceph-cod2-rgw-n1 <br> keyring = /var/lib/ceph/radosgw/ceph-rgw.ceph-cod2-rgw-n1/keyring <br> rgw socket path = /var/run/ceph/ceph.radosgw.ceph-cod2-rgw-n1.fastcgi.sock <br> log file = /var/log/ceph/client.radosgw.ceph-cod2-rgw-n1.log <br> rgw dns name = ceph-cod2-rgw-n1.**.*****.ru <br> rgw print continue = false <br> rgw frontends = ¬´civetweb port=8888¬ª</code> <br> <br>  There are also a couple of comments: <br><br>  - if the ceph-radosgw. * Service does not start, then I had a problem with creating a log file.  I decided this by simply creating the file with my hands and putting the mask 0666 on it; <br>  - you can choose the API provider for the cluster between <u>civetweb, fastcgi and apache</u> , that's what the official dock says: <br><blockquote>  As of firefly (v0.80), Ceph Object Gateway is running on Civetweb (embedded into the ceph-radosgw daemon) instead of Apache and FastCGI.  Using Civetweb to simplify the Gateway installation and configuration. </blockquote><br>  <b>In short</b> , I have set global variables, replication rules and the cluster ‚Äúsurvival‚Äù rule in our file, as well as RGW nodes admitted to the cluster.  As for the parameters osd pool default pg num and osd pool default pgp num, an interesting note was found: <br><blockquote>  And for example a count of 64 total PGs.  I‚Äôm not sure, I‚Äôm not sure what I‚Äôve been  It is a bit like it seems to be expecting between the 20s and 32 ps per OSD.  A value below 20 gives you this error, and a value above 32 gives another error. <br>  So, since there are 9 OSDs, the minimum value would be 9 * 20 = 180, and the maximum value 9 * 32 = 288.  I chose 256 and configured it dinamically. </blockquote><br>  And this one: <br><blockquote>  PG (Placement Groups) is a placement group or logical collection of objects in Ceph that are replicated to the OSD.  One group can save data to several OSD, depending on the level of complexity of the system.  The formula for calculating placement groups for Ceph is as follows: <br><br>  Number of PG = (number of OSD * 100) / number of replicas <br><br>  The result should be rounded to the nearest power of two (for example, according to the formula = 700, after rounding = 512). <br>  PGP (Placement Group for Placement purpose) - placement groups for location purposes.  The quantity must be equal to the total number of placement groups. </blockquote><br>  And on all nodes where there is a key, we change the permissions: sudo chmod + r /etc/ceph/ceph.client.admin.keyring <br><br>  <b>Before the OSD device is entered into the cluster, on OSD nodes it is necessary to perform preparatory work:</b> <br><br>  We split the disk for use as a log.  This should be an SSD (but not necessarily, just a dedicated disk is enough) and be divided into no more than 4 equal partitions, since  Partitions must be primary: <br><br> <code>parted /dev/SSD <br> mkpart journal-1 1 15G <br> mkpart journal-2 15 30G <br> mkpart journal-3 31G 45G <br> mkpart journal-4 45G 60G</code> <br> <br>  And formatted in <b>xfs</b> .  We change the rights to the disks on the OSD nodes, which are intended for magazines and which will be managed by Ceph: <br><br> <code>chown ceph:ceph /dev/sdb1 <br> chown ceph:ceph /dev/sdb2 <br> chown ceph:ceph /dev/sdb3</code> <br> <br>  <b>And be sure to change the GUID</b> for these partitions, so that udev will work correctly and the correct device rights will come after the reboot.  I stepped on this rake when, after reloading the OSD, the node rose, but the services were able to failed.  Since  A properly working udev has assigned its owner and default group as root: root.  As they say, the result exceeded expectations ... To prevent this from happening, we do this: <br><br> <code>sgdisk -t 1:45B0969E-9B03-4F30-B4C6-B4B80CEFF106 /dev/sdb <br> <b>GUID    </b></code> <br> <br>  After with our deployment of the node, we execute the <i>ceph-deploy disk zap</i> and <i>ceph-deploy osd create</i> .  This completes the basic installation of the cluster, and you can see its status with the commands <i>ceph -w</i> and <i>ceph osd tree</i> . <br><br>  <b>But how can we ensure fault tolerance for data centers?</b> <br><br>  As it turned out, Ceph has a very powerful tool - working with <i>crushmap</i> <br>  In this map, you can enter several levels of abstraction and I did it up to the primitive simply - I introduced the concept of rack and in each rack laid out the nodes on the basis of the data center.  From this point on, my data was redistributed in such a way that, recorded in one rack, had an obligatory replica in another rack.  Since  Ceph algorithm considers the storage of two replicas in one "rack" unreliable.  :) That's all, and turning off all the nodes of a single data center really left the data available. <br><br> <code>[cephadmin@ceph-deploy ceph-cluster]$ ceph osd tree <br> ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY <br> -1 1.17200 root default <br> -8 0.58600 rack ceph-cod1 <br> -2 0.19499 host ceph-cod1-osd-n1 <br> 0 0.04900 osd.0 up 1.00000 1.00000 <br> 1 0.04900 osd.1 up 1.00000 1.00000 <br> 2 0.04900 osd.2 up 1.00000 1.00000 <br> 3 0.04900 osd.3 up 1.00000 1.00000 <br> -3 0.19499 host ceph-cod1-osd-n2 <br> 4 0.04900 osd.4 up 1.00000 1.00000 <br> 5 0.04900 osd.5 up 1.00000 1.00000 <br> 6 0.04900 osd.6 up 1.00000 1.00000 <br> 7 0.04900 osd.7 up 1.00000 1.00000 <br> -4 0.19499 host ceph-cod1-osd-n3 <br> 8 0.04900 osd.8 up 1.00000 1.00000 <br> 9 0.04900 osd.9 up 1.00000 1.00000 <br> 10 0.04900 osd.10 up 1.00000 1.00000 <br> 11 0.04900 osd.11 up 1.00000 1.00000 <br> -9 0.58600 rack ceph-cod2 <br> -5 0.19499 host ceph-cod2-osd-n1 <br> 12 0.04900 osd.12 up 1.00000 1.00000 <br> 13 0.04900 osd.13 up 1.00000 1.00000 <br> 14 0.04900 osd.14 up 1.00000 1.00000 <br> 15 0.04900 osd.15 up 1.00000 1.00000 <br> -6 0.19499 host ceph-cod2-osd-n2 <br> 16 0.04900 osd.16 up 1.00000 1.00000 <br> 17 0.04900 osd.17 up 1.00000 1.00000 <br> 18 0.04900 osd.18 up 1.00000 1.00000 <br> 19 0.04900 osd.19 up 1.00000 1.00000 <br> -7 0.19499 host ceph-cod2-osd-n3 <br> 20 0.04900 osd.20 up 1.00000 1.00000 <br> 21 0.04900 osd.21 up 1.00000 1.00000 <br> 22 0.04900 osd.22 up 1.00000 1.00000 <br> 23 0.04900 osd.23 up 1.00000 1.00000</code> <br> <br>  <b>PS What do you want to say?</b> <br><br>  In principle, Ceph met my expectations.  Like object storage.  Of course, there are features of its configuration and the organization of pools for this type of storage is not quite ‚Äúbook‚Äù - like, here is a pool with a date, and this is a pool with metadata.  About this I will try to tell. <br><br>  Also tried to play around with block storage.  Unfortunately, ‚Äúthrowing away‚Äù block devices through FC Ceph does not know how, or I did not find how.  As for iSCSI, for me this is not a true and happy way.  Although it works, even with MPIO. <br><br>  <b>Three sources</b> that seemed most useful to me: <br>  <a href="http://docs.ceph.com/docs/master/">official</a> <br>  <a href="http://onreader.mdl.ru/LearningCeph/content/index.html">informal</a> <br>  <a href="http://rebirther.ru/blog/ceph-spargalka">crib</a> </div><p>Source: <a href="https://habr.com/ru/post/338782/">https://habr.com/ru/post/338782/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../338770/index.html">Stream API & ForkJoinPool</a></li>
<li><a href="../338772/index.html">Mobile applications: what is the 2017 holiday season and how to make the most of it?</a></li>
<li><a href="../338776/index.html">How I participated in a Xiaomi bug bounty and what was it for me</a></li>
<li><a href="../338778/index.html">Text Ergonomics: A user of social networks sees about 54,000 words per day</a></li>
<li><a href="../338780/index.html">[Translation] Around the circle: optical effects when designing interfaces</a></li>
<li><a href="../338784/index.html">Kaspersky Industrial CTF 2017 in Shanghai: Everything at the Cyberstorm Oil Refinery</a></li>
<li><a href="../338786/index.html">Anatomy of distributed business processes: Oracle SOA and BPM</a></li>
<li><a href="../338788/index.html">"Enterprise shame" or how to drive the developer crazy at the interview</a></li>
<li><a href="../338796/index.html">Illusion of movement</a></li>
<li><a href="../338798/index.html">20 useful services for product managers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>