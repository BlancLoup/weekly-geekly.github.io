<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Switch to Percona XtraDB Cluster. One possible configuration</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="So, I started to implement Percona XtraDB Cluster in my organization - transfer databases from a regular MySQL server to a cluster architecture. 


 B...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Switch to Percona XtraDB Cluster. One possible configuration</h1><div class="post__text post__text-html js-mediator-article">  So, I started to implement Percona XtraDB Cluster in my organization - transfer databases from a regular MySQL server to a cluster architecture. <br><br><br><h4>  Briefly about the task and input data </h4><br>  In the cluster we need to keep: <br><ul><li>  DB of several websites with users </li><li>  DB with the statistics of these users </li><li>  BD for ticket systems, project management systems and other trifles </li></ul><br>  In other words, the database of almost all of our projects, of those that run on our MySQL, should now live in a cluster. <br><br>  We keep most of the projects remotely in the DC, so the cluster will be located there. <br>  The task of spreading a cluster geographically across different data centers is not worth it. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To build a cluster, 3 servers of the same configuration are used: HP DL160 G6, 2X Xeon E5620, 24 GB RAM, 4x SAS 300GB in hardware RAID 10. Not bad brand hardware that I have been using for a long time and which has not let me down yet. <br><br><br><h4>  Why Percona? </h4><br>  - synchronous true multi-master replication (Galera) <br>  - the possibility of commercial support from Percona <br>  - fork MySQL with an impressive list of optimizations <br><br><br><h4>  Cluster layout </h4><br>  In a cluster of 3 nodes, for each of the above physical server (OS Ubuntu 12.04). <br><br>  A transparent connection to one virtual IP address is used, shared between all 3 servers using <b>keepalived</b> .  For load balancing on nodes, <b>HAProxy is</b> used, of course, installed on each server, so that in case of failure of one of them, thanks to VIP, it continues to balance the other.  We intentionally decided to use for LB and VIP the same pieces of iron as for the cluster. <br><br>  Node <b>A is</b> used as a Reference (Backup) Node that our applications will not load with requests.  However, she will be a full member of the cluster, and participate in replication.  This is due to the fact that in the event of a cluster failure or data integrity violation, we will have a node that almost certainly contains the most consistent data that applications simply could not destroy due to lack of access.  It may seem like a waste of resources, but for us, 99% data reliability is still more important than 24/7 availability.  This is the node we will use for SST - State Snapshot Transfer - automatic dumping of a new node being added to the cluster or being lifted after a failure.  In addition, Node <b>A</b> is an excellent candidate for the server, from where we will take standard periodic backups. <br><br>  Schematically it can all be depicted like this: <br><br><img src="https://habrastorage.org/storage2/691/48f/e29/69148fe29656ea85bcea067594b6c2b2.png"><br>  Node B and Node C are workhorses that hold the load, but only one of them takes over the write operation.  This is the recommendation of many specialists, and below I will focus on this issue in detail. <br><br><br><h4>  HAProxy and balancing details </h4><br>  Requests coming to port <b>3306</b> , HAProxy scatters on Round Robin between nodes <b>B</b> and <b>C.</b> <br>  What comes to <b>3307</b> is proxied only to <b>Node B.</b>  Moreover, if <b>Node B</b> suddenly falls, requests will be transferred to the specified <b>Node C</b> as the backup <b>.</b> <br><br>  To implement our idea (to write only to one of the nodes), applications must be written so that read requests go through a connection from <b>10.0.0.70reed306</b> (10.0.0.70 is our VIP), and write requests go to <b>10.0.0.70: 3307</b> . <br><br>  In our case, this will require some work on creating a new connection in the PHP config, and replacing the name of the DBHandler variable with another value.  In general, it is not so difficult for those applications that are written by us.  For third-party projects whose databases will also be in a cluster, we simply specify port 3307 by default.  These projects create a small load, and the loss of the possibility of distributed reading is not so critical. <br><br>  HAProxy <b>config</b> ( <b>/etc/haproxy/haproxy.cfg</b> ): <br><pre><code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">global</span></span> <span class="hljs-keyword"><span class="hljs-keyword">log</span></span> <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> local0 <span class="hljs-keyword"><span class="hljs-keyword">log</span></span> <span class="hljs-number"><span class="hljs-number">127.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> local1 <span class="hljs-keyword"><span class="hljs-keyword">notice</span></span> maxconn <span class="hljs-number"><span class="hljs-number">4096</span></span> chroot /usr/<span class="hljs-keyword"><span class="hljs-keyword">share</span></span>/haproxy daemon defaults <span class="hljs-keyword"><span class="hljs-keyword">log</span></span> <span class="hljs-keyword"><span class="hljs-keyword">global</span></span> mode http <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> tcplog <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> dontlognull retries <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> redispatch maxconn <span class="hljs-number"><span class="hljs-number">2000</span></span> contimeout <span class="hljs-number"><span class="hljs-number">5000</span></span> clitimeout <span class="hljs-number"><span class="hljs-number">50000</span></span> srvtimeout <span class="hljs-number"><span class="hljs-number">50000</span></span> frontend pxc-front bind <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.70</span></span>:<span class="hljs-number"><span class="hljs-number">3306</span></span> mode tcp default_backend pxc-back frontend stats-front bind *:<span class="hljs-number"><span class="hljs-number">81</span></span> mode http default_backend stats-back frontend pxc-onenode-front bind <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.70</span></span>:<span class="hljs-number"><span class="hljs-number">3307</span></span> mode tcp default_backend pxc-onenode-back backend pxc-back mode tcp balance leastconn <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> httpchk <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> c1 <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.106</span></span>:<span class="hljs-number"><span class="hljs-number">33061</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">9200</span></span> inter <span class="hljs-number"><span class="hljs-number">12000</span></span> rise <span class="hljs-number"><span class="hljs-number">3</span></span> fall <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> c2 <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.107</span></span>:<span class="hljs-number"><span class="hljs-number">33061</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">9200</span></span> inter <span class="hljs-number"><span class="hljs-number">12000</span></span> rise <span class="hljs-number"><span class="hljs-number">3</span></span> fall <span class="hljs-number"><span class="hljs-number">3</span></span> backend stats-back mode http balance roundrobin stats uri /haproxy/stats stats auth haproxy:<span class="hljs-keyword"><span class="hljs-keyword">password</span></span> backend pxc-onenode-back mode tcp balance leastconn <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> httpchk <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> c1 <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.106</span></span>:<span class="hljs-number"><span class="hljs-number">33061</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">9200</span></span> inter <span class="hljs-number"><span class="hljs-number">12000</span></span> rise <span class="hljs-number"><span class="hljs-number">3</span></span> fall <span class="hljs-number"><span class="hljs-number">3</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> c2 <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.107</span></span>:<span class="hljs-number"><span class="hljs-number">33061</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">9200</span></span> inter <span class="hljs-number"><span class="hljs-number">12000</span></span> rise <span class="hljs-number"><span class="hljs-number">3</span></span> fall <span class="hljs-number"><span class="hljs-number">3</span></span> backup backend pxc-referencenode-back mode tcp balance leastconn <span class="hljs-keyword"><span class="hljs-keyword">option</span></span> httpchk <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> c0 <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.105</span></span>:<span class="hljs-number"><span class="hljs-number">33061</span></span> <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> port <span class="hljs-number"><span class="hljs-number">9200</span></span> inter <span class="hljs-number"><span class="hljs-number">12000</span></span> rise <span class="hljs-number"><span class="hljs-number">3</span></span> fall <span class="hljs-number"><span class="hljs-number">3</span></span></code> </pre> <br><br>  In order for HAProxy to determine if a cluster node is alive, the <b>clustercheck</b> utility (included in the percona-xtradb-cluster package) is used, which displays the status of the node (Synced / Not Synced) as an HTTP response.  Each node must have a xinetd service configured: <br><br>  <b>/etc/xinetd.d/mysqlchk</b> <br><pre> <code class="hljs pgsql">service mysqlchk { <span class="hljs-keyword"><span class="hljs-keyword">disable</span></span> = <span class="hljs-keyword"><span class="hljs-keyword">no</span></span> flags = REUSE socket_type = stream port = <span class="hljs-number"><span class="hljs-number">9200</span></span> wait = <span class="hljs-keyword"><span class="hljs-keyword">no</span></span> <span class="hljs-keyword"><span class="hljs-keyword">user</span></span> = nobody <span class="hljs-keyword"><span class="hljs-keyword">server</span></span> = /usr/bin/clustercheck log_on_failure += USERID only_from = <span class="hljs-number"><span class="hljs-number">0.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span> per_source = UNLIMITED }</code> </pre><br>  <b>/ etc / services</b> <br><pre> <code class="hljs python"><span class="hljs-meta"><span class="hljs-meta">... </span></span><span class="hljs-comment"><span class="hljs-comment"># Local services mysqlchk 9200/tcp # mysqlchk</span></span></code> </pre><br><br>  HAProxy raises the web server and provides a script to view statistics, which is very convenient for visual monitoring of the cluster status. <br>  The URL looks like this: <pre>  http: // VIP: 81 / haproxy / stats </pre>  Port, as well as login and password for Basic authorization are specified in the config. <br><br>  The issue of setting up a cluster with balancing via HAProxy is well discussed here: <a href="http://www.mysqlperformanceblog.com/2012/06/20/percona-xtradb-cluster-reference-architecture-with-haproxy/">www.mysqlperformanceblog.com/2012/06/20/percona-xtradb-cluster-reference-architecture-with-haproxy</a> <br><br><br><h4>  Keepalived and VIP </h4><br><pre> <code class="hljs ruby">$ echo <span class="hljs-string"><span class="hljs-string">"net.ipv4.ip_nonlocal_bind=1"</span></span> <span class="hljs-meta"><span class="hljs-meta">&gt;&gt; </span></span>/etc/sysctl.conf &amp;&amp; sysctl -p</code> </pre><br>  <b>/etc/keepalived/keepalived.conf</b> <br><pre> <code class="hljs pgsql">vrrp_script chk_haproxy { # Requires keepalived<span class="hljs-number"><span class="hljs-number">-1.1</span></span><span class="hljs-number"><span class="hljs-number">.13</span></span> script "killall -0 haproxy" # cheaper than pidof <span class="hljs-type"><span class="hljs-type">interval</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">check</span></span> every <span class="hljs-number"><span class="hljs-number">2</span></span> seconds weight <span class="hljs-number"><span class="hljs-number">2</span></span> # <span class="hljs-keyword"><span class="hljs-keyword">add</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> points <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> prio <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> OK } vrrp_instance VI_1 { interface eth0 state MASTER # SLAVE <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> backup virtual_router_id <span class="hljs-number"><span class="hljs-number">51</span></span> priority <span class="hljs-number"><span class="hljs-number">101</span></span> # <span class="hljs-number"><span class="hljs-number">101</span></span> <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> master, <span class="hljs-number"><span class="hljs-number">100</span></span> <span class="hljs-keyword"><span class="hljs-keyword">and</span></span> <span class="hljs-number"><span class="hljs-number">99</span></span> <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> backup virtual_ipaddress { <span class="hljs-number"><span class="hljs-number">10.0</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.70</span></span> } track_script { chk_haproxy } }</code> </pre><br><br><br><h4>  Node configuration </h4><br>  On Habr√© already have an article about installing and testing PXC: <a href="http://habrahabr.ru/post/152969/">habrahabr.ru/post/152969</a> , where both issues are discussed in detail, so I omit the installation.  But I will describe the configuration. <br><br>  First of all, do not forget to synchronize the time on all nodes.  I missed this moment, and for a long time I could not understand why my SST hangs tightly - it started, hung in the processes, but in fact nothing happened. <br><br>  <b>my.cnf</b> on <b>Node A</b> (in my configs is <i>node105</i> ): <br><pre> <code class="hljs ruby">[mysqld_safe] wsrep_urls=<span class="hljs-symbol"><span class="hljs-symbol">gcomm:</span></span>/<span class="hljs-regexp"><span class="hljs-regexp">/10.0.0.106:4567,gcomm:/</span></span><span class="hljs-regexp"><span class="hljs-regexp">/10.0.0.107:4567 # wsrep_urls=gcomm:/</span></span><span class="hljs-regexp"><span class="hljs-regexp">/10.0.0.106:4567,gcomm:/</span></span><span class="hljs-regexp"><span class="hljs-regexp">/10.0.0.107:4567,gcomm:/</span></span><span class="hljs-regexp"><span class="hljs-regexp">/ #   - ,    #   , ..     #   ,      [mysqld] port=33061 bind-address=10.0.0.105 datadir=/var</span></span><span class="hljs-regexp"><span class="hljs-regexp">/lib/mysql</span></span> skip-name-resolve log_error=<span class="hljs-regexp"><span class="hljs-regexp">/var/log</span></span><span class="hljs-regexp"><span class="hljs-regexp">/mysql/error</span></span>.log binlog_format=ROW wsrep_provider=<span class="hljs-regexp"><span class="hljs-regexp">/usr/lib</span></span><span class="hljs-regexp"><span class="hljs-regexp">/libgalera_smm.so wsrep_slave_threads=16 wsrep_cluster_name=cluster0 wsrep_node_name=node105 wsrep_sst_method=xtrabackup wsrep_sst_auth=backup:password innodb_locks_unsafe_for_binlog=1 innodb_autoinc_lock_mode=2 innodb_buffer_pool_size=8G innodb_log_file_size=128M innodb_log_buffer_size=4M innodb-file-per-table</span></span></code> </pre><br><br>  Further, only different parameters: <br><br>  Node <b>B</b> (node106) <br><pre> <code class="hljs cs">[<span class="hljs-meta"><span class="hljs-meta">mysqld_safe</span></span>] wsrep_urls=gcomm:<span class="hljs-comment"><span class="hljs-comment">//10.0.0.105:4567 [mysqld] bind-address=10.0.0.106 wsrep_node_name=node106 wsrep_sst_donor=node105</span></span></code> </pre><br><br>  Node <b>C</b> (node107) <br><pre> <code class="hljs cs">[<span class="hljs-meta"><span class="hljs-meta">mysqld_safe</span></span>] wsrep_urls=gcomm:<span class="hljs-comment"><span class="hljs-comment">//10.0.0.105:4567 [mysqld] bind-address=10.0.0.107 wsrep_node_name=node107 wsrep_sst_donor=node105</span></span></code> </pre><br><br>  In the last two configs, we unequivocally tell the server where to look for the first node in the cluster (which knows where all the members of the group live), and what exactly from it, and not from the other available, you need to take data for synchronization. <br><br>  It is on this configuration that I stopped now, and I am going to gradually transfer projects to the cluster.  I plan to continue writing about my experience further. <br><br><br><h4>  Problematic issues </h4><br>  I‚Äôll mark here the questions to which I didn‚Äôt immediately find the answer, but the answer to which is especially important for understanding the technology and proper work with the cluster. <br><br><h6>  <b>Why is it recommended to write on one node of all available in the cluster?</b>  <b>After all, it would seem that this is contrary to the idea of ‚Äã‚Äãmulti-master replication.</b> </h6><br><blockquote>  When I first saw this recommendation, I was very upset.  I imagined a multi-master in such a way that you can write about any node without worrying about anything, and the changes are guaranteed to be applied synchronously on all nodes.  But the harsh realities of life are such that with this approach cluster-wide deadlocks may be possible.  The probability is especially great in case of a parallel change of the same data in long transactions.  Since  I am not yet an expert in this matter, I can not explain this process on the fingers.  But there is a good article where this problem is covered in the most detailed way: <a href="http://www.mysqlperformanceblog.com/2012/08/17/percona-xtradb-cluster-multi-node-writing-and-unexpected-deadlocks/">Percona XtraDB Cluster: Multi-node writing and Unexpected deadlocks</a> <br><br>  My own tests showed that with aggressive recording on all the nodes they went one after another, leaving only the Reference Node working, i.e.  in fact, we can say that the cluster stopped working.  This is certainly a minus of such a configuration, because the third node could in this case take the load on itself, but we are sure that the data is safe and, in the most extreme case, we can manually start it in the single server mode. <br></blockquote><br><br><h6>  <b>How to correctly specify the IP addresses of the nodes existing in the cluster when connecting a new one?</b> </h6><br><blockquote>  There are 2 directives for this: <br><pre> <code class="hljs css"><span class="hljs-selector-attr"><span class="hljs-selector-attr">[mysqld_safe]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">wsrep_urls</span></span> <span class="hljs-selector-attr"><span class="hljs-selector-attr">[mysqld]</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">wsrep_cluster_address</span></span></code> </pre><br>  The first, if I understood correctly, was added Galera relatively recently to be able to specify several node addresses at once.  There are no more fundamental differences. <br><br>  The values ‚Äã‚Äãof these directives at first caused me a special confusion. <br>  The fact is that many manuals advised to leave an empty <b>gcomm: //</b> value in <b>wsrep_urls</b> on the first node of the cluster. <br>  It turned out that it is wrong.  Having <b>gcomm: //</b> means initializing a new cluster.  Therefore, immediately after the start of the first node in its config, you need to delete this value.  Otherwise, after restarting this node, you will receive two different clusters, one of which will consist only of the first node. <br><br>  For myself, I derived the configuration order at startup and restart of the cluster (already described above in more detail) <br>  1. Node A: start with <b>gcomm: // B, gcomm: // C, gcomm: //</b> <br>  2. Node A: deleting <b>gcomm: //</b> at the end of the line <br>  3. Nodes B, C: run with <b>gcomm: // A</b> <br><br>  NB: it is necessary to specify the port number for Group Communication requests, the default is 4567. That is, the correct entry: <b>gcomm: // A: 4567</b> </blockquote><br><br><h6>  <b>Can I write to the donor node with a non-blocking xtrabackup as an SST method?</b> </h6><br><blockquote>  During SST, a <b>clustercheck</b> on the donor will issue HTTP 503, respectively for HAProxy or another LB that uses this utility to determine status, the donor node will be considered inaccessible, as well as the node to which the transfer is made.  But this behavior can be changed by editing <b>clustercheck</b> , which is essentially a regular bash script. <br>  This is done by the following edit: <br><br>  <b>/ usr / bin / clustercheck</b> <br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#AVAILABLE_WHEN_DONOR=0 AVAILABLE_WHEN_DONOR=1</span></span></code> </pre><br>  NB: note that you can only do this if you are sure that <b>xtrabackup is</b> used as SST, and not some other method.  In our case, when we use a donor without a load, such editing does not make sense at all. </blockquote><br><br><br><h4>  useful links </h4><br>  <a href="http://www.percona.com/software/percona-xtradb-cluster">Percona XtraDB Cluster</a> <br>  <a href="http://www.mysqlperformanceblog.com/category/xtradb-cluster/">XtraDB Cluster on mysqlperfomanceblog.com</a> <br>  <a href="http://forum.percona.com/index.php%3Ft%3Dthread%26frm_id%3D13">Percona Community Forums</a> <br>  <a href="https://groups.google.com/forum/%3Ffromgroups%3D">Percona Discussion Google group</a> <br>  <a href="http://www.codership.com/wiki/doku.php">Galera wiki</a> </div><p>Source: <a href="https://habr.com/ru/post/158377/">https://habr.com/ru/post/158377/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../158357/index.html">How to connect the online store to the payment system? The history of one company</a></li>
<li><a href="../158359/index.html">Will the merged letter die?</a></li>
<li><a href="../158363/index.html">New Chromebook for $ 199 - Acer C7</a></li>
<li><a href="../158371/index.html">The command line of a Linux photographer is retired!</a></li>
<li><a href="../158375/index.html">Roskomnadzor registry. More questions than answers</a></li>
<li><a href="../158379/index.html">Key people on the #AndroidDev tag</a></li>
<li><a href="../158381/index.html">Interview with CYBERMANIAC</a></li>
<li><a href="../158383/index.html">How ninja prototypes did. Ninjamock.com - interface designer</a></li>
<li><a href="../158385/index.html">Python - tail recursion optimization</a></li>
<li><a href="../158389/index.html">Cray Titan - the most powerful supercomputer of our time</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>