<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Building a Failover Solution Based on Oracle RAC and AccelStor Shared-Nothing Architecture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A considerable number of Enterprise applications and virtualization systems have their own mechanisms for building fault-tolerant solutions. In partic...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Building a Failover Solution Based on Oracle RAC and AccelStor Shared-Nothing Architecture</h1><div class="post__text post__text-html js-mediator-article"><p>  A considerable number of Enterprise applications and virtualization systems have their own mechanisms for building fault-tolerant solutions.  In particular, Oracle RAC (Oracle Real Application Cluster) is a cluster of two or more Oracle database servers that work together to load balance and provide fault tolerance at the server / application level.  To work in this mode, you need a shared storage, which is usually in the role of storage. </p><br><p>  As we have already considered in one of our <a href="https://habr.com/ru/company/accelstor/blog/441780/">articles</a> , the data storage system itself, despite the presence of duplicate components (including controllers), still has points of failure - mainly as a single data set.  Therefore, to build an Oracle solution with increased reliability requirements, the ‚ÄúN servers - one storage system‚Äù scheme needs to be complicated. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/l3/lh/1t/l3lh1tmhqobqcqow6yxo-g3j8lc.png"></div><br><a name="habracut"></a><br><p>  First, of course, you need to decide on what risks we are trying to insure.  In this article, we will not consider protection against threats like ‚Äúmeteorite has flown in‚Äù.  So the construction of a geographically dispersed disaster recovery solution will remain a topic for one of the following articles.  Here we look at the so-called Cross-Rack disaster recovery solution, when protection is built at the level of server cabinets.  The cabinets themselves can be located in the same room or in different rooms, but usually within the same building. </p><br><p>  These cabinets should contain all the necessary set of equipment and software that will ensure the operation of Oracle databases, regardless of the state of the ‚Äúneighbor‚Äù.  In other words, using the Cross-Rack disaster recovery solution, we eliminate the risks of failure: </p><br><ul><li>  Oracle Application Servers </li><li>  Storage systems </li><li>  Switching systems </li><li>  Complete failure of all equipment in the cabinet: <br><ul><li>  Failure to eat </li><li>  Cooling system failure </li><li>  External factors (man, nature, etc.) </li></ul></li></ul><br><p>  The duplication of Oracle servers implies the very principle of operation of Oracle RAC and is implemented through an application.  Duplication of switching facilities is also not a problem.  But with the duplication of storage, everything is not so simple. </p><br><p>  The simplest option is to replicate data from the main storage system to the backup one.  Synchronous or asynchronous, depending on the storage capabilities.  With asynchronous replication, the question immediately arises of ensuring the consistency of data with respect to Oracle.  But even if there is software integration with the application, in any case, in case of a crash on the main storage system, manual intervention of administrators will be required in order to switch the cluster to the backup storage. </p><br><p>  A more complex option is software and / or hardware ‚Äúvirtualizers‚Äù of storage systems that will eliminate consistency problems and manual intervention.  But the complexity of the deployment and subsequent administration, as well as the very indecent cost of such solutions, discourages many. </p><br><p>  Just for scenarios like Cross-Rack disaster recovery, the All Flash AccelStor NeoSapphire ‚Ñ¢ <a href="https://accelstor.ru/product/neosapphire-h710">H710</a> array solution using Shared-Nothing is a great solution.  This model is a two-bin storage system that uses its own FlexiRemap¬Æ technology for working with flash drives.  Thanks to <a href="https://accelstor.ru/">FlexiRemap¬Æ</a> NeoSapphire ‚Ñ¢, the H710 is capable of delivering performance up to 600K IOPS @ 4K random write and 1M + IOPS @ 4K random read, which is unattainable when using classic RAID-based storage. </p><br><p>  But the main feature of the NeoSapphire ‚Ñ¢ H710 is the execution of two nodes in the form of separate cases, each of which has its own copy of the data.  Synchronization of nodes is carried out through the InfiniBand external interface.  Thanks to this architecture, it is possible to spread nodes on different locations up to 100m away, thus providing the Cross-Rack disaster recovery solution.  Both nodes work completely in synchronous mode.  From the side of the H710 hosts looks like an ordinary dual-controller storage system.  Therefore, no additional software and hardware options and especially complex settings are required. </p><br><p>  If we compare all the above-described Cross-Rack disaster recovery solutions, then the AccelStor variant stands out noticeably from the rest: </p><br><table><tbody><tr><th></th><th>  AccelStor NeoSapphire ‚Ñ¢ Shared Nothing Architecture </th><th>  Software or hardware "virtualizer" storage </th><th>  Replication Based Solution </th></tr><tr><td colspan="4">  <b>Availability</b> </td></tr><tr><td>  Server failure </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td></tr><tr><td>  Switch failure </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td></tr><tr><td>  Storage failure </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td><td>  <font color="green"><b>Downtime</b></font> </td></tr><tr><td>  Failure of the entire cabinet </td><td>  <b>No downtime</b> </td><td>  <b>No downtime</b> </td><td>  <font color="green"><b>Downtime</b></font> </td></tr><tr><td colspan="4">  <b>Cost and complexity</b> </td></tr><tr><td>  Cost of solution </td><td>  Low * </td><td>  <font color="green">High</font> </td><td>  <font color="green">High</font> </td></tr><tr><td>  Deployment complexity </td><td>  Low </td><td>  <font color="green">High</font> </td><td>  <font color="green">High</font> </td></tr></tbody></table><br><p>  <i>* AccelStor NeoSapphire ‚Ñ¢ is still the All Flash array, which, by definition, does not cost ‚Äú3 kopecks,‚Äù especially since it has a double reserve of capacity.</i>  <i>However, comparing the final cost of the solution based on it with those of other vendors, the cost can be considered low.</i> </p><br><p>  The topology of the connection of the application servers and nodes of the All Flash array will be as follows: </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/5i/gb/cw/5igbcwdvm92bpabsylae4iysdra.png"></div><br><p>  When planning the topology, it is also highly recommended to make duplication of management switches and interconnect servers. </p><br><p>  Hereinafter we will talk about connecting through Fiber Channel.  If iSCSI is used, everything will be the same, adjusted for the types of switches used and slightly different array settings. </p><br><h3>  Preparatory work on the array </h3><br><div class="spoiler">  <b class="spoiler_title">Used hardware and software</b> <div class="spoiler_text"><p>  <b>Server and Switch Specifications</b> </p><br><table><tbody><tr><th>  Components </th><th>  Description </th></tr><tr><td>  Oracle Database 11g servers </td><td>  Two </td></tr><tr><td>  Server operating system </td><td>  Oracle Linux </td></tr><tr><td>  Oracle database version </td><td>  11g (RAC) </td></tr><tr><td>  Processors per server </td><td>  Two 16 cores Intel¬Æ Xeon¬Æ CPU E5-2667 v2 @ 3.30GHz </td></tr><tr><td>  Physical memory per server </td><td>  128GB </td></tr><tr><td>  FC network </td><td>  16Gb / s FC with multipathing </td></tr><tr><td>  FC HBA </td><td>  Emulex Lpe-16002B </td></tr><tr><td>  Dedicated public 1GbE ports for cluster management </td><td>  Intel ethernet adapter RJ45 </td></tr><tr><td>  16Gb / s FC switch </td><td>  Brocade 6505 </td></tr><tr><td>  Dedicated private 10GbE ports for data synchonization </td><td>  Intel X520 </td></tr></tbody></table><br><p>  <b>AccelStor NeoSapphhire ‚Ñ¢ All Flash Array Specification</b> </p><br><table><tbody><tr><th>  Components </th><th>  Description </th></tr><tr><td>  Storage system </td><td>  NeoSapphire ‚Ñ¢ high availability model: H710 </td></tr><tr><td>  Image version </td><td>  4.0.1 </td></tr><tr><td>  Total number of drives </td><td>  48 </td></tr><tr><td>  Drive size </td><td>  1.92TB </td></tr><tr><td>  Drive type </td><td>  SSD </td></tr><tr><td>  FC target ports </td><td>  16x 16Gb ports (8 per node) </td></tr><tr><td>  Management ports </td><td>  The 1GbE ethernet cable </td></tr><tr><td>  Heartbeat port </td><td>  The 1GbE ethernet cable connecting between two storage node </td></tr><tr><td>  Data synchronization port </td><td>  56Gb / s InfiniBand cable </td></tr></tbody></table><br></div></div><br><p>  Before you start using an array, you must initialize it.  By default, the control address of both nodes is the same (192.168.1.1).  You need to connect to them one by one and set up new (already different) management addresses and set up time synchronization, after which Management ports can be connected to a single network.  After that, nodes are combined in HA pair by assigning subnets for Interlink connections. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/c3/kw/sp/c3kwspwqm38yl7ennakzykubswq.jpeg"></div><br><p>  After the initialization is complete, you can manage the array from any node. </p><br><p>  Next, create the necessary volumes and publish them for application servers. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/6h/4s/gf/6h4sgfinagrbbngoououu-lg1au.png"></div><br><p>  It is highly recommended to create multiple volumes for Oracle ASM, as this will increase the number of target servers, which ultimately improves overall performance (for more information, see the queues in another <a href="https://habr.com/ru/company/accelstor/blog/447390/">article</a> ). </p><br><div class="spoiler">  <b class="spoiler_title">Test configuration</b> <div class="spoiler_text"><table><tbody><tr><th>  Storage Volume Name </th><th>  Volume Size </th></tr><tr><td>  Data01 </td><td>  200GB </td></tr><tr><td>  Data02 </td><td>  200GB </td></tr><tr><td>  Data03 </td><td>  200GB </td></tr><tr><td>  Data04 </td><td>  200GB </td></tr><tr><td>  Data05 </td><td>  200GB </td></tr><tr><td>  Data06 </td><td>  200GB </td></tr><tr><td>  Data07 </td><td>  200GB </td></tr><tr><td>  Data08 </td><td>  200GB </td></tr><tr><td>  Data09 </td><td>  200GB </td></tr><tr><td>  Data10 </td><td>  200GB </td></tr><tr><td>  Gridd01 </td><td>  1GB </td></tr><tr><td>  Grid02 </td><td>  1GB </td></tr><tr><td>  Grid03 </td><td>  1GB </td></tr><tr><td>  Grid4d04 </td><td>  1GB </td></tr><tr><td>  Gridrid05 </td><td>  1GB </td></tr><tr><td>  Grid06 </td><td>  1GB </td></tr><tr><td>  Redo01 </td><td>  100GB </td></tr><tr><td>  Redo02 </td><td>  100GB </td></tr><tr><td>  Redo03 </td><td>  100GB </td></tr><tr><td>  Redo04 </td><td>  100GB </td></tr><tr><td>  Redo05 </td><td>  100GB </td></tr><tr><td>  Redo06 </td><td>  100GB </td></tr><tr><td>  Redo07 </td><td>  100GB </td></tr><tr><td>  Redo08 </td><td>  100GB </td></tr><tr><td>  Redo09 </td><td>  100GB </td></tr><tr><td>  Redo10 </td><td>  100GB </td></tr></tbody></table><br></div></div><br><h3>  Some explanations about the modes of the array and the ongoing processes in emergency situations </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/5m/_k/uu/5m_kuurlb-2fy52d8p1tzufcyic.png"></div><br><p>  The data set of each node has a ‚Äúversion number‚Äù parameter.  After the initial initialization, it is the same and equal to 1. If for some reason the version number is different, then data is always synchronized from the older version to the younger version, after which the younger version is aligned, i.e.  This means that the copies are identical.  The reasons for which versions may be different: </p><br><ul><li>  Scheduled restart of one of the nodes </li><li>  An accident at one of the nodes due to a sudden shutdown (power, overheating, etc.). </li><li>  Open InfiniBand connection with inability to synchronize </li><li>  An accident on one of the nodes due to data corruption.  Here you will need to create a new HA group and complete synchronization of the data set. </li></ul><br><p>  In any case, the node that remains online increases its version number by one in order to synchronize its data set after restoring communication with the pair. </p><br><p>  If there is a broken connection via an Ethernet link, the Heartbeat temporarily switches to InfiniBand and returns back within 10s when it is restored. </p><br><h3>  Host configuration </h3><br><p>  To ensure fault tolerance and increase performance, you must enable MPIO support for the array.  To do this, add lines to the /etc/multipath.conf file, then restart the multipath service </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text">  devices { <br>  device { <br>  vendor "AStor" <br>  path_grouping_policy "group_by_prio" <br>  path_selector "queue-length 0" <br>  path_checker "tur" <br>  features "0" <br>  hardware_handler "0" <br>  prio "const" <br>  failback immediate <br>  fast_io_fail_tmo 5 <br>  dev_loss_tmo 60 <br>  user_friendly_names yes <br>  detect_prio yes <br>  rr_min_io_rq 1 <br>  no_path_retry 0 <br>  } <br>  } <br><br></div></div><br><p>  Next, in order for ASM to work with MPIO via ASMLib, you must change the file / etc / sysconfig / oracleasm and then run /etc/init.d/oracleasm scandisks </p><br><div class="spoiler">  <b class="spoiler_title">Hidden text</b> <div class="spoiler_text"><p>  # ORACLEASM_SCANORDER: Matching patterns to order disk scanning <br>  ORACLEASM_SCANORDER = "dm" </p><br><br><p>  # ORACLEASM_SCANEXCLUDE: Matching patterns to exclude disks from scan <br>  ORACLEASM_SCANEXCLUDE = "sd" </p><br><p></p><h4>  Note </h4><br><p>  <i>If you do not want to use ASMLib, you can use UDEV rules, which are the basis for ASMLib.</i> </p><br><p>  <i>Starting with version 12.1.0.2 of Oracle Database, the option is available for installation as part of ASMFD software.</i> </p><br></div></div><br><p>  Be sure to ensure that the disks created for Oracle ASM are aligned with the block size that the array (4K) physically works with.  Otherwise, there may be performance problems.  Therefore, it is necessary to create volumes with the appropriate parameters: </p><br><p>  <i>parted / dev / mapper / device-name mklabel gpt mkpart primary 2048s 100% align-check optimal 1</i> </p><br><h3>  Distributing databases to created volumes for our test configuration </h3><br><table><tbody><tr><th>  Storage Volume Name </th><th>  Volume Size </th><th>  Volume LUNs mapping </th><th>  ASM Volume Device Detail </th><th>  Allocation Unit Size </th></tr><tr><td>  Data01 </td><td>  200GB </td><td rowspan="26">  Map All Data Ports </td><td rowspan="10">  Redundancy: Normal <br>  Name: DGDATA <br>  Purpose: Data files <br></td><td rowspan="10">  4MB </td></tr><tr><td>  Data02 </td><td>  200GB </td></tr><tr><td>  Data03 </td><td>  200GB </td></tr><tr><td>  Data04 </td><td>  200GB </td></tr><tr><td>  Data05 </td><td>  200GB </td></tr><tr><td>  Data06 </td><td>  200GB </td></tr><tr><td>  Data07 </td><td>  200GB </td></tr><tr><td>  Data08 </td><td>  200GB </td></tr><tr><td>  Data09 </td><td>  200GB </td></tr><tr><td>  Data10 </td><td>  200GB </td></tr><tr><td>  <font color="#248dee">Gridd01</font> </td><td>  <font color="#248dee">1GB</font> </td><td rowspan="3">  <font color="#248dee">Redundancy: Normal</font> <font color="#248dee"><br></font>  <font color="#248dee">Name: DGGRID1</font> <font color="#248dee"><br></font>  <font color="#248dee">Purpose: Grid: CRS and Voting</font> <br></td><td rowspan="3">  <font color="#248dee">4MB</font> </td></tr><tr><td>  <font color="#248dee">Grid02</font> </td><td>  <font color="#248dee">1GB</font> </td></tr><tr><td>  <font color="#248dee">Grid03</font> </td><td>  <font color="#248dee">1GB</font> </td></tr><tr><td>  Grid4d04 </td><td>  1GB </td><td rowspan="3">  Redundancy: Normal <br>  Name: DGGRID2 <br>  Purpose: Grid: CRS and Voting <br></td><td rowspan="3">  4MB </td></tr><tr><td>  Gridrid05 </td><td>  1GB </td></tr><tr><td>  Grid06 </td><td>  1GB </td></tr><tr><td>  <font color="#248dee">Redo01</font> </td><td>  <font color="#248dee">100GB</font> </td><td rowspan="5">  <font color="#248dee">Redundancy: Normal</font> <font color="#248dee"><br></font>  <font color="#248dee">Name: DGREDO1</font> <font color="#248dee"><br></font>  <font color="#248dee">Purpose: Redo log of thread 1</font> <font color="#248dee"><br></font> <br></td><td rowspan="5">  <font color="#248dee">4MB</font> </td></tr><tr><td>  <font color="#248dee">Redo02</font> </td><td>  <font color="#248dee">100GB</font> </td></tr><tr><td>  <font color="#248dee">Redo03</font> </td><td>  <font color="#248dee">100GB</font> </td></tr><tr><td>  <font color="#248dee">Redo04</font> </td><td>  <font color="#248dee">100GB</font> </td></tr><tr><td>  <font color="#248dee">Redo05</font> </td><td>  <font color="#248dee">100GB</font> </td></tr><tr><td>  Redo06 </td><td>  100GB </td><td rowspan="5">  Redundancy: Normal <br>  Name: DGREDO2 <br>  Purpose: Redo log of thread 2 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    </td><td rowspan="5">  4MB </td></tr><tr><td>  Redo07 </td><td>  100GB </td></tr><tr><td>  Redo08 </td><td>  100GB </td></tr><tr><td>  Redo09 </td><td>  100GB </td></tr><tr><td>  Redo10 </td><td>  100GB </td></tr></tbody></table><br><div class="spoiler">  <b class="spoiler_title">Database Settings</b> <div class="spoiler_text"><ul><li>  Block size = 8K </li><li>  Swap space = 16GB </li><li> Disable AMM (Automatic Memory Management) </li><li>  Disable Transparent Huge Pages </li></ul><br></div></div><br><div class="spoiler">  <b class="spoiler_title">Other settings</b> <div class="spoiler_text"><p>  <u># vi /etc/sysctl.conf</u> <br>  ‚úì fs.aio-max-nr = 1048576 <br>  ‚úì fs.file-max = 6815744 <br>  ‚úì kernel.shmmax 103079215104 <br>  ‚úì kernel.shmall 31457280 <br>  ‚úì kernel.shmmn 4096 <br>  ‚úì kernel.sem = 250 32000 100 128 <br>  ‚úì net.ipv4.ip_local_port_range = 9000 65500 <br>  ‚úì net.core.rmem_default = 262144 <br>  ‚úì net.core.rmem_max = 4194304 <br>  ‚úì net.core.wmem_default = 262144 <br>  ‚úì net.core.wmem_max = 1048586 <br>  ‚úì vm.swappiness = 10 <br>  ‚úì vm.min_free_kbytes = 524288 # don't set this if you are using Linux x86 <br>  ‚úì vm.vfs_cache_pressure = 200 <br>  ‚úì vm.nr_hugepages = 57000 </p><br><br><p>  <u># vi /etc/security/limits.conf</u> <br>  ‚úì grid soft nproc 2047 <br>  ‚úì grid hard nproc 16384 <br>  ‚úì grid soft nofile 1024 <br>  ‚úì grid hard nofile 65536 <br>  ‚úì grid soft stack 10240 <br>  ‚úì grid hard stack 32768 <br>  ‚úì oracle soft nproc 2047 <br>  ‚úì oracle hard nproc 16384 <br>  ‚úì oracle soft nofile 1024 <br>  ‚úì oracle hard nofile 65536 <br>  ‚úì oracle soft stack 10240 <br>  ‚úì oracle hard stack 32768 <br>  ‚úì soft memlock 120795954 <br>  ‚úì hard memlock 120795954 <br></p><br><p>  <u>sqlplus ‚Äú/ as sysdba‚Äù</u> <br>  alter system set processes = 2000 scope = spfile; <br>  alter system set open_cursors = 2000 scope = spfile; <br>  alter system set session_cached_cursors = 300 scope = spfile; <br>  alter system set db_files = 8192 scope = spfile; <br></p><br><br></div></div><br><h3>  Failover test </h3><br><p>  For demonstration purposes, HammerDB was used to emulate the OLTP load.  HammerDB configuration: </p><br><table><tbody><tr><td>  <b>Number of Warehouses</b> </td><td>  256 </td></tr><tr><td>  Total Transactions per User </td><td>  1000000000000 </td></tr><tr><td>  Virtual Users </td><td>  256 </td></tr></tbody></table><br><p>  As a result, the indicator 2.1M TPM was obtained, which is far from the <a href="https://accelstor.ru/product/neosapphire-h710">H710</a> array performance limit, but is the ‚Äúceiling‚Äù for the current hardware configuration of servers (primarily due to processors) and their number.  The goal of this test is to demonstrate the resiliency of the solution as a whole, and not to achieve maximum performance.  Therefore, we will simply build on this figure. </p><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/3o/ds/id/3odsid2wr0ynssl4zuu8idq8dfq.png"></div><br><h3>  Test for failure of one of the nodes </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/ar/tf/og/artfogfilgwzy2vtxlsyvkp98vu.png"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/zk/iv/xt/zkivxtazysrew4hhzjan-vujnma.png"></div><br><p>  Hosts have lost some of the paths to the repository, continuing to work through the rest of the second node.  Productivity slipped for a few seconds due to the restructuring of the paths, and then returned to normal.  There was no interruption in service. </p><br><h3>  Test for cabinet failure with all equipment </h3><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/mq/my/2b/mqmy2bzrge24zhnj217spj2rts4.png"></div><br><p></p><div style="text-align:center;"><img src="https://habrastorage.org/webt/eq/nh/-8/eqnh-8pwbhyzqgdleh1tsiafkr0.png"></div><br><p>  In this case, the performance also slipped for a few seconds due to the restructuring of the paths, and then returned to half the value of the original indicator.  The result was halved from the initial due to the exclusion of one application server.  There was no interruption in service either. </p><br><blockquote>  If there are needs for implementing a Cross-Rack disaster recovery solution for Oracle at a reasonable cost and with little deployment / administration efforts, then working together with Oracle RAC and <a href="https://accelstor.ru/page/pochemu-accelstor">AccelStor Shared-Nothing</a> architecture will be one of the best options.  Instead of Oracle RAC there can be any other software providing clustering, the same DBMS or virtualization systems, for example.  The principle of building a solution will remain the same.  And the total is a zero value for the RTO and RPO. </blockquote></div><p>Source: <a href="https://habr.com/ru/post/448538/">https://habr.com/ru/post/448538/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448524/index.html">Israeli scientists for the first time in the world printed a living heart</a></li>
<li><a href="../448528/index.html">Free Wireguard VPN service on AWS</a></li>
<li><a href="../448530/index.html">How Megaphone burned on mobile subscriptions</a></li>
<li><a href="../448532/index.html">Space data center. Summing up the experiment</a></li>
<li><a href="../448534/index.html">Why do we need industrial switches with improved EMC?</a></li>
<li><a href="../448540/index.html">VMware NSX for the smallest. Part 5. Configuring a Load Balancer</a></li>
<li><a href="../448542/index.html">String sorting algorithm</a></li>
<li><a href="../448546/index.html">UITableView automatic header and footer sizes with AutoLayout</a></li>
<li><a href="../448550/index.html">The competition of reports on #PAYMENTSECURITY 2019 is open</a></li>
<li><a href="../448552/index.html">ProLiant 100th series - "lost younger brother"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>