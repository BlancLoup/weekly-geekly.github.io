<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AI, practical course. The basic model of recognition of emotions in images</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article, we will build a basic model of a convolutional neural network that is capable of recognizing emotions in images. Recognition of emoti...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AI, practical course. The basic model of recognition of emotions in images</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/kp/it/ob/kpitobaccifc3jg-td-z6lgmz9i.jpeg"><br><br>  In this article, we will build a basic model of a convolutional neural network that is capable of <i>recognizing emotions</i> in images.  Recognition of emotions in our case is a binary classification problem, the goal of which is to divide images into positive and negative. <br><br>  All code, documents in notebook format and other materials, including the Dockerfile, can be found <a href="https://www.dropbox.com/sh/wxg73c4wfmzuaer/AADx_2qeOUKCjk7r85R7jSwba%3Fdl%3D0">here</a> . <br><a name="habracut"></a><br><h2>  <font color="#0071c5">Data</font> </h2><br>  The first step in almost all machine learning tasks is to understand the data.  Let's do that. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  <font color="#0071c5">Dataset structure</font> </h3><br>  Raw data can be downloaded <a href="">here</a> (in the <i>Baseline.ipynb</i> document, all actions in this section are performed automatically).  The original data is in the archive format Zip *.  Unpack it and examine the structure of the received files. <br><br><img src="https://habrastorage.org/webt/wm/wj/ne/wmwjne07sdpbzoitoa0xxz1zcie.png"><br><br>  All images are stored inside the ‚Äúdataset 50:50‚Äù directory and are distributed between its two subdirectories, whose name corresponds to their class ‚Äî Negative and Positive.  Note that the task is slightly <i>unbalanced</i> - 53 percent of the images are positive, and only 47 percent are negative.  Typically, the data in classification tasks are considered unbalanced if the number of examples in different classes varies greatly.  There are a <a href="http://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html">number of ways to</a> work with unbalanced data - for example, downsampling, resampling, changing data weights, etc. In our case, the imbalance is insignificant and should not drastically affect the learning process.  It is only necessary to remember that the naive classifier, which always gives out a positive value, will provide an accuracy value of about 53 percent for this data set. <br><br>  Let's look at several images of each class. <br><br>  <b>Negative</b> <br><br><img src="https://habrastorage.org/webt/q4/5d/fc/q45dfcprljvv5tnm0aqvwwgs0uy.jpeg"><br><br><img src="https://habrastorage.org/webt/ep/0p/4q/ep0p4qkimflvz7euzaam1bc39a8.jpeg"><br><br><img src="https://habrastorage.org/webt/dj/ep/px/djeppxcpw5hgct0melwhlvjafsu.jpeg"><br><br>  <b>Positive</b> <br><br><img src="https://habrastorage.org/webt/w6/rs/5j/w6rs5je45iwv-m22jf4vjomcs1s.jpeg"><br><br><img src="https://habrastorage.org/webt/fa/r4/af/far4afuqyyajc3xfqjwnj98xkbo.jpeg"><br><br><img src="https://habrastorage.org/webt/ky/ae/q2/kyaeq2nx8wqpkmnba9y2mugejai.jpeg"><br><br>  At first glance, images from different classes are actually different from each other.  However, let's conduct a more in-depth study and try to find bad examples - similar images belonging to different classes. <br><br>  For example, we have about 90 images of snakes marked as negative and about 40 very similar images of snakes marked as positive. <br><br>  <b>Positive snake image</b> <br><br><img src="https://habrastorage.org/webt/-m/es/5g/-mes5gboq8vn6p28etujmipmjme.jpeg"><br><br>  <b>Negative Snake Image</b> <br><br><img src="https://habrastorage.org/webt/np/1f/dv/np1fdvdekusz5cokd96cjou7smu.jpeg"><br><br>  The same duality occurs with spiders (130 negative and 20 positive images), naked people (15 negative and 45 positive images), and some other classes.  It seems that the labeling of images was performed by different people, and their perception of the same image may differ.  Therefore, the marking contains its inherent inconsistency.  These two images of snakes are almost identical, with different experts attributed them to different classes.  Thus, it can be concluded that it is hardly possible to ensure 100% accuracy when working with this task due to its nature.  We believe that a more realistic estimate of accuracy will be 80 percent ‚Äî this value is based on the proportion of similar images found in various classes during the preliminary visual inspection. <br><br><h3>  <font color="#0071c5">Separation of the training / verification process</font> </h3><br>  We always strive to create the best possible model.  However, what is the meaning we put into this concept?  There are many different criteria for this, such as: quality, lead time (learning + getting output) and memory consumption.  Some of them can be easily and objectively measured (for example, time and amount of memory), while others (quality) are much more difficult to determine.  For example, your model can demonstrate 100 percent accuracy when learning with examples that have been used to do this many times, but fail when working with new examples.  This problem is called <i>overfitting</i> and is one of the most important in machine learning.  There is also an <i>under-fit</i> problem: in this case, the model cannot learn from the data presented and demonstrates poor predictions even when using a fixed training data set. <br><br>  To solve the problem of overfitting, the so-called technique of <i>holding a part of the samples is used</i> .  Its main idea is to split the source data into two parts: <br><br><ul><li>  <i>A training set</i> , which usually makes up most of the data set and is used to train the model. </li><li>  <i>The test set is</i> usually a small part of the source data, which is divided into two parts before performing all the training procedures.  This set is not used at all in training and is considered as new examples for testing the model after completion of training. </li></ul><br>  Using this method, we can observe how well our model <i>generalizes</i> (that is, it works with previously unknown examples). <br><br>  This article will use a 4/1 aspect ratio for the training and test sets.  Another technique we use is the so-called <i>stratification</i> .  This term means splitting each class independently of all other classes.  This approach allows you to maintain the same balance between the size of the classes in the training and test sets.  Stratification implicitly uses the assumption that the distribution of examples does not change when the source data is changed and remains the same when using new examples. <br><br><img src="https://habrastorage.org/webt/pn/gq/lz/pngqlzmf15cnm-4cwjvblndwpsg.png"><br><br>  We illustrate the concept of stratification with a simple example.  Suppose that we have four data groups / classes with the corresponding number of objects in them: children (5), teenagers (10), adults (80) and elderly people (5);  see the picture on the right (from <a href="https://en.wikipedia.org/wiki/Stratified_sampling">Wikipedia</a> ).  Now we need to split this data into two sets of samples in a ratio of 3/2.  When using stratification of examples, objects will be selected independently from each group: 2 objects from a group of children, 4 objects from a group of teenagers, 32 objects from a group of adults, and 2 objects from a group of elderly people.  The new data set contains 40 objects, which is exactly 2/5 of the original data.  At the same time, the balance between the classes in the new data set corresponds to their balance in the original data. <br><br>  All the above actions are implemented in a single function called <i>prepare_data</i> ;  This function can be found in the <i>utils.py</i> Python file.  This function loads data, breaks it up into training and test sets using a fixed random number (for later playback), and then distributes the data appropriately between directories on the hard disk for later use. <br><br><h3>  <font color="#0071c5">Pretreatment and augmentation</font> </h3><br>  In a previous article, preprocessing actions and possible reasons for their use in the form of augmentation data were described.  Convolutional neural networks are quite complex models, and their training requires large amounts of data.  In our case there are only 1600 examples - this, of course, is not enough. <br><br>  Therefore, we want to expand the set of data used by <i>augmentation data</i> .  In accordance with the information provided in the article on data preprocessing, the Keras * library provides the ability to augment data on the fly when reading it from a hard disk.  This can be done through the <a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a> class. <br><br><img src="https://habrastorage.org/webt/3m/j8/oe/3mj8oewbjg3j8eriel5opmj43cc.png"><br><br>  Here are two instances of generators.  The first copy is intended for training and uses many random transformations - such as rotation, shift, convolution, scaling and horizontal rotation - while reading data from the disk and transferring it to the model.  As a result, the model receives already transformed examples, and each example obtained by the model is unique due to the random nature of this transformation.  The second copy is intended for verification, and it only zooms the images.  The training and verification generators have only one common transformation - the zoom.  To ensure the computational stability of the model, it is necessary to use the range [0;  1] instead of [0;  255]. <br><br><h2>  <font color="#0071c5">Model architecture</font> </h2><br>  After studying and preparing the initial data, the model creation stage follows.  Since a small amount of data is available to us, we are going to build a relatively simple model in order to be able to train it accordingly and eliminate the oversampling situation.  Let's try the <a href="https://arxiv.org/pdf/1409.1556v6.pdf">VGG</a> style <a href="https://arxiv.org/pdf/1409.1556v6.pdf">architecture</a> , however we use fewer layers and filters. <br><br><img src="https://habrastorage.org/webt/xa/dl/ae/xadlae7ebpbynxk60facw3_bxho.png"><br><br><img src="https://habrastorage.org/webt/2b/xr/cy/2bxrcyazsu_gasst_aqr5ao7ayu.png"><br><br>  The network architecture consists of the following parts: <br>  <b>[Convolutional layer + convolutional layer + selection of the maximum value] √ó 2</b> <br>  The first part contains two superimposed convolutional layers with 64 filters (with size 3 and step 2) and a layer for selecting the maximum value (with size 2 and step 2), located after them.  This part is also commonly referred to as <i>a feature extraction unit</i> , since filters effectively extract significant features from the input data (see the article <a href="https://habr.com/company/intel/blog/415811/">Overview of convolutional neural networks to classify images</a> for more information). <br><br>  <b>Alignment</b> <br><br>  This part is mandatory, since at the output of the convolutional part four-dimensional tensors are obtained (examples, height, width and channels).  However, for the usual fully connected layer, we need a two-dimensional tensor (examples, signs) as input data.  Therefore, it is necessary to <i>align the</i> tensor around the last three axes in order to combine them into one axis.  In fact, this means that we consider each point in each feature map as a separate property and align them into one vector.  The figure below shows an example of a 4 √ó 4 image with 128 channels, which is aligned into one long vector with a length of 1024 elements. <br><br><img src="https://habrastorage.org/webt/zs/-f/_h/zs-f_h8_mr6flzz62vo21qttt0u.png"><br><br>  <b>[Full connected layer + elimination method] √ó 2</b> <br><br>  Before you is the <i>classification part of the</i> network.  It takes an aligned representation of the attributes of the images and tries to classify them in the best possible way.  This part of the network consists of two superimposed blocks consisting of a fully connected layer and <i>an exclusion method</i> .  We have already met with fully connected layers - usually these are layers with fully connected connectivity.  But what is the ‚Äúexception method‚Äù?  The exclusion method is a <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">regularization technique</a> that helps prevent oversubset.  One of the possible signs of overfitting is the extremely different values ‚Äã‚Äãof the weighting factors (the orders of the corresponding quantities).  There are many ways to solve this problem, including weight reduction and the method of elimination.  The idea of ‚Äã‚Äãthe exclusion method is to disable random neurons during training (the list of disabled neurons must be updated after each packet / epoch of training).  This greatly hinders the obtaining of completely different values ‚Äã‚Äãfor the weights, thus regularizing the network. <br><br><img src="https://habrastorage.org/webt/1x/g5/vw/1xg5vwjp4syjmujonokwl9i-ilo.png"><br><br>  An example of the use of the elimination method (picture taken from the article <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">The elimination method: an easy way to prevent oversampling in neural networks</a> ): <br><br>  <b>Sigmoid module</b> <br><br>  The output layer must correspond to the formulation of the problem.  In this case, we are dealing with a binary classification problem, so we need one output neuron with a <i>sigmoid</i> activation function, which estimates the probability P of belonging to class number 1 (in our case, these will be positive images).  Then the probability of belonging to class number 0 (negative images) can easily be calculated as 1 - P. <br><br><h2>  <font color="#0071c5">Settings and learning options</font> </h2><br>  We chose the model architecture and specified it using the Keras library for the Python language.  In addition, before you start learning the model, you must <i>compile</i> it. <br><br><img src="https://habrastorage.org/webt/th/pv/l8/thpvl8hahgpnyucfsfhxpk5qq8w.png"><br><br>  At the compilation stage, the model is configured for training.  It is necessary to specify three main parameters: <br><br><ul><li>  <i>Optimizer</i> .  In this case, we use the <a href="https://arxiv.org/pdf/1412.6980.pdf">Adam</a> * default optimizer, which is a type of stochastic gradient descent algorithm with momentum and adaptive learning rate (for more information, see S. Ruder blog entry <a href="http://ruder.io/optimizing-gradient-descent/index.html">Overview of Gradient Descent Optimization Algorithms</a> ). </li><li>  <i>Loss function</i>  Our task is a binary classification problem, so it would be appropriate to use <a href="https://en.wikipedia.org/wiki/Cross_entropy">binary cross entropy</a> as a loss function. </li><li>  <i>Metrics</i> .  This is an optional argument by which you can specify additional metrics to track during the training procedure.  In this case, we need to track accuracy along with the objective function. </li></ul><br>  Now we are ready to learn the model.  Note that the learning procedure is performed using generators initialized in the previous section. <br><br>  The number of epochs is another hyperparameter that can be customized.  Here we simply assign it a value of 10. We also want to save the model and the learning history in order to be able to load it later. <br><br><img src="https://habrastorage.org/webt/mt/3o/cd/mt3ocd0xfdxmshq_7tv1xptw5de.png"><br><br><h2>  <font color="#0071c5">Evaluation</font> </h2><br>  Now let's see how well our model works.  First of all, consider the change in metrics in the learning process. <br><br><img src="https://habrastorage.org/webt/d-/_j/1l/d-_j1lty0qibl5gkwrhy3avbgzq.png"><br><br>  In the figure, it can be seen that cross-validation entropy and accuracy do not decrease over time.  Moreover, the accuracy metric for training and test sets simply fluctuates around the value of the random classifier.  The total accuracy for the test set is 55 percent, which is only slightly better than a random estimate. <br><br>  Let's look at how model predictions are distributed between classes.  For this purpose, it is necessary to create and visualize an <i>inaccuracy matrix</i> using the appropriate function from the Sklearn * package for the Python language. <br>  Each cell in the inaccuracy matrix has its own name: <br><br><img src="https://habrastorage.org/webt/p4/9j/mj/p49jmjgtuc4fhqxfd_szqm6qvtw.png"><br><br><ul><li>  True Positive Rate = TPR (upper right cell) represents the proportion of positive examples (class 1, that is, <i>positive</i> emotions in our case), which are correctly classified as positive. </li><li>  False Positive Rate = FPR (lower right cell) represents the proportion of positive examples that are incorrectly classified as <i>negative</i> (class 0, that is, negative emotions). </li><li>  True Negative Rate = TNR (lower left cell) represents the proportion of negative examples that are correctly classified as negative. </li><li>  False Negative Rate = FNR (upper left cell) represents the proportion of negative examples that are classified in the wrong way as positive. </li></ul><br>  In our case, both the TPR and FPR values ‚Äã‚Äãare close to 1. This means that almost all the objects were classified as positive.  Thus, our model is not far removed from the naive base model with constant predictions of a larger class size (in our case, these are positive images). <br><br>  Another interesting metric that is interesting to observe is the receiver‚Äôs performance curve (ROC curve) and the area under this curve (ROC AUC).  A formal definition of these concepts can be found <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">here</a> .  In a nutshell, the ROC curve shows how well the binary classifier works. <br><br>  The classifier of our convolutional neural network has a sigmoid module as an output, which assigns the probability of example to class 1. Now suppose that our classifier shows good performance and assigns low probability values ‚Äã‚Äãfor examples of class 0 (green histogram in the figure below) and high probability values ‚Äã‚Äãfor examples class 1 (histogram blue). <br><br><img src="https://habrastorage.org/webt/wq/e_/us/wqe_usitkajallhk1ymcsq472pu.png"><br><br>  The ROC curve shows how the TPR depends on the FPR when moving the classification threshold from 0 to 1 (right figure, top).  For a better understanding of the concept of a threshold, remember that we have the probability of belonging to class 1 for each example.  However, probability is not yet a class label.  Therefore, it should be compared with the threshold to determine which class the example belongs to.  For example, if the threshold value is 1, then all examples should be classified as belonging to class 0, since the probability value cannot be more than 1, and the values ‚Äã‚Äãof the FPR and TPR indicators will be equal to 0 (since none of the samples are classified as positive ).  This situation corresponds to the leftmost point on the ROC curve.  On the other side of the curve, there is a point at which the threshold value is 0: this means that all samples are classified as belonging to class 1, and the values ‚Äã‚Äãof both TPR and FPR are 1. Intermediate points reflect the behavior of the TPR / FPR dependency when the threshold value changes. <br><br>  The diagonal line on the graph corresponds to a random classifier.  The better our classifier works, the closer its curve is to the left upper point of the graph.  Thus, the objective indicator of the quality of the classifier is the area under the ROC curve (ROC AUC).  The value of this indicator should be as close as possible to 1. AUC value of 0.5, corresponds to a random classifier. <br><br>  The AUC in our model (see figure above) is 0.57, which is not the best result. <br><br><img src="https://habrastorage.org/webt/oo/vu/-t/oovu-t0vyxvlbgb4zgp4qyvodsw.png"><br><br>  All these metrics indicate that the resulting model is only slightly better than the random classifier.  There are several reasons for this, the main ones are described below: <br><br><ul><li>  A very small amount of data for training, insufficient to highlight the characteristic features of images.  Even data augmentation could not help in this case. </li><li>  A relatively complex convolutional neural network model (compared to other machine learning models) with a large number of parameters. </li></ul><br><h2>  <font color="#0071c5">Conclusion</font> </h2><br>  In this article, we created a simple convolutional neural network model for recognizing emotions in images.  At the same time, at the training stage, a number of methods were used for augmentation of data, the model was also evaluated using a set of metrics such as: accuracy, ROC-curve, ROC AUC and inaccuracy matrix.  The model showed results, only slightly better than random ones.  The reason for this is the lack of available data. </div><p>Source: <a href="https://habr.com/ru/post/420635/">https://habr.com/ru/post/420635/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../420625/index.html">KDD 2018, first day, tutorials</a></li>
<li><a href="../420627/index.html">Asynchronous C # programming: how are you doing with performance?</a></li>
<li><a href="../420629/index.html">PHP Digest No. 137 (August 6 - 20, 2018)</a></li>
<li><a href="../420631/index.html">We are not afraid of "clouds"</a></li>
<li><a href="../420633/index.html">We write GeoIP exporter for Prometheus with visualizations in Grafana in 15 minutes</a></li>
<li><a href="../420637/index.html">WANHAO D9 / 300 3D Printer Review: Video</a></li>
<li><a href="../420639/index.html">Akka antipatterns: too many actors</a></li>
<li><a href="../420641/index.html">Technical support 3CX responds: backup and restore 3CX from the command line</a></li>
<li><a href="../420643/index.html">Almost everything is the same, only 10 times cheaper</a></li>
<li><a href="../420645/index.html">Realistic hiring practice for engineers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>