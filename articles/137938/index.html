<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Creating a reliable storage distributed to multiple servers on nfs</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When creating a cluster for processing calls based on CGP , it became necessary to configure uninterrupted storage mounted from several servers. 

 Ub...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Creating a reliable storage distributed to multiple servers on nfs</h1><div class="post__text post__text-html js-mediator-article">  When creating a cluster for processing calls based on <a href="http://communigate.com/">CGP</a> , it became necessary to configure uninterrupted storage mounted from several servers. <br><br>  Ubuntu Server 10.10 was taken as a distribution for servers.  The disk space was divided into two logical disks (sda1 for the installation of the system, and sda2 for the shared disk itself). <br><br>  After installing the base system, you must additionally install the following packages: heartbeat, pacemaker, drbd8-utils, xfs, xfsprogs, nfs-kernel-server. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Heartbeat and pacemaker are needed for server clustering.  The storage is based on drbd, xfs was used as the file system.  The distribution of the file system servers made by nfs. <br><br><a name="habracut"></a><br><h4>  1. System Setup </h4><br>  The names u1 and u2 were chosen for the nodes.  For convenience, these names were immediately registered in / etc / hosts: <br><pre>  10.0.0.84 u1
 10.0.0.115 u2
 10.0.0.120 u0
</pre><br>  u0 is the address at which the storage is available for mounting the file system from other servers. <br><br><h4>  2. Setup drbd </h4><br>  The file with the storage configuration is located in /etc/drbd.d/r0.res: <br><pre> resource r0 {
     protocol C;
     syncer {
         rate 4M;
     }
     startup {
         wfc-timeout 15;
         degr-wfc-timeout 60;
     }
     net {
         after-sb-0pri discard-zero-changes;
         after-sb-1pri discard-secondary;
         after-sb-2pri disconnect;
         cram-hmac-alg sha1;
         shared-secret somesecretword;
     }
     on u1 {
         device / dev / drbd0;
         disk / dev / sda2;
         address 10.0.0.84:7788;
         meta-disk internal;
     }
     on u2 {
         device / dev / drbd0;
         disk / dev / sda2;
         address 10.0.0.115:7788;
         meta-disk internal;
     }
 }
</pre><br>  On both nodes, the file is the same, so you can create it on one and then copy to the second. <br><br>  Such a brutal setting for the treatment of the split brain was chosen, since the storage is mainly used to store the system configuration.  That is, the loss of recent changes is not as critical as the loss of calls during downtime due to slit blain. <br><br>  After creating the configuration file, you must create the disks themselves on both servers in the cluster: <br><pre> dd if = / dev / zero of = / dev / sda2 bs = 64M
 drbdadm create-md r0
</pre><br>  After that you can start drbd.  It is important to start the drbd daemon on both servers with a difference of less than a minute (timeout for communicating with other cluster members): <br><pre> /etc/init.d/drbd start
</pre><br>  After this, in / proc / drbd will be the state of the storage, in approximately the form: <br><pre> 0: cs: Connected ro: Secondary / Secondary ds: Inconsistent / Inconsistent C r ---- </pre><br>  That is, two nodes in the secondary mode, with the disc inconsistent.  To get out of this position, you must forcibly declare one of the nodes of the cluster as the main one; to do this, you must execute the command: <br><pre> drbdadm - --overwrite-data-of-peer primary r0
</pre><br>  After that drbd will start updating the state of the secondary disk: <br><pre> 0: cs: SyncSource ro: Primary / Secondary ds: UpToDate / Inconsistent C r ----
 ns: 241984 nr: 0 dw: 0 dr: 242184 al: 0 bm: 14 lo: 510 pe: 179 ua: 510 ap: 0 ep: 1 wo: b oos: 782664
         [===&gt; ................] sync'ed: 23.6% (782664/1023932) K
         finish: 0:04:04 speed: 3,160 (3,172) K / sec
</pre><br>  After the synchronization is complete, you can create a file system on the server, where drbd is in the primary state: <br><pre> mkfs.xfs / dev / drbd0
</pre><br>  For the next steps we will be hampered by the standard daemon launch mechanism.  Therefore, you need to run the command on both servers: <br><pre> update-rc.d -f drbd remove
</pre><br><h4>  3. Configure heartbeat </h4><br>  The configuration is created in the /etc/heartbeat/ha.cf file.  It is the same on both nodes, so you can create it on one and then copy it to the second. <br><pre> logfacility daemon
 keepalive 2
 deadtime 15
 warntime 5
 initdead 120
 udpport 694
 ucast eth0 10.0.0.115
 ucast eth0 10.0.0.84
 auto_failback on
 node u1
 node u2
 use_logd yes
 crm respawn
</pre><br>  The second file is used to authenticate / etc / heartbeat / authkeys: <br><pre> auth 1
 1 sha1 somesecretword
</pre><br>  It is better not to wait for a warning from heartbeat to the wrong file attributes and change them in advance: <br><pre> chmod 600 / etc / heartbeat / authkeys
</pre><br>  After that, you can start heartbeat: <br><pre> /etc/init.d/heartbeat start
</pre><br>  After some time, the crm_mod command should show that two nodes have connected to each other: <br><pre> ============
 Last updated: Fri Feb 10 09:33:04 2012
 Stack: Heartbeat
 Current DC: u1 (86b204d8-ee3e-47c7-ba0e-1dcbd40a20da) - partition with quorum
 Version: 1.0.9-unknown
 2 Nodes configured, 2 expected votes
 2 Resources configured.
 ============

 Online: [u2 u1]
</pre><br>  Next, you need to run the crm configure edit command and enter the settings for the cluster: <br><pre> node $ id = "86b204d8-ee3e-47c7-ba0e-1dcbd40a20da" u1
 node $ id = "c6e3c21f-da3e-4031-9f28-a7e33425a817" u2
 primitive drbd0 ocf: linbit: drbd \
         params drbd_resource = "r0" \
         op start interval = "0" timeout = "240" \
         op stop interval = "0" timeout = "100" \
         op monitor interval = "20" role = "Slave" timeout = "20" depth = "0" \
         op monitor interval = "10" role = "Master" timeout = "20" depth = "0"
 primitive fs0 ocf: heartbeat: Filesystem \
         params directory = "/ shared" fstype = "xfs" device = "/ dev / drbd / by-res / r0" options = "noatime, nodiratime, nobarrier, logbufs = 8" \
         op start interval = "0" timeout = "60" \
         op stop interval = "0" timeout = "60" \
         op notify interval = "0" timeout = "60" \
         op monitor interval = "20" timeout = "40" depth = "0" \
         meta target-role = "Started"
 primitive ip0 ocf: heartbeat: ipaddr2 \
         params ip = "10.0.0.120" nic = "eth0: 0" \
         op monitor interval = "5s" \
         meta target-role = "Started"
 primitive nfs0 ocf: itl: exportfs \
         params directory = "/ shared" clientspec = "10.0.0.0/255.255.255.0" options = "rw, no_root_squash, sync, no_wdelay" fsid = "1" \
         op start interval = "0" timeout = "40" \
         op stop interval = "0" timeout = "60" \
         op monitor interval = "30" timeout = "55" depth = "0" OCF_CHECK_LEVEL = "10" \
         meta target-role = "Started"
 group ha_nfs fs0 nfs0 ip0 \
         meta target-role = "Started"
 ms ms_drbd0 drbd0 \
         meta master-max = "1" master-node-max = "1" clone-max = "2" clone-node-max = "1" notify = "true"
 colocation c_nfs inf: nfs0 ms_drbd0: Master
 order o_nfs inf: ms_drbd0: promote ha_nfs: start
 property $ id = "cib-bootstrap-options" \
         dc-version = "1.0.9-unknown" \
         cluster-infrastructure = "Heartbeat" \
         stonith-enabled = "false" \
         expected-quorum-votes = "2" \
         no-quorum-policy = "ignore" \
         symmetric-cluster = "true" \
         last-lrm-refresh = "1328625786"
 rsc_defaults $ id = "rsc_defaults-options" \
         resource-stickiness = "10,000"
</pre><br>  The exportfs settings indicate that the directory where the shared file system will be mounted, this resource can be mounted over nfs to all servers from the 10.0.0.0 network. <br><br>  After some time, the cluster monitor should show the status of resources: <br><pre> ============
 Last updated: Fri Feb 10 09:33:04 2012
 Stack: Heartbeat
 Current DC: u1 (86b204d8-ee3e-47c7-ba0e-1dcbd40a20da) - partition with quorum
 Version: 1.0.9-unknown
 2 Nodes configured, 2 expected votes
 2 Resources configured.
 ============

 Online: [u2 u1]

  Resource Group: ha_nfs
      fs0 (ocf :: heartbeat: Filesystem): Started u1
      nfs0 (ocf :: itl: exportfs): Started u1
      ip0 (ocf :: heartbeat: ipaddr2): Started u1
  Master / Slave Set: ms_drbd0
      Masters: [u1]
      Slaves: [u2]
</pre><br><h4>  4. Mounting a shared file system from other servers </h4><br>  You can use / etc / fstab to do this: <br><pre> u0: / shared / var / CommuniGate / SharedDomains nfs bg, intr 0 0
</pre><br><h4>  5. Additional links </h4><br><ol><li>  When creating the configuration, the description was taken as the basis <a href="">http://library.linode.com/linux-ha/ip-failover-heartbeat-pacemaker-drbd-mysql-ubuntu-10.04</a> </li><li>  Treatment instructions for split-brain from drbd <a href="http://www.alsigned.ru/%3Fp%3D490">http://www.alsigned.ru/?p=490</a> </li><li>  In Ubuntu 10.10 exportfs, the agent is not included, so you need to download it separately ( <a href="https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/exportfs">https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/exportfs</a> ) and install it in /usr/lib/ocf/resource.d/ heartbeat / </li><li>  There is no important part in this configuration - STONITH ( <a href="http://linux-ha.org/wiki/STONITH">http://linux-ha.org/wiki/STONITH</a> ), since there is no backup channel </li><li>  A description of the mechanism for using a common address can be found at <a href="http://www.ultramonkey.org/3/ip_address_takeover.html">http://www.ultramonkey.org/3/ip_address_takeover.html</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/137938/">https://habr.com/ru/post/137938/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../137932/index.html">Failure of the master in the PostgreSQL-cluster: how to be?</a></li>
<li><a href="../137933/index.html">Organize your audio library. Practical advice</a></li>
<li><a href="../137935/index.html">Cellular operators will notify subscribers about changes to the tariff plan via SMS</a></li>
<li><a href="../137936/index.html">For Nginx appeared commercial technical support</a></li>
<li><a href="../137937/index.html">Implementing Code Action with Roslyn</a></li>
<li><a href="../137940/index.html">RIPE NCC Password Policy Update</a></li>
<li><a href="../137941/index.html">John Barlow conveys heartfelt greetings to the sopa copiers and supporters: lecture in Russia</a></li>
<li><a href="../137943/index.html">Content for the site: better in advance than never</a></li>
<li><a href="../137945/index.html">What are patents in Russia? Likbez, Part 3</a></li>
<li><a href="../137946/index.html">Canobuvosti, 130th edition</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>