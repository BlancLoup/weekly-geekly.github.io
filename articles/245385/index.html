<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Assessing the impact of Cache levels on I / O performance in EMC VNX5400</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 After testing and writing an article about the effect of caching mechanisms on the performance of the younger model (entry level) EMC V...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Assessing the impact of Cache levels on I / O performance in EMC VNX5400</h1><div class="post__text post__text-html js-mediator-article"><h2>  Introduction </h2><br>  After testing and writing an article about the effect of caching mechanisms on the performance of the younger model (entry level) EMC <a href="http://habrahabr.ru/post/237993/">VNXe3200</a> , periodically they began to itch their hands to do the same with its older fellow VNX2 arrays.  Recently, this opportunity presented itself.  Managed to run tests on the VNX5400.  In addition to testing directly the VNX5400, we also tested the EMC ExtremCache solution (PCI-E SSD EMC XtremSF700 card on eMLC chips + EMC XtremSW software).  But about EMC ExtremCache there will be the following article.  In the meantime, let's talk about arrays VNX2. <br><a name="habracut"></a><br>  EMC <sup>2</sup> updated its line of midrange storage systems in the fall of 2013.  The second-generation VNX arrays (VNX2) looked, in my opinion, like a job well done on the mistakes made in the first-generation VNX (VNX1).  Vendor also presented them as a radical and innovative midrenge update line.  However, now, more than a year after they hit the market, the VNX2 still has not lost its relevance.  I will not dwell on the characteristics and capabilities of the arrays themselves, I will simply give links to the official documents of the <a href="https://www.emc.com/auth/collateral/hardware/data-sheets/h8520-vnx-family-ds.pdf">Data Sheet</a> and <a href="https://www.emc.com/collateral/software/specification-sheet/h8514-vnx-series-ss.pdf">Spec Sheet</a> . <br><br><h2>  Description of the stand and tests </h2><br><div class="spoiler">  <b class="spoiler_title">Under the spoiler</b> <div class="spoiler_text">  Testing was carried out using IOMETER.  The load profile is the same as when testing the <a href="http://habrahabr.ru/post/237993/">VNXe3200</a> .  Standard database pattern (Intel / StorageReview.com). <br><br><img src="https://habrastorage.org/files/2d7/142/e09/2d7142e092a54894aa7e51965d5c05fa.jpg">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      There was an HP DL360 G5 server with 1 CPU (4-core) and 4GB RAM.  The server has 2 PCI-E slots.  A dual-port Qlogic 4Gb / s HBA was installed in one of the slots, connected directly to the VNX5400.  Since the physical cores in the CPU are much lower than when testing the VNXe3200, we had to initially increase the number of I / O streams per Worker IOMETER.  The ratio of 1 Worker to 1 core CPU.  At each Worker, 3 I / O streams were initially set up with a ‚Äúmultiplier‚Äù 2. That is,  each subsequent run (15 min) of the test in a cycle increased the number of flows by 2 times.  Only 5 consecutive runs and respectively on the Worker on 3/6/12/24/48 IO threads.  In general, the test file received 12/24/48/96/192 stream.  Those.  the maximum is the same as for VNXe3200.  The time of each test is 15x5 = 75 minutes, not counting the ‚ÄúRump up time‚Äù.  The settings in IOMETER are as follows. <br><br><img src="https://habrastorage.org/files/b82/076/da9/b82076da9b2c414db45bc306ad1e09f5.jpg"><br><br>  On the VNX5400, there was already a FastVP disk pool with moons on which OSs were installed, so I didn‚Äôt tinker and create everything anew.  The pool included 3 and 100Gb SSD disks (Extreme Performance Tier) and 15 900Gb SAS 10K RPM disks in Raid5 4 + 1 (Perfomance tier), i.e.  3 private (not visible to the user) Raid Groups in 4 + 1 configuration.  Test moons with the ‚ÄúLowest Available Tier‚Äù policy were created on the pool, so that these moons would be located entirely on SAS disks and SSD disks from Extreme Perfomance Tier would not affect the results. <br><br><img src="https://habrastorage.org/files/279/002/e75/279002e7563e4926943390ee78ebd337.jpg"><br><br><img src="https://habrastorage.org/files/efc/2ca/7e3/efc2ca7e31ab4f48b787265c74a9a905.jpg"><br><br>  The server used OS Win2012R2 SP1 and software for managing access paths to the moon PowerPath from EMC <sup>2</sup> . <br><br><h4>  Some calculations. </h4><br>  EMC <sup>2</sup> recommends using 150 IOPS for SAS 10k RPM drives (FC / SAS 15k RPM - 180 IOPS, SATA / NL_SAS - 90 IOPS, SSD eMLS - 3500 IOPS, SSD SLC - 5000 IOPS) for performance calculations.  That is, in theory, our 15 disks can produce 150x15 = 2250 IOPS.  We need to calculate how much IOPS will be obtained from these server disks, taking into account our read / write load profile as a percentage of 67/33 and the overhead of writing to RAID5.  We get the following equation with one unknown 2250 = X * 0.33 * 4 + X * 0.67.  Where X is our IOPS, which will get the server from our disks, and 4 is the size of the ‚Äúpenalty‚Äù on the record for Raid5.  As a result, we get X = 2250 / 1.99 = ~ 1130 IOPS.  Let me remind you that in practice in peak loads we usually get 1.5-2 times higher figures for IOPS.  That is, with proper distribution of data across all three private Raid5 disk groups, we can count on the range of 1695-2260 IOPS. </div></div><br><h2>  Tests and Results </h2><br><h4>  1. Test of Cache Controllers (SP - storage processor) VNX5400 file 2Gb </h4><br>  This test was conducted on a 2GB file (the size of the test LUN from NTFS is 10 Gb), on the basis that it fits perfectly into SP Cache and as a result all I / O will be worked out from the RAM memory of the storage controllers.  That is, conditionally, a small ‚Äúhot‚Äù data area appeared on the array.  At the same time, FastCache on the array was turned off. <br>  As a result, we got the following graph in the Unisphere Analyzer embedded in the array. <br><br><img src="https://habrastorage.org/files/f14/b11/a0e/f14b11a0e0134014adcfc1aff6d06dec.jpg"><br><br>  At the same time, SP Cache was filled in the following way (cache works both for reading and writing): <br><br><img src="https://habrastorage.org/files/5c9/966/545/5c996654579644d09bd1470807654827.jpg"><br><br>  That is, within a few minutes after the start of the test, all 2Gb of the test file were loaded into Cache. <br><a name="anchor1"></a><br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/49e/dc5/377/49edc5377ddf49e6a7b683ee10a22dfe.jpg"><br><a name="anchor2"></a><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/8ab/82b/9b3/8ab82b9b38dd4151a76dc88049f31567.jpg"><br><br>  I remind you that it is considered to be comfortable value of the average response time for databases up to 10 ms.  The graph shows that with 192 IO streams, which is a rather large value, we got a slightly smaller average response time than in the same test on the VNXe3200 (it was 8 ms).  In absolute terms, the difference is not significant, but nonetheless is one of the reasons for the large gap in the number of IOPS.  The VNXe3200 had an average of about 23,800 IOPS per 192 IO streams in a similar test.  If we calculate the difference in percent for the response time and for IOPS, we get both there and there about the same 20-25%. <br><br><h4>  2. Test of Cache Controllers (SP - storage processor) VNX5400 file 15Gb </h4><br>  The test was conducted on a 15GB file (LUN with NTFS - 20Gb).  FastCache is still off.  The size of the RAM in the SP in the VNX5400 is 16 Gb.  But if you rely on logs from the array, then the actual amount of RAM used for caching is about 4Gb for VNX5400.  Everything else is apparently used to ensure the operation of the most basic OS controllers and another, rather wide, functional storage system (tiering, thin provisioning, snapshots, clone, replication, fast cache, etc.). <br><br><img src="https://habrastorage.org/files/5ec/986/2f8/5ec9862f8b23459bb30dc8a954d00d3b.jpg"><br><br>  Thus, our 15 Gb highly loaded (hot) data will not fit in SP Cache.  In general, this is exactly what can be seen on the graph from the Unisphere Analizer.  Fully random I / O across all 15 Gb cannot be fully cached by the controllers, so in this test, the main performance volume comes from physical spindles, i.e.  directly discs. <br><br><img src="https://habrastorage.org/files/020/241/ec5/020241ec5e54458eb60827fed06b97ec.jpg"><br><br>  At the same time, SP Cache continues to work, although not so effectively. <br><br><img src="https://habrastorage.org/files/bba/382/9d7/bba3829d76b649169c0c6a96c8023982.jpg"><br><br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/ec7/5d9/d1c/ec75d9d1cd4a435f9f6be59f06ac2848.jpg"><br><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/5f6/3fa/269/5f63fa2691fe48a0b2fbfd19ecae5ab3.jpg"><br><br>  The graph shows that the response time already on 24 streams is about 14 ms, i.e.  out of the comfort zone.  I was also surprised that I missed so much with an estimate of peak disk performance and I ‚Äúwent‚Äù to figure it out.  As a result, I found out that during the test the load not on all private RGs in the Perfomance Tier is the same.  Those.  based on performance considerations, our test file is unevenly distributed across all 3rd RGs in the Performance Tier.  On the graph it looks like this (disks 5, 6 and 7 are SSD from Extreme Perfomance Tier). <br><br><img src="https://habrastorage.org/files/c19/0e3/77d/c190e377d3ae49a1ab3023967fc3c638.jpg"><br><br>  That is, not all 15 spindles were involved in the full test.  To smooth out a similar effect in FastVP pools on VNX2 arrays there is a technology that allows redistributing ‚Äúhot‚Äù and ‚Äúcold‚Äù data not only between disks with different performance (SSD, SAS, NL_SAS), but also between private RGs within one Tier.  The redistribution of data between private RGs occurs at the same time according to a schedule as the migration of data between Tier-s.  In the array interface, this can be seen in the properties of the pool on the "Tiering" tab.  In particular, in my case immediately after the test, it looked like this. <br><br><img src="https://habrastorage.org/files/fb4/204/fdf/fb4204fdfddd4dfe979b4df779208756.jpg"><br><br>  Let me remind you that when the array was idle, the window looked like this. <br><br><img src="https://habrastorage.org/files/629/025/1ff/6290251ffb0045219acd273c2f5c6328.jpg"><br><br>  Another interesting picture can be seen by overlapping SP Cache and overall performance of the tested LUN. <br><br><img src="https://habrastorage.org/files/5dd/ae3/10d/5ddae310d1c647dd8c0d171a29f43e77.jpg"><br><br>  The graph shows that even in a rather unpleasant situation, SP Cache allows you to win "additional" IOPS. <br><br><h4>  3. Test FastCache in VNX5400 file 15Gb </h4><br>  After enabling FastCache on the array (two SSD 100Gb in Raid1), a test was performed on the same file in 15Gb (LUN with NTFS - 20Gb).  FastCache in EMC <sup>2</sup> arrays is an additional level of caching based on SSD disks, which is embedded between SP Cache and the disks of the array itself.  A 15Gb file (or conventionally a ‚Äúhot‚Äù data region) should fit completely into a 100Gb FastCache, which should improve the results of the previous test. <br>  Got the following graph from the Unisphere Analizer (in IOPS). <br><br><img src="https://habrastorage.org/files/ad5/a32/384/ad5a32384a48498c86b80720819ba301.jpg"><br><br>  FastCache was filled as follows (in%). <br><br><img src="https://habrastorage.org/files/f46/347/891/f46347891ebd44068a6096174ec6b16a.jpg"><br><br>  Read Hits / s - read operations that were processed from FastCache. <br>  Read Misses / s - operations for which data was not found in FastCache and were requested from the array disks. <br><br><img src="https://habrastorage.org/files/b28/ee2/22b/b28ee222bc084062b1138e3aa6a822a4.jpg"><br><br>  Write Hits / s - write operations in FastCache, which did not require a preliminary reset of "obsolete" data to disks (pre-clearing the cache), or operations that requested data written to the cache but not yet moved to disks. <br>  Write Misses / s - write operations not processed through FastCache or requiring forced (emergency) release of space in the cache. <br><br><img src="https://habrastorage.org/files/35b/7ad/c14/35b7adc144494cf3bdf9b3d7143f5199.jpg"><br><br>  SP Cache also didn‚Äôt stand idle and worked through some of the I / O. <br><br><img src="https://habrastorage.org/files/a33/9cd/13f/a339cd13f54f4169ae7fa81caffb5ec6.jpg"><br><br>  From the server side in IOMETR, everything looked as follows. <br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/acc/26c/c23/acc26cc23aad46cba7d30c46e9bdd718.jpg"><br><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/89d/b04/00f/89db0400f6ed439581051bd9b45ef3ee.jpg"><br><br>  The last graph shows that after loading the ‚Äúhot‚Äù area in FastCache, the response time even decreases and begins to grow only with increasing input / output streams.  At the same time, the graph ‚Äúbreaks through‚Äù a ceiling of 10 ms far beyond the value of 100 IO streams. <br><br><h4>  4. Test of FastCache in VNX5400 150Gb file </h4><br>  The following test was performed on a 150Gb file (Test LUN with NTFS 200Gb).  Those.  Our ‚Äúhot‚Äù data area in this test was about 1.5 times the size of FastCache on the array.  The following results were obtained. <br>  Graph from Unisphere Analizer (in IOPS). <br><br><img src="https://habrastorage.org/files/ce1/02d/d23/ce102dd232184cb1873b5d494b5f6dd7.jpg"><br><br>  Filling FastCache data went, but slowly enough.  By the end of the 75 minute test, about 20% were filled in (conventionally about 20Gb from a 150Gb test file). <br><br><img src="https://habrastorage.org/files/2e3/813/3a4/2e38133a467e4a479319a2f9d8b9723e.jpg"><br><br>  Hit schedules and non hit read operations in FastCache. <br><br><img src="https://habrastorage.org/files/744/90f/a36/74490fa36f53436e816f73fab92d0e85.jpg"><br><br>  Hits / s and misses / s charts for write operations in FastCache. <br><br><img src="https://habrastorage.org/files/498/dee/f8b/498deef8b0e848e3833ac31b59069fbd.jpg"><br><br>  SP Cache is also not idle. <br><br><img src="https://habrastorage.org/files/a33/9cd/13f/a339cd13f54f4169ae7fa81caffb5ec6.jpg"><br><br>  How it looked from the server and IOMETR. <br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/d19/73d/e6f/d1973de6f2f34e0faa64c372e6c6e369.jpg"><br><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/43f/8fa/3a1/43f8fa3a1e5a4a6b949eaaa133ac61f4.jpg"><br><br>  The graphs show that the IOPS values ‚Äã‚Äãare slightly higher than they were when testing a 15Gb file without FastCache.  So it would be possible to conclude that in this uncomfortable situation FastCache helps to squeeze out additional IOPS from the configuration.  But in practice, everything was not quite so. <br><br>  Firstly, in this test, all private Raid Group (Raid5 4 + 1) in the ‚ÄúPerfomance Tier‚Äù were loaded more evenly. <br><br><img src="https://habrastorage.org/files/278/fbc/a74/278fbca74d0c47bcb4fef0a876cf6265.jpg"><br><br>  Secondly, I decided to conduct an additional test with a 150Gb file, but with FastCache turned off.  I will not paint it in detail, here are the charts from IOMETR (I did not believe it at first and ran the test twice). <br><br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/e65/67c/edd/e6567cedd8e0497dabbe648e6b3cc12a.jpg"><br><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/0b0/bdc/2aa/0b0bdc2aa7184a65987a8eec7a15c110.jpg"><br><br>  That is, in a situation where the amount of hot data exceeds the size of FastCache and with a high percentage of random requests, filling in FastCache takes some time.  In such situations, FastCache introduces, albeit a small, but nonetheless additional delay in the Response time.  In this situation, you can suggest to use the optional Extreme Perfomance Tier on the SSD in the pool.  Part of the hot data in this case will "settle" on it and will not be processed through FastCache.  Accordingly, the amount of hot data processed through FastCache will decrease and will be in a more comfortable range.  That would not be unfounded, I conducted another test. <br><br><h4>  5. Test of FastCache in VNX5400 file 80Gb </h4><br>  This test was conducted on an 80Gb file (Test LUN with NTFS 100Gb), which is close enough to the FastCache volume in the array under test.  That is, the ‚Äúhot‚Äù data area was quite large, but nevertheless completely fit into FastCache. <br>  Graph from Unisphere Analizer (in IOPS). <br><br><img src="https://habrastorage.org/files/9a6/134/535/9a6134535e0b4bcc859cb7f355b5be26.jpg"><br><br>  FastCache was filled more actively than on a 150Gb file. <br><br><img src="https://habrastorage.org/files/e82/71d/38a/e8271d38af7a472f8ff5d99b1e4552eb.jpg"><br><br>  SP Cache also handled some I / O. <br><br><img src="https://habrastorage.org/files/82a/afa/fd4/82aafafd48674703b4056bd9879b7725.jpg"><br><br>  On the server side and IOMETR, everything looked the same much more rosy than on a 150Gb file. <br>  Graph of IOPS average values ‚Äã‚Äãversus the number of input / output streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/189/563/a70/189563a706244f639a33bb9093789ac8.jpg"><br><br>  Graph of average values ‚Äã‚Äãof I / O response time versus the number of I / O streams by results in IOMETER: <br><br><img src="https://habrastorage.org/files/402/c19/496/402c194960e044b5bd364d1520bcfa40.jpg"><br><br>  Starting from about 24 IO streams (about 15 minutes from the start of the test), the data began to fall more and more into FastCache.  Accordingly, the overall performance in the test began to grow as well, while the response time, as the IO flows increased, did not grow as significantly as in the test with the 150Gb file. <br><br><h2>  A small digression about disk pools </h2><br><div class="spoiler">  <b class="spoiler_title">Under the spoiler</b> <div class="spoiler_text">  When designing a disk pool, if you do not know the actual size of the hot and warm data area, EMC recommends that you maintain the following proportions for a three-level pool: <br>  10% - SSD drives <br>  20% - SAS drives <br>  70% - NL-SAS drives <br>  In addition, you should take into account that when you add flash tier to the pool automatically, all the metadata of the thin moon created on the pool will be placed on the SSD.  If there is enough space for them.  This allows you to increase the overall performance of thin moons and pool.  Under this metadata, you need to plan an additional place on the SSD at the rate of 3Gb volume for every 1Tb actually occupied by thin moons in the pool.  At the same time, the moons having the highest available tier tiring policy will take precedence when placed on an SSD dash over any other data. <br><br>  Using the lowest available tier policy for thin, deduplicated, or compressed moons results in placing their metadata on the slowest disks.  That negatively affects the performance of these moons. <br><br>  For the correct work of the tiring you need free space in the pool.  At least 10% of free space on each dash is recommended.  Otherwise, the system will not be able to ‚Äúshift‚Äù pieces (chunks) of data between different types of disks in the pool. </div></div><br><br><h2>  findings </h2><br>  Based on the tests performed, the following can be said.  No matter how strange it may sound, but FastCache is not always good.  In an improperly designed system, it can affect performance, including in the direction of deterioration.  In order not to miss the design stage, it is better to drive a copy or part of your actual loads on the demo array.  Demo array can be requested from the vendor.  If this is not possible, then it is necessary to proceed from the ratios of hot / warm / cold data that the same vendor suggests using in the calculations (based on statistical data and some considerations of your own).  The first option with a demo array, in my opinion, is preferable.  In general, on a properly designed array, the additional level of caching (FastCache) on the VNX2 gives a decent performance boost. <br><br>  The performance of the VNX5400 is not much higher than the performance of the VNXe3200, at least in small and comparable configurations (the number of disks, the size of the FastCase).  Perhaps this is due to the fact that the junior array was released only this 2014.  The same conclusions can be applied to the VNX5200, in which the SP (controllers) are no different from the VNX5400 (the service partner number for the replacement is the same).  The VNX5200 has only a limit on the maximum number of disks (125 pieces versus 250 pieces on the VNX5400), a limit on the maximum FastCache size (600Gb vs. 1000Gb for the VNX5400) and has one slot less for an optional expansion card with ports for connecting servers. <br><br>  All tests performed are ‚Äúsynthetics‚Äù, which have nothing to do with your actual loads.  However, in my opinion, such modeling helps to understand the general tendencies of storage behavior in various situations. <br><br><h2>  Like ps </h2><br>  In case you need a storage system for streaming traffic (Nr: recording and processing large video streams), then no cache will help you here.  The moment will come when it overflows and in this situation there will be the number of spindles (disks) in your storage system. <br>  If you have a very high transaction load.  A large OLTP database or VDI with thousands or tens of thousands of users, you probably need an all flash array, rather than a classic storage system and Tiering. </div><p>Source: <a href="https://habr.com/ru/post/245385/">https://habr.com/ru/post/245385/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../245375/index.html">White watch Sony SmartWatch 3 in "Connected"</a></li>
<li><a href="../245377/index.html">Hour Code in Russia</a></li>
<li><a href="../245379/index.html">YotaPhone 2: Innovation and Security</a></li>
<li><a href="../245381/index.html">To Git, or not to Git</a></li>
<li><a href="../245383/index.html">GPS monitoring for personal use (part 3. Systems overview)</a></li>
<li><a href="../245387/index.html">ReSharper 9, dotTrace 6, dotCover 3, dotMemory 4.2 and dotPeek 1.3 are available for download.</a></li>
<li><a href="../245391/index.html">Gaming Industry News Digest: November</a></li>
<li><a href="../245393/index.html">Yandex opens a new direction of its activity - Yandex Data Factory</a></li>
<li><a href="../245397/index.html">Bank statements and PHP is just</a></li>
<li><a href="../245401/index.html">How we tried to implement Yandex.Kassa</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>