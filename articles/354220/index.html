<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Applying recurrent layers to solve multiple paths</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Story 
 Recurrent layers were invented back in the 80s by John Hopfield. They formed the basis of artificial associative neural networks (Hopfield net...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Applying recurrent layers to solve multiple paths</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/51b/fc9/89e/51bfc989e70dd48cf5af4a9bebbd9f41.gif" alt="image"><br><br><h3>  Story </h3><br>  Recurrent layers were invented back in the 80s by John Hopfield.  They formed the basis of artificial associative neural networks (Hopfield networks) developed by him.  Today, recurrent networks are widespread in the tasks of processing sequences: natural languages, speech, music, video sequence, and so on. <br><br><h3>  Task </h3><br>  As part of the task of <a href="https://blog.openai.com/learning-a-hierarchy/">hierarchy reinforcement learning,</a> I decided to predict not one agent‚Äôs action, but several, using for this purpose a pre-trained network capable of predicting a sequence of actions.  In this article, I will show how to implement the ‚Äúsequence to sequence‚Äù algorithm for training this very network, and in the following, I will try to tell you how to use it in Q-learning training. <br><a name="habracut"></a><br><h3>  Environment </h3><br>  Imagine a small 2D game world, 5x5 cells.  Each cell will occupy either an object or an empty space. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/5b8/20b/3da/5b820b3da061b1a13c3ed5c5ead5ca2e.png" alt="image"><br>  Before our network we set the task: to issue a sequence of actions from a given set of actions [‚Äúleft‚Äù, ‚Äúright‚Äù, ‚Äúup‚Äù, ‚Äúdown‚Äù, ‚Äútake‚Äù, ‚Äúattack‚Äù]. <br><br>  At the entrance it is necessary to submit the state of our world, consisting of 25 separate cells, each of which can take one value from the set: [‚Äúspace‚Äù, ‚Äúenemy‚Äù, ‚Äúlife‚Äù, ‚Äúsource point‚Äù, ‚Äúdestination point‚Äù]. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f0/870/0d7/5f08700d79b656f638cfdfcdeab6b429.png" alt="image"><br><br>  You can display such a world in the form of a vector of dimension 6 * 25, and then compress the embedded algorithm.  Such a model will be very sensitive to changes in the number of cells and objects in this world. <br><br>  To get rid of such limitations, we can form the input layer as a sequence, where each element of this sequence is one object of our world.  Thus, we will feed the input sequences of different lengths (for different sizes of the simulated world) and in the process of pre-training we will be able to expand the number of objects in our world. <br><br><h3>  Sequence to sequence </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/911/e53/fae/911e53fae2b50d432469b4d503d059a2.png" alt="image"><br>  Sequence to sequence neural networks are two blocks encoder and decoder, and some kind of hidden layer of internal state connecting them. <br><br>  In turn, the encoder consists of a chain of recurrent cells (in the implementation it can be one or several). <br><br>  The most common today recurrent cell (in my subjective opinion) can be called the LSTM (Long short term memory) cell. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/403/5f7/37d/4035f737ddf1b26add27f4f69ccc1483.png" alt="image"><br><br>  Without going into the implementation of LSTM, (I <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">advise you to read more here</a> ), I will briefly describe the principle of its work. <br><br>  Three inputs C, H, X come to the LSTM input of the cell. The input of the ‚Äúconveyor‚Äù C with possible linear modifications of the signal inside the cell.  The first modification is the ‚Äúgate‚Äù. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5a4/60e/a6f/5a460ea6f112de7332ca9584300c6e9c.png" alt="image"><br><br>  By processing the signal from the H and X inputs, the ‚Äúgate‚Äù decides whether the signal coming through the pipeline C is passed. This happens by multiplying the signal C by the value of the sigmoid function with the parameters H, X, W, b. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/830/f08/d8f/830f08d8fff0c45ff6350c8c473b50ba.png" alt="image"><br><br>  The next step is to decide which new information we are going to store in the cell state.  This decision is made in two stages. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c3c/060/77f/c3c06077f1a4e436ac442e0623ac284e.png" alt="image"><br><br>  To begin with, the second sigmoid layer, called the ‚Äúinput gate layer‚Äù, decides which values ‚Äã‚Äãwe will update.  Then the tanh layer creates a vector of candidate values ‚Äã‚Äãof the output layer C, which can be added to the state.  In the next step, the cell combines the signal going through the ‚Äúpipeline‚Äù C, with the resulting one, to create an updated state.  Finally, we need to decide what will be at the H output of our cell.  This conclusion will be based on the state of our cell, but with this one a filter will pass.  First, the cell will pass a signal through the sigmoid layer, which decides which parts of the state of the cell we are going to output.  Then multiply it by the value of the ‚Äúpipeline‚Äù C passing through the tanh function. <br><br>  <a href="https://habrahabr.ru/company/wunderfund/blog/331310/">You can also read about LSTM on Habr√©</a> . <br><br>  Thus, collecting several LSTM cells in a ‚Äúchain‚Äù, we can predict a certain state, based on previous predictions in the chain. <br><br>  There are many techniques that help improve the convergence of such networks, for example, the technique of bidirectional cells.  By arranging the cells in two rows so that one row monitors the state of the previous cell, and the other one follows the state of the cell after it, you can take into account not only the word that was before the predicted one, but also the next one.  Also use ‚Äúaccents‚Äù or attention (attention) to determine the keywords in the proposal. <br><br><h3>  Implementation </h3><br>  I will ‚Äúcollect‚Äù the neural network with TensorFlow and the python language.  Also for this article, I wrote a small class to simulate the world. <br><br>  The first thing to do is to define the input layers: <br><br><pre><code class="python hljs">self.input_data_input = tf.placeholder(tf.int32, [<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>], name=<span class="hljs-string"><span class="hljs-string">'input'</span></span>) self.targets = tf.placeholder(tf.int32, [<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">None</span></span>], name=<span class="hljs-string"><span class="hljs-string">'targets'</span></span>) self.learning_rate_input = tf.placeholder(tf.float32, name=<span class="hljs-string"><span class="hljs-string">'learning_rate'</span></span>) self.target_sequences_length_input = tf.placeholder(tf.int32, (<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>,), name=<span class="hljs-string"><span class="hljs-string">'target_sequences_length'</span></span>) self.max_target_sequences_length = tf.reduce_max(self.target_sequences_length_input, name=<span class="hljs-string"><span class="hljs-string">'max_target_len'</span></span>) self.source_sequences_length_input = tf.placeholder(tf.int32, (<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>,), name=<span class="hljs-string"><span class="hljs-string">'source_sequences_length'</span></span>)</code> </pre> <br>  Next, create an encoder layer. <br><br>  Here it should be said that the <a href="https://www.tensorflow.org/programmers_guide/embedding">embedding mechanism is</a> used to reduce the dimension, the mechanics of its implementation is already present in TensorFlow. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1. Encoder embedding encoder_embed_input = tf.contrib.layers.embed_sequence(input_data_input, vocabulary_size, TF_FLAGS.FLAGS.encoding_embedding_size) # 2. Construct the encoder layer encoder_cell = tf.contrib.rnn.MultiRNNCell([self.make_cell() for _ in range(TF_FLAGS.FLAGS.num_layers)]) enc_output, enc_state = tf.nn.dynamic_rnn(encoder_cell, encoder_embed_input, sequence_length=source_sequences_length_input, dtype=tf.float32)</span></span></code> </pre><br>  We create rnn cell and add them to our network. <br><br><pre> <code class="python hljs">dec_cell = tf.contrib.rnn.LSTMCell(TF_FLAGS.FLAGS.rnn_size, initializer=tf.random_uniform_initializer(<span class="hljs-number"><span class="hljs-number">-0.1</span></span>, <span class="hljs-number"><span class="hljs-number">0.1</span></span>, seed=<span class="hljs-number"><span class="hljs-number">2</span></span>))</code> </pre><br>  More <a href="https://www.youtube.com/watch%3Fv%3DRIR_-Xlbp7s%26t%3D746s">details</a> you can see the video from <a href="https://www.youtube.com/watch%3Fv%3DRIR_-Xlbp7s%26t%3D746s">TFSummit 2017</a> . <br><br>  The output of our subnet will consist of the output (pipeline) of the last RNN cell and its hidden state.  We only need a state. <br>  Go to the decoder. <br><br>  As in the decoder, you need to prepare an embedding layer. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 1. Decoder Embedding target_vocab_size = self.vocabulary_size decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, TF_FLAGS.FLAGS.decoding_embedding_size])) decoder_embed_input = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)</span></span></code> </pre><br>  Next, create the first layer with recurrent cells and project their outputs onto a fully connected perceptron for further classification of the results. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># 2. Construct the decoder layer dec_cell = tf.contrib.rnn.MultiRNNCell([self.make_cell() for _ in range(TF_FLAGS.FLAGS.num_layers)]) # 3. Dense layer to translate the decoder's output at each time # step into a choice from the target vocabulary output_layer = Dense(target_vocab_size, kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))</span></span></code> </pre><br>  The cell outputs are fed to a fully connected layer of the classifier. <br><br>  In the decoder, we will have two branches of the graff: <br><br>  The first branch is for training, the other is for processing final tasks. <br>  For training, we need to remove the last character from the target (those that we want to get at the output of the decoder) sequences and add "GO" at the beginning of each target sequence.  This is necessary, since we will train each cell separately and for each of them it is necessary to send the correct input signal, and not the signal from the neighboring learning cell. <br><br>  An assistant is needed to implement the TensorFlow decoder layer.  In essence, this is an iterator that preprocesses the input data. <br><br>  Create an assistant and a dynamic decoder for learning. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Helper for the training process. Used by BasicDecoder to read inputs. training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=decoder_embed_input, sequence_length=target_sequences_length, time_major=False) # Basic decoder training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, encoder_state, output_layer) # Perform dynamic decoding using the decoder training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, maximum_iterations=max_target_sequences_length)[0]</span></span></code> </pre><br>  We create an assistant and a dynamic decoder for processing final tasks. <br><br><pre> <code class="python hljs">start_tokens = tf.tile(tf.constant([ua.UrbanArea.vacab_go_key], dtype=tf.int32), [TF_FLAGS.FLAGS.batch_size], name=<span class="hljs-string"><span class="hljs-string">'start_tokens'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Helper for the inference process. inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings, start_tokens, ua.UrbanArea.vacab_eos_key) # Basic decoder inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, encoder_state, output_layer) inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder, impute_finished=True, maximum_iterations=max_target_sequences_length)[0]</span></span></code> </pre><br>  Next, add our loss function. <br><br>  For sequences in TensorFlow there is a cross-entropy function, with which we will input the rnn output of the network and training examples to the input. <br><br><pre> <code class="python hljs">training_logits = tf.identity(training_decoder_output.rnn_output, <span class="hljs-string"><span class="hljs-string">'logits'</span></span>) _ = tf.identity(inference_decoder_output.sample_id, name=<span class="hljs-string"><span class="hljs-string">'predictions'</span></span>) <span class="hljs-comment"><span class="hljs-comment"># Create the weights for sequence_loss masks = tf.sequence_mask(self.target_sequences_length_input, self.max_target_sequences_length, dtype=tf.float32, name='masks') with tf.name_scope("optimization"): # Loss function self.cost = tf.contrib.seq2seq.sequence_loss(training_logits, self.targets, masks) tf.summary.scalar("loss", self.cost)</span></span></code> </pre><br>  Gradient descent and Adam optimizer will update the scale values. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Optimizer optimizer = tf.train.AdamOptimizer(self.learning_rate_input) # Gradient Clipping gradients = optimizer.compute_gradients(self.cost) capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None] self.train_op = optimizer.apply_gradients(capped_gradients)</span></span></code> </pre><br>  That's all, it is enough to get several hundred training data from our simulator and start the training session. <br><br> <code>Epoch 1/100 Batch 20/65 Loss: 1.170 Validation loss: 1.082 Time: 0.0039s <br> Epoch 1/100 Batch 40/65 Loss: 0.868 Validation loss: 0.950 Time: 0.0029s <br> Epoch 1/100 Batch 60/65 Loss: 0.939 Validation loss: 0.794 Time: 0.0031s <br> ... <br> Epoch 99/100 Batch 60/65 Loss: 0.136 Validation loss: 0.403 Time: 0.0030s <br> Epoch 100/100 Batch 20/65 Loss: 0.149 Validation loss: 0.430 Time: 0.0037s <br> Epoch 100/100 Batch 40/65 Loss: 0.110 Validation loss: 0.423 Time: 0.0031s <br> Epoch 100/100 Batch 60/65 Loss: 0.153 Validation loss: 0.397 Time: 0.0031s <br></code> <br><br>  As a result, you can get a sequence of steps to walk through our virtual labyrinth. <br><br>  The yellow sequence highlighted by the algorithm, green - the sequence proposed by an artificial neural network. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/058/62c/242/05862c242136fe2b342f1a046e2066dc.png" alt="image"><br><br>  I also added some visualization to the training. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0a5/588/3a6/0a55883a6dfab63afc399f0978395749.png" alt="image"><br><br>  You can see the complete solution in my <a href="https://github.com/Octadero/Traveler">github</a> repository. </div><p>Source: <a href="https://habr.com/ru/post/354220/">https://habr.com/ru/post/354220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../354206/index.html">Start with full stack and not regret: from senior developer to head of department for six years</a></li>
<li><a href="../354208/index.html">Learn OpenGL. Lesson 5.4 - Omnidirectional Shadow Maps</a></li>
<li><a href="../354210/index.html">The announcement of the mitap ThinkPHP # 16 in Kharkov</a></li>
<li><a href="../354214/index.html">Payment Management Privat24 from Google-tables</a></li>
<li><a href="../354216/index.html">Forbes: Alibaba and Uber have become the most profitable companies for investors</a></li>
<li><a href="../354224/index.html">slowpoke is not the fastest database</a></li>
<li><a href="../354226/index.html">Another option for generating thumbnails for images using AWS Lambda & golang + nodejs + nginx</a></li>
<li><a href="../354228/index.html">Hyper-v cluster of two nodes, without external storage or hyperconvergence on the knee</a></li>
<li><a href="../354230/index.html">What is wrong with 3D PDF and eDrawings. How do we replace the 3D viewer in our application</a></li>
<li><a href="../354232/index.html">How not to go crazy with Scrum? Experience a growing project</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>