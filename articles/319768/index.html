<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>In-depth training with the support of a virtual manager in the game against inefficiency</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="About the success of Google Deepmind now know and talk. Algorithms DQN (Deep Q-Network) defeat Man with a good margin of more and more games. The achi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>In-depth training with the support of a virtual manager in the game against inefficiency</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/42c/e79/5f0/42ce795f02824152b9ae3dcd6b0b3504.jpg"><br><p><br>  About the success of Google Deepmind now know and talk.  Algorithms DQN (Deep Q-Network) defeat Man with a good margin of more and more games.  The achievements of recent years are impressive: in literally tens of minutes of learning, the algorithms learn and win a person in Pong and other Atari games.  Recently we‚Äôve come to the third dimension - we win a person in DOOM in real time, as well as learn to drive cars and helicopters. </p><br><p>  DQN was used to teach AlphaGo by playing thousands of games by itself.  When it was not yet fashionable, in 2015, anticipating the development of this trend, the management of Phobos in the person of Alexey Spassky, ordered the Research &amp; Development department to conduct a study.  It was necessary to consider the existing machine learning technologies with a view to the possibility of using them to automate the victory in management games.  Thus, in this article we will discuss the design of a self-learning algorithm in the game of a virtual manager against a living team for increasing productivity. </p><a name="habracut"></a><br><p>  The applied task of analyzing machine learning data classically has the following solution steps: </p><br><ul><li>  problem formulation; </li><li>  data collection; </li><li>  data preparation; </li><li>  formulation of hypotheses; </li><li>  model building; </li><li>  model validation; </li><li>  presentation of results. </li></ul><br><p>  This article will discuss the key decisions in the design of an intelligent agent. <br>  More detailed descriptions of the stages from setting the task to presenting the results will be described in the following articles if the reader is interested.  Thus, it is likely that we will be able to solve the problem of a story about a multidimensional and ambiguous research result without losing clarity. </p><br><h3 id="vybor-algoritma">  Algorithm selection </h3><br><p>  So, to fulfill the task of finding the maximum efficiency of team management, it was decided to use deep learning with reinforcement, namely Q-learning.  The intellectual agent forms the utility function Q of each action available to him on the basis of remuneration or punishment from a transition to a new state of the environment, which gives him the opportunity not to accidentally choose a strategy of behavior, but to take into account the experience of the previous interaction with the gaming environment. </p><br><p>  The main reason for choosing DQN is that the agent doesn‚Äôt need a model to train with this method, nor to select an action.  This is a critical requirement for the learning method for the simple reason that a formalized model of a collective of people with practical predictive power does not yet exist.  However, an analysis of the success of artificial intelligence in logic games shows that the advantages of an approach based on expert knowledge become more pronounced as the environment becomes more complex.  This is found in checkers and chess, where the evaluation of actions based on the model was more successful than Q-learning. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/638/73a/e13/63873ae1351847b68cce5ec70f9597c2.jpg"></div><br><p>  One of the reasons that training with reinforcements still leaves office clerks without work is that the method does not scale well.  The Q-learning agent conducting the research is an active student who must repeatedly apply every action in every situation in order to create his own Q-function for evaluating the benefits of all possible actions in all possible situations. </p><br><div style="text-align:center;"><img src="https://habrastorage.org/files/bf3/3c5/a7b/bf33c5a7b9574d82ba0e5ee1f1fb5fcc.jpg"></div><br><p>  If, as in the old vintage games, the number of actions is calculated by the number of buttons on the joystick, and the states by the ball position, then the agent takes dozens of minutes and hours of training to defeat the person, then in chess and GTA5 the combinatorial explosion already makes the number of combinations of game states and possible actions of space for the passage of the student. </p><br><h3 id="gipoteza-i-model">  Hypothesis and model </h3><br><p>  To effectively use Q-learning to manage a team, we need to minimize the dimension of environmental states and actions. <br>  Our solutions: </p><br><ul><li>  The first engineering solution was to present management activities in the form of a collection of mini-games.  Each of them has discrete numbers of states and actions such that the order of combinations is comparable to successfully solved game problems.  Thus, it is not necessary to build an algorithm that will search for an optimal control strategy in multidimensional space, but many agents are superior to the human player in tactical games.  An example of such a game is task management in YouTrack.  State of the environment (roughly) - time in work and status, and actions - opening, re-opening task, the appointment of a responsible.  More details below. </li></ul><br><p>  An example of an online learning simple game: <br>  <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html">https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html</a> </p><br><ul><li>  Every office employee is an intelligent agent complicating the environment with a variety of his behavior.  Personalization allows you to avoid multi-agent environments, so with one employee one learning agent plays.  In the task-based mini-game, the agent receives an award when an employee works more efficiently (solves problems faster).  If an employee (say, a developer) turns out to be unmanageable for a Q-learning agent, the algorithm will not converge. </li></ul><br><img src="https://habrastorage.org/files/d7b/54f/d0d/d7b54fd0d6cd4956a3286e6286f48e05.jpg"><br><ul><li>  The most important simplification.  In the mini-game on the setting of the problem to the employees - continuous multi-dimensional game states, if only because of the time parameter spent on the task.  And the worst - not obviously determined by the rewards for the actions of the agent.  The gaming environment, as a finite set of states for each mini-game and the calculation of the promotion in a particular state, is 90% formalized by our business logic managers.  This is the most time-consuming and important point, because it is in the formula for expert assessment of states that the expert knowledge is contained, which is an implicit model of the environment and actions, and also determines the amount of remuneration in the agent‚Äôs training.  The success of the agent's training depended on the predictive power of this implicit model. </li></ul><br><img src="https://habrastorage.org/files/191/66e/6e6/19166e6e6fbb47b894bc0f7a26735b9c.jpg"><br><p>  The diagram shows the states of the three game environments for the three agents who manage the work on the task. </p><br><p>  States: </p><br><ul><li>  Task registered (backlog); </li><li>  The task is open; </li><li>  In work; </li><li>  The development is complete, the task is open for testing (QA); </li><li>  Open for testing; </li><li>  Being tested; </li><li>  Ready, tested; </li><li>  Closed. </li></ul><br><p>  The list of actions for each of the three agents is different.  Project Manager - Agent assigns the performer and tester, the time and priority of the task.  The agents working with Dev and QA are personal for each artist and tester.  If there is a transfer of a task further, agents receive rewards, if the task comes back - punishment. </p><br><p>  The greatest reward all agents receive at the closing task.  Also for Q-learning, DF and LF (the factor of discounting and training, respectively) were selected in such a way that the agents were focused on closing the task.  In the general case, the calculation of reinforcement takes place according to the optimal control formula, which takes into account, among other things, the difference in the estimate of time and real costs, the number of task returns, and so on.  The advantage of this solution is its ability to scale to a larger team. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  The iron on which the calculations were performed - GeForce GTX 1080. </p><br><p>  For the above mini-games with the formulation and management of the task in Youtrack, the control functions converged to higher than average values ‚Äã‚Äã(employee productivity increased relative to working with a human manager) for 3 people out of 5. The overall productivity (in hours) almost doubled.  There were no satisfied experimental staff from the test group;  dissatisfied 4;  one abstained from ratings. </p><br><p>  Nevertheless, we made conclusions for ourselves that in order to use the ‚Äúin combat‚Äù method, it is necessary to bring in the model expert knowledge in psychology.  The total duration of development and testing is more than a year. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/319768/">https://habr.com/ru/post/319768/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../319756/index.html">Reaching for milleniali: trends and mobile marketing techniques</a></li>
<li><a href="../319758/index.html">Stripe API version control system as a separate tool</a></li>
<li><a href="../319760/index.html">Terraform, Azure, Irkutsk and another 1207 words about transferring the game to the cloud</a></li>
<li><a href="../319762/index.html">7 services for finding mobile application vulnerabilities</a></li>
<li><a href="../319766/index.html">How to teach at the Yandex Interface Development School, and what I learned there</a></li>
<li><a href="../319770/index.html">Example of restoring PostgreSQL tables using the new mega features pg_filedump</a></li>
<li><a href="../319772/index.html">Spotify: Google Cloud event subsystem migration (part 1)</a></li>
<li><a href="../319774/index.html">IDEA is time to dig in?</a></li>
<li><a href="../319776/index.html">How can a company pay 133 times less taxes</a></li>
<li><a href="../319780/index.html">JetBrains master class on C ++ Russia 2017</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>