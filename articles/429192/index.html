<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>These new tricks are still able to outsmart Deepfake videos.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="For a few weeks, computer science specialist Suiwe L√º [Siwei Lyu] watched the deepfake videos created by his team with agonizing anxiety. These fake m...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>These new tricks are still able to outsmart Deepfake videos.</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/3e4/bbf/13c/3e4bbf13c7ad9c7191ad6c0790554ba6.jpg"><br><br>  For a few weeks, computer science specialist Suiwe L√º [Siwei Lyu] watched the deepfake videos created by his team with agonizing anxiety.  These fake movies, created using a machine learning algorithm, showed celebrities doing things that they wouldn't do.  They seemed to him strangely frightening, and not only because he knew that they were fake.  ‚ÄúThey look wrong,‚Äù he recalls his thoughts, ‚Äúbut it‚Äôs very difficult to pinpoint exactly what makes up such an impression.‚Äù <br><br>  But once a childish memory appeared in his brain.  Like many other children, he played peepers with children.  ‚ÄúI have always lost such competitions,‚Äù he says, ‚Äúbecause when I looked at their unblinking faces, I felt very uncomfortable.‚Äù 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      He realized that these fake films caused him similar discomfort: he lost sight of these movie stars because they did not open and close their eyes with such frequency as real people do. <br><a name="habracut"></a><br>  To find out why, Lui, a professor at the University of Albany, and his team studied every step of the <a href="https://habr.com/post/346478/">DeepFake</a> software that creates these videos. <br><br>  <a href="https://www.deepfakes.club/tutorial/">Deepfake programs</a> take a lot of images of a specific person at the entrance - you, your ex-girlfriend, Kim Jong-un - so that they can be seen from different angles, with different facial expressions, saying different words.  Algorithms learn how this character looks like, and then synthesize the knowledge gained in a video showing how this person does what he never did.  <a href="https://www.theverge.com/2018/9/14/17859188/ai-deepfakes-national-security-threat-lawmakers-letter-intelligence-community">Pornography</a>  <a href="http://www.cs.cmu.edu/~aayushb/Recycle-GAN/">Stephen Colbert</a> speaking the words of John Oliver.  President, <a href="https://www.buzzfeednews.com/article/davidmack/obama-fake-news-jordan-peele-psa-video-buzzfeed">warning</a> about the dangers of fake videos. <br><br>  These videos look convincing for a few seconds on the screen of the phone, but they are (yet) not yet perfect.  They can be seen signs of forgery, for example, in a strange way, constantly open eyes, resulting from the shortcomings of the process of their creation.  Looking into the giblets of DeepFake, Lui realized that among the images from which the program studied, there were not too many photos with closed eyes (you do not save yourself a selfie on which you blink?).  ‚ÄúIt becomes a distortion,‚Äù he says.  Neural network does not understand blinking.  Programs can also miss other ‚Äúphysiological signals inherent in humans,‚Äù says Liu‚Äôs <a href="https://arxiv.org/pdf/1806.02877.pdf">work</a> , which describes this phenomenon ‚Äî breathing at a normal speed, or the presence of a pulse.  And although this study focused on videos created using specific software, it is generally recognized that even a large set of photographs may not be able to adequately describe a person‚Äôs physical perception, so any software trained on these images will be imperfect. <br><br>  Revelation about blinking revealed a lot of fake videos.  But a few weeks after Luy and the team posted a draft of the work online, they received an anonymous letter that contained links to the next fake video posted on YouTube, where the stars opened and closed their eyes in a more normal way.  Fake creators have evolved. <br><br>  And this is natural.  As Liu noted in <a href="http://theconversation.com/detecting-deepfake-videos-in-the-blink-of-an-eye-101072">an interview with</a> The Conversation, ‚Äúyou can add a blink to the deepfake videos by including images with closed eyes in the base, or using training videos.‚Äù  Knowing what a sign of a fake is, one can avoid it - this is ‚Äúonly‚Äù a technical problem.  Which means that the fake videos will get involved in an arms race between creators and recognizers.  Studies like L√ºy‚Äôs work can only complicate the life of fake manufacturers.  ‚ÄúWe are trying to raise the bar,‚Äù he says.  ‚ÄúWe want to complicate this process and make it more time-consuming.‚Äù <br><br>  Because now it is very simple.  Download the program, google photos of celebrities, feed them to the entrance of the program.  She digests them, learns from them.  And although she is not yet completely independent, with a little help she bears and gives birth to something new and fairly realistic looking. <br><br>  ‚ÄúIt‚Äôs very blurry,‚Äù says Lui.  And he does not mean images.  ‚ÄúThis is the line between truth and fake,‚Äù he clarifies. <br><br>  This is both annoying and not surprising to anyone who has recently been alive, and sat on the Internet.  But this is especially worried about the military and intelligence agencies.  In particular, this is why Liu‚Äôs research, like some other works, is funded by a program from <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D0%25BF%25D1%2580%25D0%25B0%25D0%25B2%25D0%25BB%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BF%25D0%25B5%25D1%2580%25D1%2581%25D0%25BF%25D0%25B5%25D0%25BA%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D1%258B%25D1%2585_%25D0%25B8%25D1%2581%25D1%2581%25D0%25BB%25D0%25B5%25D0%25B4%25D0%25BE%25D0%25B2%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BB%25D1%258C%25D1%2581%25D0%25BA%25D0%25B8%25D1%2585_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B5%25D0%25BA%25D1%2582%25D0%25BE%25D0%25B2_%25D0%259C%25D0%25B8%25D0%25BD%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D1%2580%25D1%2581%25D1%2582%25D0%25B2%25D0%25B0_%25D0%25BE%25D0%25B1%25D0%25BE%25D1%2580%25D0%25BE%25D0%25BD%25D1%258B_%25D0%25A1%25D0%25A8%25D0%2590">DARPA</a> called MediFor - Media Forensics [media forensics]. <br><br>  The MediFor project was launched in 2016, when the agency noticed an increase in the activity of fake manufacturers.  The project is trying to create an automated system that studies three levels of counterfeit signs and provides an outstanding "assessment of the reality" of an image or video.  At the first level, dirty digital traces are looked for - noise from the camera of a particular model or compression artifacts.  The second level is physical: not the lighting on the face, the reflection does not look like it should look like with this lamp arrangement.  The latter is semantic: a comparison of data with verified real information.  If, for example, it is argued that the video that captured the game of football was filmed in Central Park at 14 o'clock on Tuesday, October 9, 2018, does the sky condition coincide with the weather archive?  Put all the levels together and get an estimate of the reality of the data.  DARPA hopes that by the end of MediFor, prototypes of systems will appear that can be used for large-scale testing. <br><br>  However, the watch is ticking (or is it just a repetitive sound created by an AI trained on data related to time tracking?) "In a few years, you will be able to face such a thing as fabrication of events," says Durpa‚Äôs program manager, Mat Turek.  ‚ÄúNot just a single image or edited video, but several images or videos trying to convey a convincing message.‚Äù <br><br>  In the Los Alamos National Laboratory, an informatics specialist Juston Moore imagines a potential future more vividly.  Suppose: we inform the algorithm that we need a video in which Moore robs the pharmacy;  we introduce it in the video of the security system of this institution;  send him to jail.  In other words, he is concerned that if the standards for verifying evidence will not (or will not) develop in parallel with the manufacture of fakes, it will be easy to expose people.  And if the courts cannot rely on visual data, it may turn out that real evidence will be ignored. <br><br>  We come to the logical conclusion that seeing once will be no better than hearing a hundred times.  ‚ÄúIt may happen that we will not trust any photographic evidence,‚Äù he says, ‚Äúbut I don‚Äôt want to live in such a world.‚Äù <br><br>  Such a world is not so incredible.  And the problem, according to Moore, extends far beyond replacing faces.  ‚ÄúAlgorithms can <a href="https://www.theverge.com/2017/10/30/16569402/ai-generate-fake-faces-celebs-nvidia-gan">create</a> images of people who do not belong to real people, they can strangely alter images, turning a <a href="https://junyanz.github.io/CycleGAN/">horse into a zebra</a> ,‚Äù says Moore.  They can <a href="https://research.adobe.com/stitch-together-a-perfect-scene-with-experimental-tool/">delete</a> parts of images and <a href="https://research.adobe.com/cloak-remove-unwanted-objects-in-video/">remove</a> foreground objects <a href="https://research.adobe.com/cloak-remove-unwanted-objects-in-video/">from video</a> . <br><br>  Maybe we will not be able to deal with fakes faster than they will do.  But maybe it will work out - and this opportunity provides motivation for Moore‚Äôs team on the study of digital evidence study methods.  The Los Alamos program, which combines knowledge of cyber systems, information systems, theoretical biology and biophysics, is about a year younger than DARPA.  One approach focuses on ‚Äúcompressibility‚Äù, on cases where the image contains not so much information as it seems.  ‚ÄúIn essence, we are repelled by the idea that all AI image generators have a limited set of things that they can create,‚Äù says Moore.  ‚ÄúSo even if the image seems rather complicated to me, you can find a repetitive structure in it.‚Äù  When re-processing the pixels there is not too much of everything. <br><br>  They also use sparse coding algorithms to play the game with matches.  Suppose we have two collections: a bunch of real images, and a bunch of artificially created AI images.  The algorithm studies them, creating what Moore calls the ‚Äúvocabulary of visual elements,‚Äù noting that the artificial pictures have a common thing, and what real images have in common.  If Moore's friend retweets the image of Obama, and Moore considers that the image was made using AI, he can drive it through the program and find out which of the dictionaries it will be closer to. <br><br>  Los Alamos, where one of the world's most powerful supercomputers is located, downloads resources to this program not only because someone can substitute Moore through a fake robbery.  The mission of the laboratory is ‚Äúto solve problems of national security with the help of scientific excellence‚Äù.  Their central task is nuclear safety, ensuring that bombs do not explode when they should not, and will explode when they should (please do not), as well as help in non-proliferation.  All this requires a general knowledge of machine learning, since it helps, as Moore says, ‚Äúto draw big conclusions from small data sets‚Äù. <br><br>  But besides all this, businesses like Los Alamos should be able to believe their eyes ‚Äî or know when they don‚Äôt need to be trusted.  Because what if you see satellite images of how a country mobilizes or tests nuclear weapons?  What if someone forges sensor readings? <br><br>  This is a frightening future that the work of Moore and Liu should ideally avoid.  But in a world where everything is lost, to see doesn‚Äôt mean to believe, and seemingly absolute measurements can be fake.  Everything digital is in doubt. <br><br>  But maybe ‚Äúdoubt‚Äù is the wrong word.  Many people will take fakes at face value (remember the shark <a href="https://www.snopes.com/fact-check/shark-street-hurricane/">photo</a> in Houston?), Especially if the content matches their beliefs.  ‚ÄúPeople will believe in what they tend to believe,‚Äù says Moore. <br><br>  The likelihood of this is higher for the general public watching the news than in the area of ‚Äã‚Äãnational security.  And in order to prevent the spread of misinformation among us, simpletons, DARPA offers cooperation to social networks, in an attempt to help users determine that the credibility of the video where Kim Jong-un is dancing macaroons is rather low.  Turek points out that social networks can spread the story that disproves the video as quickly as the video itself. <br><br>  But will they do it?  Exposing is a <a href="https://slate.com/health-and-science/2018/01/weve-been-told-were-living-in-a-post-truth-age-dont-believe-it.html">troublesome</a> process (although not as <a href="https://www.wnyc.org/story/walking-back-backfire-effect/%3FhootPostID%3Dc331752391fae9c049ca079f23088ff3">ineffective</a> as rumor says).  And people need to truly show the facts before they can change their opinion about fiction. <br><br>  But even if no one can change the opinion of the masses about the truthfulness of the video, it is important that people who make political and legal decisions - about who is carrying rockets or killing people - try to use machines to separate the obvious reality from the AI ‚Äã‚Äãsleep. </div><p>Source: <a href="https://habr.com/ru/post/429192/">https://habr.com/ru/post/429192/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../429182/index.html">Artificial Intelligence rested on the barrier of understanding</a></li>
<li><a href="../429184/index.html">Online media predictably broke off about Oumuamua and Harvard scientists</a></li>
<li><a href="../429186/index.html">Selection @pythonetc, October 2018</a></li>
<li><a href="../429188/index.html">PICASO 3D Designer XL Review</a></li>
<li><a href="../429190/index.html">We make our own implant for electronics</a></li>
<li><a href="../429194/index.html">7 libraries for Android development at Kotlin</a></li>
<li><a href="../429196/index.html">iOS runtime mobile exploration with Objection, or Hack own application</a></li>
<li><a href="../429198/index.html">The Kernel-Bridge Framework: Ring0 Bridge</a></li>
<li><a href="../429200/index.html">British scientists have launched a supercomputer with 1 million cores, which simulates the human brain</a></li>
<li><a href="../429202/index.html">Expensive courses: is it worth it?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>