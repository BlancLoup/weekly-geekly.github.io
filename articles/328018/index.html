<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Work with real-time logging with Heka. Experience Yandex. Money</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this article I will talk about how a system for collecting and delivering server logs of payment services is organized in Yandex.Money. I have been...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Work with real-time logging with Heka. Experience Yandex. Money</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/web/787/396/a88/787396a884f7479388e02ea2125fdadc.png" alt="image alt text"></p><br><p>  In this article I will talk about how a system for collecting and delivering server logs of payment services is organized in Yandex.Money.  I have been shipping logs for the entire last year, and during this time I have accumulated enough nuances to share my experience with the public. </p><br><p>  The system is based on the EHK stack (Elasticsearch / Heka / Kibana), with an eye on working in near real time.  I will place particular emphasis on the thin spots and nuances of processing billions of lines of text per day. <a name="habracut"></a></p><br><blockquote>  I believe in three things: monitoring, logs and backups. </blockquote><p>  For quick access to the operational logs in Yandex.Money, <strong>ElasticSearch is</strong> used, receiving data from almost hundreds of microservices that make up the payment service. </p><br><p>  By ‚Äúoperational‚Äù, I mean logs that are available in real time: the delay in delivery of new records from the log file to the Elasticsearch cluster is now less than a second.  The monitoring service always knows the exact status of the service at any time, and developers can quickly evaluate and correct the behavior of the new version of each microservice immediately after release. </p><br><p>  But it was not always so.  When I came to the company five months ago, I was assigned the task of adjusting the work of delivering operational service logs to the <strong>ElasticSearch</strong> cluster (hereinafter <strong>referred</strong> to as ES).  At that time, four schemes for their analysis and delivery to ES were used: </p><br><ul><li><p>  Heka ‚Üí TCP output ‚Üí Heka ‚Üí ES. </p><br></li><li><p>  Heka ‚Üí Apache Kafka ‚Üí Apache Flume ‚Üí ES. </p><br></li><li><p>  SyslogNg ‚Üí ES. </p><br></li><li>  nxlog ‚Üí nxlog ‚Üí es. </li></ul><br><p>  Almost all of them worked imperfectly: the Kafka cluster was unreliable, Flume periodically hung, causing ES to fall into a stupor. </p><br><blockquote>  Logs are actually a very simple thing: a lot of files that are quickly growing on the combat servers in volumes.  Therefore, simple solutions in this case are the most reliable. </blockquote><p>  Since ‚Äúthe more joints in the umbrella are, the higher the probability of its failure‚Äù, I threw away all the non-working and unnecessarily complex schemes and stopped on one in which Heka would perform the role of a processor and transport. </p><br><p><img src="https://habrastorage.org/web/f69/955/672/f69955672a114c77b269eba8bde61210.png" alt="image alt text"></p><br><p>  <em>Heka <strong>picks up logs and sends them to another Heka via TCP for further forwarding to ElasticSearch Cluster</strong> .</em> </p><br><p>  The idea is the following: Heka is installed on each server, the logs of all services of this server are packed into the Protobuf binary protocol and sent to the Heka receiving server for parsing, filtering and sending to ES. </p><br><h1 id="pochemu-ne-klassicheskiy-stek-elk-i-pochemu-deprecated-heka-a-ne-logstash">  Why not the classic ELK stack and why deprecated Heka, not Logstash </h1><br><p>  In terms of configuration and software, our ES cluster looks like this: </p><br><ul><li><p>  ElasticSearch 2.2. </p><br></li><li><p>  Kibana 4.4.2. </p><br></li><li><p>  Two master nodes: Intel Xeon 2x E5-2660, 64 GB of RAM, 2x 146 GB of RAID-10. </p><br></li><li><p>  Client node with Kibana installed: Intel Xeon 2xE5-2660, 64 GB of RAM, 2x146 GB of RAID-10. </p><br></li><li>  Four data nodes: Intel Xeon 2x E5-2640 v3;  512 GB of RAM, 3x16TB of RAID-10. </li></ul><br><p>  All ES nodes are in the same local network and are connected to the same router, which allows communication within the cluster using the transport protocol at maximum speed.  Such an organization significantly speeds up the placement of indices and the rebalance of the cluster. </p><br><p>  In the modern world, the Elasticsearch / Logstash / Kibana stack has become practically the de facto standard for working with logs.  And if there are no objections against Elasticsearch and Kibana, then there is one nuance with Logstash - it is created on jRuby (written in Java by the Ruby language interpreter) and requires JVM.  Given that Yandex.Money is a multitude of microservices hosted on hundreds of physical servers and virtual containers, it would be wrong to put heavy Logstash and JAVA on each of them.  Heka chose the choice because of its simplicity, reliability, ease, ability to filter passing messages and excellent data buffering. </p><br><p>  As for the status of the product (deprecated) - for us it is not an argument.  Bow and arrows for military purposes are also deprecated, but this does not prevent you from shooting someone in the head with a guaranteed result.  If you need, for example, a non-standard plug-in or a processor, then a whole staff of experienced developers will help to refine the product. </p><br><p>  But all this was a theory, and in the transition to practice problems began. </p><br><h1 id="dyavol-pryatalsya-v-obeme-dannyh">  The devil was hiding in the amount of data </h1><br><p>  Given the financial specifics of our work, almost all services write a lot of different information in the logs.  For example and understanding of scale: the volume of logs of some of the components of the system reaches <strong>250 thousand lines per minute</strong> . </p><br><p>  Not a single Heka, on whatever powerful hardware it may be, alone such a volume will not process - performance sagging and data loss are inevitable.  Neither, of course, is totally unacceptable, so <strong>HAProxy</strong> comes to the rescue.  The final scheme was as follows: </p><br><p><img src="https://habrastorage.org/web/e9c/6ff/196/e9c6ff196a8c49979c60c224e31a89ee.png" alt="image alt text"></p><br><p>  <em>The diagram shows the general direction of traffic logs from Heka-sources to a bunch of HAProxy + Heka.</em> </p><br><p>  On each server there is one Heka, collecting logs of microservices of this server.  Data is collected, packaged in Protobuf and sent via TCP to a load balancer serving the data center.  The backends are HAProxy, located directly on the ES data nodes, behind which are <em>Heka</em> pools.  In turn, they receive data, repack it in ESJson and send it to a local data node via HTTP. </p><br><h1 id="-i-v-raznyh-formatah-log-faylov">  ... and in different log file formats </h1><br><p>  Despite the fact that the main language in the company is Java and the logs are output via the standard log4j library, there was no single accepted format at the time of building the ‚Äúdream cluster‚Äù.  Each microservice wrote logs of its own type, including variations in the set of output fields and date-time formats.  I did not want to wait for the development to bring the logs to the common formats, so the movement towards the goal was parallel.  Simultaneously with the analysis of the existing log formats, tasks were set up for revision, and as new versions were released with the correct formats, the settings of the collectors changed. </p><br><h1 id="pereydem-k-samomu-soku--nyuansam-nastroyki">  Let's go to the very juice - the nuances of setting </h1><br><p>  The behavior of Heka is described by <a href="https://github.com/toml-lang/toml">.toml</a> files that are assembled into a single configuration at startup, and depending on the blocks described, Heka builds a data processing model. </p><br><p>  Each unit goes through several stages: </p><br><ul><li><p>  Input - input stream (this can be a file, TCP / UDP input, data read from Kafka, container Docker events, etc.). </p><br></li><li><p>  Splitter - here we indicate the beginning and end of each data block in the stream.  Mandatory step for multiline data like Java Stacktrace. </p><br></li><li><p>  Decoder - describes the rules for decoding incoming data.  We mainly use Regex decoders. </p><br></li><li><p>  Filters - the stage of filtering and changing data. </p><br></li><li><p>  Encoders - encoding data stream for recipient format. </p><br></li><li>  Output - here we describe how and where the data should go. </li></ul><br><p>  All these stages are just plugins on <strong>Go</strong> and <strong>Lua</strong> .  If necessary, you can write something of your own.  For example, a plugin filter on Lua, which will cut off sending monitoring service requests to the ES;  or cut confidential data from logs. </p><br><blockquote>  <strong>Heck</strong> in ancient Egyptian mythology - the god of magic.  And what Heka allows you to do with logs is simply magical. </blockquote><br><h2 id="parametry-servera-istochnika-logov">  Log Source Server Settings </h2><br><p>  Let us analyze the Heka configuration using the example of the source log server and the <strong>service.toml</strong> file. </p><br><pre><code class="bash hljs">[money-service-log] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"LogstreamerInput"</span></span> log_directory = <span class="hljs-string"><span class="hljs-string">"/var/log/tomcat"</span></span> file_match = <span class="hljs-string"><span class="hljs-string">"money-service.log"</span></span> splitter = <span class="hljs-string"><span class="hljs-string">"RegexSplitter"</span></span> decoder = <span class="hljs-string"><span class="hljs-string">"service_decoder"</span></span></code> </pre> <br><p>  The simplest case is a single file, rotation takes place by system means.  If there are many files and they differ in formats, then each pair of input / decoder should be described.  For a more detailed description it is better to refer to the official <a href="http://hekad.readthedocs.io/en/v0.10.0/pluginconfig/logstreamer.html">documentation</a> .  <strong>If something remains unclear - be sure to ask in the comments.</strong> </p><br><pre> <code class="bash hljs">[RegexSplitter] delimiter = <span class="hljs-string"><span class="hljs-string">'\n(\[\d\d\d\d-\d\d-\d\d)'</span></span> delimiter_eol = <span class="hljs-literal"><span class="hljs-literal">false</span></span></code> </pre> <br><p>  Since the logs can be multi-line (the same stacktraces), do not forget about RegexSplitter, which makes Heka understand where one block of text ends and another begins. </p><br><pre> <code class="bash hljs">[service_decoder] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"PayloadRegexDecoder"</span></span> match_regex = <span class="hljs-string"><span class="hljs-string">'^\[(?P&lt;timestamp&gt;\d{4}-\d{2}-\d{2}T[\d:.\+]+\])\s+(?P&lt;level&gt;[AZ]+)\s+\[(?P&lt;thread&gt;.*)\]\s+\[(?P&lt;context&gt;\S*)\]\s+\[(?P&lt;traceid&gt;\S*)\]\s+\[(?P&lt;unilabel&gt;\S*)\]\s\[(?P&lt;class&gt;\S+)\]\s-\s(?P&lt;msg&gt;.*)'</span></span> log_errors = <span class="hljs-literal"><span class="hljs-literal">true</span></span> [service_decoder.message_fields] @timestamp = <span class="hljs-string"><span class="hljs-string">"%timestamp%"</span></span> level = <span class="hljs-string"><span class="hljs-string">"%level%"</span></span> thread = <span class="hljs-string"><span class="hljs-string">"%thread%"</span></span> context = <span class="hljs-string"><span class="hljs-string">"%context%"</span></span> traceid = <span class="hljs-string"><span class="hljs-string">"%traceid%"</span></span> unilabel = <span class="hljs-string"><span class="hljs-string">"%unilabel%"</span></span> class = <span class="hljs-string"><span class="hljs-string">"%class%"</span></span> msg = <span class="hljs-string"><span class="hljs-string">"%msg%"</span></span></code> </pre> <br><p>  In <strong>match_regex, we</strong> describe log lines with a regular expression in the Go standard.  Regular expressions in Go are <em>almost</em> identical to the standard PCRE, but there are a number of nuances because of which Heka may refuse to start.  For example, some PCRE implementations will forgive this syntax: </p><br><pre> <code class="bash hljs">(?&lt;groupname&gt;.*)</code> </pre> <br><p>  But GOLANG - not forgive. </p><br><p>  Using the <strong>log_errors</strong> parameter, <strong>we</strong> collect all errors in a separate log - they will be needed later. </p><br><pre> <code class="bash hljs">[ServiceOutput] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"TcpOutput"</span></span> address = <span class="hljs-string"><span class="hljs-string">"loadbalancer.server.address:port"</span></span> keep_alive = <span class="hljs-literal"><span class="hljs-literal">true</span></span> message_matcher = <span class="hljs-string"><span class="hljs-string">"Logger == 'money-appname'"</span></span> use_buffering = <span class="hljs-literal"><span class="hljs-literal">true</span></span> [ServiceOutput.buffering] max_file_size = 100857600 max_buffer_size = 1073741824 full_action = <span class="hljs-string"><span class="hljs-string">"block"</span></span></code> </pre> <br><p>  Downstream buffering is one of the great Heka features.  By default, it stores the output buffer in the following path: </p><br><pre> <code class="bash hljs">/var/cache/hekad/output_queue/OutputName</code> </pre> <br><p>  In the settings, we limit the size of each buffer file with a capacity of 100 MB, and also set the total cache size for each Output-module to 1 GB.  The <strong>full_action</strong> parameter can take three values: </p><br><ul><li><p>  <strong>shutdown</strong> - if the buffer overflows, Heka stops; </p><br></li><li><p>  <strong>drop</strong> - when the buffer overflows, it starts working as a stack, deleting old messages in the queue; </p><br></li><li>  <strong>block</strong> - when the buffer is full, Heka suspends all operations and waits until it is possible to send data. </li></ul><br><p>  With block, you are guaranteed not to lose a single line of log.  The only disadvantage is that if the connection is broken or the channel is degraded, you will get a sharp jump in traffic when the connection is resumed.  This is due to the fact that Heka sends the accumulated buffer, trying to return to processing in real time.  The receiving pool needs to be designed with a margin, necessarily considering the possibility of such situations, otherwise you can easily turn the DDoS yourself. </p><br><p>  By the way, about the use of double and single quotes in the Heka configuration - the following is implied: </p><br><ul><li><p>  Variable values ‚Äã‚Äãin single quotes by default are treated as <strong>raw string</strong> .  Special characters do not need to be escaped. </p><br></li><li>  The values ‚Äã‚Äãof variables in double quotes are treated as plain text and require double escaping when using regular expressions in them. </li></ul><br><p>  This nuance at one time spoiled a lot of blood for me. </p><br><h2 id="konfiguraciya-bekenda">  Backend configuration </h2><br><p>  The backend for the balancer is a bundle of HAProxy and three copies of Heka on each ES data node. </p><br><p>  In HAProxy, everything is quite simple and it seems to me that it does not require explanations: </p><br><pre> <code class="bash hljs">listen pool_502 <span class="hljs-built_in"><span class="hljs-built_in">bind</span></span> 0.0.0.0:502 balance roundrobin default-server fall 5 inter 5000 weight 10 server heka_1502 127.0.0.1:1502 check server heka_2502 127.0.0.1:2502 check server heka_3502 127.0.0.1:3502 check</code> </pre> <br><p>  There are three Heka instances running on each server, differing only in ports: </p><br><pre> <code class="bash hljs">[Input_502_1] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"TcpInput"</span></span> address = <span class="hljs-string"><span class="hljs-string">"0.0.0.0:1502"</span></span> keep_alive = <span class="hljs-literal"><span class="hljs-literal">true</span></span> keep_alive_period = 180 decoder = <span class="hljs-string"><span class="hljs-string">"MultiDecoder_502"</span></span> [MultiDecoder_502] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"MultiDecoder"</span></span> subs = [<span class="hljs-string"><span class="hljs-string">'Service1Decoder'</span></span>, <span class="hljs-string"><span class="hljs-string">'Service2Decoder'</span></span>] cascade_strategy = <span class="hljs-string"><span class="hljs-string">"first-wins"</span></span> log_sub_errors = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  In the configuration, MultiDecoder is used, since many services logs pass through one port.  The first-wins policy means that after the first match, the further search of decoders stops. </p><br><pre> <code class="bash hljs">[Service1Decoder] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"ProtobufDecoder"</span></span> [Service2Decoder] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"ProtobufDecoder"</span></span> [Service1Encoder] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"ESJsonEncoder"</span></span> index = <span class="hljs-string"><span class="hljs-string">"service1-logs-%{%Y.%m.%d}"</span></span> es_index_from_timestamp = <span class="hljs-literal"><span class="hljs-literal">false</span></span> type_name = <span class="hljs-string"><span class="hljs-string">"%{Logger}"</span></span> fields = [ <span class="hljs-string"><span class="hljs-string">"DynamicFields"</span></span>, <span class="hljs-string"><span class="hljs-string">"Payload"</span></span>, <span class="hljs-string"><span class="hljs-string">"Hostname"</span></span> ] dynamic_fields = [<span class="hljs-string"><span class="hljs-string">"@timestamp"</span></span>, <span class="hljs-string"><span class="hljs-string">"level"</span></span>, <span class="hljs-string"><span class="hljs-string">"thread"</span></span>, <span class="hljs-string"><span class="hljs-string">"context"</span></span>, <span class="hljs-string"><span class="hljs-string">"traceid"</span></span>, <span class="hljs-string"><span class="hljs-string">"unilabel"</span></span>, <span class="hljs-string"><span class="hljs-string">"class"</span></span>, <span class="hljs-string"><span class="hljs-string">"msg"</span></span>] [Service1Encoder.field_mappings] Payload = <span class="hljs-string"><span class="hljs-string">"message"</span></span> Hostname = <span class="hljs-string"><span class="hljs-string">"MessageSourceAddress"</span></span></code> </pre> <br><p>  The <strong>es_index_from_timestamp</strong> parameter <strong>is</strong> needed to indicate that the date and time for generating the index name is taken not from the incoming data, but from the server local time.  It allows you to avoid mess when servers are working in different time zones and someone writes logs in UTC, and someone in MSK. </p><br><p>  The <strong>index</strong> parameter implements the principle of ‚Äúone service - one index‚Äù; a new index is created every day. </p><br><pre> <code class="bash hljs">[Service1Output] <span class="hljs-built_in"><span class="hljs-built_in">type</span></span> = <span class="hljs-string"><span class="hljs-string">"ElasticSearchOutput"</span></span> message_matcher = <span class="hljs-string"><span class="hljs-string">"Logger == 'money-service1'"</span></span> server = <span class="hljs-string"><span class="hljs-string">"http://localhost:9200"</span></span> encoder = <span class="hljs-string"><span class="hljs-string">"Service1Encoder"</span></span> use_buffering = <span class="hljs-literal"><span class="hljs-literal">true</span></span></code> </pre> <br><p>  Output plugins parse the data stream based on the <strong>message_matcher</strong> parameter corresponding to the log file name.  In order not to overload the network, Heka sends data to the local data node on which it is installed.  And already further ES itself sends the indexed data on the transport protocol between the cluster data-nodes. </p><br><h1 id="zaklyuchenie">  Conclusion </h1><br><p>  The scheme described above works successfully and indexes 25-30 thousand records per second.  The safety factor of Heka receiving pools makes it possible to withstand load peaks of up to 100 thousand records / s: </p><br><p><img src="https://habrastorage.org/web/db0/df4/b84/db0df4b8480641d3954f79d669d9e2ec.jpg" alt="image alt text"></p><br><p>  <em>Statistics from <strong>Zabbix</strong> .</em> </p><br><p>  In ElasticSearch we keep logs for the last 21 days.  Experience shows that online access is rarely needed for older data.  But if you need them, you can always request them from the log servers that store data almost forever. </p><br><p><img src="https://habrastorage.org/web/7d4/de0/1cc/7d4de01cc1c54f8b873dba1531d7d182.jpg" alt="image alt text"></p><br><p>  <em>The current state of the cluster according to Kopf.</em> </p><br><p>  I described only the part of the system that relates to the collection and delivery of logs, so in the next article I am going to tell you about the ElasticSearch cluster itself and its configuration.  I think to tell you how we virtualized it, how we moved from version 2.2 to 5.3 and transported 24 billion records with us, without losing faith in humanity. </p><br><p>  <strong>Maybe add to the story something else, some overlooked details?</strong>  <strong>Share your opinion in the comments.</strong> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/328018/">https://habr.com/ru/post/328018/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../328006/index.html">‚ÄúThe incident with Gitlab is a very good and revealing story‚Äù, - Alexey Lesovsky about PostgreSQL administration</a></li>
<li><a href="../328008/index.html">UAC Bypass or the story of three escalations</a></li>
<li><a href="../328010/index.html">Bildim under stm32duino using CMake (and scavenging from the linker)</a></li>
<li><a href="../328012/index.html">Redux as the heart of the front-end architecture of the Unified Frontal System</a></li>
<li><a href="../328014/index.html">Build 2017: text translation. Day 1</a></li>
<li><a href="../328022/index.html">Interactive 3D-installation based on "Star Wars"</a></li>
<li><a href="../328024/index.html">Video: Uptime Day, conference about monitoring and 24/7 support</a></li>
<li><a href="../328026/index.html">Mobilization without "headache"</a></li>
<li><a href="../328032/index.html">Vector and Citrix will launch virtual machines into space [literally]</a></li>
<li><a href="../328034/index.html">Volitronic can "extend" the law of Moore</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>