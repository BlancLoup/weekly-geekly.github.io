<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ML boot camp 2016 new to TOP 10</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Not so long ago, the machine learning competition from Mail.ru ended. I took the 9th place, and, in fact, I would like to share with you how it turned...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ML boot camp 2016 new to TOP 10</h1><div class="post__text post__text-html js-mediator-article">  Not so long ago, the machine learning <a href="http://mlbootcamp.ru/">competition</a> from Mail.ru ended.  I took the 9th place, and, in fact, I would like to share with you how it turned out for me.  In short, lucky. <br><br><img src="https://habrastorage.org/files/167/88f/ce3/16788fce39974ae8950b32651be8622a.png"><br><a name="habracut"></a><br>  And so, a little about my experience: <br><br>  1) In general, I am a PHP programmer, and I know very little about python, only by participating in the <a href="http://russianaicup.ru/">Russian AI Cup 2015</a> , where I first wrote on python and won a T-shirt; <br>  2) At work, I solved one problem to determine the tonality of the text, using scikit-learn; <br>  3) Began to take a course on coursera.org from Yandex, on machine learning. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This is where my experience with python and machine learning ends. <br><br><h3>  The beginning of the competition </h3><br><div class="spoiler">  <b class="spoiler_title">Task</b> <div class="spoiler_text">  It is necessary to teach a computer to predict how many seconds two matrices of sizes mÔΩòk and kÔΩòn will multiply on a computer system, if it is known how long this problem was solved on other computer systems with different matrix sizes and other system and hardware parameters. <br><br>  A set of features describing a separate computational experiment is given: the parameters of the problem (m, k, n) and the characteristics of the computing system on which the algorithm was run and the computation time.  And the test sample for which you need to predict the time. <br><br>  As a quality criterion for solving a problem, the smallest average relative error is used for implementations operating for more than one second: <br><div style="text-align:center;"><img src="https://habrastorage.org/files/3c9/715/70d/3c971570dd2847c8b88d9d851855fee6.png" alt="image"></div><br><br></div></div><br>  I put myself python3 and notebook, and began to deal with pandas from the <a href="http://mlbootcamp.ru/article/tutorial/">tutorial</a> .  I uploaded data, fed random woods, and got ~ 0.133 at the output <br><br>  Everything, on this thought about the solution of the problem ended. <br><br><h3>  Studying the subject and collecting additional information </h3><br>  I return to the task, clinging to the line ‚Äú <i>We have learned the idea of ‚Äã‚Äãthe problem from the work of A. A. Sidnev, V. P. Gergel</i> ‚Äù.  Actually, the book describes how to solve this problem.  True, I did not understand how I could implement this idea, but I saw an interesting graph: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/279/9ad/43c/2799ad43c4e7467d4f13016a7321d9df.png" alt="image"></div><br>  Hmm, but really, the dependence of time on the size of the matrices should be linear, I thought, and decided to see how things are with this with our data.  Having grouped all the examples according to their characteristics, I found that there are only 92 groups.  Having actually built graphs for each of the options, I saw that the half had a linear relationship with low emissions.  And the other half also showed a linear relationship, but with a strong dispersion.  There are many graphs, so I‚Äôll give you just three examples. <br><br>  With variance: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a82/4b2/1e4/a824b21e4d3f5c6f65d878346eb657f1.png" alt="image"></div><br>  Linear: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ef1/44e/ad6/ef144ead6322e491d695d92feb280cbb.png" alt="image"></div><br>  With very strong dispersion: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/584/190/f70/584190f70fed4ac7cb43a241f3e19314.png" alt="image"></div><br>  The last schedule was divided into a separate group, because on cross-validation, it showed poor results.  I decided to do something special for this occasion (but in the end I did not do anything special). <br><br><h3>  Work with data </h3><br>  Actually, I built linear regressions for each group, and got ~ 0.076.  Here I thought that I had found the key to solving the problem, and began to adapt to the graphics.  I tried almost all the regression models that were in scikit-learn (yes, there is no knowledge, so I solved the problem using the scientific method) the result was not particularly improved. <br><br>  Even implemented a polynomial.  I ran through all the groups, and with the help of GridSearchCV I looked for the best parameters for each group.  I noticed that in some groups it does not bend at all as I would like.  Started working with data.  First, I noticed that there are lines with missing memory data (memtRFC, memFreq, memType).  Using just logical inferences, I restored this data.  For example, there was data with memtRFC equal to 'DDR-SDRAM PC3200' and 'DDR-SDRAM PC-3200'.  Obviously, this is the same thing. <br><br>  I hoped that this would reduce the number of groups, but it did not happen.  Next, I started working with emissions.  For good, you would have to write a method that automatically determines outliers, but I did everything with my hands.  Drew charts for all groups, and visually determined outliers, and eliminated these points. <br><br><h3>  Cross validation </h3><br>  After all this, I ran into the problem that my tests show a result of 0.064, but in fact 0.073.  Reassigned apparently.  I wrote a class wrapper with the fit and predict methods within which I split the data into groups, trained the models for each group, and predicted the same for each group.  This allowed me to use cross-validation.  Actually after that the result of my tests and the loaded data was always very close. <br><br>  It looked like this: <br><br><pre><code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">MyModel</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">pass</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">fit</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, X, Y)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#     x,      ,        def predict(self, X): #     x,      ,       def get_params(self, deep=False): return {}</span></span></code> </pre> <br>  Now with the help of cross_validation.cross_val_score I could test my approach qualitatively. <br><br>  Another small increase was given to me by working with data outside the conditions of the problem.  By condition, Y cannot be less than one.  Looking at the statistics, I saw that the minimum Y = 1.000085, and my predictions gave a result of less than 1. Not much, but there were some.  I solved this problem again at random.  The result was the following formula: <br><br><pre> <code class="python hljs">time_new = <span class="hljs-number"><span class="hljs-number">1</span></span> + pow(time_predict/<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>)/<span class="hljs-number"><span class="hljs-number">10</span></span></code> </pre> <br>  At some point, I realized that it was necessary not just to solve an abstract problem, but to solve a concrete one, that is, one should strive not to accurately predict time, but to minimize the error.  This means that if the actual running time of the script is 10 seconds, and I was mistaken for 2, then my error is | 10-8 | / 10 = 0.2.  And, if the time is really 2 seconds, and I was wrong by 0.1, then | 2-0.1 | / 2 = 0.95. <br><br>  The difference is obvious.  When I realized this, and for some reason this did not happen immediately, I decided to increase the accuracy for a short time.  Added to its linear weight regression.  The selection method yielded the following formula 1 / pow (Y, 3.1).  That is, the more time, the less its importance.  Adding to this an emission-resistant model, I ended up with a bunch of 4 models <br><br><pre> <code class="python hljs">LinearRegression() <span class="hljs-comment"><span class="hljs-comment">#      TheilSenRegressor() Pipeline([('poly', PolynomialFeatures()), ('linear', TheilSenRegressor())]) #      GreadSearchCV </span></span></code> </pre> <br>  Actually the average of these models gave me a result of 0.057.  Then I jumped to the 14th place, after which I smoothly rolled down to 20. <br><br>  Attempts to somehow improve the result by selecting the best parameters, changing models, their combinations, and even adding fake points did not succeed. <br><br><h3>  Finish line </h3><br>  On clearly linear groups, the result was excellent, so it was necessary to work with data with a strong dispersion.  I also decided to add dispersion with my prediction, for this I taught the ExtraTreesRegressor at all not clearly linear groups, I took the 36 most important parameters according to the model.  I wrote a script that, in a loop, did cross-validation of data from only these 36 parameters, each time excluding one parameter.  Thus, I saw, without which parameter, the best result is obtained.  This iteration was repeated until the quality ceased to improve.  Yes, this is not quite the right approach, because the parameter excluded in the first iteration could give an increase in the fifth one.  For good it was necessary to check all the options, but it took too long, so having improved the quality by a couple of percent, I was pleased with the result. <br><br>  Further, for nonlinear groups, another model was added, but with a special condition.  If the time predicted by ExtraTreesRegressor was less than, the average time predicted by linear models, then I took the average between them, if it is more, then I took only the time predicted by linear models.  (It is better to err on the downside, because if the real time is higher, then the error is less). <br><br>  This threw me into 8th place, from where I again slipped to 10th.  The rest of my attempts to somehow improve the result, also did not bear fruit.  When recalculating all the test results, I was on the 9th place.  Between the last of the TOP 10 and not included there, the difference in points is about 0.0002.  This is very little, which indicates the great value of luck at this turn. <br><br><h3>  Results </h3><br>  And so, the conclusions that I made for myself: <br><br><ul><li>  Always use cross-validation; </li><li>  Try to minimize the error; </li><li>  It is necessary to work with data (outliers, or lack of data); </li></ul><br><h3>  Prizes </h3><br>  It‚Äôs good that the prizes were small and already for 10 places, which allowed me to squeeze into the prizes.  I drove to the office of my mail for my hard drive, where Ilya ( <a href="https://habrahabr.ru/users/sat2707/" class="user_link">sat2707</a> ) met me, handed me a prize, gave me some juice and showed a viewing platform.  I was also assured that new contests with big prizes are planned, so we will wait. <br><br>  I have everything, thank you all, good luck to all! </div><p>Source: <a href="https://habr.com/ru/post/309008/">https://habr.com/ru/post/309008/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../308996/index.html">TargetSummit Moscow 2016 in a week</a></li>
<li><a href="../308998/index.html">Dropbox - stop displaying HTML content in the browser</a></li>
<li><a href="../309000/index.html">Tarantool: examples of use</a></li>
<li><a href="../309002/index.html">The Game of Java: Java Conference in Kiev, October 14-15, 2016</a></li>
<li><a href="../309004/index.html">Data Science Week 2016. Data Technologies Forum</a></li>
<li><a href="../309010/index.html">Benefits of systemd-networkd on Linux virtual servers</a></li>
<li><a href="../309012/index.html">DevCon School schools and other good news for the day of knowledge</a></li>
<li><a href="../309014/index.html">The first mobile browser with support for Chromium extensions. New alpha Yandex Browser</a></li>
<li><a href="../309016/index.html">Graduates of the online program share their impressions</a></li>
<li><a href="../309018/index.html">How it works: A few words about DNS</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>