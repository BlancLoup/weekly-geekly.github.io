<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Autoencoder in the tasks of political event clustering</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I do not like to read articles, watch demo and code  TensorBoard Projector demo 



1. Works in Chrome . 
2. Open and click on Bookmarks in the lower ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Autoencoder in the tasks of political event clustering</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/13d/19b/e2a/13d19be2a742c97b4943026fa389d55a.png" alt="image" height="80%" width="80%"><br><div class="spoiler">  <b class="spoiler_title">I do not like to read articles, watch demo and code</b> <div class="spoiler_text">  <a href="https%253A%252F%252Fgist.githubusercontent.com%252FVolodymyrPavliukevych%252Ffe9d724a2d47085798dce306b22a377f%252Fraw%252F749c9e066c2fe5f7b1decb062c00803f3b72f6da%252Frada_full_packed_projector_config.json">TensorBoard Projector demo</a> <br><br><ol><li>  Works in <u>Chrome</u> . </li><li>  Open and click on <b>Bookmarks</b> in the lower right corner. </li><li>  In the upper right corner we can filter the classes. </li><li>  At the end of the article there are GIF images with examples of use. </li></ol><br>  <a href="https%253A%252F%252Fgithub.com%252FOctadero%252Frada">GitHub project</a> <br></div></div><br><div class="spoiler">  <b class="spoiler_title">Digression from the topic</b> <div class="spoiler_text">  <i>In this article, we will talk about <b>machine learning tools</b> , approaches and practical solutions.</i>  <i>The analysis is carried out on the basis of political events, which is not the subject of discussion of this article.</i>  <i>Please do not raise the topic of politics in the comments to this article.</i> <br></div></div><br>  For several years in a row, machine learning algorithms have been used in various fields.  Analytics of various political events, such as forecasting voting results, developing clustering mechanisms for decisions, analyzing the activities of political actors, can also become one of such areas.  In this article I will try to share the result of one of the studies in this area. <br><br><h4>  Formulation of the problem </h4><br>  Modern machine learning tools allow you to transform and visualize a large amount of data.  This fact made it possible to analyze the activities of political parties by transforming votes for 4 years into a self-organizing space of points reflecting the behavior of each of the deputies. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Each politician expressed himself in fact twelve thousand votes.  Each vote can take one of five options (did not come to the hall, came but missed the vote, voted ‚Äúfor‚Äù, ‚Äúagainst‚Äù or abstained). <br><br>  Our task is to transform all the voting results into a point in the three-dimensional Euclidean space reflecting a certain weighted position. <br><a name="habracut"></a><br><h4>  Open data </h4><br>  All initial data were obtained on the <a href="http%253A%252F%252Fdata.rada.gov.ua%252Fopen">official website</a> , and then transformed into data for the neural network. <br><br><h4>  Autoencoder </h4><br>  As was seen from the statement of the problem, it is necessary to represent twelve thousand votes in the form of a vector of dimension 2 or 3. A person can operate with categories of 2-dimensional and 3-dimensional spaces, it is extremely difficult for a person to work with a larger number of spaces. <br><br>  To reduce the bit we will use autoencoder. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/621/3a0/da8/6213a0da8af65c8bb96265056fb9aeb2.png" alt="image" height="80%" width="80%"><br><br>  The basis of autoencoders is based on the principle of two functions: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b31/4a4/e6d/b314a4e6dbf294193f2623d351274ea8.gif" alt="image">  - Encoder function; <br><br><img src="https://habrastorage.org/getpro/habr/post_images/149/893/e07/149893e07281a1f03a0fc2e03b1611d6.gif" alt="image">  - decoder function; <br><br>  The input of such a network is the original vector <img src="https://habrastorage.org/getpro/habr/post_images/779/0dd/0ef/7790dd0efb4a03a4c876741804d9b559.gif" alt="image">  dimension <img src="https://habrastorage.org/getpro/habr/post_images/e77/e19/055/e77e1905584d83cf5a3dc7079bef474f.gif" alt="image">  and the neural network transforms it into a hidden layer value <img src="https://habrastorage.org/getpro/habr/post_images/819/4ad/15f/8194ad15f7528ed684845711f3125c5e.gif" alt="image">  dimension <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" alt="image">  .  Next, the neural network decoder transforms the value of the hidden layer <img src="https://habrastorage.org/getpro/habr/post_images/819/4ad/15f/8194ad15f7528ed684845711f3125c5e.gif" alt="image">  in the output vector <img src="https://habrastorage.org/getpro/habr/post_images/787/cf7/c3a/787cf7c3a3d374114b3a07305b7fa446.gif" alt="image">  dimension <img src="https://habrastorage.org/getpro/habr/post_images/e77/e19/055/e77e1905584d83cf5a3dc7079bef474f.gif" alt="image">  , wherein <img src="https://habrastorage.org/getpro/habr/post_images/55c/dc4/aa7/55cdc4aa733af723bd7ea7542c87d520.gif" alt="image">  .  That is, the hidden layer will result in a smaller dimension, but at the same time it will be able to reflect the entire set of source data. <br><br>  For network training, the target error function is used: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/532/e92/22d/532e9222ddf8d3556354d45e53e13d1f.gif" alt="image"><br><br>  In other words, we minimize the difference between the values ‚Äã‚Äãof the input and output layers.  The trained neural network allows you to compress the dimension of the original data to a certain dimension <img src="https://habrastorage.org/getpro/habr/post_images/f24/8e8/91e/f248e891effc6650d9d31fbefc54cbe4.gif" alt="image">  on a hidden layer <img src="https://habrastorage.org/getpro/habr/post_images/819/4ad/15f/8194ad15f7528ed684845711f3125c5e.gif" alt="image">  . <br><br>  This image shows one input, one hidden and one output layer.  In practice, these layers may be more. <br><br>  I tried to tell the theory, let's move on to practice. <br><br>  Our data is already collected in JSON format from the official site, and encoded into a vector. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b80/8f4/673/b808f4673e279cecb8ab00af21dc330b.png" alt="image" height="80%" width="80%"><br><br>  Now we have a dataset of 24000 x 453 dimension. We create a neural network using TensorFlow: <br><br><pre><code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Building the encoder def encoder(x): with tf.variable_scope('encoder', reuse=False): with tf.variable_scope('layer_1', reuse=False): w1 = tf.Variable(tf.random_normal([num_input, num_hidden_1]), name="w1") b1 = tf.Variable(tf.random_normal([num_hidden_1]), name="b1") # Encoder Hidden layer with sigmoid activation #1 layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1)) with tf.variable_scope('layer_2', reuse=False): w2 = tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2]), name="w2") b2 = tf.Variable(tf.random_normal([num_hidden_2]), name="b2") # Encoder Hidden layer with sigmoid activation #2 layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w2), b2)) with tf.variable_scope('layer_3', reuse=False): w2 = tf.Variable(tf.random_normal([num_hidden_2, num_hidden_3]), name="w2") b2 = tf.Variable(tf.random_normal([num_hidden_3]), name="b2") # Encoder Hidden layer with sigmoid activation #2 layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w2), b2)) return layer_3 # Building the decoder def decoder(x): with tf.variable_scope('decoder', reuse=False): with tf.variable_scope('layer_1', reuse=False): w1 = tf.Variable(tf.random_normal([num_hidden_3, num_hidden_2]), name="w1") b1 = tf.Variable(tf.random_normal([num_hidden_2]), name="b1") # Decoder Hidden layer with sigmoid activation #1 layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1)) with tf.variable_scope('layer_2', reuse=False): w1 = tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1]), name="w1") b1 = tf.Variable(tf.random_normal([num_hidden_1]), name="b1") # Decoder Hidden layer with sigmoid activation #1 layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, w1), b1)) with tf.variable_scope('layer_3', reuse=False): w2 = tf.Variable(tf.random_normal([num_hidden_1, num_input]), name="w2") b2 = tf.Variable(tf.random_normal([num_input]), name="2") # Decoder Hidden layer with sigmoid activation #2 layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, w2), b2)) return layer_3 # Construct model encoder_op = encoder(X) decoder_op = decoder(encoder_op) # Prediction y_pred = decoder_op # Targets (Labels) are the input data. y_true = X # Define loss and optimizer, minimize the squared error loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2)) tf.summary.scalar("loss", loss) optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)</span></span></code> </pre> <br>  <a href="https%253A%252F%252Fgithub.com%252FOctadero%252Frada%252Fblob%252Fmaster%252FAutoencoder%252Fautoencoder.py">Full listing autoencoder.</a> <br><br>  The network will be trained by the RMSProb optimizer in 0.01 increments. <br>  As a result, we can see the TensorFlow operations graph. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9f0/7f8/fcc/9f07f8fccb3498b7af1a5d46de5d0599.png" alt="image"><br><br>  For an additional test, select the first four vectors and interpret their values ‚Äã‚Äãat the network input and output as a picture.  This way we can visually see that the values ‚Äã‚Äãof the output layer are ‚Äúidentical‚Äù (with an error) to the values ‚Äã‚Äãof the input layer. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b64/38e/aab/b6438eaab8559484ab53acceef6adfd5.png" alt="image"><br>  Original input. <br><img src="https://habrastorage.org/getpro/habr/post_images/2b2/971/4b1/2b29714b1972a9aa732c2b0d8718231b.png" alt="image"><br>  The values ‚Äã‚Äãof the output layer of the neural network. <br><br>  Afterwards, we consistently transfer all our original data to the network, retrieving the values ‚Äã‚Äãof the hidden layer.  These values ‚Äã‚Äãwill be our desired compressed data. <br><br>  By the way, I tried to select different layers and chose the configuration that allowed us to get closer to the minimum error. <br><br>  <b>PCA and t-SNA dimension reducers.</b> <br>  At this stage, we have 450 vectors with a dimension of 64. This is already a very good result, but not enough to give to a person.  For this reason, ‚Äúgo deeper.‚Äù  We will use the PCA and <a href="https%253A%252F%252Fdistill.pub%252F2016%252Fmisread-tsne%252F">t-SNA downscaling</a> approaches.  There are a lot of articles written about the principal component analysis (PCA) method, so I will not dwell on its analysis, but I would like to tell you about the t-SNA approach. <br><br>  The original document <a href="http%253A%252F%252Fwww.jmlr.org%252Fpapers%252Fvolume9%252Fvandermaaten08a%252Fvandermaaten08a.pdf">Visualizing data using t-SNE contains a detailed description of the algorithm</a> , for example, I will consider the option of reducing the two-dimensional dimension to one-dimensional. <br><br>  Having a two-dimensional space and three classes A, B, C located in this space, we will try to simply project the classes on one of the axes: <br><img src="https://habrastorage.org/getpro/habr/post_images/9ab/923/981/9ab923981a93e7285e265617a4d82f64.png" alt="image" height="80%" width="80%"><br><img src="https://habrastorage.org/getpro/habr/post_images/d91/c88/347/d91c88347a76732314fe7b7d3dee89f8.png" alt="image" height="80%" width="80%"><br>  As a result, no axis can give us a complete picture of the original classes.  Classes are ‚Äúmixed‚Äù, which means they lose their natural properties. <br><br>  Our task is to place elements in our finite space in proportion remotely (approximately) to how they were placed in the original space.  That is, those that were close to each other and should be closer than those that were located at a distance. <br><br><h4>  Stochastic Neighbor Embedding </h4><br>  Express the initial relationship between points in the original space as the distance in the Euclidean space between the points <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  , <img src="https://habrastorage.org/getpro/habr/post_images/46b/968/fae/46b968fae26576a2334eec1acf1f8faf.gif">  : <br><img src="https://habrastorage.org/getpro/habr/post_images/7c4/71e/2ec/7c471e2ec030689275c65e8d7011ffcd.gif">  and correspondingly <img src="https://habrastorage.org/getpro/habr/post_images/3bb/ae3/e14/3bbae3e14b5a372e185d15ed8b8e1619.gif">  for points in the desired space. <br><br>  We define the conditional probability of similarity (conditional probabilities that represent similarities) of points in the source space: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a96/090/12e/a9609012e32670705a90c56a6301435d.gif"><br><br>  This expression describes how close the point is. <img src="https://habrastorage.org/getpro/habr/post_images/46b/968/fae/46b968fae26576a2334eec1acf1f8faf.gif">  to the point <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  provided that the distance to the nearest class points we characterize as a Gaussian distribution around <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  with a given variance <img src="https://habrastorage.org/getpro/habr/post_images/5a4/4d0/8a2/5a44d08a2c46ced5dd1a8786e2d30d12.gif">  (centered on point <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  ).  Dispersion is unique for each point and is calculated separately based on the fact that points with higher density have less variance. <br><br>  The following describes the similarity point <img src="https://habrastorage.org/getpro/habr/post_images/58e/79c/4d0/58e79c4d002ce91859ceeb4c24f97f69.gif">  with a point <img src="https://habrastorage.org/getpro/habr/post_images/df6/4d2/67e/df64d267e80e6eb8723b471b58d9ee20.gif">  in the new space, respectively: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb4/91c/e2e/eb491ce2e04bdfc93a0fdaa32bfc10f2.gif"><br><br>  Again, since we are only interested in the modeling of paired similarities, we put <img src="https://habrastorage.org/getpro/habr/post_images/ca1/552/8e7/ca15528e798f655a6d250a7b51085fd0.gif">  . <br>  If the display points <img src="https://habrastorage.org/getpro/habr/post_images/df6/4d2/67e/df64d267e80e6eb8723b471b58d9ee20.gif">  and <img src="https://habrastorage.org/getpro/habr/post_images/58e/79c/4d0/58e79c4d002ce91859ceeb4c24f97f69.gif">  correctly simulate the similarity between high-size data points <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  and <img src="https://habrastorage.org/getpro/habr/post_images/58e/79c/4d0/58e79c4d002ce91859ceeb4c24f97f69.gif">  conditional probabilities <img src="https://habrastorage.org/getpro/habr/post_images/2fe/7a5/7bd/2fe7a57bda0675f1d24a33de06878396.gif">  and <img src="https://habrastorage.org/getpro/habr/post_images/504/d1b/696/504d1b696038efaba79ee08bb02e753d.gif">  will be equal.  Motivated by this observation, the SNE seeks to find a low-dimensional representation of the data that minimizes the discrepancy between <img src="https://habrastorage.org/getpro/habr/post_images/2fe/7a5/7bd/2fe7a57bda0675f1d24a33de06878396.gif">  and <img src="https://habrastorage.org/getpro/habr/post_images/504/d1b/696/504d1b696038efaba79ee08bb02e753d.gif">  . <br>  The algorithm finds the variance values. <img src="https://habrastorage.org/getpro/habr/post_images/db0/843/b31/db0843b31627d2f05c1dfb7a4665be21.gif">  for Gaussian distribution at each specific point <img src="https://habrastorage.org/getpro/habr/post_images/42c/9d0/2a8/42c9d02a8d8f13696994eb0204c82e63.gif">  .  It is unlikely that there is one value. <img src="https://habrastorage.org/getpro/habr/post_images/db0/843/b31/db0843b31627d2f05c1dfb7a4665be21.gif">  which is optimal for all points in the data set, since the data density may vary.  Lower density in dense areas <img src="https://habrastorage.org/getpro/habr/post_images/db0/843/b31/db0843b31627d2f05c1dfb7a4665be21.gif">  usually more appropriate than in more sparse areas.  SNE using a binary search selects <img src="https://habrastorage.org/getpro/habr/post_images/db0/843/b31/db0843b31627d2f05c1dfb7a4665be21.gif">  . <br>  The search takes place when taking into account the measures of effective neighbors (perplexion parameter) that will be taken into account when calculating <img src="https://habrastorage.org/getpro/habr/post_images/db0/843/b31/db0843b31627d2f05c1dfb7a4665be21.gif">  . <br>  The authors of the algorithm find an example in physics, describing this algorithm as a bunch of objects by various springs that are able to attract and repel other objects.  Leaving the system for some time, she will independently find a point of rest, having balanced the tension of all the springs. <br><br><h4>  t-Distributed Stochastic Neighbor Embedding </h4><br>  The difference between SNE and the t-SNE algorithm is to replace the Gaussian distribution with the Student's distribution (also known as t-Distribution, t-Student distribution) and change the error function to a symmetrized one. <br><br>  Thus, the algorithm first places all the original objects in a space of lower dimension.  After begins to move the object behind the object, based on how far (close) they were to other objects in the original space. <br><img src="https://habrastorage.org/getpro/habr/post_images/b15/840/e56/b15840e56fe4deb2c096140624140b9c.png" alt="image" height="80%" width="80%"><br><br><h4>  TensorFlow, TensorBoard and Projector </h4><br>  Today there is no need to implement such algorithms on their own.  We can use the ready-made math packages <i>scikit</i> , <i>matlab</i> or <i>TensorFlow</i> . <br><br>  <a href="https%253A%252F%252Fwww.octadero.com%252F2017%252F12%252F01%252Fvisualizing-neural-network-exercising-with-means-of-tensorflowkit%252F">I wrote in a previous article</a> that the TensorFlow toolkit includes a package for visualizing data and the TensorBoard learning process. <br><br>  So let's use this solution. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> os <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> tensorflow.contrib.tensorboard.plugins <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> projector <span class="hljs-comment"><span class="hljs-comment"># Create randomly initialized embedding weights which will be trained. first_D = 23998 # Number of items (size). second_D = 11999 # Number of items (size). DATA_DIR = '' LOG_DIR = DATA_DIR + 'embedding/' first_rada_input = np.loadtxt(DATA_DIR + 'result_' + str(first_D) + '/rada_full_packed.tsv', delimiter='\t') second_rada_input = np.loadtxt(DATA_DIR + 'result_' + str(second_D) + '/rada_full_packed.tsv', delimiter='\t') first_embedding_var = tf.Variable(first_rada_input, name='politicians_embedding_' + str(first_D)) second_embedding_var = tf.Variable(second_rada_input, name='politicians_embedding_' + str(second_D)) saver = tf.train.Saver() with tf.Session() as session: session.run(tf.global_variables_initializer()) saver.save(session, os.path.join(LOG_DIR, "model.ckpt"), 0) config = projector.ProjectorConfig() # You can add multiple embeddings. Here we add only one. first_embedding = config.embeddings.add() second_embedding = config.embeddings.add() first_embedding.tensor_name = first_embedding_var.name second_embedding.tensor_name = second_embedding_var.name # Link this tensor to its metadata file (eg labels). first_embedding.metadata_path = os.path.join(DATA_DIR, '../rada_full_packed_labels.tsv') second_embedding.metadata_path = os.path.join(DATA_DIR, '../rada_full_packed_labels.tsv') first_embedding.bookmarks_path = = os.path.join(DATA_DIR, '../result_23998/bookmarks.txt') second_embedding.bookmarks_path = = os.path.join(DATA_DIR, '../result_11999/bookmarks.txt') # Use the same LOG_DIR where you stored your checkpoint. summary_writer = tf.summary.FileWriter(LOG_DIR) # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will # read this file during startup. projector.visualize_embeddings(summary_writer, config)</span></span></code> </pre><br>  <a href="https%253A%252F%252Fgist.githubusercontent.com%252FVolodymyrPavliukevych%252Ffe9d724a2d47085798dce306b22a377f%252Fraw%252F749c9e066c2fe5f7b1decb062c00803f3b72f6da%252Frada_full_packed_projector_config.json">The result can be viewed on my TensorBoard Projector.</a> <br><br><ol><li>  Works in <u>Chrome</u> . </li><li>  Open and click on <b>Bookmarks</b> in the lower right corner. </li><li>  In the upper right corner we can filter the classes. </li><li>  Below are gif pictures with examples. </li></ol><br><div class="spoiler">  <b class="spoiler_title">GIF pictures larger.</b> <div class="spoiler_text"><img src="https://habrastorage.org/getpro/habr/post_images/1a7/39f/617/1a739f6175d44320f25c8d10faee41d7.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/0b0/8de/e20/0b08dee201f55066f485237625b43941.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/ab8/739/be4/ab8739be425a417a27df7dcf88dc4a80.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/68b/33b/c56/68b33bc5635aebd4e293c5a1d8570991.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/b32/a72/4f8/b32a724f818c2c380feed6b959081a35.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/7e9/0f8/a4d/7e90f8a4d132981a29ceafe20146a02b.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/621/928/a15/621928a15ec06eb95641bd1ba958a4b4.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/d7f/88a/a96/d7f88aa96e76519d0f23a1d8f0424406.gif" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/619/22a/7fb/61922a7fb5c471adce0306ce04f32598.gif" alt="image"><br></div></div><br>  Also, the whole portal is now available - <a href="http%253A%252F%252Fprojector.tensorflow.org">projector</a> , which allows you to visualize your existing dataset directly on the Google server. <br><br><ol><li>  Go to the website projector </li><li>  Click ‚ÄúLoad data‚Äù </li><li>  Choose our dataset with vectors </li><li>  Add pre-assembled metadata: labels, classes, and so on. </li><li>  We connect the color differentiation (color map) on one of the columns. </li><li>  If desired, add a json config file and publish data for public viewing. </li></ol><br>  It remains to send the link to your analyst. <br><br>  For those who are interested in the subject area, it will be interesting to look at different sections, for example, the distribution in polls of politicians from different areas. <br><br><ul><li>  Accuracy of voting of individual parties. </li><li>  Distribution (blurring) of voting politicians from one party. </li><li>  The similarity of voting politicians from different parties. </li></ul><br><h4>  findings </h4><br><ul><li>  Autoencoders are a family of relatively simple algorithms that give surprisingly fast and good convergence results. </li><li>  Automatic clustering is not the answer to the question about the nature of the source data, it requires additional analytics, but it does provide a fairly quick and clear vector in which you can start working with your data. </li><li>  TensorFlow and TensorBoard are powerful and rapidly developing machine learning tools allowing to solve tasks of varying complexity. </li></ul></div><p>Source: <a href="https://habr.com/ru/post/349048/">https://habr.com/ru/post/349048/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349038/index.html">Another article about quaternions and Euler angles</a></li>
<li><a href="../349040/index.html">How to become a web developer and not go crazy</a></li>
<li><a href="../349042/index.html">Video from UralJS # 6 mitap - get rid of this, type the Redux application and write on React without brakes</a></li>
<li><a href="../349044/index.html">What Robotics Can Teach Game AI</a></li>
<li><a href="../349046/index.html">Analysis of the regulation of cryptocurrency in world markets in early 2018</a></li>
<li><a href="../349050/index.html">Go 1.10 Release Party @ Badoo February 24</a></li>
<li><a href="../349054/index.html">LL (*) parser using Rust macro</a></li>
<li><a href="../349056/index.html">Open lesson "UML Diagrams"</a></li>
<li><a href="../349058/index.html">Remember everything</a></li>
<li><a href="../349060/index.html">Flask Mega-Tutorial, Part XI: Bare Cosmetics (Edition 2018)</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>