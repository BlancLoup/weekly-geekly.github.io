<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Search@Mail.Ru, part two: review of data preparation architectures of large search engines</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Overview of data preparation architectures of large search engines 
 Last time, you and I remembered how Go.Mail.Ru started in 2010, and how the Searc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Search@Mail.Ru, part two: review of data preparation architectures of large search engines</h1><div class="post__text post__text-html js-mediator-article"><h4>  Overview of data preparation architectures of large search engines </h4><br>  Last time, you and I <a href="http://habrahabr.ru/company/mailru/blog/149498/">remembered</a> how Go.Mail.Ru started in 2010, and how the Search was before.  In this post we will try to draw a general picture - let's focus on how others work, but first we will tell about search distribution. <a name="habracut"></a><br><br><h4>  How are search engines distributed </h4><br>  As you requested, we decided to elaborate on the basics of distribution strategies of the most popular search engines. <br><br>  There is an opinion that Internet search is one of those services that most users choose on their own, and the strongest should win this battle.  This position is extremely sympathetic to us - it is for this reason that we are constantly improving our search technologies.  But the situation on the market makes its own adjustments, and the so-called ‚Äúbrowser wars‚Äù intervene in the first place. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      There was a time when the search was not associated with the browser.  Then the search engine was just another site that the user went to on his own.  Imagine ‚ÄîInternet Explorer up to version 7, which appeared in 2006, did not have a search string;  Firefox had a search string from the first version, but at the same time it appeared only in 2004. <br><br>  Where did the search string come from?  It was not the authors of the browsers who invented it - it first appeared as part of the Google Toolbar, released in 2001.  Google Toolbar added to the browser the functionality of "quick access to Google search" - namely, the search line in your panel: <br><br><img src="https://habrastorage.org/storage2/d25/5b5/54b/d255b554b4ed2b2a9d36dd14c498180f.png"><br><br>  Why did Google release its toolbar?  This is how Douglas Edwards, Google‚Äôs brand manager at the time, describes his mission in his book ‚ÄúI'm Feeling Lucky: The Confessions of Google Employee Number 59‚Äù: <br><br>  ‚ÄúThe Toolbar was a secret weapon in our war against Microsoft.  By embedding the toolbar in the browser.  If you‚Äôre on the PC, you‚Äôll not be happy with your PC.  We‚Äôve needed to make sure Google‚Äôs search box didn‚Äôt become an obsolete relic. ‚Äù <br><br>  ‚ÄúThe toolbar was the secret weapon in the war against Microsoft.  By integrating the Toolbar into the browser, Google opened another front in the battle for direct access to users.  Bill Gates wanted to completely control how users interact with the PC: there were a lot of rumors that in the next version of Windows, the search string would be installed directly on the desktop.  It was necessary to take measures so that Google‚Äôs search bar does not become a relic of the past. ‚Äù <br><br>  How did the toolbar spread?  Yes, all the same, along with the popular software: <a href="http://www.searchenginejournal.com/google-partners-with-realplayer-for-toolbar-distribution/467/">RealPlayer</a> , Adobe <a href="http://searchenginewatch.com/article/2058454/Google-Partners-With-Adobe-For-Toolbar-Distribution-In-Shockwave-Other-Product-To-Be-Named">Macromedia Shockwave Player</a> , etc. <br><br>  It is clear that other search engines have begun to distribute their toolbars (Yahoo Toolbar, for example), and browser manufacturers did not fail to take advantage of this opportunity to gain additional source of income from search engines and embedded the search line to themselves by introducing the concept of ‚Äúdefault search engine‚Äù. <br><br>  The business departments of the browser manufacturers chose an obvious strategy: the browser is the user's entry point to the Internet, the default search settings with high probability will be used by the audience of the browser - so why not sell these settings?  And they were right in their own way, because the Internet search is a product with almost zero ‚Äústicking‚Äù. <br><br>  At this point it is worthwhile to dwell.  Many will be indignant: ‚ÄúNo, a person gets used to the search and uses only the system that he trusts,‚Äù but the practice proves the opposite.  If, say, your mailbox or account social.  network for some reason is not available, you do not go right there to another postal service or another social network, because you are ‚Äúglued‚Äù to your accounts: they are known by your friends, colleagues, family.  Changing your account is a long and painful process.  With search engines, everything is completely different: the user is not tied to this or that system.  If the search engine is unavailable for some reason, users do not sit and do not wait for it to finally work - they just go to other systems (for example, we clearly saw it on LiveInternet counters a year ago, during failures at one of our competitors ).  At the same time, users do not suffer much from the accident, because all the search engines are roughly the same (search line, query, results page) and even an inexperienced user will not be confused when working with any of them.  Moreover, in about 90% of cases the user will receive an answer to his question, no matter what system he is looking for. <br><br>  So, the search, on the one hand, has almost zero "sticking" (in English there is a special term "stickiness").  On the other hand, some search is already pre-installed in the default browser, and quite a large number of people will use it only for the reason that it is convenient to use it from there.  And if the search behind the search line satisfies the user's tasks, he can continue to use it. <br>  What are we coming to?  The leading search engines have no other choice but to fight for the browser search strings, distributing their desktop search products - toolbars, which in the installation process change the default search in the user's browser.  The instigator of this struggle was Google, the rest had to defend themselves.  You can, for example, read these words of Arkady Volozh, the creator and owner of Yandex, <a href="http://www.cnews.ru/news/top/index.shtml%3F2011/10/28/462414">in his interview</a> : <br><br>  ‚ÄúWhen in 2006‚Äì2007.  Google‚Äôs share in the Russian search market began to grow, at first we could not understand why.  Then it became obvious that Google is promoting itself by embedding it in browsers (Opera, Firefox).  And with the release of its own browser and Google‚Äôs mobile operating system, it began to destroy the relevant markets altogether. ‚Äù <br>  Since Mail.Ru is also a search, it cannot stand aside from the ‚Äúbrowser wars‚Äù.  We just entered the market a bit later than others.  Now the quality of our search has increased markedly, and our distribution is a reaction to the very struggle of toolbars that is conducted in the market.  At the same time, it is really important for us that an increasing number of people who are trying to use our Search remain satisfied with the results. <br><br>  By the way, our distribution policy is several times less active than that of the nearest competitor.  We see it on the counter top.mail.ru, which is installed on most of the sites of the RuNet.  If a user goes to the site on request through one of the distribution products (toolbar, own browser, partner browser's searchbox), the URL contains the parameter clid = ... Thus, we can estimate the capacity of distribution requests: the competitor has almost 4 times more than us <br><br>  But let's move on from distribution to how other search engines work.  After all, internal discussions of architecture, we naturally began with the study of the architectural solutions of other search engines.  I will not describe their architecture in detail - instead I will give links to open materials and highlight the features of their solutions that seem important to me. <br><br><h4>  Data preparation in large search engines </h4><br><h5>  Rambler </h5><br>  Rambler, now closed, had a number of interesting architectural ideas.  For example, it was aware of their own data storage system (NoSQL, as it is now fashionable to call such systems) and HICS ( <a href="http://webcrunch.ru/library/development/databases/rambler-hcs/">or HCS</a> ) distributed computing, which was used, in particular, for calculations in the reference graph.  Also HICS allowed to standardize the presentation of data within the search in a single universal format. <br><br>  The architecture of the Rambler was quite different from ours in the organization of the spider.  Our spider was made as a separate server, with its own, self-written, base of addresses of the downloaded pages.  For downloading each site, a separate process was launched that simultaneously downloaded pages, parsed them, highlighted new links and could immediately follow them.  The spider of the Rambler was made much easier. <br><br>  On one server there was a large text file with all known addresses of documents to Rambler, one per line, sorted in lexicographical order.  Once a day, this file was crawled and other text-based download tasks were generated, which were performed by special programs that can only download documents from the address list.  Then the documents were parsed, links were extracted and put next to this large file-list of all known documents, sorted, after which the lists were merged into a new large file, and the cycle was repeated again. <br><br>  The advantages of this approach were in simplicity, the presence of a single register of all known documents.  The drawbacks were that it was impossible to go to the newly extracted addresses of the documents right away, since downloading new documents could only happen at the next iteration of the spider.  In addition, the size of the database and its processing speed was limited to one server. <br><br>  Our spider, on the contrary, could quickly go through all the new links from the site, but was very poorly managed outside.  It was hard to ‚Äúadd‚Äù additional data to the addresses (necessary for ranking documents within the site, determining the priority of downloading), it was difficult to dump the database. <br><br><h5>  Yandex </h5><br>  Not much was known about Yandex‚Äôs internal search engine until Den Raskovalov spoke about it <a href="http://compsciclub.ru/courses/informationretrieval">in his lecture course</a> . <br><br>  From there you can find out that the search for Yandex consists of two different clusters: <br><br><ul><li>  batch processing </li><li>  data processing in real time (this is not really ‚Äúreal time‚Äù in the sense in which this term is used in control systems where the delay in the execution of tasks can be critical. Rather, it is possible to get the document into the index as quickly as possible and independently of other documents or tasks; such a "soft" version of real time) </li></ul><br>  The first one is used for standard Internet hauling, the second one is used to deliver to the index the best and most interesting documents that have just appeared.  For the time being, we will consider only batch processing, because before the index update in real time we were then quite far away, we wanted to go on updating the index once every two days. <br><br><img src="https://habrastorage.org/storage2/c38/966/903/c389669031b4b18139f4ec595604b0b7.png"><br><br>  At the same time, despite the fact that externally, the Yandex batch processing cluster was somewhat similar to our pair of pumping and indexing clusters, there were several serious differences in it: <br><br><ul><li>  The base of addresses of pages one, is stored on indexing nodes.  As a result, there are no problems with synchronizing the two databases. </li><li>  Control of pumping logic is transferred to indexing nodes, i.e.  spider nodes are very simple; they download what indexers point to them.  We have the spider himself determined what to download and when. </li><li>  And, a very important difference is that inside all the data is presented in the form of relational tables of documents, sites, links.  We have all the data was separated by different hosts, stored in different formats.  The tabular presentation of data greatly simplifies access to them, allows you to make various samples and get the most diverse index analytics.  We were deprived of all this, and at that time only synchronization of our two document bases (spider and indexer) took a week, and we had to stop both clusters for this time. </li></ul><br><br><h5>  Google </h5><br>  Google, without doubt, is a global technology leader, so it is always paid attention to, analyzed what it did, when and why.  And the Google search architecture, of course, was for us the most interesting.  Unfortunately, Google rarely opens up its architectural features; each article is a big event and almost instantly spawns a parallel OpenSource project (sometimes not one) implementing the described technologies. <br><br>  For those who are interested in the features of Google search, you can safely advise to explore almost all the presentations and speeches of one of the most important specialists in the company on the internal infrastructure - <a href="http://research.google.com/people/jeff/">Jeffrey Dean</a> , for example: <br><ul><li>  <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en//people/jeff/WSDM09-keynote.pdf">‚ÄúChallenges in Building Large-Scale Information Retrieval Systems‚Äù</a> (slides), through which you can find out how Google has developed, starting with the very first version, which was made by undergraduate and graduate students at Stanford University until 2008, until the introduction of Universal Search.  There is a <a href="http://videolectures.net/wsdm09_dean_cblirs/">video of this performance</a> and a similar performance at Standford, <a href="http://www.youtube.com/watch%3Fv%3DmodXC5IWTJI">Building Software Systems At Google and Lessons Learned.</a> </li><li>  <a href="http://research.google.com/archive/mapreduce.html">MapReduce: Simplified Data Processing on Large Clusters</a> .  The article describes a computational model that allows you to easily parallelize calculations on a large number of servers.  Immediately after this publication, the open source Hadoop platform appeared. </li><li>  ‚ÄúBigTable: A Distributed Structured Storage System‚Äù, a story about the NoSQL database BigTable, based on which HBase and Cassandra were made ( <a href="http://video.google.com/videoplay%3Fdocid%3D7278544055668715642">video can be found here</a> , slides are <a href="http://research.google.com/archive/bigtable.html">here</a> ) </li><li>  "MapReduce, BigTable, and Other Distributed System Abstractions for Handling Large Datasets" - a <a href="http://www.youtube.com/watch%3Fv%3DoRwFpQKgRps">description of the most famous technologies of Google.</a> </li></ul><br>  Based on these presentations, you can highlight the following features of the Google search architecture: <br><ul><li>  Tabular structure for data preparation.  The entire search database is stored in a huge table, where the key is the address of the document, and the meta information is stored in separate columns, combined into families.  Moreover, the table was originally designed in such a way as to work effectively with sparse data (that is, when not all documents have values ‚Äã‚Äãin columns). </li><li>  Unified MapReduce distributed computing system.  Data preparation (including creation of a search index) is a sequence of mapreduce tasks performed on BigTable tables or files in a distributed GFS file system. </li></ul><br><br>  All this looks quite reasonable: all known addresses of documents are stored in one large table, their prioritization is performed according to it, calculations over a reference graph, etc., the contents of the retrieved pages are retrieved by the search spider, and an index is built on it. <br><br>  There is another interesting presentation by another Google specialist, Daniel Peng, about the innovations in BigTable, which made it possible to quickly add new documents to the index in a few minutes.  This technology "outside" Google has been advertised <a href="http://googleblog.blogspot.com/2010/06/our-new-search-index-caffeine.htm">under the name Caffeine</a> , and in publications received the name Percolator.  Video of the performance on OSDI'2010 can be <a href="https://www.usenix.org/conference/osdi10/large-scale-incremental-processing-using-distributed-transactions-and">seen here</a> . <br><br>  Speaking very roughly, this is the same BigTable, but in which the so-called are implemented.  triggers, - the ability to upload your own pieces of code that are triggered by changes inside the table.  If so far I have described batch processing, i.e.  when data is combined and processed as far as possible, the implementation of the same on triggers is completely different.  Suppose a spider has downloaded something, placed new content in a table;  Trigger triggered, signaling ‚Äúnew content has appeared, it needs to be indexed‚Äù.  The indexing process started immediately.  It turns out that all the tasks of the search engine can eventually be divided into subtasks, each of which is triggered by a click.  Having a large number of equipment, resources and debugged code, you can solve the problem of adding new documents quickly, in just a minute - as Google demonstrated. <br><br>  The difference between Google architecture and Yandex architecture, where the index update system was also indicated in real time, is that Google claims that the whole index building procedure is performed on triggers, while Yandex only has it for a small subset of the best, valuable documents. <br><br><h5>  Lucene </h5><br>  It is worth mentioning about another search engine - Lucene.  This is a free search engine written in Java.  In a sense, Lucene is a platform for creating search engines, for example, a search engine on the web called Nutch has spun off from it.  In essence, Lucene is a search engine for creating an index and a search engine, and Nutch is the same plus a spider that pages pages, because the search engine does not necessarily search for documents that are on the web. <br><br>  In fact, Lucene itself has implemented not so many interesting solutions that could be borrowed by a large web search engine designed for billions of documents.  On the other hand, you should not forget that it was Lucene developers who launched the Hadoop and HBase projects (every time a new interesting article from Google appeared, the Lucene authors tried to apply announced solutions for themselves. For example, Hase, which is a clone of BigTable) .  However, these projects have long existed by themselves. <br><br>  For me at Lucene / Nutch, it was interesting how they used Hadoop.  For example, in Nutch, a special spider was written for web browsing, performed entirely as tasks for Hadoop.  Those.  A whole spider is simply a process that runs on Hadoop in the MapReduce paradigm.  This is a rather unusual solution, outside the scope of how Hadoop is used.  After all, this is a platform for processing large amounts of data, and this suggests that data is already available.  And here this task does not calculate or process anything, but, on the contrary, downloads it. <br><br>  On the one hand, this solution impresses with its simplicity.  After all, the spider needs to get all the addresses of one site for pumping, bypass them one by one, the spider itself must also be distributed and run on several servers.  So we are doing a mapper in the form of an address divider for sites, and we implement each individual pumping process in the form of a reducer. <br><br>  On the other hand, this is a rather bold decision, because it is hard to build sites - not every site is responsible for guaranteed time, and the cluster computing resources are spent so that it just waits for a response from someone else's web server.  And the problem of "slow" sites is always with a sufficiently large number of addresses for pumping.  For 20% of the time, Spyder pumps in 80% of documents from fast sites, then spends 80% of time trying to download slow websites - and almost never can download them all, you always have to drop something and leave it ‚Äúnext time‚Äù. <br><br>  We analyzed such a decision for some time, and as a result refused it.  Perhaps, for us, the architecture of this spider was interesting as a kind of "negative example." <br><br>  In more detail about the structure of our search engine, about how we built the search engine, I will tell in the next post. </div><p>Source: <a href="https://habr.com/ru/post/150303/">https://habr.com/ru/post/150303/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../150294/index.html">Original Samsung</a></li>
<li><a href="../150295/index.html">40 million installations of Web Of Trust</a></li>
<li><a href="../150298/index.html">Is Symmetric NAT So Scary?</a></li>
<li><a href="../150300/index.html">Time travel and programming</a></li>
<li><a href="../150302/index.html">We learn Python quality</a></li>
<li><a href="../150305/index.html">‚ÄúRunet today‚Äù, August 27, 2012. Experts of the issue: Ilya Balandin, Pavel Nikonov</a></li>
<li><a href="../150306/index.html">Simple rules for simple layout.</a></li>
<li><a href="../150307/index.html">TsODy.RF - Russia's first magazine about data centers only</a></li>
<li><a href="../150308/index.html">Unappreciated flagship</a></li>
<li><a href="../150309/index.html">A word about "innovation"</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>