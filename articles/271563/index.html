<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural network in 11 lines in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="What is the article about 
 Personally, I learn best with a small, working code I can play with. In this tutorial, we will learn the error back-propag...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural network in 11 lines in Python</h1><div class="post__text post__text-html js-mediator-article"><h4>  What is the article about </h4><br>  Personally, I learn best with a small, working code I can play with.  In this tutorial, we will learn the error back-propagation algorithm using the example of a small neural network implemented in Python. <br><br><h4>  Give the code! </h4><br><pre><code class="python hljs">X = np.array([ [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>],[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>],[<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>],[<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>] ]) y = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>]]).T syn0 = <span class="hljs-number"><span class="hljs-number">2</span></span>*np.random.random((<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>)) - <span class="hljs-number"><span class="hljs-number">1</span></span> syn1 = <span class="hljs-number"><span class="hljs-number">2</span></span>*np.random.random((<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)) - <span class="hljs-number"><span class="hljs-number">1</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">60000</span></span>): l1 = <span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-(np.dot(X,syn0)))) l2 = <span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-(np.dot(l1,syn1)))) l2_delta = (y - l2)*(l2*(<span class="hljs-number"><span class="hljs-number">1</span></span>-l2)) l1_delta = l2_delta.dot(syn1.T) * (l1 * (<span class="hljs-number"><span class="hljs-number">1</span></span>-l1)) syn1 += l1.T.dot(l2_delta) syn0 += XTdot(l1_delta)</code> </pre> <br><br>  Too compressed?  Let's break it down into simpler parts. <br><a name="habracut"></a><br><h4>  Part 1: A small toy neural network </h4><br>  A neural network that is trained through backpropagation attempts to use input data to predict the output. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <pre> <code class="bash hljs">  0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0</code> </pre><br><br>  Suppose we need to predict what the output column will look like based on the input data.  This problem could be solved by calculating the statistical correspondence between them.  And we would see that the left column correlates 100% with the output. <br><br>  Backpropagation, in the simplest case, calculates similar statistics to create a model.  Let's try. <br><br><h4>  Neural network in two layers </h4><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#  def nonlin(x,deriv=False): if(deriv==True): return f(x)*(1-f(x)) return 1/(1+np.exp(-x)) #    X = np.array([ [0,0,1], [0,1,1], [1,0,1], [1,1,1] ]) #   y = np.array([[0,0,1,1]]).T #      np.random.seed(1) #       0 syn0 = 2*np.random.random((3,1)) - 1 for iter in xrange(10000): #   l0 = X l1 = nonlin(np.dot(l0,syn0)) #   ? l1_error = y - l1 #      #     l1 l1_delta = l1_error * nonlin(l1,True) # !!! #   syn0 += np.dot(l0.T,l1_delta) # !!! print "   :" print l1</span></span></code> </pre><br><br><pre> <code class="bash hljs">   : [[ 0.00966449] [ 0.00786506] [ 0.99358898] [ 0.99211957]]</code> </pre><br><br>  Variables and their descriptions. <br><br>  X - matrix input data set;  strings - training examples <br>  y is the matrix of the output data set;  strings - training examples <br>  l0 is the first network layer defined by the input data <br>  l1 - the second layer of the network, or hidden layer <br>  syn0 - the first layer of the scale, Synapse 0, combines l0 with l1. <br>  "*" - elementwise multiplication - two vectors of the same size multiply the corresponding values, and the output is a vector of the same size <br>  "-" - elementwise subtraction of vectors <br>  x.dot (y) - if x and y are vectors, then the output will be the scalar product.  If these are matrices, then matrix multiplication will be obtained.  If the matrix is ‚Äã‚Äãonly one of them - this is the multiplication of the vector and the matrix. <br><br>  And it works!  I recommend before reading the explanations to play around a bit with the code and understand how it works.  It should run just like it is, in ipython notebook.  What you can tinker with in the code: <br><ul><li>  compare l1 after the first iteration and after the last </li><li>  look at the nonlin function. </li><li>  see how l1_error changes </li><li>  parse line 36 - the main secret ingredients are collected here (marked !!!) </li><li>  parse line 39 - the entire network is preparing for this operation (marked !!!) </li></ul><br><br><h4>  Let's sort the code by lines </h4><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np</code> </pre><br><br>  Imports numpy, a library of linear algebra.  Our only dependency. <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nonlin</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x,deriv=False)</span></span></span><span class="hljs-function">:</span></span></code> </pre><br><br>  Our nonlinearity.  Specifically, this function creates a "sigmoid."  It assigns any number to a value from 0 to 1 and converts numbers to probabilities, and also has several other properties useful for training neural networks. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/46a/00b/29e/46a00b29eb1af27149df0308929ce1b9.png" alt="image"><br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(deriv==<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>):</code> </pre><br><br>  This function also can produce derived sigmoids (deriv = True).  This is one of its useful properties.  If the output of the function is the out variable, then the derivative will be out * (1-out).  Effectively. <br><br><pre> <code class="python hljs">X = np.array([ [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>], ‚Ä¶</code> </pre><br><br>  Initialization of the input data array in the form of a numpy-matrix.  Each line is a training example.  Columns are input nodes.  We have 3 input nodes in the network and 4 training examples. <br><br><pre> <code class="python hljs">y = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>]]).T</code> </pre><br><br>  Initializes the output.  ".T" is the transfer function.  After the transfer, the matrix y has 4 rows with one column.  As in the case of input data, each row is a training example, and each column (in our case one) is an output node.  At the network, it turns out, 3 inputs and 1 output. <br><br><pre> <code class="python hljs">np.random.seed(<span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><br>  Due to this, the random distribution will be the same each time.  This will allow us to more easily track the network after making changes to the code. <br><br><pre> <code class="python hljs">syn0 = <span class="hljs-number"><span class="hljs-number">2</span></span>*np.random.random((<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>)) ‚Äì <span class="hljs-number"><span class="hljs-number">1</span></span></code> </pre><br><br>  Matrix weights network.  syn0 means "synapse zero".  Since we have only two layers, input and output, we need one matrix of weights, which will connect them.  Its dimension is (3, 1), since we have 3 inputs and 1 output.  In other words, l0 has size 3, and l1 is 1. Since we connect all nodes in l0 with all nodes l1, we need a matrix of dimension (3, 1). <br><br>  Notice that it is initialized randomly, and the average value is zero.  Behind this is quite a complex theory.  For now, just accept this as a recommendation.  Also note that our neural network is this very matrix.  We have ‚Äúlayers‚Äù of l0 and l1, but they are temporary values ‚Äã‚Äãbased on a data set.  We do not store them.  All training is stored in syn0. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> iter <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(<span class="hljs-number"><span class="hljs-number">10000</span></span>):</code> </pre><br><br>  This is where the main workout code for the network begins.  The code loop repeats many times and optimizes the network for the data set. <br><br><pre> <code class="python hljs">l0 = X</code> </pre><br><br>  The first layer, l0, is just data.  X contains 4 training examples.  We will process them all at once - this is called full batch training.  In total, we have 4 different lines of l0, but they can be thought of as one training example - at this stage it does not matter (you could load them 1000 or 10,000 without any changes in the code). <br><br><pre> <code class="python hljs">l1 = nonlin(np.dot(l0,syn0))</code> </pre><br><br>  This is a prediction step.  We allow the network to try to predict output based on input.  Then we will see how she does it so that you can tweak her in the direction of improvement. <br><br>  The line contains two steps.  The first makes the matrix multiplication l0 and syn0.  The second transmits the output through sigmoid.  They have the following dimensions: <br><br><pre> <code class="python hljs">(<span class="hljs-number"><span class="hljs-number">4</span></span> x <span class="hljs-number"><span class="hljs-number">3</span></span>) dot (<span class="hljs-number"><span class="hljs-number">3</span></span> x <span class="hljs-number"><span class="hljs-number">1</span></span>) = (<span class="hljs-number"><span class="hljs-number">4</span></span> x <span class="hljs-number"><span class="hljs-number">1</span></span>)</code> </pre><br><br>  Matrix multiplications require that the dimension equations in the middle coincide.  The final matrix has the number of rows, as in the first, and the columns - as in the second. <br><br>  We downloaded 4 training examples, and got 4 guesses (4x1 matrix).  Each pin corresponds to a guess of the network for a given input. <br><br><pre> <code class="python hljs">l1_error = y - l1</code> </pre><br><br>  Since l1 contains guesses, we can compare their difference with reality, subtracting its l1 from the correct answer y.  l1_error is a vector of positive and negative numbers characterizing the ‚Äúmiss‚Äù of the network. <br><br><pre> <code class="python hljs"> l1_delta = l1_error * nonlin(l1,<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br><br>  And here is the secret ingredient.  This line must be disassembled in parts. <br><br>  First part: derivative <br><br><pre> <code class="python hljs">nonlin(l1,<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br><br>  l1 represents these three points, and the code gives the slope of the lines shown below.  Note that for large values ‚Äã‚Äãlike x = 2.0 (green dot) and very small ones, like x = -1.0 (purple), the lines have a slight bias.  The largest angle of the point is x = 0 (blue).  It is of great importance.  Also note that all derivatives are in the range of 0 to 1. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/166/677/6ba/1666776ba379bf9217170964b83f4142.png" alt="image"><br><br>  Full Expression: Error Weighted Derivative <br><br><pre> <code class="python hljs">l1_delta = l1_error * nonlin(l1,<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br><br>  Mathematically there are more accurate ways, but in our case this one is also suitable.  l1_error is a matrix (4,1).  nonlin (l1, true) returns a matrix (4,1).  Here we multiply them element by element, and at the output we also get the matrix (4,1), l1_delta. <br><br>  By multiplying the derivatives by errors, we reduce the prediction errors made with high confidence.  If the slope of the line was small, then the network contains either a very large or a very small value.  If the guess in the network is close to zero (x = 0, y = 0.5), then it is not particularly sure.  We update these uncertain predictions and leave predictions alone with high confidence, multiplying them by values ‚Äã‚Äãclose to zero. <br><br><pre> <code class="python hljs">syn0 += np.dot(l0.T,l1_delta)</code> </pre><br><br>  We are ready to update the network.  Consider one training example.  In it, we will update the weight.  Update the leftmost weight (9.5) <br><br><img src="https://habrastorage.org/getpro/habr/post_images/631/24b/c9f/63124bc9f1769da21af0cc832259bce1.png" alt="image"><br><br><pre> <code class="python hljs">weight_update = input_value * l1_delta</code> </pre><br><br>  For the extreme left weight, this will be 1.0 * l1_delta.  Presumably, this will only slightly increase 9.5.  Why?  Since the prediction was already quite confident, and the predictions were almost correct.  A small error and a slight slope of the line mean a very small update. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2aa/762/223/2aa7622232e7ed776e01579c9fd7e295.png" alt="image"><br><br>  But since we are doing group training, we repeat the above step for all four training examples.  So this looks very much like the image above.  So what does our line do?  It calculates the weights updates for each weight, for each training example, summarizes them and updates all weights - all in one line. <br><br>  After watching the network update, let's return to our training data.  When both the input and output are 1, we increase the weight between them.  When the input is 1 and the output is 0, we reduce the weight. <br><br><pre> <code class="bash hljs">  0 0 1 0 1 1 1 1 1 0 1 1 0 1 1 0</code> </pre><br><br>  Thus, in our four training examples below, the weight of the first input relative to the output will constantly increase or remain constant, and the other two weights will increase and decrease depending on the examples.  This effect contributes to network training based on the correlation of input and output data. <br><br><h4>  Part 2: the task is more difficult </h4><br><pre> <code class="bash hljs">  0 0 1 0 0 1 1 1 1 0 1 1 1 1 1 0</code> </pre><br><br>  Let's try to predict the output based on the three input data columns.  None of the input columns is 100% correlated with the output.  The third column is not connected with anything at all, since it contains units all the way.  However, here you can see the scheme - if one of the first two columns (but not both) contains 1, then the result will also be equal to 1. <br><br>  This is a non-linear scheme, since there is no direct correspondence of one-to-one columns.  The match is based on a combination of input data, columns 1 and 2. <br><br>  Interestingly, pattern recognition is a very similar task.  If you have 100 pictures of the same size, on which bicycles and smoking pipes are depicted, the presence of certain pixels on them in certain places does not directly correlate with the presence of a bicycle or tube on the image.  Statistically, their color may seem random.  But some combinations of pixels are not random - those that form the image of a bicycle (or tube). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e73/23f/d79/e7323fd798745f034403e02247cab69b.png" alt="image"><br><img src="https://habrastorage.org/getpro/habr/post_images/6ef/f70/845/6eff7084580d7a69fc29cf18b70f4418.jpg" alt="image"><br><br><h4>  Strategy </h4><br>  To combine pixels into something that can have a one-to-one correspondence with the output, you need to add another layer.  The first layer combines the input, the second one assigns the matching to the output, using the input data of the first layer as input.  Pay attention to the table. <br><br><pre> <code class="bash hljs"> (l0)   (l1)  (l2) 0 0 1 0.1 0.2 0.5 0.2 0 0 1 1 0.2 0.6 0.7 0.1 1 1 0 1 0.3 0.2 0.3 0.9 1 1 1 1 0.2 0.1 0.3 0.8 0</code> </pre><br><br>  By randomly assigning weights, we get the hidden values ‚Äã‚Äãfor layer # 1.  Interestingly, the second column of the hidden scales already has a slight correlation with the exit.  Not perfect, but there is.  And this is also an important part of the network training process.  Training will only enhance this correlation.  It will update syn1 to match its output, and syn0 to better receive data from the input. <br><br><h4>  Neural network in three layers </h4><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">nonlin</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x,deriv=False)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>(deriv==<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> f(x)*(<span class="hljs-number"><span class="hljs-number">1</span></span>-f(x)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span>/(<span class="hljs-number"><span class="hljs-number">1</span></span>+np.exp(-x)) X = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>]]) y = np.array([[<span class="hljs-number"><span class="hljs-number">0</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">1</span></span>], [<span class="hljs-number"><span class="hljs-number">0</span></span>]]) np.random.seed(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   ,   - 0 syn0 = 2*np.random.random((3,4)) - 1 syn1 = 2*np.random.random((4,1)) - 1 for j in xrange(60000): #     0, 1  2 l0 = X l1 = nonlin(np.dot(l0,syn0)) l2 = nonlin(np.dot(l1,syn1)) #       ? l2_error = y - l2 if (j% 10000) == 0: print "Error:" + str(np.mean(np.abs(l2_error))) #     ? #      ,       l2_delta = l2_error*nonlin(l2,deriv=True) #    l1     l2? l1_error = l2_delta.dot(syn1.T) #     ,    l1? #      ,       l1_delta = l1_error * nonlin(l1,deriv=True) syn1 += l1.T.dot(l2_delta) syn0 += l0.T.dot(l1_delta)</span></span></code> </pre><br><br><pre> <code class="bash hljs">Error:0.496410031903 Error:0.00858452565325 Error:0.00578945986251 Error:0.00462917677677 Error:0.00395876528027 Error:0.00351012256786</code> </pre><br><br><h4>  Variables and their descriptions </h4><br>  X - matrix input data set;  strings - training examples <br>  y is the matrix of the output data set;  strings - training examples <br>  l0 is the first network layer defined by the input data <br>  l1 - the second layer of the network, or hidden layer <br>  l2 is the final layer, this is our hypothesis.  As the workout should approach the correct answer <br>  syn0 - the first layer of the scale, Synapse 0, combines l0 with l1. <br>  syn1 - the second layer of the scale, Synapse 1, combines l1 with l2. <br>  l2_error - network miss in quantitative terms <br>  l2_delta - network error, depending on the confidence of the prediction.  Almost coincides with the error, except for confident predictions <br>  l1_error - weighing l2_delta with weights from syn1, we calculate the error in the middle / hidden layer <br>  l1_delta - network errors from l1, scalable according to the conviction of predictions.  Almost identical to l1_error, except for confident predictions <br><br>  The code should be clear enough - it is just the previous implementation of the network, folded in two layers one above the other.  The output of the first layer l1 is the input of the second layer.  Something new is only in the next line. <br><br><pre> <code class="python hljs">l1_error = l2_delta.dot(syn1.T)</code> </pre><br><br>  Uses errors weighted by prediction confidence from l2 to calculate the error for l1.  We get, we can say, an error weighted by contributions - we calculate the contribution to the errors in l2 made by the values ‚Äã‚Äãin the nodes l1.  This step is called back propagation of errors.  Then we update syn0 using the same algorithm as in the two-layer neural network version. </div><p>Source: <a href="https://habr.com/ru/post/271563/">https://habr.com/ru/post/271563/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../271549/index.html">Lock-free algorithms and stack implementation</a></li>
<li><a href="../271555/index.html">Machine learning as a method of analyzing the microstructure of the market and its application in high-frequency trading</a></li>
<li><a href="../271557/index.html">AndroCAD: Rise to Power. Electronics and Android</a></li>
<li><a href="../271559/index.html">Vector, open and easy to use: Inkscape 0.91</a></li>
<li><a href="../271561/index.html">How to make a website adaptive using MobilizeToday</a></li>
<li><a href="../271565/index.html">Ruli24: perfect tuning for your company</a></li>
<li><a href="../271567/index.html">How to solve problems with the payment gateway: Airbnb Case</a></li>
<li><a href="../271569/index.html">12.12 at 12:00, come to Community DevCamp in Moscow</a></li>
<li><a href="../271571/index.html">Introduction to AutoCAD Architecture</a></li>
<li><a href="../271573/index.html">First February - International Aid Day</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>