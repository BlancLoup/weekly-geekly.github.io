<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Building an automatic message moderation system</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Automatic moderation systems are embedded in web services and applications where it is necessary to process a large number of user messages. Such syst...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Building an automatic message moderation system</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/xu/yp/u9/xuypu9acrj8o6qbkrhu53ue4kni.png" alt="image"><br>  Automatic moderation systems are embedded in web services and applications where it is necessary to process a large number of user messages.  Such systems can reduce the cost of manual moderation, speed it up and process all user messages in real-time.  The article will talk about building an automatic moderation system for processing the English language using machine learning algorithms.  We will discuss the whole workline from research tasks and the choice of ML algorithms to roll out in production.  Let's see where to look for ready datasets and how to collect data for a task independently. <br><a name="habracut"></a><br><br>  <i>Prepared in collaboration with Ira Stepanyuk, Data Scientist at Poteha Labs</i> <br><br><h2>  Task Description </h2><br>  We work with multiplayer active chat rooms, where short messages from dozens of users can come every minute in one chat.  The task is to isolate toxic messages and messages with any obscene statements in the dialogues from such chats.  From the point of view of machine learning, this is the problem of binary classification, where each message must be assigned to one of the classes. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      To solve this problem, first of all it was necessary to understand what toxic messages are and what exactly makes them toxic.  To do this, we looked at a large number of typical user posts on the Internet.  Here are some examples that we have already divided into toxic messages and normal ones. <br><br><div class="scrollable-table"><table><tbody><tr><th>  Toxic </th><th>  Normal </th></tr><tr><td>  Your are a damn fag * ot </td><td>  this book is so dummy </td></tr><tr><td>  ur child is so ugly (1) </td><td>  Winners losers make excuses </td></tr><tr><td>  White people are owners of black (2) </td><td>  black like my soul (2) </td></tr></tbody></table></div><br>  It can be seen that toxic messages often contain obscene words, but still this is not a prerequisite.  The message may not contain inappropriate words, but be offensive to anyone (example (1)).  In addition, sometimes toxic and normal messages contain the same words that are used in different contexts - offensive or not (example (2)).  Such messages also need to be able to distinguish. <br>  After examining various messages, for our moderation system, we called <b><i>toxic</i></b> messages that contain utterances with obscene, offensive expressions, or hate for someone. <br><br><h2>  Data </h2><br><h4>  Open data </h4><br>  One of the most famous moderation datasets is dataset from a competition at the Kaggle <a href="https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge">Toxic Comment Classification Challenge</a> .  Part of the markup in dataset is incorrect: for example, messages with obscene words can be marked as normal.  Because of this, one cannot simply take the Kernel competition and get a well-functioning classification algorithm.  We need to work more with the data, look at what examples are not enough, and add additional data with such examples. <br><br>  In addition to competitions, there are several scientific publications with links to suitable datasets ( <a href="https://github.com/t-davidson/hate-speech-and-offensive-language">example</a> ), but not all can be used in commercial projects.  Mostly in such datasets collected messages from the social network Twitter, where you can meet a lot of toxic tweets.  In addition, data is collected from Twitter, since certain hashtags can be used to search for and mark up toxic messages from users. <br><br><h4>  Manual data </h4><br>  After we collected datasets from open sources and trained on it the basic model, it became clear that open data is not enough: the quality of the model does not suit.  In addition to open data for solving the problem, we had access to unplaced selection of messages from the game messenger with a large number of toxic messages. <br><br><img src="https://habrastorage.org/webt/eh/sp/5o/ehsp5oivhvjnfgxrnjqc7wszf9u.gif" alt="image"><br><br>  To use this data for your task, you had to somehow mark it up.  At that time, there was a baseline-trained classifier, which we decided to use for semi-automatic markup.  Having driven all messages through the model, we obtained the probabilities of the toxicity of each message and sorted them in descending order.  At the beginning of this list were collected messages with obscene and offensive words.  At the end, on the contrary, there are normal user messages.  Thus, most of the data (with very large and very small probability values) could not be marked out, but immediately attributed to a certain class.  It remains to mark the messages that hit the middle of the list, which was done manually. <br><br><h4>  Data augmentation </h4><br>  Often in datasets you can see changed messages, in which the classifier is mistaken, and the person correctly understands their meaning. <br>  That's because users adapt and learn to deceive the moderation systems so that the algorithms are mistaken on toxic messages, and the meaning of the person remains clear.  What users are doing now: <br><br><ul><li>  generate typos: <i>you are stupid asswhole, fack you</i> , </li><li>  replace alphabetic characters with numbers similar in description: <i>n1gga, b0ll0cks</i> , </li><li>  insert extra spaces: <i>idiot</i> , </li><li>  remove spaces between words: <i>dieyoustupid</i> . </li></ul><br><br>  In order to train a classifier that is resistant to such substitutions, you need to do what users do: generate the same changes in the messages and add them to the training sample to the main data. <br>  In general, this struggle is inevitable: users will always try to find vulnerabilities and hacks, and moderators will implement new algorithms. <br><br><h3>  Subtask description </h3><br>  We were faced with subtasks for analyzing the message in two different modes: <br><br><ul><li>  online mode - real-time analysis of messages, with a maximum response rate; </li><li>  offline mode - analyzing log messages and highlighting toxic dialogs. </li></ul><br>  In online mode, we process each message of users and run it through the model.  If the message is toxic, then we hide it in the chat interface, and if normal, then output it.  In this mode, all messages should be processed very quickly: the model should produce a response so quickly that it does not violate the structure of the dialogue between users. <br>  There is no time limit in the offline mode, and therefore I wanted to implement a model with the highest quality. <br><br><h3>  Online mode.  Search for words in the dictionary </h3><br>  Regardless of which model is chosen further, we must find and filter messages with obscene words.  To solve this subtask, the easiest way is to compile a dictionary of unacceptable words and expressions that cannot be skipped exactly and search for such words in each message.  The search should occur quickly, so the naive algorithm for searching for substrings for such a time is not suitable.  The appropriate algorithm for searching for a set of words in a string is <a href="https://www.geeksforgeeks.org/aho-corasick-algorithm-pattern-searching/">the Aho-Korasik algorithm</a> .  Due to this approach, it is possible to quickly identify some toxic examples and block messages before they are transferred to the main algorithm.  Using the ML algorithm will allow you to "understand the meaning" of messages and improve the quality of classification. <br><br><h3>  Online mode.  Basic machine learning model </h3><br>  For the base model, it was decided to use the standard approach for text classification: TF-IDF + classical classification algorithm.  Again for reasons of speed and performance. <br><br>  TF-IDF is a statistical measure that allows you to determine the most important words for text in a body using two parameters: the word frequencies in each document and the number of documents containing a specific word (in more detail <a href="https://ru.wikipedia.org/wiki/TF-IDF">here</a> ).  Having calculated for each word in the message TF-IDF, we obtain a vector representation of this message. <br>  TF-IDF can be calculated for words in the text, as well as for n-gram words and characters.  Such an extension will work better, as it will be able to handle frequently encountered phrases and words that were not in the out-of-vocabulary. <br><br><img src="https://habrastorage.org/webt/cv/d8/gh/cvd8ghmwfr95tibnwpherqjdaz0.png" alt="image"><br>  <i>An example of using TF-IDF on n-grams of words and symbols</i> <br><br>  After converting messages into vectors, you can use any classical method for classification: <a href="https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f">logistic regression, SVM</a> , <a href="https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205">random forest, boosting</a> . <br><br>  In our problem, we decided to use logistic regression, since this model increases performance in comparison with other classical ML classifiers and predicts the probabilities of classes, which allows flexible selection of the classification threshold in production. <br><br>  The algorithm obtained using TF-IDF and logistic regression works quickly and well identifies messages with obscene words and expressions, but does not always understand the meaning.  For example, often messages with the words ' <i>black</i> ' and ' <i>feminizm</i> ' fall into the toxic class.  I wanted to fix this problem and learn to better understand the meaning of the messages using the next version of the classifier. <br><br><h3>  Offline mode </h3><br>  In order to better understand the meaning of messages, you can use neural network algorithms: <br><br><ul><li>  Embedding (Word2Vec, FastText) </li><li>  Neural Networks (CNN, RNN, LSTM) </li><li>  New pre-trained models (ELMo, ULMFiT, BERT) </li></ul><br>  We discuss some of these algorithms and how they can be used in more detail. <br><br><h4>  Word2Vec and FastText </h4><br>  Embedding patterns allow you to get vector representations of words from texts.  There are <a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/">two types of Word2Vec</a> : Skip-gram and CBOW (Continuous Bag of Words).  In Skip-gram, the context is predicted by the word, and in CBOW, the opposite: by the context, the word is predicted. <br><img src="https://habrastorage.org/webt/rc/kb/iv/rckbivfc1dvmna3bvccy6xoai_g.png" alt="image"><br>  Such models are trained in large text corpora and allow to get vector representations of words from a hidden layer of a trained neural network.  The disadvantage of such an architecture is that the model is trained on a limited set of words contained in the corpus.  This means that for all words that were not in the corpus of texts at the training stage, there will be no embeddingings.  And this situation often happens when the pre-trained models are used for their tasks: for a part of the words there will be no embeddingings, respectively, a large amount of useful information will be lost. <br><br>  To solve the problem with words that are not in the dictionary, (OOV, out-of-vocabulary) there is an improved embedding model - <a href="https://fasttext.cc/">FastText</a> .  Instead of using separate words to train a neural network, FastText breaks words into n-grams (subwords) and learns them.  To get a vector representation of a word, you need to get the vector representations of the n-gram of this word and put them together. <br><br>  Thus, to obtain feature vectors from messages, you can use the pre-trained Word2Vec and FastText models.  These characteristics can be classified using classical ML classifiers or a fully connected neural network. <br><br><img src="https://habrastorage.org/webt/jb/bp/ma/jbbpma-miqfsht7roadxuk2bap4.png" alt="image"><br>  <i>An example of the output of ‚Äúclosest‚Äù within the meaning of words using pre- <a href="https://fasttext.cc/docs/en/english-vectors.html">trained FastText</a></i> <br><br><h4>  CNN classifier </h4><br>  For the processing and classification of texts from neural network algorithms, recurrent networks (LSTM, GRU) are more often used, since they work well with sequences.  Convolutional networks (CNN) are most often used for image processing, but they <a href="https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f">can</a> also <a href="https://medium.com/jatana/report-on-text-classification-using-cnn-rnn-han-f0e887214d5f">be used</a> in the task of text classification.  Consider how this can be done. <br>  Each message is a matrix in which its vector representation is written on each line for a token (word).  Convolution is applied to such a matrix in a certain way: the convolution filter ‚Äúslides‚Äù across whole rows of the matrix (word vectors), but at the same time captures several words at a time (usually 2-5 words), thus processing words in the context of adjacent words.  Details on how this happens can be seen in the <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">picture</a> . <br><img src="https://habrastorage.org/webt/hx/yd/vf/hxydvfho2bzzmgyjedkkt9lz9gc.png" alt="image"><br>  Why use convolutional networks for word processing when you can use recurrent ones?  The fact is that convolutions work much faster.  Using them for the task of classifying messages can greatly save time on learning. <br><br><h4>  ELMo </h4><br>  ELMo (Embeddings from Language Models) is a model of embeddings based on a language model that was <a href="https://arxiv.org/abs/1802.05365">recently introduced</a> .  The new embedding model is different from Word2Vec and FastText models.  ELMo word vectors have certain advantages: <br><br><ul><li>  The representation of each word depends on the whole context in which it is used. </li><li>  The representation is based on symbols, which makes it possible to form reliable representations for out-of-vocabulary words. </li></ul><br><br>  ELMo can be used for different tasks in NLP.  For example, for our task, the message vectors obtained using ELMo can be sent to the classic ML classifier or use a convolutional or fully meshed network. <br>  The pre-trained ELMo embeddings are quite simple to use for your task, an example of usage can be found <a href="">here</a> . <br><br><h3>  Features of the implementation </h3><br><h4>  API on Flask </h4><br>  The API prototype was written in Flask, as it is easy to use. <br><br><h4>  Two Docker Images </h4><br>  For deployment, we used two docker images: the base one, where all the dependencies were installed, and the main one for launching the application.  This greatly saves assembly time, since the first image is rarely rebuilt and due to this, time is saved during the deployment.  Quite a lot of time is spent on building and downloading machine learning libraries, which is not necessary at every commit. <br><br><h4>  Testing </h4><br>  A specific feature of the implementation of a fairly large number of machine learning algorithms is that even with high values ‚Äã‚Äãof metrics on a validation dataset, the actual quality of the algorithm in production can be low.  Therefore, to test the operation of the algorithm, the whole team used a bot in Slack.  This is very convenient, because any team member can check which answer algorithms give to a specific message.  This method of testing allows you to immediately see how the algorithms will work on live data. <br>  A good alternative is to launch solutions on public sites like Yandex Toloki and AWS Mechanical Turk. <br><br><h3>  Conclusion </h3><br>  We considered several approaches to solving the problem of automatic message moderation and described the features of our implementation. <br>  The main observations obtained during the work: <br><br><ul><li>  A vocabulary search and machine learning algorithm based on TF-IDF and logistic regression made it possible to classify messages quickly, but not always correctly. </li><li>  Neural network algorithms and pre-trained embedding patterns do a better job and can determine toxicity within the meaning of a message. </li></ul><br><br>  Of course, we posted the open demo of <a href="https://www.messenger.com/t/potehatoxic">Poteha Toxic Comment Detection</a> on Facebook bot.  Help us make the bot better! <br><br>  I will be glad to answer questions in the comments. </div><p>Source: <a href="https://habr.com/ru/post/454628/">https://habr.com/ru/post/454628/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454616/index.html">Using AI to improve the efficiency of knowledge workers</a></li>
<li><a href="../454618/index.html">Productivity Pit: How Slack Hurts Our Workflow</a></li>
<li><a href="../454620/index.html">#NoDeployFriday: helps or hurts?</a></li>
<li><a href="../454622/index.html">Kreisel EVEX 910e: Historical Model - New Life</a></li>
<li><a href="../454626/index.html">DevOops yesterday and today</a></li>
<li><a href="../45463/index.html">Too easy.</a></li>
<li><a href="../454634/index.html">Security Week 23: Notepad vulnerability, a million systems with unpatched RDP</a></li>
<li><a href="../454638/index.html">Israeli startup ChargeAfter raised $ 8 million in round A</a></li>
<li><a href="../454640/index.html">Remote debugging of microservice via SSH under VPN in 4 turns</a></li>
<li><a href="../454644/index.html">Training Cisco 200-125 CCNA v3.0. Day 8. Setting the switch</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>