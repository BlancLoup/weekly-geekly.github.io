<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The book "Apache Kafka. Stream processing and data analysis "</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When any enterprise application is running, data is generated: these are log files, metrics, information about user activity, outgoing messages, etc. ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The book "Apache Kafka. Stream processing and data analysis "</h1><div class="post__text post__text-html js-mediator-article"> <a href="https://habr.com/company/piter/blog/421519/"><img src="https://habrastorage.org/webt/t6/ej/tx/t6ejtxboknf_ha_u7radv1gzrls.jpeg" align="left" alt="image"></a>  When any enterprise application is running, data is generated: these are log files, metrics, information about user activity, outgoing messages, etc. Proper manipulation of all this data is just as important as the data itself.  If you are an architect, developer or issuing engineer who wants to solve such problems, but are not familiar with Apache Kafka, then it is from this wonderful book that you will learn how to work with this free streaming platform that allows you to process data queues in real time. <br><br><h3>  Who is this book for? </h3><br>  ‚ÄúApache Kafka.  Stream processing and analysis of data was written for developers who use Kafka APIs in their work, as well as process engineers (also called SRE, DevOps or system administrators) involved in installing, configuring, configuring and monitoring its work in commercial operation.  We also did not forget about data architects and analysis engineers - those responsible for designing and building the entire data infrastructure of a company.  Some chapters, in particular, 3, 4 and 11, are focused on Java developers.  To assimilate them, it is important that the reader is familiar with the basics of the Java programming language, including issues such as exception handling and concurrency. <br><a name="habracut"></a><br>  In other chapters, especially 2, 8, 9, and 10, it is assumed that the reader has experience with Linux and is familiar with configuring the network and data storage on Linux.  In the remainder of the book, Kafka and software architecture are discussed in more general terms, so some special knowledge from readers is not required. <br><br>  Another category of people who may be interested in this book is managers and architects who work not directly with Kafka, but with those who work with it.  It is no less important for them to understand what the guarantees provided by the platform and what the compromises may be, which will have to go to their subordinates and colleagues when creating systems based on Kafka.  This book will be useful to those managers who would like to train their employees to work with Kafka or to make sure that the development team has the necessary information. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Chapter 2. Installing Kafka </h3><br>  Apache Kafka is a Java application that can run on a variety of operating systems, including Windows, MacOS, Linux, etc. In this chapter, we will focus on installing Kafka on Linux, since it is on this operating system that the platform is installed most often.  Linux is also the recommended operating system to deploy Kafka for general use.  Information on installing Kafka on Windows and MacOS can be found in Appendix A. <br><br>  <b>Install Java</b> <br><br>  Before installing ZooKeeper or Kafka, you need to install and configure a Java environment.  It is recommended to use Java 8, and this may be the version included in your operating system or directly downloaded from java.com.  Although ZooKeeper and Kafka will work with the Java Runtime Edition, it is more convenient to use the full Java Development Kit (JDK) when developing utilities and applications.  The above installation steps assume that you have installed JDK version 8.0.51 in the /usr/java/jdk1.8.0_51 directory. <br><br>  <b>Install ZooKeeper</b> <br><br>  Apache Kafka uses ZooKeeper to store metadata about the Kafka cluster, as well as details about customer-consumers (Fig. 2.1).  Although you can also run ZooKeeper using the scripts included in the Kafka distribution, installation of the full version of the ZooKeeper repository from the distribution is very simple. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/a-/ye/ed/a-yeeda4ysojxtp4kcwlnmufwlc.png" alt="image"></div><br>  Kafka has been thoroughly tested with the stable version 3.4.6 of the ZooKeeper repository, which can be downloaded from apache.org. <br><br>  <b>Standalone server</b> <br><br>  The following example demonstrates the installation of ZooKeeper with the basic settings in the / usr / local / zookeeper directory with data stored in the / var / lib / zookeeper directory: <br><br><pre><code class="hljs delphi"># tar -zxf zookeeper-<span class="hljs-number"><span class="hljs-number">3.4</span></span>.<span class="hljs-number"><span class="hljs-number">6</span></span>.tar.gz # mv zookeeper-<span class="hljs-number"><span class="hljs-number">3.4</span></span>.<span class="hljs-number"><span class="hljs-number">6</span></span> /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper # mkdir -p /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/zookeeper # cat &gt; /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper/conf/zoo.cfg &lt;&lt; EOF &gt; tickTime=<span class="hljs-number"><span class="hljs-number">2000</span></span> &gt; dataDir=/<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/zookeeper &gt; clientPort=<span class="hljs-number"><span class="hljs-number">2181</span></span> &gt; EOF # /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper/bin/zkServer.sh start JMX enabled by <span class="hljs-keyword"><span class="hljs-keyword">default</span></span> Using config: /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED # <span class="hljs-keyword"><span class="hljs-keyword">export</span></span> JAVA_HOME=/usr/java/jdk1.<span class="hljs-number"><span class="hljs-number">8.0</span></span>_51 # /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper/bin/zkServer.sh start JMX enabled by <span class="hljs-keyword"><span class="hljs-keyword">default</span></span> Using config: /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/zookeeper/bin/../conf/zoo.cfg Starting zookeeper ... STARTED #</code> </pre> <br>  Now you can check that ZooKeeper, as it should be, works offline by connecting to the client port and sending the four-letter srvr command: <br><br><pre> <code class="hljs pgsql"># telnet localhost <span class="hljs-number"><span class="hljs-number">2181</span></span> Trying ::<span class="hljs-number"><span class="hljs-number">1.</span></span>.. Connected <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> localhost. <span class="hljs-keyword"><span class="hljs-keyword">Escape</span></span> <span class="hljs-type"><span class="hljs-type">character</span></span> <span class="hljs-keyword"><span class="hljs-keyword">is</span></span> <span class="hljs-string"><span class="hljs-string">'^]'</span></span>. srvr Zookeeper <span class="hljs-keyword"><span class="hljs-keyword">version</span></span>: <span class="hljs-number"><span class="hljs-number">3.4</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span><span class="hljs-number"><span class="hljs-number">-1569965</span></span>, built <span class="hljs-keyword"><span class="hljs-keyword">on</span></span> <span class="hljs-number"><span class="hljs-number">02</span></span>/<span class="hljs-number"><span class="hljs-number">20</span></span>/<span class="hljs-number"><span class="hljs-number">2014</span></span> <span class="hljs-number"><span class="hljs-number">09</span></span>:<span class="hljs-number"><span class="hljs-number">09</span></span> GMT Latency min/avg/max: <span class="hljs-number"><span class="hljs-number">0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span>/<span class="hljs-number"><span class="hljs-number">0</span></span> Received: <span class="hljs-number"><span class="hljs-number">1</span></span> Sent: <span class="hljs-number"><span class="hljs-number">0</span></span> Connections: <span class="hljs-number"><span class="hljs-number">1</span></span> Outstanding: <span class="hljs-number"><span class="hljs-number">0</span></span> Zxid: <span class="hljs-number"><span class="hljs-number">0x0</span></span> Mode: standalone Node count: <span class="hljs-number"><span class="hljs-number">4</span></span> <span class="hljs-keyword"><span class="hljs-keyword">Connection</span></span> closed <span class="hljs-keyword"><span class="hljs-keyword">by</span></span> <span class="hljs-keyword"><span class="hljs-keyword">foreign</span></span> host. #</code> </pre> <br>  <b>ZooKeeper Ensemble</b> <br><br>  The ZooKeeper cluster is called an ensemble.  Due to the nature of the algorithm itself, it is recommended that an ensemble include an odd number of servers, for example, 3, 5, etc., since in order for ZooKeeper to respond to requests, most of the members of the ensemble (quorum) must function.  This means that an ensemble of three nodes can work with one idle node.  If there are three knots in the ensemble, there may be two. <br><br>  To set up ZooKeeper servers in an ensemble, they must have a single configuration with a list of all servers, and each server in the data directory must have a myid file with the identifier of this server.  If the hosts in the ensemble are named zoo1.example.com, zoo2.example.com, and zoo3.example.com, then the configuration file might look something like this: <br><br><pre> <code class="hljs pgsql">tickTime=<span class="hljs-number"><span class="hljs-number">2000</span></span> dataDir=/var/lib/zookeeper clientPort=<span class="hljs-number"><span class="hljs-number">2181</span></span> initLimit=<span class="hljs-number"><span class="hljs-number">20</span></span> syncLimit=<span class="hljs-number"><span class="hljs-number">5</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>=zoo1.example.com:<span class="hljs-number"><span class="hljs-number">2888</span></span>:<span class="hljs-number"><span class="hljs-number">3888</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span><span class="hljs-number"><span class="hljs-number">.2</span></span>=zoo2.example.com:<span class="hljs-number"><span class="hljs-number">2888</span></span>:<span class="hljs-number"><span class="hljs-number">3888</span></span> <span class="hljs-keyword"><span class="hljs-keyword">server</span></span><span class="hljs-number"><span class="hljs-number">.3</span></span>=zoo3.example.com:<span class="hljs-number"><span class="hljs-number">2888</span></span>:<span class="hljs-number"><span class="hljs-number">3888</span></span></code> </pre> <br>  In this configuration, initLimit is a period of time during which slave nodes can connect to the master.  The value of syncLimit limits the lag of the slave nodes from the master.  Both values ‚Äã‚Äãare given in tickTime units, that is, initLimit = 20 ¬∑ 2000 ms = 40 s.  The configuration also lists all servers in the ensemble.  They are in server.X = hostname: peerPort: leaderPort with the following parameters: <br><br><ul><li>  X - server identifier.  Must be an integer, but the count may not be from zero and not be consistent; </li><li>  hostname is the host name or IP address of the server; </li><li>  peerPort - TCP port through which the servers of the ensemble interact with each other; </li><li>  leaderPort - TCP port through which the selection of the host node. </li></ul><br>  It is enough that clients can connect to the ensemble via the clientPort port, but the ensemble members should be able to exchange messages with each other on all three ports. <br><br>  In addition to a single configuration file, each server in the dataDir directory must have a myid file.  It should contain the server identifier corresponding to the one given in the configuration file.  After completing these steps, you can start the servers, and they will interact with each other in an ensemble. <br><br><h3>  Installation broker Kafka </h3><br>  After completing the configuration of Java and ZooKeeper, you can install Apache Kafka.  The current release of Apache Kafka can be downloaded at <a href="http://kafka.apache.org/downloads.html">kafka.apache.org/downloads.html</a> . <br><br>  In the following example, we will install the Kafka platform in the / usr / local / kafka directory, configuring it to use the ZooKeeper server running earlier and save the message log segments in the / tmp / kafka-logs directory: <br><br><pre> <code class="hljs pgsql"># tar -zxf kafka_2<span class="hljs-number"><span class="hljs-number">.11</span></span><span class="hljs-number"><span class="hljs-number">-0.9</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span>.tgz # mv kafka_2<span class="hljs-number"><span class="hljs-number">.11</span></span><span class="hljs-number"><span class="hljs-number">-0.9</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span><span class="hljs-number"><span class="hljs-number">.1</span></span> /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka # mkdir /tmp/kafka-logs # export JAVA_HOME=/usr/java/jdk1<span class="hljs-number"><span class="hljs-number">.8</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>_51 # /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/bin/kafka-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">start</span></span>.sh -daemon /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/config/<span class="hljs-keyword"><span class="hljs-keyword">server</span></span>.properties #</code> </pre> <br>  After starting the Kafka broker, you can test its operation by performing some simple cluster operations, including the creation of a test topic, the generation of messages and their consumption. <br><br>  Creating and checking topics: <br><br><pre> <code class="hljs objectivec"><span class="hljs-meta"><span class="hljs-meta"># /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test Created topic </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"test"</span></span></span><span class="hljs-meta">. # /usr/local/kafka/bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test Topic:test PartitionCount:1 ReplicationFactor:1 Configs: Topic: test Partition: 0 Leader: 0 Replicas: 0 Isr: 0 #</span></span></code> </pre> <br>  Generating messages for the test topic: <br><br><pre> <code class="hljs delphi"># /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/bin/kafka-console-producer.sh --broker-list localhost:<span class="hljs-number"><span class="hljs-number">9092</span></span> --topic test Test <span class="hljs-keyword"><span class="hljs-keyword">Message</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Test <span class="hljs-keyword"><span class="hljs-keyword">Message</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> ^D #</code> </pre> <br>  Consumption of messages from the test topic: <br><br><pre> <code class="hljs delphi"># /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/bin/kafka-console-consumer.sh --zookeeper localhost:<span class="hljs-number"><span class="hljs-number">2181</span></span> --topic test --from-beginning Test <span class="hljs-keyword"><span class="hljs-keyword">Message</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> Test <span class="hljs-keyword"><span class="hljs-keyword">Message</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> ^C Consumed <span class="hljs-number"><span class="hljs-number">2</span></span> messages #</code> </pre> <br><h3>  Broker configuration </h3><br>  An example broker configuration that comes with the Kafka distribution is quite suitable for a trial run of a standalone server, but for most installations it will not be enough.  There are many Kafka configuration options that govern all aspects of installation and configuration.  For many of them, you can leave the default values, as they relate to the nuances of the Kafka broker settings, which are not applied until you work with a specific script that requires their use. <br><br><h3>  Basic broker settings </h3><br>  There are several Kafka broker settings that you should consider when deploying a platform in any environment other than a standalone broker on a separate server.  These parameters belong to the main settings of the broker, and most of them need to be changed so that the broker can work in a cluster with other brokers. <br><br>  <b>broker.id</b> <br><br>  Each Kafka broker must have an integer identifier specified by the broker.id parameter.  By default, this value is 0, but can be any number.  The main thing is that it does not happen within the same Kafka cluster.  The choice of a number can be arbitrary, and if necessary, for the sake of convenience, it can be transferred from one broker to another.  It is desirable that this number be somehow related to the host, then it will be more transparent to match the broker identifiers to the hosts under maintenance.  For example, if your host names contain unique numbers (for example, host1.example.com, host2.example.com, etc.), these numbers would be a good choice for broker.id values. <br><br>  <b>port</b> <br><br>  A generic configuration file launches Kafka with a listener on TCP port 9092. You can change this port to any other available port by changing the port configuration parameter.  Keep in mind that when choosing a port with a number less than 1024, Kafka should run as root.  And running Kafka as root is not recommended. <br><br>  <b>zookeeper.connect</b> <br><br>  The path that ZooKeeper uses to store broker metadata is specified using the zookeeper.connect configuration parameter.  In the sample configuration, ZooKeeper runs on port 2181 on the local host machine, which is listed as localhost: 2181.  The format of this parameter is a semicolon-separated list of hostname: port / path strings, including: <br><br><ul><li>  hostname is the hostname or IP address of the ZooKeeper server; </li><li>  port - client port number for the server; </li><li>  / path is an optional ZooKeeper path used as the new root (chroot) path of the Kafka cluster.  If not specified, the root path is used. </li></ul><br>  If the specified chroot path does not exist, it will be created when the broker starts. <br><br>  <b>log.dirs</b> <br><br>  Kafka saves all messages to the hard disk, and these log segments are stored in the directories specified in the log.dirs setting.  It is a comma-separated list of paths on the local system.  If several paths are specified, the broker will save sections in them according to the principle of the least used, while preserving the log segments of the same section along the same path.  Note that the broker will place the new section in the directory where the least partitions are currently stored, and not the least amount of space used, so that even distribution of data across partitions is not guaranteed. <br><br>  <b>num.recovery.threads.per.data.dir</b> <br><br>  Kafka uses a custom thread pool to process log segments.  Currently it is applied: <br><br><ul><li>  during normal start - to open the log segments of each of the sections; </li><li>  run after a crash ‚Äî to check and truncate the log segments of each section; </li><li>  stop - for accurate closing of log segments. </li></ul><br>  By default, only one stream is enabled per log directory.  Since this happens only at start and stop, it makes sense to use a larger number of them to parallelize operations.  When recovering from an incorrect shutdown, the benefits of using this approach can reach several hours if the broker restarts with a large number of partitions!  Remember that the value of this parameter is determined based on one log directory from the number specified using log.dirs.  That is, if the value of the num.recovery.threads.per.data.dir parameter is 8, and three paths are indicated in log.dirs, then the total number of threads is 24. <br><br>  <b>auto.create.topics.enable</b> <br><br>  In accordance with the Kafka default configuration, the broker should automatically create a theme when: <br><br><ul><li>  the manufacturer begins to write in the subject of the message; </li><li>  the consumer starts reading from the message subject; </li><li>  any client requests topic metadata. </li></ul><br>  In many cases, this behavior may be undesirable, especially due to the fact that it is not possible to verify the existence of a topic using the Kafka protocol without causing its creation.  If you manage the creation explicitly, manually or through an initialization system, you can set the auto.create.topics.enable parameter to false. <br><br><h3>  Default Theme Settings </h3><br>  The Kafka server configuration sets a variety of default settings for themes created.  Some of these parameters, including the number of sections and message storage options, can be set for each topic separately using administrator tools (discussed in Chapter 9).  The default values ‚Äã‚Äãin the server configuration should be set equal to the reference values ‚Äã‚Äãsuitable for most cluster topics. <br><br>  <b>num.partitions</b> <br><br>  The num.partitions parameter defines with how many sections a new topic is created, mainly when automatic creation of topics is enabled (which is the default behavior).  The default value of this parameter is 1. Keep in mind that the number of sections for a topic can only be increased, but not reduced.  This means that if it requires fewer sections than indicated in num.partitions, you will have to carefully create it manually (this is discussed in Chapter 9). <br><br>  As discussed in Chapter 1, sections are a way of scaling up topics in a Kafka cluster, so it is important that there are as many as you need to balance the load on messages across the cluster as brokers are added.  Many users prefer the number of sections to be equal to or a multiple of the number of brokers in a cluster.  This makes it possible to evenly distribute sections across brokers, which will lead to an even distribution of the load across messages.  However, this is not a mandatory requirement, because the presence of several themes allows to equalize the load. <br><br>  <b>log.retention.ms</b> <br><br>  Most often, the duration of storing messages in Kafka is limited in time.  The default value is specified in the configuration file using the log.retention.hours parameter and is equal to 168 hours, or 1 week.  However, two other parameters can be used - log.retention.minutes and log.retention.ms.  All these three parameters define the same thing - a period of time after which messages are deleted.  But it is recommended to use the log.retention.ms parameter, because if several parameters are specified, the priority is in the smallest unit of measure, so the value of log.retention.ms will always be used. <br><br>  <b>log.retention.bytes</b> <br><br>  Another way to limit the expiration of a message is based on the total size (in bytes) of the messages that are saved.  The value is set using the log.retention.bytes parameter and is applied separately.  This means that in the case of a topic of eight sections and a value of 1 GB equal to log.retention.bytes, the maximum amount of data stored for this topic will be 8 GB.  Note that the amount of preservation depends on the individual sections, and not on the topic.  This means that in the case of increasing the number of sections for a topic, the maximum amount of data stored using log.retention.bytes will also increase. <br><br>  <b>log.segment.bytes</b> <br><br>  The log save settings mentioned are for log segments, not for individual messages.  As messages are generated by Kafka broker, they are added to the end of the current log segment of the corresponding section.  When the log segment reaches the size specified by the parameter log.segment.bytes and is equal to 1 GB by default, this segment closes and a new one opens.  After closing a segment of the journal can be withdrawn from circulation.  The smaller the size of the log segments, the more often you have to close the files and create new ones, which reduces the overall efficiency of write operations to the disk. <br><br>  Selection of the size of the journal segments is important in the case when the topics are characterized by a low frequency of message generation.  For example, if the topic receives only 100 MB of messages per day, and the default value for the log.segment.bytes parameter is set, it will take 10 days to fill one segment.  And since messages cannot be declared invalid until the log segment is closed, with a value of 604,800,000 (1 week) of the parameter log.retention.ms, by the time the closed log segment is withdrawn from circulation, messages for 17 days can accumulate.  This is because when a segment closes with messages accumulated over 10 days, it has to be stored for another 7 days before it can be taken out of circulation in accordance with the provisional rules, since the segment cannot be deleted before the last message in it expires. . <br><br>  <b>log.segment.ms</b> <br><br>  Another way to control the closure of a log segment is by using the log.segment.ms parameter, which specifies the length of time after which the log segment closes.  Like the log.retention.bytes and log.retention.ms parameters, the log.segment.bytes and log.segment.ms parameters are not mutually exclusive.  Kafka closes the log segment when either the time period expires or the specified size limit is reached, depending on which of these events occurs first.  By default, the value of the log.segment.ms parameter is not specified, resulting in the closure of log segments due to their size. <br><br>  <b>message.max.bytes</b> <br><br>  Kafka broker allows you to limit the maximum size of generated messages using the message.max.bytes parameter.  The default value for this parameter is 1,000,000 (1 MB).  The manufacturer who tries to send a larger message will receive an error notification from the broker, and the message will not be accepted.  As in the case of all other sizes in bytes, specified in the broker settings, we are talking about the size of the compressed message, so that manufacturers can send messages, the size of which in uncompressed form is much larger if they can be compressed to the limits specified by the message.max.bytes parameter . <br><br>  Increasing the permissible message size seriously affects performance.  A larger message size means that broker threads that handle network connections and requests will take longer to process each request.  Also, larger messages increase the amount of data written to disk, which affects I / O throughput. <br><br><h3>  Choice of hardware </h3><br>  Choosing the right hardware for a Kafka broker is more an art than a science.  The Kafka platform itself does not have any strict hardware requirements, it will work without problems on any system.  But if we talk about performance, then it is influenced by several factors: the capacity and throughput of disks, RAM, network and CPU. <br><br>  First, you need to decide which types of performance are most important for your system, after which you can choose the best hardware configuration that fits into the budget. <br><br><h3>  Disk Bandwidth </h3><br>  The throughput of broker disks, which are used to store log segments, directly affects the performance of manufacturing customers.  Kafka messages should be recorded in the local storage, which would confirm their record.  Only after this can the dispatch operation be considered successful.  This means that the faster the write operations to disk, the less delay in the generation of messages will be. <br><br>  The obvious action in case of problems with disk bandwidth is to use hard disk drives with unwinding plates (HDD) or solid state drives (SSD).  The SSD is orders of magnitude lower than the search / access time and higher performance.  HDDs are more economical and have a higher relative capacity.  HDD performance can be improved by increasing their number in a broker, or using several data directories, or installing disks into an array of independent disks with redundancy (redundant array of independent disks, RAID).  Other factors also affect throughput, such as hard drive technology (for example, SAS or SATA), as well as the characteristics of a hard disk controller. <br><br><h3>  Disk capacity </h3><br>  Capacity is another aspect of storage.  The amount of disk space needed is determined by how many messages you need to store simultaneously.  If the broker is expected to receive 1 Tbyte of traffic per day, then with 7-day storage, he will need available for use storage for log segments of at least 7 Tbytes.  You should also take into account the overrun of at least 10% for other files, not counting the buffer for possible fluctuations of traffic or its growth over time. <br><br>  Storage capacity is one of the factors that must be considered when determining the optimal size of a Kafka cluster and deciding on its expansion.  The overall cluster traffic can be balanced with several sections for each topic, which allows using additional brokers to increase the available capacity in cases when there is not enough data density per broker.  The decision on how much disk space is needed is also determined by the replication strategy chosen for the cluster (discussed in more detail in Chapter 6). <br><br><h3>  Memory </h3><br>  In the normal mode of operation, the Kafka consumer reads from the end of the section, with the consumer constantly catching up for lost time and only slightly lagging behind the producers, if at all.            ,      ,         . ,         ,    -. <br><br>   Kafka     JVM      .  ,   X        X   ,      5 .             Kafka        .      Kafka  ,      ,       ,     Kafka. <br><br><h3>     </h3><br>   ,    Kafka,     .    (    )    .     Kafka (   )       .    1       ,       ,      .       ,     (.  6)    (   8).          ,     . <br><br><h3>  CPU </h3><br>     ,      ,           .             .  Kafka, ,              .            .    Kafka  '   .            . <br><br><h3> Kafka    </h3><br> Kafka      , , Amazon Web Services (AWS). AWS     ,     CPU,     .              Kafka.       ,      .                /      SSD.         (, AWS Elastic Block Store).            CPU   . <br>    ,     AWS      m4  r3.    m4    ,        ,      .      r3      SSD-,        .            i2  d2. <br><br><h3>  Kafka </h3><br>   Kafka         ,             (. 2.2).     ‚Äî      .    ‚Äî            .         Kafka        .         Kafka.           6. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/vo/c-/cc/voc-cc5ywmwoh4ttiidnzt1gdk4.png" alt="image"></div><br><br><h3>    ? </h3><br>   Kafka   .    ‚Äî              .     10  ,      2 ,     ‚Äî  .  ,          100 % (    ) (.  6).  ,              . <br><br>   ,   , ‚Äî     . ,                        (         ).        80 %   ,    ,             .     ,      ,   .        ,     ,          . <br><br><h3>   </h3><br>               Kafka.  ‚Äî          zookeeper.connect.    ZooKeeper     .  ‚Äî           broker.id.       broker.id    ,            .      ,    ,      ,    . <br><br><h3>     </h3><br>     Linux      ,       ,           Kafka.          ,    ,        .       /etc/sysctl.conf,        Linux,       . <br><br><h3>   </h3><br>     Linux     .           ,    ¬´¬ª  ,        Kafka. <br>     ,  ,    ,    ()  . ,      ,       Kafka.  , Kafka     ,         ,       . <br><br>      ‚Äî        .  ‚Äî   ,      -   .            .      vm.swappiness  ,  1.      ( ) ,            .     ,   . <br><br>  ,      ¬´¬ª ,      ,   .   Kafka       /.         :        (, SSD),       NVRAM   (, RAID).       ¬´¬ª ,         .       vm.dirty_background_ratio ,     ( 10).       ( ),         5.       0,                           . <br><br>   ¬´¬ª ,               ,      vm.dirty_ratio  ,     ‚Äî 20 (        ).       ,      60  80.               ,       /       .       vm.dirty_ratio       Kafka,     . <br><br>          ¬´¬ª      Kafka        .        /proc/vmstat: <br><br><pre> <code class="hljs objectivec"><span class="hljs-meta"><span class="hljs-meta"># cat /proc/vmstat | egrep </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"dirty|writeback"</span></span></span><span class="hljs-meta"> nr_dirty 3875 nr_writeback 29 nr_writeback_temp 0 #</span></span></code> </pre> <br><h3>  Disk </h3><br>         ,    RAID-    ,           .     ,        EXT4 (fourth extended file system ‚Äî    )  XFS (Extents File System ‚Äî     ). EXT4   ,       .       ,     (5),       .  EXT4     ,            .    XFS     ,   ,   EXT4.  XFS    Kafka  ,        ,    .         ,       /. <br><br>     ,        ,     noatime.      /:   (ctime),    (mtime)       (atime).     atime     .        .  atime    ,   ,      ,         (      realtime). Kafka     atime,      .   noatime       /,        ctime  mtime. <br><br><h3>     </h3><br>       Linux ‚Äî     ,    ,             .     Kafka     ,    -    .     (   ) ,         .          .              net.core.wmem_default  net.core.rmem_default ,      2 097 152 (2 ).   ,           ,       . <br><br>              TCP    net.ipv4.tcp_wmem  net.ipv4.tcp_rmem.        ,   ,       .    ‚Äî 4096 65536 2048000 ‚Äî ,     4 ,    ‚Äî 64 ,   ‚Äî 2 .      ,      net.core.wmem_max  net.core.rmem_max.        Kafka           . <br><br>      .     TCP    1  net.ipv4.tcp_window_scaling,               .   net.ipv4.tcp_max_syn_backlog ,     1024,     .   net.core.netdev_max_backlog,     1000,       ,       ,    ,       . <br><br><h3>   </h3><br>      Kafka      ,             . <br><br><h3>    </h3><br>     Java      ,           ,   .  ,     Java 7     Garbage First (G1). G1                    .         ,       ,           . <br><br>         G1   .       . <br><br><ul><li> MaxGCPauseMillis.         .     ‚Äî   G1    .      200 .  ,  G1       ,    ,    , ,      200 . </li><li> InitiatingHeapOccupancyPercent.        ,       .     45.  ,  G1       ,    45 % ,       (Eden),    . </li></ul><br>  Kafka         ,         .               64   ,  Kafka     5 .       20  MaxGCPauseMillis.    InitiatingHeapOccupancyPercent   35,       ,     . <br><br>   Kafka       G1,            .       .       : <br><br><pre> <code class="hljs pgsql"># export JAVA_HOME=/usr/java/jdk1<span class="hljs-number"><span class="hljs-number">.8</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span>_51 # export KAFKA_JVM_PERFORMANCE_OPTS="-server -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+DisableExplicitGC -Djava.awt.headless=true" # /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/bin/kafka-<span class="hljs-keyword"><span class="hljs-keyword">server</span></span>-<span class="hljs-keyword"><span class="hljs-keyword">start</span></span>.sh -daemon /usr/<span class="hljs-keyword"><span class="hljs-keyword">local</span></span>/kafka/config/<span class="hljs-keyword"><span class="hljs-keyword">server</span></span>.properties #</code> </pre> <br><h3>   </h3><br>          Kafka      ,                 .             -    ,     .         Kafka (.  6),         .        Kafka,       . <br><br>  Kafka             ,  ,                     (    , , AWS),           ,            .          .  ,   ¬´¬ª             (.    6). <br><br>  :    Kafka      ,   ,       ,     .             (     )                   .               , ,     . <br><br><h3>    ZooKeeper </h3><br> Kafka  ZooKeeper     ,   .   ZooKeeper              Kafka.     ,      ZooKeeper    Kafka  .      ZooKeeper      Kafka (     ZooKeeper   ,      ). <br><br>      ZooKeeper     .        ZooKeeper,  Kafka,      .    ZooKeeper  ,        ZooKeeper        .       ‚Äî 1 ,               .        ZooKeeper,      ,     .   ZooKeeper      ,     .  ,      Kafka   Kafka        ZooKeeper. <br><br>         Kafka,       ,    . Kafka         ZooKeeper,          .              ZooKeeper,     .        ,            , ,     .    ,            ,   . <br><br><h3>  Summary </h3><br>       ,     Apache Kafka. ,       ,         . ,   Kafka,         Kafka.             Kafka ( 3),       ( 4). <br><br>  ¬ªMore information about the book can be found on <a href="https://www.piter.com/collection/new/product/apache-kafka-potokovaya-obrabotka-i-analiz-dannyh">the publisher site.</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610575/978544610575_X.pdf">Table of Contents</a> <br>  ¬ª <a href="https://storage.piter.com/upload/contents/978544610575/978544610575_p.pdf">Excerpt</a> <a href="https://storage.piter.com/upload/contents/978544610575/978544610575_p.pdf"><br></a> <br>    20%   ‚Äî <b>Apache Kafka</b> </div><p>Source: <a href="https://habr.com/ru/post/421519/">https://habr.com/ru/post/421519/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421503/index.html">How to write instructions so that you understand</a></li>
<li><a href="../421505/index.html">Mini-life hacking on working with Yandex.Direct</a></li>
<li><a href="../421507/index.html">What were the welders for optics</a></li>
<li><a href="../421513/index.html">Smooth introduction of scram by the developers themselves (resolving contradictions, setting up a team, avoiding conflicts)</a></li>
<li><a href="../421515/index.html">Yandex has three days to remove pirated links, otherwise Roskomnadzor will block Yandex.Video service.</a></li>
<li><a href="../421521/index.html">Monsters after the holidays: AMD Threadripper 2990WX 32-Core and 2950X 16-Core (part 3 - tests)</a></li>
<li><a href="../421523/index.html">Unity: familiarity with Scriptable Objects</a></li>
<li><a href="../421525/index.html">A little bit about the differences between Russian and foreign hosters</a></li>
<li><a href="../421527/index.html">Broadcast launch of the project "Server in the clouds"</a></li>
<li><a href="../421529/index.html">Netflix, Uber, Google and you on MBLT DEV 2018</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>