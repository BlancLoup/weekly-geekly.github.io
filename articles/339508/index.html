<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to run docker-voting on Swarm, Kubernetes and Nomad</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Tl; DR 
 In this article, we will deploy the Docker application, voting on Swarm, Kubernetes and Nomad from Hashicorp. I hope you get the same pleasur...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to run docker-voting on Swarm, Kubernetes and Nomad</h1><div class="post__text post__text-html js-mediator-article">  Tl;  DR <br>  In this article, we will deploy the Docker application, voting on Swarm, Kubernetes and Nomad from Hashicorp.  I hope you get the same pleasure from reading this article as I got when experimenting with all of this. <br><br>  If you are working with technology, then it is necessary to be inquisitive.  This is necessary in order to constantly learn and be aware of what is happening in the field.  Painfully everything changes quickly. <br><br>  Container orchestration is such a hot topic of discussion that even if you have a favorite instrument, it‚Äôs still interesting to see how others work and learn something new about them. <br><a name="habracut"></a><br><h2>  Application for voting </h2><br>  I used the application to vote in previous articles.  The application works on microservice architecture and consists of 5 services. <img src="https://habrastorage.org/getpro/habr/post_images/83a/dc9/fae/83adc9fae997e0fa89535dab867e2f7b.png" alt="image">
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Vote: a frontend that allows the user to choose between a dog and a cat </li><li>  Redis: database where votes are stored </li><li>  Worker: a service that collects radish voices and stores results in a Postgres database. </li><li>  Db: Postgres database that stores voting results. </li><li>  Result: frontend shows voting results </li></ul><br>  As we see in the repository on github, there are several compose files in the application: <a href="https://github.com/dockersamples/example-voting-app">https://github.com/dockersamples/example-voting-app</a> <br><br>  Docker-stack.yml - application presentation ready for use in production.  Here is the file itself: <br><br><pre><code class="plaintext hljs">version: "3" services: redis: image: redis:alpine ports: - "6379" networks: - frontend deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data networks: - backend deploy: placement: constraints: [node.role == manager] vote: image: dockersamples/examplevotingapp_vote:before ports: - 5000:80 networks: - frontend depends_on: - redis deploy: replicas: 2 update_config: parallelism: 2 restart_policy: condition: on-failure result: image: dockersamples/examplevotingapp_result:before ports: - 5001:80 networks: - backend depends_on: - db deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure worker: image: dockersamples/examplevotingapp_worker networks: - frontend - backend deploy: mode: replicated replicas: 1 labels: [APP=VOTING] restart_policy: condition: on-failure delay: 10s max_attempts: 3 window: 120s placement: constraints: [node.role == manager] visualizer: image: dockersamples/visualizer:stable ports: - "8080:8080" stop_grace_period: 1m30s volumes: - "/var/run/docker.sock:/var/run/docker.sock" deploy: placement: constraints: [node.role == manager] networks: frontend: backend: volumes: db-data:</code> </pre> <br>  In general, there are 6 services in this file, and only 5 in the application architecture. The additional service is the visualizer, an excellent tool that provides an interface that shows where the services are deployed. <br><br><h2>  Docker swarm </h2><br>  <i>Docker Swarm is a tool for managing and creating Docker containers.</i>  <i>With Swarm, administrators and developers can create and manage a cluster of nodes as a single virtual system.</i> <br><br><h3>  Swarm components </h3><br>  A Swarm cluster consists of several nodes, some work as managers, others as performers: <br><br><ul><li>  Node managers are responsible for the internal state of the cluster. </li><li>  Implementing nodes perform tasks (= launch containers) </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/dab/383/312/dab383312018e2076c575b54508f76e7.png" alt="image"><br><br>  Managers share internal distributed storage to maintain a consistent cluster state.  This is provided by the Raft logs. <br><br>  In Swarm, services determine how applications should be deployed, and how they should work in containers. <br><br><h3>  Docker installation </h3><br>  If you have not yet installed Docker, you can <a href="https://www.docker.com/community-edition">download Docker CE (Community Edition)</a> for your OS. <br><br><h3>  Create Swarm </h3><br>  Once the Docker is installed, only one team separates us from the working Swarm. <br><br> <code>$ docker swarm init</code> <br> <br>  This is all that is required for a Swarm cluster.  Although this is a cluster with one node, it is still a cluster with all related processes. <br><br><h3>  Deploying the application </h3><br>  Among the compose files available in the application repository on github, we need docker-stack.yml in order to deploy the application through Swarm. <br><br><pre> <code class="plaintext hljs">$ docker stack deploy -c docker-stack.yml app Creating network app_backend Creating network app_default Creating network app_frontend Creating service app_visualizer Creating service app_redis Creating service app_db Creating service app_vote Creating service app_result Creating service app_worker</code> </pre> <br>  Since the stack is running on the docker for poppy, I have access to the application right away from the local machine.  You can select cats or dogs using the voting interface (port 5000), and see the results on port 5001. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d76/bd4/a97/d76bd4a9778ff0b767d2a0cc2ea21f69.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/3f2/7a9/552/3f27a955267a8580f88fca12b9be7011.png" alt="image"><br><br>  I‚Äôll not go into details right now, I just wanted to show how easy it is to deploy an application using Swarm. <br><br>  If you need a more detailed analysis of how to deploy the application through Swarm with several nodes, then you can read <a href="https://medium.com/lucjuggery/deploy-the-voting-apps-stack-on-a-docker-swarm-4390fd5eee4">this article</a> . <br><br><h2>  Kubernetes </h2><br>  <i>Kubernetes is an open source platform for automating the deployment, scaling and management of containerized applications.</i> <br><br><h3>  Kubernetes concept </h3><br>  A Kubernetes cluster consists of one or more Masters and nodes. <br><br><ul><li>  The master is responsible for managing the cluster (managing the state of the cluster, scheduling tasks, responding to an event in the cluster, etc.) </li><li>  Nodes (they used to be called minions. Yes, yes, as in the Despicable Me cartoon) provide runtime to launch the application container (via Pods) </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/6a8/f39/fb3/6a8f39fb39514033d7da84ec7a7ba79a.png" alt="image"><br><br>  To enter commands, use kubectl CLI.  Below we look at several examples of its use. <br><br>  In order to understand how applications are deployed, you need to be aware of several Kubernetes high-level objects: <br><br><ul><li>  Pod is the smallest unit that can be deployed on a node.  This is a group of containers that should work together.  But quite often Pod contains only one container. </li><li>  ReplicaSet provides the work of a specific number of replicas pod. </li><li>  Deployment manages ReplicaSet and allows rolling updates, blue / green deploy, testing, etc. </li><li>  Service defines a logical set of pods and a policy for accessing them. </li></ul><br>  In this part we will use Deployment and Service for each of the application services. <br><br><h3>  Install kubectl </h3><br>  <i><b>Kubectl</b> is the command line for deploying and managing applications in Kubernetes</i> <i><br></i> <br><br>  For installation, use the official documentation (https://kubernetes.io/docs/tasks/tools/install-kubectl/).  For example, to install on a Mac, enter the following commands: <br><br> <code>$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl <br> $ chmod +x ./kubectl <br> $ sudo mv ./kubectl /usr/local/bin/kubectl</code> <br> <br><h3>  Install minicube </h3><br>  Minicube is a comprehensive Kubenetes setup.  It creates local VMs and starts a cluster of nodes, on which all Kubernetes processes run.  Undoubtedly, this is not the tool that should be used to install a production cluster, but it is really convenient to use it for development and testing. <br><br>  Once Minicube is installed, only one command is needed to install a cluster with one node. <br><br> <code>$ minikube start <br> Starting local Kubernetes v1.7.0 cluster‚Ä¶ <br> Starting VM‚Ä¶ <br> Downloading Minikube ISO <br> 97.80 MB / 97.80 MB [==============================================] 100.00% 0s <br> Getting VM IP address‚Ä¶ <br> Moving files into cluster‚Ä¶ <br> Setting up certs‚Ä¶ <br> Starting cluster components‚Ä¶ <br> Connecting to cluster‚Ä¶ <br> Setting up kubeconfig‚Ä¶ <br> Kubectl is now configured to use the cluster.</code> <br> <br><h3>  Kubernetes Descriptor </h3><br>  In Kubernetes, containers are started via <b>ReplicaSet</b> , which is managed by <b>Deployment</b> . <br>  Below is an example .yml file describing <b>Deployment</b> .  <b>ReplicaSet</b> provides the launch of 2 <b>Nginx</b> replicas. <br><br><pre> <code class="plaintext hljs">// nginx-deployment.yml apiVersion: apps/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 # tells deployment to run 2 pods matching the template template: # create pods using pod definition in this template metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80</code> </pre> <br>  To create deployment you need to use CLI kubectl. <br><br>  To create an application consisting of microservices, you need to create a deployment file for each service.  You can do it manually, or you can use <a href="http://kompose.io/">Kompose</a> . <br><br><h3>  Using Kompose to create deployments and services </h3><br>  Kompose is a tool that converts Docker compose files into file descriptors used by Kubernetes.  With this service it turns out more convenient, and it speeds up the migration process. <br><br>  <i>Note:</i> <i><br></i>  <i>Kompose is optional, you can write everything manually, but it significantly speeds up the deployment process</i> <i><br><br></i> <ul><li>  <i>Kompose does not take into account all options used in the Docker Compose file</i> </li><li>  <i>Kompose can be installed on Linux or Mac using the following commands:</i> </li></ul><br> <code># Linux <br> $ curl -L https://github.com/kubernetes/kompose/releases/download/v1.0.0/kompose-linux-amd64 -o kompose <br> # macOS <br> $ curl -L https://github.com/kubernetes/kompose/releases/download/v1.0.0/kompose-darwin-amd64 -o kompose <br> <br> $ chmod +x kompose <br> $ sudo mv ./kompose /usr/local/bin/kompose</code> <br> <br>  Before running <b>docker-stack.yml</b> in Kompose, we change it a bit and remove the deployment key of each service.  This key is not perceived, and because of it errors may occur when generating file descriptors.  You can also delete information about networks.  In Kompose, we will give away a new file, which we call <b>docker-stack-k8s.yml</b> . <br><br><pre> <code class="plaintext hljs">version: "3" services: redis: image: redis:alpine ports: - "6379" db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data vote: image: dockersamples/examplevotingapp_vote:before ports: - 5000:80 depends_on: - redis result: image: dockersamples/examplevotingapp_result:before ports: - 5001:80 depends_on: - db worker: image: dockersamples/examplevotingapp_worker visualizer: image: dockersamples/visualizer:stable ports: - "8080:8080" stop_grace_period: 1m30s volumes: - "/var/run/docker.sock:/var/run/docker.sock" volumes: db-data:</code> </pre> <br>  From the <b>docker-stack-k8s.yml</b> file, <b>we</b> generate handles for the application using the following command: <br><br> <code>$ kompose convert --file docker-stack-k8s.yml <br> WARN Volume mount on the host "/var/run/docker.sock" isn't supported - ignoring path on the host <br> INFO Kubernetes file "db-service.yaml" created <br> INFO Kubernetes file "redis-service.yaml" created <br> INFO Kubernetes file "result-service.yaml" created <br> INFO Kubernetes file "visualizer-service.yaml" created <br> INFO Kubernetes file "vote-service.yaml" created <br> INFO Kubernetes file "worker-service.yaml" created <br> INFO Kubernetes file "db-deployment.yaml" created <br> INFO Kubernetes file "db-data-persistentvolumeclaim.yaml" created <br> INFO Kubernetes file "redis-deployment.yaml" created <br> INFO Kubernetes file "result-deployment.yaml" created <br> INFO Kubernetes file "visualizer-deployment.yaml" created <br> INFO Kubernetes file "visualizer-claim0-persistentvolumeclaim.yaml" created <br> INFO Kubernetes file "vote-deployment.yaml" created <br> INFO Kubernetes file "worker-deployment.yaml" created</code> <br> <br>  We see that for each service a deployment and service file is created. <br>  We received only one warning.  It is connected to the <b>visualizer</b> , because the Docker's socket cannot be connected.  We will not try to start this service, but focus on the rest. <br><br><h3>  Deploy the application </h3><br>  Through <b>kubectl</b> we will create all the components specified in the descriptor file.  Indicate that the files are located in the current folder. <br><br> <code>$ kubectl create -f . <br> persistentvolumeclaim "db-data" created <br> deployment "db" created <br> service "db" created <br> deployment "redis" created <br> service "redis" created <br> deployment "result" created <br> service "result" created <br> persistentvolumeclaim "visualizer-claim0" created <br> deployment "visualizer" created <br> service "visualizer" created <br> deployment "vote" created <br> service "vote" created <br> deployment "worker" created <br> service "worker" created <br> unable to decode "docker-stack-k8s.yml":...</code> <br> <br>  <i>Note: since we left the modified compose file in the current folder, we received an error, because you cannot parse it.</i>  <i>But this error can be ignored without any risk.</i> <br><br>  With the help of these commands you can see the created <b>Services</b> and <b>Deployments</b> . <br><img src="https://habrastorage.org/getpro/habr/post_images/38d/6af/b43/38d6afb43a4cee5602f9e092f1f52a77.jpg" alt="image"><br><br><h3>  We give access to the application from the outside world </h3><br>  To gain access to the interface <b>vote</b> and <b>result,</b> you need to slightly change the services created for them. <br><br>  Here is the generated voice descriptor: <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: io.kompose.service: vote name: vote spec: ports: - name: "5000" port: 5000 targetPort: 80 selector: io.kompose.service: vote status: loadBalancer: {}</code> </pre> <br>  We will change the type of service and replace <b>ClusterIP</b> with <b>NodePort</b> .  <b>ClusterIP</b> makes the service available internally, and <b>NodePort</b> allows publishing the port on each node of the cluster and makes it available to the whole world.  Let's do the same for <b>result</b> , because we want both to <b>vote</b> and to <b>result</b> was access from outside. <br><br><pre> <code class="plaintext hljs">apiVersion: v1 kind: Service metadata: labels: io.kompose.service: vote name: vote spec: type: NodePort ports: - name: "5000" port: 5000 targetPort: 80 selector: io.kompose.service: vote</code> </pre> <br>  As soon as changes are made to both services ( <b>vote</b> and <b>result</b> ), you can re-create them. <br><br> <code>$ kubectl delete svc vote <br> $ kubectl delete svc result <br> $ kubectl create -f vote-service.yaml <br> service "vote" created <br> $ kubectl create -f result-service.yaml <br> service "result" created</code> <br> <br><h3>  Access to the application </h3><br>  Now we will get the details of the <b>vote</b> and <b>result</b> services and get the ports that they provide. <br><img src="https://habrastorage.org/getpro/habr/post_images/fa4/ef5/fec/fa4ef5fec0825c7bd92faafb5c01483c.jpg" alt="image"><br><br>  <b>The vote</b> is available on port 30069, and the <b>result</b> is 31873. Now we vote and see the results. <br><img src="https://habrastorage.org/getpro/habr/post_images/831/e3c/dd4/831e3cdd4ddae53a267bbe469bb6baef.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/e8b/5d0/7ba/e8b5d07baeee81a86ae7b58ecdc6c5b5.png" alt="image"><br><br>  After we dealt with the basic components of Kubernetes, we were able to easily deploy the application.  And Kompose helped us a lot. <br><br><h2>  Hashicorp's Nomad </h2><br>  <i>Nomad is a tool for managing a cluster of machines and running an application on them.</i>  <i>It abstracts the machines and the point of the application and lets users say what they want to run.</i>  <i>And Nomad is responsible for where it will be launched and how.</i> <br><br><h3>  Nomad concept </h3><br>  The Nomad cluster consists of <b>agents</b> (agents) that can operate in <b>server</b> (server) mode or <b>client</b> (client) mode. <br><br><ul><li>  The server is responsible for the <a href="https://www.nomadproject.io/docs/internals/consensus.html">consensus protocol</a> , which allows the server to choose a leader and perform state replication. </li><li>  Clients are very light, because they interact with the server, while doing almost nothing themselves.  In client nodes, tasks are executed. </li></ul><br><img src="https://habrastorage.org/getpro/habr/post_images/c9d/c09/d85/c9dc09d852f4490132512719c5792a11.png" alt="image"><br><br>  Several types of tasks can be run on a Nomad cluster. <br><br>  In order to deploy the application you need to understand the basic concepts of Nomad: <br><br><ul><li>  Job - defines what tasks Nomad should perform.  This is described in a job file (text file in hcl format, Hashicorp Configuration Language).  A job can contain one or more tasks groups. </li><li>  Group contains several tasks that are located on the same machine. </li><li>  Task is a running process, in our case it is a Docker container </li><li>  Mapping tasks in a job is done using Allocations.  Allocation is used to ensure that tasks in a job are executed on a specific node. </li></ul><br><h3>  Installation </h3><br>  In this example, we will run the application on the Docker host, created using the Docker Machine.  The local IP is 192.168.1.100.  First, run Consul, which is used to discover and register services.  We will launch Nomad and deploy the application as Job in Nomad. <br><br><h3>  Consul for registration and discovery services </h3><br>  For detecting and registering services, a tool is recommended, for example, Consul, which will not work as Job in Nomad.  Consul can be <a href="https://www.consul.io/downloads.html">downloaded here</a> . <br><br>  This command starts the Consul server locally: <br><br> <code>$ consul agent -dev -client=0.0.0.0 -dns-port=53 -recursor=8.8.8.8</code> <br> <br>  Let's take a closer look at the options used: <br><ul><li>  - <b>dev</b> ‚Äî the flag that sets up the Consul cluster with the server and client.  This option should only be used for development and testing. </li><li>  <b>-client = 0.0.0.0</b> allows you to reach Consul services (API and DNS server) through any host interface.  This is necessary because Nomad will join the Consul via the localhost interface, and the containers through the docker-bridge (something like 172.17.x.x). </li><li>  <b>-dns-port = 53</b> specifies the port that Consul‚Äôs DNS server will use (default is 8600).  We will install the standard port 53 so that DNS can be used from the container. </li><li>  <b>-recursor = 8.8.8.8</b> defines another DNS server that will handle requests that Consul cannot handle </li></ul><br>  Nomad can be downloaded <a href="https://www.nomadproject.io/downloads.html">from this link</a> . <br><br><h3>  Create a cluster with a node </h3><br>  We downloaded Nomad and now we can run the Agent with the following settings. <br><br><pre> <code class="plaintext hljs">// nomad.hcl bind_addr = "0.0.0.0" data_dir = "/var/lib/nomad" server { enabled = true bootstrap_expect = 1 } client { enabled = true network_speed = 100 }</code> </pre> <br>  The agent will work both as a server and as a client.  We point out that <b>bind_addr</b> should work with any interface so that tasks can be received from the outside world.  Launch the Nomad Agent with the following settings: <br><br> <code>$ nomad agent -config=nomad.hcl <br> ==&gt; WARNING: Bootstrap mode enabled! Potentially unsafe operation. <br> Loaded configuration from nomad-v2.hcl <br> ==&gt; Starting Nomad agent... <br> ==&gt; Nomad agent configuration: <br> Client: true <br> Log Level: INFO <br> Region: global (DC: dc1) <br> Server: true <br> Version: 0.6.0 <br> ==&gt; Nomad agent started! Log data will stream in below:</code> <br> <br>  <i>Note: By default, Nomad connects to the local Consul instance.</i> <br>  We have just installed a cluster with one node.  Here is the information on the unique participant: <br><br> <code>$ nomad server-members <br> Name Address Port Status Leader Protocol Build Datacenter Region <br> neptune.local.global 192.168.1.100 4648 alive true 2 0.6.0 dc1 global</code> <br> <br><h3>  Deploy the application </h3><br>  To deploy an application using Swarm, you can use the compose file right away.  To deploy via Kubernetes, we need descriptors from the same compose files.  How does all this happen through Nomad? <br><br>  Firstly, there is no tool similar to Kompose for Hashicorp, so that it can simplify the migration of compose to Nomad (a good idea for an OpenSource project, by the way).  Files describing <b>Jobs</b> , <b>groups</b> , <b>tasks</b> must be written manually. <br><br>  We will analyze this in more detail when we describe Jobs for the <b>Redis</b> and <b>Vote</b> services.  For other services, it will look about the same. <br><br><h3>  Determine Job for Redis </h3><br>  This file defines the part of Redis in the application: <br><br><pre> <code class="plaintext hljs">// redis.nomad job "redis-nomad" { datacenters = ["dc1"] type = "service" group "redis-group" { task "redis" { driver = "docker" config { image = "redis:3.2" port_map { db = 6379 } } resources { cpu = 500 # 500 MHz memory = 256 # 256MB network { mbits = 10 port "db" {} } } service { name = "redis" address_mode = "driver" port = "db" check { name = "alive" type = "tcp" interval = "10s" timeout = "2s" } } } } }</code> </pre> <br>  Let's look at what is written here: <br><br><ul><li>  Job Name - redis-nomad </li><li>  Job type is a service (i.e. a lengthy operation) </li><li>  The group is given an arbitrary name;  contains one operation </li><li>  Task Redis uses docker-driver, which means it will be launched in a container </li><li>  Task will use Redis image: 3.2 </li><li>  In the resource block there are limitations for CPU and memory. </li><li>  The network block indicates that the db port must be dynamic. </li><li>  In the service block, it is defined how the Consul registration will take place: service name, IP address and health check definition </li></ul><br>  In order to check whether the Job will run correctly, use the <b>plan</b> command: <br><br><pre> <code class="plaintext hljs">$ nomad plan redis.nomad + Job: "nomad-redis" + Task Group: "cache" (1 create) + Task: "redis" (forces create) Scheduler dry-run: - All tasks successfully allocated. Job Modify Index: 0 To submit the job with version verification run: nomad run -check-index 0 redis.nomad When running the job with the check-index flag, the job will only be run if the server side version matches the job modify index returned. If the index has changed, another user has modified the job and the plan's results are potentially invalid.</code> </pre> <br>  It seems everything works.  Now deploy the task with this job: <br><br><pre> <code class="plaintext hljs">$ nomad run redis.nomad ==&gt; Monitoring evaluation "1e729627" Evaluation triggered by job "nomad-redis" Allocation "bf3fc4b2" created: node "b0d927cd", group "cache" Evaluation status changed: "pending" -&gt; "complete" ==&gt; Evaluation "1e729627" finished with status "complete"</code> </pre> <br>  We see that the placement is created.  Check its status: <br><br><pre> <code class="plaintext hljs">$ nomad alloc-status bf3fc4b2 ID = bf3fc4b2 Eval ID = 1e729627 Name = nomad-redis.cache[0] Node ID = b0d927cd Job ID = nomad-redis Job Version = 0 Client Status = running Client Description = &lt;none&gt; Desired Status = run Desired Description = &lt;none&gt; Created At = 08/23/17 21:52:03 CEST Task "redis" is "running" Task Resources CPU Memory Disk IOPS Addresses 1/500 MHz 6.3 MiB/256 MiB 300 MiB 0 db: 192.168.1.100:21886 Task Events: Started At = 08/23/17 19:52:03 UTC Finished At = N/A Total Restarts = 0 Last Restart = N/A Recent Events: Time Type Description 08/23/17 21:52:03 CEST Started Task started by client 08/23/17 21:52:03 CEST Task Setup Building Task Directory 08/23/17 21:52:03 CEST Received Task received by client</code> </pre><br>  The container is running correctly.  Check the Consul DNS server and make sure that the service also registers correctly: <br><br><pre> <code class="plaintext hljs">$ dig @localhost SRV redis.service.consul ; &lt;&lt;&gt;&gt; DiG 9.10.3-P4-Ubuntu &lt;&lt;&gt;&gt; @localhost SRV redis.service.consul ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 35884 ;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 2 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 4096 ;; QUESTION SECTION: ;redis.service.consul. IN SRV ;; ANSWER SECTION: redis.service.consul. 0 IN SRV 1 1 6379 ac110002.addr.dc1.consul. ;; ADDITIONAL SECTION: ac110002.addr.dc1.consul. 0 IN A 172.17.0.2 ;; Query time: 0 msec ;; SERVER: 127.0.0.1#53(127.0.0.1) ;; WHEN: Wed Aug 23 23:08:36 CEST 2017 ;; MSG SIZE rcvd: 103</code> </pre><br>  Task was placed on IP 172.17.0.2, and its port is 6379, as we indicated. <br><br><h3>  Determine Job for Vote </h3><br>  Define a job for the <b>vote</b> service.  Use the following file: <br><br><pre> <code class="plaintext hljs">// job.nomad job "vote-nomad" { datacenters = ["dc1"] type = "service" group "vote-group" { task "vote" { driver = "docker" config { image = "dockersamples/examplevotingapp_vote:before" dns_search_domains = ["service.dc1.consul"] dns_servers = ["172.17.0.1", "8.8.8.8"] port_map { http = 80 } } service { name = "vote" port = "http" check { name = "vote interface running on 80" interval = "10s" timeout = "5s" type = "http" protocol = "http" path = "/" } } resources { cpu = 500 # 500 MHz memory = 256 # 256MB network { port "http" { static = 5000 } } } } } }</code> </pre> <br>  But there are several differences from the file that we used for Redis: <br><br><ul><li>  <b>Vote</b> connects to <b>redis</b> using only the name of the operation.  Here is an example of a part of the <i>app.py</i> file used in the <b>vote</b> service: </li></ul><br><pre> <code class="python hljs">// app.py <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">get_redis</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> hasattr(g, <span class="hljs-string"><span class="hljs-string">'redis'</span></span>): g.redis = Redis(host=<span class="hljs-string"><span class="hljs-string">"redis"</span></span>, db=<span class="hljs-number"><span class="hljs-number">0</span></span>, socket_timeout=<span class="hljs-number"><span class="hljs-number">5</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> g.redis</code> </pre> <br>  In this case, to obtain the IP container with <b>redis, the</b> container with <b>vote</b> must use the Consul DNS server.  DNS query from the container is executed via the Docker bridge (172.17.0.1).  <b>dns_search_domains</b> specifies that Service X is registered as X.service.dc1.consul within Consul <br><br><ul><li>  We installed a static port so that the vote service on port 5000 is accessible from outside the cluster. </li></ul><br>  We can do the same setup for other services: worker, postgres and result. <br><br><h3>  Access to the application </h3><br>  When all the Jobs are running, you can check their status and make sure everything works. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/158/ac1/2cc/158ac12cc671e9044fdfe4ec2b39e916.jpg" alt="image"><br><br>  We can also view this through the Consul interface. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f7e/797/f16/f7e797f1615e9ea3935abdb7e615fc2a.png" alt="image"><br><br>  By IP nodes (in our case 192.168.1.100), we get access to interfaces with a <b>vote</b> and <b>result</b> . <br><br><h2>  Total </h2><br>  Here is such a wonderful application from the point of view of the demonstration.  I was wondering if it could be deployed without changes in the code with the help of some orchestrator.  And yes, it is possible, even without some special dances with a tambourine. <br><br>  I hope this article will help you understand the basics of Swarm, Kubernetes and Nomad.  It would also be interesting to know what you are launching at Docker and how you use the orchestrator. </div><p>Source: <a href="https://habr.com/ru/post/339508/">https://habr.com/ru/post/339508/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../339498/index.html">Overview of one Russian RTOS, part 7. Means of data exchange between tasks</a></li>
<li><a href="../339500/index.html">What the Android DevOps-engineer head hurts</a></li>
<li><a href="../339502/index.html">Fintech track GenerationS - another chance for your fintech startup</a></li>
<li><a href="../339504/index.html">Generalized search for paths for AI in platformers</a></li>
<li><a href="../339506/index.html">OpenCV. Search for road signs using contour analysis in Android</a></li>
<li><a href="../339510/index.html">As we scrum scaled</a></li>
<li><a href="../339512/index.html">Uneducated youth: yes, another post from the student‚Äôs point of view</a></li>
<li><a href="../339516/index.html">Zabbix conference 2017: how was the first day</a></li>
<li><a href="../339518/index.html">The Unified State Automated Information System is not as terrible as they are frightened: what will the unique DataMobile module of the Unified State Automated Information System help retail stores with?</a></li>
<li><a href="../339520/index.html">We meet PostgreSQL 10. Translation Release Notes</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>