<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Delivery of billions of messages strictly once</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The only requirement for all data transmission systems is that data cannot be lost . Data can usually arrive late or can be queried again, but you can...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Delivery of billions of messages strictly once</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/fe8/cf5/228/fe8cf5228d1346f8bb955a2cbe24509a.png" align="left">  The only requirement for all data transmission systems is that <b>data cannot be <i>lost</i></b> .  Data can usually arrive late or can be queried again, but you can never lose it. <br><br>  To meet this requirement, most distributed systems guarantee <i><a href="http://www.cloudcomputingpatterns.org/at_least_once_delivery/">at least one-time delivery</a></i> .  The ‚Äúat least one-time delivery‚Äù provisioning techniques are usually reduced to <i>‚Äúrepeat, repeat and repeat‚Äù</i> .  You never consider the message delivered until you receive a clear confirmation from the client. <br><br>  But as a user <i>, at least a one-time delivery</i> is <i>not exactly</i> what I want.  I want messages delivered <b>once</b> .  And <i>only</i> once. <br><a name="habracut"></a><br>  Unfortunately, to <a href="http://bravenewgeek.com/you-cannot-have-exactly-once-delivery/">achieve something close to <i>exactly one-time delivery, you</i> need an impenetrable design</a> .  The architecture provides that each case of failure must be carefully considered - it will not be possible to simply register it as part of an existing implementation after the very fact of failure.  And even then it is almost impossible to implement a system in which messages are delivered only once. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Over the past three months, we have developed a completely new deduplication system, and as close as possible to exactly one-time delivery, faced with a large number of various failures. <br><br>  The new system is able to track 100 times more messages than the old one; it differs from it in increased reliability and lower cost.  This is how we did it. <br><br><h1>  Problem </h1><br>  Most internal Segment systems handle glitches elegantly with replay, secondary message sending, blocking, and <a href="https://en.wikipedia.org/wiki/Two-phase_commit_protocol">two-stage commits</a> .  But there is one notable exception: <b>customers who send data directly to our public API</b> . <br><br>  Clients (especially mobile) often experience communication disruptions, when they can send data, but skip the response from our API. <br><br>  Imagine that you are traveling by bus and booked a room from the <a href="https://www.hoteltonight.com/">HotelTonight</a> application on your iPhone.  The application starts downloading data to the Segment servers, but the bus suddenly enters the tunnel and you lose connection.  Some of the events you sent are already processed, but the client will never receive a response from the server. <br><br>  In such cases, clients repeat sending <i>the same</i> events to the Segment API despite the fact that the server has technically received previously exactly the same messages. <br><br>  Judging by the statistics of our server, approximately <b>0.6% of</b> the events received in the last four weeks are repeated messages that we have already received. <br><br>  The level of error may seem insignificant.  But for an e-commerce application that generates billions of dollars in revenue, a difference of <b>0.6%</b> can mean the difference between profit and loss of millions of dollars. <br><br><h1>  Message Deduplication </h1><br>  So, we understand the essence of the problem - you need to remove duplicate messages that are sent to the API.  But how to do that? <br><br>  At the theoretical level, the high-level API of any deduplication system seems simple.  In Python ( <i>aka pseudo-pseudocode</i> ) we can represent it as follows: <br><br><pre><code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dedupe</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(stream)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> message <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> stream: <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> has_seen(message.id): discard(message) <span class="hljs-keyword"><span class="hljs-keyword">else</span></span>: publish_and_commit(message)</code> </pre> <br>  For each message in the stream, it is first checked whether such a message was encountered earlier (by its unique identifier).  If met, then discard it.  If not met, then reissue the message and atomically transmit. <br><br>  In order not to store all messages permanently, a ‚Äúdeduplication window‚Äù operates, which is defined as the storage time of our keys until their expiration date.  If messages do not fit into the window, they are considered obsolete.  We want to ensure that only one message with the given ID is sent in the window. <br><br>  This behavior is easy to describe, but there are two details that require special attention: <b>read / write performance</b> and <b>accuracy</b> . <br><br>  We want the system to deduplicate the billions of events in our data stream ‚Äî and at the same time with low latency and cost effectiveness. <br><br>  Moreover, we want to make sure that information about registered events is securely stored, so that we can restore it in case of failure, and that there will never be repeated messages in the output. <br><br><h1>  Architecture </h1><br>  To achieve this, we created a ‚Äútwo-stage‚Äù architecture that reads data from Kafka and removes duplicate events already registered in the four-week window. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3eb/6bb/7fa/3eb6bb7fa0dbafae90c985f1efd49a7a.png"><br>  <font color="gray">High-level deduplication architecture</font> <br><br><h1>  Kafka topology </h1><br>  To understand the work of such an architecture, first take a look at the <a href="https://kafka.apache.org/">Kafka</a> flow topology.  All incoming API calls are divided into separate messages and clearly express the Kafka input section. <br><br>  First, each incoming message is marked with a unique <code>messageId</code> , which is generated on the client side.  This is usually UUIDv4 (although we are considering switching to <a href="https://segment.com/blog/a-brief-history-of-the-uuid/">ksuid</a> ).  If the client does not report messageId, then we automatically assign it at the API level. <br><br>  We do not use vector clocks or sequence numbers, because we do not want to complicate the client side.  Using UUIDs allows <i>anyone to</i> easily send data to our API, because almost every major programming language supports it. <br><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"messageId"</span></span>: <span class="hljs-string"><span class="hljs-string">"ajs-65707fcf61352427e8f1666f0e7f6090"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"anonymousId"</span></span>: <span class="hljs-string"><span class="hljs-string">"e7bd0e18-57e9-4ef4-928a-4ccc0b189d18"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"timestamp"</span></span>: <span class="hljs-string"><span class="hljs-string">"2017-06-26T14:38:23.264Z"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"page"</span></span> }</code> </pre> <br>  Separate messages are logged in Kafka log for durability and repeatability.  They are distributed over the messageId, so that we can be sure that the same <code>messageId</code> <i>always</i> go to the same handler. <br><br>  This is an important detail when it comes to data processing.  Instead of searching the central database for a key among <b>hundreds of billions of</b> messages, we were able to narrow the search space by orders of magnitude simply by redirecting the search query to a specific section. <br><br>  Deduplication Worker is a Go program that reads Kafka input sections.  She is responsible for reading messages, checking for duplicates, and if the messages are new, for sending them to the Kafka exit theme. <br><br>  In our experience, it is extremely easy to manage the workers and topology of Kafka.  We no longer have many large <a href="https://memcached.org/">Memcached</a> instances that require fault tolerant replicas.  Instead, we used the built-in <a href="http://rocksdb.org/">RocksDB</a> databases, which do not need coordination at all and provide us with robust storage at an exceptionally low price.  Now more about this. <br><br><h1>  RocksDB Worker </h1><br>  Each worker stores a local <a href="http://rocksdb.org/">RocksDB database</a> on its local EBS hard drive.  RocksDB is a built-in <a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-building-and-open-sourcing-rocksdb/10151822347683920/">key-value repository developed by Facebook</a> and optimized for extremely high performance. <br><br>  Whenever an event is retrieved from input sections, the consumer requests RocksDB to determine if such a <code>messageId</code> previously been encountered. <br><br>  If the message is not in RocksDB, we add the key to the database, and then publish the message in the output sections of Kafka. <br><br>  If the message is already in RocksDB, the worker simply does not publish it to the output sections and updates the data in the input section with the notification that it has processed the message. <br><br><h1>  Performance </h1><br>  In order to achieve high performance from our database, we need to correspond to three types of requests for each event processed: <br><br><ol><li>  <b>Detection of the existence of</b> random keys that arrive at the input, but are unlikely to be stored in our database.  They can be located anywhere in the key space. </li><li>  <b>Record</b> new keys with high performance. </li><li>  <b>Recognition of obsolete</b> old keys that did not fall into our ‚Äúdeduplication window‚Äù. </li></ol><br>  As a result, we have to continuously scan the entire database, add new keys <i>and</i> invalidate old keys.  Ideally, this should occur within the framework of the previous data model. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ae5/dd9/dd5/ae5dd9dd536cec145493c215c3385c60.png"><br>  <font color="gray">Our database must satisfy three very different types of queries.</font> <br><br>  Generally speaking, the main part of the performance gain comes from the performance of the database - so it makes sense to understand the RocksDB device, which is why it performs the work so well. <br><br>  RocksDB is a <a href="https://en.wikipedia.org/wiki/Log-structured_merge-tree">log-structured tree (LSM-tree)</a> , that is, it continuously adds new keys to the <b>write-ahead log</b> on disk, and also stores sorted keys in memory as part of <b>memtable</b> . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/6f6/14f/c23/6f614fc23b1238c09aa54c26b0db8293.png"><br>  <font color="gray">Keys are sorted in memory as part of memtable</font> <br><br>  Key collection is an extremely fast process.  New items are written directly to disk by appending to the log (for direct saving and restoring in case of failure), and data records are sorted in memory to provide quick search and batch recording. <br><br>  Whenever a sufficient number of records arrive in <b>memtable</b> , they are saved to disk as <b>SSTable</b> (sorted rows table).  Since the strings have already been sorted in memory, they can be directly flushed to disk. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f94/240/88f/f9424088fe7bd468c8dc543b6ee7ff02.png"><br>  <font color="gray">The current state of the memtable is reset to disk as SSTable at level zero (Level 0)</font> <br><br>  Here is an example of such a reset from our working logs: <br><br> <code>[JOB 40] Syncing log #655020 <br> [default] [JOB 40] Flushing memtable with next log file: 655022 <br> [default] [JOB 40] Level-0 flush table #655023: started <br> [default] [JOB 40] Level-0 flush table #655023: 15153564 bytes OK <br> [JOB 40] Try to delete WAL files size 12238598, prev total WAL file size 24346413, number of live WAL files 3.</code> <br> <br>  Each SSTable table remains unchanged ‚Äî after it is created, it never changes ‚Äî thanks to this, the entry of new keys occurs so quickly.  No need to update files, and the record does not generate new records.  Instead, several SSTable tables at the same ‚Äúlevel‚Äù are merged into one file during the out-of-band compacting phase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/171/ffb/dc9/171ffbdc9c30a9e50030f0e64764c892.png"><br><br>  When individual SSTable tables are consolidated from one level, their keys are merged together, and then the new file is transferred to a higher level.  In our working logs you can find examples of such seals.  In this case, process 41 compacts four zero-level files and combines them into a larger first-level file. <br><br> <code>/data/dedupe.db$ head -1000 LOG | grep "JOB 41" <br> [JOB 41] Compacting 4@0 + 4@1 files to L1, score 1.00 <br> [default] [JOB 41] Generated table #655024: 1550991 keys, 69310820 bytes <br> [default] [JOB 41] Generated table #655025: 1556181 keys, 69315779 bytes <br> [default] [JOB 41] Generated table #655026: 797409 keys, 35651472 bytes <br> [default] [JOB 41] Generated table #655027: 1612608 keys, 69391908 bytes <br> [default] [JOB 41] Generated table #655028: 462217 keys, 19957191 bytes <br> [default] [JOB 41] Compacted 4@0 + 4@1 files to L1 =&gt; 263627170 bytes</code> <br> <br>  After the completion of compaction, the combined SSTable tables become the final set of database records, and the old SSTable tables are unlinked. <br><br>  If you look at the working instance, we will see how this forward-looking journal is updated, as well as how separate SSTable tables are written, read, and merged. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/9f2/4ec/03f/9f24ec03f8618b08c06c407983c21485.png"><br>  <font color="gray">The log and most recent SSTable tables take up the lion's share of I / O operations.</font> <br><br>  If you look at the SSTable statistics on the production server, you will see four "levels" of files, with increasing file sizes at each higher level. <br><br><pre> <code class="hljs haskell">** <span class="hljs-type"><span class="hljs-type">Compaction</span></span> <span class="hljs-type"><span class="hljs-type">Stats</span></span> [<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>] ** <span class="hljs-type"><span class="hljs-type">Level</span></span> <span class="hljs-type"><span class="hljs-type">Files</span></span> <span class="hljs-type"><span class="hljs-type">Size</span></span>(<span class="hljs-type"><span class="hljs-type">MB</span></span>} <span class="hljs-type"><span class="hljs-type">Score</span></span> <span class="hljs-type"><span class="hljs-type">Read</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">Rn</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">Rnp1</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">Write</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">Wnew</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">Moved</span></span>(<span class="hljs-type"><span class="hljs-type">GB</span></span>} <span class="hljs-type"><span class="hljs-type">W</span></span>-<span class="hljs-type"><span class="hljs-type">Amp</span></span> <span class="hljs-comment"><span class="hljs-comment">-------------------------------------------------------------------------------------------- L0 1/0 14.46 0.2 0.0 0.0 0.0 0.1 0.1 0.0 0.0 L1 4/0 194.95 0.8 0.5 0.1 0.4 0.5 0.1 0.0 4.7 L2 48/0 2551.71 1.0 1.4 0.1 1.3 1.4 0.1 0.0 10.7 L3 351/0 21735.77 0.8 2.0 0.1 1.9 1.9 -0.0 0.0 14.3 Sum 404/0 24496.89 0.0 3.9 0.4 3.5 3.9 0.3 0.0 34.2 Int 0/0 0.00 0.0 3.9 0.4 3.5 3.9 0.3 0.0 34.2</span></span></code> </pre> <pre> <code class="hljs lisp">Rd(<span class="hljs-name"><span class="hljs-name">MB/s</span></span>} Wr(<span class="hljs-name"><span class="hljs-name">MB/s</span></span>} Comp(<span class="hljs-name"><span class="hljs-name">sec</span></span>} Comp(<span class="hljs-name"><span class="hljs-name">cnt</span></span>} Avg(<span class="hljs-name"><span class="hljs-name">sec</span></span>} KeyIn KeyDrop <span class="hljs-number"><span class="hljs-number">0.0</span></span> <span class="hljs-number"><span class="hljs-number">15.6</span></span> <span class="hljs-number"><span class="hljs-number">7</span></span> <span class="hljs-number"><span class="hljs-number">8</span></span> <span class="hljs-number"><span class="hljs-number">0.925</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">20.9</span></span> <span class="hljs-number"><span class="hljs-number">20.8</span></span> <span class="hljs-number"><span class="hljs-number">26</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">12.764</span></span> <span class="hljs-number"><span class="hljs-number">12</span></span>M <span class="hljs-number"><span class="hljs-number">40</span></span> <span class="hljs-number"><span class="hljs-number">19.4</span></span> <span class="hljs-number"><span class="hljs-number">19.4</span></span> <span class="hljs-number"><span class="hljs-number">73</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">36.524</span></span> <span class="hljs-number"><span class="hljs-number">34</span></span>M <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">18.1</span></span> <span class="hljs-number"><span class="hljs-number">16.9</span></span> <span class="hljs-number"><span class="hljs-number">112</span></span> <span class="hljs-number"><span class="hljs-number">2</span></span> <span class="hljs-number"><span class="hljs-number">56.138</span></span> <span class="hljs-number"><span class="hljs-number">52</span></span>M <span class="hljs-number"><span class="hljs-number">3378</span></span>K <span class="hljs-number"><span class="hljs-number">18.2</span></span> <span class="hljs-number"><span class="hljs-number">18.1</span></span> <span class="hljs-number"><span class="hljs-number">218</span></span> <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">15.589</span></span> <span class="hljs-number"><span class="hljs-number">98</span></span>M <span class="hljs-number"><span class="hljs-number">3378</span></span>K <span class="hljs-number"><span class="hljs-number">18.2</span></span> <span class="hljs-number"><span class="hljs-number">18.1</span></span> <span class="hljs-number"><span class="hljs-number">218</span></span> <span class="hljs-number"><span class="hljs-number">14</span></span> <span class="hljs-number"><span class="hljs-number">15.589</span></span> <span class="hljs-number"><span class="hljs-number">98</span></span>M <span class="hljs-number"><span class="hljs-number">3378</span></span>K</code> </pre> <br>  RocksDB stores the <a href="https://en.wikipedia.org/wiki/Bloom_filter">Bloom</a> indexes and <a href="https://en.wikipedia.org/wiki/Bloom_filter">filters of</a> specific SSTable tables in these tables themselves - and they are loaded into memory.  These filters and indexes are then queried for a specific key, and then the complete SSTable table is loaded into memory as part of the LRU. <br><br>  In the absolute majority of cases, we see <i>new</i> messages that make our system of deduplication a classic case of using Bloom filters. <br><br>  Bloom's filters say whether the key is ‚Äúprobably belongs to the set‚Äù or ‚Äúdefinitely does not belong to the set‚Äù.  To produce a response, the filter saves many bits after applying different hash functions for each item that occurred before.  If all the bits from the hash function converge with the set, then it gives the answer "probably belongs to the set". <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d0/ad2/30e/5d0ad230e02c20c4c8343760797680d5.png"><br>  <font color="gray">Query the letter w in the Bloom filter when our set contains only {x, y, z}.</font>  <font color="gray">The filter returns the answer "does not belong to the set", since one of the bits does not converge</font> <br><br>  If the answer is ‚Äúprobably belongs to the set‚Äù, then RocksDB can query the source data from our SSTable tables and determine if the item really is in the set.  But in most cases, we can generally avoid querying any tables, because the filter returns the answer "definitely does not belong to the set." <br><br>  When we access RocksDB, we create a <b>MultiGet</b> request for all the relevant <code>messageId</code> we want to query.  We create it as part of a package for the sake of performance and to avoid many parallel blocking operations.  It also allows us to package data from Kafka, and usually avoids random entries in favor of sequential ones. <br><br>  This explains how read / write tasks demonstrate high performance ‚Äî but the question still remains how stale data is considered obsolete. <br><br><h1>  Delete: bind to size, not time </h1><br>  In our deduplication process, we need to decide, restrict the system according to the strict ‚Äúdeduplication window‚Äù or the total size of the database on disk. <br><br>  To avoid a system crash due to excessive deduplication for all users, we decided to choose a <b>size limit</b> , rather than <b>a time interval limit</b> .  This allows you to set a maximum size for each RocksDB instance and handle sudden ramps or load increases.  A side effect is that the time interval can be reduced to less than 24 hours, and our engineer‚Äôs duty officer is called at this boundary. <br><br>  We periodically recognize as obsolete old keys from RocksDB in order to prevent it from growing to unlimited size.  To do this, we store the <b>secondary</b> key <b>index</b> in order of number, so that we can delete the oldest keys first. <br><br>  Instead of using the RocksDB TTL, which would require saving a fixed TTL when opening the database, we delete the objects themselves by the sequence number of each nested key. <br><br>  Since ordinal numbers are stored as a secondary index, we can quickly query them and ‚Äúmark‚Äù them as deleted.  Here is our deletion function after the transfer of its sequence number. <br><br><pre> <code class="hljs dos">func (d *DB) delete(n int) error { // open a connection to RocksDB ro := rocksdb.NewDefaultReadOptions() defer ro.Destroy() // <span class="hljs-built_in"><span class="hljs-built_in">find</span></span> our offset to seek through <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> writing deletes hint, err := d.GetBytes(ro, []byte("seek_hint")) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != nil { return err } it := d.NewIteratorCF(ro, d.seq) defer it.Close() // seek to the first key, this is a small // optimization to ensure we don't use `.SeekToFirst()` // since it has to skip through a lot of tombstones. <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(hint) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> { it.Seek(hint) } <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { it.SeekToFirst() } seqs := make([][]byte, <span class="hljs-number"><span class="hljs-number">0</span></span>, n) keys := make([][]byte, <span class="hljs-number"><span class="hljs-number">0</span></span>, n) // look through our sequence numbers, counting up // <span class="hljs-built_in"><span class="hljs-built_in">append</span></span> any data keys that we <span class="hljs-built_in"><span class="hljs-built_in">find</span></span> to our <span class="hljs-built_in"><span class="hljs-built_in">set</span></span> to be // deleted <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> it.Valid() &amp;&amp; len(seqs) &lt; n { k, v := it.Key(), it.Value() key := make([]byte, len(k.Data())) val := make([]byte, len(v.Data())) <span class="hljs-built_in"><span class="hljs-built_in">copy</span></span>(key, k.Data()) <span class="hljs-built_in"><span class="hljs-built_in">copy</span></span>(val, v.Data()) seqs = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(seqs, key) keys = <span class="hljs-built_in"><span class="hljs-built_in">append</span></span>(keys, val) it.Next() k.Free() v.Free() } wb := rocksdb.NewWriteBatch() wo := rocksdb.NewDefaultWriteOptions() defer wb.Destroy() defer wo.Destroy() // preserve next sequence to be deleted. // this is an optimization so we can use `.Seek()` // instead of letting `.SeekToFirst()` skip through lots of tombstones. <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> len(seqs) &gt; <span class="hljs-number"><span class="hljs-number">0</span></span> { hint, err := strconv.ParseUint(string(seqs[len(seqs)-<span class="hljs-number"><span class="hljs-number">1</span></span>]), <span class="hljs-number"><span class="hljs-number">10</span></span>, <span class="hljs-number"><span class="hljs-number">64</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != nil { return err } buf := []byte(strconv.FormatUint(hint+<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">10</span></span>)) wb.Put([]byte("seek_hint"), buf) } // we <span class="hljs-keyword"><span class="hljs-keyword">not</span></span> only purge the keys, but the sequence numbers as well <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i := range seqs { wb.DeleteCF(d.seq, seqs[i]) wb.Delete(keys[i]) } // finally, we persist the deletions to our database err = d.Write(wo, wb) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> err != nil { return err } return it.Err() }</code> </pre> <br>  To further guarantee a high write speed, RocksDB does not return immediately and does not remove the key (you remember that the SSTable tables are immutable!).  Instead, RocksDB adds a ‚Äúgravestone‚Äù to the key, which is then removed in the process of compacting the base.  Therefore, we can quickly recognize obsolete entries during sequential write operations and avoid clogging memory when deleting old items. <br><br><h1>  Ensuring the correctness of the data </h1><br>  We have already discussed how speed, scaling and cheap search are provided in billions of messages.  The last fragment remains - how to ensure the correctness of the data in case of various failures. <br><br><h1>  EBS images and applications </h1><br>  To protect our RocksDB instances from damage due to a programmer error or failure of EBS, we periodically take snapshots of each of our hard drives.  Although EBS replicates on its own, this measure protects against damage caused by some internal mechanism.  If we need a specific instance, the client can be paused, at which time the corresponding EBS disk is detached and then reattached to the new instance.  As long as we keep the partition ID unchanged, reattaching the disk remains a completely painless procedure, which still ensures the data is correct. <br><br>  In the event of a worker crash, we rely on the <a href="https://en.wikipedia.org/wiki/Write-ahead_logging">forward write log</a> built into RocksDB to keep the message alive.  Messages are not allowed from the input section unless we have a guarantee that RocksDB has reliably saved the message in the log. <br><br><h1>  Reading the output section </h1><br>  You may have noticed that until that moment there was not that ‚Äúatomic‚Äù step, which allows you to guarantee that messages are delivered strictly once.  At any moment there is a possibility that our worker will fail: when writing to RocksDB, when publishing to the output section, or when confirming incoming messages. <br><br>  We need an atomic ‚Äúcommit‚Äù point that will uniquely cover transactions for all of these separate systems.  A certain ‚Äúsource of truth‚Äù is required for our data. <br><br>  This is where reading from the output section comes into play. <br><br>  If for some reason a worker fails or an error occurs in Kafka and then restarted, the first thing to do is to check with the ‚Äúsource of truth‚Äù about whether an event has occurred: this source is the <b>output section</b> . <br><br>  If the message is found in the output section, but <i>not</i> in RocksDB (and vice versa), then the deduplication worker will make the necessary edits to synchronize the database and RocksDB.  In fact, we use the output section at the same time as the forward-looking log and the ultimate source of truth, while RocksDB captures and verifies it. <br><br><h1>  In real work </h1><br>  Now our deduplication system has been in production for three months now and we are incredibly pleased with the results.  If in numbers, then we have: <br><br><ul><li> <b>1,5  </b>     RocksDB </li><li> <b>4-</b>        </li><li>  <b>60 </b>     RocksDB </li><li> <b>200 </b>      </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The system as a whole is fast, efficient and fault tolerant - and with a very simple architecture. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In particular, the second version of our system has several advantages over the old deduplication system. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Previously, we stored all keys in Memcached and used the atomic operator to check and set the value of the CAS record (check-and-set) to set non-existent keys. Memcached served as a point of fixation and "atomicity" when publishing keys. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Although this scheme worked quite well, it required a large amount of memory to fit all of our keys. Moreover, it was necessary to choose between accepting random Memcached failures or doubling our cost of creating memory-hungry fail-safe copies.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Kafka / RocksDB scheme provides almost all the advantages of the old system, but with increased reliability. Summing up, here are the main achievements: </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Data storage on the disk:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> saving the entire set of keys or full indexing in the memory were unacceptable roads. Transferring more data to disk and using different levels of files and indexes, we were able to significantly reduce the cost. Now we can switch to a ‚Äúcold‚Äù storage (EBS) in case of failure, and not support the work of additional ‚Äúhot‚Äù instances in case of a failure. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Selection of sections:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">of course, in order to narrow our search space and avoid loading too many indexes into memory, a guarantee is required that certain messages are sent to the correct workers. Partitioning in Kafka allows you to consistently route these messages along the correct routes, so that we can much more efficiently cache data and generate queries. </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accurate recognition of obsolete keys</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> : in Memcached, we would set a TTL for each key to determine its lifetime, and then rely on the Memcached process to exclude keys. In the case of large data packets, this would threaten a lack of memory and irregular CPU usage due to the large number of key exceptions. By instructing the client to delete the keys, we can gracefully avoid the problem by reducing the ‚Äúdeduplication window‚Äù.</font></font><br><br> <b>Kafka   </b> :  -            ,         . Kafka    ¬´ ¬ª    .     (   Kafka)     Kafka,   .   Kafka ,             ,        . <br><br> <b>   :</b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">by making packet I / O operations for calls to Kafka and RocksDB, we were able to greatly improve performance using sequential read and write. </font><font style="vertical-align: inherit;">Instead of random access, which was previously with Memcached, we have achieved much higher throughput by improving disk performance and storing only indexes in memory. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In general, we are quite satisfied with the guarantees that the deduplication system we have created. </font><font style="vertical-align: inherit;">Using Kafka and RocksDB as the basis for streaming applications </font></font><a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">is increasingly becoming the norm</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> . </font><font style="vertical-align: inherit;">And we are happy to continue the development of new distributed applications on this foundation.</font></font></div><p>Source: <a href="https://habr.com/ru/post/332448/">https://habr.com/ru/post/332448/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../332434/index.html">Systems Management Analysis</a></li>
<li><a href="../332436/index.html">Bitfury Group conducted the first successful multi-hop transaction on the Lightning Network</a></li>
<li><a href="../332438/index.html">‚ÄúUltimate‚Äù blockchain digest: useful materials on Habr√© and other sources on the topic</a></li>
<li><a href="../332442/index.html">ML Grid - Apache Ignite machine learning library</a></li>
<li><a href="../332444/index.html">Doctor Web: MEDoc contains a backdoor giving attackers access to a computer</a></li>
<li><a href="../332450/index.html">Moby / Docker in production. Failure history</a></li>
<li><a href="../332456/index.html">Running AMP applications on Cyclone V SoC</a></li>
<li><a href="../332458/index.html">Broadcast HPE Digitize: talk about our new products and solutions</a></li>
<li><a href="../332460/index.html">Why do I need your permanent collections? They are slow</a></li>
<li><a href="../332462/index.html">What if throwing all the excess out of the database into a distributed cache is our experience using Hazelcast</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>