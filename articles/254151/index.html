<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to get to the top on Kaggle, or Matriksnet at home</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I want to share the experience of participating in the Kaggle competition and machine learning algorithms, with the help of which I reached the 18th p...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to get to the top on Kaggle, or Matriksnet at home</h1><div class="post__text post__text-html js-mediator-article">  I want to share the experience of participating in the Kaggle competition and machine learning algorithms, with the help of which I reached <a href="https://www.kaggle.com/c/avazu-ctr-prediction/leaderboard/private">the 18th place out of 1604 in the Avazu</a> CTR (click-through rate) mobile advertising prediction <a href="https://www.kaggle.com/c/avazu-ctr-prediction/leaderboard/private">competition</a> .  In the process, I tried to recreate the original McTricksnet algorithm, tested several logistic regression options and worked with the characteristics.  All of this is below, plus I attach the full code so that you can see how everything works. <br><br>  The story is divided into the following sections: <br>  1. Terms of the competition; <br>  2. Creating new features; <br>  3. Logistic regression - the charm of an adaptive gradient; <br>  4. Matrixnet - recreating the full algorithm; <br>  5. Acceleration of machine learning in Python. <br><a name="habracut"></a><br><h4>  1. Competition conditions </h4><br>  Data provided: <br><ul><li>  40.4 million records for training (10 days of Avazu advertising); </li><li>  4.5 million records for testing (1 day). </li></ul><br>  The data itself can be downloaded <a href="https://www.kaggle.com/c/avazu-ctr-prediction/data">here</a> . <br><br>  As an evaluation criterion, a negative error probability was stated (Negative Likelihood Loss - NLL): 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/files/f19/591/daf/f19591dafc7541bd9af2b2eaaf9039fe.jpg"><br><br>  Where <b>N</b> is the number of records, <b>y</b> is the value of the click variable, <b>p</b> is the probability that the event was a click (‚Äú1‚Äù). <br><br>  An important property of this error function is that if <b>p is</b> based on a sigmoid function, then the private derivative (hereinafter, the gradient) will be <b>(py)</b> . <br><br><img src="https://habrastorage.org/files/029/dd1/7df/029dd17df9764fd68cd75a534f509226.jpg"><br><br><h4>  2. Creating new features </h4><br>  To start, look at the source data, with which we can work: <br><ul><li>  click - ‚Äú0‚Äù - there was no click, ‚Äú1‚Äù - was click, this is the goal for prediction; </li><li>  hour - time and date of the advertisement; </li><li>  banner_pos - the location of the banner (presumably, the first screen, the second, etc.); </li><li>  site *, app * characteristics - information about the place of the advertisement; </li><li>  device * characteristics - information about the device on which advertising is shown; </li><li>  C1, C14-C21 - encrypted characteristics (presumably including data on the geolocation of the display, time zone, and possibly other information). </li></ul><br><div class="spoiler">  <b class="spoiler_title">Sample data</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/342/809/207/3428092072eb4dcf9d8ef0619a456668.jpg"><br></div></div><br>  This is not a very big field for work, as we have no historical data on users, and most importantly, we know nothing about what advertising is shown every time.  And this is an important component (because you are not clicking on all the advertising in a row?). <br><br>  What we create new: <br><ol><li>  Polynomial characteristics of the 2nd level (the 3rd slows down the learning process too much); </li><li>  User_id.  I tested several options, it works best - device_id + device_ip + device_model + C14 (presumably geography at the city / region level).  And yes, device_id does not equal user_id; </li><li>  Frequency of contact with advertising.  Usually, users who see ads for the 100th time in a day react to it differently than those who see in the 1st.  Therefore, we consider the frequency of each display for each user_id. </li></ol><br><div class="spoiler">  <b class="spoiler_title">Example of forming user id</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/922/f04/33a/922f0433ac1a430591b44e0df53513b2.jpg"><br></div></div><br>  Ideas tried different, the above gave the best result.  When they were formed, it was mainly based on their experience in advertising and what could be pulled from Avazu data. <br><br>  We also make small transformations / transformations of data, primarily aimed at getting rid of repetitive information: <br><ul><li>  hour - select the hour, throw away the day; </li><li>  C1 - I assume that the time zone was behind this, so after 2 numbers I merge the hour with the column; </li><li>  C15 and C16 - we unite, since the width and height of the banner are easily guessed behind them, it makes no sense to leave extra characteristics; </li><li>  Site * and app * - we transform into placement *, since it is clear that the banner is shown either on the website or in the application, and the remaining values ‚Äã‚Äãare simply encrypted NULL, which is add.  does not carry information; </li><li>  We remove all values ‚Äã‚Äãthat are not met in the test data.  It helps to reduce retraining. </li></ul><br>  All changes were tested using logistic regression: they either gave improvements or accelerated the algorithm and did not worsen the results. <br><br><h4>  3. Logistic Regression - the charm of an adaptive gradient </h4><br>  Logistic regression is a popular classification algorithm.  There are 2 main reasons for this popularity: <br>  1. Easy to understand and create an algorithm; <br><br><img src="https://habrastorage.org/files/9a8/029/2bf/9a80292bfbea4bdb867b1c051ce0aa9d.jpg"><br><br>  2. The speed and adaptability of the prediction on large data due to stochastic gradient descent (stochastoc gradient descent, SGD). <br><br><img src="https://habrastorage.org/files/265/de6/d95/265de6d95a234c6fbfa19aa1340e3283.jpg"><br><br>  Using the Avazu data as an example, let's see how, due to the stochastic gradient, we do not always ‚Äúgo‚Äù to exactly the minimum: <br><br><img src="https://habrastorage.org/files/58a/f19/433/58af194333c34c3f8252d16b989a73e0.jpg"><br><br><h5>  3.1.  Adaptive gradient </h5><br>  However, if we reduce the learning rate of the algorithm (learning rate) with time, then we will come to a more accurate solution, since the gradient will not react so strongly to atypical data.  Adaptive Gradient (AdaGrad) authors suggest using the sum of all previous gradients to reduce successively the learning rates: <br><br><img src="https://habrastorage.org/files/701/5e7/a3c/7015e7a3c1cd4b548cfc50ccfbe0cc69.jpg"><br><br>  Thus, we obtain the useful properties of the algorithm: <br><ul><li>  Smoother descent to a minimum (learning speed decreases with time); </li><li>  The alpha for each characteristic is calculated individually (which is very important for sparse data, where most of the characteristics are very rare); </li><li>  The calculation of the alpha takes into account how strongly the parameter (w) of the characteristic has changed: the more strongly it changed before, the less it will change in the future. </li></ul><br>  The adaptive gradient begins to learn in the same way as the usual logistic regression, but then comes to a lower minimum: <br><br><img src="https://habrastorage.org/files/05a/76f/6de/05a76f6dea6a421498037c92fc9ad697.jpg"><br><br>  In fact, if the usual stochastic gradient descent is repeated many times on the same data, then it can approach AdaGrad.  However, this will take more iterations.  In my model, I used a combination of these techniques: 5 iterations with the usual algorithm, and then one with AdaGrad.  Here are the results: <br><br><img src="https://habrastorage.org/files/f4e/148/819/f4e148819a4e4b90a932e5290ef0479c.jpg"><br><br><h5>  3.2.  Transformation of data for logistic regression </h5><br>  In order for the logistic regression algorithm to work with the data (and they are presented in the format of text values), they need to be converted to scalar values.  I used one-hot encoding: conversion of textual characteristics to the NxM matrix with the values ‚Äã‚Äã‚Äú0‚Äù and ‚Äú1‚Äù, where N is the number of records, and M is the number of unique values ‚Äã‚Äãof this characteristic.  The main reasons are that maximum information is saved, and feature hashing allows you to quickly work with spaces with millions of characteristics as part of a logistic regression. <br><br><div class="spoiler">  <b class="spoiler_title">Example one-hot encoding</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/c7e/837/2aa/c7e8372aa8cb4a98bdb5e2299480684a.jpg"><br></div></div><br><br><h4>  4. Matrixnet - build at home </h4><br>  After I received quite good logistic regression results, it was necessary to further improve.  It was interesting for me to understand how MatrixNet of Yandex works, moreover, I expected good results from it (after all, it is one of its tasks to predict CTR inside Yandex for an advertising search result).  If you collect all the publicly available information about MatrixNet (see the full list of links at the end of the article), then you can recreate its algorithm.  I do not pretend that it is in this form that the algorithm works inside Yandex, I do not know, but in my code and in this article I used all the found "chips" and hints of them. <br><br>  Let's go in order, what consists of Maktksnet: <br><ol><li>  The base element is Classification and Regression Tree (CART); </li><li>  The main algorithm is Gradient Boosting Machine (GBM) </li><li>  The update of the main algorithm is the Stochastic Gradient Boosting Machine (SGBM). </li></ol><br><h5>  4.1.  Classification and Regression Tree (CART) </h5><br>  This is one of the <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">classic decision tree algorithms</a> , which has already been written on Habr√© (for example, <a href="http://habrahabr.ru/post/215453/">here</a> and <a href="http://habrahabr.ru/post/116385/">here</a> ).  Therefore, I will not go into details, just recall the principle of work and basic terms. <br><br><img src="https://habrastorage.org/files/050/58b/d13/05058bd13ba64e2c96dd5152d63d104c.jpg"><br><br>  Thus, the decision tree has the following parameters that define the algorithm: <br><ul><li>  Split conditions on sheets ( <b>x_1‚â•0.5</b> ) </li><li>  "Height" of a tree (number of levels with conditions, in the example above there are 2) </li><li>  The prediction rule <b>p</b> (in the example above, the expectation is used) </li></ul><br><h6>  4.1.1.  How to define a characteristic for a split condition </h6><br>  At each level, we need to define a characteristic for the split condition, which will divide the plane in such a way that we will make more accurate predictions. <br><br><img src="https://habrastorage.org/files/815/f11/3a9/815f113a99f34643a59e562a32e29acc.jpg"><br><br>  Thus, we go through all the characteristics and possible splits and choose those points where we will have a lower NLL value after the split (in the example above it is, of course, <b>x2</b> ).  To determine the split, other functions are usually used ‚Äî <a href="http://en.wikipedia.org/wiki/Decision_tree_learning">information gain and Gini impurity</a> , but in our case we choose NLL, since this is what we were asked to minimize in the task (see the description of the task in section 1). <br><br><h6>  4.1.2.  CART Regularization </h6><br>  In the case of CART, regularization is usually necessary in order not to make too confident predictions where we are not really sure.  Yandex offers to adjust the prediction formula as follows: <br><br><img src="https://habrastorage.org/files/2b7/f36/c5b/2b7f36c5bbde43adb4ef9a3a4a9ea095.jpg"><br><br>  Where <b>N</b> is the number of values ‚Äã‚Äãin the sheet, and lambda is the regularization parameter (the McTricksnet experts recommend 100, but you need to test for each task separately).  Thus, the smaller the values ‚Äã‚Äãin the sheet, the less our algorithm will be confident in the predicted value. <br><br><h6>  4.1.3.  Oblivious Trees </h6><br>  In Matrixnet, instead of the classical approach, when, after a split in <b>x1, the</b> next level of the tree cannot divide data on this characteristic, so-called are used.  forgetful trees that can split values ‚Äã‚Äãwithin the same level on the same characteristic (as if ‚Äúforgetting‚Äù that it has already been split). <br><br><img src="https://habrastorage.org/files/78a/683/34d/78a68334de9542978ac4b627a3c40dc6.jpg"><br><br>  The use of this type of trees, in my opinion, is justified primarily in those cases when there are 1-2 characteristics in the data, narrower splits for which give better results than those for not yet used characteristics. <br><br><h5>  4.2.  Gradient Boosting Machine </h5><br>  Gradient Boosting Machine (GBM) is the use of a forest of short trees, where each subsequent one does not try to guess the target value, but tries to improve the forecast of the previous trees.  Consider a simple example with regression and trees with a height of 1 (we can only do 1 split within each tree). <br><br><img src="https://habrastorage.org/files/ff3/593/145/ff35931454574cd88a4bb3073da5c980.jpg"><br><br>  In essence, each tree helps to optimize the quadratic error function: <br><br><img src="https://habrastorage.org/files/d8c/552/fd8/d8c552fd8c404732aa9ecf1a4ea2ca39.jpg"><br><br>  The main advantage of GBM in comparison with CART is a lower probability of retraining, since we give forecasts based on sheets with a larger number of values.  In MatrixNet in GBM, the ‚Äúheight‚Äù of a tree depends on the current iteration: it starts from 1, every 10 iterations increases by 1 more, but never exceeds 6. This approach allows you to significantly overclock the algorithm, without much worsening the result at the first iterations.  I tested this option, but stopped at the option when the transition to the next level is carried out after the possibilities have been exhausted at the previous one. <br><br><h6>  4.2.1.  GBM for classification </h6><br>  When working with the classification, each subsequent tree should improve the prediction of the previous one, but it should be done in such a way that the trees work for one task - the classification using the sigmoid function.  In this case, it is convenient to present the optimization problem the same as in the logistic regression, namely: <br><br><img src="https://habrastorage.org/files/c6c/c6c/171/c6cc6c171f1f40218b7622678b1ba72c.jpg"><br><br>  An interesting solution of Yandex is that the initial prediction of <b>p0 is</b> used to predict logistic regression, and the product of weights and characteristics (wTx) as one of the characteristics. <br><br><h5>  4.3.  Stochastic GBM </h5><br>  Stochastic GBM differs from ordinary GBM in that each tree is considered to be a sample of data (10-50%).  This helps to simultaneously kill 2 birds with one stone - to speed up the work of the algorithm (otherwise we would have to calculate the result for all 40.4 million training records), and also largely get rid of the problem of overtraining. <br>  Final result: <br><br><img src="https://habrastorage.org/files/c9a/8cc/aa4/c9a8ccaa4ada429a8261ad00555b0409.jpg"><br><br>  As you can see, all the same, the biggest optimization is the work with the data, and not the algorithms themselves.  Although with the usual logistic regression, it would be difficult to rise above the 30th place, where the score goes to every ten thousandth. <br><br><h4>  5. Attempts to accelerate machine learning in Python </h4><br>  This was my first project to implement machine learning algorithms on my own, that is, in the code that I used to make predictions, we didn‚Äôt use ready-made third-party solutions, all algorithms and data manipulations occur in the open, which allowed me to better understand the essence of the tasks and principles of these tools.  However, I used the best practices: logistic regression to a large extent - Abnishek <a href="https://kaggle2.blob.core.windows.net/forum-message-attachments/56731/1699/fast_solution.py%3Fsv%3D2012-02-12%26se%3D2015-03-26T22%253A00%253A28Z%26sr%3Db%26sp%3Dr%26sig%3DsEs40bqPiqGawCpXp92qTucsT9n9ru1E3JUqIWCSiXU%253D">code</a> on Kaggle, Matrixnet borrows a small part of CART from Stephen Marshall's <a href="https://seat.massey.ac.nz/personal/s.r.marsland/Code/Ch12/dtree.py">code</a> to the book Learning: Algorithmic Perspective. <br><br>  From the point of view of implementation, I started experimenting with the task in R, but then quickly abandoned it, since it is almost impossible to work with big data.  Python was chosen because of the simplicity of the syntax and the presence of a large number of libraries for machine learning. <br><br>  The main problem with CPython is that it is VERY slow (albeit much faster than R).  However, there are many options to speed it up, and as a result I used the following: <br><ul><li>  PyPy is a JIT compiler that allows you to speed up standard CPython x20 times, the main problem is that there are practically no computation libraries (NumPyPy is still in development), everything needs to be written without them.  Perfectly suited for the implementation of logistic regression with stochastic gradient descent, as in our case; </li><li>  Numba - <a href="http://habrahabr.ru/users/jit/" class="user_link">jit</a> decorators <a href="http://habrahabr.ru/users/jit/" class="user_link">allow</a> you to pre-compile some functions (as in PyPy), but the rest of the code can take full advantage of the CPython libraries.  A big plus - for precompiled functions, you can remove GIL (Global Interpreter Lock) and use several cpu.  Which is what I used to speed up MatrixNet.  The problem with Numba is the same as with PyPy - there is no support for any libraries, the main difference is that Numba has some NumPy functions. </li></ul><br>  I did not reach Cython, as I tried to accelerate with minimal blood, but I think it‚Äôs easier next time to switch to GPGPU using Theano / Numba. <br><br>  <a href="https://github.com/Ivanopolo/Avazu_Code">The complete code of all transformations with data and the learning algorithms themselves is here.</a>  Disclaimer: the code is not optimized, it is intended only for studying the algorithms themselves. <br><br>  If you find any inaccuracies or errors in the article or code, write in a personal. <br><br>  Links to sources used for the article and when working on algorithms: <br><ul><li>  Logistic regression and more - <a href="https://www.coursera.org/course/ml">www.coursera.org/course/ml</a> </li><li>  Machine learning algorithms - <a href="https://seat.massey.ac.nz/personal/s.r.marsland/MLBook.html">seat.massey.ac.nz/personal/srmarsland/MLBook.html</a> </li><li>  Adaptive Gradient - <a href="http://www.magicbroom.info/Papers/DuchiHaSi10.pdf">www.magicbroom.info/Papers/DuchiHaSi10.pdf</a> </li><li>  Gradient Boosting Machine - <a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf">statweb.stanford.edu/~jhf/ftp/trebst.pdf</a> </li><li>  Stochastic Gradient Boosting - <a href="http://statweb.stanford.edu/~jhf/ftp/stobst.pdf">statweb.stanford.edu/~jhf/ftp/stobst.pdf</a> </li><li>  Maktriksnet from Gulin - <a href="http://www.slideshare.net/yandex/matrixnet">www.slideshare.net/yandex/matrixnet</a> </li><li>  Maktriksnet from Trofimov - <a href="http://wan.poly.edu/KDD2012/forms/workshop/ADKDD12/doc/a3.pdf">wan.poly.edu/KDD2012/forms/workshop/ADKDD12/doc/a3.pdf</a> </li><li>  Maktriksnet with ANN at the heart of - <a href="http://arxiv.org/pdf/1412.6601.pdf">arxiv.org/pdf/1412.6601.pdf</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/254151/">https://habr.com/ru/post/254151/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../254137/index.html">How is the IT reception in Mail.Ru Group</a></li>
<li><a href="../254141/index.html">Private, financial and other non-public information of Fl.ru users is still in free access.</a></li>
<li><a href="../254143/index.html">IT rounded shapes as a factor</a></li>
<li><a href="../254145/index.html">Washington Post: Sit on Facebook all day? Be careful: your boss can keep an eye on you</a></li>
<li><a href="../254149/index.html">Android for developers. Perplexed and upset</a></li>
<li><a href="../254161/index.html">Extraordinary weekly build Vivaldi 1.0.138.4</a></li>
<li><a href="../254163/index.html">Why many do not like Arduino</a></li>
<li><a href="../254165/index.html">ASMX web services development techniques</a></li>
<li><a href="../254167/index.html">Python, xlsx with pictures, macros, media-art, embedded objects</a></li>
<li><a href="../254171/index.html">What is good: how we developed the criteria for assessing the quality of the layout of web projects</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>