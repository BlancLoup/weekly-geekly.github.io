<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognition of some modern captcha</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="That was the name of the work presented by me at the Baltic Science and Engineering Competition, which brought me a charming piece of paper with a Rom...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognition of some modern captcha</h1><div class="post__text post__text-html js-mediator-article"> That was the name of the work presented by me at the <a href="http://baltic.contedu.ru/">Baltic Science and Engineering Competition,</a> which brought me a charming piece of paper with a Roman edinichka, as well as a brand new laptop. <br><br>  The job was to recognize the CAPTCHA used by large mobile operators in the form of sending SMS, and to demonstrate the lack of effectiveness of their approach.  In order not to hurt anyone's pride, we will call these operators allegorically: red, yellow, green and blue. <br><br><a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The project received the official name <i>Captchure</i> and unofficial <i>Breaking Defective Security Measures</i> .  Any matches are random. <br><br>  Oddly enough, all (well, almost all) of these CAPTCHAs turned out to be rather weak.  The lowest result - 20% - belongs to the yellow operator, the greatest - 86% - to the blue.  Thus, I believe that the task of ‚Äúdemonstrating inefficiency‚Äù was successfully solved. <br><br>  The reasons for choosing cellular operators are trivial.  To the respected Scientific Jury, I told the bike that ‚Äúcellular operators have enough money to hire a programmer of any qualification, and, at the same time, they need to minimize the amount of spam;  thus, their CAPTCHA must be quite powerful, which, as my research shows, is completely different. ‚Äù  In fact, everything was much simpler.  I wanted to gain experience by <s>hacking by</s> recognizing some simple CAPTCHA, and I chose the red operator as the victim of the CAPTCHA.  And after that, the aforementioned story was born in hindsight. <br><br>  So, closer to the body.  I don‚Äôt have any mega-advanced algorithm for recognizing all four types of CAPTCHA;  instead, I wrote 4 different algorithms for each type of CAPTCHA separately.  However, despite the fact that the algorithms are different in details, in general, they turned out to be very similar. <br><br>  Like many authors before me, I split the CAPTCHA recognition task into 3 subtasks: preprocessing (preprocess), segmentation, and <a href="http://lurkmore.ru/%25D0%25A0%25D0%25B5%25D0%25BA%25D1%2583%25D1%2580%25D1%2581%25D0%25B8%25D1%258F">recognition.</a>  At the preprocess stage, various noises, distortions, etc. are removed from the original image. In the segmentation, separate characters are extracted from the original image and produced from post-processing (for example, reverse rotation).  In recognition, the characters are processed one by one by the previously trained neural network. <br><br>  Only the preprocess was significantly different.  This is due to the fact that different methods of image distortion are used in different CAPTCHA, respectively, and the algorithms for removing these distortions are very different.  Segmentation exploited the key idea of ‚Äã‚Äãfinding connectivity components with minor frills (they had to be significant only for yellow- <s>striped</s> ).  The recognition was exactly the same for three operators out of four - again, only the yellow operator was different. <br><br>  The code is written in <a href="http://www.python.org/">Python</a> using the <a href="http://opencv.willowgarage.com/wiki/">OpenCV</a> and <a href="http://leenissen.dk/fann/wp/">FANN libraries,</a> which are not <a href="http://leenissen.dk/fann/wp/">installed</a> without a decent-sized file and a tambourine into the bargain.  Therefore, my results will not be easy to reproduce - at least until the authors of the above libraries make normal Python bindings. <br><br><h4>  Red </h4><br>  As I said, I chose this particular CAPTCHA as the first rabbit.  I think a few examples will clarify the situation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d2e/a2f/03a/d2ea2f03ab012c77eade339fc2f965bf.jpg" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/aa1/15c/cb5/aa115ccb5675d703da12eaadf514306d.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/705/9ab/6c8/7059ab6c84b625a342fbec765009a19a.jpg" alt="image"><br><br>  That's it, and at first I also thought that it was very simple ... However, this impression did not come from scratch.  So: <br><ul><li>  The colors are exhausted by grayscale, and the symbols are light, the background is dark. </li><li>  No extra noise </li><li>  Constant distortion (i.e. not changing from picture to picture) </li><li>  There are always exactly five characters. </li><li>  Character sizes are about the same. </li><li>  Characters are almost always connected. </li></ul><br>  It all seemed to negate the merits of this CAPTCHA: <br><ul><li>  Sticking letters </li><li>  Nasty holey font </li><li>  Very small size (83x23 px) </li></ul><br>  Naturally, these "advantages" are such only from the point of view of the complexity of automatic recognition.  In accordance with the three-stage scheme, mentioned by me earlier, we start with the preliminary image processing, namely, a 2-fold increase. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bf8/51b/795/bf851b7955e5e23c3f779fb2218ff0fc.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/969/f84/d1e/969f84d1ea1b0b7e1b03b1e4df54b2ad.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/8e7/e99/8e6/8e7e998e6e1e3f4606fc58b16b5d713e.png" alt="image"><br><br>  As I already mentioned, the distortions are constant here, and, even though they are not linear, it is not difficult to get rid of them.  The parameters were chosen empirically. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/52a/cde/261/52acde26162ba8cd080c45c899f86aec.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/fb3/275/b10/fb3275b1039d1a013f46ce7c30053c43.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/be8/864/094/be8864094e7b7893a68eb12a7b24418b.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/9ef/5b0/563/9ef5b05630a7abcea4a7b29e402934c4.png" alt="image"><br><br>  Next, I apply a threshold conversion (popularly Threshold) with t = 200 and invert the image: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/86a/04c/1c3/86a04c1c34304181c6fcfb798b2a5b89.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/da9/79a/e7d/da979ae7dcd1fd9ac3ff7e915747f5ce.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/367/03f/222/36703f2225ac58fbe303eb222d9e3f66.png" alt="image"><br><br>  Finally, small (less than 10px) black connected regions are painted over with white: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/809/52a/c2e/80952ac2ea29f438fc9e0b293ee9b475.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/b9c/07f/eb6/b9c07feb6dacdf97d75eb061aa9fc3b1.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/bee/82a/4d0/bee82a4d087c4112727254ab1fb5bb76.png" alt="image"><br><br>  This is followed by segmentation.  As I said, the search for connected components is applied here: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e0/ae1/77d/0e0ae177dbf44b8628875c209ecba2bb.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/140/010/719/14001071925b9a8b147222b58c1f6495.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/e7f/509/6c5/e7f5096c57b767e5d95ec5d15ce01df5.png" alt="image"><br><br>  Sometimes (rarely, but sometimes) a letter breaks into several parts;  To correct this annoying misunderstanding, I use a fairly simple heuristic that evaluates the belonging of several connected components to a single symbol.  This estimate depends only on the horizontal position and size of the descriptive rectangles (bounding boxes) of each character. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0e0/ae1/77d/0e0ae177dbf44b8628875c209ecba2bb.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/ab8/58a/4da/ab858a4da385945f8e97367e96257907.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/e7f/509/6c5/e7f5096c57b767e5d95ec5d15ce01df5.png" alt="image"><br><br>  It is easy to see that many characters were combined into one component of connectivity, and therefore it is necessary to separate them.  Here comes the fact that the image is always exactly 5 characters.  This allows you to accurately calculate how many characters are in each component found. <br><br>  To explain the principle of operation of such an algorithm, you will have to delve a little into the materiel.  Denote the number of segments found by n, and the array of widths ( <a href="http://www.youtube.com/watch%3Fv%3D2OuVmTP2aoE">correctly said, yes?</a> ) Of all segments by widths [n].  We assume that if after the above steps n&gt; 5, the image could not be recognized.  Consider all possible expansions of the number 5 into positive integer terms.  There are only a few of them - only 16. Each such decomposition corresponds to some possible arrangement of symbols for the connected components found.  It is logical to assume that the wider the resulting segment, the more characters it contains.  Of all the expansions of the five, we choose only those in which the number of terms is equal to n.  We divide each element from widths by widths [0] - as if we normalize them.  We do the same with all the remaining expansions - we divide each number in them into the first term.  And now (attention, climax!), We note that the resulting ordered n-ki can be thought of as points in n-dimensional space.  With this in mind, we find the closest to Euclid decomposition of the five to normalized widths.  This is the desired result. <br><br>  By the way, in connection with this algorithm, another interesting way came to my mind to search for all decompositions of a number into terms, which I, however, did not implement, having dug in Python data structures.  In short - it comes out pretty obviously, if you notice that the number of expansions of a certain length coincides with the corresponding level of Pascal's triangle.  However, I am sure that this algorithm has long been known. <br><br>  So, after determining the number of characters in each component, the next heuristic occurs - we consider that the delimiters between characters are thinner than the characters themselves.  In order to take advantage of this intimate knowledge, we place on the segment n-1 separators, where n is the number of characters in the segment, then in a small neighborhood of each separator we calculate the image downwards.  As a result of this projection, we will get information about how many pixels in each column belong to the characters.  Finally, in each projection we find the minimum and move the separator there, after which we cut the image along these separators. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1a2/053/173/1a205317387b50eff32c56e9d5460bb7.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/2ac/401/a23/2ac401a23e5ee2e19dda314b8954e22b.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/645/67f/dec/64567fdec7a500e9c87cf61a3cefa8d7.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c8d/e69/b49/c8de69b49e7f3ea0ffde1c008baa9eea.png" alt="image"><br><br>  Finally, recognition.  As I said, for him I use the neural network.  For her training, I first run two hundred images under the general <i>trainset</i> heading through the first two stages already written and debugged, with the result that I get a folder with a large number of neatly cut segments.  Then I clean the garbage with my hands (the results of incorrect segmentation, for example), after which I bring the result to the same size and give it to FANN to be torn apart.  At the output, I get a trained neural network, which is used for recognition.  This scheme failed only once - but more on that later. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f75/29f/861/f7529f861f4adfb65f944279d7204f2b.png" alt="image"><br><br>  As a result, the test set (not used for training, the code name - <i>testset</i> ) of 100 images was correctly recognized 45. Not a very good result - it can, of course, be improved, for example, by specifying the preprocess or modifying the recognition, but, frankly, I It was too lazy to bother with it. <br><br>  In addition, I used another criterion for evaluating the performance of the algorithm ‚Äî the average error.  It was calculated as follows.  For each image there was a Levenshtein distance between the opinion of the algorithm on this image and the correct answer - after which the arithmetic average was taken over all the images.  For this type of CAPTCHA, the average error was 0.75 characters / image.  It seems to me that this is a more accurate criterion than just the percentage of recognition. <br><br>  By the way, almost everywhere (except for the yellow operator) I used exactly such a scheme - 200 pictures in a trainset, 100 - in a testset. <br><br><h4>  Green </h4><br>  I chose the next goal of greens - I wanted to take on something more serious than the selection of the distortion matrix. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d1a/924/767/d1a924767cc2afaf6c4240727e859857.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/3d3/37f/440/3d337f4407e08b00a7453f00535eedf3.png" alt="image"><br><br>  Advantages: <br><ul><li>  Three-dimensional effect </li><li>  Rotate and shift </li><li>  Uneven brightness </li></ul><br>  Disadvantages: <br><ul><li>  Characters are noticeably darker than background. </li><li>  The upper side of the rectangle can be clearly seen - can be used for reverse rotation </li></ul><br>  It turned out that even in spite of the fact that these flaws are seemingly insignificant, their exploitation makes it possible to effectively deal with all the advantages. <br><br>  Again, let's start with preprocessing.  First, we estimate the angle of rotation of the rectangle on which the characters lie.  To do this, apply the Erode operator (local minimum search) to the source image, then the Threshold to select the residuals of the rectangle and, finally, the inversion.  Get a nice white spot on a black background. <br><br>  Next comes a deep thought.  The first.  To estimate the angle of rotation of the entire rectangle, it is sufficient to estimate the angle of rotation of its upper side.  The second.  It is possible to estimate the angle of rotation of the upper side by searching for a line parallel to this side.  Third.  To describe any straight line, except strictly vertical, two parameters are enough - vertical displacement from the center of coordinates and angle of inclination, and we are only interested in the second one.  Fourth.  The task of searching for a straight line can be solved not by a very large search - there are no too big turning angles there, and we don‚Äôt need ultra-high accuracy.  The fifth.  To search for the necessary line, you can compare each direct estimate of how close it is to what you are looking for, and then choose the maximum.  The sixth.  The most important.  To estimate a certain angle of inclination of a straight line, let us imagine that the image from above is tangent to a straight line with such an inclination angle.  It is clear that from the dimensions of the image and the angle of inclination, it is possible to uniquely calculate the vertical offset of the line, so that it is uniquely defined.  Further, we will gradually move this line down.  At some point, it will touch the white spot.  Let us remember this moment and the area of ‚Äã‚Äãintersection of the line with the spot.  Let me remind you that the straight line has an 8-connected view on the plane, so angry cries from the audience that the straight line has one dimension, and the area is a two-dimensional concept, is irrelevant here.  Then for some time we will move this line downward, at each step remembering the intersection area, after which we will summarize the results obtained.  This amount will be an estimate of this angle of rotation. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/055/f0f/306/055f0f3060e80f4d6ad827c85136210c.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/3d3/2f2/dec/3d32f2dec3211c0613a27703c6ce7053.png" alt="image"><br><br>  Summarizing the above, we will look for such a straight line that when moving it down the image, the brightness of the pixels lying on this straight line increases most dramatically. <br><br>  So, the angle of rotation is found.  But we should not rush to immediately apply the knowledge obtained.  The fact is that this will destroy the image connectivity, but we still need it. <br><br>  The next step is to separate the characters from the background.  Here we are greatly helped by the fact that the characters are much darker than the background.  It is a logical step by the developers - otherwise it would be very difficult to read the picture.  Who does not believe - can try to independently binarize the image and see for yourself. <br><br>  However, the approach "in the forehead" - an attempt to cut off characters with a threshold transformation - does not work here.  The best result that I managed to achieve - at t = 140 - looks very bad.  Too much trash remains.  Therefore it was necessary to apply a workaround.  The idea here is as follows.  Symbols are usually connected.  And they often own the darkest points on the image.  And what if you try to apply a fill from these darkest points, and then throw out too small areas filled in - obvious rubbish? <br><br>  The result is, frankly, amazing.  Most of the images manage to get rid of the background completely.  However, it happens that a symbol breaks into several parts - in this case, one crutch can help in segmentation - but more on that later. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/aaf/807/bb7/aaf807bb70409a949845eff32e672294.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/d8c/b72/6ec/d8cb726ec8433ecda2f7bda25de880f8.png" alt="image"><br><br>  Next, we make a turn to the angle found earlier. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/edb/4a0/848/edb4a0848db37fc7d7846ec9f29168a7.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/507/a04/4b0/507a044b046fce55857f952aa16007be.png" alt="image"><br><br>  Finally, the combination of the Dilate and Erode operators relieves us of the small holes left in the characters, which helps to simplify recognition. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fcf/aa8/197/fcfaa8197e95de5d07355b95d4cfe0b1.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/bbc/e57/596/bbce575968974a96b9e1e93f45453bcf.png" alt="image"><br><br>  Segmentation here is much simpler than preprocess.  First of all, we look for connected components. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/32e/f8f/5de/32ef8f5de456ec9f1ed1b3f7c87afad8.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/ad7/5d4/4b1/ad75d44b11695dc0c40475039b925a02.png" alt="image"><br><br>  Then we combine the components that are horizontally close (the procedure is exactly the same as before): <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c53/947/a9a/c53947a9a4f14cc0cb3809f246143deb.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/ad7/5d4/4b1/ad75d44b11695dc0c40475039b925a02.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/933/f61/cd6/933f61cd6630c8826276b22524b9c348.png" alt="image"><br><br>  Actually, everything.  This is followed by recognition, but it is no different from the above. <br><br>  This algorithm allowed achieving results in 69% of successfully recognized images and obtaining an average error of 0.3 characters / image. <br><br><h4>  Blue </h4><br>  So, the third status "defeated" received a blue operator.  It was, so to speak, a respite before a really big fish ... <br><br><img src="https://habrastorage.org/getpro/habr/post_images/bce/189/cd8/bce189cd8bb018c4519013bd69d12db8.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/38d/cc8/0ad/38dcc80ad60bfb0d1f935585c73c1e44.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/6fe/950/5ef/6fe9505ef1777559d265219d25146eb6.png" alt="image"><br><br>  It is difficult to write something in dignity, but I still try: <br><ul><li>  Rotate characters - the only more or less serious obstacle </li><li>  Background noise in the form of characters </li><li>  Characters sometimes touch each other </li></ul><br>  In contrast to this: <br><ul><li>  Background significantly lighter than characters </li><li>  The characters fit well into the rectangle. </li><li>  Different color symbols make it easy to separate them from each other. </li></ul><br>  So, preprocess.  Let's start with the background clipping.  Since the image is tricolor, we will cut it into channels, and then we will throw away all the points that are brighter than 116 across all channels.  We will get just such a pretty mask: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/08e/f64/d37/08ef64d37c6305bdba20432324e681b5.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/c59/bf1/01b/c59bf101be685b270d509e192d0ae2d3.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/213/656/8f7/2136568f76ccd0dd2517fe2976073bfe.png" alt="image"><br><br>  Then convert the image to the HSV color space ( <a href="http://ru.wikipedia.org/wiki/HSV_(%25D1%2586%25D0%25B2%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BE%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C)">Wikipedia</a> ).  This will save information about the color of characters, and at the same time remove the gradient from their edges. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/440/c9d/dc4/440c9ddc43a8fb5c6bfaf3b54780a0cc.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/701/49d/8f2/70149d8f27ef6c113bb78fb3b7d044f9.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/d3e/ecb/a1c/d3eecba1cca0c71145d55465670904ae.png" alt="image"><br><br>  Apply the mask obtained earlier to the result: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/ad0/d42/e41/ad0d42e4139157a392567ad5aee6378a.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/749/26a/fe6/74926afe63bf54e26dae4064b939f417.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/4ee/4de/719/4ee4de719f8d50068448f1e537654918.png" alt="image"><br><br>  At this preprocess ends.  Segmentation is also quite trivial.  We begin, as always, with connected components: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/1fd/a66/6b2/1fda666b287bd2fbaa6f65b29406b517.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/4c3/8d9/7ad/4c38d97add98f3247178c9fb9100f786.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/098/fc2/e45/098fc2e45e4238967076ed0e31f00a21.png" alt="image"><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9f4/047/a6a/9f4047a6a006860ba0fea6e89fb47797.png" alt="image"><br><br>  It would be possible to stop there, but only 73% come out this way, which doesn‚Äôt suit me at all - only 4% better than the result of the obviously more complex CAPTCHA.  So, the next step is to reverse the rotation of characters.  Here we can use the already mentioned fact that local characters fit well into a rectangle.  The idea is to find a descriptive rectangle for each character, and then, by its slope, calculate the slope of the actual character.  Here, the describing rectangle is understood as such that, firstly, it contains all the pixels of a given symbol, and, secondly, it has the smallest area of ‚Äã‚Äãall possible.  I use a ready-made implementation of the search algorithm for such a rectangle from OpenCV ( <a href="http://opencv.willowgarage.com/documentation/python/imgproc_structural_analysis_and_shape_descriptors.html%3Fhighlight%3Dminarearect2">MinAreaRect2</a> ). <br><br><img src="https://habrastorage.org/getpro/habr/post_images/149/226/a9f/149226a9f495ddaa1d0220f33ffdc97a.png" alt="image"><br><br>  Further, as always, follows recognition. <br><br>  This algorithm successfully recognizes 86% of images with an average error of 0.16 characters / image, which confirms the assumption that this CAPTCHA is indeed the simplest.  However, the operator is not the largest ... <br><br><h4>  Yellow </h4><br>  The most interesting comes.  So to say, the apotheosis of my creative activity :) This CAPTCHA is really the most difficult for both the computer and, unfortunately, for a person. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d77/2b0/5d1/d772b05d1ba5cf4f14b48883e608999f.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/4ad/c63/dcc/4adc63dcc2f0ae4b1571d0fd8e51acda.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/108/22c/ae9/10822cae9304ff2aba65ef31a2cea86d.png" alt="image"><br><br>  Advantages: <br><ul><li>  Noise in the form of spots and lines </li><li>  Rotate and scale characters </li><li>  Close proximity of characters </li></ul><br>  Disadvantages: <br><ul><li>  Very limited palette </li><li>  All lines are very thin. </li><li>  Spots often do not intersect with characters </li><li>  The rotation angle of all characters is approximately the same. </li></ul><br>  Over the first step, I thought for a long time.  The first thing that came to mind was to play around with local maxima (Dilate) to remove small noise.  However, this approach led to the fact that there was not much left of the letters - only ragged outlines.  The problem was aggravated by the fact that the texture of the characters themselves is not uniform - this is clearly visible at high magnification.  To get rid of it, I decided to choose the most stupid way - I opened Paint and wrote down the codes of all the colors found in the images.  It turned out that all in these images there are four different textures, and the three of them have 4 different colors, and the last - 3;  moreover, all the components of these colors turned out to be multiples of 51. Next, I compiled a table of colors with which I managed to get rid of the texture.  However, before this ‚ÄúREMAP‚Äù, I‚Äôm also overwriting all too light pixels, which are usually located on the edges of characters - otherwise you have to mark them as noise, and then deal with them, while they contain little information. <br><br>  So, after this transformation, the image contains no more than 6 colors - 4 colors of symbols (we will conventionally call them gray, blue, light green and dark green), white (background color) and ‚Äúunknown‚Äù, denoting that the pixel color is its place could not be identified with any of the known colors.  Call conditionally - because at this point I‚Äôll get rid of the three channels and turn to the usual and convenient monochrome image. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/814/139/777/814139777fbde1a4e8284fa08f323fe0.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/98d/83a/821/98d83a82198ef45d4433e8f030e9cac5.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/5c4/faf/e0f/5c4fafe0ffa1f3128bf63d0ea4fb3b22.png" alt="image"><br><br>  The next step was to clear the image from the lines.  Here, the situation is saved by the fact that these lines are very thin - only 1 pixel.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A simple filter suggests itself: go over the entire image, comparing the color of each pixel with the colors of its neighbors (in pairs - vertically and horizontally); if the neighbors match in color, and do not match the color of the pixel itself, make it the same as the neighbors. I use a slightly more sophisticated version of the same filter that works in two stages. At first, he evaluates his neighbors at a distance of 2, at the second, at a distance of 1. This allows one to achieve this effect: </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/8f0/b66/a75/8f0b66a75e3012d81f7abea24a42fd31.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/a37/d99/c19/a37d99c1920367c0170cde2d0bf5447b.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/670/50d/e14/67050de14f3cde2d60dfa028019e619f.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then I get rid of most of the spots, as well as the ‚Äúunknown‚Äù color. To do this, I first look for all the small connected areas (smaller than 15 in area, to be exact), put them on a black and white mask, and then combine the result with the areas occupied by the ‚Äúunknown‚Äù color. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/182/81f/55b/18281f55bca302518c8366715098793c.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/99f/1b6/ca4/99f1b6ca4ec4716fdf50f75712266588.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/c96/8de/3b7/c968de3b7a29cb68651185bcf6ed58f0.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With the help of these masks, I set the image on the image</font></font><a href="http://en.wikipedia.org/wiki/Inpainting"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inpaint</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (or rather, its </font></font><a href="http://opencv.willowgarage.com/documentation/python/imgproc_miscellaneous_image_transformations.html%3Fhighlight%3Dinpaint"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implementation</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in OpenCV). This allows you to very effectively clean up most of the garbage from the image. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/e4a/06a/a09/e4a06aa0941cacc59fc1854405e2d4ec.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/988/cc4/c07/988cc4c07026ceabbf66b29ec9a8d132.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/82d/a70/fb6/82da70fb63bbce5af650facaf894be19.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, the implementation of this algorithm in OpenCV was created to work with photos and videos, rather than recognizing artificially created images with noisy text. After its application, gradients appear, which I would like to avoid in order to simplify the segmentation. Thus, it is necessary to make additional processing, namely - sharpening. For the color of each pixel, I calculate the one closest to it from the above table (remember, there are 5 colors - one for each of the textures of characters and white).</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d73/52b/5fe/d7352b5fe1266d47aac069d277156917.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/f83/b3c/98d/f83b3c98dea9fb59ca727b3305bd090f.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/b00/d92/912/b00d92912b2dffacf4a4f8c27b215ac1.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finally, the final step in the preprocess will be the removal of all remaining small connected areas. They appear after applying Inpaint, so there is no repetition here. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d1/0a1/b4c/3d10a1b4ca5651cd4ac0195219e8f9ab.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/26e/ae3/d83/26eae3d838c40e1786a2cc42f5d66a44.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/11a/669/ead/11a669ead63fecfe1378e680c36ddfaa.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We proceed to segmentation. It greatly complicates the fact that the characters are very close to each other. It may happen that one symbol does not show half of the other. It becomes quite bad when these symbols are of the same color. In addition, the remains of garbage also play a role - it may happen that in the original image the lines intersect in a large number in one place. In this case, the algorithm that I described earlier will not be able to get rid of them.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">After a week spent in fruitless attempts to write a segmentation in the same way as in the previous cases, I scored on this case and changed tactics. My new strategy was to divide the entire segmentation process into two parts. The first one estimates the angle of rotation of the characters and performs the reverse rotation. In the second of the already expanded images, the characters are re-highlighted.</font></font> So let's get started.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We begin, as always, with a search for connected components. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c57/9ec/c19/c579ecc1992a30cb0cee351c5a5e3995.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/32f/805/1e3/32f8051e3fa65362e310a8e00d142808.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/4a4/371/877/4a437187718852730c898af213b871ed.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Then you need to estimate the angle of rotation of each character. When working with the Greenpeace fan-operator, I came up with an algorithm for this, but wrote and applied it only here. In order to illustrate his work, I will draw an analogy. Imagine a piston that moves on a black and white image of a symbol from bottom to top. The piston handle, for which it is pushed, is located vertically, the working platform, which it pushes - horizontally, parallel to the bottom of the image and perpendicular to the handle. The handle is attached to the platform in the middle, and at the point of attachment there is a movable joint, as a result of which the platform can be rotated. Forgive me terminology specialists.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Let the handle move upwards, pushing the platform in front of you according to the laws of physics. We assume that only the white image of the symbol is material, and the piston easily passes through the black background. Then the piston, having reached the white color, will begin to interact with it, namely, to turn - provided that the force is still applied to the handle. He can stop in two cases: if he has rested against a symbol on both sides of the point of application of force, or if he has rested against a symbol as the very point of application of force. In all other cases, he will be able to continue driving. Attention, culmination: we will assume that the angle of rotation of the symbol is the angle of inclination of the piston at the moment when it stopped.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9b8/27f/153/9b827f1537f4508c34426c0fa332750f.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">This algorithm is fairly accurate, but I do not take into account the obviously great results (more than 27 degrees). </font><font style="vertical-align: inherit;">Of the remaining ones, I find the arithmetic average, after which the whole image is turned to a minus this angle. </font><font style="vertical-align: inherit;">Then I search for connected components again.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/8f5/65e/8a6/8f565e8a650d57a27d5aec4204148892.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/425/acf/dc3/425acfdc35800a2034501bed9064ed38.png" alt="image"><img src="https://habrastorage.org/getpro/habr/post_images/ac8/6ac/96d/ac86ac96d266b6f417420c8200805091.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Further it becomes more and more interesting. </font><font style="vertical-align: inherit;">In the previous examples, I started various frauds with the received segments, and then transferred them to the neural network. </font><font style="vertical-align: inherit;">Everything is different here. </font><font style="vertical-align: inherit;">First, in order to at least partially restore the information lost after dividing the image into connectivity components, I draw a ‚Äúbackground‚Äù on each of them in dark gray color (96) - what was next to the cut segment, but did not fall into it , after which I smooth the outlines of the characters using the same procedure as in the preprocess for the lines (with the distance to the neighbor equal to one).</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6c/87b/0b6/c6c87b0b602ca724e829f05981a804a9.png" alt="image"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Formally (in terms of program modules), segmentation ends here. The attentive reader must have noticed that the separation of stuck characters was not mentioned anywhere. Yes, this is so - I pass them on to recognition in exactly this form, and I finish it in place. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The reason is that the method of splitting together stuck characters, which was described earlier (with the smallest projection) does not work here - the authors have chosen the font very well. Therefore it is necessary to apply a different, more complex approach. At the core of this approach is the idea that a </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neural network can be used for segmentation.</font></font></i> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At the very beginning, I described an algorithm that allows us to find the number of characters in a segment with a known width of this segment and the total number of characters. The same algorithm is used here. For each segment, the number of characters in it is calculated. If he is alone there, you do not need to finish anything, and this segment immediately goes </font></font><s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">&gt;&gt; =</font></font></s><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> to the neural network. If the symbol is not one there, then potential separators are placed along the segment at equal distances. Then each separator moves in its own small neighborhood, and the neural network response to the characters around this separator is calculated at the same time, after which it remains only to choose the maximum (in fact, it all does a rather stupid algorithm, but, in principle, everything is really approximately).</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Naturally, participation of a neural network in the process of segmentation (or pre-segmentation, if you like) excludes the possibility of using the neural network training scheme that I have already described. More specifically, it does not allow to get the very first neural network - it can be used to train others. Therefore, I act quite simply - I use conventional segmentation methods (projection) to train a neural network, while using it, the above described algorithm comes into play.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is one more subtlety associated with the use of a neural network in this algorithm. In the previous examples, the neural network was trained on the almost raw results of preprocess and segmentation. Here it allowed to get no more than 12% of successful recognition. It categorically did not suit me. Therefore, before starting the next epoch of neural network training, I introduced various distortions into the source images that roughly simulate real ones: add white / gray / black dots, gray lines / circles / rectangles, rotate. I also increased the trainset from 200 images to 300 and added a so-called </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">validset</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to test the quality of training during training on 100 images. This made it possible to achieve an increase in productivity somewhere by five percent, and together with the segmentation of the neural network, it gave the result I mentioned at the beginning of the article. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Providing statistics is complicated by the fact that I ended up with </font></font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">two</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> neural networks: one gave a higher recognition percentage, and the other a smaller error. Here I give the results of the first. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In total, as I have said repeatedly, there were 100 images in the testset. Of these, 20 were successfully recognized, unsuccessfully, respectively, 80, and the error was 1.91 characters per image. Noticeably worse than all other operators, but the corresponding CAPTCHA.</font></font><br><br><h4>  Instead of conclusion </h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Everything that relates to this work, I posted </font></font><a href="http://lcme.ucoz.ru/forum/38"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">in a special forum thread on my site,</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> in particular: the </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">source code</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">neural network files</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font></font><a href=""><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">images.</font></font></a> <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Next year, I would like to participate in something - at least in the same </font></font><a href="http://baltic.contedu.ru/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Baltic competition</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (and after it, preferably in </font></font><a href="http://www.societyforscience.org/isef/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Intel ISEF</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), but the creative crisis makes itself felt - it‚Äôs impossible to think up a sane topic for the project, but continue There is no desire to mess with captcha. Perhaps the habrasoobschestvo can help me ...</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The ideas that I had, but none of which I do not like - these functional operating systems and distributed (and / or anonymous) networks. </font><font style="vertical-align: inherit;">Unfortunately, the first will probably be too difficult for me (and who needs them, these </font></font><a href="http://citeseerx.ist.psu.edu/viewdoc/summary%3Fdoi%3D10.1.1.75.2051"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">functional </font></font></a> <a href="http://web.cecs.pdx.edu/~kennyg/house/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">axes?</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), And the second has already been done and done well ( </font></font><a href="http://ru.wikipedia.org/wiki/I2P"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I2P</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , </font></font><a href="http://ru.wikipedia.org/wiki/Netsukuku"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Netsukuku</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">At the same time, I want something that, firstly, it is possible to do in a year (at least two), and secondly, I would seriously claim a high place on the same ISEF. </font><font style="vertical-align: inherit;">Maybe you can tell in which direction I should move? </font></font><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UPD 2015-04-09: the </font></font></b> <a href="https://github.com/Pastafarianist/captchure"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">repository on Github</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> .</font></font></div><p>Source: <a href="https://habr.com/ru/post/116222/">https://habr.com/ru/post/116222/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../116214/index.html">SSH tunnel home without having to leave the home PC on</a></li>
<li><a href="../116216/index.html">March update for Windows Phone 7</a></li>
<li><a href="../116217/index.html">How to choose VPS hosting</a></li>
<li><a href="../116218/index.html">Yahoo! presented an analogue of Google instant</a></li>
<li><a href="../116220/index.html">And one more wireless charger</a></li>
<li><a href="../116224/index.html">SMF - Service Management on Solaris</a></li>
<li><a href="../116225/index.html">Good old electronic portable games</a></li>
<li><a href="../116227/index.html">Work with FLA</a></li>
<li><a href="../116228/index.html">OpenOffice Automation: Ending</a></li>
<li><a href="../116230/index.html">YouTube marks Earth Hour</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>