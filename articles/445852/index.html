<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to make a DAG trigger in Airflow using Experimental API</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="When preparing our educational programs, we periodically face difficulties in terms of working with some tools. And at that moment, when we come acros...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to make a DAG trigger in Airflow using Experimental API</h1><div class="post__text post__text-html js-mediator-article"><p>  When preparing our educational programs, we periodically face difficulties in terms of working with some tools.  And at that moment, when we come across them, there is not always enough documentation and articles that would help to cope with this problem. </p><br><p>  This was the case, for example, in 2015, and we used the Hadoop cluster with a 35-user Spark for the ‚ÄúBig Data Specialist‚Äù program.  How to prepare it for such a use case with the use of YARN was not clear.  In the end, having figured it out and having traveled on their own, they made a <a href="https://habr.com/ru/company/newprolab/blog/327556/">post at Habr√©</a> and also performed at the <a href="https://www.facebook.com/afishamansarda/videos/1648050321885680/%3F__xts__%255B0%255D%3D68.ARAPjMpjpACKX-w-LDrf8tNkpuVvbQFcqTLNmoMPDDYo5FzJ-t9DC4-LN3ITe6RM2RaJNnuZ_Nbl0KQeYGYQa_qrsQwS2D9drcG-IVgEe3OlmWZEpxMEDIYazpIWUSG8iOdkFrVcLwVwwpZWil2mr-dSvwjp5EfUnmAac2uIt0doZG440GkYIumPDy7Ym0Ol_F8oMoGw1K39kZEidj0YlKQ4RlYrlsw_BnmI30W-pnAH49ViOTM5MQEq0aSbyVGE2WqhVY-oAGTMA9CEh_2d4abpy2EHFg2eGDmlMwmlqQhEAG0M6aADJfqLN33f-uWfm2H_PgiLwzb1fRXEYxu6pcQqfb4QiLM6m3o%26__tn__%3D-R">Moscow Spark Meetup</a> . </p><br><h3 id="predystoriya">  Prehistory </h3><br><p>  This time we will talk about another program - <a href="http://gaurl.ru/rzveId">Data Engineer</a> .  Our participants build two types of architecture on it: lambda and kappa.  And in the lamdba-architecture, as part of the batch processing, Airflow is used to transfer logs from HDFS to ClickHouse. </p><br><p>  All in all good.  Let them build their pipelines.  However, there is a "but": all our programs are technological from the point of view of the learning process itself.  To check the lab, we use automatic checkers: the participant needs to go to his personal account, click the ‚ÄúCheck‚Äù button, and after a while he sees some extended feedback on what he has done.  And at this very moment we begin to approach our problem. </p><a name="habracut"></a><br><p>  The verification of this lab is arranged like this: we send a control data packet to the participant's Kafka, then Gobblin shifts this data packet to HDFS, then Airflow takes this data packet and puts it into ClickHouse.  The trick is that Airflow should not do this in real-time, it does it on schedule: every 15 minutes it takes a pack of files and throws it. </p><br><p> It turns out that we need to somehow trigger their DAG independently at our request while the checker is here and now.  Googling, they found out that for later versions of Airflow there is a so-called <a href="http://airflow.apache.org/api.html">Experimental API</a> .  The word <code>experimental</code> , of course, sounds scary, but what to do ... Suddenly take off. </p><br><p>  Next, we describe the whole path: from installing Airflow to forming a POST request that triggers the DAG using the Experimental API.  We will work with Ubuntu 16.04. </p><br><h3>  1. Installing Airflow </h3><br><p>  Let's check that we have Python 3 and virtualenv. </p><br><pre> <code class="python hljs">$ python3 --version Python <span class="hljs-number"><span class="hljs-number">3.6</span></span><span class="hljs-number"><span class="hljs-number">.6</span></span> $ virtualenv --version <span class="hljs-number"><span class="hljs-number">15.2</span></span><span class="hljs-number"><span class="hljs-number">.0</span></span></code> </pre> <br><p>  If any of this is not, then install. </p><br><p>  Now create a directory in which we will continue to work with Airflow. </p><br><pre> <code class="bash hljs">$ mkdir &lt;your name of directory&gt; $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/your/new/directory $ virtualenv -p <span class="hljs-built_in"><span class="hljs-built_in">which</span></span> python3 venv $ <span class="hljs-built_in"><span class="hljs-built_in">source</span></span> venv/bin/activate (venv) $</code> </pre> <br><p>  Install Airflow: </p><br><pre> <code class="bash hljs">(venv) $ pip install airflow</code> </pre> <br><p>  The version on which we worked: 1.10. </p><br><p>  Now we need to create an <code>airflow_home</code> directory where the DAG files and Airflow plugins will be located.  After creating the directory, set the environment variable <code>AIRFLOW_HOME</code> . </p><br><pre> <code class="bash hljs">(venv) $ <span class="hljs-built_in"><span class="hljs-built_in">cd</span></span> /path/to/my/airflow/workspace (venv) $ mkdir airflow_home (venv) $ <span class="hljs-built_in"><span class="hljs-built_in">export</span></span> AIRFLOW_HOME=&lt;path to airflow_home&gt;</code> </pre> <br><p>  The next step is to execute the command that will create and initialize the data flow database in SQLite: </p><br><pre> <code class="bash hljs">(venv) $ airflow initdb</code> </pre> <br><p>  A database will be created in <code>airflow.db</code> by default. </p><br><p>  Check if Airflow is installed: </p><br><pre> <code class="bash hljs">$ airflow version [2018-11-26 19:38:19,607] {__init__.py:57} INFO - Using executor SequentialExecutor [2018-11-26 19:38:19,745] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/Grammar.txt [2018-11-26 19:38:19,771] {driver.py:123} INFO - Generating grammar tables from /usr/lib/python3.6/lib2to3/PatternGrammar.txt ____________ _____________ ____ |__( )_________ __/__ /________ __ ____ /| |_ /__ ___/_ /_ __ /_ __ \_ | /| / / ___ ___ | / _ / _ __/ _ / / /_/ /_ |/ |/ / _/_/ |_/_/ /_/ /_/ /_/ \____/____/|__/ v1.10.0</code> </pre> <br><p>  If the command has completed, Airflow has created its <code>airflow.cfg</code> configuration file in <code>AIRFLOW_HOME</code> : </p><br><pre> <code class="bash hljs">$ tree . ‚îú‚îÄ‚îÄ airflow.cfg ‚îî‚îÄ‚îÄ unittests.cfg</code> </pre> <br><p>  Airflow has a web interface.  It can be run by running the command: </p><br><pre> <code class="bash hljs">(venv) $ airflow webserver --port 8081</code> </pre> <br><p>  Now you can access the web interface in a browser on port 8081 on the host where Airflow was running, for example: <code>&lt;hostname:8081&gt;</code> . </p><br><h3 id="2-rabota-s-experimental-api">  2. Working with the Experimental API </h3><br><p>  On this Airflow is configured and ready to go.  However, we need to run the Experimental API as well.  Our checkers are written in Python, so further all requests will be on it using the <code>requests</code> library. </p><br><p>  In fact, the API is already working for simple queries.  For example, such a request allows you to test its work: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> requests &gt;&gt;&gt; host = &lt;your hostname&gt; &gt;&gt;&gt; airflow_port = <span class="hljs-number"><span class="hljs-number">8081</span></span> <span class="hljs-comment"><span class="hljs-comment">#   ,    8080 &gt;&gt;&gt; requests.get('http://{}:{}/{}'.format(host, airflow_port, 'api/experimental/test').text 'OK'</span></span></code> </pre> <br><p>  If you received such a message in response, it means that everything works. </p><br><p>  However, when we want to override DAG, we‚Äôll face the fact that this kind of request cannot be done without authentication. </p><br><p>  To do this, you will need to do a number of actions. </p><br><p>  First, you need to add this to the config: </p><br><pre> <code class="plaintext hljs">[api] auth_backend = airflow.contrib.auth.backends.password_auth</code> </pre> <br><p>  Then, you need to create your user with admin rights: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.Admin()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'new_user_name'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'set_the_password'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Then, you need to create a user with normal permissions who will be allowed to make a DAG trigger. </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> airflow &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> models, settings &gt;&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> airflow.contrib.auth.backends.password_auth <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> PasswordUser &gt;&gt;&gt; user = PasswordUser(models.User()) &gt;&gt;&gt; user.username = <span class="hljs-string"><span class="hljs-string">'newprolab'</span></span> &gt;&gt;&gt; user.password = <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span> &gt;&gt;&gt; session = settings.Session() &gt;&gt;&gt; session.add(user) &gt;&gt;&gt; session.commit() &gt;&gt;&gt; session.close() &gt;&gt;&gt; exit()</code> </pre> <br><p>  Now everything is ready. </p><br><h3 id="3-zapusk-post-zaprosa">  3. Run POST request </h3><br><p>  The POST request itself will look like this: </p><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>dag_id = newprolab &gt;&gt;&gt; url = <span class="hljs-string"><span class="hljs-string">'http://{}:{}/{}/{}/{}'</span></span>.format(host, airflow_port, <span class="hljs-string"><span class="hljs-string">'api/experimental/dags'</span></span>, dag_id, <span class="hljs-string"><span class="hljs-string">'dag_runs'</span></span>) &gt;&gt;&gt; data = {<span class="hljs-string"><span class="hljs-string">"conf"</span></span>:<span class="hljs-string"><span class="hljs-string">"{\"key\":\"value\"}"</span></span>} &gt;&gt;&gt; headers = {<span class="hljs-string"><span class="hljs-string">'Content-type'</span></span>: <span class="hljs-string"><span class="hljs-string">'application/json'</span></span>} &gt;&gt;&gt; auth = (<span class="hljs-string"><span class="hljs-string">'newprolab'</span></span>, <span class="hljs-string"><span class="hljs-string">'Newprolab2019!'</span></span>) &gt;&gt;&gt; uri = requests.post(url, data=json.dumps(data), headers=headers, auth=auth) &gt;&gt;&gt; uri.text <span class="hljs-string"><span class="hljs-string">'{\n "message": "Created &lt;DagRun newprolab @ 2019-03-27 10:24:25+00:00: manual__2019-03-27T10:24:25+00:00, externally triggered: True&gt;"\n}\n'</span></span></code> </pre> <br><p>  The request was processed successfully. </p><br><p>  Accordingly, further we give some time to DAG for processing and make a request to the ClickHouse table, trying to catch a control packet of data. </p><br><p>  Check complete. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/445852/">https://habr.com/ru/post/445852/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../445832/index.html">Radiation: an invisible killer and his daughters or a little about radon</a></li>
<li><a href="../445834/index.html">We are working on the skill of using grouping and data visualization in Python</a></li>
<li><a href="../445838/index.html">Robotics for children: the eyes of a robot</a></li>
<li><a href="../445844/index.html">GitLab 11.9 released with secret detection and several rules for Merge Requests</a></li>
<li><a href="../445850/index.html">Sharp Memowriter EL-7000 notes storage and printing device repair after battery leakage</a></li>
<li><a href="../445854/index.html">Search JS framework for generating UI</a></li>
<li><a href="../445858/index.html">Antiquities: when phones were weird</a></li>
<li><a href="../445862/index.html">JS from all sides: the top 10 reports of HolyJS 2018 Moscow</a></li>
<li><a href="../445868/index.html">Termux Step by Step (Part 2)</a></li>
<li><a href="../445878/index.html">We get rid of duplicate packages in bundles</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>