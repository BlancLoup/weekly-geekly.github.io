<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Probabilistic Models: Sampling</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello again! Today, I continue a series of articles in the Surfingbird blog devoted to different methods of recommendations, and also sometimes just d...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Probabilistic Models: Sampling</h1><div class="post__text post__text-html js-mediator-article">  Hello again!  Today, I continue a series of articles in the Surfingbird blog devoted to different methods of recommendations, and also sometimes just different probabilistic models.  Long ago, it seems, <s>last Friday last</s> summer, I wrote a small cycle about graphical probabilistic models: the <a href="http://habrahabr.ru/company/surfingbird/blog/176461/">first part</a> introduced the basics of graphical probabilistic models, in the <a href="http://habrahabr.ru/company/surfingbird/blog/177889/">second part</a> there were several examples, <a href="http://habrahabr.ru/company/surfingbird/blog/185622/">part 3</a> talked about the message passing algorithm, and in the <a href="http://habrahabr.ru/company/surfingbird/blog/188812/">fourth part</a> we We talked briefly about variational approximations.  The cycle ended with a promise to talk about sampling - well, in less than a year.  Generally speaking, in this mini-cycle, I‚Äôll talk more about the LDA model and how it helps us make textual recommendations.  But today I will start with the fact that I will fulfill the long-held promise and tell you about sampling in probabilistic models - one of the main methods of approximate inference. <br><img src="https://habrastorage.org/getpro/habr/post_images/98a/26d/964/98a26d96499f2fcd8541e0be89728b0d.png"><br><a name="habracut"></a><br><br><h3>  What is the problem </h3><br>  First of all, I remind you of what we are talking about.  One of the main tools of machine learning is the Bayes theorem: <br><img src="https://habrastorage.org/getpro/habr/post_images/138/e30/ccb/138e30ccb4c824996cfc08804c60b9fa.png"><br>  Here <i>D</i> is the data, Œ∏ is the model parameters that we want to train, and <img src="https://habrastorage.org/getpro/habr/post_images/3a1/310/83c/3a131083c486867287a21d6b223d457a.png">  - this distribution Œ∏, subject to the available data <i>D</i> ;  about the Bayes theorem, we have already spoken in detail to <a href="http://habrahabr.ru/company/surfingbird/blog/150207/">one of the previous articles</a> .  In machine learning, we usually want to find the posterior distribution <img src="https://habrastorage.org/getpro/habr/post_images/3a1/310/83c/3a131083c486867287a21d6b223d457a.png">  and then predict new variable values <img src="https://habrastorage.org/getpro/habr/post_images/441/704/d82/441704d829c19f9806c7373364218cc6.png">  .  In complex graphical models, all this, as we discussed in the previous series, usually comes down to the fact that we have a large and confusing distribution of various random variables. <img src="https://habrastorage.org/getpro/habr/post_images/15e/94c/2e8/15e94c2e888abe442279a16951d58cdd.png">  which is decomposed into a product of distributions easier, and our task is to <i>marginalize</i> , i.e.  Sum by part of the variables or find the expectation of a function from the part of the variables.  Note that all of our tasks in one way or another boil down to calculating the expectation of different functions under the condition of a complex distribution, usually a conditional distribution of a model into which the values ‚Äã‚Äãof some variables are substituted.  We can also assume that we can count the value of the joint distribution at any point of it - this usually follows easily from the general form of the model, and the difficulty lies precisely in summing-integrating along the heap of these variables. <br><br>  We already know that it is difficult to carry out an exact conclusion, and effective algorithms are obtained only for the case of a factor graph without cycles or with small cycles.  However, real models often contain a bunch of variables that are tightly connected to each other and form a mass of cycles.  We talked a little about variational approximations - one possible way to overcome the difficulties that arise.  And today I will talk about another, even more popular method - about sampling. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Monte Carlo approach </h3><br>  The idea is simple to the extreme.  We have already found out that, in principle, everything that we need is expressed in the form of expectations of various functions in our complex and confusing distribution of <i>p</i> ( <b>x</b> ).  How to calculate the waiting function by the distribution?  If we can <i>sample</i> from this distribution, i.e.  take random points <img src="https://habrastorage.org/getpro/habr/post_images/ff8/495/900/ff849590033c05f2ced20dbbbcb7b58d.png">  over the distribution <i>p</i> ( <b>x</b> ), then the expectation of any function can be approximated as the arithmetic average at the sampled points: <br><img src="https://habrastorage.org/getpro/habr/post_images/fef/146/7e8/fef1467e8ef3ae96d83c47f8998b186f.png"><br>  For example, to calculate the average (expected value), you need to average over the samples <b>x</b> . <br><br>  Thus, our entire task actually comes down to learning how to sample points from a distribution, provided that we can read the value of this distribution at any point. <br><br><h3>  What is the difficulty </h3><br>  A person brought up on the functions of one variable (that is, in fact, anyone who has listened to the basic course of mathematical analysis at a university), it may seem that there is no big problem here, and all this talk about anything is well known, as approximately calculate the integral, you just need to break the segment into parts, count the function in the middle of each small segment, rectangles there, trapezium, you know ... And indeed, there is nothing complicated in sampling from distributions from one variable.  Difficulties, as always, begin with the growth of dimensionality - if in dimension 1 you need to count 10 function values ‚Äã‚Äãto divide the interval into segments with a side of 0.1, then in dimension 100 such values ‚Äã‚Äãyou will need <sup>10,100</sup> , which is much sadder.  This and other effects that make work in a high dimension completely different from the usual dimension of 2-3, are called ‚Äúcurse of dimensionality‚Äù;  there are other interesting counterintuitive effects there, it may be worth it sometime to talk about this in more detail. <br><br>  There is one more difficulty - I wrote above that we can read the value of the distribution at any point.  In fact, usually we can consider not the value of the distribution <i>p</i> ( <b>x</b> ), but the value of some function <i>p</i> <sup>*</sup> ( <b>x</b> ), which is <i>proportional to</i> <i>p</i> ( <b>x</b> ).  Recall the Bayes theorem - from it we get not the a posteriori distribution itself, but the function proportional to it: <br><img src="https://habrastorage.org/getpro/habr/post_images/36e/e23/bf3/36ee23bf3f889660a893c9d9d2ce36e0.png"><br>  Usually at this point we said that this is sufficient, and the exact probability value can be simply calculated by normalizing the resulting distribution so that it is summed to one (that is, by calculating the denominator in the Bayes theorem).  However, this is precisely the difficult task that we are now trying to learn how to solve: to sum up all the values ‚Äã‚Äãof a complex distribution.  Therefore, in real life, we cannot rely on the value of the true probabilities, only on some function proportional to them p <sup>*</sup> . <br><br><h3>  What to do: we sample under the schedule </h3><br>  Despite all the above difficulties, sampling is still possible, and this is indeed one of the main methods of approximate inference.  Moreover, as we will soon see, sampling is not as difficult as it seems.  In this section, I will talk about the basic idea, on which one way or another, all the sampling algorithms are based.  The idea is this: <b>if you uniformly select a point under the graph of the density function of the distribution <i>p</i> , then its X-coordinate will be taken just according to the distribution <i>p</i></b> .  It is very easy to understand intuitively: imagine a density graph and break it into small bars, like this (charts are made in matplotlib): <br><img width="500" src="https://habrastorage.org/getpro/habr/post_images/3d4/207/f84/3d4207f84f5aab2e864abf1b06f2b819.png"><br>  Now you can see that when you take a random point under the graph, each X-coordinate will meet with a probability proportional to the height of its column, i.e.  just with the value of the density function on this column. <br><br>  Thus, we only need to learn how to take a random point under the density graph of the desired distribution.  How to do it? <br><br>  The first idea that really works and is often applied in practice is the so-called <i>rejection sampling</i> .  Let's assume that we have some other distribution <i>q</i> (more precisely, the function <i>q</i> <sup>*</sup> proportional to it), which we are already able to sample.  Usually this is some kind of classical distribution - uniform or normal.  It is worth noting here that sampling from a normal distribution is also not a completely trivial task, but we will skip it now, for us it is enough to know that ‚Äúpeople can do this‚Äù.  Further, suppose that we know a constant <i>c</i> about it such that for all <b>x</b> <img src="https://habrastorage.org/getpro/habr/post_images/ea7/975/974/ea79759743871f1930bc29469d6adcad.png">  .  Then we can sample <i>p</i> in this way: <br><ul><li>  we take sample <b>x</b> on distribution <i>q</i> <sup>*</sup> ( <b>x</b> ); </li><li>  take a random number <i>u</i> uniformly from the interval <img src="https://habrastorage.org/getpro/habr/post_images/e01/038/5b8/e010385b894283f78085d026c363a56a.png">  ; </li><li>  calculate <i>p</i> <sup>*</sup> ( <b>x</b> );  if it is greater than <i>u</i> , then <b>x is</b> added to the samples, and if it is less (that is, if <i>u</i> does not fall under the density curve <i>p</i> <sup>*</sup> ), then <b>x is</b> rejected (hence the sample with the deviation). </li></ul><br><br>  Here is a picture illustrating a sample with a deviation (normal distribution, chosen to majorize a mixture of two other Gaussians): <br><img width="500" src="https://habrastorage.org/getpro/habr/post_images/235/2ab/85a/2352ab85a9c37826b9d179bc4f4b0832.png"><br>  If a point under the graph <i>cq</i> <sup>*</sup> falls under the graph <i>p</i> <sup>*</sup> , i.e.  in the blue zone, we take it;  and if above it, i.e.  in the red zone, - do not take.  Again, it is easy to understand intuitively why it works: we take a random point evenly under the <i>cq</i> <sup>*</sup> schedule, and then leave only those points that fall under the <i>p</i> <sup>*</sup> schedule - it is clear that we will have just the uniform distribution under schedule <i>p</i> <sup>*</sup> .  It is also clear that if we manage to choose <i>q</i> sufficiently similar to <i>p</i> (for example, we know approximately where <i>p is</i> concentrated and cover this area with a Gaussian), then this method will be much more efficient than simply breaking the whole space into equal pieces.  But in order for the method to work, it is necessary that <i>cq</i> really approximates <i>p</i> quite well - if the area of ‚Äã‚Äãthe blue zone is 10 <sup>-10</sup> from the area of ‚Äã‚Äãred, no samples will work ... <br><br>  There are different versions of this idea, which I will not dwell on in detail.  In short, this idea and its variations work well with complex densities in dimensions, measured in units, but in a really high dimension, complexity begins.  Because of the effects of cursing the dimension, classical test distributions <i>q</i> get different unpleasant properties (for example, in high dimension the orange peel occupies almost its entire volume, and, in particular, the normal distribution is also almost entirely concentrated in a very thin ‚Äúpeel‚Äù, and not at all zero), samples in the right places can not be obtained, and the whole thing often fails.  Other methods are needed. <br><br><h3>  Metropolis-Hastings Algorithm </h3><br>  The essence of this algorithm is based on the same idea: we want to take the point uniformly under the graph of the function.  However, the approach is now different: instead of trying to cover the entire distribution with the ‚Äúcap‚Äù of the function <i>q</i> , we will act from the inside: we will build a <i>random walk</i> under the graph of the function, moving from one point to another, and from time to time take the current point of the walk as a sample.  Since in this case this random walk is a Markov chain (i.e. its next point depends only on the previous one, but there is no memory), this class of algorithms is also called MCMC sampling (Markov chain Monte Carlo).  To prove that the Metropolis-Hastings algorithm really works, you need to know the properties of Markov chains;  but we are not going to prove anything formally now, so I‚Äôll just chastely leave the <a href="http://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BF%25D1%258C_%25D0%259C%25D0%25B0%25D1%2580%25D0%25BA%25D0%25BE%25D0%25B2%25D0%25B0">link to the wiki</a> . <br><br>  In fact, the algorithm is similar to the same sample with a deviation, but with an important difference: before, the distribution of <i>q</i> was the same for the whole space, and now it will be local and will change with time, depending on the current wandering point.  As before, we consider the family of test distributions. <img src="https://habrastorage.org/getpro/habr/post_images/280/880/168/280880168a861294b9efd843c3c923fa.png">  where <img src="https://habrastorage.org/getpro/habr/post_images/763/fcc/3bf/763fcc3bf39d8658db6de2bf44800bd5.png">  - Current state.  But now <i>q</i> should not be an approximation of <i>p</i> , but should simply be some sampled distribution that is concentrated in the vicinity of the current point ‚Äî for example, a spherical Gaussian with a small dispersion is well suited.  The candidate for the new state <i>x</i> 'will now be sampled from <img src="https://habrastorage.org/getpro/habr/post_images/280/880/168/280880168a861294b9efd843c3c923fa.png">  .  The next iteration of the algorithm begins with the state <img src="https://habrastorage.org/getpro/habr/post_images/763/fcc/3bf/763fcc3bf39d8658db6de2bf44800bd5.png">  and is this: <br><ul><li>  select <i>x</i> 'by distribution <img src="https://habrastorage.org/getpro/habr/post_images/280/880/168/280880168a861294b9efd843c3c923fa.png">  ; </li><li>  calculate <br><img src="https://habrastorage.org/getpro/habr/post_images/114/838/88d/11483888db64eb537c689567cd1d8095.png"><br>  (don't be scared, <img src="https://habrastorage.org/getpro/habr/post_images/583/aa7/2e9/583aa72e99d378f855217dcff5dec3e2.png">  for symmetric distributions, for example, Gaussian, equals 1, this is just an asymmetry correction); </li><li>  With probability <i>a</i> (1, if <i>a</i> &gt; 1) <img src="https://habrastorage.org/getpro/habr/post_images/e92/6c6/24e/e926c624e05c97952a6d8d47a849942d.png">  otherwise <img src="https://habrastorage.org/getpro/habr/post_images/da1/f76/d50/da1f76d50ae678b9b2895300973f63ea.png">  . </li></ul><br><br>  The point is that we move to a new distribution center, if we take the next step, and we get a random walk depending on the distribution <i>p</i> : we always take a step, if the density function increases in this direction, and sometimes we reject it if it decreases (we do not always reject , because we need to be able to go out of local maxima and wander under the whole schedule <i>p</i> ).  I note, by the way, that when a step is rejected, we do not just discard the new point, but repeat the same time <i>x</i> <sup>( <i>t</i> ) a</sup> second time - thus there is a high probability of repeating samples around the local maxima of the distribution <i>p</i> , which corresponds to a higher density. <br><br>  And again the picture is a typical walk on a plane under the graph of a mixture of two two-dimensional Gaussians: <br><img width="500" src="https://habrastorage.org/getpro/habr/post_images/cc0/8b6/79c/cc08b679c9150d5468c7dacac7f83329.png"><br>  To emphasize how simple this algorithm really is, I will give the code that generated this graph. <br><div class="spoiler">  <b class="spoiler_title">Python code</b> <div class="spoiler_text"><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.patches <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> patches <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.path <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> path <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.mlab <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> mlab <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scipy.stats <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> stats delta = <span class="hljs-number"><span class="hljs-number">0.025</span></span> X, Y = np.meshgrid(np.arange(<span class="hljs-number"><span class="hljs-number">-4.5</span></span>, <span class="hljs-number"><span class="hljs-number">2.0</span></span>, delta), np.arange(<span class="hljs-number"><span class="hljs-number">-3.5</span></span>, <span class="hljs-number"><span class="hljs-number">3.5</span></span>, delta)) z1 = stats.multivariate_normal([<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>],[[<span class="hljs-number"><span class="hljs-number">1.0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>],[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1.0</span></span>]]) z2 = stats.multivariate_normal([<span class="hljs-number"><span class="hljs-number">-2</span></span>,<span class="hljs-number"><span class="hljs-number">-2</span></span>],[[<span class="hljs-number"><span class="hljs-number">1.5</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>],[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0.5</span></span>]]) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">z</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(x)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0.4</span></span>*z1.pdf(x) + <span class="hljs-number"><span class="hljs-number">0.6</span></span>*z2.pdf(x) Z1 = mlab.bivariate_normal(X, Y, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.0</span></span>, <span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">0.0</span></span>) Z2 = mlab.bivariate_normal(X, Y, <span class="hljs-number"><span class="hljs-number">1.5</span></span>, <span class="hljs-number"><span class="hljs-number">0.5</span></span>, <span class="hljs-number"><span class="hljs-number">-2</span></span>, <span class="hljs-number"><span class="hljs-number">-2</span></span>) Z = <span class="hljs-number"><span class="hljs-number">0.4</span></span>*Z1 + <span class="hljs-number"><span class="hljs-number">0.6</span></span>*Z2 Q = stats.multivariate_normal([<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>],[[<span class="hljs-number"><span class="hljs-number">0.05</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>],[<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0.05</span></span>]]) r = [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">0</span></span>] samples = [r] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1000</span></span>): rq = Q.rvs() + r a = z(rq)/z(r) <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> np.random.binomial(<span class="hljs-number"><span class="hljs-number">1</span></span>,min(a,<span class="hljs-number"><span class="hljs-number">1</span></span>),<span class="hljs-number"><span class="hljs-number">1</span></span>)[<span class="hljs-number"><span class="hljs-number">0</span></span>] == <span class="hljs-number"><span class="hljs-number">1</span></span>: r = rq samples.append(r) codes = np.ones(len(samples), int) * path.Path.LINETO codes[<span class="hljs-number"><span class="hljs-number">0</span></span>] = path.Path.MOVETO p = path.Path(samples,codes) fig, ax = plt.subplots() ax.contour(X, Y, Z) ax.add_patch(patches.PathPatch(p, facecolor=<span class="hljs-string"><span class="hljs-string">'none'</span></span>, lw=<span class="hljs-number"><span class="hljs-number">0.5</span></span>, edgecolor=<span class="hljs-string"><span class="hljs-string">'gray'</span></span>)) plt.show()</code> </pre> </div></div><br><br>  So, we got a random walk under the density graph <i>p</i> ( <b>x</b> );  in fact, it is still necessary to prove it, but we will not go deep into mathematics, so take my word for it.  What now to do with this random walk, we also need independent samples, and here it turns out that the next point of the walk obviously lies next to the previous one, and there is no independence at all? <br><br>  The answer is simple: you need to take not every point, but only some, separated from each other by many steps.  Since this is a random walk, then if most of <i>q is</i> concentrated in radius Œµ, and the total radius in which <i>p</i> is concentrated, is equal to <i>D</i> , then to obtain an independent sample, you will need about <img src="https://habrastorage.org/getpro/habr/post_images/30a/97b/0d7/30a97b0d778bd29f64914955538b1618.png">  steps.  In this you, again, can take a word, or you can recall the classic puzzle from the course of probability theory about how far in <i>n</i> steps the point goes, which moves with equal probability to the left and to the right;  it will go on average <img src="https://habrastorage.org/getpro/habr/post_images/77d/71f/70d/77d71f70d8df42a72894597ce054e33f.png">  , so it is logical that the steps will be a quadratic number. <br><br>  And now the main feature: this quadratic number depends on the radii of the two distributions, but it is completely <i>independent of the dimension</i> .  Monte Carlo Markov chain sampling is much less subject to curse dimension than the other methods we discussed;  that is why it has become, one might say, the gold standard.  In practice, however, it is difficult to estimate <i>D</i> , so in concrete implementations they usually do this: first, quite a few number of first steps, say 1000, are discarded (this is called burn-in, and this is really important because the starting point can fall into an unsuccessful area <i>p</i> , so that before starting the process, you need to make sure that we have already walked for a long time), and then take each nth sample, where <i>n is</i> selected experimentally, based on the actual autocorrelation in the sample sequence. <br><br><h3>  Gibbs sampling </h3><br>  And the latest sampling algorithm for today is the Gibbs sampling.  This is actually a special case of the Metropolis-Hastings algorithm, a very popular and simple special case. <br><br>  The idea of ‚Äã‚Äãsampling according to Gibbs is quite simple: suppose that we are in a very large dimension, the vector <b>x is</b> very long, and it is difficult for us to choose the entire sample at once, it does not work.  Let's try to choose a sample not all at once, but component by component.  Then surely these one-dimensional distributions will be simpler, and we will choose the sample. <br><br>  Formally speaking, we run through the components of the vector <img src="https://habrastorage.org/getpro/habr/post_images/ff8/495/900/ff849590033c05f2ced20dbbbcb7b58d.png">  , at each step, we fix all the variables, except for one, and choose sequentially <img src="https://habrastorage.org/getpro/habr/post_images/134/16a/280/13416a2809810d14ebeaff56fc613578.png">  by distribution <br><img src="https://habrastorage.org/getpro/habr/post_images/b4f/c28/615/b4fc28615d4e35a111168ffc51b761ae.png"><br><br>  This differs from the general case of wandering around Metropolis-Hastings in that now we move at each step along one of the coordinate axes;  Here is the corresponding picture: <br><img width="500" src="https://habrastorage.org/getpro/habr/post_images/487/76e/4b7/48776e4b799cfb7a0ab5150b03b29497.png"><br><br>  Gibbs sampling is a special case of the Metropolis algorithm for distributions <img src="https://habrastorage.org/getpro/habr/post_images/62a/e16/196/62ae1619611aaac939acddfaa87e7c3e.png">  , and the probability of taking each sample is always equal to 1 (you can prove it as an exercise).  Therefore, Gibbs sampling converges, and since this is the same random walk in essence, the same quadratic estimate is true. <br><br>  Gibbs sampling does not require any special assumptions or knowledge.  You can quickly make a working model, and therefore it is a very popular algorithm.  We also note that in large dimensions it may be more efficient to sample several variables at once, rather than one ‚Äî for example, it often happens that we have a bipartite graph from variables in which all the variables from one fraction are associated with all the variables from another fraction (well or with many), and are not related to each other.  In such a situation, God himself ordered to fix all the variables of one beat and sample all the variables in the other beat at the same time (this can be taken literally - since with this structure all variables of one beat are conditionally independent under the condition of the other, they can be sampled independently and in parallel), then fix all variables of the second beat and so on. <br><br><h3>  Conclusion </h3><br>  Today we managed quite a lot: we talked about different sampling methods, which is one of the main tools for approximate inference in complex probabilistic models.  Next, I plan to develop this new mini-cycle in the direction of thematic modeling, more precisely, the model LDA (latent Dirichlet allocation, latent placement of Dirichlet).  In the next series I will talk about the model itself (I <a href="http://habrahabr.ru/company/surfingbird/blog/150607/">have already described</a> it briefly, now you can talk in more detail) and explain how the Gibbs sampling works for LDA.  And then we will gradually move on to how LDA and its numerous extensions can be applied to recommender systems. </div><p>Source: <a href="https://habr.com/ru/post/226677/">https://habr.com/ru/post/226677/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../226665/index.html">Microsoft moves to proprietary processors</a></li>
<li><a href="../226667/index.html">RaspberryPi + Pioneer System Remote</a></li>
<li><a href="../226669/index.html">Cross-platform file manager? This is reality</a></li>
<li><a href="../226673/index.html">"Eat your own dog food" or how we found the most important client</a></li>
<li><a href="../226675/index.html">Test Labs 2014. ONLINE conference for testers and test managers. June 28</a></li>
<li><a href="../226679/index.html">Another way to intercept traffic through ARP Spoofing</a></li>
<li><a href="../226681/index.html">Disaster Recovery Planning. The second part of</a></li>
<li><a href="../226687/index.html">Continuation of Candy Crush, transforming pigs and mobile tanks - the main mobile news of the week</a></li>
<li><a href="../226689/index.html">Everybody lies! ‚Ñ¢ or casuistry describing business processes</a></li>
<li><a href="../226691/index.html">Amazon introduced its smartphone with 6 Fire Phone cameras</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>