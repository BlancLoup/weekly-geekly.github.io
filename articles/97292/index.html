<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Distributed computing on the .NET platform</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The widespread use of parallel computing architectures is raising interest in software development tools that can make the most of the hardware resour...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Distributed computing on the .NET platform</h1><div class="post__text post__text-html js-mediator-article">  The widespread use of parallel computing architectures is raising interest in software development tools that can make the most of the hardware resources of this type. <br><br>  However, by the current moment there is a certain gap between the technologies of hardware implementation of parallelism available in the consumer market and the software tools for their support.  So, if multi-core general-purpose computers became the norm in the middle of this decade, the emergence of OpenMP ‚Äî the popular standard for developing programs for such systems ‚Äî was noted almost ten years earlier [1].  At almost the same time, the MPI standard appeared, describing ways to transfer messages between processes in a distributed environment [2]. <br><br>  The development of both these standards, which is expressed only in extending the functionality without adapting paradigms to the object-oriented approach, leads to the fact that they are incompatible with modern programming platforms such as the Microsoft .NET Framework.  Therefore, the developers of these platforms have to make additional efforts to introduce concurrency tools into their products. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In [3], the author reviewed one of these technologies, Microsoft Parallel Extensions, which allows a fairly simple way to implement parallelism in initially sequential managed code for computers with shared memory.  The opportunity and expediency of using the .NET Framework platform for scientific calculations was also shown there.  Nevertheless, it remains an open question about the applicability of this platform for developing programs used for carrying out complex calculations on systems with distributed memory, for example, computational clusters.  These systems are based on a set of interconnected computing nodes, each of which is a full-fledged computer with its own processor, memory, input / output subsystem, operating system, each node operating in its own address space. <br><a name="habracut"></a><br><h1>  MPI.  Main idea and disadvantages </h1>  In the world of functional programming, the most common technology for creating programs for parallel computers of this type is MPI.  The main way of interaction between parallel processes in this case is the transfer of messages from one node to another.  The MPI standard fixes the interface that must be followed by both the programming system on each computing platform and the user when creating their own programs. <br><br>  MPI supports the Fortran and C languages. The MPI program is a set of parallel interacting processes.  All processes are generated once, forming a parallel part of the program.  Each process operates in its own address space; there are no shared variables or data in MPI.  The main way to communicate between processes is to send messages from one process to another.  [four] <br><br>  Despite the fact that MPI programs show a high level of performance, the technology itself has several disadvantages: <br><ul><li>  low level (MPI programming is often compared with programming in assembler), the need for detailed control over the distribution of arrays and turns of cycles between processes, as well as the exchange of messages between processes ‚Äî all this leads to a high complexity of program development; </li><li>  the need for excessive specification of data types in the transmitted messages, as well as the presence of strict restrictions on the types of data transmitted; </li><li>  the complexity of writing programs that can be performed with an arbitrary size of arrays and an arbitrary number of processes makes it almost impossible to reuse existing MPI programs; </li><li>  lack of support for an object-oriented approach. </li></ul>  Perhaps some of these shortcomings caused the lack of support for the MPI interface in such a modern development platform as the .NET Framework, which can significantly limit its applicability to solving large computational problems. <br><br>  Nevertheless, starting from the third version, the .NET Framework includes the Windows Communication Foundation (WCF) - a unified technology for creating all types of distributed applications on the Microsoft platform [5].  Unfortunately, this technology is often understood only as a framework for working with Web services based on XML, which in vain prevents us from considering WCF as an effective means for organizing parallel computing. <br><br><h1>  WCF structure </h1>  To determine whether WCF can be used as a development tool for distributed memory systems, consider the basics of this technology.  WCF service is a set of endpoints that provide clients with some useful features.  The endpoint is simply a network resource to which you can send messages.  To take advantage of the opportunities offered, the client sends messages to the endpoints in a format that is described by a contract between the client and the service.  Services expect messages to be delivered to the endpoint address, assuming messages will be recorded in the specified format. <br><br>  In order for the client to transmit information to the service, he must know ‚ÄúAPK‚Äù: address, linkage and contract. <br><br>  <strong>The address</strong> determines where to send the messages so that the endpoint receives them. <br><br>  <strong>A binding</strong> defines a channel for communications with an endpoint.  All messages circulating in the WCF application are transmitted through the channels.  A channel consists of several binding elements.  At the lowest level, the binding element is a transport mechanism that provides message delivery over the network.  The binding elements located above describe the requirements for security and transactional integrity.  WCF comes with a set of pre-made bindings.  For example, the basicHttpBinding binding is applicable to access most Web services created before 2007;  The netTcpBinding binding implements high-speed TCP data exchange for communications between two .NET systems;  netNamedPipeBinding is intended for communications within a single machine or between several .NET systems.  Building applications for work in peer-to-peer networks is also supported; for this, there is a netPeerTcpBinding binding. <br><br>  <strong>The contract</strong> defines the set of functions provided by the endpoint, that is, the operations that it can perform, and the message formats for these operations.  The operations described in the contract are mapped to the methods of the class implementing the endpoint, and in particular include the types of parameters passed to and received from each method.  WCF supports synchronous and asynchronous, one-way and full-duplex operations with arbitrary data types, which is sufficient for building distributed applications based on the .NET Framework to solve large computational problems.  However, the information provided does not help assess the practical applicability and effectiveness of WCF technology in this case. <br><br><h1>  WCF Performance Evaluation Method </h1>  We will assess the applicability of WCF from the following positions: <br><ul><li>  The possibility and complexity of developing distributed applications for solving computational problems. </li><li>  The efficiency of data exchange between components of a distributed application. </li><li>  Overall performance of distributed computing using WCF. </li></ul>  Assessment seems appropriate to carry out a relatively common and well-proven means of building parallel programs for systems with distributed memory - MPI.  With this approach, the assessment of efficiency will be reduced to a comparative analysis of the execution time of similar programs on the WCF and MPI platforms in a similar environment.  By similar programs are meant programs that use the same computational algorithm up to and including optimizations and identical data types.  A similar environment is the identity of the computing and network hardware resources used for the experiment. <br><br>  To illustrate the possibilities, as well as assessing the complexity of developing applications for computational problems using WCF, it is proposed to implement with it some demonstration algorithms described on the official MPI resource [6]. <br><br>  To test the performance of solutions, two computers with four and two processor cores of equal performance are used, respectively.  Communication is carried out via a 100Mbit Ethernet network.  WCF testing is performed on Microsoft Windows 7, Pelican HPC [7] is used to build MPI cluster on Linux 2.6.30 kernel using Open MPI 1.3.3. <br><br><h1>  Simple information sharing program </h1>  The simplest test, designed to show the efficiency of data exchange between components of a computer network, is to send an array of double-valued real numbers from one node to another and back with fixing the time taken to perform these operations.  The code in C ++ for MPI is as follows: <br><br> <code>#define NUMBER_OF_TESTS 10 <br> int main( argc, argv ) <br> int argc; <br> char **argv; <br> { <br> double    *buf; <br> int       rank, n, j, k, nloop; <br> double    t1, t2, tmin; <br> MPI_Status status; <br> MPI_Init( &amp;argc, &amp;argv ); <br> MPI_Comm_rank( MPI_COMM_WORLD, &amp;rank ); <br> if (rank == 0) <br> printf( "Kind\t\tn\ttime (sec)\tRate (MB/sec)\n" ); <br> for (n=1; n&lt;1100000; n*=2) { <br> if (n == 0) nloop = 1000; <br> else       nloop = 1000/n; <br> if (nloop &lt; 1) nloop = 1; <br> buf = (double *) malloc( n * sizeof(double) ); <br> tmin = 1000; <br> for (k=0; k&lt;NUMBER_OF_TESTS; k++) { <br> if (rank == 0) { <br> t1 = MPI_Wtime(); <br> for (j=0; j&lt;nloop; j++) { <br> MPI_Ssend( buf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD ); <br> MPI_Recv( buf, n, MPI_DOUBLE, 1, k, MPI_COMM_WORLD, <br> &amp;status ); <br> } <br> t2 = (MPI_Wtime() - t1) / nloop; <br> if (t2 &lt; tmin) tmin = t2; <br> } <br> else if (rank == 1) { <br> for (j=0; j&lt;nloop; j++) { <br> MPI_Recv( buf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD, <br> &amp;status ); <br> MPI_Ssend( buf, n, MPI_DOUBLE, 0, k, MPI_COMM_WORLD ); <br> } <br> } <br> } <br> if (rank == 0) { <br> double rate; <br> if (tmin &gt; 0) rate = n * sizeof(double) * 1.0e-6 /tmin; <br> else       rate = 0.0; <br> printf( "Send/Recv\t%d\t%f\t%f\n", n, tmin, rate ); <br> } <br> free( buf ); <br> } <br> MPI_Finalize( ); <br> return 0; <br> } <br></code> <br>  Its operation requires the presence of two MPI-processes, between which there is an exchange of arrays of double-type elements with sizes from 1 to 1048576 elements using the commands MPI_Recv () and MPI_Ssend.  It is worth noting the impact of the previously mentioned disadvantages of MPI: <br><ul><li>  The separation of the functionality of both processes is based solely on the process number (rank), which complicates the perception of the listing program. </li><li>  Before sending data (MPI_Ssend), you need to be sure that the receiving party has explicitly initialized their reception (MPI_Recv), which complicates the development process. </li><li>  The specification of the types of transmitted data (MPI_DOUBLE) together with the data itself is rational from the rational point of view, which can also lead to logical errors in the program. </li><li>  In addition, the unpleasant moment associated with the organization of a programming language is the need to manually allocate and free memory. </li></ul>  Consider now the solution of the same problem, but in C # using WCF technology.  To begin with, it is necessary to define a pair of interfaces through which information will be exchanged between computing nodes.  The need for a second interface, IClientCallback, is due to the asynchronous nature of the operations. <br><br> <code>[ServiceContract(CallbackContract = typeof(IClientCallback))] public interface IServerBenchmark <br> { <br> [OperationContract(IsOneWay = true)] void SendArray(double[] array); <br> } <br> public interface IClientCallback <br> { <br> [OperationContract(IsOneWay = true)] void SendArrayFromServer(double[] array); <br> } <br></code> <br>  The following describes a class that implements the logic of the server application, which receives an array of real numbers from the client application and sends it back: <br><br> <code>public class ServerBenchmark : IServerBenchmark <br> { <br> public void SendArray(double[] array) <br> { <br> OperationContext.Current.GetCallbackChannel&lt;IClientCallback&gt;().SendArrayFromServer(array); <br> } <br> } <br></code> <br>  The main method of the server application, registering the APK, is implemented.  At the same time, the behavior of the server application can be configured according to the settings both in the code and in the .config file of the application. <br><br> <code>class Program <br> { <br> static void Main(string[] args) <br> { <br> ServiceHost serviceHost = new ServiceHost(); <br> NetTcpBinding binding = new NetTcpBinding(); <br> serviceHost.AddServiceEndpoint(typeof(IServerBenchmark), binding, ""); <br> serviceHost.Open(); <br> Console.ReadLine(); <br> serviceHost.Close(); <br> } <br> } <br></code> <br>  The client application looks like this, where the CallBackHandler class implements the corresponding interface described earlier, and the Client class provides a static method that acts as an entry point: <br><br> <code>public class CallbackHandler : IServerBenchmarkCallback <br> { <br> private static EventWaitHandle _waitHandle = new EventWaitHandle(false, EventResetMode.AutoReset); <br> static int _totalIterations = 10; <br> public static DateTime _dateTime; <br> public void SendArrayFromServer(double[] array) <br> { <br> _waitHandle.Set(); <br> } <br> <br> class Client <br> { <br> private static InstanceContext _site; <br> static void Main(string[] args) <br> { <br> _site = new InstanceContext(new CallbackHandler()); <br> ServerBenchmarkClient client = new ServerBenchmarkClient(_site); <br> double[] arr = new double[Convert.ToInt32(args[0])]; <br> for (int index = 0; index &lt; arr.Length;index++ ) <br> arr[index] = index; <br> _dateTime = DateTime.Now; <br> for (int index = 0; index &lt; _totalIterations; index++) <br> { <br> client.SendArray(arr); <br> _waitHandle.WaitOne(); <br> } <br> Console.WriteLine((DateTime.Now - _dateTime).TotalMilliseconds / _totalIterations); <br> Console.ReadKey(); <br> } <br> } <br> } <br></code> <br>  The program performs several iterations, each of which consists in calling the remote SendArray method with passing it an array of data and waiting for the SendArrayFromServer method call, which receives the data returned from the server. <br><br>  The code presented has several important advantages regarding MPI, namely: <br><ul><li>  Uniform and one-time description of the formats of the transmitted data, implemented in the interface (contract). </li><li>  A simple implementation of calling remote methods. </li><li>  Using an object-oriented approach to development. </li></ul>  Thus, it can be argued that the use of WCF greatly simplifies the task of writing distributed applications.  The results of performance testing can be found below: <br><br><img src="https://habrastorage.org/storage/habraeffect/9d/ef/9deff5adaae9ebd70a3305b85833e423.png" alt="image"><br><br>  From this table it follows that, unfortunately, WCF technology is not suitable for developing programs that require frequent inter-process data exchange of small volume. <br><br><h1>  An example of distributed computing </h1>  Let us now consider the possibility of building a distributed application that does not require active data exchange between nodes.  Let's take one more example from the resource [6] - the calculation of the number Pi.  In this example, Pi is calculated as <img src="https://habrastorage.org/storage/habraeffect/1a/31/1a31632d625301c97a1ad0746a60ef9c.png" alt="image">  .  It is very easy to detect the parallelism of this algorithm: the integration interval is divided into as many parts as the computational nodes will be involved in the calculations.  A C ++ program for MPI that solves the problem is as follows: <br><br> <code>int main(argc,argv) <br> int argc; <br> char *argv[]; <br> { <br> int done = 0, n, myid, numprocs, i; <br> double PI25DT = 3.141592653589793238462643; <br> double mypi, pi, h, sum, x; <br> MPI_Init(&amp;argc,&amp;argv); <br> MPI_Comm_size(MPI_COMM_WORLD,&amp;numprocs); <br> MPI_Comm_rank(MPI_COMM_WORLD,&amp;myid); <br> while (!done) <br> { <br> if (myid == 0) { <br> printf("Enter the number of intervals: (0 quits) "); <br> scanf("%d",&amp;n); <br> } <br> MPI_Bcast(&amp;n, 1, MPI_INT, 0, MPI_COMM_WORLD); <br> if (n == 0) break; <br> h = 1.0 / (double) n; <br> sum = 0.0; <br> for (i = myid + 1; i &lt;= n; i += numprocs) { <br> x = h * ((double)i - 0.5); <br> sum += 4.0 / (1.0 + x*x); <br> } <br> mypi = h * sum; <br> MPI_Reduce(&amp;mypi, &amp;pi, 1, MPI_DOUBLE, MPI_SUM, 0, <br> MPI_COMM_WORLD); <br> <br> if (myid == 0) <br> printf("pi is approximately %.16f, Error is %.16f\n", <br> pi, fabs(pi - PI25DT)); <br> } <br> MPI_Finalize(); <br> return 0; <br> } <br></code> <br>  The code is rather laconic, however, all MPI disadvantages listed above are peculiar to it.  For WCF, the server part in C # looks like this: <br><br> <code>[ServiceContract] public interface IPiService <br> { <br> [OperationContract] double CalculatePiChunk(int intervals, int processId, int processesCount); <br> } <br> public class PiService : IPiService <br> { <br> public double CalculatePiChunk(int intervals, int processId, int processesCount) <br> { <br> double h = 1.0 / (double)intervals; <br> double sum = 0.0; <br> double x; <br> for (int i = processId + 1; i &lt;= intervals; i += processesCount) <br> { <br> x = h * (i - 0.5); <br> sum += 4.0 / (1.0 + x * x); <br> } <br> return h * sum; <br> } <br> } <br> public class Service <br> { <br> public static void Main(string[] args) <br> { <br> ServiceHost serviceHost = new ServiceHost(typeof(PiService)); <br> serviceHost.AddServiceEndpoint(typeof(IPiService), new NetTcpBinding(), ""); <br> serviceHost.Open(); <br> Console.ReadLine(); <br> serviceHost.Close(); <br> } <br> } <br></code> <br>  Allocation of the server part not only facilitates the understanding of the code, but also makes it possible to reuse it without being tied to any particular client.  The client itself can be implemented as follows (the remote CalculatePiChunk method is called asynchronously with its own set of parameters on each computational node): <br><br> <code>class Client <br> { <br> private static double _pi; <br> private static DateTime _startTime; <br> static int _inProcess = 0; <br> static void Main(string[] args) <br> { <br> _pi = 0; <br> int intervals = Convert.ToInt32(args[0]); <br> List&lt;String&gt; endPoints = new List&lt;string&gt;(); <br> for (int index = 1; index&lt;args.Length;index++) <br> endPoints.Add(args[index]); <br> double pi = 0; <br> _inProcess = endPoints.Length; <br> PiServiceClient[] clients = new PiServiceClient[endPoints.Length]; <br> for (int index = 0; index &lt; endPoints.Length; index++) <br> clients[index] = new PiServiceClient("NetTcpBinding_IPiService", "net.tcp://" + endPoints[index] + "/EssentialWCF"); <br> _startTime = DateTime.Now; <br> for (int index = 0; index&lt; endPoints.Length; index++) <br> clients[index].BeginCalculatePiChunk(intervals, index, endPoints.Length, GetPiCallback, clients[index]); <br> Console.ReadKey(); <br> } <br> <br> static void GetPiCallback(IAsyncResult ar) <br> { <br> double d = ((PiServiceClient)ar.AsyncState).EndCalculatePiChunk(ar); <br> lock(ar) <br> { <br> _pi += d; <br> _inProcess--; <br> if (_inProcess == 0) <br> { <br> Console.WriteLine(_pi); <br> Console.WriteLine("Calculation ms elasped: " + (DateTime.Now - _startTime).TotalMilliseconds); <br> } <br> } <br> } <br> } <br></code> <br>  The results of performance measurements are presented below: <br><br><img src="https://habrastorage.org/storage/habraeffect/66/46/6646a8e97c35c5ba8247f9b96cf70085.png" alt="image"><br><br>  According to these results, it can be judged that the use of the WCF technology of the .NET Framework for building distributed computing applications with a small number of interprocess communications shows good results: the relatively low data exchange rate is compensated for by the qualitative optimization of the JIT code [3] in such a way that .NET managed the program in many situations turns out to be more productive. <br><br>  Based on the results of the brief testing, the following conclusions can be made: <br><ul><li>  Developing applications on the .NET Framework to solve computational problems on systems with distributed memory is possible. </li><li>  WCF technology, designed to build applications of this kind, provides a much simpler way of interprocess communication than is implemented in MPI. </li><li>  In turn, this simplicity leads to a significant drop in the performance of data exchange processes: in some cases, MPI is faster than WCF by more than two and a half times. </li><li>  Thus, WCF is not suitable for solving problems that require intensive exchange of small groups of data between computing nodes. </li><li>  However, the use of this technology is justified in the case of more rare interprocess communication: in addition to a simpler development method compared to MPI, the .NET Framework provides other benefits for organizing scientific computing, such as the interoperability of the resulting programs, automatic memory management, interlanguage interaction, functional support. programming.  [3] </li></ul><br>  LITERATURE <br>  1. OpenMP Reference.  OpenMP Architecture Review Board, 2008 r. <br>  2. MPI 2.1 Reference.  University of Tennessee, 2008 r. <br>  3. Parallel programming in .NET.  Tikhonov, I. V. Irkutsk, 2009. Proceedings of the XIV Baikal All-Russian Conference "Information and Mathematical Technologies in Science and Management." <br>  4. Antonov, A. S. Parallel programming using MPI technology.  M .: Publishing House of Moscow State University. <br>  5. Resnick, Steve, Crane, Richard and Bowen, Chris.  Basics of Windows Communication Foundation for .NET Framework 3.5.  M .: DMK Press, 2008. <br>  6. The Message Passing Interface (MPI) standard.  <a href="http://www.mcs.anl.gov/research/projects/mpi/">www.mcs.anl.gov/research/projects/mpi</a> <br>  7. PelicanHPC GNU Linux.  <a href="http://pareto.uab.es/mcreel/PelicanHPC/">pareto.uab.es/mcreel/PelicanHPC</a> </div><p>Source: <a href="https://habr.com/ru/post/97292/">https://habr.com/ru/post/97292/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../97284/index.html">The problem of dynamic allocation of disk space of virtual machines</a></li>
<li><a href="../97285/index.html">Event Brokers Part 1</a></li>
<li><a href="../97287/index.html">Medvedev drove to visit Steve</a></li>
<li><a href="../97289/index.html">Bug with box-shadow in Opera</a></li>
<li><a href="../97290/index.html">Checking mass warning systems in Novosibirsk</a></li>
<li><a href="../97295/index.html">Chaos Constructions 2010: Exhibition</a></li>
<li><a href="../97296/index.html">Vuvuzel button added to YouTube player</a></li>
<li><a href="../97297/index.html">Your attitude to music programming</a></li>
<li><a href="../97298/index.html">Overview of the Huawei U8230 Communicator from life;) Belarus. Part one</a></li>
<li><a href="../97301/index.html">(updated) The level of reception of the signal on the iPhone 4 drops at times, if you take the phone in hand</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>