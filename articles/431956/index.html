<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Logic, Explainability and Future of Understanding</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Logic discovery 
 Logic is the basis of many things. But what is the basis of logic itself? 

 In character logic, characters like p and q are inserte...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Logic, Explainability and Future of Understanding</h1><div class="post__text post__text-html js-mediator-article"><h3>  Logic discovery </h3><br>  Logic is the basis of many things.  But what is the basis of logic itself? <br><br>  In character logic, characters like p and q are inserted, denoting statements (or ‚Äúpropositions‚Äù) of the type ‚Äúthis is an interesting essay‚Äù.  There are also certain rules of logic, for example, for any p and any q the expression NOT (p AND q) is similar to (NOT p) OR (NOT q). <br><br>  But where do these "rules of logic" come from?  Logic is a formal system.  Like Euclidean geometry, it can be constructed on axioms.  But what are axioms?  You can start with statements like p AND q = q AND p, or NOT NOT p = p.  But how many axioms are required?  How simple can they be? 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      This question has long been painful.  But at 20:31 on Sunday, January 29, 2000, the only axiom appeared on my computer screen.  I have already shown that nothing can be easier, but soon found that this single small axiom was enough to create all the logic: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8a5/426/b76/8a5426b76ec6ff73e4b9f521f1e66764.png"></div><a name="habracut"></a><br>  How was I to know that she was right?  Because I made the computer prove it.  And here is the proof that I printed in the book " <a href="http://www.wolframscience.com/nks">New Type of Science</a> " (already available in <a href="https://datarepository.wolframcloud.com/">the Wolfram Data repository</a> ): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9c7/d9f/11e/9c7d9f11e1cb651d7bfbe6b4edad66dd.png"></div><br>  Using the <a href="https://blog.stephenwolfram.com/2018/03/roaring-into-2018-with-another-big-release-launching-version-11-3-of-the-wolfram-language-mathematica/">latest version of the</a> Wolfram Language, anyone can <a href="https://reference.wolfram.com/language/ref/FindEquationalProof.html">generate</a> this proof in less than a minute.  And his every step is easy <a href="https://reference.wolfram.com/language/ref/ProofObject.html">to check</a> .  But why the result will be correct?  How to explain it? <br><br>  Similar questions are increasingly being asked about all sorts of computing systems and applications related to machine learning and AI.  Yes, we see what happens.  But can we understand this? <br><br>  I think that this question is deep in its essence - and crucial for the future of science and technology, and for the future of all intellectual development. <br><br>  But before we talk about this, let's discuss the axiom that I discovered. <br><br><h2>  Story </h2><br>  Logic as a formal discipline comes from Aristotle, who lived in the 4th century BC.  Within the framework of the work of his life on cataloging things (animals, causes, etc.), Aristotle compiled a catalog of admissible forms of arguments and created symbolic patterns for them, which, in fact, provided the main logic content for two thousand years to come. <br><br>  However, by the 15th century algebra was invented, and with it a clearer idea of ‚Äã‚Äãthings appeared.  But it was not until 1847 that George Boole finally formulated logic in the same way as algebra, with logical operations of the type AND and OR, operating according to rules similar to the rules of algebra. <br><br>  A few years later, people were already recording axiomatic systems for logic.  A typical example was: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/1b7/f83/b05/1b7f83b053013cbb17e8a3e3f9734097.png"></div><br>  But do we really need AND, OR and NOT for the logic?  After the first decade of the 20th century, several people have discovered that a single operation, which we now call NAND, will suffice, and, for example, p OR q can be calculated as (p NAND p) NAND (q NAND q).  The ‚Äúfunctional completeness‚Äù of NAND could have remained a wonder if not for the development of semiconductor technology ‚Äî it implements all the billions of logical operations in a modern microprocessor using a combination of transistors that perform only the NAND function or the associated NOR. <br><br>  Well, so how do the axioms of logic look in terms of NAND?  Here is their first known version, written by Henry Schaeffer in 1913 (here the dot denotes NAND): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/2fd/af7/5d1/2fdaf75d1c346bb2d8c3c4d470ffcd68.png"></div><br>  In 1910, Principia Mathematica, a three-volume work on the logic and philosophy of mathematics by Alfred North Whitehead and Bertrand Russell, popularized the idea that, perhaps, all mathematics can be derived from logic.  Given this, it was quite interesting to study the question of how simple the axioms of logic can be.  The most significant work in this area was carried out in Lviv and Warsaw (at that time these cities were part of Poland), in particular, Jan Lukasevich (as a side effect of his work in 1920 he invented a ‚ÄúPolish‚Äù notation that does not require brackets).  In 1944, at the age of 66, Lukasevich fled from the advancing Soviet army and in 1947 ended up in Ireland. <br><br>  Meanwhile, Irishman Carew Meredith, who studied at Winchester and Cambridge and became a mathematics teacher at Cambridge, was forced to return to Ireland in 1939 because of his pacifism.  In 1947, Meredith got to Lukasevich‚Äôs lecture in Dublin, which inspired him to look for simple axioms, which he did, for the most part, the rest of his life. <br><br>  Already by 1949, Meredith discovered a two-axiom system: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/237/0cf/7a8/2370cf7a8b94b221152f17f905223919.png"></div><br>  Almost 20 years later, in 1967, he managed to simplify it to: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/245/ba1/89c/245ba189c20baf7290fde8e6164b1050.png"></div><br>  Is it possible to simplify this further?  Meredith has been fiddling with this for years, figuring out where else extra NANDs can be removed.  But after 1967, he no longer advanced further (and died in 1976), although in 1969 he found a three-axiomatic system: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/650/552/dcd/650552dcd498bb69a243af9ca9373091.png"></div><br>  When I began to study the axiom systems of logic, I did not know anything about the work of Meredith.  I was fascinated by this topic as part of trying to understand what kind of behavior can grow out of simple rules.  In the 1980s, I made an <a href="http://www.wolframscience.com/nks/chap-2--the-crucial-experiment/">unexpected discovery</a> that even cellular automata with the simplest possible rules ‚Äî such as my favorite <a href="https://www.wolframscience.com/nks/p27--how-do-simple-programs-behave/">rule 30</a> ‚Äî can lead to incredibly complex behavior. <br><br>  Having spent the 1990s trying to understand the commonality of this phenomenon, I finally wanted to see how it can be applied to mathematics.  In mathematics, we, in fact, begin to work with axioms (for example, in arithmetic, in geometry, in logic), and then we try to prove a whole set of complex theorems based on them. <br><br>  However, how simple can axioms be?  That is what I wanted to establish in 1999.  As a first example, I decided to study logic (or, equivalently, Boolean algebra).  Refuting all my expectations, my experience with cellular automata, Turing machines and other systems - including even partial differential equations - says that you can simply start listing the simplest cases possible, and at some point see something interesting. <br><br>  But is it possible to ‚Äúopen logic‚Äù in this way?  There was only one way to say this.  And at the end of 1999, I arranged everything to start exploring the space of all possible axiom systems, starting with the simplest. <br><br>  In a sense, any axiom system defines a set of constraints, for example, on p ¬∑ q.  It does not say what p ¬∑ q is, it only gives properties that p ¬∑ q must satisfy (for example, it can state that q ¬∑ p = p ¬∑ q).  Then the question is whether it is possible to deduce from these properties all the theorems of logic that are executed when p ¬∑ q is Nand [p, q]: neither more nor less. <br><br>  Something can be checked directly.  We can take the axiom system and see which forms p ¬∑ q satisfy the axioms, if p and q are, say, true and false.  If the axiom system is that q ¬∑ p = p ¬∑ q, then yes, p ¬∑ q can be Nand [p, q] - but not necessarily.  It may also be And [p, q] or Equal [p, q], or many more other options that do not satisfy the same levels as the NAND function in logic.  But by the time we reach the axiom system {((p ¬∑ p) ¬∑ q) ¬∑ (q ¬∑ p) = q}, we are reaching a state in which Nand [p, q] (and the equivalent Nor [p , q]) remain the only working models of p ¬∑ q - at least, if we assume that q and p have only two possible values. <br><br>  Is this then a system of axioms for logic?  Not.  Because it implies, for example, the existence of a variant, when p and q have three values, but there is no such thing in logic.  However, the fact that this system of axioms from one axiom comes close to what we need indicates that it is worth looking for a single axiom from which logic is reproduced.  This is exactly what I did in January 2000 (in our time, this task has become easier, thanks to the rather new and very convenient Wolfram Language, Groupings function). <br><br>  It was quite easy to verify that axioms in which there were 3 or fewer NANDs (or "operator points") did not work.  By 5 am on Sunday, January 29th (aha, then I was an owl), I found that the axioms containing 4 NANDs would not work either.  When I stopped work, at about 6 am, I had 14 candidates in my arms with five NANDs.  But having continued work on Sunday evening and having conducted additional tests, I had to drop them all. <br><br>  Needless to say, the next step was to test the axioms with 6 NAND.  They turned out to be 288 684. But my code worked efficiently, and not much time passed before the following appeared on the screen (yes, from Mathematica Version 4): <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cb1/eb6/b59/cb1eb6b59aa53f1cc55cc91c2ec76271.png"></div><br>  At first I did not understand what I did.  I only knew that I had 25 nonequivalent axioms with 6 NANDs, which were able to advance further than axioms with 5 NANDs.  But were there any axioms that gave rise to logic?  I had an empirical method capable of discarding unnecessary axioms.  But the only way to know exactly the correctness of a specific axiom was to prove that it is successfully able to reproduce, say, Schaeffer's axiom for logic. <br><br>  It took a little play with the programs, but after a few days I found that most of the 25 axioms received did not work.  As a result, two survived: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6f1/a4a/5e0/6f1a4a5e040e1acbd2e14406b8a05eff.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/311/20f/394/31120f394c33c0d1c95c0d22ee9caf45.png"></div><br>  And to my great joy, I managed to prove with the help of a computer that both are axioms for logic.  The technique used guaranteed the absence of simpler axioms for logic.  Therefore, I knew that I had come to a goal: after a century (and maybe a couple of millennia) of searches, we can finally declare that we have found the simplest axiom for logic. <br><br>  Soon after, I discovered systems of two axioms with 6 NANDs in general, which, as I proved, are capable of reproducing logic: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/7b9/311/d22/7b9311d22392f15be81d05b43df9861c.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/920/f41/3e1/920f413e173d5e9c859b580d27eb3819.png"></div><br>  And if we take the commutativity p ¬∑ q = q ¬∑ p as a matter of course, then the logic can be obtained from an axiom containing only 4 NANDs. <br><br><h2>  Why is it important </h2><br>  Well, let's say, it is very cool to be able to say that someone ‚Äúcompleted the work begun by Aristotle‚Äù (or at least by Boolem) and discovered the simplest possible system of axioms for logic.  Is this just a curiosity, or does this fact have important consequences? <br><br>  Before the platform developed by me in the book A New Kind of Science, I think it would be difficult to consider this fact to be more than just a wonder.  But now it should be clear that he is connected with all sorts of basic questions, such as whether mathematics should be considered a discovery or an invention. <br><br>  The kind of mathematics that people do is based on a handful of certain systems of axioms - each of which defines a particular area of ‚Äã‚Äãmathematics (logic, group theory, geometry, set theory).  But speaking in the abstract, there is an infinite number of systems of axioms - each of which defines the area of ‚Äã‚Äãmathematics that can be studied, even if people have not done it yet. <br><br>  Before the book A New Kind of Science, I apparently meant that everything that exists ‚Äúsomewhere there‚Äù in the computational universe should be ‚Äúless interesting‚Äù than what people had created and studied.  But my discoveries concerning simple programs indicate that in systems that are simply ‚Äúsomewhere out there‚Äù exist, no less rich possibilities are hidden than in systems carefully selected by people. <br><br>  So what about the axiom system for mathematics?  To compare the existing ‚Äúsomewhere out there‚Äù with what people have studied, you need to know whether the axiom systems for the existing areas of mathematics we have studied lie.  And, based on the traditional systems created by people, it can be concluded that they have to be somewhere very, very far away - and in general they can be found only if you already know where you are. <br><br>  But my discovery of the axiom system answered the question ‚ÄúHow far is the logic?‚Äù For such things as a cellular automaton, it is quite simple to number (as I did in the 1980s) all possible cellular automata.  It is a little harder to do this with axiom systems - but not much.  In one approach, my axiom can be designated as 411; 3; 7; 118 - or, in the Wolfram Language: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/315/5d5/ef0/3155d5ef0d85da9df2a25e407c617625.png"></div><br>  And, at least in the space of possible functional forms (not taking into account the marking of variables), there is a visual representation of the location of this axiom: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/bb2/e6c/eb2/bb2e6ceb2b5a36d51e32d16727e89ef9.png"></div><br>  Given the fundamental importance of logic for such a large number of formal systems that people study, one could think that in any reasonable representation, logic corresponds to one of the simplest possible axioms.  But at least in a presentation using NAND, this is not the case.  For it, there is still a very simple system of axioms, but it is likely to prove to be a hundred-thousandth system of axioms from all possible that will occur if you simply start to number the systems of axioms, starting with the simplest. <br><br>  Given this, the obvious next question will be: what about all the other systems of axioms?  How do they behave?  It is this question that explores the book A New Kind of Science.  And in it, I argue that things like systems observed in nature are often better described by these very ‚Äúother rules‚Äù that we can find, numbering possibilities. <br><br>  As for systems of axioms, I made a picture representing what is happening in the "areas of mathematics" corresponding to various systems of axioms.  The series shows the consequences of a certain system of axioms, and the squares denote the truth of a certain theorem in this system of axioms (yes, <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2593%25D1%2591%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258F_%25D0%25BE_%25D0%25BD%25D0%25B5%25D0%25BF%25D0%25BE%25D0%25BB%25D0%25BD%25D0%25BE%25D1%2582%25D0%25B5">G√∂del's theorem</a> takes effect at some point, after which it becomes incredibly difficult to prove or disprove a given theorem in a given system of axioms; in practice, by my methods this happens just to the right of what is shown in the picture). <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/627/56f/8e5/62756f8e5e47459b575ad148feacee99.png"></div><br>  Is there something fundamentally special in the fields of mathematics that have been ‚Äústudied by people‚Äù?  Judging by this and other pictures, nothing obvious comes to mind.  I suspect that there is only one peculiarity in these areas - the historical fact that they have been studied.  (You can make statements like the fact that "they describe the real world" or "related to how the brain works," but the results described in the book state the opposite). <br><br>  Well, well, what is the meaning of my axiom system for logic?  Its size makes it possible to feel the final content of logic as an axiomatic system.  And it makes us consider - at least for now - that we need to regard logic more as a ‚Äúconstruction invented by man‚Äù than as a ‚Äúdiscovery‚Äù that happened for ‚Äúnatural reasons‚Äù. <br><br>  If history went differently, and we would constantly search (as it is done in the book) for a set of possible simplest systems of axioms, then we might have ‚Äúdiscovered‚Äù a system of axioms for logic, like that system whose properties seem interesting to us.  But since we have studied such a small number of all possible axiom systems, I think it would be reasonable to consider logic as an ‚Äúinvention‚Äù ‚Äîa specially created construction. <br><br>  In a sense, in the Middle Ages, logic looked like this ‚Äî when possible syllogisms (permissible types of arguments) were represented as Latin mnemonics like bArbArA and cElErAnt.  Therefore, it is now interesting to find the mnemonic representation of what we know now as the simplest axiom system for logic. <br><br>  Starting with ((p ¬∑ q) ¬∑ r) ¬∑ (p ¬∑ ((p ¬∑ r) ¬∑ p)) = r, you can think of each p ¬∑ q as a prefix or Polish record (inverse of the ‚Äúreverse Polish record‚Äù of the HP calculator ) in the form of Dpq - therefore, the whole axiom can be written as = DDDpqrDpDDprpr.  There is also an English mnemonic on this subject - FIGure OuT Queue, where the roles of p, q, r are played by u, r, e.  Or you can look at the first letters of the words in the following sentence (where B is the operator, and p, q, r are a, p, c): ‚ÄúBit by bit, a program computed Boolean algebra's best binary axiom covering all cases‚Äù [ gradually computed using the program, the best binary axiom of Boolean algebra describes all cases]. <br><br><h2>  Proof mechanics </h2><br>  Okay, so how to prove the correctness of my axiom system?  The first thing that comes to mind is to show that it is possible to derive from it a known system of axioms for logic - for example, the system of Schaeffer's axioms: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b5f/54a/161/b5f54a161b3ff941c7459003cd9dffd9.png"></div><br>  There are three axioms here, and we need to derive each.  Here is what can be done to display the first, using the latest version of Wolfram Language: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/47b/f3e/10c/47bf3e10c7a79db3328816c3ff931a68.png"></div><br>  It is noteworthy that it has now become possible to do this.  In the ‚Äúobject of the proof‚Äù it is written that 54 steps were used for the proof.  Based on this object, we can generate a ‚Äúnotebook‚Äù describing each of the steps: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b2d/b12/0f3/b2db120f3a2bd50432ed16525ca72342.png"></div><br>  In general, the entire sequence of intermediate lemmas is proved here, which allows us to finally derive the final result.  Between lemmas, there is a whole network of mutual dependencies: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc1/272/d6f/dc1272d6fd14710be4e2a49ec684efa8.png"></div><br>  But the networks involved in the derivation of all three axioms in the system of Schaeffer's axioms - for the latter, an incredible 504 steps are used: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c33/0e5/d15/c330e5d159782e0d4cff4931f2b9f605.png"></div><br>  Yes, it is obvious that these networks are rather confused.  But before discussing what this complexity means, let's talk about what happens at each step of this evidence. <br><br>  The main idea is simple.  Imagine that we have an axiom, which is simply written as p ¬∑ q = q ¬∑ p (mathematically, this means that the operator is commutative).  More precisely, the axiom says that for any expressions p and q, p ¬∑ q is equivalent to q ¬∑ p. <br><br>  Well, suppose we want to deduce from this axiom that (a ¬∑ b) ¬∑ (c ¬∑ d) = (d ¬∑ c) ¬∑ (b ¬∑ a).  This can be done using the axiom to transform d ¬∑ c into c ¬∑ d, b ¬∑ a into a ¬∑ b, and finally, (c ¬∑ d) ¬∑ (a ¬∑ b) into (a ¬∑ b) ¬∑ (c ¬∑ d ). <br><br>  FindEquationalProof does essentially the same thing, although it does not follow these steps in exactly the same order, and changes both the left side of the equation and the right side. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9f4/f43/f6b/9f4f43f6b97bb816052eeb50ec01dcb2.png"></div><br>  Having obtained such evidence, you can simply track each step and verify that they give the stated result.  But how to find evidence?  There are many possible sequences of substitutions and transformations.  How to find a sequence that successfully leads to the final result? <br><br>  It would be possible to decide: why not try all possible sequences, and if there is a working one among them, should it end up?  The problem is that you can quickly come to an astronomical number of sequences.  The main part of the art of automatic proof of theorems consists of finding ways to reduce the number of sequences to check. <br><br>  It quickly slips into technical details, but the basic idea is easy to discuss if you know the basics of algebra.  Suppose we are trying to prove an algebraic result like <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fc0/402/9b6/fc04029b6fa134be318b5b9999d0beeb.png"></div><br>  There is a guaranteed way to do this: simply by applying the rules of algebra to reveal each of the parties, you can immediately see their similarities: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c93/6e3/5b0/c936e35b0bf0addb94eece9e2379f15f.png"></div><br>  Why does this work?  Because there is a way to work with such expressions, systematically reducing them until they take the standard form.  Is it possible to do the same operation in arbitrary axiom systems? <br><br>  Not right away.  In algebra, this works because it has a special property that ensures that you can always ‚Äúmove forward‚Äù along the path to simplify expressions.  But in the 1970s, various scientists discovered independently several times (under names like the Knut-Bendix algorithm or <a href="https://ru.wikipedia.org/wiki/%25D0%2591%25D0%25B0%25D0%25B7%25D0%25B8%25D1%2581_%25D0%2593%25D1%2580%25D1%2591%25D0%25B1%25D0%25BD%25D0%25B5%25D1%2580%25D0%25B0">Gr√∂bner basis</a> ) that even if the axiom system does not have the necessary intrinsic properties, it is potentially possible to find "additions" to this system in which there is. <br><br>  This is what happens in the typical evidence that FindEquationalProof produces (based on the Waldmeister system, ‚Äúmaster of trees‚Äù).  There are so-called.  ‚ÄúLemmas of Critical Pairs‚Äù that do not directly advance proof, but make possible the emergence of paths capable of this.  Complicated by the fact that, although the final expression we want to get is rather short, on the way to it, you may have to go through a much longer intermediate expression.  Therefore, for example, the proof of the first Schaeffer axiom has such intermediate steps: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b0f/c98/287/b0fc98287cfb9bb8968eea6664aaa0ec.png"></div><br>  In this case, the largest of the steps is 4 times more than the original axiom: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cdd/885/774/cdd885774255091503fdb07d55fd8b0b.png"></div><br>  Such expressions can be represented as a tree.  Here is its tree, in comparison with the tree of the original axiom: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a7f/465/f16/a7f465f1661555adf78695f71ea4fb8c.png"></div><br>  And here is how the sizes of intermediate steps develop in the course of the proofs of each of Schaeffer's axiom: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5d2/ba7/c59/5d2ba7c5933f2d3d77b0b8acfb594020.png"></div><br><h2>  Why is it so hard? </h2><br>  Is it surprising that this evidence is so complex?  Not really.  After all, we are well aware that mathematics can be difficult.  In principle, it could be that all the truths in mathematics would be easy to prove.  But one of the side effects of the 1931 G√∂del theorem is that even for those things that have evidence, the path to them can be arbitrarily long. <br><br>  This is a symptom of a much more general phenomenon, which I call <a href="https://www.wolframscience.com/nks/chap-12--the-principle-of-computational-equivalence/">computational irreducibility</a> .  Consider a system governed by a simple cellular automaton rule (of course, in any of my essays somewhere there will necessarily be cellular automata).  Run this system. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/511/f6e/5b2/511f6e5b21afb7a21c62622cab036fd4.png"></div><br>  One could decide that if the system is based on a simple rule, then there should be a quick way to understand what the system is doing.  But it is not.  According to my <a href="http://www.wolframscience.com/nks/p715--basic-framework/">principle of computational equivalence</a> , the work of the system often corresponds to calculations, the complexity of which coincides with any calculations that we could carry out in order to understand the behavior of the system.  ,    ,  ,     ,     . <br><br>   : ,   ,     .      ,    ,      -,    .         ,     ,  ,  . <br><br>  -     ,    -.  ,   ,    ,  ,       ,    ,      .   ,    ,   .    ‚Äì      ,      ,          ,   . <br><br>  ,       ‚Äì      ,    ,      ‚Äì   ,    ,   ,        ,      . <br><br>           ,       ‚Äì  ,         . <br><br>     ? ,     ,    .         (  -      ).     ,              ‚Äì     . <br><br><h2>   ? </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In a sense, proof is needed to know the truth of something. Of course, especially in our time, the proof faded into the background, giving way to pure computation. In practice, the desire to generate something by calculations is much more common than the desire to ‚Äústep back‚Äù and construct a proof of the truth of something. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In pure mathematics, however, one often encounters concepts that include, at least nominally, an infinite number of cases (‚Äútrue for all primes,‚Äù etc.), for which head-on calculations are not suitable. . And when the question of confirmation arises (‚Äúcan this program end with an error?‚Äù Or ‚Äúcan this cryptocurrency be spent twice?‚Äù) It is more reasonable to try to prove this than to calculate all possible cases.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But in real mathematical practice, proof is more than finding out the truth. When Euclid wrote his " </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%259D%25D0%25B0%25D1%2587%25D0%25B0%25D0%25BB%25D0%25B0_(%25D0%2595%25D0%25B2%25D0%25BA%25D0%25BB%25D0%25B8%25D0%25B4)"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Beginnings</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ", he simply pointed out the results, and the evidence "left to the reader." But, one way or another, especially in the last century, evidence has turned into something that does not just happen behind the scenes, but is the main carrier through which it is necessary to translate concepts. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">It seems to me that as a result of some fad history, evidence is being offered today as a subject that people have to understand, and programs are considered just something that a computer has to do.</font></font> Why did it happen so?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, at least in the past, the evidence could be presented in text form - therefore, if anyone used them, then only people. A program is almost always recorded in the form of a computer language. And for a very long time these languages ‚Äã‚Äãwere created so that they could be more or less directly translated into low-level computer operations ‚Äî that is, the computer understood them right away, but people did not necessarily. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But one of the main goals of my activity over the past few decades was to change this state of affairs, and to develop in Wolfram Language a real ‚Äúlanguage of computational communication‚Äù in which computational ideas can be transmitted so that both computers and people can understand them.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Such a language has many consequences. One of them is the changing role of evidence. Suppose we are looking at some mathematical result. In the past, the only plausible way to bring it to the point of understanding was to produce evidence readable by people. But now another is possible: you can issue a program for the Wolfram Language, which counts the result. And this is in many ways a much richer way to convey the truth of the result. Each part of the program is something precise and unambiguous - anyone can start it. There is no such problem as trying to understand some part of the text that requires filling in certain spaces. Everything is stated in the text quite clearly.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What about evidence? Are there clear and precise ways to write evidence? Potentially yes, although it is not particularly easy. And although the foundation of the Wolfram Language has existed for 30 years, only today there has emerged a reasonable way to present with its help such structurally straightforward proofs as one of my axioms above. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">You can imagine the creation of evidence in Wolfram Language just as people create programs ‚Äî and we are working to provide high-level versions of such ‚Äúhelp with evidence‚Äù functionality. However, nobody created the proof of my axiom system - the computer found it. And this is more like the output of the program than the program itself. However, like the program, the proof in some sense can also be ‚Äúrun‚Äù to verify the result.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Creating clarity </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Most of the time, people using Wolfram Language, or Wolfram | Alpha, want to calculate something. </font><font style="vertical-align: inherit;">They need to get a result, and not understand why they got exactly such results. </font><font style="vertical-align: inherit;">But in Wolfram | Alpha, especially in such areas as mathematics and chemistry, a popular feature among students is the construction of ‚Äústep-by-step‚Äù solutions.</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/77d/a19/015/77da190153b5c78394653c8bdee8bb98.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">When the Wolfram | Alpha system calculates, for example, the integral, it uses all sorts of powerful algorithmic techniques that are optimized for getting answers. </font><font style="vertical-align: inherit;">But when she is asked to show the stages of calculations, she does something else: she needs to explain step by step why exactly this result is obtained.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There would be no use in explaining how the result was actually obtained; This is a very inappropriate process for humans. She needs to understand which operations people have learned can be used to get results. Often she comes up with some useful trick. Yes, she has a systematic way for this that always works. But there are too many ‚Äúmechanical‚Äù stages in it. A "trick" (substitution, partial integration, etc.) does not work in the general case, but in this particular case it will give a faster way to the answer. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And what about getting clear versions of other things? For example, work programs in the general case. Or evidence of my axiom system.</font></font><br><br>   . ,      ,   .     ‚Äì    .        ,     .    Wolfram Language       ,       ,      . <br><br>     ,    Wolfram Language    ,    ,          .        Wolfram Language,    ,   ,   . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, it often happens that the actual execution of the code leads to such things that are not obvious from the program. I will soon mention such extreme cases as cellular automata. But for now, let's imagine that we have created a program that allows us to imagine what it does in general. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In this case, I found that computational essays presented in the form of Wolfram Notebooks are an excellent tool for explaining what is happening. It is important that the Wolfram Language, it allows you to run even the smallest parts of the programs separately (with the corresponding symbolic expressions in the role of input and output data). After that, you can imagine the sequence of program steps as a sequence of elements of a dialogue that forms the basis of a computational notebook.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In practice, it is often necessary to create visualizations of input and output data. Yes, everything can be expressed in the form of a single symbolic representation. But it is much easier for people to understand the visual representation of things than any one-dimensional language-like line. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, creating good visualizations is akin to art. But at Wolfram Language we did a great job of automating this art - often with the help of quite complex machine learning and other algorithms that perform such things as the layout of networks or graphic elements.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What about starting with a simple program tracking? It is hard to do. I have experimented with this for decades, and have never been completely satisfied with the results. Yes, you can zoom in and see many details of what is happening. But I didn‚Äôt find enough good technicians to understand the whole picture, and automatically give out some particularly useful things. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">At some level, this task is similar to reverse engineering. You are shown the machine code, chip circuit, whatever. And you need to take a step back and recreate a high-level description that repelled a person who somehow ‚Äúcompiled‚Äù into what you see.</font></font><br><br>      ,      ,      ,   ,      .              (,      ,     ),   ,      - ¬´ ¬ª  . <br><br>      .  ,         .     ¬´-¬ª    ¬´¬ª?   ,        .        ,     .   ,      ,     . <br><br> ,          ,    .     ,  ,          ,   . <br><br><h2>   </h2><br>    ¬´  ¬ª?    ,     . <br><br>       ,  - ?   -   .       .       ,       .           . <br><br>   ,      ¬´¬ª ,    .                . ,  ,      ¬´ ¬ª   ,      . <br><br>      ? , -  .    ,   ,   ,        . <br><br>  How it works?        ,      .      ,           ,       ,   . ,          :      ,    ,    . <br><br>      ¬´¬ª   ,     ,     ,       . <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b79/395/b3b/b79395b3bad1a1a63252ad645d54cc4b.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The cellular automaton case has one simplifying feature: because it works on the basis of simple deterministic rules, it has equally repeating structures. In nature, for example, we usually do not encounter such identical repetition. There's just one, say, a tiger, very similar to the other, so we call them both "tiger", although the arrangement of their atoms is not identical. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What is the general meaning of all this? It consists in using the idea of ‚Äã‚Äãsymbolic representation. We say that we can assign a certain symbol - often this word - which can be used to symbolically describe a whole class of things, without listing in detail all the components of these things every time.</font></font><br><br>     :    ,         . <br><br> ,    , , : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/27d/923/5ac/27d9235ac9fd46bd22ef8c526e0afbc6.png"></div><br>   ‚Äì      .  ,      .      .    ¬´¬ª     : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ba2/102/c89/ba2102c898a14c573ddd207694fd5696.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Yes, this, similar to the ‚Äúdictionary compression‚Äù scheme, is useful for reaching the first level of explicability. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But let's go back to the proof of my axiom system. </font><font style="vertical-align: inherit;">The lemmas created in this proof are specially chosen as reusable elements. </font><font style="vertical-align: inherit;">However, by excluding them, we are still left with evidence that people cannot understand right away. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What other step can you take? </font><font style="vertical-align: inherit;">We need to come up with some description of a higher level.</font></font> What could it be? <br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Concept concepts </font></font></h2><br>  If you are trying to explain something to someone, then it will be much easier to do if you find something else, but similar, that the person has already been able to understand.  Imagine how you will explain the concept of the modern drone to a person from the Stone Age.  It will be difficult to do.  But it will be much easier to explain this to a man who lived 50 years ago, and who has already seen helicopters and model airplanes. <br><br>  And in the end, the bottom line is that when we explain something, we do it in a language known to us as well as to the one to whom we explain it.  And the richer the language, the less new elements we have to introduce in order to convey what we are trying to explain. <br><br>  There is a pattern that repeats throughout the history of the mind.  A certain set of things notice many times.  Gradually they begin to understand that these things are somehow abstractly similar, and they can all be described in terms of a new concept, which is described by some new word or phrase. <br><br>  Let's say we noticed things like water, blood, and oil.  At some point, we understand that there is a generalized concept of "fluid", and all of them can be described as liquid.  And when we have such a concept, we can begin to argue in its terms, finding more concepts ‚Äî say, viscosity ‚Äî based on it. <br><br>  When does it make sense to combine things into a concept?  A difficult question that cannot be answered without having foreseen all that can be done with this concept.  In practice, in the process of evolution of the language and ideas of man, a certain process of consistent approximation is observed. <br><br>  In the modern system of machine learning occurs much like a faster summation of information.  Imagine that we took all sorts of objects around the world, and fed them <a href="http://reference.wolfram.com/language/ref/FeatureSpacePlot.html">FeatureSpacePlot</a> functions to see what happens.  If we get certain clusters in the space of singularities, then we can conclude that each of them can be defined as corresponding to some kind of ‚Äúconcept‚Äù, which can be, for example, marked with a word. <br><br>  Honestly, what happens with FeatureSpacePlot - as in the process of human intellectual development - is in a sense a step-by-step process.  To distribute features across feature spaces, FeatureSpacePlot uses features that it learned to extract based on previous categorization attempts. <br><br>  Well, if we accept the world, what it is, what best categories ‚Äî or concepts ‚Äî can we use to describe things?  This issue is constantly evolving.  And in general, all breakthroughs - be it science, technology or something else - are often accurately associated with an awareness of the ability to usefully identify a new category or concept. <br><br>  But in the process of evolution of our civilization there is a certain spiral.  First, some definite concept is defined - for example, the idea of ‚Äã‚Äãa program.  After that, people begin to use it and reflect on it in terms.  Pretty soon, on the basis of this concept, many different concepts are constructed.  And then another level of abstraction is defined, new concepts are created, based on the previous one. <br><br>  History is characteristic of the technological set of knowledge of modern civilization, and its intellectual set of knowledge.  Both there and there are towers of concepts and levels of abstraction one after another. <br><br><h2>  Learning problem </h2><br>  So that people can communicate using a certain concept, they need to learn about it.  And, yes, some concepts (such as the constancy of objects) are automatically realized by people, just by observing nature.  But, let's say, if you look at the list of common words of modern English, it becomes clear that most of the concepts used by our modern civilization do not apply to those that people are aware of independently, observing nature. <br><br>  Instead, which is very much like modern machine learning, they need a special knowledge of the world "under supervision", organized in order to emphasize the importance of certain concepts.  And in more abstract areas (such as mathematics), they probably need to face concepts in their immediate abstract form. <br><br>  Well, will we need to find out more and more all the time as the amount of accumulated intellectual knowledge of civilization increases?  There may be anxiety that at some point our brain simply will not be able to keep up with the development, and we will have to add some extra help.  But it seems to me that this is, fortunately, one of those cases where the problem can be solved "at the software level." <br><br>  The problem is this: at any time in history there is a certain set of concepts that are important for life in the world during this period.  And, yes, with the development of civilization, new concepts are revealed and new concepts are introduced.  However, there is another process: new concepts introduce new levels of abstraction, usually including a large number of earlier concepts. <br><br>  We can often observe this in technology.  There was a time when working on a computer needed to know a lot of low-level details.  But over time, these details are abstracted, so now you need only a general concept.  You click on the icon and the process starts - you do not need to understand the subtleties of operating systems, interrupt handlers, schedulers, and all other details. <br><br>  And, of course, the Wolfram Language provides an excellent example of such work.  It makes a lot of effort to automate a lot of low-level details (for example, which of the algorithms should be used) and allows users to think with high-level concepts. <br><br>  Yes, people are still needed who understand the details that underlie the abstractions (although I‚Äôm not sure how many stone mites modern society needs).  But for the most part, education can concentrate on a high level of knowledge. <br><br>  It is often assumed that in order to achieve high-level concepts in the process of teaching a person, you first need to somehow summarize the history of how they came to these concepts historically.  But usually - and, perhaps, always - it seems to be wrong.  We can give an extreme example: imagine that in order to learn how to work with a computer, you first need to go through the entire history of mathematical logic.  However, in reality, it is known that people immediately turn to modern concepts of computing, without having to learn any history. <br><br>  But how does the network's conceptual clarity look like?  Are there concepts that can be understood only by understanding other concepts?  Given the training of people on the basis of interaction with the environment (or the training of a neural network), it is likely that there may be some order of them. <br><br>  But it seems to me that a certain principle, similar to the universality of calculations, suggests that having a ‚Äúclean brain‚Äù in your hands, you can start from anywhere.  So, if some newcomers would learn about category theory and almost nothing else, they would no doubt have built a network of concepts where this theory is at its root, and what we know as the basics of arithmetic would be studied somewhere in the analogue of our institute. <br><br>  Of course, such aliens could build their own set of technologies and their environment in a very different way from us - just like our history could have become completely different if computers could be successfully developed in the 19th century, and not in the middle of the 20th. <br><br><h2>  Math progress </h2><br>  I often thought about the extent to which the historical trajectory of mathematics is subject to the role of chance, and to what degree it was inevitable.  As I already mentioned, at the level of formal systems there are many possible systems of axioms on which something can be constructed that is formally reminiscent of mathematics. <br><br>  But the real history of mathematics did not begin with an arbitrary system of axioms.  At the time of the Babylonians, it began with attempts to use arithmetic for commerce and for geometry with the goal of developing the land.  And from these practical roots began to add further levels of abstraction, which eventually led to modern mathematics - for example, the numbers gradually generalized from positive integers to rational, then to roots, then to all integers, to decimal fractions, to complex numbers, to <a href="https://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25B5%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25BE%25D0%25B5_%25D1%2587%25D0%25B8%25D1%2581%25D0%25BB%25D0%25BE">algebraic numbers</a> , <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25B2%25D0%25B0%25D1%2582%25D0%25B5%25D1%2580%25D0%25BD%25D0%25B8%25D0%25BE%25D0%25BD">quaternions</a> , and so on. <br><br>  Is there an inevitability of such a development of abstractions?  I suspect that to some extent, yes.  Perhaps the same is true of other types of concept formation.  Having reached a certain level, you get the opportunity to study various things, and over time, the groups of these things become examples of more general and abstract constructions - which in turn define a new level, starting from which you can learn something new. <br><br>  Are there ways to break out of this cycle?  One of the possibilities may be related to mathematical experiments.  You can systematically prove things related to certain mathematical systems.  But you can empirically just notice mathematical facts - as <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D1%2580%25D0%25B8%25D0%25BD%25D0%25B8%25D0%25B2%25D0%25B0%25D1%2581%25D0%25B0_%25D0%25A0%25D0%25B0%25D0%25BC%25D0%25B0%25D0%25BD%25D1%2583%25D0%25B4%25D0%25B6%25D0%25B0%25D0%25BD_%25D0%2590%25D0%25B9%25D0%25B5%25D0%25BD%25D0%25B3%25D0%25BE%25D1%2580">Ramanujan</a> once remarked that <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msup><mi>e</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mtext>&amp;#xA0;</mtext><mi>p</mi><mi>i</mi><mtext>&amp;#xA0;</mtext><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mn>163</mn></mrow></mrow></msup></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="9.2ex" height="2.419ex" viewBox="0 -935.7 3961 1041.5" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-65" x="0" y="0"></use><g transform="translate(466,362)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-70" x="353" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-69" x="857" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-73" x="1556" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-71" x="2025" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-72" x="2486" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMATHI-74" x="2937" y="0"></use><g transform="translate(2332,0)"><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMAIN-31"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMAIN-36" x="500" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/431956/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi2tvt0700SUyAoqKclCwUEseGrmg#MJMAIN-33" x="1001" y="0"></use></g></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msup><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mtext>&nbsp;</mtext><mi>p</mi><mi>i</mi><mtext>&nbsp;</mtext><mi>s</mi><mi>q</mi><mi>r</mi><mi>t</mi><mrow class="MJX-TeXAtom-ORD"><mn>163</mn></mrow></mrow></msup></math></span></span><script type="math/tex" id="MathJax-Element-1"> e ^ {\ pi \ sqrt {163}} </script>  very close to the whole number.  The question is: are such things "random mathematical facts" or are they somehow embedded in the "fabric of mathematics"? <br><br>  The same question can be asked about the questions of mathematics.  Is the question of the existence of odd <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BE%25D0%25B2%25D0%25B5%25D1%2580%25D1%2588%25D0%25B5%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B5_%25D1%2587%25D0%25B8%25D1%2581%25D0%25BB%25D0%25BE">perfect numbers</a> (the answer to which has not been found since the days of Pythagoras) the key question for mathematics, or is it in some sense a random question not related to the fabric of mathematics? <br><br>  In the same way as it is possible to number systems of axioms, it is possible to present numbering of possible questions in mathematics.  But this immediately causes a problem.  Godel's theorem asserts that in such axiom systems as the one that relates to arithmetic, there are ‚Äúformally insoluble‚Äù theorems that cannot be proved or disproved within the framework of this axiom system. <br><br>  However, the specific examples created by G√∂del seemed very far from what could really arise in the course of studying mathematics.  And for a long time it was believed that in some way the phenomenon of insolvability was something that, in principle, exists, but it will not have a relation to ‚Äúreal mathematics‚Äù. <br><br>  However, according to my principle of computational equivalence and my experience in the computational universe, I am almost sure that this is not the case - and that, in fact, undecidability is very close even in typical mathematics.  I would not be surprised if the tangible part of the unsolved problems of mathematics today (the <a href="https://ru.wikipedia.org/wiki/%25D0%2593%25D0%25B8%25D0%25BF%25D0%25BE%25D1%2582%25D0%25B5%25D0%25B7%25D0%25B0_%25D0%25A0%25D0%25B8%25D0%25BC%25D0%25B0%25D0%25BD%25D0%25B0">Riemann hypothesis</a> , P = NP, etc.) prove to be insoluble. <br><br>  But if around is completely undecidable, how is it that so much of everything in mathematics is successfully resolved?  I think this is because successfully solved problems were specially chosen to avoid insolvability, simply because of how the development of mathematics is structured.  Because if we, in fact, form consistent levels of abstraction, based on the concepts that we have proved, then we pave the way able to move forward without turning into undecidability. <br><br>  Of course, experimental mathematics or ‚Äúrandom questions‚Äù can immediately lead us to a field full of undecidability.  But, at least for now, the basic discipline of mathematics has evolved wrong. <br><br>  And what about these "random facts of mathematics"?  Yes, the same as with other areas of intellectual research.  ‚ÄúRandom facts‚Äù are not included in the path of intellectual development until a structure ‚Äî usually some abstract concepts ‚Äî is built around them. <br><br>  A good example is my favorite discoveries of the origin of complexity in such systems, usually 30 cellular automata.  Yes, similar phenomena have already been observed even thousands of years ago (for example, randomness in a sequence of prime numbers).  But without a broader conceptual platform, few people paid attention to them. <br><br>  Another example is nested sequences (fractals).  There are some examples of how they met in the mosaic of the XIII century, but no one paid attention to them until the whole platform appeared around the fractals in the 1980s. <br><br>  The same story repeats over and over: until abstract concepts are defined, it is difficult to talk about new concepts, even when faced with the phenomenon that demonstrates them. <br><br>  I suspect that it is the same in mathematics: there is a certain inevitable overlap of some abstract concepts on top of others, defining the trajectory of mathematics.  Is this path unique?  No doubt not.  In the vast space of possible mathematical facts, there are certain directions that are chosen for further constructions.  But you could choose others. <br><br>  Does this mean that themes in mathematics are inevitably given by historical accidents?  Not as much as one might think.  Indeed, as mathematics has been discovered again and again, starting with such things as algebra and geometry, there is a remarkable tendency in which different directions and different approaches lead to equivalent or corresponding to each other results. <br><br>  Perhaps, to some extent, this is a consequence of the principle of computational equivalence, and the phenomena of computational universality: although the basic rules (or ‚Äúlanguage‚Äù) used in different areas of mathematics are different, the translation method between them appears as a result - and at the next level of abstraction The path chosen will no longer be so critical. <br><br><h2>  Logical proof and automation of abstraction </h2><br>  Let's return to logical proofs.  How are they related to typical math?  So far, nothing.  Yes, the proof has nominally the same form as the standard mathematical proof.  But it is not "friendly to people-mathematicians."  These are only mechanical parts.  It is not related to high-level abstract concepts that will be understood by the person-math. <br><br>  It would help us a lot if we found out that non-trivial proof lemmas already appeared in the mathematical literature (I don‚Äôt think it was so, but our ability to search by theorems has not yet reached such a level that we can be sure).  But if they do, it will probably give us a way to relate these lemmas to other things in mathematics, and define their circle of abstract concepts. <br><br>  But how to make the proof explicable without it? <br><br>  Perhaps there is some other way to carry out the proof, which is fundamentally stronger connected with the existing mathematics.  But even with the evidence that we have now, one can imagine the ‚Äútweaking‚Äù of new concepts that would define a higher level of abstraction and place this evidence in a more general context. <br><br>  Not sure how to do this.  I was considering the possibility of nominating a premium (something like my <a href="https://www.wolframscience.com/prizes/tm23/">2007 award for a Turing machine</a> ) for ‚Äútransforming the proof into an explicable form.‚Äù  However, it is completely incomprehensible how to assess the ‚Äúexplanability‚Äù.  One could ask to record an hour-long video in which a successful explanation of the proof suitable for a typical mathematician would be given - but this would be very subjective. <br><br>  But just as it is possible to automate the search for beautiful network layouts, it is possible that we can also automate the process of turning evidence into explicable.  The current proof, in fact, without explanation, suggests to consider several hundred lemmas.  But suppose we could define a small number of ‚Äúinteresting‚Äù lemmas.  Perhaps we could somehow add them to our canon of famous mathematics, and then we would be able to use them to understand the proof. <br><br>  There is an analogy with the development of languages.  Creating the Wolfram Language, I try to identify the ‚Äúpieces of computational work‚Äù that people often need.  We create from them functions built into the language, with specific names that people can use to refer to them. <br><br>  A similar process is going on - although not at all in such an organized way - in the evolution of natural languages.  ‚ÄúPieces of meaning‚Äù, which prove to be useful, end up with their words in the language.  Sometimes they begin with phrases consisting of several existing words.  But the most influential usually find themselves so far from existing words that they appear in the form of new words, which are potentially quite difficult to define. <br><br>  When developing the Wolfram Language, whose functions are called with the help of English words, I rely on the general human understanding of the English language (and sometimes on the word comprehension used in common computer applications). <br><br>  It would be necessary to do something similar in determining which lemmas to add to the mathematical canon.  It would be necessary not only to make sure that each lemma would somehow be ‚Äúinteresting in essence‚Äù, but also if possible to choose lemmas that are ‚Äúeasy to deduce‚Äù from the existing mathematical concepts and results. <br><br>  But what makes the lemma "interesting in its essence"?  I must say that before I worked on my book, I accused of choosing lemmas (or cups) in any area of ‚Äã‚Äãmathematics that is described and named in textbooks, a great deal of arbitrariness and historical accidents. <br><br>  But, having analyzed the theorems from the base logic in detail, I was surprised to find something completely different.  Suppose that we have built all the correct theorems of logic in the order of their size <br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">(for example, p = p will be the first, p AND p = p - a little later, etc.). This list has quite a lot of redundancy. Most theorems turn out to be a trivial extension of theorems that have already appeared in the list. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But sometimes a theorem appears, producing new information that cannot be proved on the basis of theorems that have already appeared in the list. A remarkable fact: </font></font><a href="https://www.wolframscience.com/nks/p817--implications-for-mathematics-and-its-foundations/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">there are 14 such theorems</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> , and they, in fact, correspond exactly to the theorems that are usually given names in logic textbooks (here AND is ‚àß, OR is ‚à®, and NOT is ¬¨.)</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/a00/078/057/a0007805787992eee313fa748deba866.png"></div><br>  ,    ,  ¬´¬ª    ,        . ,     -      ,     ,   ,   ,   ‚Äì     ,     . <br><br>    NAND, , ,    ? -,       ,  ,           : <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/3a8/4b3/022/3a84b3022c9b5408ce524671b006c814.png"></div><br>  NAND    ,   AND, OR  NOT. ,   ,    ,    NAND   .     NAND        NAND.      ,    : a = (a ¬∑ a) ¬∑ (a ¬∑ a)    , a = (a ¬∑ a) ¬∑ (a ¬∑ b)   <a href="https://studfiles.net/preview/3545493/page:3/"> </a> , (a ¬∑ a) ¬∑ b = (a ¬∑ b) ¬∑ b   ¬´¬ª,   . <br><br> ,      ¬´ ¬ª  NAND,     ? ,    ¬´ ¬ª  . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Of course, any theorem can have many possible proofs. But, let's say, we will use only the evidence that FindEquationalProof produces. Then it turns out that in the proof of the first thousand NAND theorems the most popular lemma is a ¬∑ a = a ¬∑ ((a ¬∑ a) ¬∑ a), followed by lemmas of the type (a ¬∑ ((a ¬∑ a) ¬∑ a)) ¬∑ ( a ¬∑ (a ¬∑ ((a ¬∑ a) ¬∑ a))) = a. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What are these lemmas? They are useful for the methods used by FindEquationalProof. But for people, they seem to be not very suitable. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What about lemmas that turn out to be short? a ¬∑ b = b ¬∑ a is definitely not the most popular, but the shortest. (a ¬∑ a) ¬∑ (a ¬∑ a) = a is more popular, but longer. There are also such lemmas as (a ¬∑ a) ¬∑ (b ¬∑ a) = a.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">How useful will these lemmas be? Here is one way to test this. Let's look at the first thousand NAND theorems and estimate how much the addition of lemmas shortens the proofs of these theorems (at least those found by the FindEquationalProof):</font></font><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/549/2f6/5d5/5492f65d5a0b28d9b2ad56e910d4a673.png"></div><br> a ¬∑ b = b ¬∑ a  ,       100 . (a ¬∑ a) ¬∑ (a ¬∑ a) = a   ;    ¬´  ¬ª FindEquationalProof,    ,   (   ). (a ¬∑ a) ¬∑ (b ¬∑ a) = a    ,    ,  a ¬∑ b = b ¬∑ a. ,     a ¬∑ b = b ¬∑ a,     . <br><br>   , ,   ,            .    ,     ¬´ ¬ª  a ¬∑ b = b ¬∑ a  (a ¬∑ a) ¬∑ (b ¬∑ a) = a,      ‚Äì  ,  ,   ¬´¬ª. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/5c5/410/c0e/5c5410c0e4690236d76c45f5e32e53f3.png"></div><br><h2>    ? </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There are different ways of modeling things. For several hundred years, the exact science dominated the idea of ‚Äã‚Äãfinding mathematical equations that could be solved to show how the system behaves. But since the appearance of my book, there has been an active shift in creating programs that can be run to see how the systems behave. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Sometimes such programs are written for a specific task; sometimes they are long sought. And nowadays, at least one class of such programs is derived using machine learning, using the method of reverse motion from well-known examples of system behavior.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And how easy is it to ‚Äúunderstand what is happening‚Äù with the help of these various types of modeling? Finding the "exact solution" of mathematical equations is a big plus - then the behavior of the system can be described by an exact mathematical formula. But even when this is not, it is often possible to write down some mathematical statements that are sufficiently abstract so that they can be associated with other systems and behaviors. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As I wrote above, with the program - such as a cellular automaton - everything can be different. Quite often it happens that we are immediately confronted with computational irreducibility, which limits our ability to go through a short way and "explain" what is happening.</font></font><br><br>       ?  -         ,    .  ,   ,    .       ? <br><br>       .    ,    ,    ,     ,  . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Instead of using a neural network to model the behavior of a system, let's consider the creation of a neural network that classifies a certain aspect of the world: for example, taking an image and distributing them by content (‚Äúboat‚Äù, ‚Äúgiraffe‚Äù, etc.). When we train a neural network, it learns to produce the correct output. But you can potentially imagine this process as an internal construction of a sequence of differences (something like a game of ‚Äú </font></font><a href="https://ru.wikipedia.org/wiki/20Q"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">twenty questions</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ‚Äù), which ultimately determines the correct conclusion.</font></font><br><br>     ?     . , ¬´     ?¬ª        ,    . ,     ,        .          . <br><br>      , , ,    ,     .         .     ,   ,      .      ,      . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Again, if we found that some specific differences are often found in neural networks, we could decide that they deserve to be explored for us and add them to the standard canon of ways to describe the world. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Can we expect that a small amount of such differences will allow us to create something meaningful? It is like asking whether a small number of theorems will help us to understand such a thing as logical proof. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">I think the answer is not clear. If you study, for example, a large collection of scientific papers in mathematics, you can ask questions about the frequency of use of various theorems. It turns out that the frequency of the theorems almost perfectly corresponds </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%2597%25D0%25B0%25D0%25BA%25D0%25BE%25D0%25BD_%25D0%25A6%25D0%25B8%25D0%25BF%25D1%2584%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to Zipf‚Äôs law</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> (and the </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A6%25D0%25B5%25D0%25BD%25D1%2582%25D1%2580%25D0%25B0%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25B4%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2582%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">central limit theorem</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,</font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25BE_%25D0%25BD%25D0%25B5%25D1%258F%25D0%25B2%25D0%25BD%25D0%25BE%25D0%25B9_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D0%25B8"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">implicit function </font></font></a><font style="vertical-align: inherit;"></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A2%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25BB%25D0%25B8_%25E2%2580%2594_%25D0%25A4%25D1%2583%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25B8"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">theorem</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> and </font><a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D0%25B5%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%25A2%25D0%25BE%25D0%25BD%25D0%25B5%25D0%25BB%25D0%25BB%25D0%25B8_%25E2%2580%2594_%25D0%25A4%25D1%2583%25D0%25B1%25D0%25B8%25D0%25BD%25D0%25B8"><font style="vertical-align: inherit;">Tonelli-Fubini theorem</font></a><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">The same thing probably happens with differences that ‚Äúshould know‚Äù, or new theorems that ‚Äúshould know‚Äù. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Knowledge of several theorems will enable us to go far enough, but there will always be an endless exponential tail, and it will not be possible to get to the end.</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Future knowledge </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Studying mathematics, science or technology, one can see similar basic ways of qualitative development, consisting in building a set of ever-increasing abstractions. </font><font style="vertical-align: inherit;">It would be nice to quantify this process. </font><font style="vertical-align: inherit;">Perhaps you can count how certain terms or descriptions that are often encountered at one time are included in higher levels of abstraction, which in turn have new terms or descriptions associated with them.</font></font><br><br> ,              . ,           .  ,             .        ,  ,   ¬´ ¬ª                  . <br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">One could decide that computational irreducibility would lead to the fact that creating this higher level computation model would inevitably be more complicated. But the key point is that we are only trying to reproduce the joint behavior of programs, and not their separate behavior. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But what happens if this process is repeated over and over again, reproducing the idealized intellectual history of man and creating a growing tower of abstractions? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Presumably, an analogy can be drawn here with critical phenomena in physics and the </font></font><a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B5%25D0%25BD%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25B3%25D1%2580%25D1%2583%25D0%25BF%25D0%25BF%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">renormalization group</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> method </font><font style="vertical-align: inherit;">. If so, you can imagine that we can determine the trajectory in the platform space for the presentation of concepts. What will this trajectory do?</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Perhaps it will have a fixed value, when at any point in the story there is approximately the same number of concepts worth studying - new concepts are slowly opening up and old ones are being absorbed. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What can this mean for mathematics? </font><font style="vertical-align: inherit;">For example, that any ‚Äúrandom mathematical facts‚Äù, discovered empirically, will eventually be considered upon reaching a certain level of abstraction. </font><font style="vertical-align: inherit;">There is no clear understanding of how this process will work. </font><font style="vertical-align: inherit;">After all, at any level of abstraction there are always new empirical facts to which we must ‚Äújump over‚Äù. </font><font style="vertical-align: inherit;">It may also happen that the "raising the level of abstraction" will move more slowly than is necessary to perform these "jumps".</font></font><br><br><h2><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Future understanding </font></font></h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What does all this mean for the future understanding? </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the past, when people studied nature, they had a small number of reasons for understanding it. Sometimes they personify certain aspects of it in the form of spirits or deities. But they took it for what it is, without thinking about the possibility of understanding all the details of the causes of the processes. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">With the advent of modern science ‚Äî and especially when an increasing majority of our lives are spent in artificial environments dominated by technologies developed by us ‚Äî these expectations have changed. And when we study the calculations performed by AI, we do not like that we can not understand them.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">However, there will always be a competition between what the systems of our world do and what our brains can calculate from their behavior. If we decide to interact only with systems that are much simpler than the brain in computing power, then we can expect that we can systematically understand what they are doing. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But if we want to use all the computational capabilities available in the universe, then inevitably the systems with which we interact will achieve the computational power of our brain. And this means that, according to the principle of computational irreducibility, we can never systematically ‚Äúovertake‚Äù or ‚Äúunderstand‚Äù the operation of these systems.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But how then can we use them? Well, just like people have always used the system of nature. Of course, we do not know all the details of their work or opportunities. But at some level of abstraction, we know enough to understand how to achieve our goals with their help. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What about areas like math? In mathematics, we are accustomed to the fact that we build a set of our knowledge so that we can understand every step. But experimental mathematics ‚Äî as well as such possibilities as the automatic proof of theorems ‚Äî make it obvious that there are areas in which such a method will not be available to us.</font></font><br><br>      ¬´¬ª? ,  .      ,       .       ,     . <br><br>  -          ,          .          .    , ,   ,       ‚Äì     ‚Äì  . </div><p>Source: <a href="https://habr.com/ru/post/431956/">https://habr.com/ru/post/431956/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431944/index.html">Yealink Meeting Server 2.0 - New Video Conferencing Features</a></li>
<li><a href="../431946/index.html">Security Week 49: Hacking Dell and Marriott</a></li>
<li><a href="../431948/index.html">Deep Mind taught your AI to predict protein structure</a></li>
<li><a href="../431950/index.html">How to forecast demand and automate purchases using machine learning: Ozon case</a></li>
<li><a href="../431954/index.html">Former vice president of Sun and DEC became president of MIPS / Wave, talks about Russia and RISC / V</a></li>
<li><a href="../431958/index.html">Tesla's Virtual Power Station with rechargeable batteries (Powerwall) expands to 1,000 homes in Australia</a></li>
<li><a href="../431960/index.html">Nvidia went crazy and opens up PhysX under BSD-3</a></li>
<li><a href="../431962/index.html">Orbit Fab plans to refuel satellites right in orbit</a></li>
<li><a href="../431964/index.html">Merge sort</a></li>
<li><a href="../431968/index.html">Large FAQ about long-distance trains and non-obvious rules</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>