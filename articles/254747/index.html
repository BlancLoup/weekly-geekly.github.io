<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Comparison of deep learning libraries on the example of the handwriting numbers classification problem</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Kruchinin Dmitry, Dolotov Evgeny, Kustikova Valentina, Druzhkov Pavel, Kornyakov Kirill 

 Introduction 
 At present, machine learning is an actively ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Comparison of deep learning libraries on the example of the handwriting numbers classification problem</h1><div class="post__text post__text-html js-mediator-article">  <i>Kruchinin Dmitry, Dolotov Evgeny, Kustikova Valentina, Druzhkov Pavel, Kornyakov Kirill</i> <br><br><h2>  Introduction </h2><br>  At present, machine learning is an actively developing field of research.  This is connected both with the possibility of faster, <s>higher, stronger</s> , simpler and cheaper to collect and process data, and with the development of methods for identifying laws from these data, according to which physical, biological, economic and other processes take place.  In some tasks, when such a law is difficult to determine, use deep learning. <br><br>  <i><b>Deep learning</b></i> considers the methods of modeling high-level abstractions in data using a set of consecutive nonlinear transformations, which, as a rule, are represented as artificial neural networks.  Today, neural networks are successfully used to solve problems such as forecasting, pattern recognition, data compression, and several others. <br><a name="habracut"></a><br>  The relevance of the topic of machine learning and, in particular, deep learning is confirmed by the regular appearance of articles on this topic in Habr√©: <br><ul><li>  <a href="http://habrahabr.ru/post/249089/">Deep learning and Caffe on New Year's holidays.</a> </li><li>  <a href="http://habrahabr.ru/post/173819/">Data mining: Toolkit - Theano.</a> </li><li>  <a href="http://habrahabr.ru/post/226347/">About cats, dogs, machine learning and deep learning.</a> </li><li>  <a href="http://habrahabr.ru/search/%3Fq%3D%25D0%259E%25D0%25B1%25D0%25B7%25D0%25BE%25D1%2580%2B%25D0%25BD%25D0%25B0%25D0%25B8%25D0%25B1%25D0%25BE%25D0%25BB%25D0%25B5%25D0%25B5%2B%25D0%25B8%25D0%25BD%25D1%2582%25D0%25B5%25D1%2580%25D0%25B5%25D1%2581%25D0%25BD%25D1%258B%25D1%2585%2B%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B5%25D1%2580%25D0%25B8%25D0%25B0%25D0%25BB%25D0%25BE%25D0%25B2%2B%25D0%25BF%25D0%25BE%2B%25D0%25B0%25D0%25BD%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%25D1%2583%2B%25D0%25B4%25D0%25B0%25D0%25BD%25D0%25BD%25D1%258B%25D1%2585%2B%25D0%25B8%2B%25D0%25BC%25D0%25B0%25D1%2588%25D0%25B8%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25BC%25D1%2583%2B%25D0%25BE%25D0%25B1%25D1%2583%25D1%2587%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258E%26target_type%3Dposts%26order_by%3Drelevance">Weekly reviews of the most interesting materials on data analysis and machine learning.</a> </li></ul><br>  This article is devoted to a comparative analysis of some deep learning software tools, of which a great many have recently appeared [ <a href="https://habr.com/ru/company/intel/blog/254747/">1</a> ].  Such tools include software libraries, extensions of programming languages, as well as independent languages ‚Äã‚Äãthat allow the use of ready-made algorithms for creating and teaching neural network models.  Existing deep learning tools have different functionalities and require different levels of knowledge and skills from the user.  The right choice of tools is an important task, allowing you to achieve the desired result in the shortest time and with less expenditure of effort. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The article provides a brief overview of the design and training tools for neural network models.  The focus is on four libraries: <a href="http://caffe.berkeleyvision.org/">Caffe</a> , <a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a> , <a href="http://torch.ch/">Torch</a> and <a href="http://deeplearning.net/software/theano/">Theano</a> .  The basic capabilities of these libraries are considered, examples of their use are given.  The quality and speed of the libraries are compared when constructing the same neural network topologies for solving the <a href="http://www.machinelearning.ru/wiki/index.php%3Ftitle%3D%25D0%259A%25D0%25BB%25D0%25B0%25D1%2581%25D1%2581%25D0%25B8%25D1%2584%25D0%25B8%25D0%25BA%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">problem of classifying</a> handwritten numbers ( <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> is used as a training and test sample).  An attempt is also made to evaluate the usability of the libraries in question in practice. <br><br><h2>  MNIST dataset </h2><br>  Further, the <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a> handwritten digit image database will be used as the data set under study ( <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 1</a> ).  Images in this database have a resolution of 28x28 and are stored in a grayscale format.  The numbers are centered on the image.  The entire base is divided into two parts: training, consisting of 50,000 images, and test - 10,000 images. <br><br><a name="Fig1"></a><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/707/f86/988/707f869888aee2b361b56c8325f45cb5.jpg" alt="image"></div><br>  Fig.  1. <a href="http://www.cad.zju.edu.cn/home/dengcai/Data/MNIST/images.html">Examples of images of numbers in the MNIST database</a> <br><br><h2>  Software tools for solving problems of deep learning </h2><br>  There are many software tools for solving problems of deep learning.  In [ <a href="https://habr.com/ru/company/intel/blog/254747/">1</a> ] you can find a general comparison of the functionality of the most well-known, here we give general information about some of them ( <a href="https://habr.com/ru/company/intel/blog/254747/">table 1</a> ).  The first six program libraries implement the widest range of deep learning methods.  Developers provide opportunities for creating fully connected neural networks (fully connected neural network, FC NN [ <a href="https://habr.com/ru/company/intel/blog/254747/">2</a> ]), convolutional neural networks (CNN) [ <a href="https://habr.com/ru/company/intel/blog/254747/">3</a> ], autocoders (autoencoder, AE) and limited Boltzmann machines (restricted Boltzmann machine, RBM) [ <a href="https://habr.com/ru/company/intel/blog/254747/">4</a> ].  It is necessary to pay attention to the remaining libraries.  Although they have less functionality, in some cases their simplicity helps to achieve greater performance. <br><br><a name="Table1"></a>  Table 1. Capabilities of deep learning software [ <a href="https://habr.com/ru/company/intel/blog/254747/">1</a> ] <br><table><tbody><tr><td>  # </td><td>  <b>Title</b> </td><td>  <b>Tongue</b> </td><td>  <b>OC</b> </td><td>  <b>FC NN</b> </td><td>  <b>CNN</b> </td><td>  <b>AE</b> </td><td>  <b>RBM</b> </td></tr><tr><td>  <b>one</b> </td><td>  <a href="https://github.com/rasmusbergpalm/DeepLearnToolbox">DeepLearnToolbox</a> </td><td>  Matlab </td><td>  Windows linux </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>2</b> </td><td>  <a href="http://deeplearning.net/software/theano/">Theano</a> </td><td>  Python </td><td>  Windows, Linux, Mac </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>3</b> </td><td>  <a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a> </td><td>  Python </td><td>  Linux, Vagrant </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>four</b> </td><td>  <a href="https://github.com/nitishsrivastava/deepnet">Deepnet</a> </td><td>  Python </td><td>  Linux </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>five</b> </td><td>  <a href="https://github.com/kyunghyuncho/deepmat">Deepmat</a> </td><td>  Matlab </td><td>  ? </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>6</b> </td><td>  <a href="http://torch.ch/">Torch</a> </td><td>  Lua, C </td><td>  Linux, Mac OS X, iOS, Android </td><td>  + </td><td>  + </td><td>  + </td><td>  + </td></tr><tr><td>  <b>7</b> </td><td>  <a href="http://cran.r-project.org/web/packages/darch/index.html">Darch</a> </td><td>  R </td><td>  Windows linux </td><td>  + </td><td>  - </td><td>  + </td><td>  + </td></tr><tr><td>  <b>eight</b> </td><td>  <a href="http://caffe.berkeleyvision.org/">Caff</a> e </td><td>  C ++, Python, Matlab </td><td>  Linux, OS X </td><td>  + </td><td>  + </td><td>  - </td><td>  - </td></tr><tr><td>  <b>9</b> </td><td>  <a href="http://milakov.github.io/nnForge/">nnForge</a> </td><td>  C ++ </td><td>  Linux </td><td>  + </td><td>  + </td><td>  - </td><td>  - </td></tr><tr><td>  <b>ten</b> </td><td>  <a href="https://github.com/antinucleon/cxxnet">CXXNET</a> </td><td>  C ++ </td><td>  Linux </td><td>  + </td><td>  + </td><td>  - </td><td>  - </td></tr><tr><td>  <b>eleven</b> </td><td>  <a href="https://code.google.com/p/cuda-convnet/">Cuda convnet</a> </td><td>  C ++ </td><td>  Linux, Windows </td><td>  + </td><td>  + </td><td>  - </td><td>  - </td></tr><tr><td>  <b>12</b> </td><td>  <a href="http://www.mathworks.com/matlabcentral/fileexchange/24291-cnn-convolutional-neural-network-class">Cuda CNN</a> </td><td>  Matlab </td><td>  Linux, Windows </td><td>  + </td><td>  + </td><td>  - </td><td>  - </td></tr></tbody></table><br>  Based on the information provided in [ <a href="https://habr.com/ru/company/intel/blog/254747/">1</a> ] and the recommendations of specialists, four libraries were chosen for further consideration: <a href="https://habr.com/ru/company/intel/blog/254747/">Theano</a> , <a href="https://habr.com/ru/company/intel/blog/254747/">Pylearn2</a> - one of the most mature and functionally complete libraries, <a href="https://habr.com/ru/company/intel/blog/254747/">Torch</a> and <a href="https://habr.com/ru/company/intel/blog/254747/">Caffe</a> - widely used by the community.  Each library is reviewed according to the following plan: <br><ol><li>  Brief reference information. </li><li>  Technical features (OS, programming language, dependencies). </li><li>  Functionality </li><li>  An example of the formation of a network such as logistic regression. </li><li>  Training and use of the constructed model for classification. </li></ol><br>  After reviewing the listed libraries, they are compared on a number of test network configurations. <br><br><a name="CaffeLib"></a><h3>  Caffe library </h3><br> <a href=""><img src="https://habrastorage.org/files/179/e36/288/179e362881ff46de97de62b69490d0fd.png"></a> <br>  <a href="http://caffe.berkeleyvision.org/">Caffe</a> has been developing since September 2013. <a href="http://daggerfs.com/">Yangqing Jia</a> began his development during his studies at the University of California at Berkeley.  From this point on, Caffe is actively supported by the Berkeley Vision and Learning Center ( <a href="http://bvlc.eecs.berkeley.edu/">BVLC</a> ) and the <a href="https://github.com/BVLC/caffe">GitHub</a> developer community.  The library is distributed under the BSD 2-Clause license. <br><br>  Caffe is implemented using the C ++ programming language, there are wrappers in Python and MATLAB.  The officially supported operating systems are Linux and OS X, there is also an <a href="https://github.com/niuzhiheng/caffe">unofficial port on Windows</a> .  Caffe uses the BLAS library (ATLAS, Intel MKL, OpenBLAS) for vector and matrix calculations.  Along with this, external dependencies include glog, gflags, OpenCV, protoBuf, boost, leveldb, nappy, hdf5, lmdb.  To speed up computing, Caffe can be run on a GPU using the basic capabilities of CUDA technology or the <a href="https://developer.nvidia.com/cuDNN">cuDNN</a> deep learning primitive library. <br><br>  Caffe developers support the creation, training and testing of fully connected and convolutional neural networks.  Input data and transformations are described by the concept of a <i>layer</i> .  Depending on the storage format, the following types of raw data layers can be used: <br><ul><li>  DATA - defines the data layer in the format leveldb and lmdb. </li><li>  HDF5_DATA - data layer in hdf5 format. </li><li>  IMAGE_DATA is a simple format that assumes that the file contains a list of images with an indication of the class label. </li><li>  other. </li></ul><br>  Transformations can be specified using layers: <br><ul><li>  INNER_PRODUCT is a fully bound layer. </li><li>  CONVOLUTION - convolutional layer. </li><li>  POOLING - layer spatial association. </li><li>  Local Response Normalization (LRN) - layer of local normalization. </li></ul><br>  Along with this, various activation functions can be used in the formation of transformations. <br><ul><li>  The positive part (Rectified-Linear Unit, ReLU). </li><li>  Sigmoidal function (SIGMOID). </li><li>  Hyperbolic Tangent (TANH). </li><li>  Absolute value (ABSVAL). </li><li>  Exponentiation (POWER). </li><li>  Binominal normal log likelihood function (binomial normal log likelihood, BNLL). </li></ul><br>  The last layer of the neural network model should contain the error function.  The library has the following functions: <br><ul><li>  Mean-Square Error (MSE). </li><li>  Regional error (Hinge loss). </li><li>  Logistic error function (Logistic loss). </li><li>  Info gain information loss function. </li><li>  Sigmoid cross-entropy loss. </li><li>  Softmax function.  Summarizes sigmoidal cross-entropy in the case of more than two classes. </li></ul><br>  In the process of teaching models, various optimization methods are used.  Caffe developers provide the implementation of a number of methods: <br><ul><li>  Stochastic Gradient Descent (Stochastic Gradient Descent, SGD) [ <a href="https://habr.com/ru/company/intel/blog/254747/">6</a> ]. </li><li>  Algorithm with adaptive learning rate (AdaGrad) [ <a href="https://habr.com/ru/company/intel/blog/254747/">7</a> ]. </li><li>  Accelerated Gradient Descent of Nesterov (Nesterov's Accelerated Gradient Descent, NAG) [ <a href="https://habr.com/ru/company/intel/blog/254747/">8</a> ]. </li></ul><br>  In the Caffe library, the topology of the neural networks, the initial data and the learning method are specified using the configuration files in the prototxt format.  The file contains a description of the input data (training and test) and layers of the neural network.  Let us consider the stages of building such files using the example of the ‚Äúlogistic regression‚Äù network ( <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 2</a> ).  Further, we assume that the file is called linear_regression.prototxt, and it is located in the examples / mnist directory. <br><br><a name="Fig2"></a><div style="text-align:center;"><img src="https://habrastorage.org/files/a6b/fab/6e7/a6bfab6e7ed84973a4aeeaed93c8c2d0.png"></div><br>  Fig.  2. The structure of the neural network <br><br><ol><li>  Set the network name. <br><pre><code class="hljs pgsql"><span class="hljs-type"><span class="hljs-type">name</span></span>: "LinearRegression"</code> </pre> <br></li><li>  The MNIST database stored in the lmdb format is used as the training set.  To work with lmdb or leveldb formats, a ‚ÄúDATA‚Äù type layer is used in which you must specify some parameters that describe the input data (data_param): the path to the data on the hard disk (source), the data type (backend), the sample size (batch_size).  You can also perform various transformations with the data (transform_param).  For example, you can normalize the image by multiplying all values ‚Äã‚Äãby 0.00390625 (the number inverse to 255).  The top parameter specifies one or more names that will be used to identify the output layer.  In this example, these are processed images (data) and labels of classes that own images (label). <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">layers</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">name</span></span>: <span class="hljs-string"><span class="hljs-string">"mnist"</span></span> type: DATA top: <span class="hljs-string"><span class="hljs-string">"data"</span></span> top: <span class="hljs-string"><span class="hljs-string">"label"</span></span> data_param { source: <span class="hljs-string"><span class="hljs-string">"examples/mnist/mnist_train_lmdb"</span></span> backend: LMDB batch_size: <span class="hljs-number"><span class="hljs-number">64</span></span> } <span class="hljs-selector-tag"><span class="hljs-selector-tag">transform_param</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">scale</span></span>: <span class="hljs-number"><span class="hljs-number">0.00390625</span></span> } }</code> </pre><br></li><li>  We define a fully connected layer (the output of each neuron of the previous layer is connected with the input of each neuron of the next layer).  The full link layer in the Caffe library is set using the INNER_PRODUCT layer.  The input name is specified using the bottom parameter.  In this layer, the input data are processed images (data).  The number of neurons in the layer is determined automatically (by the number of outputs in the previous layer), and the number of output neurons is indicated using the num_output parameter.  The result of the layer is set by the same name as the layer name (ip). <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">layers</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">name</span></span>: <span class="hljs-string"><span class="hljs-string">"ip"</span></span> type: INNER_PRODUCT bottom: <span class="hljs-string"><span class="hljs-string">"data"</span></span> top: <span class="hljs-string"><span class="hljs-string">"ip"</span></span> inner_product_param { num_output: <span class="hljs-number"><span class="hljs-number">10</span></span> } }</code> </pre><br></li><li>  At the end, add a layer that calculates the error function.  It takes as input the result of the previous fully meshed layer (ip) and the class numbers for each image (label).  After calculations, the results of this layer can be addressed by the name loss. <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">layers</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">name</span></span>: <span class="hljs-string"><span class="hljs-string">"loss"</span></span> type: SOFTMAX_LOSS bottom: <span class="hljs-string"><span class="hljs-string">"ip"</span></span> bottom: <span class="hljs-string"><span class="hljs-string">"label"</span></span> top: <span class="hljs-string"><span class="hljs-string">"loss"</span></span> }</code> </pre><br></li></ol><br>  Network configuration ready.  Next, you need to define the parameters of the training procedure in the prototxt format file (let's call it solver.prototxt).  The training parameters include the path to the file with the network configuration (net), the frequency of testing during training (test_interval), the parameters of stochastic gradient descent (base_lr, weight_decay and others), the maximum number of iterations (max_iter), the architecture on which the calculations will be performed (solver_mode), path to save the trained network (snapshot_prefix). <br><pre> <code class="hljs mel">net: <span class="hljs-string"><span class="hljs-string">"examples/mnist/linear_regression.prototxt"</span></span> test_iter: <span class="hljs-number"><span class="hljs-number">100</span></span> test_interval: <span class="hljs-number"><span class="hljs-number">500</span></span> base_lr: <span class="hljs-number"><span class="hljs-number">0.01</span></span> momentum: <span class="hljs-number"><span class="hljs-number">0.9</span></span> weight_decay: <span class="hljs-number"><span class="hljs-number">0.0005</span></span> lr_policy: <span class="hljs-string"><span class="hljs-string">"inv"</span></span> <span class="hljs-keyword"><span class="hljs-keyword">gamma</span></span>: <span class="hljs-number"><span class="hljs-number">0.0001</span></span> power: <span class="hljs-number"><span class="hljs-number">0.75</span></span> display: <span class="hljs-number"><span class="hljs-number">100</span></span> max_iter: <span class="hljs-number"><span class="hljs-number">10000</span></span> <span class="hljs-keyword"><span class="hljs-keyword">snapshot</span></span>: <span class="hljs-number"><span class="hljs-number">5000</span></span> snapshot_prefix: <span class="hljs-string"><span class="hljs-string">"examples/mnist/linear_regression"</span></span> solver_mode: GPU</code> </pre><br>  Training is performed using the main library application.  In this case, a certain set of keys is transferred, in particular, the name of the file containing the description of the parameters of the training procedure. <br><pre> <code class="hljs pgsql">caffe train <span class="hljs-comment"><span class="hljs-comment">--solver=solver.prototxt</span></span></code> </pre><br>  After learning, the resulting model can be used to classify images, for example, using Python wrappers: <br><ol><li>  We connect the library Caffe.  Set the test mode and specify the architecture for performing calculations (CPU or GPU). <br><pre> <code class="hljs haskell"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> caffe caffe.set_phase_test() caffe.set_mode_cpu()</code> </pre><br></li><li>  We create a neural network, specifying the following parameters: MODEL_FILE ‚Äî network configuration in prototxt format, PRETRAINED ‚Äî trained caffemodel network, IMAGE_MEAN ‚Äî average image (calculated from a set of input images and used for subsequent intensity normalization), channel_swap sets the color model, raw_scale ‚Äî maximum intensity value, image_dims - image resolution.  After that, load the image for classification (IMAGE_FILE). <br><pre> <code class="hljs dos"><span class="hljs-built_in"><span class="hljs-built_in">net</span></span> = caffe.Classifier(MODEL_FILE, PRETRAINED, IMAGE_MEAN, channel_swap=(<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>), raw_scale=<span class="hljs-number"><span class="hljs-number">255</span></span>, image_dims=(<span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-number"><span class="hljs-number">28</span></span>)) input_image = caffe.io.load_image(IMAGE_FILE)</code> </pre><br></li><li>  We get the neural network response for the selected image and display the results on the screen. <br><pre> <code class="hljs scala">prediction = net.predict([input_image]) print <span class="hljs-symbol"><span class="hljs-symbol">'prediction</span></span> shape:', prediction[<span class="hljs-number"><span class="hljs-number">0</span></span>].shape print <span class="hljs-symbol"><span class="hljs-symbol">'predicted</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span></span>:', prediction[<span class="hljs-number"><span class="hljs-number">0</span></span>].argmax()</code> </pre><br></li></ol><br>  Thus, by simple actions you can get the first results of experiments with deep neural network models.  More complex and detailed examples can be seen on <a href="http://caffe.berkeleyvision.org/">the developers website</a> . <br><br><a name="Pylearn2Lib"></a><h3>  Pylearn2 library </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/9d7/76c/927/9d776c927386d5c930ae52856e58df81.png" alt="image"><br>  <a href="http://deeplearning.net/software/pylearn2/">Pylearn2</a> is a library being developed in the <a href="https://sites.google.com/a/lisa.iro.umontreal.ca/mila/">LISA</a> lab at the University of Montreal since February 2011.  It has about 100 developers on <a href="https://github.com/lisa-lab/pylearn2">GitHub</a> .  The library is distributed under the BSD 3-Clause license. <br><br>  Pylearn2 is implemented in Python, currently the Linux operating system is supported, it is also possible to run on any operating system using a virtual machine, since  developers provide a configured virtual environment wrapper based on Vagrant.  Pylearn2 is a superstructure above <a href="http://deeplearning.net/software/theano/">Theano</a> library.  Additionally required PyYAML, PIL.  To speed up the calculations, Pylearn2 and Theano use <a href="https://code.google.com/p/cuda-convnet/">Cuda-convnet</a> , which is implemented in C ++ / CUDA, which gives a significant increase in speed. <br><br>  Pylearn2 supports the ability to create fully connected and convolutional neural networks, various types of auto-encoders (Contractive Auto-Encoders, Denoising Auto-Encoders) and limited Boltzmann machines (Gaussian RBM, the spike-and-slab RBM).  There are several error functions: cross-entropy, log-likelihood.  The following teaching methods are available: <br><ul><li>  Batch Gradient Descent (BGD). </li><li>  Stochastic Gradient Descent (Stochastic Gradient Descent, SGD). </li><li>  Nonlinear conjugate gradient descent (NCG) method. </li></ul><br>  In the Pylearn2 library, neural networks are set using their descriptions in the configuration file in YAML format.  YAML files are a convenient and fast way to serialize objects, as it is developed using object-oriented programming techniques. <br><br>  Consider the procedure for the formation of YAML files describing the structure of a neural network and the way it is trained, using the example of logistic regression. <br><ol><li>  Define the training set.  Pylearn2 has already implemented a class for working with the MNIST database.  Training will be carried out on the first 50,000 images. <br><pre> <code class="hljs css">!<span class="hljs-selector-tag"><span class="hljs-selector-tag">obj</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:pylearn2.train.Train</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">dataset</span></span>: &amp;train !obj:pylearn2.datasets.mnist.MNIST { which_set: <span class="hljs-string"><span class="hljs-string">'train'</span></span>, one_hot: <span class="hljs-number"><span class="hljs-number">1</span></span>, start: <span class="hljs-number"><span class="hljs-number">0</span></span>, stop: <span class="hljs-number"><span class="hljs-number">50000</span></span> },</code> </pre><br></li><li>  We describe the structure of the network.  To do this, use the class that implements the logistic regression.  It is enough to specify the necessary parameters.  The number of input neurons in the fully connected layer (nvis) is 784 (by the number of pixels in the image), the output (n_classes) is 10 (by the number of object classes), the initial weights (iranges) are determined with zeros. <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">model</span></span>: !<span class="hljs-selector-tag"><span class="hljs-selector-tag">obj</span></span><span class="hljs-selector-pseudo"><span class="hljs-selector-pseudo">:pylearn2.models.softmax_regression.SoftmaxRegression</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">n_classes</span></span>: <span class="hljs-number"><span class="hljs-number">10</span></span>, irange: <span class="hljs-number"><span class="hljs-number">0</span></span>., nvis: <span class="hljs-number"><span class="hljs-number">784</span></span>, },</code> </pre><br></li><li>  Let's choose a neural network learning algorithm and its parameters.  For learning, choose the method of packet gradient descent (BGD).  The batch_size parameter is responsible for the size of the training sample used at each step of the gradient descent.  Setting the line_search_mode parameter to exhaustive means that the packet gradient descent method (BGD) will try to use a binary search to reach the best point along the gradient direction, which speeds up the convergence of the gradient descent.  During the training, we will track the results of the classification at the training, validation (images from 50,000 to 60,000) and test samples.  The stop criterion is the maximum number of optimization iterations. <br><pre> <code class="hljs erlang-repl">algorithm: !obj:pylearn2.training_algorithms.bgd.BGD { batch_size: <span class="hljs-number"><span class="hljs-number">128</span></span>, line_search_mode: <span class="hljs-string"><span class="hljs-string">'exhaustive'</span></span>, monitoring_dataset: { <span class="hljs-string"><span class="hljs-string">'train'</span></span> : *train, <span class="hljs-string"><span class="hljs-string">'valid'</span></span> : !obj:pylearn2.datasets.mnist.MNIST { which_set: <span class="hljs-string"><span class="hljs-string">'train'</span></span>, one_hot: <span class="hljs-number"><span class="hljs-number">1</span></span>, start: <span class="hljs-number"><span class="hljs-number">50000</span></span>, stop: <span class="hljs-number"><span class="hljs-number">60000</span></span> }, <span class="hljs-string"><span class="hljs-string">'test'</span></span> : !obj:pylearn2.datasets.mnist.MNIST { which_set: <span class="hljs-string"><span class="hljs-string">'test'</span></span>, one_hot: <span class="hljs-number"><span class="hljs-number">1</span></span>, } }, termination_criterion: !obj:pylearn2.termination_criteria.And { criteria: [ !obj:pylearn2.termination_criteria.EpochCounter { max_epochs: <span class="hljs-number"><span class="hljs-number">150</span></span> }, ] } },</code> </pre><br></li><li>  For further use of the trained model, you must save the result.  Note that the model is saved in the pkl format. <br><pre> <code class="hljs erlang-repl">extensions: [ !obj:pylearn2.train_extensions.best_params.MonitorBasedSaveBest { channel_name: <span class="hljs-string"><span class="hljs-string">'valid_y_misclass'</span></span>, save_path: <span class="hljs-string"><span class="hljs-string">"%(save_path)s/softmax_regression_best.pkl"</span></span> }, ]</code> </pre><br></li></ol><br>  Thus, the network configuration has been prepared and the necessary infrastructure for training and classification has been determined, which are performed by calling the appropriate Python script.  For training, you must run the following command line: <br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">python</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">train</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.py</span></span> &lt;   &gt;<span class="hljs-selector-class"><span class="hljs-selector-class">.yaml</span></span></code> </pre> <br>  More complex and detailed examples can be seen on the <a href="http://deeplearning.net/software/pylearn2/">official website</a> or in the <a href="https://github.com/ITLab-Vision/DNN-develop/tree/master/pylearn2/mnist">repository</a> . <br><br><a name="TorchLib"></a><h3>  Torch Library </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/32b/e1b/502/32be1b5020b9d66efb3c3f22f0129bb4.png" alt="image"><br>  <a href="http://torch.ch/">Torch</a> is a library for scientific computing with broad support for machine learning algorithms.  Developed by the <a href="http://www.idiap.ch/">Idiap Research Institute</a> , <a href="http://www.nyu.edu/">New York University</a> and <a href="http://www.nec-labs.com/">NEC Laboratories America</a> , since 2000, distributed under the BSD license. <br><br>  The library is implemented in Lua using C and CUDA.  The fast scripting language Lua in conjunction with SSE, OpenMP, CUDA technologies allow Torch to show good speed compared to other libraries.  Currently, Linux, FreeBSD, Mac OS X operating systems are supported. The main modules also work on Windows.  The Torch dependencies are packages imagemagick, gnuplot, nodejs, npm and others. <br><br>  The library consists of a set of modules, each of which is responsible for different stages of working with neural networks.  For example, the <i>nn</i> module provides configuration of the neural network (definition of layers, and their parameters), the <i>optim</i> module contains implementations of various optimization methods used for training, and <i>gnuplot</i> provides the ability to visualize data (graphing, displaying images, etc.).  Installing <a href="https://github.com/torch/torch7/wiki/Cheatsheet">additional modules</a> allows you to extend the functionality of the library. <br><br>  Torch allows you to create complex neural networks using the container mechanism.  <i>The container</i> is a class that combines the declared components of a neural network into one common configuration, which can later be transferred to the learning procedure.  The neural network component can be not only fully connected or convolutional layers, but also activation or error functions, as well as ready-made containers.  Torch allows you to create the following layers: <br><ul><li>  Full connected layer (Linear). </li><li>  Activation functions: hyperbolic tangent (Tanh), the choice of minimum (Min) or maximum (Max), softmax-function (SoftMax) and others. </li><li>  Convolutional layers: convolution (Convolution), thinning (SubSampling), spatial union (MaxPooling, AveragePooling, LPPooling), difference normalization (SubtractiveNormalization). </li></ul><br>  Error functions: mean square error (MSE), cross-entropy (CrossEntropy), etc. <br><br>  When training can be used the following optimization methods: <br><ul><li>  Stochastic Gradient Descent (SGD), </li><li>  Average stochastic gradient descent (Averaged SGD) [ <a href="https://habr.com/ru/company/intel/blog/254747/">9</a> ]. </li><li>  The Broyden ‚Äì Fletcher ‚Äì Goldfarb ‚Äì Shanno algorithm (L-BFGS) [ <a href="https://habr.com/ru/company/intel/blog/254747/">10</a> ]. </li><li>  Conjugate Gradient (CG) Method. </li></ul><br>  Consider the process of configuring a neural network in Torch.  You must first declare the container, then add layers to it.  The order of adding layers is important because  the output of the (n-1) -th layer will be the input of the n-th. <br><pre> <code class="hljs cs">regression = nn.Sequential() regression:<span class="hljs-keyword"><span class="hljs-keyword">add</span></span>(nn.Linear(<span class="hljs-number"><span class="hljs-number">784</span></span>,<span class="hljs-number"><span class="hljs-number">10</span></span>)) regression:<span class="hljs-keyword"><span class="hljs-keyword">add</span></span>(nn.SoftMax()) loss = nn.ClassNLLCriterion()</code> </pre><br>  Use and training of a neural network: <br><ol><li>  Loading input data X. The function torch.load (path_to_ready_dset) allows you to load prepared in advance dataset in text or binary format.  As a rule, this is a Lua-table consisting of three fields: size, data and labels.  If there is no ready dataset, you can use the standard functions of the Lua language (for example, io.open (filename [, mode])) or functions from the Torch library packages (for example, image.loadJPG (filename)). </li><li>  Determining the network response for input X: <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">Y</span></span> = regression:forward(X)</code> </pre><br></li><li>  Calculation of the error function E = loss (Y, T), in our case, this is a likelihood function. <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">E</span></span> = loss:forward(Y,T)</code> </pre><br></li><li>  Gradient miscalculation according to the backpropagation algorithm. <br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">dE_dY</span></span> = loss:backward(Y,T) regression:backward(X,dE_dY)</code> </pre><br></li></ol><br>  Now let's put it all together.  In order to train the neural network in the Torch library, you need to write your own learning cycle.  It declares a special function (closure), which will calculate the network response, determine the error value and recalculate the gradients, and transfer this closure to the gradient descent function to update the weights of the network. <br><pre> <code class="hljs delphi">--   :      w, dE_dw = regression:getParameters() <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> eval_E = <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">function</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(w)</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">dE_dw</span></span></span><span class="hljs-function">:</span></span>zero() --   <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> Y = regression:<span class="hljs-keyword"><span class="hljs-keyword">forward</span></span>(X) <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> E = loss:<span class="hljs-keyword"><span class="hljs-keyword">forward</span></span>(Y,T) <span class="hljs-keyword"><span class="hljs-keyword">local</span></span> dE_dY = loss:backward(Y,T) regression:backward(X,dE_dY) return E, dE_dw <span class="hljs-keyword"><span class="hljs-keyword">end</span></span> --      optim.sgd(eval_E, w, optimState)</code> </pre><br>  where optimState is the gradient descent parameters (learningRate, momentum, weightDecay, etc.).  Full cycle of training can be found <a href="">here</a> . <br><br>  It is easy to see that the declaration procedure, like the learning procedure, takes less than 10 lines of code, which indicates the ease of use of the library.  At the same time, the library allows working with neural networks at a fairly low level. <br><br>  Saving and loading of the trained network is carried out with the help of special functions: <br><pre> <code class="hljs pgsql">torch.save(<span class="hljs-type"><span class="hljs-type">path</span></span>, regression) net = torch.<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>(<span class="hljs-type"><span class="hljs-type">path</span></span>)</code> </pre><br>  Once loaded, the network can be used for classification or additional training.  If it is necessary to find out which class the sample element belongs to, then it suffices to go through the network and calculate the output: <br><pre> <code class="hljs dos">result = <span class="hljs-built_in"><span class="hljs-built_in">net</span></span>:forward(sample)</code> </pre><br>  More complex examples can be found in <a href="http://code.cogbits.com/wiki/doku.php%3Fid%3Dtutorial_basics">the library‚Äôs training materials</a> . <br><br><a name="TheanoLib"></a><h3>  Theano library </h3><br><img src="https://habrastorage.org/getpro/habr/post_images/f74/132/448/f741324488697000a1880b99f5d9f1d8.png" alt="image"><br>  <a href="http://deeplearning.net/software/theano/">Theano</a> is an extension of the Python language that allows you to efficiently calculate mathematical expressions containing multidimensional arrays.  The library got its name in honor of the wife of the ancient Greek philosopher and mathematician Pythagoras - Feano (or Teano).  Theano is developed in the <a href="https://sites.google.com/a/lisa.iro.umontreal.ca/mila/">LISA</a> lab to support the rapid development of machine learning algorithms. <br><br>  The library is implemented in Python, supported on Windows, Linux and Mac OS.  Theano includes a compiler that translates mathematical expressions written in Python into effective C or CUDA code. <br><br>  Theano provides a basic set of tools for configuring neural networks and learning them.  It is possible to implement multi-layer fully interconnected networks (Multi-Layer Perceptron), convolutional neural networks (CNN), recurrent neural networks (Recurrent Neural Networks, RNN), auto-encoders and limited Boltzmann machines.  Various activation functions are also provided, in particular, sigmoidal, softmax-function, cross-entropy.  In the course of training, a packet gradient descent (Batch SGD) is used. <br><br>  Consider the configuration of a neural network in Theano.  For convenience, we will implement the LogisticRegression class ( <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 3</a> ), which will contain variables ‚Äî learning parameters W, b and functions for working with them ‚Äî counting the network response (y = softmax (Wx + b)) and the error function.  Then, to train the neural network, we create the function train_model.  For it, it is necessary to describe the methods that determine the error function, the rule for calculating gradients, the method of changing the weights of the neural network, the size and location of the mini-batch sample (the images themselves and the answers for them).  After determining all the parameters, the function is compiled and passed to the learning cycle. <br><br><a name="Fig3"></a><div style="text-align:center;"><img src="https://habrastorage.org/files/b5d/782/88d/b5d78288d7064300bb122faed1b57fc9.png"></div><br>  Fig.  3. Class diagram for the implementation of the neural network in Theano <br><div class="spoiler">  <b class="spoiler_title">Software class implementation</b> <div class="spoiler_text"><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> LogisticRegression(<span class="hljs-keyword"><span class="hljs-keyword">object</span></span>): def __init__(self, <span class="hljs-keyword"><span class="hljs-keyword">input</span></span>, n_in, n_out): # y = W * x + b #  ,  ,     self.W = theano.shared( #     <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>=numpy.zeros((n_in, n_out), dtype=theano.config.floatX), <span class="hljs-type"><span class="hljs-type">name</span></span>=<span class="hljs-string"><span class="hljs-string">'W'</span></span>, borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) self.b = theano.shared(<span class="hljs-keyword"><span class="hljs-keyword">value</span></span>=numpy.zeros((n_out,), dtype=theano.config.floatX), <span class="hljs-type"><span class="hljs-type">name</span></span>=<span class="hljs-string"><span class="hljs-string">'b'</span></span>, borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) #    softmax,   -  y_pred self.p_y_given_x = T.nnet.softmax(T.dot(<span class="hljs-keyword"><span class="hljs-keyword">input</span></span>, self.W) + self.b) self.y_pred = T.argmax(self.p_y_given_x, axis=<span class="hljs-number"><span class="hljs-number">1</span></span>) self.params = [self.W, self.b] #    def negative_log_likelihood(self, y): <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>]), y]) # x -     #   (minibatch)      x # y -      x = T.matrix(<span class="hljs-string"><span class="hljs-string">'x'</span></span>) y = T.ivector(<span class="hljs-string"><span class="hljs-string">'y'</span></span>) #      MNIST    <span class="hljs-number"><span class="hljs-number">28</span></span>*<span class="hljs-number"><span class="hljs-number">28</span></span> classifier = LogisticRegression(input=x, n_in=<span class="hljs-number"><span class="hljs-number">28</span></span> * <span class="hljs-number"><span class="hljs-number">28</span></span>, n_out=<span class="hljs-number"><span class="hljs-number">10</span></span>) #   ,        <span class="hljs-keyword"><span class="hljs-keyword">cost</span></span> = classifier.negative_log_likelihood(y) #   ,    Theano - grad g_W = T.grad(<span class="hljs-keyword"><span class="hljs-keyword">cost</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">cost</span></span>, wrt=classifier.W) g_b = T.grad(<span class="hljs-keyword"><span class="hljs-keyword">cost</span></span>=<span class="hljs-keyword"><span class="hljs-keyword">cost</span></span>, wrt=classifier.b) #      updates = [(classifier.W, classifier.W - learning_rate * g_W), (classifier.b, classifier.b - learning_rate * g_b)] #   ,         train_model = theano.<span class="hljs-keyword"><span class="hljs-keyword">function</span></span>( inputs=[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span>], outputs=<span class="hljs-keyword"><span class="hljs-keyword">cost</span></span>, updates=updates, givens={ x: train_set_x[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span> * batch_size: (<span class="hljs-keyword"><span class="hljs-keyword">index</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) * batch_size], y: train_set_y[<span class="hljs-keyword"><span class="hljs-keyword">index</span></span> * batch_size: (<span class="hljs-keyword"><span class="hljs-keyword">index</span></span> + <span class="hljs-number"><span class="hljs-number">1</span></span>) * batch_size] } )</code> </pre><br></div></div><br>  To quickly save and load neural network parameters, you can use functions from the cPickle package: <br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> cPickle save_file = <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>(<span class="hljs-string"><span class="hljs-string">'path'</span></span>, <span class="hljs-string"><span class="hljs-string">'wb'</span></span>) cPickle.dump(classifier.W.get_value(borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), save_file, <span class="hljs-number"><span class="hljs-number">-1</span></span>) cPickle.dump(classifier.b.get_value(borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), save_file, <span class="hljs-number"><span class="hljs-number">-1</span></span>) save_file.<span class="hljs-keyword"><span class="hljs-keyword">close</span></span>() file = <span class="hljs-keyword"><span class="hljs-keyword">open</span></span>(<span class="hljs-string"><span class="hljs-string">'path'</span></span>) classifier.W.set_value(cPickle.<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>(save_file), borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) classifier.b.set_value(cPickle.<span class="hljs-keyword"><span class="hljs-keyword">load</span></span>(save_file), borrow=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)</code> </pre><br>  It is easy to see that the process of creating a model and determining its parameters requires writing a three-dimensional and noisy code.  The library is low-level.  It should be noted its flexibility, as well as the availability of the implementation and use of its own components.  The official website of the library has a large number of <a href="http://www.deeplearning.net/tutorial/">teaching materials</a> on various topics. <br><br><h2>  Comparing libraries on the example of the handwriting numbers classification task </h2><br><h3>  Test infrastructure </h3><br>  The following test infrastructure was used in the course of experiments to evaluate library performance: <br><ol><li><a name="cpu_conf"></a>  Ubuntu 12.04, Intel Core i5-3210M @ 2.5GHz (CPU experiments). </li><li><a name="gpu_conf"></a>  Ubuntu 14.04, Intel Core i5-2430M @ 2.4GHz + NVIDIA GeForce GT 540M (GPU experiments). </li><li>  GCC 4.8, NVCC 6.5. </li></ol><br><h3>  Network topologies and learning parameters </h3><br>  Computational experiments were carried out on fully connected and convolutional neural networks of the following structure: <br><ol><li>  Three-layer fully connected neural network (MLP, <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 4</a> ): <br><ul><li>  1st layer - FC (in: 784, out: 392, activation: tanh). </li><li>  2d layer - FC (in: 392, out: 196, activation: tanh). </li><li>  3d layer - FC (in: 196, out: 10, activation: softmax). </li></ul><br><a name="Fig4"></a><div style="text-align:center;"><img src="https://habrastorage.org/files/878/92f/47d/87892f47da3645e3b36dec271e19b8a4.png"></div><br>  Fig.  4. Structure of a three-layer full mesh network <br><br></li><li>  Convolutional neural network (CNN, <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 5</a> ): <br><ul><li>  1st layer - convolution (in filters: 1, out filters: 28, size: 5x5, stride: 1x1). </li><li>  2d layer - max-pooling (size: 3x3, stride: 3x3). </li><li>  3d layer - convolution (in filters: 28, out filters: 56, size: 5x5, stride 1x1). </li><li>  4th layer - max-pooling (size: 2x2, stride: 2x2). </li><li>  5th layer - FC (in: 224, out: 200, activation: tanh). </li><li>  6th layer - FC (in: 200, out: 10, activation: softmax). </li></ul><br><a name="Fig5"></a><div style="text-align:center;"><img src="https://habrastorage.org/files/97b/c66/13d/97bc6613d18c41a3a014fdcd3bc3427c.png"></div><br>  Fig.  5. The structure of the convolutional neural network <br><br></li></ol><br>  All weights were initialized randomly according to a uniform distribution law in the range (‚àí6 / (n_in + n_out), 6 / (n_in + n_out)), where n_in, n_out is the number of neurons at the input and output of the layer, respectively.  The parameters of the stochastic gradient descent (SGD) are chosen equal to the following values: learning rate - 0.01, momentum - 0.9, weight decay - 5e-4, batch size - 128, maximum number of iterations - 150. <br><br><h3>  Experimental results </h3><br>  The learning time of the neural networks described earlier ( <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 4</a> , <a href="https://habr.com/ru/company/intel/blog/254747/">5</a> ) using the four libraries reviewed is presented below ( <a href="https://habr.com/ru/company/intel/blog/254747/">Fig. 6</a> ).  It is easy to see that Pylearn2 shows worse performance (both on the CPU and on the GPU) compared to other libraries.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As for the rest, the learning time depends heavily on the network structure. </font><font style="vertical-align: inherit;">The best result among the implementations of networks running on the CPU, showed the Torch library (and on CNN it overtook itself, running on the GPU). </font><font style="vertical-align: inherit;">Among GPU implementations, the best result (on both networks) was shown by the Caffe library. </font><font style="vertical-align: inherit;">In general, the use of Caffe left only positive impressions.</font></font><br><br><a name="Fig6"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CPU implementation (see </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">infrastructure</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ):</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/files/0e8/aa9/08b/0e8aa908b0d8419ca06d708b780f7c0a.png"></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Implementation GPU (see </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">infrastructure</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ):</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/files/6c1/d24/4cc/6c1d244cc1bc41d2983b21d95d2c40c6.png"></div><br>  Fig.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">6. The learning time of the MLP and CNN networks described in the previous paragraph. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">As for the time for classifying one image on the CPU using trained models ( </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 7</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), it is easy to see that the Torch library was out of competition on both test neural networks. </font><font style="vertical-align: inherit;">A little behind her was Caffe on CNN, which at the same time showed the worst classification time for MLP.</font></font><br><br><a name="Fig7"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CPU implementation (see </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">infrastructure</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ):</font></font><br><div style="text-align:center;"><img src="https://habrastorage.org/files/183/6c1/209/1836c120923e4dbeb4a0daf3350f48f7.png"></div><br>  Fig.<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">7. Classification time of one image using trained MLP and CNN networks </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we look at the classification accuracy, on the MLP network it is higher than 97.4%, and CNN - ~ 99% for all libraries ( </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Table 2</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). </font><font style="vertical-align: inherit;">The obtained accuracy values ‚Äã‚Äãare somewhat lower than those listed on the </font></font><a href="http://yann.lecun.com/exdb/mnist"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MNIST</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> website </font><font style="vertical-align: inherit;">on the same neural network structures. </font><font style="vertical-align: inherit;">Small differences are due to differences in the settings of the initial weights of the networks and the parameters of optimization methods used in the learning process. </font><font style="vertical-align: inherit;">Actually, the goal of achieving maximum accuracy values ‚Äã‚Äãwas not, it was rather necessary to build identical network structures and set the most similar training parameters.</font></font><br><br><a name="Table2"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Table 2. The average value and dispersion of classification accuracy indicators for 5 experiments </font></font><br><table><tbody><tr><td></td><td colspan="2"> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></b> </td><td colspan="2"> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pylearn2</font></font></b> </td><td colspan="2"> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Theano</font></font></b> </td><td colspan="2"> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Torch</font></font></b> </td></tr><tr><td></td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accuracy</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,%</font></font></td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dispersion</font></font></i> </td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accuracy</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,%</font></font></td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dispersion</font></font></i> </td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accuracy</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,%</font></font></td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dispersion</font></font></i> </td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Accuracy</font></font></i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ,%</font></font></td><td> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Dispersion</font></font></i> </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MLP</font></font></b> </td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 98.26 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0.0039 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 98.1 </font></font></td><td>  0 </td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 97.42 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0.0023 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 98.19 </font></font></td><td>  0 </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">CNN</font></font></b> </td><td>  99.1 </td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0.0038 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 99.3 </font></font></td><td>  0 </td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 99.16 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 0.0132 </font></font></td><td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> 99.4 </font></font></td><td>  0 </td></tr></tbody></table><br><h3><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Comparing selected libraries </font></font></h3><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Based on the research of the library's functional, as well as the performance analysis, each of them was rated on a scale from 1 to 3 using the following criteria: </font></font><br><ul><li> <b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The learning rate</font></font></i></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reflects the learning time of the neural network models considered at the stage of conducting experiments.</font></font></li><li> <b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The grading rate</font></font></i></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> reflects the grading time of a single image.</font></font></li><li> <b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Usability</font></font></i></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> is a criterion that allows you to estimate the time spent studying a library.</font></font></li><li> <b><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The flexibility of</font></font></i></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> setting up links between layers, setting parameters for methods, and the availability of various data processing methods.</font></font></li><li>  <b><i></i></b> ‚Äî       (  ,  , ,   ,      ). </li><li>     <b><i></i></b>    </li></ul><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Consider the estimates obtained for each criterion, we place the places of each library from the first to the third ( </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Table 3</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). According to the results of computational experiments in terms of speed of work, the Caffe library is most preferable ( </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Fig. 6</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ). At the same time, it turned out to be the most convenient to use. From a position of flexibility, the Theano library showed the best results. In terms of functionality, Pylearn2 is the most complete, but its use is complicated by the need to understand the internal structure (the formation of YAML files requires this). The most detailed and understandable material for the study provide developers Torch. Having shown the average indicators for each criterion separately, it was she who won in the rating of the reviewed libraries.</font></font><br><br><a name="Table3"></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Table 3. Library comparison results (ranks from 1 to 3 for each criterion) </font></font><br><table><tbody><tr><td></td><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Learning speed</font></font></b> </td><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Grading speed</font></font></b> </td><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Convenience</font></font></b> </td><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Flexibility</font></font></b> </td><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Functional</font></font></b> </td><td>  <b>Documentation</b> </td><td>  <b>Amount</b> </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Caffe</font></font></b> </td><td>  <b>one</b> </td><td>  2 </td><td>  <b>one</b> </td><td>  3 </td><td>  3 </td><td>  2 </td><td>  12 </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Pylearn2</font></font></b> </td><td>  3 </td><td>  3 </td><td>  2 </td><td>  3 </td><td>  <b>one</b> </td><td>  3 </td><td>  15 </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Torch</font></font></b> </td><td>  2 </td><td>  <b>one</b> </td><td>  2 </td><td>  2 </td><td>  2 </td><td>  <b>one</b> </td><td>  <b>ten</b> </td></tr><tr><td> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Theano</font></font></b> </td><td>  2 </td><td>  2 </td><td>  3 </td><td>  <b>one</b> </td><td>  2 </td><td>  2 </td><td>  12 </td></tr></tbody></table><br><h2>  Conclusion </h2><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Summarizing, we can say that the Torch library is the most mature. </font><font style="vertical-align: inherit;">At the same time, the libraries of Caffe and Theano are not inferior to it by many criteria ( </font></font><a href="https://habr.com/ru/company/intel/blog/254747/"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">table 3</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> ), therefore the possibility of their subsequent use cannot be ruled out. </font><font style="vertical-align: inherit;">In the future, Caffe and Torch libraries are planned to be used to study the applicability of deep learning methods to the tasks of detecting people, pedestrians and cars. </font></font><br><br> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The work was done in the laboratory "Information Technologies" of the Faculty of the VMK UNN them.</font></font></i>  <i>N.I.</i> <i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Lobachevsky with the support of the company Itseez.</font></font></i> <br><br><h2>  Used sources </h2><br><ol><li><a name="Paper1"></a> Kustikova, VD, Druzhkov, PN: A Survey of Deep Learning Methods and Software for Image Classification and Object Detection. In: Proc. of the 9th Open German-Russian Workshop on Pattern Recognition and Image Understanding. (2014) </li><li><a name="Paper2"></a> Hinton, GE: Learning Multiple Layers of Representation. In: Trends in Cognitive Sciences.  Vol. 11. pp. 428-434. (2007) </li><li><a name="Paper3"></a> LeCun, Y., Kavukcuoglu, K., Farabet, C.: Convolutional networks and applications in vision. In: Proc. of the IEEE Int. Symposium on Circuits and Systems (ISCAS).  pp. 253-256.  (2010) </li><li><a name="Paper4"></a> Hayat, M., Bennamoun, M., An, S.: Learning Non-Linear Reconstruction Models for Image Set Classification. In: Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition. (2014) </li><li><a name="Paper5"></a> Restricted Boltzmann Machines (RBMs), <a href="http://www.deeplearning.net/tutorial/rbm.html">www.deeplearning.net/tutorial/rbm.html</a> . </li><li><a name="Paper6"></a> Bottou, L.: Stochastic Gradient Descent Tricks. Neural Networks: Tricks of the Trade, <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">research.microsoft.com/pubs/192769/tricks-2012.pdf</a> . </li><li><a name="Paper7"></a> Duchi, J., Hazan, E., Singer, Y.: Adaptive Subgradient Methods for Online Learning and Stochastic Optimization. In: The Journal of Machine Learning Research. (2011) </li><li><a name="Paper8"></a> Sutskever, I., Martens, J., Dahl, G., Hinton, G.: On the Importance of Initialization and Momentum in Deep Learning. In: Proc. of the 30th Int.  Conf. on Machine Learning.  (2013) </li><li><a name="Paper9"></a>     (ASGD), <a href="http://research.microsoft.com/pubs/192769/tricks-2012.pdf">research.microsoft.com/pubs/192769/tricks-2012.pdf</a> . </li><li><a name="Paper10"></a>  ---, <a href="http://en.wikipedia.org/wiki/Limited-memory_BFGS">en.wikipedia.org/wiki/Limited-memory_BFGS</a> . </li></ol></div><p>Source: <a href="https://habr.com/ru/post/254747/">https://habr.com/ru/post/254747/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../254733/index.html">Using Asterisk to receive data from security systems</a></li>
<li><a href="../254737/index.html">Talking panda or what can be done with FFmpeg and OpenCV on Android</a></li>
<li><a href="../254739/index.html">Techno-Designed Alarm Clock - Arduino Based Desk Clock</a></li>
<li><a href="../254741/index.html">Traffic Inspector: full reboot</a></li>
<li><a href="../254743/index.html">Stepic seeking talents</a></li>
<li><a href="../254749/index.html">Bubot is a very easy Python 3 framework for robot programming and home automation.</a></li>
<li><a href="../254751/index.html">How Wheatfield Diffie helped Bob and Alice trick Eve</a></li>
<li><a href="../254753/index.html">Determine the weight of chess pieces by regression analysis</a></li>
<li><a href="../254755/index.html">From complex to simple: the evolution of interfaces of mobile trading terminals</a></li>
<li><a href="../254757/index.html">We write the postal address, as humans</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>