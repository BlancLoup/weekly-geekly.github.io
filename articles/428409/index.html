<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Analysis of the incident on October 21 at GitHub</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Fatal 43 seconds that caused the daily degradation of the service 

 Last week an incident occurred in GitHub, which led to the degradation of the ser...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Analysis of the incident on October 21 at GitHub</h1><div class="post__text post__text-html js-mediator-article">  <b>Fatal 43 seconds that caused the daily degradation of the service</b> <br><br>  Last week an <a href="https://blog.github.com/2018-10-21-october21-incident-report/">incident</a> occurred in GitHub, which led to the degradation of the service for 24 hours and 11 minutes.  The incident did not affect the entire platform, but only a few internal systems, which led to the display of outdated and inconsistent information.  Ultimately, user data was not lost, but manual reconciliation of a few seconds of writing to the database is still performed.  Throughout most of the crash, GitHub also could not handle web scams, create and publish GitHub Pages sites. <br><br>  All of us at GitHub would like to sincerely apologize for the problems that all of you have had.  We are aware of your trust in GitHub and are proud to build resilient systems that support the high availability of our platform.  With this incident, we let you down and deeply regret.  Although we cannot undo the problems due to the degradation of the GitHub platform for a long time, we can explain the causes of the incident, talk about the lessons learned and measures that will allow the company to better protect against such failures in the future. <br><a name="habracut"></a><br><h1>  Prehistory </h1><br>  Most custom GitHub services run in our own <a href="https://githubengineering.com/evolution-of-our-data-centers/">data centers</a> .  The data center topology is designed to provide a reliable and expandable boundary network in front of several regional data centers that provide computing systems and data storage systems.  Despite the levels of redundancy embedded in the physical and logical components of the project, it is still possible that sites will not be able to interact with each other for some time. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      On October 21, at 10:52 pm UTC, scheduled repairs to replace faulty 100G optical equipment led to a loss of communication between the East Coast hub (US East Coast) and the main East Coast data center.  The connection between them was restored after 43 seconds, but this short shutdown caused a chain of events that led to 24 hours and 11 minutes of service degradation. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c1e/fd8/71b/c1efd871b9017afe95d9605703ba7734.png"><br>  <i><font color="gray">High-level GitHub network architecture, including two physical data centers, 3 POP and cloud storage in several regions, connected via peering</font></i> <br><br>  In the past, we discussed how to use <a href="https://githubengineering.com/orchestrator-github">MySQL to store GitHub metadata</a> , as well as our approach to ensuring <a href="https://githubengineering.com/mysql-high-availability-at-github">MySQL high availability</a> .  GitHub manages several MySQL clusters ranging in size from hundreds of gigabytes to almost five terabytes.  Each cluster has dozens of read replicas for storing metadata other than Git, so our applications provide pull-requests, issues, authentication, background processing, and additional functions outside the Git object store.  Different data in different parts of the application is stored in different clusters using functional segmentation. <br><br>  To improve performance on a large scale, applications send records to the appropriate primary server for each cluster, but in most cases delegate read requests to a subset of replica servers.  We use <a href="https://github.com/github/orchestrator">Orchestrator</a> to manage MySQL cluster topologies and automatic failover.  During this process, the Orchestrator considers a number of variables and is built on top of <a href="https://raft.github.io/">Raft</a> for consistency.  Orchestrator can potentially implement topologies that applications do not support, so you need to monitor your Orchestrator configuration for application-level expectations. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/686/ea1/657/686ea165750913fa41b771482266b887.png"><br>  <i><font color="gray">In a conventional topology, all applications perform reading locally with low latency.</font></i> <br><br><h1>  Chronicle of the incident </h1><br><h4>  10/21/2018 22:52 UTC </h4><br>  During the aforementioned Orchestrator network split, the main data center began the process of de-selecting a guide according to the consensus algorithm Raft.  The West Coast data center and Orchestrator public cloud nodes on the East Coast were able to reach a consensus - and they began working off the failure of clusters to send records to the western data center.  Orchestrator began to create a database cluster topology in the West.  After the connection was restored, the applications immediately sent traffic by writing to the new main servers in US West. <br><br>  On the database servers in the eastern data center there were records for a short period that were not replicated to the western data center.  Since the database clusters in both data centers now contained records that were not in another data center, we could not safely return the primary server back to the eastern data center. <br><br><h4>  10/21/2018 22:54 UTC </h4><br>  Our internal monitoring systems began to generate alerts indicating numerous system failures.  At this time, several engineers responded and worked on sorting incoming notifications.  By 23:02, the engineers of the first response group determined that the topologies for numerous database clusters are in an unexpected state.  When requesting the Orchestrator API, the database replication topology was displayed, containing only servers from the western data center. <br><br><h4>  10.21.2018 23:07 UTC </h4><br>  At this point, the response team decided to manually block the internal deployment tools in order to prevent additional changes.  At 23:09 the group established a <a href="https://twitter.com/githubstatus/status/1054147648930897920">yellow</a> site health <a href="https://twitter.com/githubstatus/status/1054147648930897920">status</a> .  This action automatically assigned the situation the status of an active incident and sent a warning to the incident coordinator.  At 23:11 the coordinator joined the work and after two minutes decided to <a href="https://twitter.com/githubstatus/status/1054148705450946560">change the status to red</a> . <br><br><h4>  10/21/2018 23:13 UTC </h4><br>  At that time, it was clear that the problem affected several DB clusters.  To work attracted additional developers from the engineering database.  They began to investigate the current state, to determine what actions need to be taken to manually configure the US East Coast database as the primary one for each cluster and redesign the replication topology.  This was not easy, because by this time the Western database cluster had been taking entries from the application level for almost 40 minutes.  In addition, in the eastern cluster, there were several seconds of records that were not replicated to the west and did not allow the new records to replicate back to the east. <br><br>  Protecting the confidentiality and integrity of user data is GitHub's highest priority.  Therefore, we decided that more than 30 minutes of data recorded in the western data center, leave us only one solution to the situation in order to save this data: transfer forward (failing-forward).  However, applications in the east, which depend on writing information to the Western MySQL cluster, are currently unable to cope with the additional delay due to the transfer of most of their database calls back and forth.  This decision will lead to the fact that our service will become unsuitable for many users.  We believe that the long-term degradation of service quality was worth it to ensure the consistency of our users' data. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a66/119/b52/a66119b52dbb23111ddfe47bca1194d8.png"><br>  <i><font color="gray">In the wrong topology, replication from West to East is disrupted, and applications cannot read data from current replicas, because they depend on low latency to maintain transaction performance.</font></i> <br><br><h4>  10/21/2018 23:19 UTC </h4><br>  Queries about the state of database clusters have shown that it is necessary to stop the execution of tasks that write push-type metadata.  We made a choice and deliberately went to a partial degradation of the service, pausing web builds and assembling GitHub Pages, so as not to jeopardize the data that we already received from users.  In other words, the strategy was to prioritize: data integrity instead of website usability and quick recovery. <br><br><h4>  10.22.2018, 00:05 UTC </h4><br>  Engineers from the response team began to develop a plan for eliminating data inconsistencies and launched a failover procedure for MySQL.  The plan was to restore the files from the backup, synchronize the replicas at both sites, return to the stable service topology, and then resume the processing of jobs in the queue.  We updated the status to let users know that we are going to perform managed failover on the internal storage system. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/eb4/cb7/a34/eb4cb7a34add07f272b99fc93f161d56.png"><br>  <i><font color="gray">The recovery plan assumed the transfer forward, recovery from backups, synchronization, roll back and working off the delay, before returning to the green status</font></i> <br><br>  Although MySQL backups are made every four hours and stored for many years, but they lie in a remote cloud storage blob objects.  Recovery of several terabytes from backup took several hours.  Long was the transfer of data from the remote backup service.  Most of the time was spent on the process of unpacking, checking the checksum, preparing and uploading large backup files to the freshly prepared MySQL servers.  This procedure is tested daily, so everyone was well aware of how long the recovery would take.  However, before this incident, we never had to completely rebuild the entire cluster from a backup.  Other strategies have always worked, such as deferred replicas. <br><br><h4>  10/22/2018 00:41 UTC </h4><br>  By this time, a backup process was initiated for all affected MySQL clusters, and engineers were tracking progress.  At the same time, several groups of engineers studied ways to speed up the transfer and recovery without further degradation of the site or the risk of data damage. <br><br><h4>  10.22.2018 06:51 UTC </h4><br>  Several clusters in the eastern data center completed the restoration from backups and began to replicate new data with the West Coast.  This slowed down the loading of pages that performed the write operation across the entire country, but reading the pages from these clusters of the database returned actual results if the read request got into the just replica.  Other larger DB clusters continued to recover. <br><br>  Our teams identified a recovery method directly from the West Coast to overcome bandwidth limitations caused by booting from external storage.  It has become almost 100% clear that the recovery will be completed successfully, and the time to create a healthy replication topology depends on how long the catch-up replication takes.  This estimate was linearly interpolated based on the available replication telemetry, and the status page was <a href="https://twitter.com/githubstatus/status/1054264047250608130">updated</a> to set the wait at two hours as the estimated recovery time. <br><br><h4>  10.22.2018 07:46 UTC </h4><br>  GitHub posted an <a href="https://blog.github.com/2018-10-21-october21-incident-report">informational blog post</a> .  We ourselves use GitHub Pages, and all the assemblies paused a few hours ago, so the publication required extra effort.  Sorry for the delay.  We intended to send this message much earlier and in the future we will ensure the publication of updates in the face of such restrictions. <br><br><h4>  10.22.2018, 11:12 UTC </h4><br>  All primary databases are again transferred to the East.  This led to the site becoming much more responsive, since the records were now sent to the database server located in the same physical data center as our application layer.  Although this significantly improved performance, there are still dozens of database read replicas that lagged behind the main copy by several hours.  These deferred replicas caused users to see inconsistent data when interacting with our services.  We distribute the read load across a large pool of read replicas, and each request to our services has good chances of getting into the read replica with a delay of several hours. <br><br>  In fact, the time to catch a lagging replica is reduced exponentially rather than linearly.  When users in the US and Europe woke up, the recovery process took longer than expected due to the increased load on the records in the database clusters. <br><br><h4>  10.22.2018 13:15 UTC </h4><br>  We were approaching the peak load on GitHub.com.  The response team discussed how to proceed.  It was clear that the lag of replication to a consistent state is increasing, not decreasing.  Earlier, we started preparing additional MySQL read replicas in the East Coast public cloud.  As soon as they became available, it became easier to distribute the flow of read requests among several servers.  Reducing the average load on the replica reads accelerated replication. <br><br><h4>  10.22.2018, 16:24 UTC </h4><br>  After synchronization of the replicas, we returned to the original topology, eliminating the problems of delay and availability.  As part of a conscious decision to prioritize data integrity over quickly correcting the situation, we <a href="https://twitter.com/githubstatus/status/1054408042836606977">retained the red status of the</a> site when we began to process the accumulated data. <br><br><h4>  10.22.2018, 16:45 UTC </h4><br>  At the recovery stage, it was necessary to balance the increased load associated with the lag, potentially overloading our ecosystem partners with notifications and returning to one hundred percent efficiency as soon as possible.  More than five million hook events and 80 thousand requests for building web pages remained in the queue. <br><br>  When we re-enabled the processing of this data, we processed about 200,000 useful tasks with webheads, which exceeded the internal TTL and were discarded.  Upon learning of this, we stopped processing and launched an increase in TTL. <br><br>  To avoid further reducing the reliability of our status updates, we left the degradation status until we finish processing the accumulated amount of data and make sure that the services have clearly returned to the normal level of performance. <br><br><h4>  10.22.2018, 23:03 UTC </h4><br>  All incomplete webhuk events and Pages assemblies are processed, and the integrity and proper operation of all systems is confirmed.  Site status <a href="https://twitter.com/githubstatus/status/1054508689560870912">updated to green</a> . <br><br><h1>  Next steps </h1><br><h4>  Correction of data inconsistencies </h4><br>  During recovery, we fixed MySQL binary logs with records in the main data center that were not replicated to the west.  The total number of such records is relatively small.  For example, in one of the busiest clusters, there are only 954 entries in those seconds.  We are currently analyzing these logs and determining which records can be automatically reconciled and which will require user assistance.  Several teams participate in this work, and our analysis has already determined the category of records that the user then repeated - and they were successfully preserved.  As stated in this analysis, our main goal is to preserve the integrity and accuracy of the data you store on GitHub. <br><br><h4>  Communication </h4><br>  Trying to convey important information to you during the incident, we made several public assessments of the recovery time based on the processing speed of the accumulated data.  In retrospect, our estimates did not take into account all variables.  We apologize for the confusion and will strive to provide more accurate information in the future. <br><br><h4>  Technical measures </h4><br>  In the course of this analysis, a number of technical measures were identified.  The analysis continues; the list may be updated. <br><br><ul><li>  Adjust Orchestrator configuration to prevent primary databases from moving out of the region.  Orchestrator worked according to the settings, although the application level did not support this topology change.  The choice of a leader within a region is usually safe, but the sudden appearance of a delay due to the transfer of traffic across the continent was the main cause of this incident.  This is an emergent, new behavior of the system, because before we did not encounter the internal section of the network of this magnitude. </li><li>  We have accelerated the migration to the new status reporting system, which will provide a more suitable platform for discussing active incidents with clearer and clearer wording.  Although many parts of GitHub were available throughout the entire incident, we could only select green, yellow, and red statuses for the entire site.  We admit that it does not give an exact picture: what works and what does not.  The new system will display the various components of the platform so that you know the status of each service. </li><li>  A few weeks before this incident, we launched a corporate-wide engineering initiative to support the maintenance of GitHub traffic from several data centers in an active / active / active architecture.  The objective of this project is to support N + 1 redundancy at the data center level in order to withstand the failure of a single data center without external intervention.  This is a lot of work and will take some time, but we believe that several well-connected data centers in different regions will provide a good compromise.  The last incident pushed this initiative even more. </li><li>  We will take a more active position in checking our assumptions.  GitHub is growing rapidly and has accumulated a considerable amount of complexity over the past decade.  It is becoming increasingly difficult to capture and convey to the new generation of employees the historical context of compromises and decisions taken. </li></ul><br><h4>  Organizational measures </h4><br>  This incident greatly influenced our idea of ‚Äã‚Äãthe reliability of the site.  We learned that tightening operational control or improving response time are insufficient guarantees of reliability in such a complex system of services as ours.  To support these efforts, we will also begin the systematic practice of testing failure scenarios before they occur in reality.  This work includes deliberate troubleshooting and the use of chaos engineering tools. <br><br><h1>  Conclusion </h1><br>  Know how you rely on GitHub in your projects and business.  We more than anyone care about the availability of our service and the safety of your data.  The analysis of this incident will continue to find an opportunity to serve you better and to justify your trust. </div><p>Source: <a href="https://habr.com/ru/post/428409/">https://habr.com/ru/post/428409/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../428393/index.html">Webinar "Test environment 2.0 in the cloud and how to learn how to cook them"</a></li>
<li><a href="../428395/index.html">The book "The theoretical minimum for Big Data. All you need to know about big data</a></li>
<li><a href="../428401/index.html">Development processes through the eyes of exploitation. A look on the other side of the barricade</a></li>
<li><a href="../428403/index.html">Event digest for HR-specialists in IT in November 2018</a></li>
<li><a href="../428407/index.html">6 typical plots of world literature</a></li>
<li><a href="../428411/index.html">Radar technology: a list of languages, tools and platforms that have passed through the hands of Lamoda</a></li>
<li><a href="../428413/index.html">Cooling systems in data centers Selectel</a></li>
<li><a href="../428415/index.html">TP-Link Omada OC200 Cloud Controller Overview</a></li>
<li><a href="../428417/index.html">MatLab / Octave machine learning: examples of algorithms supported by formulas</a></li>
<li><a href="../428419/index.html">Drag and Swipe in RecyclerView. Part 2: Drag and Drop Controllers, Grid, and Custom Animations</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>