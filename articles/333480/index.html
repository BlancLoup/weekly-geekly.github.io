<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DBMS for 1C Fresh. Quickly. Reliably Is free</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="For several years now, the button has been using 1C Fresh publication technology for working with accounting databases. We have recently written about...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DBMS for 1C Fresh. Quickly. Reliably Is free</h1><div class="post__text post__text-html js-mediator-article">  For several years now, the button has been using 1C Fresh publication technology for working with accounting databases.  We have recently <a href="https://habrahabr.ru/company/knopka/blog/324662/">written</a> about our operating experience.  Our use of PostgreSQL as a DBMS aroused interest and a number of questions, so we decided to talk about this in more detail. <br><br><img src="https://habrastorage.org/web/86d/7f7/b0b/86d7f7b0b5c0428a9a777d8699a50b6f.png" alt="image"><br><a name="habracut"></a><br><br><h2>  Introduction </h2><br>  At the very beginning, we used the traditional MS SQL Server 1C as a DBMS.  When our Fresh grew to the need for horizontal scaling, it became clear that for economic reasons an alternative to the Microsoft product is needed.  Then we carefully looked in the direction of PostgreSQL, especially since experts from 1C recommended it for use in the 1C Fresh installation.  We made a simple comparison of performance on standard operations and found that the base of Enterprise Accounting (BP) 3.0, containing about 600 areas, works just as well.  At that time, we switched to a schema of several virtual machines on Linux with an application server and a DBMS.  About this a bit told in the <a href="https://infostart.ru/public/502732/">article</a> .  But for various reasons, a year later, an application server on Windows and a DBMS with several information bases (IB) on Linux came to the scheme.  But we dare to assure you that we had no problems with the operation of the application server on Linux, the changes are related to some of our other work features. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      So, PostgreSQL was chosen as the database server.  In this article, Button tells how we made friends with one another, and how it all worked. <br><br>  In the first installation, we used PostgreSQL 8.4.3-3.1C ready-made deb packages provided by 1C, since this was the recommended version for use.  But unfortunately, we encountered a dependency problem when installing on Debian Wheezy, which is an oldstable release at that time, containing apache2.2 packages (apache2.4 support in 1C was not there yet).  At that time, we kept the DBMS and the application server on the same host, so we had to use oldstable.  To install this version of PostgreSQL, libc6 from Debian Jessie was required.  As a result of such a cross-over, a system with which you can‚Äôt do anything special, even the installation of an NFS client, broke up the dependencies necessary for work.  But switching to another Linux distribution was not strategically advantageous for us.  When we began to actively use microservices (for simplicity, we call them <a href="http://knopka.com/blog/157">robots</a> ) that interacted with the application server through a COM connection, dangling connections began to appear in the databases, which led to a memory leak.  This problem was solved by switching to a new version of PostgreSQL, but at that moment, 1C has not yet released the distribution of this version. <br><br>  We made a willful decision to switch to using the PostgreSQL 9.6 assembly from the <a href="https://postgrespro.ru/products/1c_build">Postgres Professional</a> repository.  Problems with dependencies and memory leaks are left behind, and we began to address issues of scalability, load balancing, and increased availability time.  Currently, 1C specialists have already updated PostgreSQL assemblies, the most recent 9.6.3, this is a very current version and it is safer to use it.  According to information from 1C, new assemblies will be released promptly. <br><br>  We are currently working on Debian Jessie and we will continue to look at all the issues in this distribution. <br><br>  We monitor the state of our system using Zabbix, it looks like this: <br><br><img src="https://habrastorage.org/web/a5c/355/24a/a5c35524a8924c81ab3fa3ac9b6b513a.png"><br><br>  The charts still contain old servers, but the product has already been transferred from them to PostgreSQL 9.6. <br><br>  And we measure the number of transactions: <br><br><img src="https://habrastorage.org/web/ca2/97d/e73/ca297de736be4247846663e8f3c4e13a.png" alt="image"><br><br>  And the number of lines added, modified and deleted: <br><br><img src="https://habrastorage.org/web/57d/856/6b2/57d8566b27994a44992458b7ab79057a.png" alt="image"><br><br>  In the operation of the DBMS, we closely monitor our performance and all the settings below are born from real operation and observations.  We now have 2 main DBMS servers, each of which is allocated 8 cores and 40 GB of memory.  Disks with databases are located on the SSD.  These resources are enough for us to maintain 7 IB BP 3.0 with the data sharing mode enabled (200-800 areas in one database).  With this configuration, we have achieved a good resource utilization on the one hand and a good margin for growth and peak loads on the other. <br><br><h2>  Clusters in PostgreSQL </h2><br>  When hosting multiple IBs on the same virtual machine, we are faced with administrative complexity.  Each IB wanted its own server settings, any DBMS shutdown turned off all databases, and WAL archiving didn‚Äôt make any sense at all, because recovery through this mechanism would roll back all databases on the server. <br><br>  To begin, each of our databases was placed in a separate PostgreSQL cluster, which allowed them to flexibly manage them, run several independent copies of PostgreSQL on one host, configure streaming replication, archive WAL, and restore data at a time (PiTR). <br><br>  In practice, the deployment process looks like this: <br><br>  We have a machine, with Debian 8 Jessie installed, PostgreSQL 9.6 already installed from the Postgres Professional repository. <br><br>  Create our new cluster: <br><br><pre><code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_createcluster 9.6 -p 5433 -d /databases/db_01</span></span></code> </pre> <br>  After that, the / db_01 directory will be created in / databases, which will house the data files, and the configuration files in /etc/postgresql/9.6/db_01/.  The cluster will use PostgreSQL version 9.6. Its instance will be launched on port 5433. Now the cluster is not running, you can check it with the command: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_lsclusters</span></span></code> </pre> <br>  We make changes to the configuration file of the postgresql.conf cluster, approximate parameter values ‚Äã‚Äãcan be obtained using <a href="http://pgtune.leopard.in.ua/">PgTune</a> .  What one or another parameter is responsible for is described in detail in the <a href="https://postgrespro.ru/docs/postgresql/9.6/index.html">documentation</a> , we will only explain those parameters that PgTune does not take into account. <br><br>  <i>max_parallel_workers_per_gather</i> it is useful to set a value equal to the number of workers that will be used in parallel for sequential reading of tables.  This will speed up many read operations.  Based on the number of cores on your host, exceeding this number will not give a performance boost, but on the contrary, degradation will occur. <br><br>  <i>max_locks_per_transaction is</i> by default 64, but during the work the value was raised to 300. The parameter assigns the number of object locks allocated for the transaction.  If a slave server is used, this value on it must be equal to or greater than on the master server. <br><br>  If the file system is not productive enough, place pg_xlog on a separate storage. <br><br>  For example, the config file of one cluster: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># ----------------------------- # PostgreSQL configuration file # ----------------------------- #------------------------------------------------------------------------------ # FILE LOCATIONS #------------------------------------------------------------------------------ data_directory = '/db/disk_database_db_01/db_01' hba_file = '/etc/postgresql/9.6/db_01/pg_hba.conf' ident_file = '/etc/postgresql/9.6/db_01/pg_ident.conf' external_pid_file = '/var/run/postgresql/9.6-db_01.pid' #------------------------------------------------------------------------------ # CONNECTIONS AND AUTHENTICATION #------------------------------------------------------------------------------ listen_addresses = '*' port = 5433 max_connections = 100 unix_socket_directories = '/var/run/postgresql' ssl = true ssl_cert_file = '/etc/ssl/certs/ssl-cert-snakeoil.pem' ssl_key_file = '/etc/ssl/private/ssl-cert-snakeoil.key' #------------------------------------------------------------------------------ # RESOURCE USAGE (except WAL) #------------------------------------------------------------------------------ shared_buffers = 1536MB work_mem = 7864kB maintenance_work_mem = 384MB dynamic_shared_memory_type = posix shared_preload_libraries = 'online_analyze, plantuner,pg_stat_statements' #------------------------------------------------------------------------------ # WRITE AHEAD LOG #------------------------------------------------------------------------------ wal_level = replica wal_buffers = 16MB max_wal_size = 2GB min_wal_size = 1GB checkpoint_completion_target = 0.9 #------------------------------------------------------------------------------ # REPLICATION #------------------------------------------------------------------------------ max_wal_senders = 2 wal_keep_segments = 32 #------------------------------------------------------------------------------ # QUERY TUNING #------------------------------------------------------------------------------ effective_cache_size = 4608MB #------------------------------------------------------------------------------ # RUNTIME STATISTICS #------------------------------------------------------------------------------ stats_temp_directory = '/var/run/postgresql/9.6-db_01.pg_stat_tmp' #------------------------------------------------------------------------------ # CLIENT CONNECTION DEFAULTS #------------------------------------------------------------------------------ datestyle = 'iso, dmy' timezone = 'localtime' lc_messages = 'ru_RU.UTF-8' # locale for system error message # strings lc_monetary = 'ru_RU.UTF-8' # locale for monetary formatting lc_numeric = 'ru_RU.UTF-8' # locale for number formatting lc_time = 'ru_RU.UTF-8' # locale for time formatting default_text_search_config = 'pg_catalog.russian' #------------------------------------------------------------------------------ # LOCK MANAGEMENT #------------------------------------------------------------------------------ max_locks_per_transaction = 300 # min 10 #------------------------------------------------------------------------------ # VERSION/PLATFORM COMPATIBILITY #------------------------------------------------------------------------------ escape_string_warning = off standard_conforming_strings = off #------------------------------------------------------------------------------ # CUSTOMIZED OPTIONS #------------------------------------------------------------------------------ online_analyze.threshold = 50 online_analyze.scale_factor = 0.1 online_analyze.enable = on online_analyze.verbose = off online_analyze.min_interval = 10000 online_analyze.table_type = 'temporary' plantuner.fix_empty_table = false</span></span></code> </pre><br>  It is time to start the newly created cluster and start using it: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_ctlcluster 9.6 db_01 start</span></span></code> </pre> <br>  To connect via a non-standard port in the 1C Enterprise server administration snap-in, specify the following in the parameters of the information base in the "Database server" field: <br><br> <code>hostname port=5433</code> <br> <br>  It is also important not to forget about regular base maintenance.  VACUUM and ANALYZE are very helpful. <br><br><h2>  Stream replication </h2><br>  Implementing a standby server is pretty simple. <br><br>  Set up a master server <br>  We make changes to the postgresql.conf configuration file, not forgetting that we are configuring our new cluster and the file is located in /etc/postgresql/9.6/db_01/ <br><br> <code>listen_addresses = '*' <br> wal_level = replica <br> max_wal_senders = 3 <br> wal_keep_segments = 128</code> <br> <br>  Create a new replica role: <br><br><pre> <code class="bash hljs">postgres=<span class="hljs-comment"><span class="hljs-comment"># CREATE ROLE replica WITH REPLICATION PASSWORD 'MyBestPassword' LOGIN;</span></span></code> </pre> <br>  And allow the connection for the slave server, correcting the file pg_hba.conf <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># TYPE DATABASE USER ADDRESS METHOD host replication replica 192.168.0.0/24 md5</span></span></code> </pre><br>  After that, you need to restart the cluster: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_ctlcluster 9.6 db_01 restart</span></span></code> </pre><br>  Now it's time to set up the slave server.  Suppose that its configuration is identical to the master, a cluster has been created with the same name, the data files are in a directory that is mounted similarly to the master.  Stop the cluster on the slave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_ctlcluster 9.6 db_01 stop</span></span></code> </pre><br>  In the postgresql.conf file, enable standby mode: <br> <code>hot_standby = on <br></code> <br><br>  Clean up the directory with the data files on the slave / databases / db_01 / and make a copy of the current status of the wizard on the slave: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># cd /databases/db_01 # rm -Rf /databases/db_01 # su postgres -c "pg_basebackup -h master.domain.local -p 5433 -U replica -D /databases/db_01 -R -P --xlog-method=stream"</span></span></code> </pre><br>  A recovery.conf file will be created, correct it if necessary: <br><br> <code>standby_mode = 'on' <br> primary_conninfo = 'user=replica password=MyBestPassword host=master.domain.local port=5433 sslmode=prefer sslcompression=1 krbsrvname=postgres'</code> <br> <br>  In this case, we do not failover, which will automatically take on the role of the master.  For the replica to start working as a wizard, just rename the recovery.conf file and restart the cluster. <br><br>  We start the slave cluster: <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_ctlcluster 9.6 db_01 start</span></span></code> </pre><br>  Check that replication is on.  The wal sender process will appear on the master, and the wal receiver will appear on the slave.  More detailed information about replication can be obtained by running on the wizard: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> *,pg_xlog_location_diff(s.sent_location,s.replay_location) byte_lag <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> pg_stat_replication s;</code> </pre><br>  On the slave, you can follow the value that shows when the last replication took place: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">now</span></span>()-pg_last_xact_replay_timestamp();</code> </pre> <br><h2>  WAL Backup and Archiving </h2><br>  Initially, the backup was done by pg_dump once a day, which, together with the internal backup mechanism of the areas in 1C Fresh, gave an acceptable backup scheme.  But sometimes there are situations when a large amount of work has been done between the moment of the last backup and the moment of the accident.  In order not to lose these changes, archiving WAL files will help us. <br><br>  To enable WAL archiving, three conditions must be met: <br>  Wal_level must be replica or higher <br>  Archive_mode = on parameter <br>  The archive_command contains shell commands, for example: <br><br> <code>archive_command = 'test ! -f /wal_backup/db_01/%f &amp;&amp; cp %p /wal_backup/db_01/%f'</code> <br> <br>  So we copy the archived WAL segments into the / wal_backup / db_01 / directory <br><br>  For the changes to take effect, a cluster restart is required.  So, when the next WAL segment is ready, it will be copied, but in order to use it during restoration, you need a basic copy of the cluster, to which changes from the archive WAL segments will be applied.  Using a simple script, we will create a base copy and put it next to the WAL files. <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment">#!/bin/bash db="db_01" wal_arch="/wal_backup" datenow=`date '+%Y-%m-%d %H:%M:%S'` mkdir /tmp/pg_backup_$db su postgres -c "/usr/bin/pg_basebackup --port=5433 -D /tmp/pg_backup_$db -Ft -z -Xf -R -P" test -e ${wal_arch}/$db/base.${datenow}.tar.gz &amp;&amp; rm ${wal_arch}/$db/base.${datenow}.tar.gz cp /tmp/pg_backup_$db/base.tar.gz ${wal_arch}/$db/base.${datenow}.tar.gz</span></span></code> </pre><br>  In order to restore a backup for a certain period of time (PiTR), stop the cluster and delete the contents <br><br><pre> <code class="bash hljs"><span class="hljs-comment"><span class="hljs-comment"># pg_ctlcluster 9.6 db_01 stop # rm -Rf /databases/db_01</span></span></code> </pre><br>  Then we unpack the base copy, check the rights and edit (or create) the recovery.conf file with the following content: <br><br> <code>restore_command = 'cp /wal_backup/db_01/%f %p' <br> recovery_target_time = '2017-06-12 21:33:00 MSK' <br> recovery_target_inclusive = true <br></code> <br><br>  Thus, we will restore the data to the point in time 2017-06-12 21:33:00 MSK, and the breakpoint will be immediately after reaching this time. <br><br><h2>  Conclusion </h2><br>  In combat operation, a bunch of 1C Fresh and PostgreSQL showed themselves worthily, of course, not everything was smooth right away, but we managed.  The DBMS works perfectly under load, which is generated by about a hundred users, a lot of background tasks, and even our robots load and issue extracts day and night, create documents from other tools, analyze the state of several thousand areas. <br><br>  Picture about the reliability of the 1C Fresh and PostgreSQL bundles: <br><br><img src="https://habrastorage.org/web/5a8/0c8/0e8/5a80c80e8004421da2d80de0163d2197.png" alt="image"><br><br>  We have more than 350 GB of information databases that feel great, grow and develop.  What and you want! </div><p>Source: <a href="https://habr.com/ru/post/333480/">https://habr.com/ru/post/333480/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../333468/index.html">Installing MS SQL ODBC Driver for Linux and building a plugin for Qt 5.9</a></li>
<li><a href="../333470/index.html">Getting started in Kubernetes with Minikube</a></li>
<li><a href="../333474/index.html">Let's talk about micro-optimizations on the example of Tizen code</a></li>
<li><a href="../333476/index.html">Tuning typical roles of Windows. Part Two: Terminal Server and Deduplication</a></li>
<li><a href="../333478/index.html">Upgrading Fujitsu ETERNUS DX Entry Storage Systems</a></li>
<li><a href="../333482/index.html">Hazards in measuring productivity</a></li>
<li><a href="../333484/index.html">Setting up the development environment: needlework circle (Part 1)</a></li>
<li><a href="../333486/index.html">Calls between browsers: WebRTC pitfalls</a></li>
<li><a href="../333488/index.html">Read your product code. Whole</a></li>
<li><a href="../333490/index.html">Machine learning and dark matter search: competition from CERN and Yandex</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>