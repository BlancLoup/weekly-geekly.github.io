<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>JD Humanoid Robot and Microsoft Cognitive Services</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Today we will tell you about one interesting project. It used Microsoft Cognitive Services , which makes it easy to apply artificial intelligence tech...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>JD Humanoid Robot and Microsoft Cognitive Services</h1><div class="post__text post__text-html js-mediator-article">  Today we will tell you about one interesting project.  It used <a href="https://azure.microsoft.com/en-us/services/cognitive-services/%3F">Microsoft Cognitive Services</a> , which makes it easy to apply artificial intelligence technology by calling the REST API (and no training is needed).  And all this with the example of the cute robot JD Humanoid.  More under the cut! <br><br><img src="https://habrastorage.org/webt/_l/g3/jp/_lg3jpgfmfyagld5dpvesxwhlfy.jpeg"><a name="habracut"></a><br><br>  Cognitive Services also contains client libraries for various programming languages, which further simplifies their use.  We decided to integrate these services into an application created to control the <a href="https://www.ez-robot.com/Shop/AccessoriesDetails.aspx%3FproductNumber%3D31">JD Humanoid robot of EZ</a> (hereinafter, we will call it simply ‚ÄúEZ robot‚Äù).  We chose this particular humanoid robot, since it is quite simple to assemble.  In addition, .NET, UWP SDK and even Mono SDK are attached to it, which opens up wide possibilities for its implementation with individual settings. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      The package also includes <a href="https://www.ez-robot.com/Tutorials/Help.aspx%3Fid%3D207">the EZ Builder application</a> , which allows you to control the EZ robot and implement specific scenarios based on built-in functional plug-ins and Blockley blocks.  It is mainly intended for educational purposes, but it also has a ‚Äúmovement creator‚Äù function, which allows you to create robot movements and export them for use in applications created using the SDK.  Of course, this application is featuring a starting point for setting up and getting started with any EZ robot. <br><br>  The application is based on the existing <a href="https://github.com/ppedro74/ezrobot-playground/tree/master/sdk-applications/EZFormApplication">Windows Forms project</a> , which uses the EZ robot's own capabilities and efficiently implements the work with the camera.  Taking this application as a basis, we expanded it and connected it to the following Cognitive Services: Face API, Emotion API, Speech API, Voice Recognition API, Language Understanding Intelligent Service, Speaker Recognition API, Computer Vision API, Custom Vision API.  Due to this, the robot has new features: <br><br><ul><li>  Voice command recognition - in addition to the Win Form application buttons that trigger specific actions of the robot, we added speech recognition and natural language understanding functions so that our EZ robot understands the commands spoken aloud. </li><li>  Face Recognition and Identification - EZ robot is able to recognize faces by several parameters, as well as identify people by faces. </li><li>  Emotion Recognition - when recognizing faces, the EZ robot also determines emotions. </li><li>  Speaker Recognition - EZ is able to recognize people by voice. </li><li>  Computer vision - EZ robot can also describe the environment. </li><li>  Recognition of objects by their own parameters - the EZ robot is able to recognize specific objects placed in its field of vision. </li></ul><br>  The cognitive capabilities of the robot are demonstrated in <a href="https://youtu.be/vf89wZiuzzE">this</a> video. <br><br><h2>  Working with EZ Robot SDK </h2><br>  Later in this document, we briefly describe how to work with the SDK of the EZ robot, as well as describe in detail the implementation scenarios using Cognitive Services. <br><br><h4>  Connect to an EZ robot </h4><br>  The main prerequisite for working with the application is the ability to connect to the robot from the code of our application, call movements and receive incoming images from the camera.  The application runs on the developer's computer, since the EZ robot does not have its own runtime and storage environment where it would be possible to place and run the application. <br><br>  Thus, it works on a computer that connects to the EZ robot via a WiFi network, directly through an access point turned on by the robot itself (AP mode), or via a WiFi network created by another router (client mode).  We chose the second option because it supports an Internet connection when developing and launching an application.  Network settings when working with an EZ robot are described in detail <a href="https://www.ez-robot.com/Tutorials/UserTutorials/92/1">here</a> .  When connected to the WiFi network, the EZ robot is assigned an IP address, which is later used to connect to the robot from the application.  When using the SDK, the procedure is as follows: <br><br><pre><code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> EZ_B; <span class="hljs-comment"><span class="hljs-comment">//   EZ   SDK var ezb = new EZB(); this.ezb.Connect("robotIPAddress");</span></span></code> </pre> <br>  Since the EZ robot camera is an isolated network device, we must connect to it for further use. <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> camera = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Camera(<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.ezb); <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.camera.StartCamera(<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> ValuePair(<span class="hljs-string"><span class="hljs-string">"EZB://"</span></span> + <span class="hljs-string"><span class="hljs-string">"robotIPAddress"</span></span>), CameraWidth, CameraHeight);</code> </pre> <br>  The official <a href="https://www.ez-robot.com/EZ-Builder/SDKHelp.aspx">documentation of the EZ robot SDK package</a> contains detailed examples of calling the special functions of the EZ robot, which become available after connecting to it. <br><br><h4>  Creating robot movements </h4><br>  The SDK allows you to interact with the robot's servomotor and control its movements.  In order to create motion, it is necessary to specify frames (specific positions) and actions consisting of a set of such frames.  Implementing it manually in code is not very easy, but in the EZ Builder application you can define frames, create actions from them, and then export them to code and use them in the application.  To do this, we need to create a new project in EZ Builder, add the Auto Position plug-in and click the button with the gear image. <br><br><img src="https://habrastorage.org/webt/bg/r1/em/bgr1emqxoebfa0klkeul8w0zno0.jpeg"><br><br>  <i>Figure 1. Auto Position Plugin</i> <br><br>  In the future, you can create new frames in the Frames panel, changing the angles of the robot servos.  The next step is to create the necessary movements from the existing frames on the Action panel. <br><br><img src="https://habrastorage.org/webt/72/ql/oh/72qlohm6d4ttie4dx3mlq4wbd6o.jpeg"><br><br>  <i>Figure 2. Auto Position frames</i> <br><br><img src="https://habrastorage.org/webt/kt/g_/cs/ktg_cs2yhw-np6j4lfzf5-itn-8.jpeg"><br><br>  <i>Figure 3. Auto Position actions</i> <br><br>  The created action can be exported via the import / export toolbar. <br><br><img src="https://habrastorage.org/webt/pm/ou/81/pmou81wazzbl4blydesjz-nhfgi.jpeg"><br><br>  <i>Figure 4. Exporting the source code of the Auto Position plugin</i> <br><br>  When you finish exporting the action, you can copy the code and paste it into your application.  If we use different positions, we should rename the class AutoPositions, giving it a name that accurately reflects the type of movement.  Then it can be used in the code as follows: <br><br><pre> <code class="cs hljs"><span class="hljs-comment"><span class="hljs-comment">// WavePositions    AutoPositions   private WavePositions wavePosition; //    EZ private void EzbOnConnectionChange(bool isConnected) { this.ezbConnectionStatusChangedWaitHandle.Set(); if (isConnected) { //      WavePosition wavePosition = new WavePositions(ezb); } } //   Waving private async void Wave() { wavePosition.StartAction_Wave(); //   Wave   5  await Task.Delay(5000); wavePosition.Stop(); //     ezb.Servo.ReleaseAllServos(); }</span></span></code> </pre> <br><h4>  Obtaining images from the camera </h4><br>  Since in the application we use images from the robot's camera as input data when calling Cognitive Services, we need to find a way to get these images.  It is done this way. <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">var</span></span> currentBitmap = camera.GetCurrentBitmap; MemoryStream memoryStream = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MemoryStream(); currentBitmap.Save(memoryStream, System.Drawing.Imaging.ImageFormat.Jpeg); memoryStream.Seek(<span class="hljs-number"><span class="hljs-number">0</span></span>, SeekOrigin.Begin); <span class="hljs-comment"><span class="hljs-comment">//    ,    Cognitive Services</span></span></code> </pre> <br><h4>  Robot voice functions </h4><br>  As already mentioned, the application runs on the developer‚Äôs computer and simply sends commands to the robot.  The robot does not have its own runtime.  If you need to synthesize speech (in other words, you want the robot to utter a couple of phrases), then you need to choose one of the two options offered by the SDK.  The first option involves the use of a standard audio device: the sound will be played by the developer‚Äôs computer, not by the speaker of the robot.  This option is useful when you need a robot to speak through a computer speaker, for example, during presentations.  However, in most cases, it is desirable that the sound is reproduced by the robot itself.  Below is the implementation of both variants of calling the audio function using the SDK: <br><br><pre> <code class="cs hljs"><span class="hljs-comment"><span class="hljs-comment">//      ezb.SpeechSynth.Say(" "); //       ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(" "));</span></span></code> </pre> <br><h2>  Cognitive Services Integration </h2><br>  In this section, we will look at specific cognitive function scenarios implemented in the robot control application. <br><br><h4>  Recognition of voice commands </h4><br>  You can trigger the robot by giving voice commands through a microphone connected to the developer‚Äôs computer, since the EZ robot itself doesn‚Äôt have a microphone.  We first used the EZ robot SDK for speech recognition.  As it turned out, the recognition was not accurate enough, and the robot performed the wrong actions based on incorrectly understood commands.  To increase recognition accuracy and freedom of action when submitting commands, we decided to use the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech/getstarted/getstartedclientlibraries">Microsoft Speech API</a> , which converts speech to text, as well as the Language Understanding Intelligent Service ( <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/luis/home">LUIS</a> ) service to recognize the action required by a particular command.  Use the links to these products to get more information about them and get started. <br><br>  First you need to create an LUIS application, in which the necessary actions are attached to each team.  The process of creating a LUIS application is described in <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-get-started-create-app">this</a> guide to get started.  LUIS offers a web-based option where you can easily create an application and specify the desired actions.  If necessary, you can also create entities that the LUIS application will recognize by the commands sent to the service.  The result of exporting the LUIS application is contained in this repository in the <i>LUIS Mode</i> l folder. <br><br>  After preparing the LUIS application, we implement the following logic: waiting for voice commands, calling the Microsoft Speech API, calling the LUIS recognition service.  As a basis for this functionality, we used the <a href="https://github.com/Azure-Samples/Cognitive-Speech-STT-Windows">following</a> sample. <br><br>  It contains the logic of recognizing long and short phrases from a microphone or from a .wav file and then recognizing the action by LUIS or without it. <br><br>  We used the MicrophoneRecognitionClientWithIntent class, which contains functions for microphone command waiting, speech recognition, and the required action.  In addition, the call function for waiting for short phrases is performed using the SayCommandButton_Click descriptor. <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.CognitiveServices.SpeechRecognition; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">SayCommandButton_Click</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">object</span></span></span></span><span class="hljs-function"><span class="hljs-params"> sender, EventArgs e</span></span></span><span class="hljs-function">)</span></span> { WriteDebug(<span class="hljs-string"><span class="hljs-string">"---       ----"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.micClient = SpeechRecognitionServiceFactory.CreateMicrophoneClientWithIntentUsingEndpointUrl( <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.DefaultLocale, Settings.Instance.SpeechRecognitionApiKey, Settings.Instance.LuisEndpoint); <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.micClient.AuthenticationUri = <span class="hljs-string"><span class="hljs-string">""</span></span>; <span class="hljs-comment"><span class="hljs-comment">//     this.micClient.OnIntent += this.OnIntentHandler; this.micClient.OnMicrophoneStatus += this.OnMicrophoneStatus; //      this.micClient.OnPartialResponseReceived += this.OnPartialResponseReceivedHandler; this.micClient.OnResponseReceived += this.OnMicShortPhraseResponseReceivedHandler; this.micClient.OnConversationError += this.OnConversationErrorHandler; //     this.micClient.StartMicAndRecognition(); }</span></span></code> </pre> <br>  The command call logic uses the OnIntentHandler descriptor - here we analyze the response received from the LUIS service. <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">async</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">OnIntentHandler</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">object</span></span></span></span><span class="hljs-function"><span class="hljs-params"> sender, SpeechIntentEventArgs e</span></span></span><span class="hljs-function">)</span></span> { WriteDebug(<span class="hljs-string"><span class="hljs-string">"---   OnIntentHandler () ---"</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">dynamic</span></span> intenIdentificationResult = JObject.Parse(e.Payload); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> res = intenIdentificationResult[<span class="hljs-string"><span class="hljs-string">"topScoringIntent"</span></span>]; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> intent = Convert.ToString(res[<span class="hljs-string"><span class="hljs-string">"intent"</span></span>]); <span class="hljs-keyword"><span class="hljs-keyword">switch</span></span> (intent) { <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-string"><span class="hljs-string">"TrackFace"</span></span>: { <span class="hljs-comment"><span class="hljs-comment">//     ToggleFaceRecognitionEvent?.Invoke(this, null); break; } case "ComputerVision": { var currentBitmap = camera.GetCurrentBitmap; var cvc = new CustomVisionCommunicator(Settings.Instance.PredictionKey, Settings.Instance.VisionApiKey, Settings.Instance.VisionApiProjectId, Settings.Instance.VisionApiIterationId); var description = await cvc.RecognizeObjectsInImage(currentBitmap); ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(description)); break; } //...  / default: break; } }</span></span></code> </pre> <br><h4>  Recognition and identification of faces and emotions </h4><br>  To implement the function of recognizing faces and emotions, we used the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview">Face API</a> and <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/emotion/home">Emotion API</a> services.  Following the links above, you can learn more about these services and how to get started with them, get instructions on creating an API key and integrating services into your application. <br><br>  EZ is able to recognize faces using the SDK.  Calling Cognitive Services is not required.  However, this is only a basic type of face recognition without additional parameters (for example, age, gender, facial hair, etc.).  But we still use this local recognition function: it will help you to understand that there is a face in the image.  After that, we will use the Cognitive Services Face API to get additional face parameters.  This eliminates the need for unnecessary API calls. <br><br>  The EZ robot's face recognition function also provides information on the location of the face in the picture.  We decided to use this so that the robot turned its head and its camera was aimed directly at the face.  We borrowed this code from <a href="https://github.com/ppedro74/ezrobot-playground/tree/master/sdk-applications/EZFormApplication">the Win Form application</a> as the basis of our project.  At the same time, we added a sensitivity parameter that determines the speed of movement and adjustment of the position of the robot head. <br><br>  So, in order for the Face API to identify specific people, we need to create a group of such people, register them and train the recognition model.  We managed to do this effortlessly using the <b>Intelligent Kiosk Sampl</b> e application.  The application can be downloaded from <a href="https://github.com/Microsoft/Cognitive-Samples-IntelligentKiosk">github</a> .  Remember to use the same Face API key for the Intelligent Kiosk application and the robot application. <br><br>  For greater accuracy of face recognition, it is advisable to train the model on image samples taken by a camera that will be used later (the model will be trained on images of the same quality, which will improve the performance of the Face Identification API interface).  To do this, we implemented our own logic, the execution of which allows you to save images from the camera of the robot.  In the future, they will be used to train Cognitive Services models: <br><br><pre> <code class="cs hljs"><span class="hljs-comment"><span class="hljs-comment">//      var currentBitmap = camera.GetCurrentBitmap; currentBitmap.Save(Guid.NewGuid().ToString() + ".jpg", ImageFormat.Jpeg);</span></span></code> </pre> <br>  Next, we run the <i>HeadTracking</i> method, which performs the function of tracking, identifying and identifying individuals.  In short, this class is the first to determine whether the face of the robot is in front of the camera.  If so, then the position of the head of the robot changes accordingly (face tracking is performed).  Then, the <i>FaceApiCommunicator</i> method will be called, which, in turn, will invoke the Face API (face detection and identification) interfaces, as well as the Emotion API.  The last section processes the result obtained from the Cognitive Services APIs. <br><br>  In the case of face recognition, the robot says: ‚ÄúHello!‚Äù And adds the name of the person, except when the robot defines the expression of sadness on its face (using the Emotion API interface).  Then the robot tells a funny joke.  If it was not possible to identify the person, the robot simply says ‚Äúhello‚Äù.  At the same time, he distinguishes between men and women and builds phrases accordingly.  According to the results obtained from the Face API interface, the robot also makes an assumption regarding the age of the person. <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">async</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">HeadTracking</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!<span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.headTrackingActive) { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span>; } <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> faceLocations = <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.camera.CameraFaceDetection.GetFaceDetection(<span class="hljs-number"><span class="hljs-number">32</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (faceLocations.Length &gt; <span class="hljs-number"><span class="hljs-number">0</span></span>) { <span class="hljs-comment"><span class="hljs-comment">//        if (this.fpsCounter == 1) { foreach (var objectLocation in faceLocations) { this.WriteDebug(string.Format(" H:{0} V:{1}   ", objectLocation.HorizontalLocation, objectLocation.VerticalLocation)); } } } //  ,     if (faceLocations.Length == 0) { return; } //    ( ) var faceLocation = faceLocations.First(); var servoVerticalPosition = this.ezb.Servo.GetServoPosition(HeadServoVerticalPort); var servoHorizontalPosition = this.ezb.Servo.GetServoPosition(HeadServoHorizontalPort); //Track face var yDiff = faceLocation.CenterY - CameraHeight / 2; if (Math.Abs(yDiff) &gt; YDiffMargin) { if (yDiff &lt; -1 * RobotSettings.sensitivity) { if (servoVerticalPosition - ServoStepValue &gt;= mapPortToServoLimits[HeadServoVerticalPort].MinPosition) { servoVerticalPosition -= ServoStepValue; } } else if (yDiff &gt; RobotSettings.sensitivity) { if (servoVerticalPosition + ServoStepValue &lt;= mapPortToServoLimits[HeadServoVerticalPort].MaxPosition) { servoVerticalPosition += ServoStepValue; } } } var xDiff = faceLocation.CenterX - CameraWidth / 2; if (Math.Abs(xDiff) &gt; XDiffMargin) { if (xDiff &gt; RobotSettings.sensitivity) { if (servoHorizontalPosition - ServoStepValue &gt;= mapPortToServoLimits[HeadServoHorizontalPort].MinPosition) { servoHorizontalPosition -= ServoStepValue; } } else if (xDiff &lt; -1 * RobotSettings.sensitivity) { if (servoHorizontalPosition + ServoStepValue &lt;= mapPortToServoLimits[HeadServoHorizontalPort].MaxPosition) { servoHorizontalPosition += ServoStepValue; } } } this.ezb.Servo.SetServoPosition(HeadServoVerticalPort, servoVerticalPosition); this.ezb.Servo.SetServoPosition(HeadServoHorizontalPort, servoHorizontalPosition); //  //    API var currentBitmap = camera.GetCurrentBitmap; (var faces, var person, var emotions) = await FaceApiCommunicator.DetectAndIdentifyFace(currentBitmap); //  ,       if (person != null &amp;&amp; !ezb.SoundV4.IsPlaying) { //     if (emotions[0].Scores.Sadness &gt; 0.02) { ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream("   ,     .    !  .      .   ,       ". )); //,     Thread.Sleep(25000); } else { ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream("" + person.Name)); Wave(); } } //  ,      else if (faces != null &amp;&amp; faces.Any() &amp;&amp; !ezb.SoundV4.IsPlaying) { if (faces[0].FaceAttributes.Gender == "male") ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(", !     " + faces[0].FaceAttributes.Age)); else ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(", !     " + faces[0].FaceAttributes.Age)); Wave(); } }</span></span></code> </pre> <br>  The following is the <i>FaceApiCommunicator</i> code, which contains message exchange logic with Face API and Emotion API. <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.Common.Contract; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.Emotion; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.Face; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.Face.Contract; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Collections.Generic; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Drawing; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.IO; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Linq; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Text; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> System.Threading.Tasks; <span class="hljs-keyword"><span class="hljs-keyword">namespace</span></span> <span class="hljs-title"><span class="hljs-title">EZFormApplication.CognitiveServicesCommunicators</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">FaceApiCommunicator</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">const</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> FaceApiEndpoint = <span class="hljs-string"><span class="hljs-string">"https://westeurope.api.cognitive.microsoft.com/face/v1.0/"</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> List&lt;FaceResult&gt; personResults = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> List&lt;FaceResult&gt;(); <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> DateTime lastFaceDetectTime = DateTime.MinValue; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">async</span></span> Task&lt;(Face[] faces, Person person, Emotion[] emotions)&gt; DetectAndIdentifyFace(Bitmap image) { FaceServiceClient fsc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> FaceServiceClient(Settings.Instance.FaceApiKey, FaceApiEndpoint); EmotionServiceClient esc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> EmotionServiceClient(Settings.Instance.EmotionApiKey); <span class="hljs-comment"><span class="hljs-comment">//  //        Emotion[] emotions = null; Person person = null; Face[] faces = null; //     10  if (lastFaceDetectTime.AddSeconds(10) &lt; DateTime.Now) { lastFaceDetectTime = DateTime.Now; MemoryStream memoryStream = new MemoryStream(); image.Save(memoryStream, System.Drawing.Imaging.ImageFormat.Jpeg); //   memoryStream.Seek(0, SeekOrigin.Begin); faces = await fsc.DetectAsync(memoryStream, true, true, new List&lt;FaceAttributeType&gt;() { FaceAttributeType.Age, FaceAttributeType.Gender }); if (faces.Any()) { var rec = new Microsoft.ProjectOxford.Common.Rectangle[] { faces.First().FaceRectangle.ToRectangle() }; //  //  ; -           memoryStream = new MemoryStream(); image.Save(memoryStream, System.Drawing.Imaging.ImageFormat.Jpeg); memoryStream.Seek(0, SeekOrigin.Begin); // Emotion API       , //       ‚Äî   Emotion API      emotions = await esc.RecognizeAsync(memoryStream, rec); //  var groups = await fsc.ListPersonGroupsAsync(); var groupId = groups.First().PersonGroupId; //     var identifyResult = await fsc.IdentifyAsync(groupId, new Guid[] { faces.First().FaceId }, 1); var candidate = identifyResult?.FirstOrDefault()?.Candidates?.FirstOrDefault(); if (candidate != null) { person = await fsc.GetPersonAsync(groupId, candidate.PersonId); } } } return (faces, person, emotions); } } public class FaceResult { public string Name { get; set; } public DateTime IdentifiedAt { get; set; } } }</span></span></code> </pre> <br><h4>  Recognition of sound from the speaker </h4><br>  The application of the robot supports the identification of a person not only by the image of his face, but also by his voice.  This data is sent to the <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speaker-recognition/home">Speaker Recognition API</a> .  You can use the link provided for more information about this API. <br><br>  As in the case of Face API, Speaker Recognition requires a recognition model trained in voice information from the speakers used.  First you need to create a sound material for recognition in the format of wav.  For this, the code below will work.  Having created sound recognition material, we will use <a href="https://github.com/Microsoft/Cognitive-SpeakerRecognition-Windows">this application</a> as a sample.  With it, we will create profiles of people whom our robot should recognize by voice. <br><br>  It should be borne in mind that in the created profiles there is no user name field.  This means that you need to save a pair of created ProfileId and Name values ‚Äã‚Äãin the database.  In the application, we store this pair of values ‚Äã‚Äãas entries in a static list: <br><br><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> List&lt;Speaker&gt; ListOfSpeakers = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> List&lt;Speaker&gt;() { <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Speaker() { Name = <span class="hljs-string"><span class="hljs-string">"Marek"</span></span>, ProfileId = <span class="hljs-string"><span class="hljs-string">"d64ff595-162e-42ef-9402-9aa0ef72d7fb"</span></span> } };</code> </pre> <br>  In this way, we can create an entry in the .wav format and then send it to the Speaker Recognition service (to register or identify people).  We created a logic that allows you to record voice data in a .wav file.  To achieve this result from the .NET application, we use the winmm.dll interop assembly: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">WavRecording</span></span> { [DllImport(<span class="hljs-string"><span class="hljs-string">"winmm.dll"</span></span>, EntryPoint = <span class="hljs-string"><span class="hljs-string">"mciSendStringA"</span></span>, ExactSpelling = <span class="hljs-literal"><span class="hljs-literal">true</span></span>, CharSet = CharSet.Ansi, SetLastError = <span class="hljs-literal"><span class="hljs-literal">true</span></span>)] <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">extern</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">int</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">Record</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">string</span></span></span></span><span class="hljs-function"><span class="hljs-params"> lpstrCommand, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">string</span></span></span></span><span class="hljs-function"><span class="hljs-params"> lpstrReturnString, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> uReturnLength, </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> hwndCallback</span></span></span><span class="hljs-function">)</span></span>; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">string</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">StartRecording</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-comment"><span class="hljs-comment">//MCIErrors ‚Äî       var result = (MCIErrors)Record("open new Type waveaudio Alias recsound", "", 0, 0); if (result != MCIErrors.NO_ERROR) { return "Error code: " + result.ToString(); } //       .wav     Speaker Recognition result = (MCIErrors)Record("set recsound time format ms alignment 2 bitspersample 16 samplespersec 16000 channels 1 bytespersec 88200", "", 0, 0); if (result != MCIErrors.NO_ERROR) { return "Error code: " + result.ToString(); } result = (MCIErrors)Record("record recsound", "", 0, 0); if (result != MCIErrors.NO_ERROR) { return "Error code: " + result.ToString(); } return "1"; } public string StopRecording() { var result = (MCIErrors)Record("save recsound result.wav", "", 0, 0); if (result != MCIErrors.NO_ERROR) { return "Error code: " + result.ToString(); } result = (MCIErrors)Record("close recsound ", "", 0, 0); if (result != MCIErrors.NO_ERROR) { return "Error code: " + result.ToString(); } return "1"; } }</span></span></code> </pre> <br>  Then we will create the SpeakerRecognitionCommunicator component responsible for communicating with the Speaker Recognition API: <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.SpeakerRecognition; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.SpeakerRecognition.Contract.Identification; ... <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">SpeakerRecognitionCommunicator</span></span> { <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">async</span></span></span><span class="hljs-function"> Task&lt;IdentificationOperation&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RecognizeSpeaker</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">string</span></span></span></span><span class="hljs-function"><span class="hljs-params"> recordingFileName</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> srsc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> SpeakerIdentificationServiceClient(Settings.Instance.SpeakerRecognitionApiKeyValue); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> profiles = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> srsc.GetProfilesAsync(); <span class="hljs-comment"><span class="hljs-comment">//   ,       Guid[] testProfileIds = new Guid[profiles.Length]; for (int i = 0; i &lt; testProfileIds.Length; i++) { testProfileIds[i] = profiles[i].ProfileId; } //IdentifyAsync   ,        OperationLocation processPollingLocation; using (Stream audioStream = File.OpenRead(recordingFileName)) { processPollingLocation = await srsc.IdentifyAsync(audioStream, testProfileIds, true); } IdentificationOperation identificationResponse = null; int numOfRetries = 10; TimeSpan timeBetweenRetries = TimeSpan.FromSeconds(5.0); // while (numOfRetries &gt; 0) { await Task.Delay(timeBetweenRetries); identificationResponse = await srsc.CheckIdentificationStatusAsync(processPollingLocation); if (identificationResponse.Status == Microsoft.ProjectOxford.SpeakerRecognition.Contract.Identification.Status.Succeeded) { break; } else if (identificationResponse.Status == Microsoft.ProjectOxford.SpeakerRecognition.Contract.Identification.Status.Failed) { throw new IdentificationException(identificationResponse.Message); } numOfRetries--; } if (numOfRetries &lt;= 0) { throw new IdentificationException("   "); } return identificationResponse; } }</span></span></code> </pre> <br>  Finally, we have integrated the two functional elements discussed earlier into the ListenButton_Click handler.  At the first click of the mouse, it initiates the recording of voice information, and the second - sends the recording to the service Speaker Recognition.   ,   EZ   ,       ,     (   ). <br><br><pre> <code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">private</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">async</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">ListenButton_Click</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">object</span></span></span></span><span class="hljs-function"><span class="hljs-params"> sender, EventArgs e</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> vr = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> WavRecording(); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (!isRecording) { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> r = vr.StartRecording(); <span class="hljs-comment"><span class="hljs-comment">//  if (r == "1") { isRecording = true; ListenButton.Text = "   "; } else WriteDebug(r); } else { var r = vr.StopRecording(); if (r == "1") try { var sr = new SpeakerRecognitionCommunicator(); var identificationResponse = await sr.RecognizeSpeaker("result.wav"); WriteDebug( ); wavePosition.StartAction_Wave(); var name = Speakers.ListOfSpeakers.Where(s =&gt; s.ProfileId == identificationResponse.ProcessingResult.IdentifiedProfileId.ToString()).First().Name; ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(" " + )); await Task.Delay(5000); wavePosition.Stop(); ezb.Servo.ReleaseAllServos(); } catch (IdentificationException ex) { WriteDebug("Speaker Identification Error: " + ex.Message); wavePosition.StartAction_Wave(); ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(", ")); //,     await Task.Delay(5000); wavePosition.Stop(); ezb.Servo.ReleaseAllServos(); } catch (Exception ex) { WriteDebug(": " + ); } else WriteDebug(r); isRecording = false; ListenButton.Text = " "; } }</span></span></code> </pre> <br><h4>   </h4><br>       ,   ¬´¬ª,   ,      .     <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home">Computer Vision API</a> .    ,     Computer Vision API. <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.ProjectOxford.Vision; ... <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">async</span></span></span><span class="hljs-function"> Task&lt;</span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">string</span></span></span><span class="hljs-function">&gt; </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">RecognizeObjectsInImage</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">Bitmap image</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-comment"><span class="hljs-comment">//   westeurope var vsc = new VisionServiceClient(visionApiKey, "https://westeurope.api.cognitive.microsoft.com/vision/v1.0"); MemoryStream memoryStream = new MemoryStream(); image.Save(memoryStream, System.Drawing.Imaging.ImageFormat.Jpeg); memoryStream.Seek(0, SeekOrigin.Begin); var result = await vsc.AnalyzeImageAsync(memoryStream,new List&lt;VisualFeature&gt;() { VisualFeature.Description }); return result.Description.Captions[0].Text; }</span></span></code> </pre> <br>    ,      ,    <i>ComputerVision</i> . <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-string"><span class="hljs-string">"ComputerVision"</span></span>: { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> currentBitmap = camera.GetCurrentBitmap; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> cvc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CustomVisionCommunicator(); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> description = <span class="hljs-keyword"><span class="hljs-keyword">await</span></span> cvc.RecognizeObjectsInImage(currentBitmap); ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream(description)); <span class="hljs-keyword"><span class="hljs-keyword">break</span></span>; }</code> </pre> <br><h4>      </h4><br>     <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home">Custom Vision API</a> ,       .        .  ,           .   ,    ,         ,       . Custom Vision API  -,     ,      .       CustomVisionCommunicator,         : <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.Cognitive.CustomVision; <span class="hljs-keyword"><span class="hljs-keyword">using</span></span> Microsoft.Cognitive.CustomVision.Models; ... <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">CustomVisionCommunicator</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> predictionKey; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">string</span></span> visionApiKey; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> Guid projectId; <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> Guid iterationId; PredictionEndpoint endpoint; VisionServiceClient vsc; <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">CustomVisionCommunicator</span></span></span><span class="hljs-function">(</span><span class="hljs-params"></span><span class="hljs-function"><span class="hljs-params"></span>)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.visionApiKey = Settings.Instance.VisionApiKey; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.predictionKey = Settings.Instance.PredictionKey; <span class="hljs-keyword"><span class="hljs-keyword">this</span></span>.projectId = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> Guid(Settings.Instance.VisionApiProjectId); <span class="hljs-comment"><span class="hljs-comment">//      this.iterationId = new Guid(Settings.Instance.VisionApiIterationId); PredictionEndpointCredentials predictionEndpointCredentials = new PredictionEndpointCredentials(predictionKey); //   ,     ,     endpoint = new PredictionEndpoint(predictionEndpointCredentials); vsc = new VisionServiceClient(visionApiKey, "https://westeurope.api.cognitive.microsoft.com/vision/v1.0"); } public List&lt;ImageTagPrediction&gt; RecognizeObject(Bitmap image) { MemoryStream memoryStream = new MemoryStream(); image.Save(memoryStream, System.Drawing.Imaging.ImageFormat.Jpeg); //     memoryStream.Seek(0, SeekOrigin.Begin); var result = endpoint.PredictImage(projectId, memoryStream,iterationId); return result.Predictions.ToList(); } }</span></span></code> </pre> <br>      Custom Vision API    ,     ,    ,    ,  ,   ,  ,        .     Custom Vision API  ,      : <br><br><pre> <code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">case</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span>: { ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream((<span class="hljs-string"><span class="hljs-string">",    !   !"</span></span>))); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> currentBitmap = camera.GetCurrentBitmap; <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> cvc = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> CustomVisionCommunicator(); <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> predictions = cvc.RecognizeObject(currentBitmap); <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (RecognizeObject(<span class="hljs-string"><span class="hljs-string">""</span></span>)) { <span class="hljs-comment"><span class="hljs-comment">//     grabPosition.StartAction_Takefood(); await Task.Delay(1000); ezb.SoundV4.PlayData(ezb.SpeechSynth.SayToStream((" "))); } else ezb.SpeechSynth.Say("   "); break; }</span></span></code> </pre> <br><h2>  Conclusion </h2><br>      ,    -   .         Cognitive Services,         ,       Cognitive Services.     ,      EZ        SDK    Cognitive Services. <br><br><h4>  findings </h4><br>           EZ  Cognitive Services. <br><br><ul><li>   Face API  Custom Vision       ,      .          ,      ,    .     Custom Vision        . </li><li>   Speaker Recognition API      .wav    (. ,   Speaker Recognition, ).     ,       . </li><li>    .wav       winmm.dll ‚Äî   ,          .wav.    :       . </li><li>    Speaker Recognition ‚Äî         ,  API    .      ,      . </li><li>     EZ        ‚Äî    ,   EZ  ,     .           Cognitive Service Microsoft Speech API   Language Understanding Intelligent Service (LUIS). </li><li>  EZ JD           ,      ,          .     ,      ,   SDK         . </li></ul><br><h2>  useful links </h2><br> Cognitive Services: <br><ul><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/speech/home">Speech API</a> </li><li> <a href="https://github.com/Azure-Samples/Cognitive-Speech-STT-Windows"> Speech Recognition</a> </li><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/luis/home">Language Understanding Intelligent Service (LUIS)</a> </li><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/face/overview">Face API</a> </li><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/emotion/home">Emotion API</a> </li><li> <a href="https://azure.microsoft.com/en-us/services/cognitive-services/speaker-recognition/">Speaker Recognition API</a> </li><li> <a href="https://github.com/Microsoft/Cognitive-SpeakerRecognition-Windows">     </a> </li><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home">Computer Vision API</a> </li><li> <a href="https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/home">Custom Vision API</a> </li><li> <a href="https://github.com/Microsoft/Cognitive-Samples-IntelligentKiosk">Intelligent Kiosk Sample</a> ( UWP    ,  Cognitive Services, ,   ) </li></ul><br>  EZ: <br><ul><li> <a href="https://www.ez-robot.com/EZ-Builder/sdk">EZ Robot SDK</a> </li></ul><br><h2>   </h2><br>        ()       ,            Face API  Speaker Recognition API   .        -,       . <br><br><h2>  about the author </h2><br><img src="https://habrastorage.org/webt/__/gs/vr/__gsvrbmjrtcxzzv1ldp6iwiijy.jpeg" align="left" width="120"><blockquote> <b> </b> ‚Äî   Microsoft  . <br> ¬´As a technology evangelist at Microsoft, I do have an opportunity to learn and work with the newest technologies and subsequently help developers with adoption of these with ultimate goal of making their project/business even more successful. My area of focus is Azure Cloud in general, and especially services related to topics such as Micro Services, Internet of Things, Chat Bots, Artificial Intelligence. I like to spend my free time with technology, working on interesting projects, but I can also enjoy time outside of IT. I like to take in hand, put on or kick to almost any sports equipment and I really enjoy time when I can change the urban grey for the forest green.¬ª </blockquote><br>       ,        <a href="">Facebook</a> .  ,      ,    . </div><p>Source: <a href="https://habr.com/ru/post/351224/">https://habr.com/ru/post/351224/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../351214/index.html">Ideal requirements, and how to deal with it</a></li>
<li><a href="../351216/index.html">Strong typing for TypeScript Vue.js applications</a></li>
<li><a href="../351218/index.html">Flask Mega-Tutorial, Part XV: Improving Application Structure</a></li>
<li><a href="../351220/index.html">FastTrack Training. "Network Basics". "The Basics of Telephony." Part 1. Eddie Martin. December 2012</a></li>
<li><a href="../351222/index.html">Pros and cons of doing business in the United States: observations after a year of development of their company</a></li>
<li><a href="../351226/index.html">I am a network architect and this worries me</a></li>
<li><a href="../351228/index.html">A study of cyber attacks in 2017: 47% of attacks are directed at the infrastructure of companies</a></li>
<li><a href="../351230/index.html">Revelations of an employee of large IT companies [in Silicon Valley]</a></li>
<li><a href="../351232/index.html">Pygest # 24. News, releases, articles, interesting projects and libraries from the world of Python [March 2018 - April 9, 2018]</a></li>
<li><a href="../351234/index.html">5 errors when developing WebRTC calls from the browser</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>