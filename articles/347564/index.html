<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Xception: compact deep neural network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In the past few years, neural networks have made their way into all branches of machine learning, but they have undoubtedly produced the greatest sens...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Xception: compact deep neural network</h1><div class="post__text post__text-html js-mediator-article">  In the past few years, neural networks have made their way into all branches of machine learning, but they have undoubtedly produced the greatest sensation in the field of computer vision.  As part of the <a href="http://www.image-net.org/">ImageNet</a> competition, a number of different architectures of convolutional networks were presented, which then diverged across frameworks and libraries. <br> <a href="https://habrahabr.ru/post/347564/"><img src="https://habrastorage.org/webt/su/kz/to/sukztoj0iuadqoxrv9tdmzfvf3m.jpeg" align="right" width="250"></a> <br>  In order to improve the quality of recognition of their networks, the researchers tried to add more layers to the networks, however, over time, it came to be understood that sometimes performance limitations simply do not allow training and using such deep networks.  This was the motivation to use depthwise separable convolutions and create an Xception architecture. <br><br>  If you want to know what it is, and see how to use such a network in practice to learn how to distinguish cats from dogs, welcome to Cat. <br><a name="habracut"></a><br><h2>  Inception module </h2><br>  Every time we add another layer to the convolutional network, we need to make a strategic decision about its characteristics.  What size of core do we use?  3x3?  5x5?  Or maybe put max pooling? <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/sg/7t/tu/sg7ttuirleaml3_j7dwo2tn0iqs.png"></div><br>  In 2015, the Inception architecture was proposed, the idea of ‚Äã‚Äãwhich was as follows: instead of choosing the kernel size, let's take several options at once, use them all at the same time and concatenate the results.  However, this significantly increases the number of operations that must be performed to calculate single layer activations, so the authors of the <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">original article</a> propose such a trick: let's make a convolution with a 1x1 core in front of each convolutional block, reducing the dimension of the signal that goes to the input of convolutions with larger core sizes . <br><br>  The resulting construction in the figure below constitutes the complete Inception module. <br><div style="text-align:center;"><img src="https://habrastorage.org/webt/by/ah/k3/byahk38pud3wpqlimvqqbib5dxq.png"></div><br>  But what does this have to do with making our network more compact? <br>  In 2016, Fran√ßois Chollet, the author and developer of the Keras framework, published <a href="https://arxiv.org/pdf/1610.02357.pdf">an article</a> in which he suggested going further and using the so-called Extreme Inception module, also known as depthwise separable convolution. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Depthwise separable convolution </h2><br>  Imagine we took a standard convolutional layer with <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.716ex" height="2.298ex" viewBox="0 -780.1 1169.4 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="1011" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-1"> C_2 </script>  filters of size 3x3, to the input of which the dimension tensor is supplied <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2217;</mo><mi>M</mi><mo>&amp;#x2217;</mo><msub><mi>C</mi><mn>1</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.99ex" height="2.298ex" viewBox="0 -780.1 5162.3 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="1996" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="3270" y="0"></use><g transform="translate(3992,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31" x="1011" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mo>‚àó</mo><mi>M</mi><mo>‚àó</mo><msub><mi>C</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-2"> M * M * C_1 </script>  where <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.442ex" height="2.057ex" viewBox="0 -780.1 1051.5 885.9" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi></math></span></span><script type="math/tex" id="MathJax-Element-3"> M </script>  Is the width and height of the tensor, and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mn>1</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.716ex" height="2.298ex" viewBox="0 -780.1 1169.4 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31" x="1011" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-4"> C_1 </script>  - number of channels. <br>  What does this layer do?  It collapses all the channels of the original signal at the same time. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><msub><mi>C</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.716ex" height="2.298ex" viewBox="0 -780.1 1169.4 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="1011" y="-213"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-5"> C_2 </script>  different bundles.  At the exit of such a layer, a dimension tensor is obtained. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mo stretchy=&quot;false&quot;>(</mo><mi>M</mi><mo>&amp;#x2212;</mo><mn>2</mn><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><mo stretchy=&quot;false&quot;>(</mo><mi>M</mi><mo>&amp;#x2212;</mo><mn>2</mn><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><msub><mi>C</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="23.614ex" height="2.66ex" viewBox="0 -832 10167.2 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-28" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="389" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2212" x="1663" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="2663" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-29" x="3164" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="3776" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-28" x="4498" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="4888" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2212" x="6162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="7162" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-29" x="7663" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="8275" y="0"></use><g transform="translate(8997,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="1011" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">(</mo><mi>M</mi><mo>‚àí</mo><mn>2</mn><mo stretchy="false">)</mo><mo>‚àó</mo><mo stretchy="false">(</mo><mi>M</mi><mo>‚àí</mo><mn>2</mn><mo stretchy="false">)</mo><mo>‚àó</mo><msub><mi>C</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-6"> (M - 2) * (M - 2) * C_2 </script>  . <br><br>  Let's take two steps in succession instead: <br><img src="https://habrastorage.org/webt/8x/xc/k5/8xxck53kuhqon_9y9xc6vs77ukg.png" align="right"><br><ol><li>  We fold the original 1x1 tensor by convolution, just as we did in the Inception block, getting the tensor <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>M</mi><mo>&amp;#x2217;</mo><mi>M</mi><mo>&amp;#x2217;</mo><msub><mi>C</mi><mn>2</mn></msub></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="11.99ex" height="2.298ex" viewBox="0 -780.1 5162.3 989.6" role="img" focusable="false" style="vertical-align: -0.487ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1273" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-4D" x="1996" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="3270" y="0"></use><g transform="translate(3992,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMATHI-43" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="1011" y="-213"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>M</mi><mo>‚àó</mo><mi>M</mi><mo>‚àó</mo><msub><mi>C</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-7"> M * M * C_2 </script>  .  This operation is called pointwise convolution. </li><li>  We turn each channel separately 3x3 convolutions (in this case, the dimension will not change, since we are not turning all the channels together, as in the usual convolutional layer).  This operation is called depthwise spatial convolution </li></ol><br><div class="spoiler">  <b class="spoiler_title">A little tricks on terminology.</b> <div class="spoiler_text">  In fact, usually when it comes to depthwise separable convolution, it is implied that they first do the convolution through the channels, and then the 1x1 convolution, but I give exactly the order of operations that is specified in the original article.  In general, according to the author, the order of these operations does not affect the final result. <br></div></div><br><h3>  Why does this make the network more compact? </h3><br>  Let's look at a specific example.  Let we collapse the image with 16 channels with a convolution layer with 32 filters.  In total, this convolutional layer will have <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-8-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>16</mn><mo>&amp;#x2217;</mo><mn>32</mn><mo>&amp;#x2217;</mo><mn>3</mn><mo>&amp;#x2217;</mo><mn>3</mn><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>$</mo></mrow><mn>4</mn><mo>,</mo><mn>60</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="22.341ex" height="2.539ex" viewBox="0 -832 9619.1 1093.4" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-36" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1223" y="0"></use><g transform="translate(1945,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="3169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33" x="3891" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33" x="5337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-3D" x="6115" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-24" x="7171" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-34" x="7672" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2C" x="8172" y="0"></use><g transform="translate(8618,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-36"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-30" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mo>‚àó</mo><mn>32</mn><mo>‚àó</mo><mn>3</mn><mo>‚àó</mo><mn>3</mn><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo>$</mo></mrow><mn>4</mn><mo>,</mo><mn>60</mn></math></span></span><script type="math/tex" id="MathJax-Element-8"> 16 * 32 * 3 * 3 = $ 4,60</script>  weights as we will have <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-9-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>16</mn><mo>&amp;#x2217;</mo><mn>32</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.845ex" height="1.937ex" viewBox="0 -728.2 2946.9 834" role="img" focusable="false" style="vertical-align: -0.246ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-36" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1223" y="0"></use><g transform="translate(1945,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mo>‚àó</mo><mn>32</mn></math></span></span><script type="math/tex" id="MathJax-Element-9"> 16 * 32 </script>  3x3 bundle. <br><br>  How many scales will there be in a similar depthwise separable convolution block?  First, we will have <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-10-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>16</mn><mo>&amp;#x2217;</mo><mn>32</mn><mo>&amp;#x2217;</mo><mn>1</mn><mo>&amp;#x2217;</mo><mn>1</mn><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>$</mo></mrow><mn>51</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="20.145ex" height="2.298ex" viewBox="0 -832 8673.4 989.6" role="img" focusable="false" style="vertical-align: -0.366ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-36" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1223" y="0"></use><g transform="translate(1945,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="500" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="3169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31" x="3891" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="4614" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31" x="5337" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-3D" x="6115" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-24" x="7171" y="0"></use><g transform="translate(7672,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-35"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-31" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>16</mn><mo>‚àó</mo><mn>32</mn><mo>‚àó</mo><mn>1</mn><mo>‚àó</mo><mn>1</mn><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo>$</mo></mrow><mn>51</mn></math></span></span><script type="math/tex" id="MathJax-Element-10"> 16 * 32 * 1 * 1 = $ 51</script>  weights have pointwise convolution.  Secondly, we will have <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-11-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mn>32</mn><mo>&amp;#x2217;</mo><mn>3</mn><mo>&amp;#x2217;</mo><mn>3</mn><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>$</mo></mrow><mn>28</mn></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="15.625ex" height="2.298ex" viewBox="0 -832 6727.4 989.6" role="img" focusable="false" style="vertical-align: -0.366ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32" x="500" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="1223" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33" x="1945" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-2217" x="2668" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-33" x="3391" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-3D" x="4169" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-24" x="5225" y="0"></use><g transform="translate(5726,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-32"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/347564/&amp;xid=17259,15700023,15700186,15700190,15700248,15700253&amp;usg=ALkJrhi9C7PGatXjjAZ83byEKJtOlwpwrA#MJMAIN-38" x="500" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mn>32</mn><mo>‚àó</mo><mn>3</mn><mo>‚àó</mo><mn>3</mn><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo>$</mo></mrow><mn>28</mn></math></span></span><script type="math/tex" id="MathJax-Element-11"> 32 * 3 * 3 = $ 28</script>  scales at depthwise convolution.  In total, we get 800 weights, which is much smaller than that of the ordinary convolutional layer. <br><br><h3>  Why does this even work? </h3><br>  A regular convolutional layer simultaneously processes both spatial information (the correlation of neighboring points within one channel) and inter-channel information, since the convolution is applied to all channels at once.  The Xception architecture is based on the assumption that these two types of information can be processed sequentially without losing network quality, and decomposes the usual convolution into pointwise convolution (which only processes inter-channel correlation) and spatial convolution (which only processes spatial correlation within a single channel) . <br><br>  Let's look at the real effect.  For comparison, let's take two truly deep convolutional network architectures - ResNet50 and InceptionResNetV2. <br><br>  The ResNet50 has 25,636,712 weights, and the pre-trained model in Keras weighs 99 MB.  The accuracy that this model achieves on ImageNet dataset is 75.9%. <br><br>  InceptionResNetV2 has 55,873,736 learning parameters and weighs 215 MB, reaching an accuracy of 80.4%. <br><br>  What happens with the Xception architecture?  The network has 22,910,480 weights and weighs 88 MB.  The accuracy of classification on ImageNet is 79%. <br><br>  Thus, we get the network architecture, which exceeds ResNet50 in accuracy and only slightly inferior to InceptionResNetV2, while <b>significantly gaining in size</b> , and therefore the required resources both for training and for using this model. <br><br><h2>  Less words, more code </h2><br>  Let us analyze with a short example how to apply this architecture to a real task.  For these purposes, we will take <a href="https://www.kaggle.com/c/dogs-vs-cats">Dogs vs Cats</a> with Kaggle and in half an hour we will teach our network to distinguish cats from dogs. <br><br>  Let's break the data into three parts: train (4000 images), validation (2000 images) and test (10,000 images). <br><br><img src="https://habrastorage.org/webt/dc/lb/u9/dclbu9flug4juwthvnsht5rpd4i.jpeg"><br><br>  Since Francois Chollet, the author of the Xception architecture, is also the creator of Keras, he kindly provided the weights of this network, trained on ImageNet, in his framework, which we will use to finish the network under our task using transfer learning. <br><br>  Load the weights from ImageNet into Xception, from which the last fully connected layers are removed.  Run our dataset through this network to get the signs that the convolutional network can extract from the images (the so-called bottleneck features): <br><br><pre><code class="python hljs">input_tensor = Input(shape=(img_height,img_width,<span class="hljs-number"><span class="hljs-number">3</span></span>)) base_model = xception.Xception(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, input_shape=(img_width, img_height, <span class="hljs-number"><span class="hljs-number">3</span></span>), pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>) data_generator = image.ImageDataGenerator(rescale=<span class="hljs-number"><span class="hljs-number">1.</span></span> / <span class="hljs-number"><span class="hljs-number">255</span></span>) train_generator = data_generator.flow_from_directory( train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode=<span class="hljs-keyword"><span class="hljs-keyword">None</span></span>, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>) bottleneck_features_train = base_model.predict_generator( train_generator, nb_train_samples // batch_size) np.save(open(<span class="hljs-string"><span class="hljs-string">'bottleneck_features_train.npy'</span></span>, <span class="hljs-string"><span class="hljs-string">'wb'</span></span>), bottleneck_features_train)</code> </pre> <br>  Create a fully meshed network and train it on features derived from convolutional layers, using the specially deferred part of the initial data set for validation: <br><br><pre> <code class="python hljs">model = Sequential() model.add(Dense(<span class="hljs-number"><span class="hljs-number">256</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=base_model.output_shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">128</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(optimizer=SGD(lr=<span class="hljs-number"><span class="hljs-number">0.01</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) checkpointer = ModelCheckpoint(filepath=<span class="hljs-string"><span class="hljs-string">'top-weights.hdf5'</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">1</span></span>, save_best_only=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>) history = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], validation_data=(validation_data, validation_labels))</code> </pre> <br>  Already after this stage, which takes a couple of minutes on the GeForce GTX 1060 video card, we get accuracy of about 99.4% on the validation dataset. <br><br>  Now we will try to train the network with augmentation of the input data, loading weights into the convolutional layers from ImageNet, and into fully connected layers - weights that our network has just learned: <br><br><pre> <code class="python hljs">input_tensor = Input(shape=(img_height,img_width,<span class="hljs-number"><span class="hljs-number">3</span></span>)) base_model = xception.Xception(weights=<span class="hljs-string"><span class="hljs-string">'imagenet'</span></span>, include_top=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, input_shape=(img_width, img_height, <span class="hljs-number"><span class="hljs-number">3</span></span>), pooling=<span class="hljs-string"><span class="hljs-string">'avg'</span></span>) top_model = Sequential() top_model.add(Dense(<span class="hljs-number"><span class="hljs-number">256</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>, input_shape=base_model.output_shape[<span class="hljs-number"><span class="hljs-number">1</span></span>:])) top_model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) top_model.add(Dense(<span class="hljs-number"><span class="hljs-number">128</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>)) top_model.add(Dropout(<span class="hljs-number"><span class="hljs-number">0.5</span></span>)) top_model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) top_model.load_weights(<span class="hljs-string"><span class="hljs-string">'top-weights.hdf5'</span></span>) model = Model(inputs=base_model.input, outputs=top_model(base_model.output)) model.compile(optimizer=SGD(lr=<span class="hljs-number"><span class="hljs-number">0.005</span></span>, momentum=<span class="hljs-number"><span class="hljs-number">0.1</span></span>, nesterov=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'binary_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>])</code> </pre> <br>  Having trained the network in this mode, for another five epochs (about twenty minutes), we will achieve an accuracy of 99.5% on validation data. <br><br>  Checking the model on the data that she has never seen, and which were not used to set up hyper parameters (test dataset), we see an accuracy of about 96.9%, which looks quite acceptable. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c14/662/236/c14662236320d9c9dcb8c5965c888181.png" align="right"><br>  The complete code for this experiment can be found on <a href="https://github.com/hokmund/xception-transfer-learning">GitHub</a> . <br><br><h2>  What's next? </h2><br>  In 2017, Google added pre-trained networks of the <a href="https://habrahabr.ru/post/352804/">MobileNet</a> architecture to <a href="https://habrahabr.ru/post/352804/">TensorFlow</a> , using principles similar to Xception to make models even smaller.  These models are suitable for performing computer vision tasks directly on mobile phones or IoT devices, which have extremely limited memory reserves and a weak processor. <br><br>  Thus, we quietly came to the point in the history of computer science, when you write an application that determines whether a bird is depicted in the photo, it can be done in fifteen minutes, and this application will run directly on your smartphone. </div><p>Source: <a href="https://habr.com/ru/post/347564/">https://habr.com/ru/post/347564/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../347550/index.html">Preparing .xlsx files: introduction, cell styles</a></li>
<li><a href="../347554/index.html">How the creators of Speedtest evaluate mobile Internet in Russia</a></li>
<li><a href="../347556/index.html">Running applications from the blockchain on the SAP Cloud Platform</a></li>
<li><a href="../347558/index.html">Script to delete universal Windows Store apps</a></li>
<li><a href="../347562/index.html">Announcement of ElixirLangMoscow Meetup # 7</a></li>
<li><a href="../347570/index.html">Again about services for ICO: read and be terrified</a></li>
<li><a href="../347572/index.html">The story of how two graduates of "mathex" made the first sales</a></li>
<li><a href="../347574/index.html">We use Apple Pay and Troika card as a pass to work</a></li>
<li><a href="../347576/index.html">Object in a case or Optional in Java 8 and Java 9. Part 2: ‚ÄúHow it is done in Java 8‚Äù</a></li>
<li><a href="../347578/index.html">‚ÄúWhen you tell a true story, they believe it much more‚Äù - Interview with Oleg Shelaev, part 2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>