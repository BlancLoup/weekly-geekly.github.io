<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Netapp - Reality vs. Marketing</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day 

 It just so happened that I have been working on data storage systems for the last 5 years, 4 years of which I dedicated to EMC mid-tier sy...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Netapp - Reality vs. Marketing</h1><div class="post__text post__text-html js-mediator-article"><h6>  Good day </h6><br><img src="https://habrastorage.org/getpro/habr/post_images/8c5/891/c1c/8c5891c1c941e2eb2e93f0550df59aa1.jpg" alt="image" align="right"><br>  It just so happened that I have been working on data storage systems for the last 5 years, 4 years of which I dedicated to EMC mid-tier systems, which I was generally pleased with.  About EMC, I may devote a separate post, and this one will be devoted to NetApp storage systems, which have been dealt with over the past year in rather complex configurations.  The view from the buyer, user, administrator, without any technical details and beautiful pictures. <br><br>  Who cares - welcome under cat. <br><a name="habracut"></a><br><h6>  How it all began </h6><br>  It all started with the fact that they decided to build a second, remote data center.  Since the resources of the old storage system have come to an end, and in any case we need to build DCs from scratch, we decided to purchase something that can withstand the load of about 1000 virtual machines and about 20 Oracle product databases + a lot of development.  Well, respectively, once a remote DC, it means replication and fault tolerance at the level of the entire data center.  I will not dwell on all aspects of the choice, but I will say that the applicants were Hitachi, EMC and NetApp.  We chose NetApp, because  After the tests, we liked how Oracle works on NFS and the lack of FC SAN as a class, we can use the existing 10gb network.  On that and stopped. <br><br><h6>  What tasks were set </h6><br><ol><li>  Remote sites in active-active mode (in the sense of part of the bases there, part there, not RAC) </li><li>  Oracle data loss in case of a failure of one of the parties - 0 seconds </li><li>  Base switching time to the other side - up to 5 minutes using clustered software (Veritas) </li><li>  Permanent availability of VMware virtual machines </li></ol>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h6>  What is the result? </h6><br>  And it turned out 2 <a href="http://www.netapp.com/ru/products/storage-systems/fas6200/fas6200-tech-specs-more.aspx">FAS6280</a> systems, 2 controllers in each, on two platforms for databases, SnapMirror replication + one <a href="http://www.netapp.com/ru/products/storage-systems/fas3200/fas3200-product-compare.aspx">FAS3270</a> system in the <a href="http://www.netapp.com/ru/products/protection-software/metrocluster.aspx">MetroCluster</a> (SyncMirror) configuration for virtual machines.  Ontap version - 8.1.2 on all systems.  Everywhere - <a href="http://www.netapp.com/ru/products/platform-os/flexvol.aspx">FlexVol</a> and <a href="http://www.netapp.com/ru/products/platform-os/raid-dp.aspx">RaidDP</a> . <br><br>  I will say right away - there is no problem with the metro cluster on the FAS3270.  Totally.  Absolutely.  It works and performs exactly the tasks that were assigned to it, pulls about 1000 virtual machines divided in half between the sites.  No problems and pitfalls.  If the controller goes into reboot, the virtuals freeze for 15 seconds on I / O and continue to work as if nothing had happened.  Reverse switching, when the controller returns, takes about the same amount of time.  I am satisfied with this system for all 200%.  But, let's say honestly, the load on it is still about 50% of the place and about 25-30% of input-output disks (about 4,500 disk iops per 75 active disks / side).  As practice shows, this is exactly what allows him to work without problems. <br><br>  In conjunction with NetApp, a specification was drawn up for the FAS6280, a sizing was conducted, and we discussed all the pitfalls that may arise in the case of our tasks.  We were assured that everything will work as we have discussed.  And we discussed the following picture: <br><ul><li>  Each database contains 3 sections: data + index, archivelog, redo + undo. </li><li>  20 databases divided into 4 controllers, replication between a pair of controllers. </li><li>  2 sections, data + arch - asynchronous snapmirror, once a minute.  The redo section is a synchronous snapmirror. </li><li>  Development is spread by a thin layer on all 4 controllers. </li></ul><br>  At the time of launch, recycling at the site was about 40% for each controller, recycling for disc water-output - 20%.  From which we can conclude that the system just idle. <br>  Now the disposal at the site is about 85%, on disks about 70% on average, in peaks 90%.  If the figures - 100% recycling, it is 32500 disk operations on the controller.  Disk operations! = Count iops on nfs. <br><br><h6>  Problem One - Synchronous <a href="http://www.netapp.com/ru/products/protection-software/snapmirror.aspx">SnapMirror</a> </h6><br>  Promised - 30 sessions of synchronous replication on the controller.  Everything.  No details, except what is indicated - synchronous replication is very sensitive to the latency of the network.  No details about the load, direction. <br><br>  In fact, synchronous replication breaks down even with 2 sessions <b>directed in different directions</b> (the fact that this may turn out to be the key, we realized after almost 2 months). <br>  It looked like this: <br><pre><code class="hljs pgsql">[fas6280a_1:wafl.nvlog.sm.sync.<span class="hljs-keyword"><span class="hljs-keyword">abort</span></span>:<span class="hljs-keyword"><span class="hljs-keyword">notice</span></span>]: NVLOG synchronization aborted <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> snapmirror source VOL_prod_ru, reason=<span class="hljs-string"><span class="hljs-string">'Out of NVLOG files'</span></span> [fas6280a_1:snapmirror.sync.fail:<span class="hljs-keyword"><span class="hljs-keyword">notice</span></span>]: Synchronous SnapMirror <span class="hljs-keyword"><span class="hljs-keyword">from</span></span> fas6280a_1:VOL_prod_ru <span class="hljs-keyword"><span class="hljs-keyword">to</span></span> fas6280a_2:VOL_prod_ru failed.</code> </pre> <br>  They opened the case, consulted directly with the engineers of NetApp - the verdict always sounded the same - you have problems with the network.  We conducted our own research on this subject, spent almost a month on it - our verdict, the network is in order, the latency under load is about 0.5ms and does not jump.  The solution was suggested by one of the technicians from the depths of NetApp - he said that he didn‚Äôt work in different directions, because there is something related to the Consistency Point (CP) mechanism itself, we didn‚Äôt understand the details, but it was worth deploying replications in one direction - all was well. <br>  Since this did not suit us, we bought 4 shelves of disks and took all the Redo / Undo on FAS3270, to the metro cluster, and forgot about the problems of synchronous replication as a class.  True, the intermittent load from the virtual machines, the high load on the processors and the high response time requirements for Redo did not allow us to stay on this decision, but this is a separate story. <br><br><h6>  Problem Two - Asynchronous SnapMirror </h6><br>  Everything is easier here - if we set the synchronization once a minute, we get a never ending session, because the data do not have time to overflow in the allotted time, the changes are considered too long.  We stopped at the 5 minute schedule, shifted by 1 minute for each section.  When the load has increased to 80% of disk utilization, we have a synchronization lag for the heaviest bases of the order of 15‚Äì20 minutes all the time, during peak times (backup, for example, disks of 90‚Äì95% utilization), the lag increases to infinity.  From which it follows that there can be no switching in 5 minutes, for only dokatyvanie changes at best 20 minutes + reverse resynchronization after replication reversal, which takes a very long time on large partitions, the same 20 minutes at best. <br><br><h6>  Problem three - QOS, or in NetApp terminology - <a href="http://www.netapp.com/ru/products/platform-os/flexshare.aspx">FlexShare</a> </h6><br>  The system manages the scheduler, works at the volume level (volume), allows you to set priorities from 1 to 99 for each volume relative to other volumes plus relative to system processes.  The description is gorgeous, it would seem, everything is just as 2x2.  In fact: <br><ul><li>  It is impossible to limit the upper bar for the volume, well, if suddenly some application broke and started to torment I / O, everyone will suffer. </li><li>  No matter what priority is set, say, for the sale of 99, for the development - 1, the load of the development will still affect the prod.  But yes, the response time of the design section will be slightly worse if the load is constant. </li><li>  If the development load is not constant, but intermittent, the priority does not work at all.  He needs time to adapt. </li><li>  The priority does not speed up the execution of SnapMirror, no matter how high the system is. </li><li>  Priority raises the overall latency through the system, but not much.  Noticeable only with very high loads. </li><li>  With a very high controller load, it is impossible to execute most commands, the result comes 3-5 minutes after running the command to execute, for example, you cannot even see the status of replication, and this blocks the operation of cluster software. </li></ul><br>  It is because of the latter that priority is turned off on our systems.  They opened a case on this issue - there is no result. <br><br><h6>  The fourth problem is free space on the unit <i>(a set of several raid-groups united in one space)</i> . </h6><br>  Those who have experienced NetApp from previous years probably know that if the unit is filled with more than 90% of the data, the system becomes extremely lazy and does not want to work.  Marketers stubbornly claim that this problem has been completely eliminated since version 8.1 (or maybe even 8.0, I don‚Äôt remember exactly, to be honest).  In fact - a complete lie.  We did not keep track of the place, filled the unit by as much as 93% - that's it, hello.  The response time of the system as a whole has increased almost 2 times.  And it provided that we do not use deduplication, compression and other goodies, only thin volumes (thin provisioning).  The system was released only with the release of space to 85%.  Point. <br>  Moreover, try to fill the unit with 85-95%, create a dozen snapmirror sessions in different directions, load the system with I / O percent by 80, and then delete the partition, 1/5 the size of the unit.  The result - more than an hour of inoperative databases due to the fact that the system has gone to itself, the response time for all sections ranged from 300ms to 5s.  I didn‚Äôt react to the console, there was even a thought to reboot the controller, but first they began to urgently take off all the load from it, killed the replication on the receiving side, and after some time the system began to recover.  NetApp recommendation - kill files one by one, do not immediately remove 12TB via vol destroy. <br><br><h6>  Problem fifth - 2 units </h6><br>  But the joke is that if you have 100% loaded disks on one unit, then the second unit with a load of 10% will refuse to work, and the latency there will be comparable to the unit that has 100%.  This is directly related to how data is recorded in NetApp. <br>  The fact is that unlike classic block storage systems that can drive data directly to disks (write-through), if the block size is more than a certain value (usually 16kb or 32kb, but can be customized), NetApp never does that and Essentially always writes a write-back due to the peculiarities of the work of <a href="http://blog.aboutnetapp.ru/archives/19">WAFL</a> .  This is due to a large amount of memory on the controllers, even younger models.  And this is precisely the reason for the super-low latency on recording, of which NetApp is so proud if the system is not heavily loaded.  The cache is the same for all units, it overflows, it becomes impossible to write even in an unloaded unit. <br><br>  Conclusion - you should not overload disks by more than 60% if you want stable low latency on the system.  Well, it does not make sense to make a separate unit, if you have the same drives. <br><br><h6>  Small conclusions </h6><br><ul><li>  If there is a task to build a DR solution up to 50km, take MetroCluster and do not even think.  The difference in price is ridiculous, completely transparent to the hosts.  Of the minuses - highly desirable separate dark optics between the DC.  But in fact it works when dividing resources, it is checked.  You can fit into an existing SAN, but I did not tell you that. </li><li>  If there is a task to get low latency - do not allow disk loading of more than 50-60% (90-110 iops on a SAS disk 3.5 "). </li><li>  Try to avoid an intermittent load, the more linear the load, the easier it is for NetApp to digest it. </li><li>  Try to avoid loading with different blocks.  WAFL is extremely worried. </li><li>  Do not allow more than 85% of the load data of the unit. </li><li>  Get ready to change the iron warranty.  Lot.  Often.  But this is now affecting all manufacturers of primary and secondary systems. </li></ul><br>  Here, in general, probably all my observations for the year of communication with these systems.  The systems are actually not very bad, they are very easy to learn.  All pleasant administration. </div><p>Source: <a href="https://habr.com/ru/post/212453/">https://habr.com/ru/post/212453/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../212441/index.html">Anatomy of Ember.js (part one, theoretical)</a></li>
<li><a href="../212443/index.html">Migrating from raid1 to raid10 without losing data in Debian</a></li>
<li><a href="../212447/index.html">Social Cloud - All-In-One Collaboration Solution</a></li>
<li><a href="../212449/index.html">Connecting a Wacom Pro pen tablet to Linux or how bash helps artists</a></li>
<li><a href="../212451/index.html">Fake Debian developers require free access to Valve games</a></li>
<li><a href="../212455/index.html">MtGox problems and spambo-smith</a></li>
<li><a href="../212457/index.html">Analogue cmd from Python for Groovy</a></li>
<li><a href="../212459/index.html">Home current measurement</a></li>
<li><a href="../212461/index.html">UPS for router</a></li>
<li><a href="../212465/index.html">Difficulties when starting SMS service</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>