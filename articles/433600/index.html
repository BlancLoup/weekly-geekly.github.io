<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Phone Pixel 3 is learning to determine the depth in photos</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Portrait mode on Pixel smartphones allows you to take professional-looking photos that draw attention to the subject with a blurred background. Last y...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Phone Pixel 3 is learning to determine the depth in photos</h1><div class="post__text post__text-html js-mediator-article">  Portrait mode on Pixel smartphones allows you to take professional-looking photos that draw attention to the subject with a blurred background.  Last year, we described how we calculate depth using a single camera and Phase-Detection Autofocus (PDAF), also known as <a href="https://www.usa.canon.com/internet/portal/us/home/learn/education/topics/article/2018/July/Intro-to-Dual-Pixel-Autofocus-(DPAF)/Intro-to-Dual-Pixel-Autofocus-(DPAF)">dual-pixel autofocus</a> .  This process used the <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">traditional stereo algorithm</a> without learning.  This year on Pixel 3, we adopted machine learning to improve the depth estimate and to produce even better results in portrait mode. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/505/531/899/505531899e63adc78fbd74d94f1c3a3a.gif"><br>  <i>Left: original image captured in <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR +</a> .</i>  <i>To the right is a comparison of shooting results in portrait mode using depth from traditional stereo and machine learning.</i>  <i>Learning outcomes result in fewer errors.</i>  <i>In the traditional stereo result, the depth of many horizontal lines behind a man is incorrectly estimated to be equal to the depth of the man himself, as a result of which they remain sharp.</i> <br><a name="habracut"></a><br><h2>  A brief excursion into the previous material </h2><br>  <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Last year,</a> we described that portrait mode uses a neural network to separate pixels belonging to people and background images, and complements this two-level mask with depth information obtained from PDAF pixels.  All this was done to get a blur, depending on the depth close to what a professional camera can give. <br><br>  For PDAF, it takes two slightly different shots of the scene.  Switching between shots, you can see that the person does not move, and the background shifts horizontally - this effect is called <a href="https://ru.wikipedia.org/wiki/%25D0%259F%25D0%25B0%25D1%2580%25D0%25B0%25D0%25BB%25D0%25BB%25D0%25B0%25D0%25BA%25D1%2581">parallax</a> .  Since parallax is a function of the distance of a point from the camera and the distance between two points of view, we can determine the depth by comparing each point in one picture with its corresponding point in another. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://habrastorage.org/getpro/habr/post_images/e15/41c/044/e1541c044baa5454ddee71ef45c9a96c.gif"><br>  <i>The PDAF images on the left and in the middle look similar, but the parallax can be seen on the right in an enlarged fragment.</i>  <i>The easiest way to notice it is on the circular structure in the center of the increase.</i> <br><br>  However, the search for such correspondences in PDAF images (this method is called stereo depth) is an extremely difficult task, since the points between photos are shifted very little.  Moreover, all stereo technologies suffer from aperture problems.  If you look at the scene through a small aperture, it will be impossible to find the correspondence of points for lines parallel to the stereo baseline, that is, the line connecting the two cameras.  In other words, studying in the presented photo horizontal lines (or vertical lines in pictures with portrait orientation) all the shifts in one image relative to another look approximately the same.  In last year's portrait mode, all these factors could lead to errors in determining the depth and the appearance of unpleasant artifacts. <br><br><h2>  Improving depth assessment </h2><br>  With portrait mode in Pixel 3, we correct these errors, using the fact that the parallax of stereo photographs is just one of many clues that are present in the images.  For example, points that are far from the focus plane seem less sharp, and this will be a hint from the defocused depth.  In addition, even when viewing an image on a flat screen, we can easily estimate the distance to objects, since we know the approximate size of everyday objects (that is, we can use the number of pixels depicting a person‚Äôs face to estimate how far he is).  This will be a semantic hint. <br><br>  Manually developing an algorithm that combines these hints is extremely difficult, but using MO, we can do this while improving the performance of the hints from the PDAF parallax.  Specifically, we are training a <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">convolutional neural network</a> written in <a href="https://www.tensorflow.org/">TensorFlow</a> , which accepts pixels from a PDAF as an input and learns to predict depth.  This new, improved depth estimation method based on MO is used in Pixel 3 portrait mode. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7db/0ff/5e3/7db0ff5e3063425f703aaf2b3f30fd77.png"><br>  <i>Our convolutional neural network accepts a PDAF image as input and produces a depth map.</i>  <i>The network uses an encoder-decoder-style architecture with additional connections within the layer [ <a href="https://arxiv.org/abs/1505.04597">skip connections</a> ] and residual blocks [ <a href="https://arxiv.org/abs/1512.03385">residual blocks</a> ].</i> <br><br><h2>  Neural Network Training </h2><br>  To train the network, we need a lot of PDAF images and the corresponding high-quality depth maps.  And since we need the depth prediction to be useful in portrait mode, we need the training data to be similar to the photos that users take from smartphones. <br><br>  To do this, we designed a special device "Frankenfon", in which five Pixel 3 phones were combined and a WiFi connection was established between them, which allowed us to take photos from all phones at the same time (with a difference of no more than 2 ms).  With this device, we calculated high-quality depth maps based on photos, using both motion and stereo from several points of view. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/995/827/05b/99582705b2c447fa6faf95c5c114d20f.gif"><br>  <i>Left: device for collecting training data.</i>  <i>In the middle: an example of switching between five photos.</i>  <i>Synchronization of cameras ensures the ability to calculate the depth in dynamic scenes.</i>  <i>Right: Total Depth.</i>  <i>Low confidence points, where the juxtaposition of pixels in different photographs was uncertain due to the weakness of the textures, are colored black, and are not used in training.</i> <br><br>  The data obtained with this device turned out to be ideal for network training for the following reasons: <br><br><ul><li>  Five points of view guarantee parallax in several directions, which saves us from the problem of aperture. </li><li>  The location of the cameras ensures that any point of the image is repeated on at least two photographs, which reduces the number of points that cannot be matched. </li><li>  The baseline, that is, the distance between the cameras, is larger than that of the PDAF, which guarantees a more accurate depth estimate. </li><li>  Synchronization of cameras ensures the ability to calculate the depth in dynamic scenes. </li><li>  The portability of the device guarantees the possibility of taking photos in nature, simulating photos that users take with the help of smartphones. </li></ul><br>  However, despite the ideality of data obtained using this device, it is still extremely difficult to predict the absolute depth of scene objects ‚Äî any given PDAF pair can correspond to various depth maps (everything depends on the characteristics of the lenses, focal length, etc.).  To take all this into account, we estimate the relative depth of the objects in the scene, which is enough to obtain satisfactory results in portrait mode. <br><br><h2>  Combine it all </h2><br>  Estimation of depth using MO on Pixel 3 should work quickly so that users do not have to wait too long for the results of images in portrait mode.  However, to obtain good depth estimates using small defocusing and parallax, one has to feed the neural network photos in full resolution.  To ensure fast results, we use <a href="https://www.tensorflow.org/lite/">TensorFlow Lite</a> , a cross-platform solution for running MO models on mobile and embedded devices, as well as a powerful Pixel 3 GPU, which allows you to quickly calculate the depth of unusually large input data.  Then we combine the obtained depth estimates with masks from our neural network, highlighting people, in order to get beautiful shooting results in portrait mode. <br><br><h2>  Try it yourself </h2><br>  In the Google Camera App versions 6.1 and above, our depth maps are embedded in images of portrait mode.  This means that you can use the <a href="https://blog.google/products/photos/keep-your-favorite-photos-date-live-albums/">Google Photos Depth Editor</a> to change the degree of blur and focus point after you have taken the shot.  You can also use <a href="https://github.com/drewnoakes/metadata-extractor-images/blob/master/jpg/metadata/Android%2520Depth%2520Map.jpg.txt">third-party</a> programs to extract depth maps from jpeg, and study them yourself.  Also <a href="https://photos.app.goo.gl/KQWZwexwvJX6JyH16">via the link</a> you can take an album showing relative depth maps and corresponding images in portrait mode, for comparing the traditional stereo and MO-approach. </div><p>Source: <a href="https://habr.com/ru/post/433600/">https://habr.com/ru/post/433600/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../433582/index.html">What if the 30/70 profit sharing ceases to be the gamedev standard?</a></li>
<li><a href="../433586/index.html">How we didn't win the hackathon</a></li>
<li><a href="../433588/index.html">The amazing performance of parallel C ++ 17 algorithms. Myth or Reality?</a></li>
<li><a href="../433592/index.html">Background: Yandex. Phone</a></li>
<li><a href="../433596/index.html">Magellan error: Buffer overrun or world expedition using SQLite FTS</a></li>
<li><a href="../433602/index.html">The basis of the rate of evolution may be mathematical simplicity.</a></li>
<li><a href="../433604/index.html">Comfortable work with Android Studio</a></li>
<li><a href="../433606/index.html">SIEM depths: out-of-box correlations. Part 3.2. Event Normalization Methodology</a></li>
<li><a href="../433608/index.html">Car of the future. Screens instead of autoglass?</a></li>
<li><a href="../433610/index.html">Notes phytochemist. Persimmon</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>