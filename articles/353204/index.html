<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>About streams and tables in Kafka and Stream Processing, part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="* Michael G. Noll is an active contributor to Open Source projects, including Apache Kafka and Apache Storm. 

 The article will be useful primarily f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>About streams and tables in Kafka and Stream Processing, part 1</h1><div class="post__text post__text-html js-mediator-article">  <i>* Michael G. Noll is an active contributor to Open Source projects, including Apache Kafka and Apache Storm.</i> <i><br><br></i>  <i>The article will be useful primarily for those who are just acquainted with Apache Kafka and / or stream processing.</i> <br><br>  In this article, perhaps in the first of a mini-series, I want to explain the concepts of <i>Streams</i> and Tables in stream processing and, in particular, in <b>Apache Kafka</b> .  I hope you will have a better theoretical presentation and ideas that will help you solve your current and future tasks better and / or faster. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Content: <br><br>  * Motivation <br>  * Streams and Tables in simple language <br>  * Illustrated examples <br>  * Streams and Tables in Kafka in simple language <br>  * A closer look at Kafka Streams, KSQL and analogues in Scala <br>  * Tables stand on the shoulders of giants (on streams) <br>  * Turning the Database Inside-Out <br>  * Conclusion <br><a name="habracut"></a><br><h2>  Motivation, or why should it care? </h2><br>  In my daily work, I communicate with many Apache Kafka users and those who do streaming with Kafka through <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams</a> and <a href="https://github.com/confluentinc/ksql">KSQL</a> (streaming SQL for Kafka).  Some users already have experience in stream processing or using Kafka, some have experience using RDBMS, such as Oracle or MySQL, some have neither one nor the other experience. <br><br>  A frequently asked question: ‚ÄúWhat is the difference between streams and tables?‚Äù In this article I want to give both answers: both short (TL; DR) and long, so that you can get a deeper understanding.  Some of the explanations below will be slightly simplified, because it simplifies understanding and memorization (for example, as a simpler model of Newton's attraction is quite sufficient for most everyday situations, which saves us from having to go directly to Einstein‚Äôs relativistic model, <u>but so</u> complicated). <br><br>  Another common question: ‚ÄúWell, but why should I care?  How will this help me in my daily work? ‚ÄùIn short, for many reasons!  As you begin to use stream processing, you will soon realize that in practice, in most cases, <u>both</u> streams <u>and</u> tables are required.  Tables, as I will explain later, represent a state.  Whenever you perform any processing <u>with</u> <i>[stateful processing]</i> , such as <i>joins</i> (for example, enriching data <i>[ <a href="https://www.techopedia.com/definition/28037/data-enrichment">data enrichment</a> ]</i> in real time by combining the flow of facts with dimension tables <i>[ <a href="https://en.wikipedia.org/wiki/Dimension_(data_warehouse)">dimension tables</a> ]</i> ) or <i>aggregation ]</i> (for example, real-time calculation of the average for key business indicators in 5 minutes), then the tables introduce a streaming picture.  Otherwise, this means that you will have to do it yourself <i>[a lot of DIY pain]</i> . <br><br>  Even the notorious WordCount example, probably your first ‚ÄúHello World‚Äù from this area, falls into the ‚Äúwith state‚Äù category: this is an example of processing with a state where we aggregate a stream of rows into a continuously updated table / map to count words.  Thus, regardless of whether you implement WordCount streaming or something more complicated, like <i>fraud detection</i> , you want an easy-to-use streaming solution with basic data structures and everything you need inside (hint: streams and tables ).  You certainly don‚Äôt want to build a complex and unnecessary architecture where you need to combine (only) stream processing technology with remote storage, such as Cassandra or MySQL, and possibly with the addition of Hadoop / HDFS to ensure fault tolerance processing <i>[fault-tolerance processing ]</i> (three things - too much). <br><br><h2>  Stream and Tables in simple language </h2><br>  Here is the best analogy I could come up with: <br><br><ul><li>  <b>Stream</b> in Kafka is the complete history of all the events that have happened (or just business events) in the world <u>from the beginning of time to the present day</u> .  He represents the past and present.  As we move from today to tomorrow, new developments are constantly added to world history. </li><li>  <b>The table</b> in Kafka is a state of the world <u>today</u> .  She is present.  It is the <i>aggregation of</i> all the events in the world that is constantly changing as we move from today to tomorrow. </li></ul><br>  And as an aperitif to the future post: if you have access to the entire history of events in the world (stream), then you can restore the state of the world <u>at any time</u> , that is, a table at an arbitrary time <code>t</code> in the stream, where <code>t</code> not limited to only <code>t=</code>  In other words, we can create ‚Äúsnapshots‚Äù of the state of the world (table) at any time point <code>t</code> , for example, 2560 BC, when the Great Pyramid was built at Giza, or 1993, when the European Union <br><br><h2>  Illustrated examples </h2><br>  The first example shows a <b>stream</b> with geo-locations of users that are aggregated into a table fixing the <u>current (last) position of each user</u> .  As I will explain later, this also turns out to be the default semantics for tables when you read the <i>[Topic]</i> Kafka topic directly into the table. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/714/a96/2f2/714a962f243e65ecffd247fa2e9b9f44.gif" alt="Saving in the table the last position according to data from the stream"><br><br>  The second usage example demonstrates the same <b>stream of</b> user geolocation updates, but now the stream is aggregated into a <b>table</b> that records the <u>number of places visited by each user</u> .  Since the aggregation function is different (here: counting the number), the table contents are also different.  More precisely, other values ‚Äã‚Äãby key. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/69d/cad/9d6/69dcad9d61446c057b1f50fde3cd3aae.gif" alt="Saving in the table the current number of visited places"><br><br><h2>  Stream and Tables in Kafka in simple language </h2><br>  Before we get into the details, let's start with a simple one. <br><br>  <b>A topic</b> in Kafka is an unlimited sequence of key-value pairs.  Keys and values ‚Äã‚Äãare ordinary byte arrays, i.e.  <code>&lt;byte[], byte[]&gt;</code> . <br><br>  <b>Stream</b> is a topic with a <i>schema</i> .  Keys and values ‚Äã‚Äãare no longer arrays of bytes, but have a specific type. <br><blockquote>  Example: <code>&lt;byte[], byte[]&gt;</code> topic is read as <code>&lt;User, GeoLocation&gt;</code> stream user geolocation. </blockquote><br>  <b>A table</b> is a table in the usual sense of the word (I feel the joy of those of you who are already familiar with RDBMS and are just getting acquainted with Kafka).  But looking through the prism of stream processing, we see that the table is also an <u>aggregated stream</u> (you really didn‚Äôt expect us to dwell on the definition ‚Äútable is a table‚Äù, is it?). <br><blockquote>  Example: <code>&lt;User, GeoLocation&gt;</code> stream with geodata updates is aggregated into a <code>&lt;User, GeoLocation&gt;</code> table, which tracks the user's last position.  At the aggregation stage, <i>[UPSERT]</i> values ‚Äã‚Äãin the table are updated according to the key from the input stream.  We saw this in the first illustrated example above. </blockquote><blockquote>  Example: <code>&lt;User, GeoLocation&gt;</code> stream is aggregated into <code>&lt;User, Long&gt;</code> table, which tracks the number of visited locations for each user.  At the stage of aggregation, values ‚Äã‚Äãare continuously calculated (and updated) by keys in the table.  We saw this in the second illustrated example above. </blockquote><br>  Total: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e8b/606/47f/e8b60647f7660816bc8bc67e0ef13760.png" alt="Topic-Stream-Table"><br><br>  Topics, streams and tables have the following properties in Kafka: <br><table><thead><tr><th>  Type of </th><th>  There are partitions </th><th>  Is not limited </th><th>  There is order </th><th>  Changeable </th><th>  Key uniqueness </th><th>  Scheme </th></tr></thead><tbody><tr><td>  Topic </td><td>  Yes </td><td>  Yes </td><td>  Yes </td><td>  Not </td><td>  Not </td><td>  Not </td></tr><tr><td>  Stream </td><td>  Yes </td><td>  Yes </td><td>  Yes </td><td>  Not </td><td>  Not </td><td>  Yes </td></tr><tr><td>  Table </td><td>  Yes </td><td>  Yes </td><td>  Not </td><td>  Yes </td><td>  Yes </td><td>  Yes </td></tr></tbody></table><br>  Let's look at how topics, streams, and tables relate to the Kafka Streams API and KSQL, as well as draw analogies with programming languages ‚Äã‚Äã(it is ignored in analogies, for example, that topics / streams / tables can be partitioned): <br><table><thead><tr><th>  Type of </th><th>  Kafka streams </th><th>  KSQL </th><th>  Java </th><th>  Scala </th><th>  Python </th></tr></thead><tbody><tr><td>  Topic </td><td>  - </td><td>  - </td><td> <code>List / Stream</code> </td> <td> <code>List / Stream [(Array[Byte], Array[Byte])]</code> </td> <td> <code>[]</code> </td> </tr><tr><td>  Stream </td><td> <code>KStream</code> </td> <td> <code>STREAM</code> </td> <td> <code>List / Stream</code> </td> <td> <code>List / Stream [(K, V)]</code> </td> <td> <code>[]</code> </td> </tr><tr><td>  Table </td><td> <code>KTable</code> </td> <td> <code>TABLE</code> </td> <td> <code>HashMap</code> </td> <td> <code>mutable.Map[K, V]</code> </td> <td> <code>{}</code> </td> </tr></tbody></table><br>  But this summary at this level may be of little use to you.  So let's take a closer look. <br><br><h2>  A closer look at Kafka Streams, KSQL and analogues in Scala </h2><br>  I will start each of the following sections with the analogy in Scala (imagine that stream processing is done on the same machine) and the Scala REPL so that you can copy the code and play with it yourself, then I will explain how to do the same in Kafka Streams and KSQL (flexible, scalable and fault tolerant stream processing on distributed machines).  As I mentioned at the beginning, I simplify the explanations below a little.  For example, I will not consider the influence of partitioning in Kafka. <br><blockquote>  <b>If you do not know Scala:</b> Do not be embarrassed!  You do not need to understand Scala analogs in all details.  It is enough to pay attention to what operations (for example, <code>map()</code> ) are connected together, what they are (for example, <code>reduceLeft()</code> is an aggregation), and how the ‚Äúchain‚Äù of streams relates to the ‚Äúchain‚Äù of tables. </blockquote><br><h3>  Topics </h3><br>  A topic in Kafka consists of key-value messages.  The topic does not depend on the serialization format or the ‚Äútype‚Äù of messages: the keys and values ‚Äã‚Äãin the messages are treated as ordinary <code>byte[]</code> arrays.  In other words, from this point of view, we have no idea what is inside the data. <br><br>  Kafka Streams and KSQL do not have a ‚Äútopic‚Äù concept.  They only know about streams and tables.  Therefore, I will show here only an analogue of the topic in Scala. <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// Scala analogy scala&gt; val topic: Seq[(Array[Byte], Array[Byte])] = Seq((Array(97, 108, 105, 99, 101),Array(80, 97, 114, 105, 115)), (Array(98, 111, 98),Array(83, 121, 100, 110, 101, 121)), (Array(97, 108, 105, 99, 101),Array(82, 111, 109, 101)), (Array(98, 111, 98),Array(76, 105, 109, 97)), (Array(97, 108, 105, 99, 101),Array(66, 101, 114, 108, 105, 110)))</span></span></code> </pre> <br><h3>  Streams </h3><br>  Now we are reading the topic in the stream, adding information about the schema (schema for reading <i>[schema-on-read]</i> ).  In other words, we turn a raw, untyped topic into a ‚Äútyped topic‚Äù or stream. <br><br><blockquote>  <b>Schema for reading vs Schema for writing <i>[schema-on-write]</i> :</b> Kafka and its topics do not depend on the serialization format of your data.  Therefore, you must specify a schema when you want to read the data in a stream or table.  This is called <u>a read scheme</u> .  The reading scheme has both advantages and disadvantages.  Fortunately, you can choose an intermediate between a read scheme and a write scheme, defining a contract for your data ‚Äî much like you probably define API contracts in your applications and services.  This can be achieved by choosing a structured, but extensible data format, such as <a href="https://avro.apache.org/">Apache Avro,</a> with a <a href="https://avro.apache.org/">roll</a> -out of the registry for your Avro-schemas, such as <a href="https://github.com/confluentinc/schema-registry">Confluent Schema Registry</a> .  And yes, both Kafka Streams and KSQL support Avro, if you're interested. </blockquote><br>  In Scala, this is achieved using the <code>map()</code> operation below.  In this example, we get a stream from the <code>&lt;String, String&gt;</code> pairs.  Notice how we can now look inside the data. <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// Scala analogy scala&gt; val stream = topic | .map { case (k: Array[Byte], v: Array[Byte]) =&gt; new String(k) -&gt; new String(v) } // =&gt; stream: Seq[(String, String)] = // List((alice,Paris), (bob,Sydney), (alice,Rome), (bob,Lima), (alice,Berlin))</span></span></code> </pre> <br>  In Kafka Streams, you read the topic in <code>KStream</code> via <code>StreamsBuilder#stream()</code> .  Here you have to define the desired scheme using the <code>Consumed.with()</code> parameter when reading data from the topic: <br><br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">StreamsBuilder</span></span> builder = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-type"><span class="hljs-type">StreamsBuilder</span></span>(); <span class="hljs-type"><span class="hljs-type">KStream</span></span>&lt;<span class="hljs-type"><span class="hljs-type">String</span></span>, <span class="hljs-type"><span class="hljs-type">String</span></span>&gt; stream = builder.stream(<span class="hljs-string"><span class="hljs-string">"input-topic"</span></span>, <span class="hljs-type"><span class="hljs-type">Consumed</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">with</span></span>(<span class="hljs-type"><span class="hljs-type">Serdes</span></span>.<span class="hljs-type"><span class="hljs-type">String</span></span>(), <span class="hljs-type"><span class="hljs-type">Serdes</span></span>.<span class="hljs-type"><span class="hljs-type">String</span></span>()));</code> </pre> <br>  In KSQL, you have to do something like the following in order to read the topic as <code>STREAM</code> .  Here you define the desired scheme by specifying the names and types of columns when reading data from the topic: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> STREAM myStream (username <span class="hljs-built_in"><span class="hljs-built_in">VARCHAR</span></span>, location <span class="hljs-built_in"><span class="hljs-built_in">VARCHAR</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> (KAFKA_TOPIC=<span class="hljs-string"><span class="hljs-string">'input-topic'</span></span>, VALUE_FORMAT=<span class="hljs-string"><span class="hljs-string">'...'</span></span>)</code> </pre> <br><h3>  Tables </h3><br>  Now we are reading the same topic in the table.  First, we need to add schema information (a schema for reading).  Secondly, you must convert the stream to a table.  The semantics of the table in Kafka states that the final table should map each message key from the topic to the last value for that key. <br><br>  Let's first use the first example, where the summary table tracks the last location of each user: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/714/a96/2f2/714a962f243e65ecffd247fa2e9b9f44.gif" alt="Stream conversion to a table"><br><br>  In Scala: <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// Scala analogy scala&gt; val table = topic | .map { case (k: Array[Byte], v: Array[Byte]) =&gt; new String(k) -&gt; new String(v) } | .groupBy(_._1) | .map { case (k, v) =&gt; (k, v.reduceLeft( (aggV, newV) =&gt; newV)._2) } // =&gt; table: scala.collection.immutable.Map[String,String] = // Map(alice -&gt; Berlin, bob -&gt; Lima)</span></span></code> </pre> <br>  Adding information about the scheme is achieved using the first <code>map()</code> operation - just like in the example with the stream above.  Stream is converted into a <i>[stream-to-table] table</i> using an <u>aggregation step</u> (more on this later), which in this case is a (stateless) UPSERT operation on the table: this is a <code>groupBy().map()</code> step, which contains the <code>reduceLeft()</code> operation for each key.  Aggregation means that for each key we compress the set of values ‚Äã‚Äãinto one.  Note that this particular aggregation of <code>reduceLeft()</code> without state ‚Äî the previous value of aggV is not used when calculating a new value for a given key. <br><br>  What is interesting about the relationship between streams and tables is that the command above creates a table equivalent to the short version below (remember <i>referential transparency</i> ), where we build the table directly from the stream, which allows us to skip the schema / type setting because the stream is already typed.  We can see that the <u>table is <i>derivation</i> , stream aggregation</u> : <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// Scala analogy, simplified scala&gt; val table = stream | .groupBy(_._1) | .map { case (k, v) =&gt; (k, v.reduceLeft( (aggV, newV) =&gt; newV)._2) } // =&gt; table: scala.collection.immutable.Map[String,String] = // Map(alice -&gt; Berlin, bob -&gt; Lima)</span></span></code> </pre> <br>  In Kafka Streams, you usually use <code>StreamsBuilder#table()</code> to read a Kafka topic in <code>KTable</code> simple one-liner: <br><br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">KTable</span></span>&lt;<span class="hljs-type"><span class="hljs-type">String</span></span>, <span class="hljs-type"><span class="hljs-type">String</span></span>&gt; table = builder.table(<span class="hljs-string"><span class="hljs-string">"input-topic"</span></span>, <span class="hljs-type"><span class="hljs-type">Consumed</span></span>.<span class="hljs-keyword"><span class="hljs-keyword">with</span></span>(<span class="hljs-type"><span class="hljs-type">Serdes</span></span>.<span class="hljs-type"><span class="hljs-type">String</span></span>(), <span class="hljs-type"><span class="hljs-type">Serdes</span></span>.<span class="hljs-type"><span class="hljs-type">String</span></span>()));</code> </pre> <br>  But for clarity, you can also read the topic first in <code>KStream</code> , and then perform the same aggregation step as shown above to turn <code>KStream</code> into <code>KTable</code> . <br><br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">KStream</span></span>&lt;<span class="hljs-type"><span class="hljs-type">String</span></span>, <span class="hljs-type"><span class="hljs-type">String</span></span>&gt; stream = ...; <span class="hljs-type"><span class="hljs-type">KTable</span></span>&lt;<span class="hljs-type"><span class="hljs-type">String</span></span>, <span class="hljs-type"><span class="hljs-type">String</span></span>&gt; table = stream .groupByKey() .reduce((aggV, newV) -&gt; newV);</code> </pre> <br>  In KSQL, you have to do something like the following in order to read the topic as <code>TABLE</code> .  Here you must define the desired scheme by specifying the names and types for the columns when reading from the topic: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> myTable (username <span class="hljs-built_in"><span class="hljs-built_in">VARCHAR</span></span>, location <span class="hljs-built_in"><span class="hljs-built_in">VARCHAR</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">WITH</span></span> (KAFKA_TOPIC=<span class="hljs-string"><span class="hljs-string">'input-topic'</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">KEY</span></span>=<span class="hljs-string"><span class="hljs-string">'username'</span></span>, VALUE_FORMAT=<span class="hljs-string"><span class="hljs-string">'...'</span></span>)</code> </pre> <br>  What does this mean?  This means that the table is actually an <i>aggregated stream</i> , as we said at the very beginning.  We saw this directly in the special case above, when the table was created directly from the topic.  However, this is actually the general case. <br><br><h2>  Tables stand on the shoulders of giants (on streams) </h2><br>  Conceptually, only stream is the construction of first-order data in Kafka.  On the other hand, the table is either (1) derived from an existing stream by means of a key <i>[per-key]</i> aggregation, or (2) is derived from an existing table, which always expands to an aggregated stream (we could call the last tables ‚Äúprotostream‚Äù <i>[ "Ur-stream"]</i> ). <br><blockquote>  Tables are often also described as a <i>materialized view of the</i> stream.  Stream representation is nothing but aggregation in this context. </blockquote><br>  Of the two cases, the more interesting discussion is (1), so let's focus on that.  And this probably means that I need to first figure out how the aggregation works in Kafka. <br><br><h3>  Aggregations in Kafka </h3><br>  Aggregation is one type of stream processing.  Other types, for example, include filtering <i>[filters]</i> and <i>joins [joins]</i> . <br><br>  As we found out earlier, the data in Kafka are presented in the form of key-value pairs.  Further, the first property of aggregations in Kafka is that they are all calculated <u>by key</u> .  That is why we have to group <code>KStream</code> before the aggregation stage in Kafka Streams via <code>groupBy()</code> or <code>groupByKey()</code> .  For the same reason, we had to use <code>groupBy()</code> in the Scala examples above. <br><blockquote>  <b>Partitioning <i>[partition]</i> and message keys:</b> An equally important aspect of Kafka, which I ignore in this article, is that the topics, streams, and tables are <u>partitioned</u> .  In fact, the data is processed and aggregated by key by partitions.  By default, messages / records are divided into partitions based on their keys, so in practice the simplification of ‚Äúaggregation by key‚Äù instead of the technically more complex and more correct ‚Äúaggregation by key by partition‚Äù is quite acceptable.  But if you use the custom <i>partitioning assigners</i> , then you must take this into account in your processing logic. </blockquote><br>  The second property of aggregations in Kafka is that aggregations are continuously updated as soon as new data enters the incoming streams.  Together with the property of computing by key, this requires the presence of a table or, more precisely, it requires <u>a</u> <i>mutable table</i> as a result and, therefore, the type of returned aggregations.  Previous values ‚Äã‚Äã(aggregation results) for the key are constantly overwritten with new values.  Both in Kafka Streams and in KSQL, aggregations always return a table. <br><br>  Let us return to our second example, in which we want to calculate, by our flow, the number of places visited by each user: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/69d/cad/9d6/69dcad9d61446c057b1f50fde3cd3aae.gif" alt="Stream conversion to a table"><br><br>  Counting is the type of aggregation.  To calculate the values, we only need to replace the aggregation stage from the previous section <code>.reduce((aggV, newV) -&gt; newV)</code> with <code>.map { case (k, v) =&gt; (k, v.length) }</code> .  Note that the return type is a table / map (and, please, ignore the fact that in the Scala code, <code>map</code> immutable <i>[immutable map]</i> , because Scala uses immutable maps by default). <br><br><pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">// Scala analogy scala&gt; val visitedLocationsPerUser = stream | .groupBy(_._1) | .map { case (k, v) =&gt; (k, v.length) } // =&gt; visitedLocationsPerUser: scala.collection.immutable.Map[String,Int] = // Map(alice -&gt; 3, bob -&gt; 2)</span></span></code> </pre> <br>  The code on Kafka Streams is equivalent to the Scala example above: <br><br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">KTable</span></span>&lt;<span class="hljs-type"><span class="hljs-type">String</span></span>, <span class="hljs-type"><span class="hljs-type">Long</span></span>&gt; visitedLocationsPerUser = stream .groupByKey() .count();</code> </pre> <br>  In KSQL: <br><br><pre> <code class="sql hljs"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> visitedLocationsPerUser <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> username, <span class="hljs-keyword"><span class="hljs-keyword">COUNT</span></span>(*) <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> myStream <span class="hljs-keyword"><span class="hljs-keyword">GROUP</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> username;</code> </pre> <br><h3>  Tables - aggregated streams (input stream ‚Üí table) </h3><br>  As we saw above, tables are aggregations of their input streams, or, in short, tables are aggregated streams.  Whenever you perform an aggregation in Kafka Streams or KSQL, the result is always a table. <br><br>  The feature of the aggregation stage determines whether the table is directly retrieved from the stream via UPSERT stateless semantics (the table maps the keys to their last value in the stream, which is aggregation when reading the Kafka topic directly into the table), by counting the number of seen values ‚Äã‚Äãfor each key <i>[stateful counting]</i> states (see our last example), or more complex aggregations, such as summation, averaging, and so on.  When using Kafka Streams and KSQL, you have many options for aggregation, including windowed aggregations with ‚Äútumbling windows‚Äù, ‚Äúhopping‚Äù windows, and ‚Äúsession‚Äù windows. <br><br><h3>  There are streams of changes in the tables (table ‚Üí output stream) </h3><br>  Although the tables is an input stream aggregation, it also has its own output stream!  Like change data records (CDC) in databases, every change in a table in Kafka is recorded in an internal stream of changes called the <u>changelog stream</u> table.  Many calculations in Kafka Streams and KSQL are actually performed on the <i>changelog stream</i> .  This allows Kafka Streams and KSQL, for example, to correctly process historical data in accordance with the semantics of <i>event time processing [event-time processing semantics]</i> - remember that the stream represents both the present and the past, while the table can represent only the present (or, more precisely, the fixed moment of time <i>[snapshot in time]</i> ). <br><blockquote>  <b>Note:</b> In Kafka Streams, you can explicitly convert a table to a stream of <i>[changelog stream]</i> changes via <code>KTable#toStream()</code> . </blockquote><br>  Here is the first example, but now with the <i>changelog stream</i> : <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d2/a8f/75f/5d2a8f75f358f373c06ce59c84e208d6.gif" alt="Changelog stream"><br><br>  Please note that the <i>changelog stream of the</i> table is a copy of the input stream of this table.  This is due to the nature of the corresponding aggregation function (UPSERT).  And if you're wondering: ‚ÄúWait, isn't it 1 to 1 copying, wasting disk space?‚Äù - Under the hood, Kafka Streams and KSQL are optimized to minimize unnecessary data copying and local / network IO.  I ignore these optimizations in the diagram above to better illustrate what is happening in principle. <br><br>  And finally, a second usage example, which includes the <i>changelog stream</i> .  Here the stream of table changes is different, because here is another aggregation function that performs <i>per-key</i> counts. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7ee/020/310/7ee02031023664005af83a4a31179bb6.gif" alt="Changelog stream"><br><br>  But these internal <i>changelog streams</i> also have an architectural and operational impact.  The streams of changes are continuously backed up and saved as topics in Kafka, and the topic itself is part of the magic that provides elasticity and resiliency in Kafka Streams and KSQL.  This is due to the fact that they allow you to move processing tasks between machines / virtual machines / containers without losing data and for all operations, regardless of whether the processing is with <i>[stateful]</i> or without <i>[stateless]</i> .  The table is part of the state <i>[state] of</i> your application (Kafka Streams) or query (KSQL), so Kafka is required to transfer not only processing code (which is easy), but also processing states, including tables, between machines in a fast and reliable way ( which is much more complicated).  Whenever the table has to be moved from client machine A to machine B, then on a new assignment B the table is reconstructed from its <i>changelog stream</i> to Kafka (on the server side) the exact same state as on machine A. We can see it on The last diagram above, where the ‚Äú <i>counting table‚Äù []</i> can be easily restored from its <i>changelog stream</i> without the need to recycle the input stream. <br><br><h3>  Duality Stream Table </h3><br>  The term <a href="https://docs.confluent.io/current/streams/concepts.html">stream-table duality</a> refers to the above relationship between streams and tables.  This means, for example, that you can turn a stream into a table, this table into another stream, a received stream into another table, and so on.  For more information, see the Confluent blog post: <a href="https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/">Introduction to Kafka Streams: Stream Processing Made Simple</a> . <br><br><h2>  Turning the Database Inside-Out </h2><br>  In addition to what we covered in the previous sections, you may have come across the <a href="https://www.confluent.io/blog/turning-the-database-inside-out-with-apache-samza/">Turning the Database Inside-Out</a> article, and now you might be interested in looking at this in its entirety?  Since I don‚Äôt want to go into details now, let me briefly compare the world of Kafka and stream processing with the world of databases.  Be vigilant: further, <i>black-and-white simplifications</i> . <br><br>  In databases, the <u>table</u> is a first-order construction.  This is what you work with.  ‚ÄúStreams‚Äù also exist in databases, for example, in the form of a <a href="https://dev.mysql.com/doc/refman/5.7/en/binary-log.html">binlog in MySQL</a> or <a href="http://www.oracle.com/technetwork/middleware/goldengate/overview/index.html">GoldenGate in Oracle</a> , but they are usually hidden from you in the sense that you cannot interact with them directly.  The database is aware of the present, but it does not know about the past (if you need the past, recover data from your <i>backup tapes</i> , which, ha ha, just hardware streams). <br><br>  In Kafka and stream processing, <u>stream</u> is a first order construct.  Tables are derivatives of streams <i>[derivations of streams]</i> , as we have seen before.  Stream knows about the present and about the past.  As an example, the <a href="https://open.nytimes.com/publishing-with-apache-kafka-at-the-new-york-times-7f0e3b7d2077">New York Times keeps all published articles</a> - 160 years of journalism from the 1850s - at Kafka, the source of reliable data. <br><br>  In short: the database thinks first with a table, and then with a stream.  Kafka thinks first with stream and then with the table.   , Kafka- ,          ,   ‚Äî   ,   WordCount,           ,        . , Kafka         ,        Kafka Streams  KSQL,       (  ).     Kafka    ,   , <a href="https://yokota.blog/2018/03/05/stream-relational-processing-platforms/">-</a> <i>[stream-relational]</i> ,     [stream-only]. <br><blockquote>     ,   ‚Äî . Kafka   ,   ‚Äî . </blockquote><br><h2>  Conclusion </h2><br> ,       ,        Kafka     . ,     ,            ¬´    ¬ª  ¬´    Kafka  ¬ª. <br><br>         -    Kafka, Kafka Streams  KSQL,    : <br><br><ul><li>  ,   <a href="https://github.com/confluentinc/ksql">KSQL</a> ,  SQL-  Kafka,     Kafka   - .  ,        ,      Kafka   ,         .      KSQL clickstream (   Docker),      Kafka, KSQL, Elasticsearch  Grafana     <i>real-time dashboard</i> . </li><li>  ,   Java  Scala       <a href="https://kafka.apache.org/documentation/streams/">Kafka Streams API</a> . </li><li>  , , ,   ,          KSQL,     Kafka Streams,      KSQL. </li></ul><br>   ,   Kafka Streams  KSQL,  Kafka   ,      ,    (, , , ,  ,  ,  ).  ,    .  :-) <br><br> ,     ¬´  ,  1¬ª.          ,         ,        .      ?           ! <br><br> <i>      ‚Äî , ,      .</i> </div><p>Source: <a href="https://habr.com/ru/post/353204/">https://habr.com/ru/post/353204/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../353194/index.html">For what programmer Continuous Integration and where to start</a></li>
<li><a href="../353196/index.html">Divan Vice President: how I work as a product director on a complete remote</a></li>
<li><a href="../353198/index.html">SelectelTechDay: how it was</a></li>
<li><a href="../353200/index.html">Episode 0. Hack vs Mac. Xcode build time</a></li>
<li><a href="../353202/index.html">Marvin Minsky "The Emotion Machine": Chapter 1 "How we control ourselves"</a></li>
<li><a href="../353206/index.html">93-byte Snake Game</a></li>
<li><a href="../353208/index.html">Another boxed CMS or a worthy alternative?</a></li>
<li><a href="../353210/index.html">How do users teach Yandex to warn about phone spam</a></li>
<li><a href="../353212/index.html">Dockarize Socket.io, redis and php</a></li>
<li><a href="../353214/index.html">Why in 2018 I use the development method, which is already 30 years old</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>