<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How we did the autopilot for the service station</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! I work in a small startup in Berlin that develops auto pilots for cars. We are finishing the project for the service stations of a major Ger...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How we did the autopilot for the service station</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  I work in a small startup in Berlin that develops auto pilots for cars.  We are finishing the project for the service stations of a major German automaker and I would like to tell you about it: how we did it, what difficulties we encountered and what new we discovered.  In this part I will tell you about the perception module and a little about the architecture of the solution as a whole.  About the remaining modules, perhaps, we will tell in the following sections.  I would be very happy feedback and view from the side of our approach. <br><a name="habracut"></a><br>  The press release of the project by the customer can be found <a href="https://newsroom.porsche.com/ru/2019/company/ru-porsche-autonomous-driving-workshop-cooperation-kopernikus-automotive-startup-autobahn-17017.html">here</a> . <br><br>  To begin with, I will say why the automaker turned to us, and did not make the project on its own.  It is difficult for large German concerns to change processes, and the format of developing cars is rarely suitable for software - iterations are long and require good planning.  It seems to me that German automakers understand this, and therefore you can meet startups that are founded by them, but work as an independent company (for example, AID from Audi and Zenuity from Volvo).  Other automakers organize events like Startup Autobahn, where they are looking for possible contractors for tasks and new ideas.  They can order a product or a prototype and get the finished result in a short period of time.  This may turn out to be faster than trying to do the same thing yourself, and in terms of costs, it is no more expensive than its own design.  The complexity of changing processes well demonstrates the number of permissions required to start testing the autopilot machine with customers: consent to video filming people (even if we do not save data, and streaming video is used only anonymously without identifying specific people), consent to video shooting territories, the consent of the trade union and the working consul to test these technologies, the consent of the security service, the consent of the IT service - this is not the whole list. <br><br><h2>  Task </h2><br>  In the current project, the customer wants to understand whether it is possible to control the vehicles in the service centers with the help of ‚ÄúAI‚Äù.  The user script is as follows: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ol><li>  A technician wants to start working with a machine that is somewhere in the parking lot outside the test area. </li><li>  He chooses a car on the tablet, selects the service box, and clicks ‚ÄúCall in‚Äù. </li><li>  The car drives in and stops at the end point (elevator, ramp, or something else). </li><li>  When the technician finishes work on the machine, he presses the button on the tablet, the car leaves and parks on some free space outside. </li></ol><br>  Features: Not all machines have cameras.  On those machines on which they are, we do not have access to them.  The only data on the machine that we have access to is sonar and odometer <br><br><div class="spoiler">  <b class="spoiler_title">Sonary and odometry</b> <div class="spoiler_text">  Sonars are distance sensors, are installed in a circle on a car and often look like round dots, they allow you to estimate the distance to an object, but only close and with low accuracy.  Odometry - data on the actual speed and direction of the car.  Knowing this data and the initial position, one can quite accurately determine the current position of the machine. <br></div></div><br>  Thus, the car should be controlled by external sensors installed in the service area. <br><br><h2>  Decision </h2><br>  The architecture of the final product is as follows: <br><br><ul><li>  In the service area, we install external cameras, lidars and other things not (hello Tesla). </li><li>  The data from the cameras is sent to Jetson TX2 (three cameras each), which are concerned with the task of searching for the machine and preprocessing images from the cameras. </li><li>  Next, these cameras come to the central server, which is proudly called the Control Tower and on which they fall into the perception, tracking and path-planning modules.  As a result of the analysis, a decision is made on the further direction of the car‚Äôs movement and it is sent to the car. </li><li>  At this stage of the project, another Jetson TX2 is put in the car, which with the help of our driver connects to the Vector, which decrypts the vehicle data and sends commands.  The TX2 receives control commands from the central server and transmits them to the vehicle. </li></ul><br>  For infrastructure level, <a href="https://ru.wikipedia.org/wiki/ROS_(%25D0%25BE%25D0%25BF%25D0%25B5%25D1%2580%25D0%25B0%25D1%2586%25D0%25B8%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B8%25D1%2581%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0)">ROS is</a> used. <br><br>  Here is what happens after the technician chose the car and clicked ‚Äúcall in‚Äù: <br><br><ol><li>  The system is looking for a car: we send a command to the car to flash alarms, after which we can determine which of the cars in the parking lot is chosen by the technician.  At the initial stage of development, we also considered the option of identifying a car by the plate number, but in some areas the number of the parked car may not be visible.  In addition, if we made the definition of the car by the registration number, then the resolution of the photos would have to be greatly increased, which would have a negative impact on performance, and so we use the same image to search and control the car.  This stage occurs once and is repeated only if for some reason we lost the car in tracking. </li><li>  As soon as the car is found, we throw pictures from the cameras on which the car falls into the perception module, which produces a segmentation of the space and gives the coordinates of all the objects, their orientation and size.  This process is ongoing, running at a rate of about 30 frames per second.  Subsequent processes are also ongoing and run until the machine arrives at the end point. </li><li>  The tracking module receives input from perception, sonar and odometry, keeps in memory all the objects found, combines them, specifies the location, predicts the position and speed of objects. </li><li>  Next, the path planner, which is divided into two parts: the global path planner for the global route and the local path planner for the local one (responsible for avoiding obstacles), builds the path and decides where to go to our car, sends the command. </li><li>  Jetson takes the car by car and broadcasts it to the car. </li></ol><br>  Check out the same way as the race. <br><br><h2>  Perception </h2><br>  One of the main and, in my opinion, the most interesting module is perception.  This module describes the data from the sensors in such a way that you can accurately make a decision about the movement.  In our project, he gives the coordinates, orientation and dimensions of all objects falling on the camera.  When designing this module, we decided to start with algorithms that would allow for image analysis in a single pass.  We tried: <br><br><ol><li>  <a href="https://openreview.net/forum%3Fid%3DSy2fzU9gl">Disentangled VAE</a> .  A small modification made to Œ≤-VAE allowed us to train the network so that the latent vectors stored the image information in a schematic top-down view. </li><li>  Conditional GAN ‚Äã‚Äã(the most famous implementation is <a href="https://affinelayer.com/pixsrv/">pix2pix</a> ).  This network can be used to build maps.  We also used it to build a schematic top view, putting data from one or all cameras into it at the same time and waiting for a schematic top view of the output. </li></ol><br>  <i>One of the Conditional GAN ‚Äã‚Äãiterations for a single camera, from left to right: input image, network prediction, expected result</i> <br><br><img src="https://habrastorage.org/webt/w1/m9/ax/w1m9ax7idlos-x3jawcob3kmxzm.png"><br><br>  In fact, the idea of ‚Äã‚Äãthese approaches is to ensure that the final network can understand the location and orientation of all cars and other moving objects that hit the camera by looking at the input photo once.  Data about the objects in this case will be stored in latent vectors.  The network was trained on data from the simulator, which is an exact copy of the point where the demonstration will take place.  And we managed to achieve certain results, but we decided not to use these methods for several reasons: <br><br><ul><li>  For the allotted time, we could not learn how to use data from latent vectors to describe the image.  The result of the network has always been a picture - a top view with a schematic arrangement of objects.  This is less accurate and we feared that such accuracy would not be enough to drive a car. </li><li>  The solution is not scalable: for all subsequent installations and for cases when you need to change the direction of some cameras, you need to reconfigure the simulator and repeat full training. </li></ul><br>  Nevertheless, it was interesting to us to understand the possibilities of these approaches, and we will have them in mind for future tasks. <br><br>  After that, we approached the task on the other hand, through the usual search for objects + network to determine the spatial position of the objects found (for example, <a href="https://arxiv.org/abs/1712.03904">such</a> or <a href="https://arxiv.org/abs/1803.11493">such</a> ).  This option seemed to us the most accurate.  The only drawback is that it is slower than the approaches proposed before this, but it fits into our possible delay frames, since the vehicle‚Äôs speed in the service area is no more than 5 km / hour.  The most interesting work in the field of predicting the 3D position of the object seemed to us <a href="https://arxiv.org/abs/1612.00496">this one</a> , which shows quite good results on <a href="http://www.cvlibs.net/datasets/kitti/eval_object.php%3Fobj_benchmark%3D3d">KITTI</a> .  We built a similar network with some changes and wrote our own algorithm for determining the surrounding box, and to be more precise, the algorithm for estimating the coordinates of the center of the projection of the object on the ground - for making decisions about the direction of motion we do not need data on the height of the objects.  The image of the object and its type (car, pedestrian, ..) is fed to the network input, its size and orientation in space are output.  Next, the module evaluates the center of the projection and gives data for all objects: the center coordinates, orientation and dimensions (width and length). <br><br>  In the final product, each picture is first run through the network to search for objects, then all objects are sent to the 3D network to predict the orientation and size, after which we estimate the center of the projection of each and send it and the data on the orientation and size further.  A feature of this method is that it is very strongly tied to the accuracy of the boundary boundary of the object search network.  For this reason, networks like YOLO did not suit us.  The optimal balance of performance and accuracy of the boundary box seemed to us on the RetinaNet network. <br><br>  It is worth noting one thing with which we were lucky on this project: the land is flat.  Well, that is, not as flat as in a well-known community, but there are no bends in our territory.  This allows the use of fixed monocular cameras to project objects into the coordinates of the earth‚Äôs plane without information about the distance to the object.  Future plans include the introduction of monocular depth prediction.  There is a lot of work on this topic, for example, one of the latest and very interesting, which we are currently trying for future projects.  Depth prediction will not only work on flat ground, it should potentially increase the accuracy of obstacles, simplify the process of configuring new cameras and allow you to abandon the need to label each object - it does not matter to us what kind of object it is if it is some kind of obstacle. <br><br>  That's all, thank you for reading and happy to answer questions.  As a bonus, I want to tell you about an unexpected negative effect: the autopilot does not care about the orientation of the car, for it does not matter how to drive - front or back.  The main thing is to drive optimally and not bump into anyone.  Therefore, there is a high probability that the car will pass part of the way in reverse, especially in small areas where high maneuverability is required.  However, people are used to the fact that the car basically moves in front, and often expect the same behavior from the autopilot.  If a business person sees a car that, instead of driving in front, goes backwards, then he may consider that the product is not ready and contains errors. <br><br>  <b>PS</b> I am sorry that there are no images and videos with real testing, but I can not publish them for legal reasons. </div><p>Source: <a href="https://habr.com/ru/post/457500/">https://habr.com/ru/post/457500/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../457480/index.html">Creating a listening application for viewing mobile MMORPG traffic</a></li>
<li><a href="../457490/index.html">Aysioshechka from Zuckerberg - briefly and in the Libra case</a></li>
<li><a href="../457494/index.html">‚ÄúAnd if I don‚Äôt know math, am I hopeless?‚Äù - experts answer frequently asked questions about the professions in Data Science</a></li>
<li><a href="../457496/index.html">"Find the five differences." Scalable Generation Difference - New Test Portion</a></li>
<li><a href="../4575/index.html">Monster.com told Runet the first "boo"</a></li>
<li><a href="../457504/index.html">Why write your React Data Grid in 2019</a></li>
<li><a href="../457508/index.html">Raising a child vs Machine learning: compares a young mother</a></li>
<li><a href="../457510/index.html">Use mcrouter to scale memcached horizontally.</a></li>
<li><a href="../457512/index.html">Logical replication between PostgreSQL versions</a></li>
<li><a href="../457514/index.html">Nevangary</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>