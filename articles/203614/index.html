<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to data deduplication</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 


 In the field of business continuity there are many different problems associated with the rapid growth of data in modern IT infrastru...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to data deduplication</h1><div class="post__text post__text-html js-mediator-article"><h4>  Introduction </h4><br><p>  In the field of business continuity there are many different problems associated with the rapid growth of data in modern IT infrastructures.  In my opinion, we can distinguish two main ones: </p><br><br><ol><li>  How to plan a place to store large amounts of data </li><li>  How to backup this data </li></ol><br><p>  Indeed, the growth of data on terabytes per year for some large organization is today a very real scenario.  But what about efficient storage and backup?  After all, there are a maximum of 24 hours in a day and the backup window cannot grow indefinitely (unlike the data itself).  Today I want to tell how deduplication can help reduce the severity of this problem. </p><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7b4/f95/240/7b4f95240712d396f2ad26beff3d624a.png" title="Introduction to data deduplication"><br><a name="habracut"></a><br><h4>  Deduplication </h4><br><p>  In a broad sense, there are two main types of deduplication: </p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  <b>File-level deduplication</b> ( <b>file-level</b> deduplication) is the unit of deduplication in this method, as it is easy to understand, is a separate file when duplicate files are excluded from the data storage system.  When they talk about file-level deduplication, they often also mention <i>Single-Instance Storage (SIS)</i> technology. </li><li>  <b>Block-level deduplication</b> (block deduplication) - here the deduplication unit is a data block of arbitrary length, which is often repeated in various logical objects of the storage system. </li></ul><br><p>  Usually, the more granular deduplication scheme is used, the greater is the space savings in the data storage. </p><br><br><p>  <b>What is SIS?</b>  The essence of the method is simple, for example, if there are 2 files that are absolutely identical, then one of them is replaced by a link to the other.  This mechanism works successfully in mail servers (for example, Exchange) and in databases.  For example, if one corporate mail user sends an email with an attachment to several recipients, this file will be saved in the Exchange database only once. </p><br><br><p>  Sounds great!  But only as long as the files are absolutely identical.  If one of the identical files is changed by at least a byte, a separate modified copy of it will be created and the deduplication efficiency will decrease. </p><br><br><p>  <b>Block deduplication</b> works at the level of data blocks recorded on the disk, to assess the identity or uniqueness of which hash functions are used.  The deduplication system stores a hash table for all data blocks stored in it.  As soon as the deduplication system finds matching hashes for different blocks, it intends to save the blocks as a single instance and a set of links to it.  You can also compare data blocks from different computers (global deduplication), which further increases the effectiveness of deduplication, since disks of different computers with the same operating system can store many repetitive data.  It is worth noting that the highest efficiency will be achieved by reducing the block size and maximizing the block repeatability ratio.  In this connection, there are two methods of block deduplication: with a <b>constant</b> (predetermined) and <b>variable</b> (dynamically matched for specific data) length. </p><br><br><h4>  Deduplication scopes </h4><br><p>  Most product developers with deduplication support are focused on the backup market.  In this case, over time, backup copies can take up two to three times more space than the original data itself.  Therefore, file deduplication has been used in backup products for a long time, which, however, may not be sufficient under certain conditions.  Adding block deduplication can significantly improve storage utilization and make it easier to meet system resiliency requirements. </p><br><br><p>  Another way to use deduplication is to use it on the servers of the production system.  This can be done by the OS itself, additional software, or data storage hardware (DSS).  This requires attentiveness, for example, Windows 2008 ‚Äî the OS positioned as capable of producing data deduplication does only SIS.  At the same time, storage systems can perform deduplication at the block level, representing the file system for the user in expanded (original) form, hiding all the details associated with deduplication.  Suppose there are 4 GB of data on the storage system that is deduplicated to 2 GB.  In other words, if the user accesses such a storage, he will see 4 GB of data and that is exactly the amount that will be placed in backup copies. </p><br><br><h4>  Reduced interest and high expectations </h4><br><p>  The percentage of disk space saved is the most important area that is easily manipulated when it says ‚Äú95% reduction in backup file sizes‚Äù.  However, the algorithm used to calculate this ratio may not be completely relevant to your particular situation.  The first variable to take into account are file types.  Formats such as ZIP, CAB, JPG, MP3, AVI - this is already compressed data, which give a smaller deduplication rate than uncompressed data.  Equally important is the frequency of data changes for deduplication and the amount of historical data.  If you are using a product that deduplicates existing data on a file server, then you should not worry.  But if deduplication is used as part of the backup system, then you need to answer the following questions: </p><br><br><ul><li>  How often does the data change? </li><li>  Are these changes significant or only a few blocks in the file? </li><li>  How often is the backup performed and how many files are stored? </li></ul><br><p>  Deduplication is easy to calculate online with the help of special calculators, but you cannot imagine how it will be in your particular situation.  As you can see, the percentage depends on many factors and in theory reaches 95%, but in practice it can reach only a few percent. </p><br><br><h4>  Time is our all </h4><br><p>  Speaking about backup deduplication, it is important for us to know how fast it is executed.  There are three main types of deduplication: </p><br><br><ul><li>  <b>source</b> (on the side of the data source); </li><li>  <b>target</b> (or "post deduplication processing"); </li><li>  <b>continuous</b> (or ‚Äútransit deduplication‚Äù); </li></ul><br><h5>  First type: Deduplication on the data source side </h5><br><p>  It runs on the device itself, where the source data is located.  Any data marked for backup is divided into blocks, hash is calculated for them.  There are 3 potential problems here. <br><br></p><ol><li>  The first problem is that the <b>resources of the source machine</b> are <b>involved</b> .  Therefore, you need to make sure that it has enough processor resources and RAM.  There is no reasonable reason to perform deduplication on an already loaded mail server.  Of course, some manufacturers talk about the ease of their decisions, but this does not negate the fact that the efficiency of the original environment will be affected, and this may be unacceptable. </li><li>  The second problem is where is it better to store the hash tables?  You can have hash tables on the same source server, or on a centralized server on the network (this should be done if global deduplication is used), but <b>this solution creates an additional load on the network</b> . </li><li>  Despite its disadvantages, source deduplication has its right to use, for example, in companies with a small IT infrastructure, where there are several servers in the infrastructure, it is irrational to use global deduplication. </li></ol><br><h5>  Target (or post-process) deduplication </h5><br><p>  Suppose that data from all computers are sent to one backup repository.  Once the data is received, the repository can create a hash table with blocks of this data.  The first advantage of this method is a larger amount of data, and the larger the data pool, the larger the hash table and, accordingly, the greater the chance of finding identical blocks.  The second advantage is that the whole process takes place outside the productive network. </p><br><br><p>  However, this option does not solve all the problems.  There are some points that need to be taken into account. <br></p><ol><li>  The first is the <b>dependence on free space</b> .  If you have an extensive infrastructure, then the size of the required space can be very large. </li><li>  Also, the second drawback of target deduplication is the <b>demands on the disk subsystem of the repository</b> .  Typically, the data must be written to the repository disk before being broken into blocks, and only then the hashing and deduplication process begins.  This makes the disk subsystem a bottleneck of the architecture. </li><li>  The third drawback is that <b>each hash function has a hash collision probability</b> , that is, a situation where the same hash is calculated for two different blocks.  This causes damage to the original data.  To prevent it, it is necessary to choose a hash algorithm with a minimum collision probability, which in turn requires more computational power.  This is usually not a problem, as the target deduplication uses hardware that can handle this load.  It must be said that the probability of hash collisions of modern hash functions is rather small. </li><li>  A fourth potential flaw is that the <b>full amount of data from the ‚Äúproduction‚Äù must be transferred through the network</b> without creating a significant load on the network and the productive system itself.  This can be solved by using night or other less loaded hours for the system, or by isolating this traffic to another network (which is common practice in medium and large companies). </li></ol><br><h5>  Transit Deduplication </h5><br><p>  <b>Transit deduplication</b> is explained as the process that occurs during the transfer of data from source to target.  The term is a bit confusing.  Data is not really deduplicated "in the wire."  In fact, this means that the data collected in the target memory of the device is deduplicated there before the write operation to disk.  This takes the disk search time out of the equation.  Transit deduplication can be considered the best form of target deduplication.  It has all the advantages of a global data view along with the unloading of the hashing process, but none of the disadvantages of slow I / O disks. </p><br><br><p>  However, it still represents a lot of network traffic and potential hash collisions.  This method requires the greatest computing resources (processor and memory) among all listed. </p><br><br><h4>  Summarizing </h4><br><p>  Deduplication technologies can help reduce storage costs.  You should carefully choose the type of deduplication.  Ultimately, deduplication will allow the company to more slowly increase the cost of storing its growing data. </p><br><br><h4>  Useful materials </h4><br>  [1] <a href="http://www.petri.co.il/data-deduplication-introduction.htm" title="Original article">Original article (eng.)</a> <br>  [2] Veeam blog article: <a href="http://www.veeam.com/blog/how-to-get-unbelievable-deduplication-results-with-windows-server-2012-and-veeam-backup-replication.html" title="How To Get Unbelievable Deduplication Results with Windows Server 2012 and Veeam Backup &amp; Replication!">How to Get Unbelievable Deduplication Results with Windows Server 2012 and Veeam Backup &amp; Replication!</a> <br>  [3] Video (English): <a href="http://www.veeam.com/videos/winserver-dedupe-best-practices-joep-2069.html" title="Deduplication best practices with MS Windows Server 2012 and Veeam Backup &amp; Replication 6.5">Deduplication best practices with MS Windows Server 2012 and Veeam Backup &amp; Replication 6.5</a> <br>  [4] <a href="http://www.veeam.com/ru/vm-backup-features.html" title="Built-in compression and deduplication in Veeam Backup &amp; Replication">Built-in compression and deduplication in Veeam Backup &amp; Replication</a> <p></p><p></p></div><p>Source: <a href="https://habr.com/ru/post/203614/">https://habr.com/ru/post/203614/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../203602/index.html">Pochta Rossii invites private carriers and implements GLONASS</a></li>
<li><a href="../203608/index.html">Order Nexus 5 D821 from Japan</a></li>
<li><a href="../20361/index.html">Using MySQL as a file system</a></li>
<li><a href="../203610/index.html">Ideal site - TK as the basis of the site, built on the basis of competent software solutions</a></li>
<li><a href="../203612/index.html">Turn any webcam into a powerful microscope</a></li>
<li><a href="../203618/index.html">Introducing Coarray Fortran: will we be parallel?</a></li>
<li><a href="../20362/index.html">Usability Bulletin. Issue number 15</a></li>
<li><a href="../203622/index.html">Web analytics with Google Tag Manager</a></li>
<li><a href="../203624/index.html">Google Inc. has patented a ‚Äúsmart‚Äù answering machine for social networks / e-mail / sms</a></li>
<li><a href="../203626/index.html">Dual SSD + HDD hard drive in one package</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>