<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>What happens in the brains of the neural network and how to help them</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, on Habr√© has appeared many articles on neural networks. Of these, articles about the Rosenblatt Perceptron seemed very interesting: Rosenbla...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>What happens in the brains of the neural network and how to help them</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/0c2/d50/06c/0c2d5006c907ab64346ebcf410dafdb3.png" align="right">  Recently, on Habr√© has appeared many articles on neural networks.  Of these, articles about the Rosenblatt Perceptron seemed very interesting: <a href="http://habrahabr.ru/post/140301/">Rosenblatt's Perceptron - what is forgotten and invented by history?</a>  and <a href="http://habrahabr.ru/post/140387/">What is the role of the first "random" layer in the Rosenblatt perceptron</a> .  In them, as in many others, a lot has been written about how networks cope with problem solving, and generalize to some extent their knowledge.  But I would like to somehow visualize these generalizations and the decision process.  To see in practice what the perceptron learned there, and to feel how successfully he succeeded.  It is possible to experience bitter irony regarding the achievement of mankind in the field of AI. <br>  The language we will have is C #, just because I recently decided to learn it.  I analyzed two of the most simple examples: the Rosenblatt single-layer perceptron, trained by error correction, and Rumelhart's multilayer perceptron, trained by the method of back propagation of error.  For those who, like me, it became interesting what they actually learned there, and how well they are really able to generalize - welcome under the cat. <br><br>  <b>CAUTION!</b>  <b>A lot of pictures.</b>  <b>Pieces of code.</b> <br><a name="habracut"></a><br>  First I want to invite you to admire the process of learning a neural network.  Every frame after 1000 learning points.  Indicates the speed of learning and the root mean square error for this thousand cycles. <iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/FhmaYLNnCsk%3Ffeature%3Doembed&amp;xid=25657,15700022,15700186,15700191,15700253&amp;usg=ALkJrhgUXToMm5qIDLl31Mczg1CTdsrUpA" frameborder="0" allowfullscreen=""></iframe><br><br>  From the code I will show only what may be useful to others who want to do everything with their own hands or check the correctness of my conclusions.  The code of important elements is torn from the root of the test project in which it was launched, so there may be links to elements that I don‚Äôt cite.  But the code is working, all the pictures are screenshots from my training project. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      First you need to choose a task, such that it looked at it, and it immediately became clear whether the perceptron had learned anything and anything.  Take two coordinates (x and y), and assign many random points to them.  This will be the input data.  Let's draw some graph and ask the perceptron to determine if the point is above or below this graph.  But Rosenblatt‚Äôs perceptron works in integers, and indeed the problem is too simple.  Then let's round each coordinate to an integer and present it in binary form: one digit-one entry.  For consistency, all the examples consider the range of coordinates (0.1), so that before rounding it should be multiplied by the maximum integer value. <br><br>  For example, imagine each coordinate is a two-bit number.  A pair of random numbers (0.2, 0.7) pointing to a point above the graph, then after rounding it will go to (1.3) and give us the following example: <br><br><pre><code class="cs hljs"><span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[]{<span class="hljs-number"><span class="hljs-number">0.2</span></span>, <span class="hljs-number"><span class="hljs-number">0.7</span></span>} =&gt; <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> NeuralTask { Preview = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[]{<span class="hljs-number"><span class="hljs-number">0.25</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">75</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>}, Input=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] {<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>,<span class="hljs-number"><span class="hljs-number">1</span></span>}, Output=<span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[]{<span class="hljs-number"><span class="hljs-number">1</span></span>}}</code> </pre> <br>  The function that converts random numbers into training examples looks like this: <br><br><div class="spoiler">  <b class="spoiler_title">Conversion function</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> Convertion = (<span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[] random, <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>) =&gt; { <span class="hljs-keyword"><span class="hljs-keyword">var</span></span> input = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[]{Math.Floor(random [<span class="hljs-number"><span class="hljs-number">0</span></span>]*<span class="hljs-number"><span class="hljs-number">0x4</span></span>)/<span class="hljs-number"><span class="hljs-number">0x4</span></span>, Math.Floor(random [<span class="hljs-number"><span class="hljs-number">1</span></span>]*<span class="hljs-number"><span class="hljs-number">0x4</span></span>)/<span class="hljs-number"><span class="hljs-number">0x4</span></span>}, <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> x = (<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>)(input[<span class="hljs-number"><span class="hljs-number">0</span></span>] * <span class="hljs-number"><span class="hljs-number">4</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">byte</span></span> y = (<span class="hljs-keyword"><span class="hljs-keyword">byte</span></span>)(input[<span class="hljs-number"><span class="hljs-number">1</span></span>] * <span class="hljs-number"><span class="hljs-number">4</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> res = (y &gt; <span class="hljs-keyword"><span class="hljs-keyword">value</span></span> * <span class="hljs-number"><span class="hljs-number">4</span></span> ? <span class="hljs-number"><span class="hljs-number">1</span></span> : <span class="hljs-number"><span class="hljs-number">0</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> NeuralTask() { input = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[<span class="hljs-number"><span class="hljs-number">4</span></span>]{ (x&amp;<span class="hljs-number"><span class="hljs-number">2</span></span>)&gt;&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>, x&amp;<span class="hljs-number"><span class="hljs-number">1</span></span>, (y&amp;<span class="hljs-number"><span class="hljs-number">2</span></span>)&gt;&gt;<span class="hljs-number"><span class="hljs-number">1</span></span>, y&amp;<span class="hljs-number"><span class="hljs-number">1</span></span>}, output = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[<span class="hljs-number"><span class="hljs-number">1</span></span>] { res }, preview = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span>[<span class="hljs-number"><span class="hljs-number">3</span></span>] { input[<span class="hljs-number"><span class="hljs-number">0</span></span>], input[<span class="hljs-number"><span class="hljs-number">1</span></span>], res } }; };</code> </pre></div></div><br><br>  Here it is necessary to clarify that all this was done for educational and research purposes, therefore it was not optimized for speed and unearthly beauty, but somewhere it was written so that it was convenient to program any conceivable perceptron.  Therefore, in particular, the input and output data are in double.  It turns out such a simple picture.  Well, or a little more complicated, if each axis is chopped into 256 sections, and the function is more complicated: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f78/eb8/4c3/f78eb84c3033bf02a57bc0ef9b024e39.png"><img src="https://habrastorage.org/getpro/habr/post_images/ece/57b/678/ece57b6784b2641a06c6d50eb41ab137.png"><br><br>  Hereinafter, the green color - values ‚Äã‚Äãare greater than zero, the more saturated, the greater the number, red - values ‚Äã‚Äãless than zero, blue - values ‚Äã‚Äãequal to 0 and their closest surroundings. <br><br><div class="spoiler">  <b class="spoiler_title">Source</b> <div class="spoiler_text">  Our perceptron itself consists of a synapse: <br><div class="spoiler">  <b class="spoiler_title">class Synaps</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Synaps</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> weight; <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">   </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> virtual public double Weight { get { return weight; } set { if (weight != value) { weight = value; if (axon != null) ChangeActionPotentialHandler(axon.ActionPotential); } } } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">    . ,  ,   ,     ,      .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> private IAxon axon; //   ,    . public IAxon Axon { get { return axon; } set { //    ,       . if (axon != null) axon.RemoveChangeActionPotentialHandler(ChangeActionPotentialHandler); axon = value; if (axon != null) { axon.AddChangeActionPotentialHandler(ChangeActionPotentialHandler); ChangeActionPotentialHandler(axon.ActionPotential); } } } public double ActionPotential; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  ,     .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> protected Action WhenActionPotentialhanged; public void AddActionPotentialChangeHandler(Action handler) { WhenActionPotentialhanged += handler; } public void RemoveActionPotentialChangeHandler(Action handler) { WhenActionPotentialhanged -= handler; } virtual protected void ChangeActionPotentialHandler(double axonActionPotential) { ActionPotential = axonActionPotential * weight; //      ,       . if (WhenActionPotentialhanged != null) WhenActionPotentialhanged); //          } }</span></span></code> </pre></div></div><br><br>  Nothing unexpected, except that the numbers are stored in double and instead of the standard event used custom.  Now the neuron.  Neurons here all together correspond to the IAxon interface: <br><br><div class="spoiler">  <b class="spoiler_title">interface IAxon</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">interface</span></span> <span class="hljs-title"><span class="hljs-title">IAxon</span></span> { <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> double ActionPotential { get; } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">    .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> void AddChangeActionPotentialHandler(Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;double&gt;</span></span></span><span class="hljs-comment"> handler); void RemoveChangeActionPotentialHandler(Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;double&gt;</span></span></span><span class="hljs-comment"> handler); </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">,        .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> PointF Position { get; set; } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">      .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> string Name { get; set; } }</span></span></code> </pre></div></div><br><br>  Nothing too unexpected, except for Position, showing where the neuron is to draw. <br><br>  The input uses sensory neurons, which can be set directly: <br><br><div class="spoiler">  <b class="spoiler_title">class SensoryNeuron</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">SensoryNeuron</span></span> : <span class="hljs-title"><span class="hljs-title">IAxon</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">protected</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> actionPotential; <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">double</span></span> ActionPotential { <span class="hljs-keyword"><span class="hljs-keyword">get</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> actionPotential; } <span class="hljs-keyword"><span class="hljs-keyword">set</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (actionPotential != <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>) { actionPotential = <span class="hljs-keyword"><span class="hljs-keyword">value</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (WhenChangeActionPotential != <span class="hljs-literal"><span class="hljs-literal">null</span></span>) WhenChangeActionPotential(actionPotential); } } } }</code> </pre></div></div><br><br>  Finally, the neuron itself is in a rather generalized form: <br><br><div class="spoiler">  <b class="spoiler_title">class Neuron</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment"> .     </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public class Neuron : IAxon { </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  ,      .    ,     ,     .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public Synaps[] Synapses = new Synaps[0]; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  , ,            .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> protected bool synapsPotentialChanged = false; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">      .  ,  ,   . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public void AppendSinaps(Synaps target) { //     .  , , ,      ,     . Synapses = Synapses.Concat(new Synaps[1] { target }).ToArray(); target.AddActionPotentialChangeHandler(ChangeSynapsPotentialHandler); //   . synapsPotentialChanged = true; } virtual protected void ChangeSynapsPotentialHandler() { synapsPotentialChanged = true; } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">   .     ,    .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> protected DTransferFunction transferFunctionDelegate; public virtual DTransferFunction TransferFunction { get { return transferFunctionDelegate; } set { transferFunctionDelegate = value; } } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">             . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public virtual void Excitation() { if (!synapsPotentialChanged) return; //        synapsPotentialChanged = false; synapsPotentials = 0; for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; Synapses.Length; i++) synapsPotentials += Synapses[i].ActionPotential; double newValue = transferFunctionDelegate(synapsPotentials); if (actionPotential != newValue) { //        . ,   . actionPotential = newValue; if (WhenChangeActionPotential != null) WhenChangeActionPotential(actionPotential); } } }</span></span></span></span></code> </pre></div></div><br>  Here: <br><br><div class="spoiler">  <b class="spoiler_title">Activation function</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">        . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="argument"&gt;</span></span></span><span class="hljs-comment">  </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;returns&gt;</span></span></span><span class="hljs-comment">    </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/returns&gt;</span></span></span><span class="hljs-comment"> public delegate double DTransferFunction(double argument); DTransferFunction BarrierTransferFunction = (double x) =&gt; x </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;= 0 ? 0 : 1;</span></span></span></span></code> </pre></div></div><br>  Finally, from all of this, the neural network and its learning algorithm are obtained: <br><br><div class="spoiler">  <b class="spoiler_title">class NeuralNetwork, PerceptronClassic, ErrorCorrection</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">abstract</span></span> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">NeuralNetwork</span></span> { <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public SensoryNeuron[] Input = new SensoryNeuron[0]; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment">            ,    .      . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment">           ,          . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public Neuron[] ExcitationOrder = new Neuron[0]; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">  </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public Neuron[] Output = new Neuron[0]; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment"> ,       </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="input"&gt;</span></span></span><span class="hljs-comment">   - </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="output"&gt;</span></span></span><span class="hljs-comment">   </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> abstract public void create(uint input, uint output); public void execute(double[] data) { //               for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; Input.Length &amp;&amp; i &lt; data.Length; i++) { Input[i].ActionPotential = data[i]; } for (int i = 0; i &lt; ExcitationOrder.Length; i++) ExcitationOrder[i].Excitation(); } public double[] Result() { // return output.Select(s =&gt;</span></span></span><span class="hljs-comment"> s.ActionPotential).ToArray(); //TODO     Linq  . double[] res = new double[Output.Length]; for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; res.Length; i++) res[i] = Output[i].ActionPotential; return res; } } public class PerceptronClassic : NeuralNetwork { //     ,    public int neuronCountsOverSensoric = 15; //     public int ANeuronSynapsCount; //      public Neuron[] Layer; //        override public void create(uint inputCount, uint outputCount) { rnd = rndSeed &gt;</span></span></span><span class="hljs-comment">= 0 ? new Random(rndSeed) : new Random(); //   this.Input = new SensoryNeuron[inputCount]; for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; inputCount; i++) Input[i] = new SensoryNeuron() {Name = "S" + i}; //    -  Layer = new Neuron[inputCount + neuronCountsOverSensoric]; for (int i = 0; i &lt; Layer.Length; i++) { //    Layer[i] = new RosenblattNeuron(); //         SensoryNeuron[] sub = Input.OrderBy((cell) =&gt;</span></span></span><span class="hljs-comment"> rnd.NextDouble()).Take(ANeuronSynapsCount).ToArray(); //   ,    for (int j = 0; j </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; sub.Length; j++) { Synaps s = new Synaps(); s.Axon = sub[j]; s.Weight = rnd.Next(2) * 2 - 1; Layer[i].AppendSinaps(s); } } //     . for (int i = 0; i &lt; Layer.Length; i++) Layer[i].Name = "A" + i; //   -   Output = new Neuron[outputCount]; for (int i = 0; i &lt; Output.Length; i++) { Output[i] = new RosenblattNeuron(); Output[i].Name = "R" + i; //        for (int j = 0; j &lt; Layer.Length; j++) { Synaps s = new Synaps(); s.Axon = Layer[j]; Output[i].AppendSinaps(s); //   0 } } //       int lastIndex = 0; ExcitationOrder = new Neuron[Layer.Length + Output.Length]; foreach (Neuron cell in Layer) ExcitationOrder[lastIndex++] = cell; foreach (Neuron cell in Output) ExcitationOrder[lastIndex++] = cell; } } /// &lt;summary&gt;</span></span></span><span class="hljs-comment">   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public class ErrorCorrection : LearningAlgorythm { // ,        override protected double LearnNet(double[] required) { double error = 0; for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; required.Length &amp;&amp; i &lt; net.Output.Length; i++) { if (required[i] != net.Output[i].ActionPotential) { error += 1; //     for (int j = 0; j &lt; (net as PerceptronClassic).Layer.Length; j++) //    ,   if ((net as PerceptronClassic).Layer[j].ActionPotential &gt;</span></span></span><span class="hljs-comment"> 0) //      foreach (RosenblattNeuron cell in net.Output) //    ,       //    ‚Äì ,   ,   ‚Äì .    . cell.Synapses[j].Weight += required[i] </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;= 0 ? -1 : 1; } } return error; } /// &lt;summary&gt;</span></span></span><span class="hljs-comment">       </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public void LearnTasksSet() { if (data == null) { Console.WriteLine("  "); return; } data.Reset(); LearnedTaskSetsCount++; ErrorsInSet = LearnedTasksInSetCount = 0; int max = 1000; while (data.MoveNext() &amp;&amp; --max &gt;= 0) LearnCurrentTask(); //     ,    . } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">         .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="loops"&gt;</span></span></span><span class="hljs-comment">    </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> public void LearnSetManyTimesUntilSuccess(int loops) { for (int i = 0; i </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt; loops; i++) { LearnTasksSet(); if (ErrorsInSet == 0) { break; } } } }</span></span></span></span></code> </pre></div></div><br>  The data source is <code>Enumerable&lt;NeuralTask&gt;</code> and with each <code>Enumerable&lt;NeuralTask&gt;</code> it rearranges points in a sequence in random order. <br><br></div></div><br>  Everything is ready, you can run. <br><br>  For the simplest problem, two bits per axis, so that we were lucky to find a suitable separable decomposition, we had to add 8 more neurons to layer A than 12 inputs.  A suitable decomposition was found on the third attempt.  Perceptron correctly classified all 16 possible values. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a56/1f0/beb/a561f0beb99f92b38efee0a0382624d1.png"><br><br>  Here you can see that there are only 16 possible values, and I generated more.  So that random numbers are guaranteed to cover all possible options. <br><br>  Since everything is so wonderful, let's move on to a slightly more complicated task, where we have 256 options for each axis.  Function consider the simplest.  Let's generate, for a start, 64 points.  In the layer of neurons only 20 more than the inputs - 36 pieces.  And immediately success. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/92a/fe0/a2c/92afe0a2c2bfe72a06c18c5d2a724d30.png"><br><br>  And now we will do the most interesting.  We take the network we received and draw in the picture all the possible values ‚Äã‚Äãthat the network gives at all points.  And here it turns out the saddest thing.  The level of generalization achieved by the network is not very impressive. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/712/7f2/f70/7127f2f705bb2bb1aa265ba8a2a0fdda.png"><br><br>  It turns out that the network in a very general way represents what pattern lies behind the points given to it.  Let's try to provide the network with more complete data about the function being studied.  Generate 256 points.  36 neurons, as last time, are not enough for the network to encounter a suitable linearly separable decomposition.  Now we needed to create 70 associative neurons, drive out the training set of tasks 615 times and vacuum the cooler so that the processor does not overheat for joy in just one second of training.  The generalization achieved by the network has become better, but it can be seen with the naked eye that the improvement obtained is disproportionately expended. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/180/b21/c31/180b21c31a43305cee14c3886388a4d6.png"><br><br>  In the hearts, we cover the space with 2048 points.  By force we create already 266 neurons in the associative layer, select the optimal number of synapses per neuron (it turns out 8) learn the set 411 times (until the network stops to make mistakes) and look at the achieved result. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c18/a46/8c3/c18a468c3194a06bce1ae2f5b339b0df.png"><br><br>  I don‚Äôt know about you, but I don‚Äôt see any quality improvement.  The network regularly memorizes all values ‚Äã‚Äãby heart, in no way approaching the generalization of the pattern proposed to it.  And in general it is clear that the nature of the profound conclusions made by the network is not so much dependent on the nature of the task before it. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f15/eb2/012/f15eb201219777217526254c5e9a1558.png"><br><br>  Thanks, of course, that I managed to at least learn it.  It seems that the theorems are correct, and if I have a sufficient number of neurons, I can force the network to memorize at least all 65536 variants possible for this task, but we will need approximately 1500-2000 neurons and water cooling for this.  In order to remember all the information contained in such a network, we need 5 bits for each synapse (4 bits for the axon number and weight bits), 16 bits for each neuron for the axon weight and 40 bits for all synapses.  And one teaching example weighs 17 bits.  In our example with 2048 points, it turns out that the training information weighs only two times more than the information about the received network. <br><br><h4>  Convenient tasks </h4><br><img src="https://habrastorage.org/getpro/habr/post_images/237/2df/c9b/2372dfc9b20af113cb5968f245f13988.png" align="right">  So what's the problem?  What is the reason for such a sad picture?  Let's try to solve the problem analytically.  Suppose we have a perceptron, but only the weights of the first layer, as well as the second, are amenable to learning.  In the second layer, we have only three neurons.  The first is connected to the first 8 inputs and has not a barrier, but simply a linear activation function.  The second neuron is the same, but is only responsible for converting the second 8 bits to normal coordinates.  The third is connected with all, has a barrier function and is intended to give 1 if at least one input has at least something.  In the next layer, two of the neurons are summed, again without a barrier function, but with very important weighting factors reflecting the parameters of the function.  And finally, the last neuron will compare two input signals.  Simple, logical and not a bit interesting.  However, this is almost the minimum possible number of neurons and synapses involved for a given task.  And now try to imagine how many neurons you need to express any of these operations in a single-layer perceptron, whose synapse weights in the first layer can only be -1 and 1. For example, casting 8 bits to one number.  I'll tell you - you need about 512 pieces of neurons, and we haven't started comparing this yet. <br><br>  That is, the problem is not that Rosenblatt's perceptron cannot learn this data set.  The problem is that doing it is very, very uncomfortable.  Anyone who is friends with the theorem of large numbers can try to estimate what the probability is to find a linearly separable decomposition suitable for this.  Rosenblatt's Perceptron is convenient to solve problems that are well represented in the form of a spotted blue-green diffuse gradient, but everything becomes sad when it is not. <br><br><h3>  And what about Rumelhart's multilayer perceptron? </h3><br>  But what if only Rosenblatt‚Äôs single-layer perceptron has problems, and a multi-syllable perceptron that is trained using the error propagation method will be very good, and even magical?  Let's try. <br><br><div class="spoiler">  <b class="spoiler_title">Source</b> <div class="spoiler_text">  First, now we will add a function to calculate the first derivative of the neuron activation function at current values.  All of this can be scary to optimize and syncline the calculation of the derivative directly into the formula in which it will be applied, but our task is not to speed things up, but to figure out how it works.  Therefore, a separate function: <br><br><div class="spoiler">  <b class="spoiler_title">Derivative of the activation function</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">      . </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="argument"&gt;</span></span></span><span class="hljs-comment">  </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="funcResult"&gt;</span></span></span><span class="hljs-comment">  ,    </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;returns&gt;</span></span></span><span class="hljs-comment">       .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/returns&gt;</span></span></span><span class="hljs-comment"> public delegate double DTransferFunctionDerivative(double argument, double funcResult);</span></span></code> </pre></div></div><br><br>  As a sigmoid, we use, for example, Hypertangent: <br><br><pre> <code class="cs hljs">DTransferFunction Function = (x) =&gt; Math.Tanh(x), DTransferFunctionDerivative Derivative = (x, th) =&gt; (<span class="hljs-number"><span class="hljs-number">1</span></span> - th) * (<span class="hljs-number"><span class="hljs-number">1</span></span> + th)</code> </pre><br><br>  Now the neuron itself.  It differs only in that for each calculation the value also calculates the derivative of it from the sum of the input signals, and the presence of a variable in which the value used for back propagation of the error will be stored. <br><div class="spoiler">  <b class="spoiler_title">class NeuronWithDerivative</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">NeuronWithDerivative</span></span> : <span class="hljs-title"><span class="hljs-title">Neuron</span></span> { <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">    .           .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> protected DTransferFunctionDerivative transferFunctionDerivativeDelegate; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> override public DTransferFunctionDerivative TransferFunction { get { return transferFunctionDerivativeDelegate; } set { transferFunctionDerivativeDelegate = value; } } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">        </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> protected double actionPotentialDerivative = 0; </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">        .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public double ActionPotentialDerivative { get { return actionPotentialDerivative; } } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">     . ,    .   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> public double BackProprigationParametr = 0; public override void Excitation() { base.Excitation(); actionPotentialDerivative = transferFunctionDerivativeDelegate(synapsPotentials, actionPotential); } }</span></span></code> </pre></div></div><br><br>  I, lately, like to use LINQ, because it's easier and faster to write experimental code.  For the convenience of this business, my little home function expands its capabilities.  I use it instead of List.ForEach so that the call is beautiful and single-line. <br><br><div class="spoiler">  <b class="spoiler_title">static class Tools</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">static</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">Tools</span></span> { <span class="hljs-comment"><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">         .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="source"&gt;</span></span></span><span class="hljs-comment">   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="func"&gt;</span></span></span><span class="hljs-comment">,   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> public static void Each</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment">(this IEnumerator</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> source, Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> func) { while (source.MoveNext()) func(source.Current); } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">             .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="source"&gt;</span></span></span><span class="hljs-comment">   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="func"&gt;</span></span></span><span class="hljs-comment">,   .     -   .      .   -  .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> public static void Each</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment">(this IEnumerator</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> source, Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType, int&gt;</span></span></span><span class="hljs-comment"> func) { for (int i = 0; source.MoveNext(); i++) func(source.Current, i); } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">         .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="source"&gt;</span></span></span><span class="hljs-comment">   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="func"&gt;</span></span></span><span class="hljs-comment">,   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> public static void Each</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment">(this IEnumerable</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> source, Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> func) { source.GetEnumerator().Each(func); } </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;summary&gt;</span></span></span><span class="hljs-comment">             .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/summary&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="source"&gt;</span></span></span><span class="hljs-comment">   .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">///</span></span></span><span class="hljs-comment"> </span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;param name="func"&gt;</span></span></span><span class="hljs-comment">,   .     -   .      .   -  .</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;/param&gt;</span></span></span><span class="hljs-comment"> public static void Each</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment">(this IEnumerable</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType&gt;</span></span></span><span class="hljs-comment"> source, Action</span><span class="hljs-doctag"><span class="hljs-comment"><span class="hljs-doctag">&lt;SequenceType, int&gt;</span></span></span><span class="hljs-comment"> func) { source.GetEnumerator().Each(func); } }</span></span></code> </pre></div></div><br><br>  . <br><br><div class="spoiler"> <b class="spoiler_title">class RumelhartPerceptron</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-comment"><span class="hljs-comment">//          . public class RumelhartPerceptron : NeuralNetwork { /// &lt;summary&gt;   .  .&lt;/summary&gt; DTransferFunctionDerivative TransferFunctionDerivative; /// &lt;summary&gt;    ,      &lt;/summary&gt; public int[] NeuronsCount = new int[0]; override public void create(uint input, uint output) { //  . Input = (new SensoryNeuron[input]).Select((empty, index) =&gt; new SensoryNeuron(){Name = "S[" + index + "]"}).ToArray(); //     Func&lt;string, NeuronWithDerivative&gt; create = (name) =&gt; { NeuronWithDerivative neuron = new NeuronWithDerivative(); neuron.Name = name; neuron.TransferFunction = TransferFunction; neuron.TransferFunctionDerivative = TransferFunctionDerivative; return neuron; }; Func&lt;IAxon, Synaps&gt; createSynaps = (axon) =&gt; { Synaps s = new Synaps(); s.Axon = axon; return s; }; /// &lt;summary&gt;  &lt;/summary&gt; //       NeuronWithDerivative[][] Layers = NeuronsCount.Select((count, layer) =&gt; Enumerable.Range(0, count).Select(index =&gt; create("A[" + layer + "][" + index + "]")).ToArray()).ToArray(); //      //   Output = Enumerable.Range(0, (int)output).Select(index =&gt; create("R[" + index + "]")).ToArray(); //         . Layers[0].Each(neuron =&gt; { Input.Select(createSynaps).Each(synaps =&gt; { neuron.AppendSinaps(synaps); }); }); //  ,    ,      . Layers.Skip(1).Each((layer, layerIndex) =&gt; { layer.Each(neuron =&gt; { Layers[layerIndex].Select(createSynaps).Each(synaps =&gt; { neuron.AppendSinaps(synaps); }); }); }); //        Output.Each(neuron =&gt; { Layers.Last().Select(createSynaps).Each(synaps =&gt; { neuron.AppendSinaps(synaps); }); }); //      ExcitationOrder = Layers.SelectMany(layer =&gt; layer).Concat(Output).ToArray(); //       -1  +1 Random rnd = new Random(); ExcitationOrder.Each(neuron =&gt; neuron.Synapses.Each(synaps =&gt; synaps.Weight = rnd.NextDouble() * 2 - 1)); } }</span></span></code> </pre></div></div><br><br> , ,  .    LINQ      ,   . ,  ,   . <br><br><div class="spoiler"> <b class="spoiler_title">class BackPropagationLearning</b> <div class="spoiler_text"><pre> <code class="cs hljs"> <span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-keyword"><span class="hljs-keyword">class</span></span> <span class="hljs-title"><span class="hljs-title">BackPropagationLearning</span></span> : <span class="hljs-title"><span class="hljs-title">LearningAlgorythm</span></span> { <span class="hljs-comment"><span class="hljs-comment">// ,     public double LearningSpeed = 0.01; override protected double LearnNet(double[] required) { double[] errors = net.Output.Select((neuron, index) =&gt; neuron.ActionPotential - required[index]).ToArray(); //     .     ,                . net.ExcitationOrder.Cast&lt;NeuronWithDerivative&gt;().Each(neuron =&gt; { neuron.BackProprigationParametr = 0; }); //   BackProprigationParametr    dE/dS[i] = dE/dO[i] * F'[i](S[i]) //          . BP[i] = dE/dO[i] * F'[i] = 2*(O[i]-T[i])*F'[i]; net.Output.Cast&lt;NeuronWithDerivative&gt;().Each((neuron, index) =&gt; { neuron.BackProprigationParametr = 2 * errors[index] * neuron.ActionPotentialDerivative; }); //      ,       BP[j] = SUM( dE/dO[i] * F'[i] * W[j,i] ) * F'[j] = SUM ( BP[i] * W[j,i] * F'[j]) net.ExcitationOrder.Reverse().Cast&lt;NeuronWithDerivative&gt;().Each(neuron =&gt; { neuron.Synapses.SkipWhile(synaps =&gt; !(synaps.Axon is NeuronWithDerivative)).Each(synaps =&gt; { (synaps.Axon as NeuronWithDerivative).BackProprigationParametr += neuron.BackProprigationParametr * (synaps.Axon as NeuronWithDerivative).ActionPotentialDerivative * synaps.Weight; }); }); //     ,    delta W[i,j] = -speed * dE/dS[j] * X[i]; net.ExcitationOrder.Reverse().Cast&lt;NeuronWithDerivative&gt;().Each(neuron =&gt; { neuron.Synapses.Each(synaps =&gt; { synaps.Weight += -LearningSpeed * neuron.BackProprigationParametr * synaps.Axon.ActionPotential; }); }); //     . (        ,  ). return errors.Select(e =&gt; e * e).Average(); } public void LearnSomeTime(int sek) { DateTime begin = DateTime.Now; while (TimeSpan.FromTicks(DateTime.Now.Ticks - begin.Ticks).Seconds &lt; sek) { LearnTasksSet(); } } }</span></span></code> </pre></div></div></div></div><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Here is the code. It differs from the canonical implementation only in that the learning rate is not sewn into the back propagated error, but multiplied by the change in weight immediately before use. We minimize the sum of the squares of errors, as Wikipedia teaches us, which in this part was filled with habrovchanami.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">What is the task of letting our network begin? Let's take the same x and y from the [0,1] section, at the points that are higher than the graph, we will expect the network output to be +1, at the points below the -1 graph. In addition, we will make more than one fixed sequence of training examples, and we will create a new training example each time anew, so that it cannot be said that the network did not have information about some important area of ‚Äã‚Äãspace. We give the created packs of variables of 1000 pieces per set. In the preview, not one point is shown, but several last ones created only for beauty. It turns out somehow. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/95e/f0f/eae/95ef0feaedafbe5a39933dbc7cbbe82f.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Create a neural network. Wikipedia tells us that three layers should be enough. In order not to seem small, let's fix 30 neurons each in a layer. Let's try to teach with some similarity of </font></font><a href="http://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%25B8%25D0%25BC%25D0%25B8%25D1%2582%25D0%25B0%25D1%2586%25D0%25B8%25D0%25B8_%25D0%25BE%25D1%2582%25D0%25B6%25D0%25B8%25D0%25B3%25D0%25B0"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">the annealing simulation algorithm</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">, gradually manually reducing the speed of learning, as the root-mean-square error in the last 1000 training examples ceases to improve. Programming annealing was too lazy, because my article is not about that. We train, we train, and finally, when the quality of the network ceases to improve, we draw in the picture the values ‚Äã‚Äãthat the network will produce for each of our points in the 1x1 square. </font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/7d0/361/af9/7d0361af94b7b47b90adafe2e665a5dd.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Well, as for me, this is a rather modest result, and this is also the best of 5 attempts, he at least has two concavities, this is not always possible for networks of this size. Pay attention to the picture with the results of 4 more attempts in a row.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/2d0/119/637/2d0119637d69ecc330710ad9e588031c.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All attempts give, in principle, a similar result, and at all the picture is filled up to the left, despite the fact that the desired graph in all these cases is located symmetrically. </font><font style="vertical-align: inherit;">The dullness of the network can be attributed to many different reasons, including errors in the programmer‚Äôs DNA, but the fact that it‚Äôs convenient to roll up the networks to the left should have some kind of rational explanation. </font><font style="vertical-align: inherit;">So maybe we made a mistake somewhere? </font><font style="vertical-align: inherit;">Let's offer the network to cope with quite a banal task - we give a simple linear relationship as a problem.</font></font> We look. <br><img src="https://habrastorage.org/getpro/habr/post_images/fca/1ec/a95/fca1eca95e2d8ca58e533f035ff935e2.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">All is well, all is right. </font><font style="vertical-align: inherit;">By manually tracking the state of the variables, you can verify that the algorithm is working correctly. </font><font style="vertical-align: inherit;">Then we give the puzzle a little more difficult. </font><font style="vertical-align: inherit;">The result can be seen in the following picture.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/407/943/6e9/4079436e9e7b277049e3f1e944f3e348.png"><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Full fiasco </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">We look at it and understand that something is wrong here. The task is not that simple, it is primitive. But the network is decisively unable to find its solution in any approximation. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Have you guessed why? It clearly follows from the picture why this is impossible, and at the same time the answer to the question why all the decisions from the previous picture were left to the left.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">If we try to design a solution to this problem analytically - manually, with a pen and paper, we will very quickly encounter the correct answer. If you have a neuron with a symmetric sigmoid, you will not force it to do the transformation output = k * input + b with any tricks. A neural network with a symmetric with respect to zero sigmoid at the point (0,0) cannot output anything except 0 (hello, by the way, Rosenblatt's perceptron convergence theorem, there is also such a special point there). </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">To solve this problem, we can add another input to the neural network, and give it a constant value of 1, independent of the input data. And then the network, as if by magic, is getting smarter and is learning the task before it in the shortest possible time and with incredible accuracy.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/9aa/7fb/4c4/9aa7fb4c4ca6f382d21b881d7e3ba7a9.png"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And here the most interesting begins. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">But can there be a good approximation without an additional reference input? </font><font style="vertical-align: inherit;">Will we be able to come up with a solution for the network topology from the previous picture? </font><font style="vertical-align: inherit;">It turns out that this is possible.</font></font><br><br><h4><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Brain against back distribution </font></font></h4><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">In the course of the network, I suggested several times to find a solution for the network manually. Now we will analyze one of these solutions in detail. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Our network inputs are called S [0] and S [1], the neurons of the first layer are A [0] [1], A [0] [2] and A [0] [3], respectively, the next layer is A [1] [ 0] and A [1] [1] and, finally, the output of R [0]. What did we lack the last time we tried to solve the problem analytically? We lacked a reference constant. Take one neuron, for example, A [0] [0] and hang very high weights at synapses, for example, 1000 each. In addition to a small area in the immediate vicinity of 0, the action potential on this neuron will be equal to 1.</font></font><img src="https://habrastorage.org/getpro/habr/post_images/7f6/2e2/d64/7f62e2d64fdeb2946d305b90fc375cfc.png" align="right">  What's next?<font style="vertical-align: inherit;"><font style="vertical-align: inherit;">A [0] [1], we will transmit information about the first coordinate and have weights of synapses respectively 1 and 0, neuron A [0] [2] - information about the second coordinate, and have synapses with weights 0 and 1. We want so that the function of the first coordinate is compared with the second coordinate. To do this, we simply transmit the second coordinate to the second neuron of the second layer. Assign synapse weights, respectively, 0.0 and 1. And in the first synapse of the second layer, we want to get the value of k * xb. Accordingly, k = -0.5 b would be equal to 0.75 if the activation functions of the neurons did not bend the values. About the input x = 1 on the neuron A [0] [1] there will be only 0.76 potential. So for comparison, we need about b = 0.65. With this value, the neuron A [1] [0] should have approximately the same value as the neuron A [1] [1] for points lying on our original line. Well now,in order to compare these two values, let's endow the output neuron R [0] with values ‚Äã‚Äãat -1 and 1. Synapses. Let's display what we have in the picture. Right beauty! Blue zero values ‚Äã‚Äãare approximately where they are needed. Top green. Red below. Of course, for the time being it is not green enough and not red enough. However, the final fine-tuning of the weights of synapses will be able to do the error back-propagation algorithm not only worse, but better than me. We start the algorithm, and after a small number of steps, we have a fairly tolerable approximation.However, the final fine-tuning of the weights of synapses will be able to do the error back-propagation algorithm not only worse, but better than me. We start the algorithm, and after a small number of steps, we have a fairly tolerable approximation.However, the final fine-tuning of the weights of synapses will be able to do the error back-propagation algorithm not only worse, but better than me. We start the algorithm, and after a small number of steps, we have a fairly tolerable approximation.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d2/cbf/13e/5d2cbf13e06f21c08bc6e866b0aad05a.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Here you can see how the network looks like: </font></font><br><br><div class="spoiler"> <b class="spoiler_title"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">XML to which the finished network is exported</font></font></b> <div class="spoiler_text"><pre> <code class="xml hljs"><span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">rumelhart</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">input</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">SensoryNeuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0396039603960396"</span></span></span><span class="hljs-tag">/&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">SensoryNeuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,232673267326733"</span></span></span><span class="hljs-tag">/&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">input</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">excitationOrder</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"999,800400355468"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"39,5960554596225"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"999,545226476388"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"232,5674536851"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,116342019068401"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,13712492177543"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0450346503673436"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,308744483692756"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0718365877898986"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][2]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,29693700450834"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,0240967983057654"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,000954328645772886"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,31992553337836"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"S[1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,307111386479124"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[1][0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,683083451961352"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,02404884109051"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,02404884109051"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,649771926175146"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,0755957778251805"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,382508459201211"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][2]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,113580916074308"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[1][1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0324886810522597"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,404744328902586"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,404744328902586"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,161865952018599"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0188318116762727"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,40909563283595"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[0][2]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,418412636280091"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">excitationOrder</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">output</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">name</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"R[0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,707598983150799"</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-1,36308077548559"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[1][0]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"-0,931097921420856"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">synaps</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">weight</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"1,50019153981243"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">axon</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"A[1][1]"</span></span></span><span class="hljs-tag"> </span><span class="hljs-attr"><span class="hljs-tag"><span class="hljs-attr">potential</span></span></span><span class="hljs-tag">=</span><span class="hljs-string"><span class="hljs-tag"><span class="hljs-string">"0,0487392444542643"</span></span></span><span class="hljs-tag"> /&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">Neuron</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">output</span></span></span><span class="hljs-tag">&gt;</span></span> <span class="hljs-tag"><span class="hljs-tag">&lt;/</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name">rumelhart</span></span></span><span class="hljs-tag">&gt;</span></span></code> </pre></div></div><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">There is one interesting point in all this manual solution. </font><font style="vertical-align: inherit;">The fact is that the perceptron, driven by the error back-propagation algorithm, in principle could not find this solution in our situation. </font><font style="vertical-align: inherit;">Because between the initial state, when all synapses have an initial value in the range [-1,1] and a final one, at which two synapses weigh a lot, there is a very wide gap filled with very bad solutions, and the gradient descent algorithm will carefully push the network out this abyss. </font><font style="vertical-align: inherit;">As I call this </font></font><b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">solutions have high incoherence</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. The annealing simulation algorithm may accidentally throw the network into that area, but for this, the annealing temperature must be high (for the chance to throw the network so far) and very quickly decrease so that it does not immediately drop out. And since the weights of a part of synapses should be large, while others, on the contrary, have very little difference, we should accidentally hit not only large values, but also quite by chance, it‚Äôs good to get into small values, and the algorithm should freeze very sharply when that - I found this because the area of ‚Äã‚Äãsuitable solutions is very narrow. Even at a learning rate of 0.0001, a simple stochastic gradient descent can easily throw a network out of it. In general, there is a solution, but I just cannot find its network.</font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Although, as you probably noticed, if you give a network three layers of 30 neurons, the network can independently find a way to partially solve the zero point problem. Although doing it is very uncomfortable. If we give the network an additional reference input, the picture is no longer skewed to one side. But even more, the very process of finding a solution becomes much more productive - free from the need to spend half of its priceless neurons to create a constant, the network was able to fully turn around and act as if it were on one layer and a few dozen more neurons. Details can be admired in the picture.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/8a2/3f3/32a/8a23f332af99a8f294e37c9ac0148ffa.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">And the solution to this problem is not the only way to improve the quality of network training at times by manipulating the topology. I showed it precisely because it could be beautifully demonstrated with an analytical solution using a simple example. </font></font><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Interestingly, adding one input with a constant significantly improves network performance even in cases where neighborhoods of zero value do not fall into the input task with which we train the network. For example, below I proposed a regular network, and a network with an additional input the same task and the same varying learning speed. Moreover, the entire graph was shifted by one axis on both axes, so that the input of the network received values ‚Äã‚Äãin the range from 1 to 2. The results speak for themselves.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/621/d47/a9a/621d47a9ab594a7a6611ef1260fc8100.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Finally, if we feed this algorithm the first initial integer problem proposed by the Rosenblatt perceptron, the results will be noticeably better than last time. </font><font style="vertical-align: inherit;">But we must also take into account the fact that in a multilayer perceptron there are 30 more neurons of synapses, because writing data on the network requires significantly more bytes than the original data itself weigh.</font></font><br><br><img src="https://habrastorage.org/getpro/habr/post_images/998/a4c/f3b/998a4cf3b8fbc1ef3ec7b3ff92a18bb4.png"><br><br><h3>  findings </h3><br><ul><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The Rosenblatt perceptron can be taught only by such knowledge, which at the point with zero inputs implies zero output. </font><font style="vertical-align: inherit;">If you were interested in the convergence theorem, but did not notice that this follows from it, then reread it more carefully. </font><font style="vertical-align: inherit;">I would never believe that Rosenblatt or Minsky could err in the evidence;</font></font></li><li>       ,   .      ,          ,      ; </li><li>       ,         .  ,     ,    ,      ; </li><li> ,    ,         .       ,    ; </li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The fact that a task can be solved in principle using a neural network does not mean at all that this solution can be achieved from the initial state of the network. </font><font style="vertical-align: inherit;">Moreover, the connectedness of the solution space should be considered as one of the main factors in the training of neural networks;</font></font></li><li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Adding to the input a simple constant value in very many cases greatly improve the quality of the network for a variety of tasks. </font></font></li></ul><br><br> <b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">UPD:</font></font></b><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> Fixed a bug in the code. </font><font style="vertical-align: inherit;">It would seem critical, in the implementation of reverse distribution, but for some reason all the results of the network did not change. </font><font style="vertical-align: inherit;">It surprises me even more.</font></font></div><p>Source: <a href="https://habr.com/ru/post/219647/">https://habr.com/ru/post/219647/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../219623/index.html">An overview of specialized ways to circumvent locks on the Internet</a></li>
<li><a href="../219627/index.html">Epson - report from Droidcon Moscow 2014</a></li>
<li><a href="../219629/index.html">Useful techniques for working with Apache Camel</a></li>
<li><a href="../219635/index.html">Microsoft Azure for Research Grant</a></li>
<li><a href="../219637/index.html">10 game mechanics in HTML Academy</a></li>
<li><a href="../219651/index.html">Rakes that are not worth stepping on</a></li>
<li><a href="../219653/index.html">As we were beaten by current on April 1</a></li>
<li><a href="../219655/index.html">Automate acceptance testing for Android applications using Calabash</a></li>
<li><a href="../219657/index.html">String theory for guitarists</a></li>
<li><a href="../219659/index.html">Review of the home robot ver 0.3</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>