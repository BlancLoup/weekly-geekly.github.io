<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>DevOps was invented by developers so that admins work more</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="4 years ago, the use of containers in production was exotic, but now it is already the norm for both small companies and large corporations. Let's try...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>DevOps was invented by developers so that admins work more</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/0t/qv/o-/0tqvo-vl2gsif26xkr3xsmdwqqk.jpeg" align="left" width="400">  4 years ago, the use of containers in production was exotic, but now it is already the norm for both small companies and large corporations.  Let's try to look at the whole story with devops / containers / microservices in retrospect, take another fresh look at what tasks we initially tried to solve, what solutions we have now and what is not enough for complete happiness? </p><br><p>  I will talk more about the production environment, as I see the bulk of the unsolved problems there. </p><a name="habracut"></a><br><p>  Previously, the production environment looked like this: </p><br><ul><li>  monolithic application working in splendid isolation on a server or virtual server </li><li>  DB on separate servers </li><li>  frontends </li><li>  supporting infrastructure services (caches, queue brokers, etc.) </li></ul><br><p>  At some point, the business began to shift greatly in IT (to a digital product, as it is fashionable to say now), this led to the need to increase both development volumes and speed.  The development methodology has changed to meet new requirements, and this in turn has caused a number of problems at the interface between development and operation: </p><br><ul><li>  monolithic applications difficult to develop a crowd of developers </li><li>  difficult to manage dependencies </li><li>  difficult to release </li><li>  Difficult to deal with problems / bugs in a large application. </li></ul><br><p>  As a solution to these problems, we first obtained microservices, which transferred the complexity from the code area to the integration field. </p><br><blockquote>  If somewhere something has departed, then somewhere something must arrive.  M. Lomonosov </blockquote><p>  In the general case, no sane admin responsible for the availability of the infrastructure as a whole would agree to such changes.  To somehow compensate for this, we have something called DevOps.  I will not even try to argue about what devops are, better see what results we got as a result of the participation of developers in operational issues: </p><br><ul><li>  docker is a convenient way to pack software for deployment in various environments (yes, I really think that docker is just a package :) </li><li>  infrastructure as a code - our infrastructure has become much more complicated and now we just have to fix a way to restore it from scratch somewhere (it was optional before) </li><li>  orchestration - earlier we could afford to pour virtuals / iron servers with our hands for each application, now there are a lot of them and we want to have some kind of "cloud", which we just say "run the service in three copies on different pieces of hardware" </li><li>  a huge amount of tooling to manage this whole farm </li></ul><br><p>  A side effect of these new technologies and approaches is that the barriers to the creation of microservices have finally disappeared. </p><br><p>  When a novice admin or romantic developer looks at a new picture of the world, he thinks that the infrastructure is now a ‚Äúcloud‚Äù on top of a certain number of servers, which is easily scaled by adding servers if necessary.  We kind of built an abstraction over our infrastructure, and now we are not interested in what is happening inside. </p><br><p>  This illusion is shattered immediately after we have a load and we begin to be sensitive to the response time of our services.  For example: </p><br><img src="https://habrastorage.org/webt/26/fm/z3/26fmz3g5qftcmjtt5mjjjxmv7ke.jpeg"><br><p>  Why do some service instances run slower than others?  Immediately after this, questions of this type begin: </p><br><ul><li>  maybe there servers are weaker? </li><li>  maybe someone ate resources? </li><li>  need to find which servers the instances run on: <br>  <em>dc1d9748-30b6-4acb-9a5f-d5054cfb29bd</em> <br>  <em>7a1c47cb-6388-4016-bef0-4876c240ccd6</em> <br>  and look at the neighboring containers and resource consumption </li></ul><br><p>  That is, we began to destroy our abstraction: now we want to know the topology of our "cloud", and the next step we want to manage it. </p><br><p>  As a result, the typical "cloud" infrastructure currently looks something like this (plus, if you recognize your own :) </p><br><ul><li>  there are docker- <em>, kube- servers</em> on each of them 20-50 containers </li><li>  bases work on separate pieces of iron and as a rule the general </li><li>  resource-intensive services <strong>on separate glands</strong> so as not to disturb anyone </li><li>  latency-sensitive services <strong>on separate glands</strong> , so that no one <strong>bothers</strong> them </li></ul><br><p>  I decided to try to put together (from the long-known components) and test a little approach that could save the "cloud" black box for the user. </p><br><p>  We will begin of course with the formulation of the problem: </p><br><ul><li>  we have our server resources and a certain number of services (applications) between which they need to be divided </li><li>  the load on neighboring services should not affect the target service </li><li>  opportunity to utilize idle resources </li><li>  we want to understand how much resources are left and when it‚Äôs time to add capacity </li></ul><br><p>  I tried to peep the solution or approach from existing orchestrators, or rather their cloud-based commercial installations of <strong>Google Satellite Engine</strong> , <strong>Amazon EC2 Container Service</strong> .  As it turned out, this is just a separate installation of kubernetes on top of the virtualoks you rented.  That is, they are not trying to solve the problem of resource allocation virtualok on your services. </p><br><p>  Then I remembered my longtime experience with <strong>Google App Engine</strong> (the most true cloud in my opinion).  In the case of GAE, you really do not know anything about the underlying infrastructure, but simply fill in the code there, it works and automatically scales.  We pay for each hour of each instance of the selected class (CPU Mhz + Memory), the cloud itself regulates the number of such instances, depending on the current load on the application.  Separately, I note that the processor frequency in this case shows only what part of the processor time will be allocated to your instance.  That is, if we have 2.4Ghz percents, and we allocate 600Mhz, then we give 1/4 of the time of one core. </p><br><p>  We will take this approach as a basis.  From the technical side, there is nothing complicated about it, there are cgroups in linux since 2008 (there is a <a href="https://habrahabr.ru/company/selectel/blog/303190/">detailed description</a> on Habr√©).  Focus on open questions: </p><br><ul><li>  how to choose restrictions?  If you ask any developer how much memory his service needs, with a probability of 99% he will answer: "well, let 4Gb probably fit".  The same question about the CPU will definitely remain unanswered :) </li><li>  How much do resource limits work in practice? </li></ul><br><h3 id="cgroupscpu">  Cgroups: CPU </h3><br><ul><li>  shares: proportions of processor allocation </li><li>  quota: hard limit on CPU time per real time </li><li>  cpusets: binding processes to specific cpu (+ NUMA) </li></ul><br><p>  For the test, I wrote an http service that threshes half the time of the request with cpu and half the time just sleeps.  We will run it on a server of 8 cores / 32Gb (hyper-threading is turned off for simplicity).  Let us load it over yandex.tank from a neighboring machine (on a fast network), first only to it, and after some time to a neighboring service.  The response time will be tracked by a histogram with bakts from 20ms to 100ms in increments of 10ms. </p><br><p>  A starting point: </p><br><pre><code class="hljs dos">docker run -d --name service1 --<span class="hljs-built_in"><span class="hljs-built_in">net</span></span> host -e HTTP_PORT=<span class="hljs-number"><span class="hljs-number">8080</span></span> httpservice docker run -d --name service2 --<span class="hljs-built_in"><span class="hljs-built_in">net</span></span> host -e HTTP_PORT=<span class="hljs-number"><span class="hljs-number">8081</span></span> httpservice</code> </pre> <br><p>  bar chart <br><br></p><br><img src="https://habrastorage.org/webt/lb/vo/pl/lbvoplv0bsxqxrjl2gqktu4claw.jpeg"><br><p>  Consumption of cpu in the context of containers: <br><br></p><br><img src="https://habrastorage.org/webt/xh/nd/g-/xhndg-z-jmgjkwpxkqs5wps7cjy.jpeg"><br><p>  We see that at the time of loading the load on service2, the response time of service1 has improved.  I had many hypotheses why this could happen, but I accidentally saw the answer in perf: </p><br><pre> <code class="hljs perl">perf <span class="hljs-keyword"><span class="hljs-keyword">stat</span></span> -p &lt;pid&gt; <span class="hljs-keyword"><span class="hljs-keyword">sleep</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span></code> </pre> <br><p>  Slowly (no load on the neighbor): </p><br><img src="https://habrastorage.org/webt/8v/or/tt/8vorttjil_rktrlmto3-xvn-2b0.png"><br><p>  Fast (with a load on the neighbor): </p><br><img src="https://habrastorage.org/webt/lm/tm/eq/lmtmeqxarctzo4f4nb0bze-suqu.png"><br><p>  The pictures show that we spend the same number of processor cycles in 10 seconds in both cases, but the speed of their "waste" is different (1.2Ghz vs 2.5Ghz).  Of course, this turned out to be the "best friend of performance" - the power saving mode. </p><br><p>  We repair: </p><br><pre> <code class="hljs ruby"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-string"><span class="hljs-string">`seq 0 7`</span></span> <span class="hljs-keyword"><span class="hljs-keyword">do</span></span> echo ‚Äúperformance‚Äù &gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/devices</span></span><span class="hljs-regexp"><span class="hljs-regexp">/system/cpu</span></span><span class="hljs-regexp"><span class="hljs-regexp">/cpu$i/cpufreq</span></span><span class="hljs-regexp"><span class="hljs-regexp">/scaling_governor done</span></span></code> </pre> <br><p>  Run the same test again: </p><br><img src="https://habrastorage.org/webt/-t/wi/_7/-twi_70wmikzjxrph45rkbovsnk.png"><br><img src="https://habrastorage.org/webt/8b/rr/un/8brrunwtymfstehuphi6c132jja.png"><br><p>  Now, we see how service2 begins to interfere with expected service1.  In fact, when no restrictions / priorities are set, we have the distribution of shares of processor time equally (cpu shares 1024: 1024).  At the same time there is no competition for resources, the process can utilize all available resources.  If we want a predictable response time, we need to focus on the <strong>worst case</strong> . </p><br><p>  Let's try to hold service1 with quotas, but first we will quickly understand how cpu quotas are configured: </p><br><ul><li>  period - real time </li><li>  quota - how much CPU time can be spent per period </li><li>  if we want to cut 2 cores: quota = 2 * period </li><li>  if the process has spent quota, processor time is not allocated to it (throttling) until the current period ends </li></ul><br><p>  Let's allocate two cores to service1 (2ms cpu for 1ms) and serve up the increasing load: </p><br><pre> <code class="hljs pgsql">docker run -d <span class="hljs-comment"><span class="hljs-comment">--name service1 --cpu-period=1000 --cpu-quota=2000 ‚Ä¶</span></span></code> </pre> <br><p>  Bar chart: </p><br><img src="https://habrastorage.org/webt/xz/07/h3/xz07h3gw60kztb1uplztlyzv4qy.png"><br><p>  Actual cpu consumption: </p><br><img src="https://habrastorage.org/webt/1c/d6/u_/1cd6u_vvzrkqivvkpubkgnfy8ag.png"><br><p>  Throttled time: </p><br><img src="https://habrastorage.org/webt/bz/rp/zm/bzrpzmckkgz9f7uhtwwhuubcy-a.png"><br><p>  As a result of this test, we found the limit of service performance without degrading the response time at the current quota. </p><br><ul><li>  we know how many limit requests can be submitted from the balancer to such an instance </li><li>  we can calculate in% cpu utilization by service <br>  Fact: <strong>/sys/fs/cgroup/cpu/docker/id/cpuacct.usage</strong> <br>  Limit: <strong>period / quota</strong> <br>  Trigger: <strong>[service1] cpu usage&gt; 90%</strong> (both on each cluster machine, and on the cluster as a whole) </li></ul><br><p>  Distribute resources: </p><br><ul><li>  we divide the car into ‚Äúslots‚Äù without overselling for latency-sensitive services (quota) </li><li>  if we are preparing to increase the load, we start each service so much that consumption is &lt;N% </li><li>  if there is a clever orchestrator and a desire, we do it dynamically </li><li>  the number of free slots is our stock, we keep it at a comfortable level </li></ul><br><img src="https://habrastorage.org/webt/e_/mn/rk/e_mnrkpyiroztnwqzv0x8ysd4yq.png"><br><p>  In order to ‚Äúfinish off‚Äù the machine with a background load, we will try to set the <strong>maximum</strong> cpu-shares to our slots with quotas, and set the ‚Äúbackground‚Äù tasks the <strong>minimum</strong> priority. </p><br><pre> <code class="hljs delphi">docker run --<span class="hljs-keyword"><span class="hljs-keyword">name</span></span> service1 --cpu-shares=<span class="hljs-number"><span class="hljs-number">262144</span></span> --cpu-period=<span class="hljs-number"><span class="hljs-number">1000</span></span> --cpu-quota=<span class="hljs-number"><span class="hljs-number">2000</span></span> ... docker run --<span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=stress1 --rm -it --cpu-shares=<span class="hljs-number"><span class="hljs-number">2</span></span> progrium/stress --cpu <span class="hljs-number"><span class="hljs-number">12</span></span> --timeout <span class="hljs-number"><span class="hljs-number">300</span></span>s</code> </pre> <br><img src="https://habrastorage.org/webt/ot/yf/jk/otyfjkrnr-wicd__t7yrfwmyw6m.png"><br><img src="https://habrastorage.org/webt/7y/vt/x5/7yvtx55uxou_xvw84iuxbo2oowa.png"><br><p>  After this test, I stuck on exercises for 2-3 days with different scheduler settings ( <a href="https://en.wikipedia.org/wiki/Completely_Fair_Scheduler">CFS</a> ) and studying its internal structure.  The conclusions without details are as follows: </p><br><ul><li>  time allocated slots (slices) </li><li>  you can turn <strong>sysctl ‚Äìa | grep kernel.sched_ handles to</strong> reduce scheduling errors, but for my test I did not get a significant effect </li><li>  I set the quota to 2ms / 1ms, it turned out to be quite a small slot </li><li>  as a result, I decided to try the 20ms / 10ms quota (the same 2 cores) </li><li>  200ms / 100ms on 8 cores can be ‚Äúburned‚Äù for 200/8 = wall 50ms, that is, throttling in the limit will be 50ms, this is noticeable against the background of the response time of my test service </li></ul><br><p>  We try 20ms / 10ms: </p><br><pre> <code class="hljs delphi">docker run --<span class="hljs-keyword"><span class="hljs-keyword">name</span></span> service1 --cpu-shares=<span class="hljs-number"><span class="hljs-number">262144</span></span> --cpu-period=<span class="hljs-number"><span class="hljs-number">10000</span></span> --cpu-quota=<span class="hljs-number"><span class="hljs-number">20000</span></span> ... docker run --<span class="hljs-keyword"><span class="hljs-keyword">name</span></span>=stress1 --rm -it --cpu-shares=<span class="hljs-number"><span class="hljs-number">2</span></span> progrium/stress --cpu <span class="hljs-number"><span class="hljs-number">12</span></span> --timeout <span class="hljs-number"><span class="hljs-number">300</span></span>s</code> </pre> <br><img src="https://habrastorage.org/webt/wt/tr/rj/wttrrj5sa5pmasnermmwflnvkuo.png"><br><img src="https://habrastorage.org/webt/30/av/h0/30avh0xu7pzqpdcxzvrlyet9qxq.png"><br><p>  I considered such indicators acceptable and decided to finish with the CPU: </p><br><ul><li>  we loaded the car to 100% cpu usage, but the response time of the service remained at an acceptable level <br><ul><li>  need to test and select parameters </li><li>  ‚ÄúSlots‚Äù + background load - a working resource allocation model </li></ul></li></ul><br><h3 id="cgroupsmemory">  Cgroups: memory </h3><br><p>  The story of memory is more obvious, but I would like to briefly touch on a couple of examples.  Why do we even need to limit the memory services: </p><br><ul><li>  a leaked service can eat up all the memory, and the OOM killer can nail him and not the neighbor </li><li>  A leak service or actively reading from a disk can ‚Äúwash‚Äù page cache, which is very necessary for a neighboring service. <br>  Moreover, using cgroups, we get extended statistics on memory consumption by various groups of processes.  For example, you can understand which of the services how much page cache uses. </li></ul><br><p>  I decided to test the following scenario: our service is actively working with the disk (reads a piece from a 20Gb file from a random offset to each request), the data completely fits into memory (we preheat the cache), next we run a service that reads a huge huge file someone logs came to read). </p><br><pre> <code class="hljs pgsql">dd <span class="hljs-keyword"><span class="hljs-keyword">if</span></span>=/dev/zero <span class="hljs-keyword"><span class="hljs-keyword">of</span></span>=datafile count=<span class="hljs-number"><span class="hljs-number">20024</span></span> bs=<span class="hljs-number"><span class="hljs-number">1048576</span></span> #   <span class="hljs-number"><span class="hljs-number">20</span></span>GB docker run -d <span class="hljs-comment"><span class="hljs-comment">--name service1 .. DATAFILE_PATH=/datadir/datafile ‚Ä¶</span></span></code> </pre> <br><p>  We warm up the cache from the cgroup service: </p><br><pre> <code class="hljs objectivec">cgexec -g memory:docker/&lt;<span class="hljs-keyword"><span class="hljs-keyword">id</span></span>&gt; cat datafile &gt; /dev/null</code> </pre> <br><p>  Check that the file is in cache: </p><br><pre> <code class="hljs nginx"><span class="hljs-attribute"><span class="hljs-attribute">pcstat</span></span> /datadir/datafile</code> </pre> <br><img src="https://habrastorage.org/webt/wk/pr/9c/wkpr9ckuyk2pttsipu8rzi4_kg8.png"><br><p>  We check that the cache was credited to our service: </p><br><img src="https://habrastorage.org/webt/yv/vc/rf/yvvcrfcjrr-f7pj0u6wye--6uvc.png"><br><img src="https://habrastorage.org/webt/al/pi/ui/alpiuieqj251mohh3cdqta2ndva.png"><br><p>  Start the load and try to "flush" the cache: </p><br><pre> <code class="hljs javascript">docker run --rm -ti --name service2 ubuntu cat datafile1 &gt; <span class="hljs-regexp"><span class="hljs-regexp">/dev/</span></span><span class="hljs-literal"><span class="hljs-literal">null</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/xo/xk/__/xoxk__i9p_yz8wefulwyilcmdh0.png"><br><img src="https://habrastorage.org/webt/nw/b3/hx/nwb3hxgcchstfihe2ck1pvisruu.png"><br><p>  As soon as we "washed" the cache a little, 1 this immediately affected the response time. </p><br><p>  Let's do the same, but limit service2 1Gb (the limit applies to both RSS and page cache): </p><br><pre> <code class="hljs javascript">docker run --rm -ti --memory=<span class="hljs-number"><span class="hljs-number">1</span></span>G --name service2 ubuntu cat datafile1 &gt; <span class="hljs-regexp"><span class="hljs-regexp">/dev/</span></span><span class="hljs-literal"><span class="hljs-literal">null</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/lm/6i/pz/lm6ipzdlafl3eiewm7gdx8d81bc.png"><br><img src="https://habrastorage.org/webt/1i/5t/wq/1i5twqsrjt8eduy9nc6i8ansxw4.png"><br><p>  Now we see that the limit works. </p><br><h3 id="cgroupsblkio-disk-io">  Cgroups: blkio (disk i / o) </h3><br><ul><li>  all by analogy with the CPU </li><li>  it is possible to set the weight (priority) </li><li>  iops / traffic limits for read / write </li><li>  can be customized for specific drives </li></ul><br><p>  We will do the same as with the CPU: we will cut off the iops quota for critical services, but set the maximum priority, set the minimum priority for background tasks.  Unlike the CPU, the limit is not very clear here (there is no 100%). </p><br><p>  First, find out the limit of our specific SATA disk with our load profile.  The service from the previous test: 20Gb file and random reading of 1Mb per request, but this time we clamped our service from memory to eliminate the use of page cache. </p><br><img src="https://habrastorage.org/webt/q2/ii/xc/q2iixcrkqaaktmgzdfj80eyminw.png"><br><img src="https://habrastorage.org/webt/-o/-b/fv/-o-bfvl7onlk85enzhhp16mjd6m.png"><br><img src="https://habrastorage.org/webt/dm/s_/jo/dms_joizjcsk1dowkxok9a84qse.png"><br><p>  We received a little more than 200 iops, try to hold the service for 100 iops for reading: </p><br><pre> <code class="hljs delphi">docker run -d --<span class="hljs-keyword"><span class="hljs-keyword">name</span></span> service1 -m <span class="hljs-number"><span class="hljs-number">10</span></span>M --device-<span class="hljs-keyword"><span class="hljs-keyword">read</span></span>-iops /dev/sda:<span class="hljs-number"><span class="hljs-number">100</span></span> ‚Ä¶</code> </pre> <br><img src="https://habrastorage.org/webt/tp/co/2z/tpco2zgbijxjbi5lzib4wgqasjw.png"><br><img src="https://habrastorage.org/webt/wz/il/bq/wzilbqalfqwo5vn3p3q8ommvt7w.png"><br><p>  The limit works, we were not allowed to read more than 100 iops.  In addition to the restriction, we now have extended statistics on disk utilization by specific groups of processes.  For example, you can find out the actual number of read / write operations on each disk (/sys/fs/cgroup/blkio/[id‚ÇΩ/blkio.throttle.io_serviced), and these are only those queries that actually reached the disk. </p><br><p>  Let's try to reload the disk as a background task (for now, without limits / priorities): </p><br><pre> <code class="hljs javascript">docker run -d --name service1 -m <span class="hljs-number"><span class="hljs-number">10</span></span>M ‚Ä¶ docker run -d --name service2 -m <span class="hljs-number"><span class="hljs-number">10</span></span>M ubuntu cat datafile1 &gt; <span class="hljs-regexp"><span class="hljs-regexp">/dev/</span></span><span class="hljs-literal"><span class="hljs-literal">null</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/pz/wx/-c/pzwx-ceg54skh8n-vguqpxawtia.png"><br><img src="https://habrastorage.org/webt/f8/0q/es/f80qesqfm7byvch8wrzscom8tco.png"><br><p>  We got the expected picture, but since service2 was reading consistently, we got more iops in total. </p><br><p>  Now we‚Äôll set priorities: </p><br><pre> <code class="hljs javascript">docker run -d --name service1 -m <span class="hljs-number"><span class="hljs-number">10</span></span>M --blkio-weight <span class="hljs-number"><span class="hljs-number">1000</span></span> docker run -d --name service2 -m <span class="hljs-number"><span class="hljs-number">10</span></span>M --blkio-weight <span class="hljs-number"><span class="hljs-number">10</span></span> ubuntu cat datafile1 &gt; <span class="hljs-regexp"><span class="hljs-regexp">/dev/</span></span><span class="hljs-literal"><span class="hljs-literal">null</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/v4/6i/jh/v46ijh7mbc7salmhdkgbok0dumi.png"><br><img src="https://habrastorage.org/webt/yb/vb/zq/ybvbzq83vjm1qgkurm3u45wlo70.png"><br><p>  I'm used to the fact that nothing immediately works out of the box :) After a couple of days of exercises with IO linux schedulers (I remind you, I had an ordinary spindle SATA disk): </p><br><ul><li>  I could not configure cfq, but there is something to twist </li><li>  The best result on this test was given by the <strong>deadline</strong> scheduler with the following settings: <br><pre> <code class="hljs ruby">echo deadline &gt; <span class="hljs-regexp"><span class="hljs-regexp">/sys/block</span></span><span class="hljs-regexp"><span class="hljs-regexp">/sda/queue</span></span><span class="hljs-regexp"><span class="hljs-regexp">/scheduler echo 1 &gt; /sys</span></span><span class="hljs-regexp"><span class="hljs-regexp">/block/sda</span></span><span class="hljs-regexp"><span class="hljs-regexp">/queue/iosched</span></span><span class="hljs-regexp"><span class="hljs-regexp">/fifo_batch echo 250 &gt; /sys</span></span><span class="hljs-regexp"><span class="hljs-regexp">/block/sda</span></span><span class="hljs-regexp"><span class="hljs-regexp">/queue/iosched</span></span><span class="hljs-regexp"><span class="hljs-regexp">/read_expire</span></span></code> </pre> <br><img src="https://habrastorage.org/webt/gg/qg/g0/ggqgg0yqqmvyznoxzcmy4ilna28.png"><br><img src="https://habrastorage.org/webt/qq/cn/mg/qqcnmgct2gfzltt-ii8vfd_mynu.png"><br><img src="https://habrastorage.org/webt/tn/io/fq/tniofqbs6ew9mruu_ld63suqyio.png"></li></ul><br><p>  I found these results acceptable and did not explore the topic further. </p><br><h3 id="itogo">  Total </h3><br><ul><li>  if you really want to, set up and test everything thoroughly, you can run hadoop next to the combat database in prime time :) </li><li>  before the "true" cloud we are still very far away and there is a car of unresolved issues </li><li>  you need to look at the correct metrics, it is very disciplined and makes you deal with each anomaly, both in production and during similar tests </li></ul><br><p>  <em>Advertising: all the interesting metrics that I found during this test, we added to our agent (now they are in beta testing, will soon be available to everyone).</em>  <em>We have a <a href="https://okmeter.io/">2 week trial</a> , but if you want to look at the cgroups metrics, write to us, we will extend the trial to you.</em> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/349610/">https://habr.com/ru/post/349610/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../349594/index.html">How to build a community. Translation of the book ‚ÄúSocial Architecture‚Äù: Chapter 4. Protocol for the C4 Collaboration</a></li>
<li><a href="../349596/index.html">The hierarchy of IT-systems and the choice of software for the organization of work</a></li>
<li><a href="../349598/index.html">The work of a large distributed team: the advantages of remoteness, problem solving, useful tools</a></li>
<li><a href="../349602/index.html">Flags in function arguments</a></li>
<li><a href="../349604/index.html">Flask Mega-Tutorial, Part XII: Dates and Times (Edition 2018)</a></li>
<li><a href="../349612/index.html">UX Cinema - Emotions. The senses. Interview</a></li>
<li><a href="../349616/index.html">How-to: change of the main domain in the G Suite for the entire company and with the preservation of all data</a></li>
<li><a href="../349618/index.html">Code Integrity Protection with PGP. Part 1. Basic concepts and tools</a></li>
<li><a href="../349620/index.html">Fill holes in the application server 1C and around</a></li>
<li><a href="../349622/index.html">Five reasons to like Flutter.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>