<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Working with text data in scikit-learn (translation of documentation) - part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article presents a translation of a chapter on textual data training from official scikit-learn documentation. 

 The purpose of this chapter is ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Working with text data in scikit-learn (translation of documentation) - part 1</h1><div class="post__text post__text-html js-mediator-article">  <i>This article presents a translation of a chapter on textual data training from official <a href="http://scikit-learn.org/0.15/tutorial/text_analytics/working_with_text_data.html">scikit-learn</a> documentation.</i> <br><br>  The purpose of this chapter is to study some of the most important tools in scikit-learn on one particular task: analyzing a collection of text documents (news articles) on 20 different topics. <br>  In this chapter, we will look at how: <br><ul><li>  upload file and category contents </li><li>  highlight feature vectors suitable for machine learning </li><li>  train a one-dimensional model to perform categorization </li><li>  use the grid search strategy to find the best configuration for feature extraction and for the classifier </li></ul><br><a name="habracut"></a><br><h3>  Installation Instructions </h3><br>  To begin the practice session described in this chapter, you must have scikit-learn installed and all the components on which it depends (numpy, Scipy). <br>  For installation instructions and recommendations for different operating systems, go to <a href="http://scikit-learn.org/stable/install.html">this page</a> . <br>  You can find the local copy of this lesson in your folder: <br>  <i>scikit-learn / doc / tutorial / text_analytics /</i> <br>  <b><i>Now scikit-learn is not installed with the doc / folder and other content.</i></b>  <b><i>You can download it from <a href="https://github.com/scikit-learn/scikit-learn">github.</a></i></b> <br>  The learning examples folder should contain the following files and folders: <br><ul><li>  * .rst files - source of training documents processed using sphinx </li><li>  data - folder for storing data sets in the learning process </li><li>  skeletons - samples of incomplete exercise scripts </li><li>  solutions - exercise solutions </li></ul><br>  You can also copy the skeletons to a new folder anywhere on your hard drive, called sklearn_tut_workspace, where you will edit your own exercise files.  So the original skeletons will remain unchanged: <br><pre><code class="hljs matlab"><span class="hljs-comment"><span class="hljs-comment">% cp -r skeletons work_directory/sklearn_tut_workspace</span></span></code> </pre> <br>  Machine learning algorithms need data.  Go to each $ TUTORIAL_HOME / data subfolder and run the fetch_data.py script from there (first read them). <br>  For example: <br><pre> <code class="hljs mel">% cd $TUTORIAL_HOME/data/languages % less fetch_data.py % <span class="hljs-keyword"><span class="hljs-keyword">python</span></span> fetch_data.py</code> </pre> <br><br><h3>  Download 20 news datasets </h3><br>  The data set is called ‚ÄúTwenty Newsgroups‚Äù.  Here is its official description, taken from the <a href="http://people.csail.mit.edu/jrennie/20Newsgroups/">site</a> : <br><blockquote>  The ‚ÄúThe 20 Newsgroups‚Äù data is a collection of about 20,000 news documents, divided (approximately) evenly among 20 different categories.  As far as we know, it was originally collected by Ken Leng (Ken Lang), perhaps for his work ‚ÄúNewsweeder: Learning to filter netnews‚Äù (‚ÄúNews Browser: learning to filter news from the network‚Äù), although he did not explicitly state this.  The 20 newsgroups collection has become a popular set of data for experimenting with machine learning techniques for text-based applications, such as text classification or clustering. </blockquote><br>  Next, we will use the built-in dataset loader to fetch ‚ÄúThe 20 newsgroups‚Äù from scikit-learn.  Otherwise, the sample can be downloaded manually from the web site, use the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html">sklearn.datasets.load_files</a> function and specifying the ‚Äú20news-bydate-train‚Äù folder to save the unpacked archive. <br>  To make the first example run faster, we will work only with a part of our data set, divided into 4 categories out of 20 possible: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>categories = [<span class="hljs-string"><span class="hljs-string">'alt.atheism'</span></span>, <span class="hljs-string"><span class="hljs-string">'soc.religion.christian'</span></span>, ... <span class="hljs-string"><span class="hljs-string">'comp.graphics'</span></span>, <span class="hljs-string"><span class="hljs-string">'sci.med'</span></span>]</code> </pre> <br>  We can upload a list of files matching the desired categories, as shown below: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.datasets <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> fetch_20newsgroups &gt;&gt;&gt; twenty_train = fetch_20newsgroups(subset=<span class="hljs-string"><span class="hljs-string">'train'</span></span>, ... categories=categories, shuffle=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, random_state=<span class="hljs-number"><span class="hljs-number">42</span></span>)</code> </pre> <br>  The returned data set is the scikit-learn set: a one-dimensional container with fields that can be interpreted as keys in the python dictionary (dict keys), in other words, as signs of the object attributes.  For example, target_names contains a list of the names of the categories requested: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>twenty_train.target_names [<span class="hljs-string"><span class="hljs-string">'alt.atheism'</span></span>, <span class="hljs-string"><span class="hljs-string">'comp.graphics'</span></span>, <span class="hljs-string"><span class="hljs-string">'sci.med'</span></span>, <span class="hljs-string"><span class="hljs-string">'soc.religion.christian'</span></span>]</code> </pre> <br>  The files themselves are loaded into memory as the data attribute.  You can also refer to the file names: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>len(twenty_train.data) <span class="hljs-number"><span class="hljs-number">2257</span></span> &gt;&gt;&gt; len(twenty_train.filenames) <span class="hljs-number"><span class="hljs-number">2257</span></span></code> </pre> <br>  Let's print the first lines of the first file loaded: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>print(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>.join(twenty_train.data[<span class="hljs-number"><span class="hljs-number">0</span></span>].split(<span class="hljs-string"><span class="hljs-string">"\n"</span></span>)[:<span class="hljs-number"><span class="hljs-number">3</span></span>])) From: sd345@city.ac.uk (Michael Collier) Subject: Converting images to HP LaserJet III? Nntp-Posting-Host: hampton &gt;&gt;&gt; print(twenty_train.target_names[twenty_train.target[<span class="hljs-number"><span class="hljs-number">0</span></span>]]) comp.graphics</code> </pre> <br>  Algorithms for learning with a teacher (supervised learning) require that each document in a training set has a litter of a certain category.  In our case, the category is the name of the news sample, which ‚Äúby chance‚Äù turns out to be the name of the folder containing the characteristic documents. <br>  To increase speed and efficient use of memory, scikit-learn loads the target attribute as an array of integers that corresponds to the index of the category name from the target_names list.  The category index of each sample is stored in the target attribute: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>twenty_train.target[:<span class="hljs-number"><span class="hljs-number">10</span></span>] array([<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>])</code> </pre> <br>  You can get the category name: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> t <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> twenty_train.target[:<span class="hljs-number"><span class="hljs-number">10</span></span>]: ... print(twenty_train.target_names[t]) ... comp.graphics comp.graphics soc.religion.christian soc.religion.christian soc.religion.christian soc.religion.christian soc.religion.christian sci.med sci.med sci.med</code> </pre> <br>  You may notice that the samples were randomly shuffled (using a randomly generated number - fixed RNG seed).  This method is suitable if you want to use only the first samples for quick training of the model and if you want to get a general idea of ‚Äã‚Äãthe results before the subsequent retraining on the full set of data. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Extracting features from text files </h3><br>  To use machine learning on text documents, first of all, you need to translate the text content into a numerical feature vector. <br><h5>  "Bag of words" (set of words) </h5><br>  The most intuitive way to do the transformation described above is to present the text as a set of words: <br><ol><li>  assign a unique integer index to each word appearing in the documents in the training set (for example, building a dictionary of words with integer indices). </li><li>  for each document #i, count the number of uses of each word w and save it (the number) in X [i, j].  This will be the value of the sign #j, where j is the index of the word w in the dictionary. </li></ol><br>  The ‚Äúbag of words‚Äù view implies that n_features are a number of unique words in the package.  Usually, this amount exceeds 100,000. <br>  If n_samples == 10000, then X, saved as a float32 numpy array, would require <b>10000 x 100000 x 4 bytes = 4GB of RAM (RAM)</b> , which is hardly feasible in modern computers. <br>  Fortunately, <b>most of the values ‚Äã‚Äãin X are zeros</b> , since less than a couple of hundred unique words are used in one document.  Therefore, the ‚Äúbag of words‚Äù is often a <b>highly dimensional sparse data set</b> .  We can save a lot of free RAM by storing only non-zero parts of the feature vectors. <br>  The scipy.sparse matrices are the data structures that do just that - structure the data.  Scikit-learn has built-in support for these structures. <br><br><h3>  Tokenize text with scikit-learn </h3><br>  Text preprocessing, tokenization and filtering of stop words are included in the high-level component, which allows you to create a dictionary of characteristic features and translate documents into feature vectors: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> CountVectorizer &gt;&gt;&gt; count_vect = CountVectorizer() &gt;&gt;&gt; X_train_counts = count_vect.fit_transform(twenty_train.data) &gt;&gt;&gt; X_train_counts.shape (<span class="hljs-number"><span class="hljs-number">2257</span></span>, <span class="hljs-number"><span class="hljs-number">35788</span></span>)</code> </pre> <br>  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html">CountVectorizer</a> supports the counting of N-gram words or character sequences.  The vectorizer builds a dictionary of feature indexes: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>count_vect.vocabulary_.get(<span class="hljs-string"><span class="hljs-string">u'algorithm'</span></span>) <span class="hljs-number"><span class="hljs-number">4690</span></span></code> </pre> <br>  The index value of a word in a dictionary is related to its frequency of use throughout the training building. <br><br><h3>  From use to frequency </h3><br>  Counting word usage is a good start, but there is a problem: in long documents, the average number of word usage will be higher than in short ones, even if they are devoted to one topic. <br>  To avoid these potential inconsistencies, it is sufficient to divide the number of uses of each word in the document by the total number of words in the document.  This new feature is called tf - Frequency term. <br>  The next refinement of the tf measure is the weight loss of the word that appears in many documents in the corpus, and hence is less informative than those used only in a small part of the corpus.  As an example of low-normative words, there can be official words, articles, prepositions, conjunctions, etc. <br>  This decrease is called <a href="http://en.wikipedia.org/wiki/Tf%25E2%2580%2593idf">tf ‚Äì idf</a> , which means ‚ÄúTerm Frequency times Inverse Document Frequency‚Äù. <br>  Both measures <b>tf</b> and <b>tf ‚Äì idf</b> can be calculated as follows: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> sklearn.feature_extraction.text <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> TfidfTransformer &gt;&gt;&gt; tf_transformer = TfidfTransformer(use_idf=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>).fit(X_train_counts) &gt;&gt;&gt; X_train_tf = tf_transformer.transform(X_train_counts) &gt;&gt;&gt; X_train_tf.shape (<span class="hljs-number"><span class="hljs-number">2257</span></span>, <span class="hljs-number"><span class="hljs-number">35788</span></span>)</code> </pre> <br>  In the example code above, we first use the fit (..) method to run our evaluation algorithm on the data, and then the transform (..) method to transform our numeric matrix to the tf-idf representation.  These two steps can be combined and give the same result at the output, but faster, which can be done by skipping over processing.  To do this, use the fit_transform (..) method, as shown below: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">&gt;&gt;&gt; </span></span>tfidf_transformer = TfidfTransformer() &gt;&gt;&gt; X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts) &gt;&gt;&gt; X_train_tfidf.shape (<span class="hljs-number"><span class="hljs-number">2257</span></span>, <span class="hljs-number"><span class="hljs-number">35788</span></span>)</code> </pre> <br><br>  ... <br>  Continuation will be in <a href="http://habrahabr.ru/post/266025/">part 2</a> . </div><p>Source: <a href="https://habr.com/ru/post/264339/">https://habr.com/ru/post/264339/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../264327/index.html">Tor Weekly News - July 22, 2015</a></li>
<li><a href="../264329/index.html">Security Week 32: Android Stagefright, New Car Riders, Do Not Track 2.0</a></li>
<li><a href="../264333/index.html">Define key products using linear regression</a></li>
<li><a href="../264335/index.html">Vulnerability in Mozilla Firefox web browser is exploited in-the-wild</a></li>
<li><a href="../264337/index.html">Glyuching, attacks on third-party channels and hacker projects on Kickstarter (continued)</a></li>
<li><a href="../264341/index.html">STC Metrotek. Part 2. Bercut-MMT and the rake race</a></li>
<li><a href="../264343/index.html">React boilerplate - Rocket React</a></li>
<li><a href="../264345/index.html">Introduction to JavaScript Iterators on ES6</a></li>
<li><a href="../264347/index.html">The third PyCon Russia will be held in Yekaterinburg on September 18-19</a></li>
<li><a href="../264349/index.html">Underground carders market. Translation of the book "Kingpin". Chapter 6. ‚ÄúI Miss Crime‚Äù</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>