<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Classification of data by the support vector machine</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Good day! 

 In this article I want to talk about the problem of classifying data using the Support Vector Machine (SVM). This classification has a fa...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Classification of data by the support vector machine</h1><div class="post__text post__text-html js-mediator-article">  Good day! <br><br>  In this article I want to talk about the problem of classifying data using the Support Vector Machine (SVM).  This classification has a fairly wide application: from pattern recognition or the creation of spam filters to calculate the distribution of hot aluminum particles in rocket exhaust. <br><br>  First, a few words about the original problem.  The task of classification is to determine which class of at least two initially known objects this object belongs to.  Typically, such an object is a vector in <i>n-</i> dimensional real space <img src="https://habrastorage.org/storage/habraeffect/ec/9e/ec9e94384dbf5d14049bc5fa7e7d0aeb.png">  .  The coordinates of the vector describe the individual attributes of the object.  For example, the color <b><i>c</i></b> specified in the RGB model is a vector in three-dimensional space: <b><i>c</i></b> = ( <i>red, green, blue</i> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <a name="habracut"></a><br><br>  If there are only two classes (‚Äúspam / not spam‚Äù, ‚Äúgive credit / not give credit‚Äù, ‚Äúred / black‚Äù), then the problem is called binary classification.  If there are several classes, a multi-class (multi-class) classification.  There may also be samples of each class - objects about which it is known in advance to which class they belong.  Such tasks are called <i>learning with a teacher</i> , and known data is called a <i>learning sample</i> .  (Note: if classes are not initially set, then we have <i>a clustering task.</i> ) <br><br>  The support vector method discussed in this article is about learning with a teacher. <br><br>  So, the mathematical formulation of the classification problem is as follows: let <b>X</b> be an object space (for example, <img src="https://habrastorage.org/storage/habraeffect/ec/9e/ec9e94384dbf5d14049bc5fa7e7d0aeb.png">  ), <b><i>Y</i></b> - our classes (for example, <b><i>Y</i></b> = {-1,1}).  Dana training sample: <img src="https://habrastorage.org/storage/habraeffect/eb/d6/ebd63e72e26bec724d5b6934f3b22c11.png">  .  Required to build a function <img src="https://habrastorage.org/storage/habraeffect/2a/bb/2abbfe4e68a84c7d15cf6d55d4f1e3f5.png">  (classifier) ‚Äã‚Äãassociating the class <b><i>y with</i></b> an arbitrary object <b><i>x.</i></b> <br><br><h4>  Support Vector Machine </h4><br><br>  This method initially refers to binary classifiers, although there are ways to make it work for multiclassification tasks. <br><br><h5>  Idea of ‚Äã‚Äãthe method </h5><br><br>  It is convenient to illustrate the idea of ‚Äã‚Äãthe method in the following simple example: the points in the plane are divided into two classes (Fig. 1).  Let's draw a line separating these two classes (the red line in Fig. 1).  Further, all new points (not from the training set) are automatically classified as follows: <br><br>  the point above the line falls into class <b>A</b> , <br>  the point below the line is in class <b>B.</b> <br><br><img src="https://habrastorage.org/storage/habraeffect/8c/98/8c98d4824065028420f290d88e52b40e.png"><br><br>  Such a line is called the <i>dividing</i> line.  However, in spaces of high dimensions, the straight will no longer divide our classes, since the concept of ‚Äúbelow the line‚Äù or ‚Äúabove the line‚Äù loses all meaning.  Therefore, instead of straight lines, it is necessary to consider hyperplanes ‚Äî spaces whose dimension is one less than the dimension of the original space.  AT <img src="https://habrastorage.org/storage/habraeffect/3e/89/3e89720bd70e886270233185080bfd1a.png">  For example, a hyperplane is a regular two-dimensional plane. <br><br>  In our example, there are several straight lines separating two classes (Fig. 2): <br><br><img src="https://habrastorage.org/storage/habraeffect/7c/5f/7c5f4284e204a7c4b544a9ca175a2b13.png"><br><br>  From the point of view of classification accuracy, it is best to choose a straight line, the distance from which to each class is maximum.  In other words, we choose the straight line that separates the classes in the best way (the red straight line in Fig. 2).  Such a straight line, and in the general case a hyperplane, is called the optimal separating hyperplane. <br><br>  The <i>vectors</i> that are closest to the separating hyperplane are called support vectors.  In Figure 2, they are marked in red. <br><br><h5>  A bit of math </h5><br><br>  Let there is a training set: <img src="https://habrastorage.org/storage/habraeffect/ec/0f/ec0fd2147020136102f199d847315336.png">  . <br><br>  The support vector machine builds the classification function <i>F</i> in the form <img src="https://habrastorage.org/storage/habraeffect/9e/39/9e39396ca18b921d9afbef6d92607ddb.png">  , <br>  Where <img src="https://habrastorage.org/storage/habraeffect/bc/aa/bcaa9f6be3acdb74579d883fa63c21f3.png">  Is the scalar product, <b><i>w</i></b> is a normal vector to the dividing hyperplane, <i>b</i> is an auxiliary parameter.  Those objects for which <i>F ( <b>x</b> ) = 1</i> fall into one class, and objects with <i>F ( <b>x</b> ) = -1</i> into another.  The choice of such a function is not accidental: any hyperplane can be given in the form <img src="https://habrastorage.org/storage/habraeffect/bb/77/bb77637fa821ab46507330ed645ceb33.png">  for some <b><i>w</i></b> and <i>b.</i> <br><br><img src="https://habrastorage.org/storage/habraeffect/41/27/41273d1e28d6b6c0b7c9a42eac2be771.png"><br><br>  Next, we want to choose such <b><i>w</i></b> and <i>b</i> which maximize the distance to each class.  It can be calculated that this distance is equal to <img src="https://habrastorage.org/storage/habraeffect/31/86/3186bd99eb78eadf2af35359b431c58e.png">  .  The problem of finding the maximum <img src="https://habrastorage.org/storage/habraeffect/31/86/3186bd99eb78eadf2af35359b431c58e.png">  equivalent to the problem of finding a minimum <img src="https://habrastorage.org/storage/habraeffect/05/47/0547d2ad77f3163ca1641d382a173ad2.png">  .  We write it all in the form of an optimization problem: <br><br><img src="https://habrastorage.org/storage/habraeffect/0e/06/0e0620af2c03143d1e40db91806d375e.png"><br><br>  which is a standard quadratic programming problem and is solved using Lagrange multipliers.  A description of this method can be found <a href="http://en.wikipedia.org/wiki/Lagrange_multipliers">on Wikipedia.</a> <br><br><h4>  Linear inseparability </h4><br><br>  In practice, cases where the data can be divided by a hyperplane, or, as they say, <i>linear</i> , are quite rare.  An example of linear inseparability can be seen in Figure 3: <br><br><img src="https://habrastorage.org/storage/habraeffect/3b/0a/3b0af1a82f51ee4b5fbbadf8b51d8fab.png"><br><br>  In this case, do this: all the elements of the training sample are embedded in the space <b>X of</b> a higher dimension using a special mapping <img src="https://habrastorage.org/storage/habraeffect/ea/d4/ead4cdea29783cc66b34f5d3706f86e8.png">  .  With this mapping <img src="https://habrastorage.org/storage/habraeffect/2b/4c/2b4ccca025f8774728d4201a3d645406.png">  is chosen so that in the new space <b>X the</b> sample is <i>linearly</i> separable. <br><br>  The classifying function <i>F</i> takes the form <img src="https://habrastorage.org/storage/habraeffect/46/9f/469fa9d46d14e1afb2d5b26ff598851b.png">  .  Expression <img src="https://habrastorage.org/storage/habraeffect/8f/3b/8f3bee609b3f807563af6f7e4ae5ba37.png">  called <i>the</i> classifier <i>kernel</i> .  From a mathematical point of view, the core can be any positive definite symmetric function of two variables.  Positive definiteness is necessary for the corresponding Lagrange function in the optimization problem to be bounded below, i.e.  optimization task would be correctly defined. <br><br>  The accuracy of the classifier depends, in particular, on the choice of the core.  In the video, you can see an illustration of classification using a polynomial kernel: <br><br><iframe width="420" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/3liCbRZPrZA%3Ffeature%3Doembed&amp;xid=25657,15700022,15700186,15700191,15700253,15700256,15700259&amp;usg=ALkJrhhKr-C4C865K5QcfkuccuhP5nnz2w" frameborder="0" allowfullscreen=""></iframe><br><br>  The most common in practice are the following cores: <br><br>  <b>Polynomial:</b> <img src="https://habrastorage.org/storage/habraeffect/8d/73/8d734dc93985270c76bfc0e059556264.png"><br><br>  <b>Radial basic function:</b> <img src="https://habrastorage.org/storage/habraeffect/df/c6/dfc66ea3b4a833ef4033ba07362b31d3.png"><br><br>  <b>Gaussian radial basis function:</b> <img src="https://habrastorage.org/storage/habraeffect/4d/b0/4db09c6643fd0bb3d19f4458707209ec.png"><br><br>  <b>Sigmoid:</b> <img src="https://habrastorage.org/storage/habraeffect/35/37/353764f90f9af20442bcc97c6a9b4a08.png"><br><br><h4>  Conclusion and literature </h4><br><br>  Among other classifiers I want to mention also the method of relevant vectors (Relevance Vector Machine, RVM).  Unlike SVM, this method gives the probabilities with which an object belongs to a given class.  Those.  if the SVM says " <b><i>x</i></b> belongs to class A", then the RVM says " <b><i>x</i></b> belongs to class A with probability <i>p</i> and class B with probability <i>1-p</i> ". <br><br>  1. <b>Christopher M. Bishop.</b>  <i>Pattern recognition and machine learning,</i> 2006. <br><br>  2. <b>K.V. Vorontsov.</b>  <i><a href="http://www.ccas.ru/voron/download/SVM.pdf">Lectures on the support vectors method.</a></i> </div><p>Source: <a href="https://habr.com/ru/post/105220/">https://habr.com/ru/post/105220/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../105209/index.html">Bicycle teaming with monorail</a></li>
<li><a href="../105214/index.html">Apacer now on Habrahabr</a></li>
<li><a href="../105215/index.html">Developing web applications in Common Lisp (part two)</a></li>
<li><a href="../105217/index.html">Circle - a new form of USB hubs</a></li>
<li><a href="../105219/index.html">Button to rotate the screen on the X220 tablet</a></li>
<li><a href="../105221/index.html">Facebook and Skype are ready to merge in unison</a></li>
<li><a href="../105222/index.html">Inspector Gadget - 80th Edition</a></li>
<li><a href="../105223/index.html">iDish or virtual dishes</a></li>
<li><a href="../105224/index.html">What is considered someone else's post?</a></li>
<li><a href="../105225/index.html">You can now disable email conversations in Gmail.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>