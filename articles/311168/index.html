<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>ClusterR clustering, part 2</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article focuses on clustering, more precisely, my recently added ClusterR package to CRAN. The details and examples below are mostly based on the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>ClusterR clustering, part 2</h1><div class="post__text post__text-html js-mediator-article">  This article focuses on clustering, more precisely, my recently added <b>ClusterR</b> package to CRAN.  The details and examples below are mostly based on the Vignette package. <br><br>  <a href="https://en.wikipedia.org/wiki/Cluster_analysis">Cluster analysis</a> or clustering is the task of grouping a set of objects so that the objects within one group (called a cluster) are more similar (in one sense or another) to each other than to objects in other groups (clusters).  This is one of the main tasks of the research analysis of data and the standard technique of statistical analysis used in various fields, including  machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics. <br><br>  The most well-known examples of clustering algorithms are <i>clustering based on connectivity</i> (hierarchical clustering), <i>clustering based on centers</i> (k-means method, k-medoid method), <i>distribution-based clustering</i> (GMM - Gaussian mixture models) and <i>clustering based on density</i> (DBSCAN - Density-based spatial clustering of applications with noise - spatial clustering of applications with noise based on density, OPTICS - ordering points to identify the clustering structure - ordering points to determine the clustering structure, etc.). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In the <a href="https://habrahabr.ru/company/infopulse/blog/310288/">first part</a> : Gaussian distribution mixture (GMM), k-means method, k-means method in mini-groups. <br><a name="habracut"></a><br><h4>  K-medoid method </h4><br>  The <a href="https://en.wikipedia.org/wiki/K-medoids">k-Medoid Algorithm</a> (L. Kaufman, P. Russo, 1987) is a clustering algorithm that has in common with the k-means algorithm and the Medoid shift algorithm.  Alogrhythms of k-medium and k-medoids are separating, and both are trying to minimize the distance between the points, presumably from one cluster and the point designated by the center of this cluster.  Unlike the k-means algorithm, the k-medoid method chooses <i>points from a dataset as centers</i> (medoids or standards) and works with <i>an arbitrary metric of distances</i> between points.  A useful tool for determining k is the width of the contour.  The k-medoid method is more resistant to noise and anomalous values ‚Äã‚Äãthan k-means, because it minimizes the sum of pairwise deviations, rather than the sum of squares of Euclidean distances.  A medoid can be defined as a cluster object, whose average deviation from all other objects in the cluster is minimal, i.e.  this is the closest point to the cluster center. <br><br>  The most common implementation of k-medoid clustering is the separation algorithm around the medoids (PAM - Partitioning Around Medoids).  RAM has two phases: ‚Äúbuild‚Äù (BUILD) and ‚Äúrearrange‚Äù (SWAP).  In the BUILD phase, the algorithm searches for a good set of source meadoids, and in the SWAP phase, all possible movements between BUILD-medoids and values ‚Äã‚Äãare made until the objective function stops decreasing (Clustering in an object-oriented environment, A. Struyf, M. Hubert , P. Russo, 1997). <br><br>  In the ClusterR package, the <b>Cluster_Medoids</b> and <b>Clara_Medoids functions</b> correspond to the PAM (partitioning around medoids) and CLARA (clustering large applications) algorithms. <br><br>  In the following code example, I use the <i>mushroom</i> data to illustrate how the k-medoid method works with a distance metric other than Euclidean.  The <i>mushroom</i> data consists of 23 categorical attributes (including class) and 8124 values.  The package documentation has more information about this data. <br><br><h5>  Cluster_Medoids </h5><br>  As input data, the <i>Cluster_Medoids</i> function can also accept a deviation matrix (in addition to the data matrix).  In the case of data mushroom, where all variables are categorical (with two or more unique values), it makes sense to use the distance <i>gower</i> .  Distance <i>gower</i> applies different functions for different indicators depending on the type (numerical, ordered and unordered list).  This deviation metric is implemented in a number of R packages, among others in the cluster package ( <i>daisy</i> function) and the FD package ( <i>gowdis</i> function).  I will use the <i>gowdis</i> function from the FD package, since it also allows you to specify user-defined weights as a separate factor. <br><br><pre><code class="python hljs">data(mushroom) X = mushroom[, <span class="hljs-number"><span class="hljs-number">-1</span></span>] y = <span class="hljs-keyword"><span class="hljs-keyword">as</span></span>.numeric(mushroom[, <span class="hljs-number"><span class="hljs-number">1</span></span>]) <span class="hljs-comment"><span class="hljs-comment">#     gwd = FD::gowdis(X) #   'gower'    gwd_mat = as.matrix(gwd) #     cm = Cluster_Medoids(gwd_mat, clusters = 2, swap_phase = TRUE, verbose = F)</span></span></code> </pre> <br><table><tbody><tr><th>  adjusted_rand_index </th><th>  avg_silhouette_width </th></tr><tr><td>  0.5733587 </td><td>  0.2545221 </td></tr></tbody></table>  As previously mentioned, the <i>FD</i> package <i>gowdis</i> function allows the user to set different weights for each individual variable.  The weights parameter can be adjusted, for example, using a <a href="http://www.math.vu.nl/~sbhulai/papers/paper-vandenhoven.pdf">random search</a> , to achieve better clustering results.  For example, using such weights, each individual variable can be improved and <i>adjusted-rand-index</i> (external validation), and <i>average silhouette width</i> (average contour width, internal validation). <br><table><tbody><tr><th>  predictors </th><th>  weights </th></tr><tr><td>  cap_shape </td><td>  4.626 </td></tr><tr><td>  cap_surface </td><td>  38.323 </td></tr><tr><td>  cap_color </td><td>  55.899 </td></tr><tr><td>  bruises </td><td>  34.028 </td></tr><tr><td>  odor </td><td>  169.608 </td></tr><tr><td>  gill_attachment </td><td>  6.643 </td></tr><tr><td>  gill_spacing </td><td>  42.08 </td></tr><tr><td>  gill_size </td><td>  57.366 </td></tr><tr><td>  gill_color </td><td>  37.938 </td></tr><tr><td>  stalk_shape </td><td>  33.081 </td></tr><tr><td>  stalk_root </td><td>  65.105 </td></tr><tr><td>  stalk_surface_above_ring </td><td>  18.718 </td></tr><tr><td>  stalk_surface_below_ring </td><td>  76.165 </td></tr><tr><td>  stalk_color_above_ring </td><td>  27.596 </td></tr><tr><td>  stalk_color_below_ring </td><td>  26.238 </td></tr><tr><td>  veil_type </td><td>  0.0 </td></tr><tr><td>  veil_color </td><td>  1.507 </td></tr><tr><td>  ring_number </td><td>  37.314 </td></tr><tr><td>  ring_type </td><td>  32.685 </td></tr><tr><td>  spore_print_color </td><td>  127.87 </td></tr><tr><td>  population </td><td>  64.019 </td></tr><tr><td>  habitat </td><td>  44.519 </td></tr></tbody></table><br><pre> <code class="python hljs">weights = c(<span class="hljs-number"><span class="hljs-number">4.626</span></span>, <span class="hljs-number"><span class="hljs-number">38.323</span></span>, <span class="hljs-number"><span class="hljs-number">55.899</span></span>, <span class="hljs-number"><span class="hljs-number">34.028</span></span>, <span class="hljs-number"><span class="hljs-number">169.608</span></span>, <span class="hljs-number"><span class="hljs-number">6.643</span></span>, <span class="hljs-number"><span class="hljs-number">42.08</span></span>, <span class="hljs-number"><span class="hljs-number">57.366</span></span>, <span class="hljs-number"><span class="hljs-number">37.938</span></span>, <span class="hljs-number"><span class="hljs-number">33.081</span></span>, <span class="hljs-number"><span class="hljs-number">65.105</span></span>, <span class="hljs-number"><span class="hljs-number">18.718</span></span>, <span class="hljs-number"><span class="hljs-number">76.165</span></span>, <span class="hljs-number"><span class="hljs-number">27.596</span></span>, <span class="hljs-number"><span class="hljs-number">26.238</span></span>, <span class="hljs-number"><span class="hljs-number">0.0</span></span>, <span class="hljs-number"><span class="hljs-number">1.507</span></span>, <span class="hljs-number"><span class="hljs-number">37.314</span></span>, <span class="hljs-number"><span class="hljs-number">32.685</span></span>, <span class="hljs-number"><span class="hljs-number">127.87</span></span>, <span class="hljs-number"><span class="hljs-number">64.019</span></span>, <span class="hljs-number"><span class="hljs-number">44.519</span></span>) gwd_w = FD::gowdis(X, w = weights) <span class="hljs-comment"><span class="hljs-comment">#  'gower'    d_mat_w = as.matrix(gwd_w) #     cm_w = Cluster_Medoids(gwd_mat_w, clusters = 2, swap_phase = TRUE, verbose = F)</span></span></code> </pre><br><table><tbody><tr><th>  adjusted_rand_index </th><th>  avg_silhouette_width </th></tr><tr><td>  0.6197672 </td><td>  0.3000048 </td></tr></tbody></table><br><h5>  Clara_medoids </h5><br>  <a href="https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/CLARA">CLARA</a> (CLustering LARge Applications - clustering large applications) is the obvious choice for clustering large data sets.  Instead of searching for medoids for the entire data set ‚Äî calculating the incompatibility matrix is ‚Äã‚Äãalso an impossible task ‚Äî CLARA takes a small sample and applies the PAM algorithm (Partitioning Around Medoids) to generate the best set of medoids for this sample.  The quality of the obtained medoids is determined by the average inappropriateness between each object in the dataset and the medoid of its cluster. <br><br>  The <i>Clara_Medoids</i> function in the ClusterR package uses the same logic, applying the <i>Cluster_Medoids</i> function to each sample.  <i>Clara_Medoids</i> takes two more parameters: <i>samples</i> and <i>sample_size</i> .  The first determines the number of samples that need to be formed from the original data set, the second - the share of data in each iteration of the sampling (a fractional number between 0.0 and 1.0).  It is worth mentioning that the <i>Clara_Medoids</i> function <i>does</i> not accept the incompatibility matrix as an input, unlike the <i>Cluster_Medoids</i> function. <br><br>  I will apply the Clara_Medoids function to the <i>mushroom</i> dataset previously used, taking the <i>Hamming distance</i> as the dissimilarity metric, and compare the computation time and result with the result of the <i>Cluster_Medoids</i> function.  <i>Hamming distance</i> is suitable for <i>mushroom</i> data, because  it applies to discrete variables and is defined as the number of attributes that take different values ‚Äã‚Äãfor two compared instances ("Data Mining Algorithms: An Explanation With R", Powell Chikos, 2015, p. 318). <br><br><pre> <code class="python hljs">cl_X = X <span class="hljs-comment"><span class="hljs-comment">#    #  Clara_Medoids     #      for (i in 1:ncol(cl_X)) { cl_X[, i] = as.numeric(cl_X[, i]) } start = Sys.time() cl_f = Clara_Medoids(cl_X, clusters = 2, distance_metric = 'hamming', samples = 5, sample_size = 0.2, swap_phase = TRUE, verbose = F, threads = 1) end = Sys.time() t = end - start cat('time to complete :', t, attributes(t)$units, '\n') #   : 3.104652 </span></span></code> </pre><br><table><tbody><tr><th>  adjusted_rand_index </th><th>  avg_silhouette_width </th></tr><tr><td>  0.5944456 </td><td>  0.2678507 </td></tr></tbody></table><br><pre> <code class="python hljs">start = Sys.time() cl_e = Cluster_Medoids(cl_X, clusters = <span class="hljs-number"><span class="hljs-number">2</span></span>, distance_metric = <span class="hljs-string"><span class="hljs-string">'hamming'</span></span>, swap_phase = TRUE, verbose = F, threads = <span class="hljs-number"><span class="hljs-number">1</span></span>) end = Sys.time() t = end - start cat(<span class="hljs-string"><span class="hljs-string">'time to complete :'</span></span>, t, attributes(t)$units, <span class="hljs-string"><span class="hljs-string">'\n'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#   : 14.93423 </span></span></code> </pre><br><table><tbody><tr><th>  adjusted_rand_index </th><th>  avg_silhouette_width </th></tr><tr><td>  0.5733587 </td><td>  0.2545221 </td></tr></tbody></table><br>  Using the <i>Hamming</i> distance and <i>Clara_Medoids</i> , and <i>Cluster_Medoids</i> return approximately the same result (compared to the results for the <i>gower</i> distance), but the <i>Clara_Medoids</i> function is more than four times faster than the <i>Cluster_Medoids</i> for this data set. <br><br>  With the results of the last two code fragments, the width of the contour can be plotted using the <b>Silhouette_Dissimilarity_Plot</b> function.  It is worth mentioning here that the incompatibility and the width of the contour in the <i>Clara_Medoids</i> function is on the best sample, and not on the entire data set, as for the <i>Cluster_Medoids</i> function. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     "Clara_Medoids" Silhouette_Dissimilarity_Plot(cl_f, silhouette = TRUE)</span></span></code> </pre><br><img src="https://habrastorage.org/files/d3e/30b/379/d3e30b379bd5427e8e16c3e5d097cbe4.png"><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#     "Cluster_Medoids" Silhouette_Dissimilarity_Plot(cl_e, silhouette = TRUE)</span></span></code> </pre><br><img src="https://habrastorage.org/files/ae5/be5/83e/ae5be583edb1493499dbc14e252fa118.png"><br><h4>  Links for k-medoid functions </h4><br>  The implementation of k-medoid functions ( <i>Cluster_Medoids</i> and <i>Clara_Medoids</i> ) took me quite a lot of time, because at first I thought that the initial medoids were initialized in the same way as the centers in the k-means algorithm.  Since  I did not have access to the book by Kaufman and Rousseau, the <b>cluster</b> package with very detailed documentation helped a lot.  It includes both algorithms, and PAM (Partitioning Around Medoids - separation around medoids), and CLARA (CLustering LARge Applications - clustering large applications), if you like, you can compare the results. <br><br>  In the following code snippet, I‚Äôll <i>compare the</i> <i>cluster‚Äôs</i> pam function <i>cluster</i> and <i>Cluster_Medoids of the ClusterR</i> package and the resulting medoids based on the previous quantization example. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#==================================== #  pam  cluster #==================================== start_pm = Sys.time() set.seed(1) pm_vq = cluster::pam(im2, k = 5, metric = 'euclidean', do.swap = TRUE) end_pm = Sys.time() t_pm = end_pm - start_pm cat('time to complete :', t_pm, attributes(t_pm)$units, '\n') #   : 12.05006  pm_vq$medoids</span></span></code> </pre><br><pre> <code class="diff hljs"># [,1] [,2] [,3] # [1,] 1.0000000 1.0000000 1.0000000 # [2,] 0.6325856 0.6210824 0.5758536 # [3,] 0.4480000 0.4467451 0.4191373 # [4,] 0.2884769 0.2884769 0.2806337 # [5,] 0.1058824 0.1058824 0.1058824</code> </pre><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#==================================== #  Cluster_Medoids  ClusterR   6  (   OpenMP) #==================================== start_vq = Sys.time() set.seed(1) cl_vq = Cluster_Medoids(im2, clusters = 5, distance_metric = 'euclidean', swap_phase = TRUE, verbose = F, threads = 6) end_vq = Sys.time() t_vq = end_vq - start_vq cat('time to complete :', t_vq, attributes(t_vq)$units, '\n') #   : 3.333658  cl_vq$medoids</span></span></code> </pre><br><pre> <code class="diff hljs"># [,1] [,2] [,3] # [1,] 0.2884769 0.2884769 0.2806337 # [2,] 0.6325856 0.6210824 0.5758536 # [3,] 0.4480000 0.4467451 0.4191373 # [4,] 0.1058824 0.1058824 0.1058824 # [5,] 1.0000000 1.0000000 1.0000000</code> </pre><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment">#==================================== #  Cluster_Medoids  ClusterR    #==================================== start_vq_single = Sys.time() set.seed(1) cl_vq_single = Cluster_Medoids(im2, clusters = 5, distance_metric = 'euclidean', swap_phase = TRUE, verbose = F, threads = 1) end_vq_single = Sys.time() t_vq_single = end_vq_single - start_vq_single cat('time to complete :', t_vq_single, attributes(t_vq_single)$units, '\n') #   : 8.693385  cl_vq_single$medoids</span></span></code> </pre><br><pre> <code class="diff hljs"># [,1] [,2] [,3] # [1,] 0.2863456 0.2854044 0.2775613 # [2,] 1.0000000 1.0000000 1.0000000 # [3,] 0.6325856 0.6210824 0.5758536 # [4,] 0.4480000 0.4467451 0.4191373 # [5,] 0.1057339 0.1057339 0.1057339</code> </pre><br>  For this data set (5625 rows and 3 columns), the <i>Cluster_Medoids</i> function returns the same medoids almost four times faster than the <i>pam</i> function if OpenMP is available (6 streams). <br><br>  The current version of the <b>ClusterR</b> package is available in my <a href="https://github.com/mlampros/ClusterR">Github repository</a> , and for bug reports, please use <a href="https://github.com/mlampros/ClusterR/issues">this link</a> . </div><p>Source: <a href="https://habr.com/ru/post/311168/">https://habr.com/ru/post/311168/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../311156/index.html">Alternative version of Java application packaging in Docker</a></li>
<li><a href="../311160/index.html">Intel¬Æ Parallel Studio XE 2017: ‚ÄúPython Comes to Us‚Äù and Others</a></li>
<li><a href="../311162/index.html">SBM Composer visual development environment overview</a></li>
<li><a href="../311164/index.html">Who owns online courses? Copyright and pitfalls</a></li>
<li><a href="../311166/index.html">A person. The creator of Perl, Larry Wall - "the generous lifelong dictator"</a></li>
<li><a href="../311172/index.html">$ mol: reactive micromodular ui-framework</a></li>
<li><a href="../311174/index.html">How to make tooltips in javafx</a></li>
<li><a href="../311180/index.html">Manage a bunch of timers in javascript</a></li>
<li><a href="../311182/index.html">Testing Huawei KunLun in Moscow. Installation</a></li>
<li><a href="../311186/index.html">Challenging chronic diseases in an era of hyper-connectivity</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>