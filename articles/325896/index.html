<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Surf Studio: machine learning in production</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introducing the guest post from the company Surf Studio ( Certified Google Developer Agency ). 

 Hi, Habr. My name is Alexander Olferuk ( @olferuk ),...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Surf Studio: machine learning in production</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/058/1cd/4fd/0581cd4fda7b444c8eb8dfb75135fff5.png"><br>  <i>Introducing the guest post from the company Surf Studio ( <a href="https://developers.google.com/agency/directory/">Certified Google Developer Agency</a> ).</i> <br><br>  Hi, Habr.  My name is Alexander Olferuk ( <a href="https://habrahabr.ru/users/olferuk/">@olferuk</a> ), I do machine learning in <a href="http://surfstudio.ru/">Surf</a> .  Since 2011, we have been developing mobile applications for large businesses, and now we are preparing to release a B2B product with TensorFlow.  Thanks to colleagues from Google for the opportunity to tell a little about our experience. <br><br>  In modern machine learning there are many enthusiasts, but critically there are not enough professionals.  In our team, I watched the transformation of such enthusiasts into specialists with combat experience.  While developing the first commercial product for us related to machine learning, the team was faced with a bunch of nuances.  All the favorite competitions at Kaggle were very far from solving the problems of real business.  Now I want to share experiences, show examples and tell a little about what we went through. <br><a name="habracut"></a><br><h3>  Task </h3><br>  The task of our system is to predict the optimal margin for retail goods, having a three-year history of sales.  The goal is to make the client's business more profitable, of course.  The data is aggregated by week: the technical characteristics of the goods are known, how many units of the goods were sold, how many stores the goods are in stock, purchase and retail prices, as well as the resulting profits.  This is raw data, sales are influenced by many more factors.  All other signs, ranging from inflation and commodity prices, and ending with the weather, we collected on our own.  Before us was a catalog with more than 20,000 products.  Because of the difference in their types, we came to the construction of not one, but immediately a family of models.  Each of them trains on stories about products, in terms of sales behaving the same way. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      In machine learning competitions, the value for prediction is usually predetermined.  In a real business project, we are entitled to choose it ourselves.  Which is better: try to predict the profit, mark-up, or maybe the quantity of goods sold?  We start from the fact that we can predict effectively, and that will allow the client to earn more money. <br><img src="https://habrastorage.org/files/bc1/315/af0/bc1315af01c440a69d9a68e965f3a60a.png"><br><br>  The X axis here is time broken by weeks.  Y is the normalized value, including profit (blue line) and the number of sales (orange line). Since they almost coincide, we can rely on or on any of them in our task. <br><br>  There is another, no less important difference.  The real world is not as perfect as artificial models.  If in competitions the only important criterion is accuracy, then in a real business project it is vital to keep a delicate balance between accuracy and common sense. <br><br><h3>  How to start a project? </h3><br>  The first thing you need to select the appropriate metric.  It should not only reflect the accuracy of the solution, but also correspond to the logic of the subject area.  Obviously, when choosing a markup for a product to maximize profits, it is pointless to choose its zero or extremely large.  It is unprofitable either to the buyer or to the store.  So the solution must be sought in the confidence interval, based on reasonable price limits for a particular product, as well as on its old price. <br><img src="https://habrastorage.org/files/c06/6cc/46e/c066cc46e8634b1a8aaee53237aea013.png"><br>  A lot has been said about cross-validation, <a href="http://scikit-learn.org/stable/modules/cross_validation.html">many techniques</a> have been <a href="http://scikit-learn.org/stable/modules/cross_validation.html">described</a> .  Our task is connected with historical data, that is, changing over time.  Therefore, it is impossible to break the sample into random folds; one can only predict the future values ‚Äã‚Äãof the profit level, learning from the past. <br><br>  However, not all data is equally useful.  As I said, when building models, we take into account real life.  Anomalous data can distort the overall picture and significantly affect the effectiveness of the model.  Therefore, the data collected during the crisis of the end of 2015 - the beginning of 2016, we decided not to consider. <br><br><h3>  Pipeline </h3><br>  After selecting the metric and cross-validation, the team‚Äôs efforts must be made to implement the end-to-end structure as early as possible. <br><div style="text-align:center;"><img src="https://habrastorage.org/files/bc9/11c/a75/bc911ca750f746098e06dcefb2643466.png"></div><br>  The structure on the diagram is constant and should not change with the development of the project.  At first, it is better to limit data cleansing at least in the fight against missing values: for most classifiers, the absence of NaN values ‚Äã‚Äãin the data is critical.  It is also necessary to postpone the transformation of signs and the addition of new ones. <br><br>  After that, you need to choose a baseline model, for example, a decisive forest.  Use it to generate the first reporting.  Answer two questions: ‚ÄúWhat would the customer want to see?‚Äù, ‚ÄúWhat kind of results will be understandable and useful for subject matter experts?‚Äù.  Reports and necessary schedules form based on the answers to these questions.  In the course of further development, it is worth going back to this step, replying again, saving the answers, analyzing the progress. <br><br>  In practice, we received a system with formed reporting rather late.  Therefore, to visually see the positive dynamics in the development of the model is very difficult. <br><br>  In the role of architecture for solving such problems, pipelines, which are abstractions representing a chain of transformations or a combination of functions, have perfectly recommended themselves. <br><br>  In the initial implementation, each of the stages in this chain is a transformer that accepts an object of class <b>ndarray</b> (is a numpy-object) as input. <br><br>  We decided to improve this solution.  At each stage, I wanted to get a new Pandas dataframe.  In this case, the classifiers will receive all the signs necessary for learning from the final table, and visualization will be simplified, because all the explanatory labels are at hand. <br><br>  We refused such libraries as <a href="https://github.com/paulgb/sklearn-pandas">sklearn-pandas</a> or <a href="https://github.com/spotify/luigi">luigi</a> .  In fact, we wrote our own bike.  This is a small and very raw helper, which we made exclusively for ourselves.  In the near future, the haircut, but you can use it <a href="https://github.com/surfstudio/Bacchus">now</a> .  We have tried to make a transparent and capacious interface with the above features. <br><br>  Here are some example steps from our pipeline. <br><br>  Add metal prices, as well as prices with a lag of 1 and 2 months: <br><br><pre><code class="python hljs">(<span class="hljs-string"><span class="hljs-string">'add_metal'</span></span>, DFFeatureUnion([ (<span class="hljs-string"><span class="hljs-string">'metal'</span></span>, DFPipeline([ (<span class="hljs-string"><span class="hljs-string">'load_metal'</span></span>, MetalAppender()), (<span class="hljs-string"><span class="hljs-string">'metal_lag'</span></span>, Lagger([<span class="hljs-number"><span class="hljs-number">4</span></span>, <span class="hljs-number"><span class="hljs-number">8</span></span>])) ])) ]))</code> </pre> <br>  And one more: <br><br><pre> <code class="python hljs">(<span class="hljs-string"><span class="hljs-string">'lags'</span></span>, Lagger(columns_strategies={ <span class="hljs-string"><span class="hljs-string">'Z'</span></span>: { <span class="hljs-string"><span class="hljs-string">'lags'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>], <span class="hljs-string"><span class="hljs-string">'groupby'</span></span>: <span class="hljs-string"><span class="hljs-string">'name'</span></span> }, <span class="hljs-string"><span class="hljs-string">'X'</span></span>: { <span class="hljs-string"><span class="hljs-string">'lags'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-string"><span class="hljs-string">'groupby'</span></span>: <span class="hljs-string"><span class="hljs-string">'name'</span></span> }, <span class="hljs-string"><span class="hljs-string">'Y'</span></span>: { <span class="hljs-string"><span class="hljs-string">'lags'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-string"><span class="hljs-string">'groupby'</span></span>: <span class="hljs-string"><span class="hljs-string">'name'</span></span> }, <span class="hljs-string"><span class="hljs-string">'markup'</span></span>: { <span class="hljs-string"><span class="hljs-string">'lags'</span></span>: [<span class="hljs-number"><span class="hljs-number">1</span></span>], <span class="hljs-string"><span class="hljs-string">'groupby'</span></span>: <span class="hljs-string"><span class="hljs-string">'name'</span></span> } }))</code> </pre> <br>  Here the logic is somewhat more complicated than simply moving the column with the <b>shift</b> operator from the Pandas box.  We must bear in mind that each of the signs is shifted only for a particular product, and not in the entire table at once.  In order to solve this problem, the <b>Lagger</b> class was created. <br><br><h3>  Development model </h3><br>  Further development of the project is a spiral model of development.  If you have new ideas: connect new signs or another way of processing existing ones - check.  With the built architecture, testing your every hypothesis will look like this: <br><br><ol><li>  We modify the data processing pipeline, make the necessary changes; </li><li>  retrain estimator, select the optimal hyperparameters; </li><li>  we calculate the new score for the obtained classifier; </li><li>  We visualize the results, form reports, compare, draw conclusions. </li></ol><br>  A small clarification on the third paragraph.  It is worth paying attention not to the absolute values ‚Äã‚Äãof the metric, but only to its change.  Not only that: you can‚Äôt trust this difference if, on the scale of your sample, everything fits into the framework of random deviation. <br><br>  There are good project organization guidelines.  You can read about <a href="https://blog.godatadriven.com/how-to-start-a-data-science-project-in-python">this</a> and <a href="https://drivendata.github.io/cookiecutter-data-science">this</a> , for example. <br><br>  If you open the project folder, here's what we will see inside: <br><br><pre> <code class="bash hljs">project/ ‚îú‚îÄ‚îÄ data/ &lt;-     ‚îú‚îÄ‚îÄ cache/ &lt;- pickle-    ‚îú‚îÄ‚îÄ notebooks/ &lt;-  ‚îú‚îÄ‚îÄ scripts/ &lt;- *.py-    ‚îú‚îÄ‚îÄ logs/ &lt;-  ‚îú‚îÄ‚îÄ out/ &lt;-   ,   : ‚îî‚îÄ reports/ &lt;- ,    xls- ‚îî‚îÄ plots/ &lt;- ,      plotly ‚îú‚îÄ‚îÄ requirements &lt;-     ‚îî‚îÄ‚îÄ README.md</code> </pre> <br>  How to divide the task between team members to work without interfering with each other?  We came up with this scheme: in the working directory there is a set of Jupyter notebooks, Python scripts and the data itself: cache, reports, graphics.  Everyone works in a separate notebook.  I think it is not necessary to say that the notebook should be well commented, and all calculations in them are reproducible. <br><br>  How many notebooks do you need and how big should they be?  From our experience - a separate notebook for each experiment.  Example: "Checking the need to introduce a confidence interval for predictions."  This includes both the necessary logic code and visualization.  Again, good and see what <a href="https://svds.com/jupyter-notebook-best-practices-for-data-science/">others advise</a> . <br><br>  As soon as the experiment is completed and the hypothesis is verified, all the necessary functions are tested and sent to the Python scripts.  If the experiment justified itself, of course. <br><br><h3>  Visualize it </h3><br>  Before submitting data to the entrance to the classifier and evaluate the result, it is necessary to figure out what we are dealing with.  Here are some tools that helped us in our work: <br><br>  ‚Ä¢ To <b>visualize missing values,</b> use the <a href="https://github.com/ResidentMario/missingno">MissingNo</a> library. <br>  ‚Ä¢ In order to <b>evaluate the nature of the distribution of the trait</b> , we used histograms: violin plot (it is provided, for example, by the seaborn library), box plot.  Why do you need it? <br><br><ul><li>  The nature of the distribution tells you what to do with the missing value: filling with a mod is quite suitable for the beveled ones, but in the case of a normal distribution, you should use the expectation. </li><li>  Skewed data needs to be processed accordingly.  For example, using logarithmization or finding the root of the Nth degree makes the distribution of a trait more like a normal one.  This usually helps to increase accuracy. </li></ul><br>  ‚Ä¢ To <b>estimate the importance of the feature</b> , factor plot was used. <br>  ‚Ä¢ To <b>assess the pairwise correlation of</b> features, the correlation matrix and the scatterplot matrix were used.  The goal is to find strongly correlated symptoms and exclude similar ones, if any.  They do not bear any clear value for the classifier, they only increase the variance of the predictions. <br><br><img src="https://habrastorage.org/files/7fa/4d7/9ff/7fa4d79ff4354a3390235886a5dbd247.png"><br>  This is how we tested the effect of inflation.  The graph in blue shows the change in the level of the retail price of goods in time.  However, it is only necessary to subtract inflation (orange dots), it becomes clear that the retail price fluctuates around a certain level (if you count everything in January 2014 prices, that is, zero week).  So, we correctly took into account the influence of the external economic factor. <br><br><h3>  Moving to the clouds with Google </h3><br>  So, we have built a pipeline that does everything necessary: ‚Äã‚Äãfrom loading and processing data to the formation of the final prediction.  Now you need to think about how to make models more accurate. <br><br>  For this we can: <br><br><ul><li>  choose the best signs; </li><li>  add new ones; </li><li>  search for the best hyperparameters of the models. </li></ul><br>  If you want to modify the finished pipeline, for example, change MinMaxScaler to StandardScaler - you will have to process the data and adjust the parameters of the models again.  Whether you have a lot of data or not much, on home and work computers, to run through dozens of grid search cycles to find the best hyperparameters is for the patient.  It is too long.  Highly. <br><br>  Our solution is ripe right away: we are moving to <a href="https://cloud.google.com/compute/">Google Cloud</a> .  We, however, had to refuse <a href="https://cloud.google.com/datalab/">DataLab</a> : only Python version 2.7 is supported.  As part of the task, we did not need a rich infrastructure, we needed a very powerful virtual machine, and we could deploy <a href="https://github.com/jupyterhub/jupyterhub">JupyterHub</a> ourselves. <br><br>  On the remote machine we created several directories: common and separate for each team member.  This allowed everyone not only to work in a common environment, but also, if desired, to organize their own git flow in a separate section.  For the sake of security, everyone went over https only, the ssh certificate was also made. <br><br>  From the interesting: Google's scripts for starting virtual machines are written in Python 3.5 and did not want to be friends with our Python 3.6.  Fortunately, everything turned out to be solved. <br><br>  Was the game worth the candle?  Of course!  It took a couple of days for the working computers to go through all the hyperparameters for the pipeline.  Everything was much faster in the Google cloud.  Are you leaving home from work?  I came in the morning and everything is ready. <br><br><h3>  What is the result? </h3><br>  From the client's ERP-system (1C, SAP, Oracle and others), you can download historical sales data on demand.  The group of products for which you need to generate forecasts is indicated separately.  Add additional data collected from open sources. <br><img src="https://habrastorage.org/files/a01/c25/84f/a01c2584f6724de1932c39d803012962.png"><br>  Since the optimal parameters of the model are found and cached, it remains little: to train the model and use it to generate new reports.  The reports for the client are collected as aggregated statistics on the available data and forecasts: by company, by product category, by individual goods. <br><br>  The client can adjust the pricing policy on the basis of the received forecasts. <br><br><h3>  A / B-test on hardcore </h3><br>  To show business that smart machines can not only recognize pictures of watermelons and write funny poems, results are needed.  As long as there are no well-known results in the industry, everyone will look at such decisions with great fear. <br><br>  We are very lucky with the client.  They believed in a new technology for them and gave us an important chance to conduct a hardcore A / B test.  Our code was entrusted to formulate price recommendations for all products of the network in the whole region.  The total data is comparable with the data for the regions where prices are formed in the old manner.  If everything goes well, we will be proud of having changed the world a little bit, made our modest contribution to the industry and the penetration of machine learning into business.  Cross your fingers for us? <br><br><h3>  At the end </h3><br>  Remember that simple solutions are often the best, and the best code is clean. <br>  Brush your teeth twice a day. <br>  Reread McConnell. <br>  Crossfit and sports betting are not worth the time spent. <br>  Spend more time with your family. <br>  <i>Good luck, happiness, health!</i> </div><p>Source: <a href="https://habr.com/ru/post/325896/">https://habr.com/ru/post/325896/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../325884/index.html">7 bad tips to the REST API designer</a></li>
<li><a href="../325888/index.html">ThinkPHP # 14 Mitap in Kharkov. 5 years together</a></li>
<li><a href="../325890/index.html">Review of the section "Design" at the conference DUMP-2017</a></li>
<li><a href="../325892/index.html">Management of risks. Part 2</a></li>
<li><a href="../325894/index.html">Reverse engineering of a radio-controlled tank using GNU Radio and HackRF</a></li>
<li><a href="../325898/index.html">Excursion to the Glasgow Science Center - and about the development of the game on "Fixico"</a></li>
<li><a href="../325900/index.html">Unity3d / Android: check user on Node.JS own server</a></li>
<li><a href="../325902/index.html">Fintech: adapt or die</a></li>
<li><a href="../325904/index.html">rssh, or How to allow SCP, but disable SSH</a></li>
<li><a href="../325906/index.html">From idea to release in one bottle. Cloud Management Development Process - Visual Studio Team Services</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>