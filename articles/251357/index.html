<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Data exchange using MPI. Working with the MPI Library on the example of the Intel¬Æ MPI Library</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In this post, we will discuss the organization of data exchange using MPI using the example of the Intel MPI Library. We think that this information w...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Data exchange using MPI. Working with the MPI Library on the example of the Intel¬Æ MPI Library</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/079/279/18c/07927918c7974ead9b28d869081bd3c7.png"><br><br>  In this post, we will discuss the organization of data exchange using MPI using the example of the Intel MPI Library.  We think that this information will be of interest to anyone who wants to get acquainted with the field of parallel high-performance computing in practice. <br><br>  We will provide a brief description of how data is organized in parallel applications based on MPI, as well as links to external sources with a more detailed description.  In the practical part, you will find a description of all the development stages of the ‚ÄúHello World‚Äù demo MPI application, starting with setting up the necessary environment and ending with launching the program itself. <br><a name="habracut"></a><br><h1>  MPI (Message Passing Interface) </h1><br>  <a href="https://ru.wikipedia.org/wiki/Message_Passing_Interface">MPI</a> is an interface for transferring messages between processes that perform a single task.  It is intended, first of all, for systems with distributed memory ( <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2581%25D1%2581%25D0%25BE%25D0%25B2%25D0%25BE-%25D0%25BF%25D0%25B0%25D1%2580%25D0%25B0%25D0%25BB%25D0%25BB%25D0%25B5%25D0%25BB%25D1%258C%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25B0%25D1%2580%25D1%2585%25D0%25B8%25D1%2582%25D0%25B5%25D0%25BA%25D1%2582%25D1%2583%25D1%2580%25D0%25B0">MPP</a> ) in contrast to, for example, <a href="https://ru.wikipedia.org/wiki/OpenMP">OpenMP</a> .  A distributed (cluster) system, as a rule, is a set of computational nodes connected by high-performance communication channels (for example, <a href="https://ru.wikipedia.org/wiki/InfiniBand">InfiniBand</a> ). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      MPI is the most common standard for data communication interface in parallel programming.  MPI <a href="http://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf">Standardization</a> is engaged in <a href="http://www.mpi-forum.org/">MPI Forum</a> .  There are implementations of MPI for most modern platforms, operating systems and languages.  MPI is widely used in solving various problems of computational physics, pharmaceuticals, materials science, genetics and other fields of knowledge. <br><br>  A parallel program from the point of view of MPI is a set of processes running on different compute nodes.  Each process is generated based on the same program code. <br><br>  The main operation in MPI is messaging.  In MPI, almost all the basic communication patterns are implemented: point-to-point, collective (collective) and one-sided. <br><br><h1>  Work with MPI </h1><br>  Let's look at a live example of how a typical MPI program works.  As a demonstration application, we take the source code of the sample supplied with the Intel MPI Library.  Before starting our first MPI program, you need to prepare and set up a working environment for experiments. <br><br><h3>  Setting up a cluster environment </h3><br>  For experiments, we need a pair of computational nodes (preferably with similar characteristics).  If there are no two servers at hand, you can always use cloud-services. <br><br>  For the demonstration, I chose <a href="http://aws.amazon.com/ec2/">Amazon Elastic Compute Cloud</a> (Amazon EC2).  New users Amazon offers a <a href="http://aws.amazon.com/free/">trial year of free use of</a> entry-level servers. <br><br>  Working with Amazon EC2 is intuitive.  If you have questions, you can refer to the detailed <a href="http://aws.amazon.com/documentation/ec2/">documentation</a> (in English).  If you wish, you can use any other similar service. <br><br>  We create two working virtual servers.  In the management console, select <b>EC2 Virtual Servers in the Cloud</b> , then <b>Launch Instance</b> (‚ÄúInstance‚Äù means an instance of a virtual server). <br><br>  The next step is choosing the operating system.  Intel MPI Library supports both Linux and Windows.  For the first acquaintance with MPI, choose OS Linux.  Choose <b>Red Hat Enterprise Linux 6.6 64-bit</b> or <b>SLES11.3 / 12.0</b> . <br>  Select <b>Instance Type</b> (server type).  For experiments, we can use t2.micro (1 vCPUs, 2.5 GHz, Intel Xeon processor family, 1 GiB of RAM).  As a newly registered user, I could use this type for free - marked ‚ÄúFree tier eligible‚Äù.  Set the <b>Number of instances</b> : 2 (the number of virtual servers). <br><br>  After the service prompts us to launch <b>Launch Instances</b> (configured virtual servers), we save the SSH keys that we need to communicate with the virtual servers from the outside.  The status of the virtual servers and the IP address for communicating with the servers of the local computer can be monitored in the management console. <br><br>  An important point: in the <b>Network &amp; Security / Security Groups</b> settings, you need to create a rule that will open ports for TCP connections ‚Äî this is needed for the MPI process manager.  The rule might look like this: <br><blockquote>  Type: Custom TCP Rule <br>  Protocol: TCP <br>  Port Range: 1024-65535 <br>  Source: 0.0.0.0/0 <br></blockquote><br>  For security reasons, you can set a stricter rule, but for our demo, this is enough. <br><br>  <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstances.html">Here</a> you can read instructions on how to connect to virtual servers from a local computer (in English). <br>  I used <a href="http://www.chiark.greenend.org.uk/~sgtatham/putty/">Putty</a> to communicate with working servers from a computer on Windows, <a href="http://winscp.net/eng/download.php">WinSCP</a> for transferring files.  <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html">Here</a> you can read the instructions for setting them up to work with Amazon services (in English). <br><br>  The next step is to configure SSH.  In order to configure passwordless SSH with public key authorization, you must perform the following steps: <br><ol><li>  On each of the hosts, we launch the ssh-keygen utility - it will create a pair of private and public keys in the $ HOME / .ssh directory; </li><li>  We take the contents of the public key (the file with the .pub extension) from one server and add it to the $ HOME / .ssh / authorized_keys file on another server; </li><li>  Perform this procedure for both servers; </li><li>  Let's try to connect via SSH from one server to another and back to check the correctness of the SSH configuration.  When you first connect, you may need to add the remote host's public key to the $ HOME / .ssh / known_hosts list. </li></ol><br><h3>  Configuring MPI Library </h3><br>  So, the working environment is configured.  Time to install MPI. <br>  As a demonstration option, take the 30-day trial version of the <a href="https://software.intel.com/en-us/intel-mpi-library">Intel MPI Library</a> (~ 300MB).  If desired, you can use other implementations of MPI, for example, <a href="http://www.mpich.org/">MPICH</a> .  The latest available version of the Intel MPI Library at the time of writing of article 5.0.3.048, and we will take it for experiments. <br><br>  Install the Intel MPI Library, following the instructions of the built-in installer (you may need superuser privileges). <br><blockquote>  $ tar xvfz l_mpi_p_5.0.3.048.tgz <br>  $ cd l_mpi_p_5.0.3.048 <br>  $ ./install.sh <br></blockquote><br>  Perform an installation on each of the hosts with the same installation path on both nodes.  A more standard way to deploy MPI is to install in the network storage available on each of the working nodes, but the description of setting up such storage is beyond the scope of the article, therefore we restrict ourselves to a simpler option. <br><br>  To compile the demo MPI program, we use the GNU C compiler (gcc). <br>  In the standard set of RHEL programs, the image from Amazon does not exist; therefore, you need to install it: <br><blockquote>  $ sudo yum install gcc <br></blockquote><br>  As a demo MPI program, take test.c from the standard set of Intel MPI Library examples (located in the intel / impi / 5.0.3.048 / test folder). <br>  To compile it, the first step is setting up the Intel MPI Library environment: <br><blockquote>  $.  /home/ec2-user/intel/impi/5.0.3.048/intel64/bin/mpivars.sh <br></blockquote><br>  Next, we compile our test program using a script from the Intel MPI Library (all necessary MPI dependencies will be set automatically when compiled): <br><blockquote>  $ cd /home/ec2-user/intel/impi/5.0.3.048/test <br>  $ mpicc -o test.exe ./test.c <br></blockquote><br>  The resulting test.exe is copied to the second node: <br><blockquote>  $ scp test.exe ip-172-31-47-24: /home/ec2-user/intel/impi/5.0.3.048/test/ <br></blockquote><br>  Before starting the MPI program, it will be useful to make a test run of some standard Linux utility, for example, 'hostname': <br><blockquote>  $ mpirun -ppn 1 -n 2 -hosts ip-172-31-47-25, ip-172-31-47-24 hostname <br>  ip-172-31-47-25 <br>  ip-172-31-47-24 <br></blockquote><br>  The 'mpirun' utility is a program from the Intel MPI Library designed for running MPI applications.  This is a kind of "runner".  It is this program that is responsible for running an instance of an MPI program on each of the nodes listed in its arguments. <br><br>  Regarding options, '-ppn' is the number of processes launched per node, '-n' is the total number of processes started, '-hosts' is the list of nodes where the specified application will be launched, the last argument is the path to the executable file (this can be and an application without MPI). <br><br>  In our example with the launch of the hostname utility, we should get its output (the name of the compute node) from both virtual servers, then it can be argued that the MPI process manager is working correctly. <br><br><h1>  "Hello World" using MPI </h1><br>  As a demo MPI application, we took test.c from the standard set of Intel MPI Library examples. <br><br>  The MPI demo application collects from each of the parallel MPI processes running some information about the process and the computing node on which it is running, and prints this information on the head node. <br><br>  Let us consider in more detail the main components of a typical MPI program. <br><br><pre><code class="hljs cpp"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">include</span></span></span><span class="hljs-meta"> </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">"mpi.h"</span></span></span></span></code> </pre>  Includes the mpi.h header file, which contains declarations of the main MPI functions and constants. <br>  If we use special scripts from the Intel MPI Library (mpicc, mpiicc, etc.) to compile our application, the path to mpi.h is automatically entered.  Otherwise, the path to the include folder will have to be set at compilation. <br><br><pre> <code class="hljs lisp">MPI_Init (<span class="hljs-name"><span class="hljs-name">&amp;argc</span></span>, <span class="hljs-symbol"><span class="hljs-symbol">&amp;argv</span></span>)<span class="hljs-comment"><span class="hljs-comment">; ... MPI_Finalize ();</span></span></code> </pre>  The call MPI_Init () is necessary to initialize the MPI program execution environment.  After this call, you can use the remaining MPI functions. <br>  The final call to the MPI program is MPI_Finalize ().  Upon successful completion of the MPI program, each of the running MPI processes makes a call to MPI_Finalize (), in which the internal MPI resources are cleaned.  Calling any MPI function after MPI_Finalize () is invalid. <br><br>  To describe the remaining parts of our MPI program, it is necessary to consider the basic terms used in MPI programming. <br><br>  An MPI program is a set of processes that can send messages to each other through various MPI functions.  Each process has a special identifier - rank.  The rank of the process can be used in various operations for sending MPI messages, for example, the rank can be specified as an identifier of the message recipient. <br><br>  In addition, there are special objects in MPI, called communicators, which describe process groups.  Each process within a single communicator has a unique rank.  The same process may relate to different communicators and, accordingly, may have different ranks within different communicators.  Each data transfer operation in MPI must be performed within the framework of some kind of communicator.  By default, the MPI_COMM_WORLD communicator is always created, which includes all existing processes. <br><br>  Let's go back to test.c: <br><br><pre> <code class="hljs objectivec"><span class="hljs-built_in"><span class="hljs-built_in">MPI_Comm_size</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;size); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Comm_rank</span></span> (<span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;rank);</code> </pre>  MPI_Comm_size () calls the variable in the size (size) of the current MPI_COMM_WORLD communicator (the total number of processes that we specified with the mpirun option '-n'). <br>  MPI_Comm_rank () writes to the rank variable of the current MPI process as part of the MPI_COMM_WORLD communicator. <br><br><pre> <code class="hljs lisp">MPI_Get_processor_name (<span class="hljs-name"><span class="hljs-name">name</span></span>, <span class="hljs-symbol"><span class="hljs-symbol">&amp;namelen</span></span>)<span class="hljs-comment"><span class="hljs-comment">;</span></span></code> </pre>  Calling MPI_Get_processor_name () will write in the name variable a string identifier (name) of the computation node on which the corresponding process was started. <br><br>  The collected information (process rank, MPI_COMM_WORLD dimension, processor name) is then sent from all non-zero ranks to zero using the MPI_Send () function: <br><pre> <code class="hljs objectivec"><span class="hljs-built_in"><span class="hljs-built_in">MPI_Send</span></span> (&amp;rank, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Send</span></span> (&amp;size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Send</span></span> (&amp;namelen, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Send</span></span> (name, namelen + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_CHAR</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>);</code> </pre> <br>  The MPI_Send () function has the following format: <br><blockquote>  MPI_Send (buf, count, type, dest, tag, comm) <br>  buf is the address of the memory buffer in which the data being sent are located; <br>  count - the number of data elements in the message; <br>  type - the type of the data elements of the message being sent; <br>  dest - the rank of the recipient process of the message; <br>  tag is a special tag for identifying messages; <br>  comm is the communicator in which the message is being sent. <br></blockquote>  A more detailed description of the MPI_Send () function and its arguments, as well as other MPI functions can be found in the <a href="http://www.mpi-forum.org/docs/mpi-3.0/mpi30-report.pdf">MPI standard</a> (the language of the documentation is English). <br><br>  At the zero rank, messages sent by other ranks are accepted and printed on the screen: <br><pre> <code class="hljs objectivec">printf (<span class="hljs-string"><span class="hljs-string">"Hello world: rank %d of %d running on %s\n"</span></span>, rank, size, name); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (i = <span class="hljs-number"><span class="hljs-number">1</span></span>; i &lt; size; i++) { <span class="hljs-built_in"><span class="hljs-built_in">MPI_Recv</span></span> (&amp;rank, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, i, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;stat); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Recv</span></span> (&amp;size, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, i, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;stat); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Recv</span></span> (&amp;namelen, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_INT</span></span>, i, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;stat); <span class="hljs-built_in"><span class="hljs-built_in">MPI_Recv</span></span> (name, namelen + <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_CHAR</span></span>, i, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-built_in"><span class="hljs-built_in">MPI_COMM_WORLD</span></span>, &amp;stat); printf (<span class="hljs-string"><span class="hljs-string">"Hello world: rank %d of %d running on %s\n"</span></span>, rank, size, name); }</code> </pre>  For clarity, the zero rank additionally prints its data like the ones it took from remote ranks. <br><br>  The MPI_Recv () function has the following format: <br><blockquote>  MPI_Recv (buf, count, type, source, tag, comm, status) <br>  buf, count, type - memory buffer for receiving the message; <br>  source - the rank of the process from which the message should be received; <br>  tag - the tag of the received message; <br>  comm is the communicator in which data is received; <br>  status is a pointer to a special MPI data structure that contains information about the result of the data receiving operation. <br></blockquote><br>  In this article we will not delve into the subtleties of the functions MPI_Send () / MPI_Recv ().  A description of the various types of MPI operations and the subtleties of their work is the topic of a separate article.  We only note that the zero rank in our program will receive messages from other processes strictly in a certain sequence, starting with the first rank and incrementally (this is determined by the source field in the MPI_Recv () function, which varies from 1 to size). <br><br>  The MPI_Send () / MPI_Recv () functions described are examples of the so-called two-point (point-to-point) MPI operations.  In such operations, one rank communicates with another within a specific communicator.  There are also collective (collective) MPI operations in which more than two ranks can participate in data exchange.  Collective MPI operations are a topic for a separate (and, possibly, not one) article. <br><br>  As a result of our demo MPI program, we get: <br><blockquote>  $ mpirun -ppn 1 -n 2 -hosts ip-172-31-47-25, ip-172-31-47-24 /home/ec2-user/intel/impi/5.0.3.048/test/test.exe <br>  Hello world: rank 0 of 2 running on ip-172-31-47-25 <br>  Hello world: rank 1 of 2 running on ip-172-31-47-24 <br></blockquote><br><br>  Are you interested in this post and would like to take part in the development of MPI technology?  The Intel MPI Library development team (Nizhny Novgorod) is currently actively looking for fellow engineers.  Additional information can be found on the <a href="http://www.intel.ru/jobs">official website of Intel</a> and on the <a href="http://brainstorage.me/jobs/27551">BrainStorage</a> website. <br><br>  And finally, a small survey about possible topics for future publications on high-performance computing. </div><p>Source: <a href="https://habr.com/ru/post/251357/">https://habr.com/ru/post/251357/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251347/index.html">Context Model Pattern via Aero Framework</a></li>
<li><a href="../251349/index.html">Processing logs based on previous messages in logstash / elasticsearch</a></li>
<li><a href="../251351/index.html">CoinFest-2015-Moscow</a></li>
<li><a href="../251353/index.html">We write a bot for MMORPG with assembler and draenei. Part 3</a></li>
<li><a href="../251355/index.html">Several subtleties of using jade in Meteor-projects</a></li>
<li><a href="../251359/index.html">Complete energy autonomy or how to survive with solar panels in the outback (part 1. theoretical)</a></li>
<li><a href="../251361/index.html">MVC and Model 2. Component Knowledge and Responsibilities</a></li>
<li><a href="../251363/index.html">Modifying HTTP traffic using FiddlerScript and .NET plug-ins to Fiddler</a></li>
<li><a href="../251365/index.html">Top 10 Mistakes That Newbies Make to Java</a></li>
<li><a href="../251369/index.html">News from the latest builds of Opera Developer 29</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>