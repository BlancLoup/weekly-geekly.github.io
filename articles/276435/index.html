<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommendations for Designing the User Interface of RealSense Applications</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Intel¬Æ RealSense ‚Ñ¢ technology supports two types of camera depth: a front view camera, a short-range (F200) is designed for installation on laptops, u...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommendations for Designing the User Interface of RealSense Applications</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/cb4/5a3/8dc/cb45a38dc92341a4ae0958cba4672cae.jpeg"><br>  Intel¬Æ RealSense ‚Ñ¢ technology supports two types of camera depth: a front view camera, a short-range (F200) is designed for installation on laptops, ultrabooks, transformers and all-in-one computers;  rear-view camera, long-range (R200) ‚Äã‚Äãis designed for installation on tablets and as a separate removable device.  Both cameras are available as stand-alone peripherals and are embedded in computer devices currently available on the market.  When using Intel RealSense technology to develop applications for such devices, it should be remembered that the principle of interaction with three-dimensional applications without tactile feedback differs significantly from the model of work to which developers are accustomed to creating applications for touch control. <br>  In this article, we describe some of the common principles and problems of user interfaces for the F200 and R200 cameras and show how you can build visual feedback into your applications using the Intel¬Æ RealSense ‚Ñ¢ SDK API. <br><a name="habracut"></a><br><h1>  <font color="#0071c5">Guidelines for creating user interfaces and using the API for the F200 camera</font> </h1><br><h2>  <font color="#0071c5">Result 1. Understanding the volumetric shooting space and interaction areas for laptops and all-in-one computers</font> </h2><br><h4>  UI usage scenario </h4><br>  Consider the use cases shown in Fig.  one. <br><br><img src="https://habrastorage.org/files/b3c/0b9/2ef/b3c0b92efc3840ab9fd73021fa27c4e2.png"><br>  <i>Figure 1. Surround space</i> <br><br>  The pyramid emanating from the camera in this figure is what is called the volumetric shooting space or field of view of the camera.  For the F200, the volumetric shooting space is determined by the deviations from the horizontal and vertical axes of the camera, as well as the effective distance between the user and the camera.  If the user moves beyond this pyramid, the camera will not be able to track the interaction mode.  Below for reference is a table with parameters of the field of view. <br><table><tbody><tr><th>  Parameter </th><th>  Range </th></tr><tr><td>  Effective Gesture Recognition Range </td><td>  0.2‚Äì0.6 m </td></tr><tr><td>  Effective face recognition range </td><td>  0.35‚Äì1.2 m </td></tr><tr><td>  Field of view of a color image camera, degrees </td><td>  77 x 43 x 70 (cone) </td></tr><tr><td>  Infrared (IR) camera field of view, degrees </td><td>  90 x 59 x 73 (cone) <br>  IR illuminator field of view = n / d x 56 x 72 (pyramid) </td></tr><tr><td>  Color Image Resolution </td><td>  Up to 1080p at a frame rate of 30 frames per second (fps) </td></tr><tr><td>  Depth Map Resolution </td><td>  Up to 640 x 480 at 60 fps </td></tr></tbody></table><br>  The color imaging camera and the depth camera in the F200 device have different resolutions, so developers should consider the volumetric shooting space for the intended modes of operation with the application.  As shown in the table above, the effective range of gesture recognition is small, whereas face tracking works at a greater distance. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Why is this important in terms of user interface?  End users have no idea how the camera ‚Äúsees‚Äù them.  Since they know about the interaction zones, this can lead to irritation when working with the application, since it is impossible to determine exactly what the problem arose.  The image on the left in Fig.  1 user‚Äôs hand is in the camera‚Äôs field of view, and the image on the right is out of sight;  in this case, tracking may be lost.  The problem is further complicated if the application uses control using both hands or several control modes at once, for example, using face and hands simultaneously.  Also consider changing the camera's field of view when you deploy the application on devices of different sizes, such as laptops and all-in-one computers: in the latter case, the interaction zone will be located higher than on laptops.  In fig.  2 shows the different scenarios in which users are in front of different devices. <br><br><img src="https://habrastorage.org/files/2bc/bc4/4db/2bcbc44dbdb240b38cee0a36e058fb03.png"><br>  <i>Figure 2. Camera field of view and device size</i> <br><br>  Information about these parameters will help build an effective feedback mechanism into the application to provide users with clear instructions on how to use the device and the camera correctly.  Now let's see how to get some of these parameters of the visual field in the application via the SDK. <br><br><h4>  Technical implementation </h4><br>  The Intel RealSense SDK provides an API for capturing camera field of view and range data.  The <i>QueryColorFieldOfView</i> and <i>QueryDepthFieldOfView APIs</i> work in the device interface regardless of the device type.  This is how the code is implemented. <br><br><img src="https://habrastorage.org/files/1f0/ab1/f27/1f0ab1f27cca41809ae2152fc5d56afb.png"><br><br>  Although the returned data structure has the format PXCPointF32, the returned values ‚Äã‚Äãindicate the angles of X (horizontal view) and Y (vertical view) in degrees.  These are the manufacturer-specified values ‚Äã‚Äãfor this camera model, and not software-configured on the device. <br>  The next parameter of the volumetric shooting space is the distance.  The QueryDepthSensorRange API returns the range value in millimeters.  This value is also set by the manufacturer by default for this model, and is not programmed on a specific device. <br>  Knowledge of these APIs and how to implement them in code will help create an effective feedback system for users.  In fig.  3 and 4 show examples of visual feedback for volumetric shooting. <br><br><img src="https://habrastorage.org/files/6b0/986/581/6b0986581649447a858c4311be3cc2bc.png"><br>  <i>Figure 3. Camera distance tips</i> <br><br><img src="https://habrastorage.org/files/418/168/629/4181686296d54dcaaecf998afab7fdf8.png"><br>  <i>Figure 4. Schematic representation of the surrounding world</i> <br><br>  Simple hints indicate the near and far borders of the interaction zone.  Without prompts, the user simply will not understand what needs to be done if the system stops responding to his actions.  Filter distance data and show a prompt after a short delay.  Also, use hints and tips instead of error notifications.  A schematic depiction of the surrounding world will help users navigate and become familiar with the concepts of the depth camera interaction zone. <br><br>  It is recommended to use such schematic images on the help screens and in educational screensavers, as well as in games, the users of which can work with the camera for the first time.  For maximum efficiency, you should show a schematic depiction of the surrounding world only when educating users and on help screens.  Instructions should be simple and comprehensible, in their preparation it is necessary to focus on the intended audience of the application. <br><br>  Instead of the APIs listed above, you can use the alerts provided in each SDK to record specific user actions.  Consider, for example, the following solution for facial recognition.  The following table lists the <i>PXC [M] FaceData</i> module alerts. <br><br>  As you already know, the SDK supports detection of up to 4 people in sight.  Using the face ID, you can receive alerts related to each person, depending on the needs of the application.  Tracking can also be completely lost (for example, if the face has moved into the camera‚Äôs field of view, and then out of sight at a speed that is too high to track).  In such a scenario, you can use the data from the survey shooting space along with alerts to create a reliable feedback mechanism for users. <br><br><table><tbody><tr><th>  Alert Type </th><th>  Description </th></tr><tr><td>  ALERT_NEW_FACE_DETECTED </td><td>  New face detected. </td></tr><tr><td>  ALERT_FACE_NOT_DETECTED </td><td>  There is no face in the scene. </td></tr><tr><td>  ALERT_FACE_OUT_OF_FOV </td><td>  The face is out of sight of the camera. </td></tr><tr><td>  ALERT_FACE_BACK_TO_FOV </td><td>  The face is back in sight of the camera. </td></tr><tr><td>  ALERT_FACE_LOST </td><td>  Lost face tracking. </td></tr></tbody></table><br>  The SDK also allows you to detect an overlay, i.e. cases where the object to be removed is blocked by a foreign object.  For a description of unsupported and partially supported scenarios, see the F200 Camera User Interface Design Guide.  No matter what type of overlay you are trying to track, the next set of alerts will be very useful. <br><br><table><tbody><tr><th>  Alert Type </th><th>  Description </th></tr><tr><td>  ALERT_FACE_OCCLUDED </td><td>  The face is blocked. </td></tr><tr><td>  ALERT_FACE_NO_LONGER_OCCLUDED </td><td>  The face is no longer blocked. </td></tr><tr><td>  ALERT_FACE_ATTACHED_OBJECT </td><td>  A person is blocked by an object, such as a hand. </td></tr><tr><td>  ALERT_FACE_OBJECT_NO_LONGER_ATTACHED </td><td>  The face is no longer blocked by any object. </td></tr></tbody></table><br>  Now let's move on to alerts in the hand tracking module.  They are available in the PXC [M] HandData module of the SDK.  As you can see, some of these alerts also implicitly involve the determination of the range (remember about the different range of action of the face recognition modules and hand recognition modules). <br><table><tbody><tr><th width="300">  Alert Name </th><th>  Description </th></tr><tr><td>  ALERT_HAND_OUT_OF_BORDERS </td><td>  The monitored hand is outside the two-dimensional bounding box or the three-dimensional bounding cube specified by the user. </td></tr><tr><td>  ALERT_HAND_INSIDE_BORDERS </td><td>  The tracked hand has returned to the inside of the two-dimensional bounding box or the three-dimensional bounding cube specified by the user. </td></tr><tr><td>  ALERT_HAND_TOO_FAR </td><td>  Tracked arm is too far from the camera. </td></tr><tr><td>  ALERT_HAND_TOO_CLOSE </td><td>  The tracked arm is too close to the camera. </td></tr><tr><td>  ALERT_HAND_DETECTED </td><td>  The monitored hand is recognized, its mark is available. </td></tr><tr><td>  ALERT_HAND_NOTE_DETECTED </td><td>  A previously detected arm is lost because it is either out of sight or blocked. </td></tr><tr><td>  And many others... </td><td>  See the documentation. </td></tr></tbody></table><br>  Now you know what features the SDK provides, and you can effortlessly apply them in the application code.  An example is shown in the following code snippet. <br><br><img src="https://habrastorage.org/files/2fe/cd5/552/2fecd55521e04b488a3f990aa07a60c2.png"><br><img src="https://habrastorage.org/files/b1b/58b/8c8/b1b58b8c8b06402c8ea7d6ea30b796d0.png"><br><br>  Replace wprintf_s instructions with visual feedback implementation logic.  You can include not all alerts, but only some of them, as shown below. <br><br><img src="https://habrastorage.org/files/b01/653/de8/b01653de87e54083a1f2f07e6a3ec05b.png"><br><br>  In fig.  5 and 6 show examples of effective visual feedback using alerts. <br><br><img src="https://habrastorage.org/files/c58/7d6/cad/c587d6cade5a473a9e38f9ea2f1316d4.png"><br>  <i>Figure 5. User image in camera view</i> <br><br><img src="https://habrastorage.org/files/abb/87b/090/abb87b0907394da3a2be64bd9198ec18.png"><br>  <i>Figure 6. Overlay user image</i> <br><br><h4>  API references in the SDK documentation </h4><br><ul><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fquerycolorfieldofview_device_pxccapture.html">QueryColorFieldOfView</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fquerydepthfieldofview_device_pxccapture.html">QueryDepthFieldOfView</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fquerydepthsensorrange_device_pxccapture.html">QueryDepthSensorRange</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Falerttype_alertdata_pxcfacedata.html">Alerts on the field of view of facial recognition module</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fmanuals_handling_alerts.html">Handshake module field of view alerts</a> </li></ul><br><h2>  <font color="#0071c5">Result 2. Reduced user fatigue.</font> </h2><br><h4>  User Interface Use Case: Select the appropriate input method for the required accuracy. </h4><br>  When creating applications using the Intel RealSense SDK, it is important to remember the features of the input modes.  The selection of suitable input modes for different scenarios plays a crucial role in the operation of the application.  Input using the keyboard, mouse and touch screen is highly accurate, while input using gestures has low accuracy.  For example, to work with applications that require a lot of work with data, it is preferable to use keyboard and mouse input, rather than gestures.  Try to imagine what it will try to select a specific cell in Excel with your finger instead of the mouse (see. Fig. 7).  Such actions will not cause anything but extreme irritation and fatigue of the user.  When trying to perform precise actions, users naturally tighten their muscles, which, in turn, leads to increased fatigue. <br><br><img src="https://habrastorage.org/files/c12/46c/5e8/c1246c5e84a147dc86431070d3916298.png"><br>  <i>Figure 7. Choosing the right input method</i> <br><br>  You can use the touch controls or the mouse to select items in the menu.  The input modes supported by the Intel RealSense SDK provide a direct, natural touch-free interaction mechanism and allow you to create exciting applications.  Use these modes in such a way that it does not require many repeated gestures.  For the use of gestures, permanent actions are best suited, in which mistakes will not lead to unwanted risk. <br><br><h4>  Selecting the direction of movement of gestures </h4><br>  It is recommended to use gestures directed horizontally or in an arc.  If a choice is available, then for the convenience of users, try to use horizontal motions instead of vertical motions.  In addition, do not use actions that force users to raise their arms above shoulder level.  Remember the effect of "gorilla hands"? <br><br><img src="https://habrastorage.org/files/5ce/cb6/3fa/5cecb63fad7846c79c950872b909babc.png"><br>  <i>Figure 8. Selecting the direction of movement of gestures</i> <br><br><h4>  The choice of relative or absolute movement </h4><br>  Allow relative movement instead of absolute movement in all expedient cases.  With relative movement, the user can ‚Äúreset‚Äù the arrangement of the virtual hand on the screen in order to achieve a more comfortable position of his own hand in front of the camera.  This is about the same as raising the mouse and rearranging it from the edge of the pad to the middle if you need to move the pointer further.  In absolute motion, the relationship between the position of the pointer on the screen and the position of the hand on the screen is always preserved.  Applications should use a model of movement that is most appropriate for each specific context. <br><br><h4>  Understanding speed </h4><br>  Part of the accuracy problem is the speed factor.  If users move their hands too fast in front of the camera, then there is a risk of complete loss of tracking, since the hands may be out of the shooting volume space.  When used in applications gestures with fast movements increases user fatigue and increases the risk of errors.  Therefore, it is very important to take into account the speed factor and its effect both on the effective range (close to the camera, at a distance of 20 to 55 cm, fast movement can be detected at a speed of up to 2 m / s) and on the shooting space (with a short distance from the camera only one hand can be in sight). <br><br><h4>  Understanding user actions and interactions with objects </h4><br>  Natural human movements are not always smooth: the human body often moves unevenly and jerking, which is interpreted by the camera as several different interactions.  When creating applications for the Intel RealSense SDK, remember the relationship between actions and objects.  For example, if there are objects that you can ‚Äútake‚Äù with your hand using gestures, you should consider the size of such objects and their location, you need to take into account the distance to the edges of the screen and the place where you can ‚Äúdrag‚Äù such objects, as well as ways to detect tracking failures . <br><br>  Here are some recommendations to help overcome such problems. <br><ul><li>  Objects must be large enough so that they are not affected by a shiver or an uneven movement of the hand.  The distance between objects should be large enough so that users cannot accidentally take the wrong object. </li><li>  Do not place the elements of interaction too close to the edges of the screen, because in this case the risk of the user's hand leaving the field of view and loss of tracking increases, which will cause inevitable and righteous irritation to the user. </li><li>  If the interface is important dragging objects, it should be obvious exactly where you can drag a taken object and where it can be released. </li><li>  If a tracking object fails when the user moves the object, the object being moved must return to its original location, and the user should be notified of the tracking failure. </li></ul><br><h4>  Technical implementation: speed and accuracy </h4><br>  If the application does not require data on the joints of the hand, but more often uses fast hand movements, it makes sense to use the Blob module.  The following table lists the various possible scenarios and the estimated accuracy in each of them.  When tracking the entire arm with the data on the joints, the movement should be slower, but this limitation can be circumvented by using either the extremity tracking or the Blob mode.  In addition, thanks to the Blob mode, you will get a number of advantages if the application is intended for children. <br><table><tbody><tr><th width="100">  Tracking mode </th><th width="100">  Only hands? </th><th>  Output </th><th>  The load on computing resources </th><th>  Restrictions </th></tr><tr><td>  Full hand </td><td>  Yes </td><td>  Segmented image, limb points, lateral side of the arm, alerts, joint data, finger data, open or closed palm, gestures </td><td>  The highest multiple streams </td><td>  2 arms, 60 cm range, slow hand movement </td></tr><tr><td>  Extremities </td><td>  Yes </td><td>  Segmented image, limb points, side of arm, alert </td><td>  Medium, single thread </td><td>  2 hands, range 60 cm, average speed of the movement of hands </td></tr><tr><td>  Blob </td><td>  Not </td><td>  Segmented image, limb points, contour line </td><td>  Low single stream </td><td>  4 objects, range 100 cm, high speed </td></tr></tbody></table><br>  If the application requires more control and you need to control the speed, then you can get the speed data at the level of the arm joints using <i>PXCMHandConfiguration.EnableJointSpeed</i> .  This allows you to get either the absolute value of speed, calculated on the basis of the current and previous positions of the hands, or the average speed for a certain period of time.  However, with this approach, the load on the CPU and RAM significantly increases, so this method should be applied only when absolutely necessary. <br><br>  Since it is impossible to force users to move smoothly without jerking, the Smoother program (PXC [M] Smoother) is also included in the SDK, smoothing jerks when moving hands in front of the camera.  This program uses various linear and square algorithms.  You can experiment with them and choose the most suitable.  In fig.  9 below that the uneven movement of the arm is largely smoothed by this program. <br><br><img src="https://habrastorage.org/files/ad6/f1e/f49/ad6f1ef498314dcca6b4b26e3c925b4e.png"><br>  <i>Figure 9. Data with and without smoothing</i> <br><br>  Another way to detect too fast hand movement is by listing <i>TRACKINGSTATUS_HIGH_SPEED</i> in the <i>PXCMHandData.TrackingStatusType</i> property.  When a face is detected, fast movements can lead to loss of tracking.  Use <i>PXCMFaceData.AlertData.AlertType</i> - <i>ALERT_FACE_LOST</i> to determine lost tracking.  If you are using hand gestures to control the operating system with the Touchless Controller, use the <i>PXC [M] TouchlessController's</i> <i>SetPointerSensitivity</i> and <i>SetScrollSensitivity</i> <i>functions</i> to adjust pointer sensitivity and scrolling. <br><br><h4>  Bounding box </h4><br>  An effective mechanism to achieve smooth action and interaction with objects is the use of restrictive frameworks.  They provide users with a clear visual indication of the source and destination of the object with which the user interacts. <br><br>  The face and hand tracking modules in the SDK support the <i>PXCMHandData.IHand.QueryBoundingBoxImage</i> API, which returns the location and dimensions of the tracked hand (two-dimensional bounding box) on the depth map.  The <i>PXCMFaceData.DetectionData.QueryBoundingRect</i> API returns the bounding box of the detected face.  You can also use <i>PXCMHandData.AlertType</i> - <i>ALERT_HAND_OUT_OF_BORDERS</i> to detect the exit of a hand beyond the bounding box. <br><br><h4>  API references in the SDK documentation </h4><br><ul><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fmanuals_blob_tracking.html">Blob tracking algorithm</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fenablejointspeed_pxchandconfiguration.html">EnableJointSpeed</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fmanuals_the_smoother_utility.html">Smoother program</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fmember_functions_pxctouchlesscontroller.html">TouchlessController and SetScrollSensitivity</a> </li></ul><br><h1>  <font color="#0071c5">Recommendations for creating user interfaces and using the API for the R200 camera</font> </h1><br>  The R200 camera is built into the tablets and comes in the form of a removable device.  It is designed to capture the space around the user.  Among the possible scenarios for using the R200 camera, such solutions as augmented reality and shooting of the entire human body should be noted.  The surrounding world is in the field of view of this camera, therefore the nature and set of problems for designing user interfaces differ from those described above for the F200 camera.  This section describes some of the known user interface problems associated with the Scene Perception module (to be used by developers in augmented reality applications) and with the 3D Scanning module. <br><br><h2>  <font color="#0071c5">Result 1. Understanding the survey shooting space and interaction areas for tablets.</font> </h2><br><h4>  UI usage scenario </h4><br>  As seen in fig.  10, the viewing angles of the R200 camera both vertically and horizontally, as well as its range, are significantly different from those of the F200.  The R200 can be used in two different modes: in active mode (when the user moves while shooting a scene) and in passive mode (when the user is working with a still image).  When shooting an object or scene, make sure that the object is in the field of view of the camera while the user is shooting it in active mode.  Also note that the range of this camera (depending on whether it is used indoors or outdoors) differs from the range of the F200 camera.  How do I get these data points at run time to provide the user with visual feedback? <br><br><img src="https://habrastorage.org/files/8ee/e83/944/8eee839446004f238381fcab8a96c47a.png"><br>  <i>Figure 10. Surround space of the R200 camera</i> <br><br><h4>  Technical implementation </h4><br>  We have already discussed the <i>QueryColorFieldOfView ()</i> API and <i>QueryDepthFieldOfView ()</i> above in the section on the F200 camera.  These functions do not depend on the device, they can be used for volumetric shooting with the R200 camera.  However, to detect the range of the R200 camera, you need to use a specialized API designed only for this device.  To obtain such data for the R200 camera, you must use the <i>QueryDSMinMaxZ</i> API, available as part of the <i>PXCCapture</i> interface.  It returns the minimum and maximum range of the camera in millimeters. <br><br><h4>  API references in the SDK documentation </h4><br><ul><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fquerydsminmaxz_device_pxccapture.html">QueryDSMinMaxZ</a> </li></ul><br><h2>  <font color="#0071c5">Result 2. Understanding user actions and scene interaction.</font> </h2><br>  User Interface Scenario: Scheduling Considering the Scene and Camera Features <br>  When working with the camera in active mode, be aware of the limitations of the camera.  These depths will be less accurate when shooting a scene with very bright areas, with reflective and with black surfaces.  Information about when the failure of tracking is possible will help build an effective feedback mechanism into the application to gently remind the user of the necessary actions, rather than shutting down the work with an error. <br><br><h4>  Technical implementation </h4><br>  Scene Perception and 3D Scanning modules have different requirements, and therefore they use different mechanisms to detect minimum requirements. <br><br>  <b>Scene Perception</b> .  Always use the <i>CheckSceneQuality</i> API in the <i>PXCScenePerception</i> module to determine if the scene is suitable for tracking.  The API returns a value between 0 and 1. The higher the return value, the better the scene to track.  This is how the code is implemented. <br><br><img src="https://habrastorage.org/files/8e6/f0a/719/8e6f0a7198dd4570bccdeaaee10c47dd.png"><br><br>  After the scene quality is deemed satisfactory and tracking begins, you should dynamically check the tracking status using the <i>TrackingAccuracy</i> API in the <i>PXCScenePerception</i> module.  This API provides tracking accuracy. <br><table><tbody><tr><th>  Name </th><th>  Description </th></tr><tr><td>  HIGH </td><td>  High tracking accuracy </td></tr><tr><td>  LOW </td><td>  Low tracking accuracy </td></tr><tr><td>  MED </td><td>  Average tracking accuracy </td></tr><tr><td>  FAILED </td><td>  Tracking failed </td></tr></tbody></table><br>  To maximize the quality of the scene data, you can also adjust the voxel resolution (voxel is a unit of resolution for a 3D image).  Depending on what the camera is tracking (space-sized room, table surface, or close object), adjust the resolution of the voxels according to the table below for best results. <br><table><tbody><tr><th width="200">  Name </th><th>  Description </th></tr><tr><td>  LOW_RESOLUTION </td><td>  Low voxel resolution.  Use this resolution to track room size (4/256 m). </td></tr><tr><td>  MED_RESOLUTION </td><td>  The average resolution of voxels.  Use this resolution to track the tabletop (2/256 m). </td></tr><tr><td>  HIGH_RESOLUTION </td><td>  High resolution voxels.  Use this permission to track small objects (1/256 m). </td></tr></tbody></table><br>  <b>3D Scanning</b> The <b>3D Scanning</b> algorithm provides the alerts shown in the table below.  To obtain this data use <i>PXC3DScan :: AlertEvent</i> . <br><table><tbody><tr><th width="200">  Name </th><th>  Description </th></tr><tr><td>  ALERT_IN_RANGE </td><td>  The subject is at a suitable distance. </td></tr><tr><td>  ALERT_TOO_CLOSE </td><td>  The subject is too close to the camera.  Ask the user to move the object away from the camera. </td></tr><tr><td>  ALERT_TOO_FAR </td><td>  The subject is too far away from the camera.  Have the user move the object toward the camera. </td></tr><tr><td>  ALERT_TRACKING </td><td>  The subject is being tracked correctly. </td></tr><tr><td>  ALERT_LOST_TRACKING </td><td>  The tracking of the object is lost. </td></tr></tbody></table><br>  If the application has available camera tracking data and the limitations of the module used, this data can be used to provide visual feedback, clearly telling users how their actions were interpreted by the camera.  In case of loss of tracking, you can show how to work with the camera more correctly.  Examples of visual feedback are shown here for example only, they must be adapted to the requirements of the application and with the user interface device. <br><br>  Sample curriculum at startup. <br><br><img src="https://habrastorage.org/files/087/c5c/555/087c5c55502d4c378d6549d6b61da4fb.png"><br>  <i>Figure 11. Training</i> <br><br>  Preview of the captured area or subject. <br><br><img src="https://habrastorage.org/files/2ab/e6f/7ab/2abe6f7ab1b84fd89e61f337fbeda5c0.png"><br>  <i>Figure 12. Preview</i> <br><br>  Hints for the user. <br><br><img src="https://habrastorage.org/files/1a3/9e2/8f3/1a39e28f30f74b518af5c5734ae9f553.png"><br>  <i>Figure 13. User Tips</i> <br><br><h4>  Reduced fatigue when the user holds the device in their hands </h4><br>  Most applications will use the device with both active and inactive camera modes.  (These two modes differ as follows: the camera works in active mode when the user holds the tablet in hand to view the scene through the camera or to shoot; the camera works in the inactive mode when the user laid down the tablet and works with the contents on the screen, while the camera is turned off).  To reduce user fatigue, it is necessary to understand how the user holds and uses the device in each of these modes, and to select the interaction zones accordingly.  When using the camera in the active mode, the user gets tired faster, because he keeps the device on weight, as shown in fig.  14. <br><br><img src="https://habrastorage.org/files/861/6d6/3d1/8616d63d1c034f728eecb5541b2cef26.png"><br>  <i>Figure 14. Use of the device in active and inactive modes</i> <br><br><h4>  Choosing the right mode for action </h4><br>  The usage mode also directly determines the nature of interaction with the application through the user interface.  In active mode, the user holds the device with both hands.  Therefore, any visual elements of the application, such as buttons, should be located in easily accessible places on the screen.  Studies show that in such cases it is best to use the edges of the screen.  Recommended touch zones are shown in Fig.  15. In addition, the touch accuracy is reduced in the asset mode, so the active mode is best suited for short-term shooting. <br><br>  On the contrary, in inactive mode it is more convenient for the user to work with the device, the user interacts with the interface elements more precisely and can use the application for a long time. <br><br><img src="https://habrastorage.org/files/545/88e/828/54588e828c8a47b683a4c5b9839b673f.png"><br>  <i>Figure 15. Touch zones in active and inactive modes</i> <br><br><h4>  API references in the SDK documentation </h4><br><ul><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Fmanuals_configuration_and_tra2.html">Configuring Scene Perception API and Tracking Information</a> </li><li>  <a href="https://software.intel.com/sites/landingpage/realsense/camera-sdk/v1.1/documentation/html/index.html%3Falertevent_pxc3dscan.html">API 3D Scanning Alerts</a> </li></ul><br><h1>  <font color="#0071c5">Conclusion</font> </h1><br>  When developing applications using Intel¬Æ RealSense ‚Ñ¢ technology, developers should take into account the needs and features of the end users from the earliest stages.  The recommendations provided in this article will serve as the basis for solving some important problems of user interfaces and for implementing the necessary components in code using the SDK. <br><br><h1>  <font color="#0071c5">Additional materials</font> </h1><br><ul><li>  <a href="https://software.intel.com/sites/default/files/managed/27/50/Intel%2520RealSense%2520SDK%2520Design%2520Guidelines%2520F200%2520v2.pdf">Recommendations for creating user interfaces for the F200 camera</a> </li><li>  <a href="https://software.intel.com/sites/default/files/managed/d5/41/Intel%2520RealSense%2520SDK%2520Design%2520Guidelines%2520R200%2520v1_1.pdf">Recommendations for creating user interfaces for the R200 camera</a> </li><li>  <a href="https://software.intel.com/en-us/articles/best-ux-practices-for-intel-realsense-camera-f200-applications">Best practices for creating user interfaces in applications for the F200 camera</a> </li><li>  <a href="http://myeventagenda.com/sessions/0B9F4191-1C29-408A-8B61-65D7520025A8/7/5">Link to the presentation and record performance at the IDF conference</a> </li></ul></div><p>Source: <a href="https://habr.com/ru/post/276435/">https://habr.com/ru/post/276435/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../276425/index.html">Analysis of hosters and their tariffs for virtual servers</a></li>
<li><a href="../276427/index.html">‚ÄúWhy haven't artificial intelligence been invented yet?‚Äù Or testing CNTK tools from Microsoft Research</a></li>
<li><a href="../276429/index.html">Work with HealthKit. Part 2</a></li>
<li><a href="../276431/index.html">Creating an Android application. Personalization</a></li>
<li><a href="../276433/index.html">A new version of Veeam Backup FREE Edition has been released: a brief overview and useful information about NFR-keys to the full version</a></li>
<li><a href="../276441/index.html">PyNSK # 6 - the sixth meeting of the Novosibirsk Python community</a></li>
<li><a href="../276443/index.html">Meaningful use of console applications in C #</a></li>
<li><a href="../276445/index.html">RainyJs - as Angular, only for Ajax</a></li>
<li><a href="../276447/index.html">As I wrote the game for 3 years</a></li>
<li><a href="../276449/index.html">A small comparison of online translators</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>