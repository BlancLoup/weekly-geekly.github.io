<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How to move from ESXi to KVM / LXD and not go crazy</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Maxnet System has been using the free version of VMware - ESXi since version 5.0 for a long time as a hypervisor. The paid version of vSphere frighten...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How to move from ESXi to KVM / LXD and not go crazy</h1><div class="post__text post__text-html js-mediator-article">  Maxnet System has been using the free version of VMware - ESXi since version 5.0 for a long time as a hypervisor.  The paid version of vSphere frightened the licensing model, while the free version had a number of flaws that were not available in the paid version, but you could put up with them.  But when in new versions of ESXi the new web interface refused to work with the old one, and monitoring of the RAID-arrays stopped showing signs of life, the company decided to look for a more universal and open solution.  The company already had a good experience and a pleasant impression of the LXC - Linux Containers.  Therefore, it became obvious that the dream hypervisor will be hybrid and combine for different loads KVM and LXD - an evolutionary continuation of LXC.  In search of information on KVM, the company faced misconceptions, rakes and harmful practices, but tests and time put everything in its place. <br><br><img src="https://habrastorage.org/webt/s-/vc/6s/s-vc6shq5cfia5bjjcuhixbgukq.jpeg"><br><br>  About how to cope with the move from ESXi to KVM and not pierce the wheel on the rake, will tell <strong>Lev Nikolaev</strong> ( <a href="https://habr.com/ru/users/maniaque/" class="user_link">maniaque</a> ) - the administrator and developer of highly loaded systems, an information technology trainer.  Let's talk about the Network, storage, containers, KVM, LXD, LXC, provisioning and convenient virtual machines. <br><a name="habracut"></a><br><h2>  Prologue </h2><br>  Immediately we denote the key thoughts, and then analyze them in more detail. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      <strong>Network.</strong>  As long as the speeds of your interfaces do not exceed 1 Gbps, you will have enough bridge.  As soon as you want to squeeze more - it will limit you. <br><br>  <strong>Storage.</strong>  Create shared network storage.  Even if you are not ready to use 10 Gb / s inside the network, even 1 Gb / s will give you 125 MB / s of storage.  For a number of loads this will be enough with a margin, and the migration of virtual machines will be an elementary matter. <br><br>  <strong>Container or KVM?</strong>  Pros, cons, pitfalls.  What kind of load is better to put in a container, and which ones should be left in KVM? <br><br>  <strong>LXD or LXC</strong> .  Is LXD LXC?  Or another version?  Or a superstructure?  What is it all about?  Let's dispel the myths and understand the differences between LXD and LXC. <br><br>  <strong>Convenient provisioning</strong> .  Which is more convenient: take the same image or install the system from scratch every time?  How to do it quickly and accurately every time? <br><br>  <strong>Convenient virtual machine.</strong>  There will be scary stories about loaders, sections, LVM. <br><br>  <strong>Miscellaneous</strong> .  Many small questions: how to quickly drag a virtual machine from ESXi to KVM, how to migrate well, how to properly virtualize disks? <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/HqsxBkxGxqg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br><h2>  Reason for moving </h2><br>  Where did we get the crazy idea of ‚Äã‚Äãmoving from ESXi to KVM / LXD?  ESXi is popular among small and medium businesses.  This is a good and cheap hypervisor.  But there are nuances. <br><br>  We started with version 5.0 - it is convenient, everything works!  The next version is 5.5 too. <br><br>  Since version 6.0 - is already more difficult.  On the ESXi, the Web-interface did not immediately become free, only from version 6.5, before it needed a utility under Windows.  We put up with it.  Who runs OS X buys Parallels and installs this utility.  This is a well-known pain. <br><br>  Periodically flew monitoring.  It was necessary to restart the management services in the server console - then CIM Heartbeat reappeared.  We suffered, as it did not always fall off. <br><br>  Version ESXi 6.5 - trash, waste and atrocities.  Horrible hypervisor.  And that's why. <br><br><ul><li>  <strong>Angular falls out with an exception still at the entrance to the Web interface.</strong>  As soon as you enter your username and password - immediately an exception! </li><li>  <strong>The ability to remotely monitor the status of a RAID array</strong> as it is convenient for us does not work.  It used to be convenient, but in version 6.5 everything is bad. </li><li>  <strong>Weak support for modern network cards from Intel</strong> .  Intel and ESXi network cards cause pain.  The ESXi support forum has a thread of agony about this.  VMware and Intel are not friendly, and relations will not improve in the near future.  The sad thing is that even paid solutions customers experience problems. </li><li>  <strong>No migration within ESXi</strong> .  Unless you consider the migration procedure with pause, copying and running.  We put the car on pause, quickly copy it and run it in another place.  But it is impossible to call it migration - after all there is a simple one. </li></ul><br>  After looking at it all, we got the crazy idea of ‚Äã‚Äãmoving from ESXi 6.5. <br><br><h2>  the wish list </h2><br>  To begin with, we wrote a wish list for the ideal future we are leaving for. <br><br>  <strong>Management from under SSH</strong> , and Web and other optional.  The web interface is great, but when you are on a business trip with an iPhone, it‚Äôs inconvenient and difficult to enter the ESXi web interface and do something there.  Therefore, the only way to manage everything is SSH, there will be no other. <br><br>  <strong>Windows Virtualization.</strong>  Sometimes clients ask for strange things, and our mission is to help them. <br><br>  <strong>Always fresh drivers and the ability to customize the network card</strong> .  Adequate desire, but unrealized under pure ESXi. <br><br>  <strong>Live migration, not clustering</strong> .  We want the ability to drag machines from one hypervisor to another without feeling any delays, downtime and inconvenience. <br><br>  The wish list is ready, then the heavy search began. <br><br><h2>  Flour choice </h2><br>  The market revolves around KVM or LXC under different sauces.  Sometimes it seems that Kubernetes is somewhere on top, where everything is fine, the sun and paradise, and at the lower level are Morlocks - KVM, Xen or something like that ... <br><br>  For example, Proxmox VE is Debian, which has a kernel pulled from Ubuntu.  It looks weird, but does it bring in production? <br><br>  Our neighbors downstairs are alt linux.  They came up with a beautiful solution: assembled Proxmox VE as a package.  They just put the package in one team.  This is convenient, but we do not roll Alt Linux in production, so we did not fit. <br><br><h3>  Take KVM </h3><br>  In the end, we chose KVM.  Not taken, Xen, for example, because of the community - it is much more from KVM.  It seemed that we would always find the answer to our question.  We later found out that the size of a community does not affect its quality. <br><br>  Initially, we expected that we would take Bare Metal machine, add Ubuntu, with which we work, and roll KVM / LXD from above.  We were counting on the ability to run containers.  Ubuntu is a well-known system and there are no surprises in terms of solving boot / restore problems for us.  We know where to kick if the hypervisor does not start.  Everything is clear and convenient for us. <br><br><h2>  KVM Crash Course </h2><br>  If you are from the world of ESXi, then you will find a lot of interesting things.  Learn three words: QEMU, KVM and libvirt. <br><br>  <strong>QEMU</strong> translates the desires of a virtualized OS into calls to the normal process.  Works great almost everywhere, but slowly.  QEMU itself is a separate product that virtualizes a bunch of other devices. <br><br>  Next comes the <strong>QEMU-KVM</strong> bundle.  This is the Linux kernel module for QEMU.  It is expensive to virtualize all instructions, so we have a KVM kernel module that <strong>translates only a few instructions</strong> .  As a result, it is significantly faster, because only a few percent of the instructions from the total set are processed.  This is all the costs of virtualization. <br><br>  If you just have QEMU, starting a virtual machine without a binding looks like this: <br><br><pre><code class="plaintext hljs">$ qemu &lt; &gt;</code> </pre> <br>  In parameters you describe a network, block devices.  Everything is wonderful, but uncomfortable.  Therefore, there is a libvirt. <br><br>  <strong>The task of libvirt is to be a single tool for all hypervisors</strong> .  It can work with anything: with KVM, with LXD.  It seems that it remains only to learn the syntax of libvirt, but in fact it works worse than in theory. <br><br>  These three words are all that is needed to raise the first virtual machine in KVM.  But again, there are nuances ... <br><br>  At libvirt there is a config where virtualkeys and other settings are stored.  It stores the configuration in xml-files - stylish, fashionable and straight from the 90s.  If desired, they can be edited by hand, but why, if there are convenient commands.  It is also convenient that changes to xml files are wonderfully versioned.  We use <strong>etckeeper</strong> - versionin directory etc.  It is already time to use etckeeper. <br><br><h2>  LXC Crash Course </h2><br>  There are many misconceptions about LXC and LXD. <br><br><blockquote>  LXC is the ability of the modern kernel to use namespaces - to pretend that it is not at all the kernel that was originally. </blockquote><br>  These namespaces can be created any number for each container.  Formally, the core is one, but it behaves like many identical cores.  LXC allows you to run containers, but provides only basic tools. <br><br>  Canonical, which stands behind Ubuntu and aggressively moves containers forward, has released <strong>LXD, an analogue of libvirt</strong> .  This is a harness that makes it easier to launch containers, but inside it is still LXC. <br><br><blockquote>  LXD is a container hypervisor that is based on LXC. </blockquote><br>  Enterprise reigns in LXD.  LXD stores the config in its database - in the <code>/var/lib/lxd</code> .  There LXD keeps its config in the config in SQlite.  It does not make sense to copy it, but you can write down the commands that you used to create the container configuration. <br><br>  There is no upload as such, but most of the changes are automated by commands.  This is an analogue Docker-file, only with manual control. <br><br><h2>  Production </h2><br>  What did we encounter when we swam into operation with this? <br><br><h3>  Network </h3><br>  How much hellish trash and intoxication on the Internet about the network in KVM!  90% of materials say to use bridge. <br><br><blockquote>  Stop using bridge! </blockquote><br>  What's wrong with him?  Recently, I have a feeling that insanity is going on with containers: we put Docker over Docker so that you can run Docker in Docker while watching Docker.  Most do not understand what bridge is doing. <br><br>  It puts your network controller in <strong>promiscuous mode</strong> and accepts all traffic, because it does not know which is and which is not.  As a result, all bridge traffic goes through a wonderful, fast Linux network stack, and there is a lot of copying.  In the end, everything is slow and bad.  Therefore do not use bridge in production. <br><br><h3>  SR-IOV </h3><br>  <strong>SR-IOV is the ability to virtualize within a network card</strong> .  The network card itself is able to allocate a part of itself for virtual machines, which requires some iron support.  That is what will prevent migrate.  Migrating a virtual machine to where SR-IOV is missing is painful. <br><br>  SR-IOV should be used where it is supported by all hypervisors, as part of the migration.  If not, then you have macvtap. <br><br><h3>  macvtap </h3><br>  This is for those whose network card does not support SR-IOV.  This is the light version of the bridge: different MAC addresses are hung on the same network card, and <strong>unicast filtering is used</strong> : the network card accepts not everything, but strictly according to the list of MAC addresses. <br><br>  More bloody details can be found in Toshiaki Makita <a href="https://events.static.linuxfound.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdf">'s</a> remarkable report <a href="https://events.static.linuxfound.org/sites/events/files/slides/LinuxConJapan2014_makita_0.pdf">‚ÄúVirtual switching technologies and Linux bridge‚Äù</a> .  He is full of pain and suffering. <br><br><blockquote>  90% of the materials on how to build a network in KVM are useless. </blockquote><br>  If someone says that bridge is cool, don't talk to this person anymore. <br><br>  With macvtap, the <strong>CPU saves about 30%</strong> due to the smaller number of copies.  But with promiscuous mode there are some nuances.  It is impossible to connect from the hypervisor itself from the host to the guest‚Äôs network interface.  The Toshiaki report described in detail about this.  But in short - it will not work. <br><br>  From the hypervisor itself rarely go through SSH.  There it is more convenient to start the console, for example, Win-console.  It is possible to ‚Äúwatch‚Äù traffic on the interface - it is impossible to connect via TCP, but traffic on the hypervisor is visible. <br><br><blockquote>  If your speed is above 1 Gigabit - choose macvtap. </blockquote><br>  At interface speeds of up to or around 1 Gigabit per second, you can use the bridge.  But if you have a 10 Gb network card and you want to dispose of it somehow, then only macvtap remains.  There are no other options.  In addition to SR-IOV. <br><br><h3>  systemd-networkd </h3><br>  <strong>This is a great way to store network configuration on the hypervisor itself</strong> .  In our case, this is Ubuntu, but for other systems, systemd works. <br><br>  We used to have the <code>/etc/network/interfaces</code> file in which we kept everything.  One file is inconvenient to edit every time - systemd-networkd allows you to split the configuration into a scattering of small files.  This is convenient because it works with any versioning system: sent to Git and see when and what change happened. <br><br>  There is a flaw that our networkers have discovered.  When I need to add a new VLAN in the hypervisor, I go and configure.  Then I say: "systemctl restart systemd-networkd".  At this moment everything is fine with me, but if BGP sessions are raised from this machine, they break.  Our networkers do not approve. <br><br>  For the hypervisor, nothing terrible happens.  Systemd-networkd is not suitable for border border servers, servers with raised BGP, and for hypervisors it is excellent. <br><br>  Systemd-networkd is far from the finals and will never be completed.  But this is more convenient than editing one huge file.  An alternative to systemd-networkd in Ubuntu 18.04 is Netplan.  This is a ‚Äúcool‚Äù way to configure a network and step on a rake. <br><br><h3>  Network device </h3><br>  After installing KVM and LXD on the hypervisor, the first thing you see is two bridges.  One made himself a KVM, and the second - LXD. <br><br><blockquote>  LXD and KVM are trying to deploy their network. </blockquote><br>  If you still need a bridge - for test machines or to play, kill the bridge, which is turned on by default and create your own - the one you want.  KVM or LXD do it terribly - slip dnsmasq, and the horror begins. <br><br><h3>  Storage </h3><br><blockquote>  No matter what implementations you like - use shared storage. </blockquote><br>  For example, on iSCSI for virtual machines.  You will not get rid of the ‚Äúpoint of failure‚Äù, but you will be able to <strong>consolidate storage at one point</strong> .  This opens up new interesting opportunities. <br><br>  For this, it is necessary to have interfaces at least 10 Gbit / s inside the data center.  But even if you only have 1 Gbit / s - do not be upset.  This is about 125 MB / s - quite good for hypervisors that do not require a high disk load. <br><br>  KVM is able to migrate and drag storage.  But, for example, in workload mode, moving a virtual machine to a couple of terabytes is a pain.  For migration with shared storage, there is only enough RAM to transfer, which is elementary.  This <strong>shortens the migration time</strong> . <br><br><h3>  As a result, LXD or KVM? </h3><br>  Initially, we assumed that for all virtual machines where the kernel coincides with the host system, we take LXD.  And where we need to take another core - take KVM. <br><br>  In reality, the plans did not take off.  To understand why, let's take a closer look at LXD. <br><br><h3>  Lxd </h3><br>  The main advantage is the saving of memory on the core.  The core is the same and when we launch new containers the core is the same.  At this pros ended and the cons began. <br><br>  <strong>A block device with rootfs needs to be mounted.</strong>  It's harder than it looks. <br><br>  <strong>There is really no migration</strong> .  It is, and is based on the wonderful gloomy instrument criu, which our compatriots are sawing.  I am proud of them, but in simple cases criu doesn't work. <br><br>  <strong>zabbix-agent behaves strangely in a container</strong> .  If you run it inside a container, then you will see a series of data from the host system, and not from the container.  So far nothing can be done about it. <br><br>  <strong>When looking at the list of processes on the hypervisor, it is impossible to quickly figure out from which container a particular process is growing</strong> .  It takes time to figure out what the namespace is, what and where.  If the load somewhere jumped more than usual, then quickly do not understand.  This is the main problem - a limitation in response capabilities.  For each case, a mini-investigation is conducted. <br><br><blockquote>  The only plus point in LXD is saving memory on the core and reducing overhead. </blockquote><br>  But Kernel Shared Memory in KVM saves memory. <br><br>  I see no reason to introduce serious production and LXD.  Despite Canonical's best efforts in this area, LXD in production brings more problems than solutions.  In the near future the situation will not change. <br><br>  But you can't say that LXD is evil.  It is good, but in limited cases, which will be discussed later. <br><br><h3>  Criu </h3><br>  Criu is a gloomy utility. <br><br>  Create an empty container, it will arrive with a DHCP client and tell it: ‚ÄúSuspend!‚Äù Get an error, because there is a DHCP client: ‚ÄúHorror, horror!  It opens the socket with the raw sign - what a nightmare! ‚ÄùWorse than ever. <br><br><blockquote>  Impressions of containers: no migration, Criu works through time. </blockquote><br>  I like the recommendation from the LXD team about what to do with Criu so that there are no problems: <br><br>  - <em>Get a fresher version from the repository!</em> <br><br>  Is it possible to put it somehow from the package in order not to run into the repository? <br><br><h3>  findings </h3><br>  <strong>LXD is wonderful if you want to create a CI / CD infrastructure.</strong>  We take LVM - Logical Volume Manager, do a snapshot with it, and start the container on it.  Everything works great!  For a second, a new clean container is created, which is set up for testing and rolling chef - we actively use it. <br><br>  <strong>LXD is weak for serious production</strong> .  We cannot understand what to do with LXD in production if it does not work well. <br><br>  <strong>Choose KVM and only KVM!</strong> <br><br><h3>  Migration </h3><br>  I will say this briefly.  For us, the migration turned out to be a wonderful new world that we like.  Everything is simple there - there is a command for migration and two important options: <br><br><pre> <code class="plaintext hljs">virsh migrate &lt;vm&gt; qemu+ssh://&lt;hypervisor&gt;/system --undefinesource -persistent</code> </pre> <br>  If you type ‚ÄúKVM migration‚Äù in Google and open the first material, you will see the command for migration, but without the last two keys.  You will not see any mention that they are important: ‚ÄúJust execute this command!‚Äù Execute the command ‚Äî does it really migrate, but just how? <br><br>  Important migration options. <br><br>  <strong>undefinesource - remove a virtual machine from the hypervisor from which we are migrating.</strong>  If you reboot after such a migration, the hypervisor from which you left will restart this machine.  You'd be surprised, but this is normal. <br><br>  <strong>Without the second parameter ‚Äî the persistent ‚Äî the hypervisor where you moved, does not at all consider that this is a permanent migration.</strong>  After the reboot, the hypervisor will not remember anything. <br><br><pre> <code class="plaintext hljs">- virsh dominfo &lt;vm&gt; | grep persistent</code> </pre> <br>  Without this option, the virtual machine - circles on the water.  If the first parameter is specified without the second, then guess what will happen. <br><br>  There are many such moments with KVM. <br><br><ul><li>  Network: you are always told about the bridge - it's a nightmare!  You read and think - how so? </li><li>  Migration: about her, too, will not say anything intelligible, until you yourself beat your head against this wall. </li></ul><br><h2>  Where to begin? </h2><br>  Late to start - I'm talking about something else. <br><br><h3>  Provisioning: how to deploy it </h3><br><blockquote>  If you are satisfied with the standard installation options, then the preseed mechanism is beautiful. </blockquote><br>  Under ESXi, we used virt-install.  This is a regular way to deploy a virtual machine.  It is convenient because you create a preseed file that describes the image of your Debian / Ubuntu.  Start a new car by feeding it an ISO distribution kit and a preseed file.  Then the machine itself rolls out.  You connect to it via SSH, catch it in the chef, roll kukbuks - that's it, you rushed to the prod! <br><br>  But if you have enough virt-install, I have bad news.  This means that you have not reached the stage when you want to do something differently.  We came to understand that virt-install is not enough.  We have come to some "golden image", which we clone and then run virtuals. <br><br><h3>  And how to arrange a virtual machine? </h3><br>  Why did we come to this image, and why is provisioning important at all?  Because there is still a poor understanding in the community that there are big differences between a virtual machine and a regular machine. <br><br>  <strong>The virtual machine does not need a complicated boot process and a smart bootloader</strong> .  It is much easier to attach the disks of a virtual machine to a machine that has a full set of tools than trying to get out somewhere in recovery mode. <br><br>  <strong>The virtual machine needs the simplicity of the device</strong> .  Why do we need partitions on a virtual disk?  Why do people take a virtual disk, and put there partitions, not LVM? <br><br>  <strong>The virtual machine needs maximum extensibility</strong> .  Usually virtualka grow.  This is a ‚Äúcool‚Äù process - increasing the partition in the MBR.  You delete it, at this moment wiping the sweat from your forehead, and you think: ‚ÄúJust not write it down now, just not write it down!‚Äù - and create it again with new parameters. <br><br><h3>  LVM @ lilo </h3><br>  As a result, we came to LVM @ lilo.  This is a downloader that allows customization from a single file.  If to configure the GRUB config you edit a special file that manages the template engine and builds the monster boot.cfg, then with Lilo there is one file and nothing more. <br><br>  LVM without partitions allows you to make the system perfect and simple.  The problem is that GRUB cannot live without MBR or GPT, and it is freezing.  We tell him: ‚ÄúGRUB is established here,‚Äù but he cannot, because there are no partitions. <br><br>  LVM allows you to quickly expand and make backups.  Standard dialogue: <br><br>  <em>- Guys, how do you make a virtual backup?</em> <br><br>  <em>- ... we take a block device and copy.</em> <br><br>  <em>- tried to deploy back?</em> <br><br>  <em>- Well, no, everything works for us!</em> <br><br>  You can lick the block device in a virtual machine at any time, but if there is a file system there, then any entry to it requires three movements - this procedure is not atomic. <br><br>  If you do snapshot of a virtual machine from the inside, then it can talk to the file system so that it comes to the right consistent state.  But it is not suitable for everything. <br><br><h2>  How to build a container? </h2><br>  For start and creation of the container there are regular means from templates.  LXD offers a Ubuntu 16.04 or 18.04 template.  But if you are an advanced fighter and want not a regular template, but your custom rootfs, which you can customize, the question arises: how to create a container from scratch in LXD? <br><br><h3>  Container from scratch </h3><br>  <strong>Prepare rootfs</strong> .  This will help debootstrap: explain which packages are needed, which are not - and set. <br><br>  <strong>Explain LXD that we want to create a container from a specific rootfs</strong> .  But first we create an empty container with a short command: <br><br><pre> <code class="plaintext hljs">curl --unix-socket /var/lib/lxd/unix.socket -X POST -d '{"name": "my-container", "source": {"type": "none"}}' lxd/1.0/containers</code> </pre> <br>  It can even be automated. <br><br>  A thoughtful reader will say - where are the rootfs my-container?  Where is it indicated where it lies?  But I did not say that this is all! <br><br>  <strong>Mount the container rootfs</strong> to where it will live.  Then we specify that the rootfs container will live there: <br><br><pre> <code class="plaintext hljs">lxc config set my-container raw.lxc "lxc.rootfs=/containers/my-container/rootfs"</code> </pre> <br>  Again this is automated. <br><br><h3>  Container life </h3><br>  <strong>The container does not have its own kernel</strong> , so its loading is simpler <strong>:</strong> systemd, init and fly! <br><br>  If you are not using standard tools for working with LVM, then in most cases you will need to install container rootfs in the hypervisor to start the container. <br><br>  I sometimes find articles in which ‚Äúautofs‚Äù are advised.  Do not do this.  Systemd has automount units that work, but autofs does not.  Therefore, systemd automount units can and should be used, and autofs is not worth it. <br><br><h2>  findings </h2><br>  <strong>We like KVM with migration</strong> .  With LXD, not yet on the way, although for tests and building infrastructure we use it where there is no production load. <br><br>  <strong>We like the KVM performance</strong> .  It‚Äôs more usual to look at the top, to see a process there that is related to this virtual machine, and to understand who is doing what with us.  This is better than using a set of strange utilities with containers to find out what is behind the underwater knocks. <br><br>  <strong>We are delighted with the migration.</strong>  This is largely due to the common repository.  If we migrated while dragging the disks, we would not be so happy. <br><br><blockquote>  If you, like Lev, are ready to talk about overcoming the difficulties of operation, integration or support, now is the time <a href="https://conf.ontico.ru/lectures/propose/%3Fconference%3Ddc2019-moscow">to submit a report</a> to the autumn <a href="https://devopsconf.io/moscow/2019">DevOpsConf</a> conference.  And we in the program committee will help prepare the same encouraging and useful talk like this. <br><br>  We are not waiting for the Call for Papers deadline and have already accepted several reports to <a href="https://devopsconf.io/moscow/2019/abstracts">the</a> conference <a href="https://devopsconf.io/moscow/2019/abstracts">program</a> .  Subscribe to the <a href="http://eepurl.com/bN_0E1">newsletter</a> and the <a href="https://t.me/DevOpsConfChannel">telegram channel</a> and you will be informed about the news on the preparation for DevOpsConf 2019 and do not miss new articles and videos. </blockquote></div><p>Source: <a href="https://habr.com/ru/post/458922/">https://habr.com/ru/post/458922/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../458914/index.html">What (not) you need to know to create games on Unity</a></li>
<li><a href="../458916/index.html">Under the hood of React. We write our implementation from scratch</a></li>
<li><a href="../458918/index.html">What can you learn from the design of giperkazualnyh games</a></li>
<li><a href="../45892/index.html">Stages of development of a promotional site. Request</a></li>
<li><a href="../458920/index.html">DevOps Fan Conference</a></li>
<li><a href="../458928/index.html">XLNet vs. BERT</a></li>
<li><a href="../458930/index.html">How do students from Perm get to the final of the international data analysis championship Data Mining Cup 2019</a></li>
<li><a href="../458932/index.html">Yota - or how you can find out everything</a></li>
<li><a href="../458934/index.html">Deploying Applications on Multiple Kubernetes Clusters with Helm</a></li>
<li><a href="../458936/index.html">‚ÄúIt's easier to answer than to remain silent‚Äù - a great interview with the father of transactional memory, Maurice Herlihy</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>