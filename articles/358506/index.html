<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>AI, practical course. Project planning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This article will focus on: 



- design of the idea in a real project using different methods of analysis and appropriate project management tools; 
...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>AI, practical course. Project planning</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/jh/pc/m4/jhpcm4p4irxwqmpbqzlyrumkswg.jpeg"><br><br>  This article will focus on: <br><br><ul><li>  design of the idea in a real project using different methods of analysis and appropriate project management tools; </li><li>  using the CRISP-DM methodology (interdisciplinary standard process for data mining); </li><li>  defining standard tasks for any AI project. </li></ul><br>  Some tasks are presented with a focus on group work.  If you are working alone, you can skip these sections or complete the corresponding tasks, <a href="http://www.debonogroup.com/six_thinking_hats.html">combining several functions</a> . <br><a name="habracut"></a><br>  As an educational project for this series of articles, we took an application that will recognize emotions on downloaded images using an image processing algorithm (emotion recognition), create musical accompaniment suitable for recognized emotions, and then edit the video, combining images and musical accompaniment. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  <font color="#0071c5">Design design in the project</font> </h2><br>  Making the design in the project begins with determining the scope of the project, that is, what you will create and how.  This eliminates possible obscure situations during development. <br><br>  For example: <br><br><ul><li>  How many pictures can I upload? </li><li>  What is meant by the recognition of emotions? </li><li>  How will we represent the emotion on the computer? </li><li>  What emotions will be supported? </li><li>  How are we going to assign an emotion to a set of images? </li><li>  How will we train the music making model? </li><li>  How will the emotion recognition component and the music production component be connected? </li><li>  How will the process of creating music? </li><li>  How to ensure that the melody created by the computer, emotions? </li><li>  How will the final video with smooth transitions between frames be mounted? </li></ul><br>  You need to find the answer to each of these questions before you can go into the implementation phase.  In addition, you need a methodology that will help you formulate your own questions about the future of the application, so as not to miss any important aspects. <br><br><h2>  <font color="#0071c5">Project Analysis Methods</font> </h2><br>  We recommend using these three simple but effective techniques for analyzing a project and formulating tasks. <br><br><ul><li>  <b>Hierarchical decomposition</b> .  This technique is based on the principle of ‚Äúdivide and conquer‚Äù and implies an iterative decomposition of tasks into subtasks until the time to complete a single subtask is only two hours or the time required for decomposition becomes comparable to the time to complete a task.  At the top of the hierarchy is a block that denotes the entire system.  At the next level, there are blocks associated with logically autonomous system components, which together provide the result necessary for the user, etc., in the reverse order.  Since you start with the system as a whole, and then disassemble its level after level into a number of mutually exclusive components, this prevents you from missing a single task. </li><li>  <b>Analysis of possible situations</b> .  When using this method, you constantly ask yourself the question ‚ÄúWhat if?‚Äù To discover the hidden nuances. </li><li>  <b>Modeling user path</b> .  This technique is used by many user interface developers and product managers and involves the creation of a number of tasks and scenarios that your application must support.  Imagine that you yourself use the application, and simulate the typical modes that the user can call.  If you can mentally go from beginning to end without a hitch, there is no need to add other tasks - this is your stop criterion.  However, at the very beginning you will most likely encounter problems at every step of the user's path.  In this case, just write down the essence of the problem and create a task to solve it after. </li></ul><br><h3>  <font color="#0071c5">Analysis of video editing applications</font> </h3><br>  Based on the hierarchical decomposition method for our project, we decide that the video editing application, like any other application involving user interaction, can be divided into: <br><br><ol><li>  the interface part, i.e. the component directly interacting with the user; </li><li>  software and hardware, i.e., a component, where interesting AI processes take place and which, in turn, can be divided into: <ul><li>  o emotion recognition component; </li><li>  o component to create music. </li></ul></li></ol><br>  In the interface and hardware-software parts, there are three main components: the user interface (blue), the recognition of emotions (orange) and the creation of musical accompaniment (green), as shown in Fig.  one. <br><br><img src="https://habrastorage.org/webt/0h/ny/dk/0hnydkfbpjmwn6an6gd-0yxt6us.jpeg"><br>  <i>Fig.</i>  <i>1. Scheme of the application for video editing.</i> <br><br><h3>  <font color="#0071c5">User interface</font> </h3><br>  The user interacts with this component, using the buttons to upload images, start the video editing process, and organize sharing or downloading the result.  Although the user interface is an important element, in this series of educational materials we will focus on intelligent information processing in software and hardware using new technologies from Intel. <br><br>  <a href="https://lab.datamonsters.co/slideshow-music/">Experience the benefits of an AI-based video editing app.</a> <br><br>  <b>Step 1. Recognizing emotions</b> <br><img src="https://habrastorage.org/webt/yt/hk/xd/ythkxde6zjosh_afnc48jxmaoxc.jpeg"><br><br>  <b>Step 2. Video editing</b> <br><img src="https://habrastorage.org/webt/_u/oz/dn/_uozdnvz0dxzlhyj6d8jrghjhzm.jpeg"><br><br>  <b>Step 3. Exchange</b> <br><img src="https://habrastorage.org/webt/vc/tx/1j/vctx1jillys2itmkg___cknkqai.jpeg"><br><br><h3>  <font color="#0071c5">Emotion recognition by hardware and software (image processing)</font> </h3><br>  This component is responsible for assigning emotions to images.  Each intelligent component that processes data has two processes: <br><br><ol><li>  the learning process or data extraction component, ie, the learning phase; </li><li>  the process of applying the component, i.e. the phase of application and testing. </li></ol><br>  Each of these processes is associated with a number of <a href="https://software.intel.com/en-us/articles/hands-on-ai-part-4-project-planning">standard AI tasks</a> . <br><br>  <b>During the learning phase,</b> you supply the component with response data and teach the machine the "rule" of assigning the appropriate responses to input data from the same set or distribution.  The tasks of the training phase include: <br><br><ol><li>  define input and output data for each stage of data processing; </li><li>  find or create a machine learning dataset that matches specific input and output data; </li><li>  train a machine learning model; </li><li>  evaluate the machine learning model. </li></ol><br>  <b>During the application and testing phase, the</b> trained model is used to predict responses when unknown objects are received from the same population or distribution.  Tasks include: <br><br><ol><li>  define input and output data for each stage of data processing; </li><li>  deploy a machine learning model. </li></ol><br>  In the broader context, the tasks and steps typical of almost every AI project can be defined using the <a href="https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining">CRISP-DM methodology</a> .  Within this methodology, a project based on data mining or AI is presented in the form of a cycle with six states, as shown in Fig.  2. At each stage there are different tasks and subtasks, for example, data markup, model assessment and function development.  The cycle arises due to the fact that any real intelligent system can be improved. <br><br><img src="https://habrastorage.org/webt/ay/bx/bv/aybxbvy6knl897tqczhpcga-6cc.jpeg"><br>  <i>Figure 2. Relationships between different phases of CRISP-DM.</i> <br><br>  Moving from theory to practice, let's formulate the input and output data for training and test the model of emotion recognition. <br><br>  We have six different emotions: anxiety, sadness, joy, calm, determination and fear. <br><br><ul><li>  Each emotion is represented in the computer as a code from 1 to 6 in accordance with the method of encoding the content. </li><li>  Each image is represented as a three-dimensional tensor (three-dimensional array), where three dimensions are height, width, and color channel (for example, RGB) </li></ul><br>  <b>Training</b> <br><br><ul><li>  Input data: four-dimensional tensor (a set of three-dimensional images with labels, i.e. images and classes of their emotions) </li><li>  Output: a trained machine learning model. </li></ul><br>  <b>Testing</b> <br><br><ul><li>  Input data: three-dimensional tensor (image without a label), a learning model. </li><li>  Output data: multinomial probability distribution by code / class. </li></ul><br>  Next, you need to see how the project fits into the big picture, and apply system analysis methods to eliminate inconsistencies: <br><br><ol><li>  The user will upload multiple images.  Therefore, we must decide how, from the emotions recognized for each individual image, to get a single emotional mood for the entire video. </li><li>  Images will be accompanied by music.  Therefore, we must understand how a melody will be created on the basis of a particular emotion. </li><li>  The content of the images must be synchronized with the music, and the transitions between the images must be smooth so that the user can enjoy watching. </li></ol><br>  To begin, we will create the music for one image.  We will return to several images after we consider the component of creating musical accompaniment and understand how to integrate it with the component of emotion recognition. <br><br><h3>  <font color="#0071c5">Creation of musical accompaniment by software and hardware</font> </h3><br>  This component produces a melody in response to an emotion.  To connect the component of creating music with the component of image processing, we need a connecting link between the emotion code and the audio signal: <br><br><ol><li>  Take a randomly known song from a previously created database of famous songs. </li><li>  Adjust the tempo, scale, rhythm of the melody in accordance with the emotion using a simple script. </li><li>  Start the process of creating music based on machine learning using a basic melody tuned to the emotion. </li></ol><br>  The process of creating musical accompaniment should complete the basic melody tuned to the emotion, identifying the most naturally sounding next musical note by learning in the past on a wide body of songs. <br><br>  As in the case of the emotion recognition component, we need to define the input and output data for learning and testing the music creation model. <br><br>  <b>Training</b> <br><ul><li>  Input: collection of songs. </li><li>  Output: A trained machine learning model for creating musical accompaniment (for example, predicting the next note in response to a specific note). </li></ul><br>  <b>Testing</b> <br><br><ul><li>  Input data: a trained machine learning model for creating musical accompaniment And a prepared basic melody (a series of notes). </li><li>  Output: a sequence of notes that creates a basic melody. </li></ul><br>  Taking into account the constant APIs for the music creation component, we need to implement the modulation script depending on the emotion and complete all the AI ‚Äã‚Äãtasks related to creating the music accompaniment, including searching for a data set to train the music creation model or searching for basic melodies .  We will discuss these aspects later in the articles.  For our educational project we will use the following: <br><br><ul><li>  Base tunes are taken from the open collection of musical compositions (musical compositions created before 1922) </li><li>  The process of creating music is taught in the choirs of Bach, i.e., the notes symbolically representing the works of Bach, which were prepared as part <a href="http://bachbot.com/">of the BachBot project</a> </li><li>  Source files are taken from <a href="http://web.mit.edu/music21/">the music21 project.</a> </li></ul><br><h3>  <font color="#0071c5">Component connection</font> </h3><br>  Options for combining images and melodies are presented below with their own advantages and disadvantages. <br><br><img src="https://habrastorage.org/webt/uj/fd/_n/ujfd_ny2kfsxcrytwtmcahf5iam.jpeg"><br><br>  <b>Option A:</b> One base tune is modulated according to the prevailing emotion for all images. <br><br><ul><li>  Benefits: Smooth transitions between images. </li><li>  Disadvantages: The user may experience discomfort if the musical accompaniment remains the same when the joyful image changes to sad. </li></ul><br><img src="https://habrastorage.org/webt/cx/bt/z1/cxbtz1salndexv2ofvzib1ax6eg.jpeg"><br><br>  <b>Option B:</b> One base tune is modulated separately for each emotion found on the images. <br><br><ul><li>  Benefits: Smooth transitions between images and a clear correspondence between the emotion in the image and musical accompaniment. </li><li>  Disadvantages: None. </li></ul><br><img src="https://habrastorage.org/webt/si/ak/ud/siakudrfzwhba2khpkv6auuor2k.jpeg"><br><br>  <b>Option :</b> Different basic melodies are modulated separately for each emotion found on the images. <br><br><ul><li>  Benefits: A clear match between the emotion in the image and music. </li><li>  Disadvantages: Non-smooth transitions between images, because music can change dramatically when emotions change. </li></ul><br>  For example, we have three images with different emotions (joy, tranquility and fear).  We'll use Jingle Bells as the base song, and we'll use BachBot to create the music.  As a result, we will get three songs created based on three versions of ‚ÄúJingle Bells‚Äù (the first for joy, the second for calm, and the third for fear).  Each incoming image is processed separately by calling the API to recognize emotions using one base song that applies to all images.  The base song is modulated for each emotion, after which for each image the corresponding version of the song is used, modulated to fit the emotion. <br><br>  For more information, see: <br><br><ul><li>  <a href="https://software.intel.com/en-us/file/demosystemapispdf">Technical specifications for the main interfaces</a> </li><li>  <a href="https://software.intel.com/en-us/file/projectdecompositionpdf">Initial levels of project decomposition</a> </li></ul><br><h3>  <font color="#0071c5">General recommendations for project management</font> </h3><br>  To decompose a project, do the following: <br><br><ol><li>  determine the structure of the upper level (up to three levels of decomposition); </li><li>  talk to each member separately (usually performed by a system architect) to <ul><li>  learn about the capabilities and requirements of the respective components; </li><li>  define official API requirements; </li><li>  refine the project decomposition hierarchy.  In this case, the smallest integration details will be revealed at an early stage of the project, thereby reducing the amount of work and improvements at the integration stage. </li></ul></li></ol><br>  Below are general guidelines that will help you to draw up your plan in the project. <br><br><ul><li>  One specialist from your project team (usually a system architect) will be responsible for decomposition, and all other team members should be actively involved in the process, providing initial data and feedback on the technical specifications.  This will ensure the integrity of the idea - the main feature of a well-designed system ¬≤. </li><li>  To find the right abstraction (input and output) for each component, you may need to consider several components at the same time.  In this case, determine all possible input and output formats for each system, and then select a configuration suitable for both components. </li><li>  Since tasks are repeated from project to project, after defining AI tasks you can save time by creating a set of standard AI tasks as a subtree in the decomposition hierarchy. </li><li>  Start with a simple case (for example, one image, one emotion, or one melody) and go away from it instead of trying to create a model of integration of components for a whole application from scratch. </li></ul><br>  In subsequent articles, we will consider in detail all the tasks for our educational project. <br><br><h2>  <font color="#0071c5">Control card of decomposition elements and resources for the AI ‚Äã‚Äãproject</font> </h2><br>  Below is a list of typical tasks for AI projects.  Use it as a template for your project. <br><br><ol><li>  Formulate a business problem by defining input and output data, for example, objects and labels or target variables. </li><li>  Analyze the data. <ul><li>  Make a selection of data. </li><li>  Perform a search analysis of the data. </li></ul></li><li>  Clear the data. <ul><li>  Delete duplicate, extraneous data, etc. </li><li>  Normalize function values. </li></ul></li><li>  Create a test model for the assessment methodology. </li><li>  Develop a machine learning model. </li><li>  Prepare a machine learning dataset. <ul><li>  Collect baseline data. </li><li>  Find a suitable data set. </li><li>  Configure the storage infrastructure. </li></ul></li><li>  If tags are missing, annotate the source data. <ul><li>  Make recommendations for annotation. </li><li>  Perform an annotation process. </li><li>  Check the quality of the annotation. </li></ul></li><li>  Teach a machine learning model. <ul><li>  Select a library. <ul><li>  Perform a comparative analysis of existing libraries. </li><li>  Install and configure the most appropriate library. </li></ul></li><li>  Select and configure the infrastructure for machine learning. <ul><li>  Perform a comparative analysis of private and public cloud environments and performance technologies. </li><li>  Perform resource planning to achieve your machine learning goals. </li></ul></li><li>  Select an algorithm. </li><li>  Create a prototype of the algorithm. </li><li>  Refine your model by setting up hyper parameters and adding improvements related to this subject area. </li></ul></li><li>  Perform a model evaluation. </li><li>  Deploy the model. <ul><li>  Determine the nature of service level agreements (SLAs) for machine learning APIs. </li><li>  Put the machine learning model in the API (container or re-use in a more efficient programming language). </li><li>  Test the API under load to see if it complies with the terms of service level agreements (SLA). </li></ul></li></ol><br><h2>  <font color="#0071c5">Conclusion</font> </h2><br>  In this article, we looked at three popular systems analysis methods that were applied to the project to create a video editing application.  Having performed a hierarchical decomposition, we identified three main components of the application: the user interface, emotion recognition, and the creation of musical accompaniment.  We performed a detailed analysis of the AI ‚Äã‚Äãcomponents for emotion recognition and musical accompaniment creation, including the definition of input and output data, training and testing, as well as the integration of components.  Finally, we gave recommendations on project planning, and also shared a set of standard AI tasks based on the CRISP-DM methodology, suitable for any AI project. </div><p>Source: <a href="https://habr.com/ru/post/358506/">https://habr.com/ru/post/358506/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../358494/index.html">Working with the clipboard in JavaScript using the asynchronous Clipboard API</a></li>
<li><a href="../358496/index.html">Applying trigonometry rules to create high-quality animation.</a></li>
<li><a href="../358500/index.html">SPLUNK VS ELK?</a></li>
<li><a href="../358502/index.html">noBackend, or How to survive the era of fat clients</a></li>
<li><a href="../358504/index.html">How to deploy Adaptavist ScriptRunner artifacts</a></li>
<li><a href="../358508/index.html">Check Point. A selection of useful materials from TS Solution</a></li>
<li><a href="../358510/index.html">Task life cycle</a></li>
<li><a href="../358512/index.html">Carrot models, bottlenecks and speech recognition: the absence of dictionaries in the field of artificial intelligence</a></li>
<li><a href="../358514/index.html">Release Rust 1.26</a></li>
<li><a href="../358516/index.html">Terminal server for AutoCAD</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>