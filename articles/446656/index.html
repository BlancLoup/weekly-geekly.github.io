<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Speech coding at 1600 bps with LPCNet neural vocoder</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This is a continuation of the first article about LPCNet . In the first demo, we presented an architecture that combines signal processing and deep le...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Speech coding at 1600 bps with LPCNet neural vocoder</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/6ba/d56/c2e/6bad56c2eecd2e1aad4190ba40d1be74.jpg"><br><br>  This is a continuation of the <a href="https://people.xiph.org/~jm/demo/lpcnet/">first article about LPCNet</a> .  In the first demo, we presented an <a href="https://jmvalin.ca/papers/lpcnet_icassp2019.pdf">architecture</a> that combines signal processing and deep learning to improve the efficiency of neural speech synthesis.  This time, turn LPCNet into a neural speech codec with a very low bitrate (see <a href="https://jmvalin.ca/papers/lpcnet_codec.pdf">scientific article</a> ).  It can be used on current equipment and even on phones. <br><br>  For the first time, a neural vocoder works in real time on a single processor core of the phone, and not on a high-speed GPU.  The total bit rate of 1600 bps is about ten times less than the usual broadband codecs provide.  The quality is much better than existing vocoders with a very low bitrate and is comparable to more traditional codecs that use a higher bitrate. <br><a name="habracut"></a><br><h3>  Waveform Encoders and Vocoders </h3><br>  There are two large types of speech codecs: waveform coders and vocoders.  Waveform coders include Opus, AMR / AMR-WB and all codecs that can be used for music.  They try to provide a decoded waveform as close as possible to the original ‚Äî usually with some perceptual features.  On the other hand, vocoders are actually synthesizers.  The encoder extracts information about the pitch and shape of the vocal tract, transmits this information to the decoder, and he re-synthesizes speech.  This is almost like speech recognition followed by reading text in a speech synthesizer, except that the text encoder is much simpler / faster than speech recognition (and conveys a bit more information). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Vocoders have existed since the 70s, but since their decoders perform speech synthesis, they cannot be much better than conventional speech synthesis systems, which until recently sounded just awful.  That is why vocoders were usually used at speeds below 3 KB / s.  In addition, waveform coders simply provide better quality.  This continued until recently, when neural speech synthesis systems, such as <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">WaveNet, appeared</a> .  Suddenly, the synthesis began to sound much better, and, of course, there were people who wanted <a href="https://arxiv.org/pdf/1712.01120.pdf">to do a vocoder from WaveNet</a> . <br><br><h3>  LPCNet Review </h3><br>  WaveNet produces very high-quality speech, but requires hundreds of gigaflops of computing power.  LPCNet significantly reduced computational complexity.  The Vocoder is based on WaveRNN, which enhances WaveNet using a recurrent neural network (RNN) and sparse matrices.  LPCNet further enhances WaveRNN with <a href="https://en.wikipedia.org/wiki/Linear_prediction">linear prediction</a> (LPC), which performed well in older vocoders.  It predicts a sample from a linear combination of previous samples and, most importantly, it does it many times faster than a neural network.  Of course, it is not universal (otherwise the 70's vocoders would have sounded great), but it can seriously reduce the load on the neural network.  This allows you to use a network smaller than WaveRNN, without sacrificing quality. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/652/8fc/7df/6528fc7df88256e797551173b11f5e1d.png"></div><br>  <i><font color="gray">Take a closer look at LPCNet.</font></i>  <i><font color="gray">The yellow part on the left is calculated once per frame, and its output is used for the network of the sampling frequency on the right (blue).</font></i>  <i><font color="gray">The computational unit predicts a sample at time t based on previous samples and linear prediction coefficients.</font></i> <br><br><h1>  Compression characteristics </h1><br>  LPCNet synthesizes speech from vectors of 20 signs for each frame of 10 ms.  Of these, 18 signs are <a href="https://en.wikipedia.org/wiki/Cepstrum">cepstral</a> coefficients representing the shape of the spectrum.  The two remaining ones describe the pitch: one parameter for the pitch of the pitch period and the other for the <i>power</i> (how much the signal correlates with itself if you enter a delay per pitch of the pitch).  If you store the parameters as floating-point values, then all this information takes up to 64 kbps during storage or transmission.  This is too much, because even the Opus codec provides very high-quality speech coding for a total of 16 kbps (for 16 kHz mono).  Obviously, you need to apply strong compression here. <br><br><h3>  Height </h3><br>  All codecs rely heavily on pitch, but unlike waveform coders, where pitch ‚Äúsimply‚Äù helps reduce redundancy, vocoders do not have a fallback.  If it is wrong to pick up the pitch, they will start generating bad-sounding (or even illegible) speech.  Without going into details (see the scientific article), the LPCNet encoder is struggling not to make a mistake in height.  The search begins with a search for <i>correlations</i> in time in the speech signal.  See below how a typical search works. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3e4/024/a6a/3e4024a6aac9fcd8cd1fb8eb750918e6.gif"><br>  <i><font color="gray">The height step is the period during which the step signal is repeated.</font></i>  <i><font color="gray">The animation searches for a step that corresponds to the maximum correlation between the signal x (n) and its copy of x (nT) with a delay.</font></i>  <i><font color="gray">The T value with the maximum correlation is the pitch of the height</font></i> <br><br>  This information needs to be encoded as few bits as possible, without worsening the result too much.  Since we naturally perceive the frequency in a logarithmic scale (for example, each musical octave doubles the previous frequency), there is a sense in logarithmic coding.  The height of the speech signal in most people (we are not trying to cover the soprano here) is between 62.5 and 500 Hz.  With seven bits (128 possible values), we get a resolution of about a quarter of a tone (the difference between and before and re is one tone). <br><br>  So, with the height finished?  Well, not so fast.  People don't speak like robots from 1960s films.  The pitch of the voice can vary even within a 40-ms packet.  It is necessary to take this into account, leaving the bits for the parameter for changing the height: 3 bits to encode the difference up to 2.5 semitones between the beginning and the end of the packet.  Finally, it is necessary to code the correlation of the height steps, distinguishing vowels and consonants (for example, s and f).  Two bits are enough for correlation. <br><br><h3>  Cepstrum </h3><br>  While pitch contains external speech characteristics (prosody, emotions, accent, ...), the spectral response determines <i>what</i> was said (with the exception of tonal languages ‚Äã‚Äãsuch as Chinese, where pitch is important for meaning).  The vocal cords produce approximately the same sound for any vowel, but the shape of the vocal tract determines which sound will be pronounced.  The voice path acts as a filter, and the task of the encoder is to evaluate this filter and transmit it to the decoder.  This can be effectively done if you transform the spectrum into a <a href="https://en.wikipedia.org/wiki/Cepstrum">cepstrum</a> (yes, this is a ‚Äúspectrum‚Äù with a changed letter order, here we are funny guys in digital signal processing). <br><br>  For a 16 kHz input signal, the cepstrum mainly represents a vector of 18 numbers every 10 ms, which should be compressed as much as possible.  Since we have four such vectors in a 40 ms packet and they usually resemble each other, we want to eliminate redundancy as much as possible.  This can be done using neighboring vectors as predictors and passing only the difference between the prediction and the real value.  At the same time, we don‚Äôt want to depend too much on previous packages if one of them disappears.  It seems that the problem has already been solved ... <br><br>  <font color="brown"><i>If you only have a hammer, everything looks like a nail - Abraham Maslow.</i></font> <br><br>  If you <a href="https://xiph.org/daala/">worked a</a> lot <a href="https://en.wikipedia.org/wiki/AV1">with video codecs</a> , then you probably met the concept of B-frames.  Unlike video codecs, which divide a frame into multiple packets, we, on the contrary, have many frames in one packet.  We start with the coding of the <i>key frame</i> , i.e. the independent vector, and the <b>end of the</b> packet.  This vector is encoded without prediction, occupying 37 bits: 7 for total energy (first cepstral coefficient) and 30 bits for the remaining parameters, using <a href="https://en.wikipedia.org/wiki/Vector_quantization">vector quantization</a> (VQ).  Then comes the (hierarchical) B-frames.  Of the two keywords (one of the current package and one of the previous one), a cepstrum between them is predicted.  As a predictor for coding the difference between the real value and the prediction, you can choose either of two key frames or their average value.  We reapply VQ and encode this vector using a total of 13 bits, including the predictor selection.  Now we only have two vectors and very few bits.  Use the last 3 bits to simply select the predictor for the remaining vectors.  Of course, all this is much easier to understand in the picture: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/530/395/c02/530395c02aba2079c8e82e79f98071f4.png"></div><br>  <i><font color="gray">Prediction and quantization of a cepstrum for a packet k.</font></i>  <i><font color="gray">Green vectors are quantized independently, blue - with prediction, and red - use prediction without residual quantization.</font></i>  <i><font color="gray">The prediction is shown by arrows</font></i> <br><br><h3>  Putting it all together </h3><br>  Adding all of the above, we get 64 bits per 40-millisecond packet or 1600 bits per second.  If you want to calculate the compression ratio, then uncompressed wideband speech is 256 kbps (16 kHz 16 bits per sample), which means a compression ratio of 160 times!  Of course, you can always play with the quantizers and get a lower or higher bitrate (with a corresponding impact on quality), but you need to start somewhere.  Here is a tablet with a layout where these bits go. <br><br><table><tbody><tr><td align="center" colspan="2">  <b>Bit allocation</b> </td></tr><tr><td>  Parameter </td><td>  Bit </td></tr><tr><td>  Step height </td><td>  6 </td></tr><tr><td>  Height modulation </td><td>  3 </td></tr><tr><td>  Height correlation </td><td>  2 </td></tr><tr><td>  Energy </td><td>  7 </td></tr><tr><td>  Independent VQ Capr (40 ms) </td><td>  thirty </td></tr><tr><td>  Predicted kepstr vq (20 ms) </td><td>  13 </td></tr><tr><td>  Cepstra interpolation (10 ms) </td><td>  3 </td></tr><tr><td>  Total </td><td>  64 </td></tr></tbody></table><br>  64 bits per packet 40 ms, with 25 packets per second, 1600 bps is obtained. <br><br><h1>  Implementation </h1><br>  <a href="https://github.com/mozilla/LPCNet">LPCNet source code</a> is available under the BSD license.  It includes a library that simplifies the use of codec.  Please note that the development is not finished: both the format and the API are <b>bound to</b> change.  The repository also has a demo application <code>lpcnet_demo</code> , in which it is easy to test the codec from the command line.  For complete instructions, see the README.md file. <br><br>  Who wants to dig deeper, there is an option to train new models and / or use LPCNet as a building block for other applications, such as speech synthesis (LPCNet is only one component of the synthesizer, it does not perform synthesis by itself). <br><br><h3>  Performance </h3><br>  Neural speech synthesis requires a lot of resources.  At last year's ICASSP conference, Bastian Klein and colleagues from Google / DeepMind presented a <a href="https://arxiv.org/pdf/1712.01120.pdf">WaveNet-based 2400 bps codec</a> that received a bit stream from codec2.  Although it sounds amazing, the computational complexity of hundreds of gigaflops means that it cannot be run in real time without an expensive GPU and no serious effort. <br><br>  On the contrary, our 1600 bit / s codec produces only 3 gigaflops and is designed to work in real time on much more affordable hardware.  In fact, it can be used today in real-world applications.  Optimization required to write some code for AVX2 / FMA and Neon instruction sets (only embedded code, no assembler).  Thanks to this, we can now encode (and especially decode) speech in real time not only on a PC, but also on more or less modern phones.  Below is the performance on x86 and ARM processors. <br><br><table><tbody><tr><td colspan="4" align="center">  Performance </td></tr><tr><td>  CPU </td><td>  Frequency </td><td>  % of one core </td><td>  To real time </td></tr><tr><td>  AMD 2990WX (Threadripper) </td><td>  3.0 GHz * </td><td>  14% </td><td>  7,0x </td></tr><tr><td>  Intel Xeon E5-2640 v4 (Broadwell) </td><td>  2.4 GHz * </td><td>  20% </td><td>  5.0x </td></tr><tr><td>  Snapdragon 855 (Cortex-A76 on <b>Galaxy S10</b> ) </td><td>  2.82 GHz </td><td>  31% </td><td>  3.2x </td></tr><tr><td>  Snapdragon 845 (Cortex-A75 on <b>Pixel 3</b> ) </td><td>  2.5 GHz </td><td>  68% </td><td>  1.47x </td></tr><tr><td>  AMD A1100 (Cortex-A57) </td><td>  1.7 GHz </td><td>  102% </td><td>  0.98x </td></tr><tr><td>  BCM2837 (Cortex-A53 on Raspberry Pi 3) </td><td>  1.2 GHz </td><td>  310% </td><td>  0.32x </td></tr><tr><td>  * turbo mode </td><td></td><td></td><td></td></tr></tbody></table><br><br>  The numbers are quite interesting.  Although only Broadwell and Threadripper are shown, on the x86 platform, Haswell and Skylake processors have similar performance (taking into account the clock frequency).  However, ARM processors differ markedly from each other.  Even with the difference in frequency, the A76 is five to six times faster than the A53: quite expected, since the A53 is mainly used for energy efficiency (for example, in big.LITTLE systems).  However, LPCNet can work in real time on a modern phone using only one core.  Although it would be nice to run it in real time and on the Raspberry Pi 3. Now this is far away, but nothing is impossible. <br><br>  On x86, it is still a mystery about the reason for the performance limit of five times the theoretical maximum.  As you know, matrix-vector multiplication operations are less efficient than matrix-by-matrix operations, because there are more downloads per operation ‚Äî specifically, one load per matrix for each FMA operation.  On the one hand, performance is related to the L2 cache, which provides only 16 bits per cycle.  On the other hand, Intel claims that L2 can give up to 32 bits per cycle on Broadwell and 64 bits per cycle on Skylake. <br><br><h1>  results </h1><br>  We conducted audio tests on the model MUSHRA, to compare the quality of coding.  Testing conditions: <br><br><ul><li>  <b>Sample</b> : original (if your result is better than the original, there is clearly something wrong with your test) <br></li><li>  <b>1600 bps LPCNet</b> : our demo <br></li><li>  <b>Uncompressed LPNet</b> : ‚ÄúLPNet with 122 equivalent units‚Äù from the <a href="https://people.xiph.org/~jm/demo/lpcnet/">first article</a> <br></li><li>  <b>9000 bps opus / wideband</b> : the lowest bitrate at which Opus 1.3 encodes wideband audio <br></li><li>  <b>2400 bps MELP</b> : a well-known low bitrate vocoder (similar in quality to codec2) <br></li><li>  <b>Speex 4000 bps</b> : This broadband vocoder should never be used, but it‚Äôs a good bottom line. </li></ul><br>  In the first test (set 1) we have eight speech fragments of statements from two men and two women.  The files in the first set refer to the same database (that is, the same recording conditions) that was used for training, but these specific people were excluded from the training set.  In the second test (set 2), we used some of the files from the Opus test (uncompressed), recording the sound in different conditions, to make sure that LPCNet comes to some generalization.  There are 100 participants in both tests, so the errors are quite small.  See results below. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/dc6/e7d/cc5/dc6e7dcc5b08735cb492ad07bf7894af.svg"></div><br>  <i><font color="gray">Subjective quality (MUSHRA) in two tests</font></i> <br><br>  Overall, LPCNet at 1600 bps looks good - much better than MELP at 2400 bps, and not far behind Opus at 9000 bps.  At the same time, uncompressed LPCNet is slightly better in quality than Opus at 9000 bps.  This means that it is possible to provide better quality than Opus, at bitrates in the range of 2000-6000 bit / s. <br><br><h3>  Listen to yourself </h3><br>  Here are samples from the audio test: <br><br>  Woman (Set 1) <br><br><ul><li>  <a href="">Sample</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Uncompressed LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br>  Male (Set 1) <br><br><ul><li>  <a href="">Sample</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Uncompressed LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br>  Mixed (set 2) <br><br><ul><li>  <a href="">Sample</a> </li><li>  <a href="">LPCNet 1600 bps</a> </li><li>  <a href="">Uncompressed LPNet</a> </li><li>  <a href="">Opus 9000 bps</a> </li><li>  <a href="">MELP 2400 bps</a> </li><li>  <a href="">Speex 4000 bps</a> </li></ul><br><br><h1>  Where can this be used? </h1><br>  We believe that this is a cool technology in itself, but it also has practical application.  Here are some options. <br><br><h3>  VoIP in countries with poor connectivity </h3><br>  Not everyone always has a high-speed connection available.  In some countries, communication is very slow and unreliable.  The 1600 bit / s speech codec works fine in such conditions, even transmitting packets several times for reliability.  Of course, due to the overhead of packet headers (40 bytes for IP + UDP + RTP), it is better to make more packets: 40, 80 or 120 ms. <br><br><h3>  Amateur / HF Radio </h3><br>  For the past ten years, <a href="http://www.rowetel.com/%3Fpage_id%3D434">David Rowe</a> has been working on speech coding for radio communications.  He developed <a href="http://www.rowetel.com/%3Fpage_id%3D452">Codec2</a> , which transmits voice at speeds from 700 to 3200 bps.  During the past year, David and I discussed how to improve Codec2 with neural synthesis, and now we finally did it.  In his blog, David <a href="https://www.rowetel.com/%3Fp%3D6639">wrote</a> about his own implementation of a codec based on LPCNet for integration with <a href="https://freedv.org/">FreeDV</a> . <br><br><h3>  Increased reliability with packet loss </h3><br>  The ability to encode a decent quality bitstream in a small number of bits is useful for providing redundancy on an unreliable channel.  Opus has a Forward Error Correction (FEC) mechanism, known as LBRR, which encodes the previous frame with a lower bit rate and sends it in the current frame.  It works well, but adds significant overhead.  Duplication of the stream at 1600 bps is much more efficient. <br><br><h1>  Plans </h1><br>  There are still many possibilities to explore using LPCNet.  For example, the improvement of existing codecs (of the same Opus).  As in other codecs, Opus quality degrades pretty quickly at very low bit rates (below 8000 bps), because the waveform codec does not have enough bits to match the original.  But the transmitted linear prediction information is enough for LPCNet to synthesize decently sounding speech - better than Opus can do on this bitrate.  In addition, the rest of the information transmitted by Opus (residual forecast) helps LPCNet synthesize an even better result.  In a sense, LPCNet can be used as a fancy post filter to improve the quality of Opus (or any other codec) without changing the bitstream (i.e., maintaining full compatibility). <br><br><h1>  Additional resources </h1><br><ol><li>  J.-M.Valin, J.Skoglund, <a href="https://jmvalin.ca/papers/lpcnet_codec.pdf">1.6 Kbps wideband neural vocoder using LPCNet</a> , <i>Sent to Interspeech 2019</i> , arXiv: <a href="https://arxiv.org/abs/1903.12087">1903.12087</a> . </li><li>  J.-M.Valin, J.Skoglund, <a href="https://jmvalin.ca/papers/lpcnet_icassp2019.pdf">LPCNet: improved neural speech synthesis through linear prediction</a> , <i>Proc.</i>  <i>ICASSP, 2019</i> , arXiv: <a href="https://arxiv.org/abs/1810.11846">1810.11846</a> . </li><li>  A. van den Oord, S. Dileman, H. Zen, K. Simonian, O. Vinyals, A. Graves, N. Kalkhbrenner, E. Seigneur, K. Kavukuglu, <a href="https://arxiv.org/pdf/1609.03499.pdf">WaveNet: A Generative Model for Unprocessed Sound</a> , 2016. </li><li>  N. Karlhbrenner, E. Elsen, K. Simonyan, S. Nouri, N. Casagrande, E. Lokhart, F. Stimberg, A. van den Oord, S. Dileman, K. Kavukuglu, <a href="https://arxiv.org/pdf/1802.08435.pdf">Effective Neural Synthesis of Sound</a> , 2018. </li><li>  V.B. Klein, F.S.K.Lim, A.Lubs, J.Skoglund, F.Stimberg, K.Vang, TS.Walters, <a href="https://arxiv.org/pdf/1712.01120.pdf">Low bitrate speech coding based on Wavenet</a> , 2018 </li><li>  LPCNet <a href="https://github.com/mozilla/LPCNet/">source code</a> . </li><li>  <a href="https://github.com/drowe67/LPCNet">FreeDV codec based on</a> David Row's <a href="https://github.com/drowe67/LPCNet">LPCNet</a> . </li><li>  Join the development discussion on <a href="http://irc//irc.freenode.net/opus">#opus at irc.freenode.net</a> (‚Üí <a href="https://webchat.freenode.net/%3Fchannels%3Dopus">web interface</a> ) </li></ol></div><p>Source: <a href="https://habr.com/ru/post/446656/">https://habr.com/ru/post/446656/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../446646/index.html">InterSystems IRIS 2019.1 release</a></li>
<li><a href="../446648/index.html">Developing Kubernetes operator with Operator Framework</a></li>
<li><a href="../446650/index.html">How much do testers cost and what do their salaries depend on? Build a portrait of a successful QA-specialist</a></li>
<li><a href="../446652/index.html">MVCC-4. Snapshots of data</a></li>
<li><a href="../446654/index.html">How we saved the code review</a></li>
<li><a href="../446658/index.html">Interview with Andrei Stankevich about sports programming</a></li>
<li><a href="../446660/index.html">AI, student and big prizes: how to do machine learning in 8th grade</a></li>
<li><a href="../446662/index.html">Transactions and their control mechanisms</a></li>
<li><a href="../446666/index.html">Squeezing the maximum out of graphing calculators: games on TI-83</a></li>
<li><a href="../446668/index.html">Python for the Web: what does a junior need to know in order to work and develop</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>