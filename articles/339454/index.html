<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>How the brain hits a tree, or how we made a recommendation system using a neural network.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="How would you make a recommendation system? Many immediately had a picture in their head as they import and stack  Xgboost  CatBoost. Initially, the s...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>How the brain hits a tree, or how we made a recommendation system using a neural network.</h1><div class="post__text post__text-html js-mediator-article"><p>  How would you make a recommendation system?  Many immediately had a picture in their head as they import and stack <del>  Xgboost </del>  CatBoost.  Initially, the same picture appeared in our heads, but we decided to do this on neural networks in the HYIP wave, the benefit of which was a lot of time.  The experience of their creation, testing, results and our thoughts are described below. </p><br><p><img src="https://habrastorage.org/webt/0a/el/lh/0aellhydtfwdfara6ptj3x0g-ow.jpeg"></p><a name="habracut"></a><br><h3 id="postanovka-zadachi">  Formulation of the problem </h3><br><p>  We were faced with the task of creating a model that chooses people who should send an offer to use some service provided by the company.  Previously, this was the decisive forest.  We decided to do it on the ‚Äúomnipotent‚Äù neural networks, at the same time understanding how they work and whether they are really complex.  After studying the subject area, our attention focused on several architectures. </p><br><h3 id="zainteresovavshie-nas-arhitektury">  Interested in architecture </h3><br><p>  <em>Further, by clicking on the name of something, you can get to the article, where it is well explained.</em> </p><br><ol><li>  <strong><a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">CNN</a></strong> <br>  The architecture used mainly for image processing. <br>  What it is and how it works can be read <a href="https://habrahabr.ru/post/309508/">here</a> and <a href="http://ufldl.stanford.edu/tutorial/supervised/ConvolutionalNeuralNetwork/">here</a> . <br>  We decided to put it both separately and before LSTM for data compression and allocation of dependencies on transactions (which were presented in the form of very sparse tables). <br>  By itself, it worked worse than when paired with LSTM, about which it is written further, but significantly accelerated training and improved quality. </li></ol><br><p>  We used convolutional networks with one-dimensional <a href="http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/">conv conv</a> . </p><br><ol><li>  <strong><a href="https://ru.wikipedia.org/wiki/%25D0%2594%25D0%25BE%25D0%25BB%25D0%25B3%25D0%25B0%25D1%258F_%25D0%25BA%25D1%2580%25D0%25B0%25D1%2582%25D0%25BA%25D0%25BE%25D1%2581%25D1%2580%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BF%25D0%25B0%25D0%25BC%25D1%258F%25D1%2582%25D1%258C">Lstm</a></strong> <br>  This is a popular architecture of the recurrent neural network, which showed itself well in the tasks of finding patterns on the time series, texts and classification, remembering important patterns, forgetting irrelevant ones. </li></ol><br><p>  Very detailed and understandable about its device is written <a href="https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b">here</a> . <br>  We used 2 LSTM layers in our task, each had 128 neurons. <br>  Evaluated the work of three models based on the above architecture. </p><br><ul><li>  This <a href="https://arxiv.org/abs/1611.06455">article</a> provides a comparison of 3 networks, we took the one that showed the best result (FCN).  The authors argue that this architecture is a good baseline for the classification of time series.  In short, it consists of 3 blocks, in each of which first comes CNN-&gt; BN ( <a href="https://habrahabr.ru/post/309302/">batch normalization</a> ) -&gt; ReLU activation.  After that comes the global average pooling. </li><li>  The architecture from this <a href="https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf">article</a> .  A typical approach to NLP is to represent words as a vector (embending), but the authors tried to encode the letters (using the one-hot encoding method), it turned out pretty good.  We solved our problem in a similar way.  Those.  we have the words - these are categories, and the text is the history of the user's purchases.  The architecture itself has the following form: (CNN-&gt; MaxPooling) x3 -&gt; Flatten -&gt; (Dense-&gt; Dropout) x2 -&gt; Softmax. </li><li>  After reading various blogs ( <a href="https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/">for example, this one</a> ), I got the idea to try the following architecture: Conv1D-&gt; Max Pooling-&gt; LSTMx2 -&gt; (Dense-&gt; Dropout) x2-&gt; Softmax. </li></ul><br><ol><li><p>  <strong><a href="https://ru.wikipedia.org/wiki/%25D0%25A0%25D0%25B0%25D0%25B7%25D0%25BB%25D0%25BE%25D0%25B6%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25BC%25D0%25B0%25D1%2582%25D1%2580%25D0%25B8%25D1%2586%25D1%258B">Factorization</a></strong> <br>  Imagine a matrix, each row is a user, each column is an item that the user can rate.  The values ‚Äã‚Äãof the matrix in position (i, j) are the estimate of the i-th user to the j-th item.  Now, using the mathematical apparatus, we present this matrix as a product of two matrices (users, features) (features, objects).  Where features are any signs that we highlight.  Now, scalarly multiplying the user row by the subject column, we get a number indicating how much this subject likes this user. <br>  Libraries for factoring: <br>  <a href="https://github.com/lyst/lightfm">lightfm</a> <br>  <a href="https://github.com/srendle/libfm">libfm</a> </p><br></li><li>  <strong><a href="https://arxiv.org/pdf/1705.00105.pdf">Pairwise</a></strong> <br><ul><li>  <a href="https://ru.wikipedia.org/wiki/%25D0%259A%25D0%25BE%25D0%25BB%25D0%25BB%25D0%25B0%25D0%25B1%25D0%25BE%25D1%2580%25D0%25B0%25D1%2582%25D0%25B8%25D0%25B2%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2584%25D0%25B8%25D0%25BB%25D1%258C%25D1%2582%25D1%2580%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F">Collaborative filtering</a> is a prediction method based on the assumption that if people had similar behavior or ratings in the past, they would be similar in the future. </li></ul></li></ol><br><p>  It can be based both on the external (explicit) behavior of users, when they themselves assess something (Netflix), and on implicit (regular) patterns, when their behavior coincides, in our case they have similar transactional sequences. </p><br><p>  The approach of this article is based on collaborative filtering and takes into account both explicit and implicit regularities.  The architecture is based on a neural network that examines user preferences by pairs of elements, taking into account the representations of these items, which were built before and stored in rows of matrices Us (user x feature), It (item x feature) <br>  by analogy with matrix factorization.  When training the network for the user, 2 subjects are selected for which he gave his estimates and the corresponding rows of the Us and It matrices are cut off, connected and served into the network.  The loss function consists of two parts: the first is responsible for the losses at the output of the neural network itself, and the second is responsible for the fact that the scalar product of the vectors that were applied to the network training is also true.  For example, we say that item_1 is better than item_2, which means that the neural network at the output should give a number greater than zero, and the scalar product of the corresponding vectors should be positive.  Thus factoring is done using this neural network.  Further, we assume that for each user there is a linear order on the items and based on this assumption we rank all the items for a specific user. </p><br><p>  What did we do?  We said that category_1 is better than category_2 if the user paid on it more often and based on this assumption they used this architecture. </p><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d5946bcb253013491.png"></p><br><h2 id="nemnogo-o-dannyh">  A bit about data </h2><br><p>  QIWI provided us with a history of user purchases, as well as information that the user was recommended to buy, and whether he followed the recommendations within 15 days.  We tried to predict the history of purchases of goods that he is likely to buy.  We were interested in only a few categories of products (total of 5).  Thus, we were able to measure the usual True True, False Positive, True Negative, False Negative and <a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">everything that depends on them</a> for the categories we are interested in. </p><br><p>  Also, the difficulty was that the data were greatly unbalanced, almost the entire volume of transactions lay in categories that we are not interested in. </p><br><h2 id="hod-raboty">  Working process </h2><br><p>  Here we will discuss the behavior of models and the quality of various tests. <br>  We will consistently describe how different models worked on different datasets. </p><br><p>  <strong>Lstm</strong> <br>  At the beginning we took a small amount of data, a transaction history of about 12k users and tried to predict the category of the next purchase.  And this is how the above-mentioned architectures worked: </p><br><ul><li>  Below is a schedule of working ‚Äúbare‚Äù LSTM (LSTM-&gt; Softmax).  <strong>There were about 9 thousand users in the training, on the test - 3700</strong> . </li></ul><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d593b273543471801.png"></p><br><p>  As you can see, she is very much retrained.  The following quality metrics were obtained on the test data: <strong>Accuracy: 0.447;</strong>  <strong>MRR: 0.576</strong> .  But having seen that it was pretty good soon, they thought that most likely our model predicts to the user what he basically bought.  To do this, we checked the quality of the algorithm on those people whose predictable category is not present in the history of their transactions ( <strong>hereinafter referred to as special test</strong> ).  There were not many such people in the test (approximately 500).  They obtained the following results: <strong>Accuracy: 0.037;</strong>  <strong>MRR: 0.205</strong> .  On such people, it quickly turned out noticeably worse. </p><br><p>  But we did not despair, tried various options with LSTM, eventually came to the following architecture: Conv1D-&gt; Max Pooling-&gt; LSTM (with rec dropout) x2 -&gt; (Dense-&gt; Dropout) x2-&gt; Softmax.  Here's how she worked: </p><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d5135af9551996679.png"></p><br><p>  The picture is much better.  The network is not so retrained.  We look at the test <strong>soon</strong> : <strong>Accuracy: 0.391;</strong>  <strong>MRR: 0.534</strong> .  The result is slightly worse than with the past network.  But let's not rush to conclusions and look at the special test: <strong>Accuracy: 0.150;</strong>  <strong>MRR: 0.312</strong> .  Thanks to a special test, we see that this model better finds patterns in the data.  Remember this architecture, we still need it. </p><br><ul><li>  Next, we evaluated the work of the following 2 architectures, which are based on CNN. <br>  On the same data as in the previous paragraph, we launched FCN and Char CNN (their device was mentioned above).  Learning outcomes: </li></ul><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d517042a784316858.png"></p><br><p>  The graph indicates retraining, but not as strong as with simple LSTM.  The following results were obtained on the test: <strong>Accuracy: 0.529 MRR: 0.668</strong> .  On a special test: <strong>Accuracy: 0.014;</strong>  <strong>MRR: 0.217</strong> .  Immediately look at the work of CNN's Char, but in fewer epochs: </p><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d503f551634988707.png"></p><br><p>  Here the network is retrained much faster, it gives good results on the test, but according to a special test we see that it has poorly learned complex patterns: <br>  <strong>Test: Accuracy: 0.543;</strong>  <strong>MRR: 0.689</strong> ; <br>  <strong>Special test: Accuracy: 0.0098;</strong>  <strong>MRR: 0.214</strong> . </p><br><p>  <strong>Pairwise</strong> <br>  To begin with, we wrote our model according to the article. <br>  We did this on Pytorch.  This is a very convenient framework if you want to make an off-the-shelf neural network.  Recently published <a href="https://habrahabr.ru/post/334380/">an article</a> with its description from mail. </p><br><p>  First of all, we have achieved approximately the same quality of the <a href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">NDCG</a> as authors on dataset <a href="https://grouplens.org/datasets/movielens/">movielens</a> . </p><br><p> After that we remade it for our data.  We divided the data into two ‚Äútemporary parts‚Äù. <br>  At the entrance of the neural network, we gave a vector in which for each person for each category was the percentage of its payments to this category of the total.  Therefore, the data were quite capacious, and she studied many times faster than LSTM, about 30 minutes on a video card, against several hours of LSTM. <br>  Look at the quality. <br>  At 300,000 users.  Pomerin metrics for all classes, not just those that interest us. <br>  | NDCG @ 5: 0.46 |  Accuracy @ 3: 0.62 | <br>  These indicators vary from sample (which users we take), changes within 7%. <br>  Now a big datasset with several million users. <br>  We study at all classes, and we rank only those that interest us. <br>  The answer is considered correct if we predicted the class in which the user then paid. <br>  The picture is approximately as follows: </p><br><table><thead><tr><th>  Class 1 (small) </th><th>  Class 2 </th><th>  Class 3 </th><th>  Class 4 </th></tr></thead><tbody><tr><td>  almost doesn't predict it </td><td>  the class is bigger, but she almost never predicts him </td><td>  Precision = 0.45 Recall = 0.9 </td><td>  Precision = 0.27 Recall = 0.6 </td></tr></tbody></table><br><p>  <strong>Factoring (lightfm).</strong> <br>  We used it for 500.000 users as well as in Pairwise, representing for each user for each category a percentage of payments to this category. <br>  Everything can be seen from the training schedules (below is the iteration number). </p><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d516aa34371223446.png"></p><br><p><img src="https://habrastorage.org/webt/59/d6/5d/59d65d519fdd1602499651.png"></p><br><p>  From the preceding paragraphs we can draw two conclusions: </p><br><ol><li>  Our task is better solved by classification than ranking. </li><li>  A multi-class AUC is a specific thing and you should always look at how it counts. </li></ol><br><h2 id="rezultaty">  results </h2><br><p>  After further testing, we decided that CNN + LSTM works better than others. <br>  We took a big dataset and looked at how well the 5 classes we needed were predicted. <br>  There were 5 million users on the training, 1.4 million users on the test.  In order to increase the speed of learning (and it was necessary to increase it), we did not measure the intermediate quality of classification, so the training schedule will not be :(. </p><br><p>  We are interested in conversion (precision).  This metric shows the proportion of people who bought goods of this category, of the total number of people who were sent a recommendation of this category.  It is clear that the good performance of this metric does not mean that the network works well, as it can recommend very little and achieve high conversion.  Let's look at True Positive, False Positive, True Negative, False Negative to see the whole picture. </p><br><p>  Below the table with 4 elements mean the following: </p><br><table><thead><tr><th>  True negative </th><th>  False Positive </th></tr></thead><tbody><tr><td>  <strong>False negative</strong> </td><td>  <strong>True positive</strong> </td></tr></tbody></table><br><p>  Num - the number of people who actually belong to this class. </p><br><p>  <strong>Class 1</strong> : </p><br><table><thead><tr><th>  TN: 1440209 </th><th>  FP: 1669 </th></tr></thead><tbody><tr><td>  <strong>FN: 3895</strong> </td><td>  <strong>TP: 1842</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.52 </td><td>  0.32 </td><td>  0.40 </td><td>  5737 </td></tr></tbody></table><br><hr><br><p>  <strong>Class 2</strong> : </p><br><table><thead><tr><th>  TN: 1275939 </th><th>  FP: 55822 </th></tr></thead><tbody><tr><td>  <strong>FN: 43994</strong> </td><td>  <strong>TP: 71860</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.56 </td><td>  0.62 </td><td>  0.59 </td><td>  115854 </td></tr></tbody></table><br><hr><br><p>  <strong>Class 3</strong> : </p><br><table><thead><tr><th>  TN: 1136065 </th><th>  FP: 108802 </th></tr></thead><tbody><tr><td>  <strong>FN: 91882</strong> </td><td>  <strong>TP: 110866</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.50 </td><td>  0.55 </td><td>  0.52 </td><td>  202748 </td></tr></tbody></table><br><hr><br><p>  <strong>Class 4</strong> : </p><br><table><thead><tr><th>  TN: 1372454 </th><th>  FP: 18812 </th></tr></thead><tbody><tr><td>  <strong>FN: 21889</strong> </td><td>  <strong>TP: 34460</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.65 </td><td>  0.61 </td><td>  0.63 </td><td>  56349 </td></tr></tbody></table><br><hr><br><p>  <strong>Class 5</strong> : </p><br><table><thead><tr><th>  TN: 1371649 </th><th>  FP: 18816 </th></tr></thead><tbody><tr><td>  <strong>FN: 22391</strong> </td><td>  <strong>TP: 34759</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.65 </td><td>  0.61 </td><td>  0.63 </td><td>  57150 </td></tr></tbody></table><br><hr><br><p>  Since we only look at 5 classes, we also need to add the class ‚Äúother‚Äù, and, naturally, there are a lot more than the rest. </p><br><p>  <strong>Class 6 (other)</strong> : </p><br><table><thead><tr><th>  TN: 18749 </th><th>  FP: 327257 </th></tr></thead><tbody><tr><td>  <strong>FN: 2501</strong> </td><td>  <strong>TP: 1099108</strong> </td></tr></tbody></table><br><table><thead><tr><th>  precision </th><th>  recall </th><th>  f1-score </th><th>  Num </th></tr></thead><tbody><tr><td>  0.77 </td><td>  1.00 </td><td>  0.87 </td><td>  1101609 </td></tr></tbody></table><br><hr><br><p>  Our network predicts very well the classes in which the user will pay in the next 15 days, even those that are several times smaller than others (for example, class 1 compared to the 3rd class). </p><br><h2 id="sravnenie-s-predyduschim-algoritmom">  Comparison with the previous algorithm. </h2><br><p>  Comparative tests on the same data showed that our algorithm provides several times greater conversion than the previous algorithm based on crucial forests. </p><br><h2 id="itog">  Total </h2><br><p>  From our article we can draw several conclusions. <br>  1) CNN and LSTM can be used for time series recommendations. <br>  2) For a small number of classes it is better to solve the problem of classification, rather than ranking. </p><br><p>  Neural networks is an interesting topic, but it takes a lot of time and can give unexpected surprises. <br>  For example, our first Pairwise model on PyTorch worked poorly for the first week, because due to the abundance of code, somewhere in the error function, we cut off the distribution of the gradient. </p><br><p>  Finally we introduce ourselves. <br><img src="https://habrastorage.org/webt/2z/qd/dc/2zqddcemrvpegar4cn7tkrfuzdu.jpeg"><br>  From left to right: Ahmedkhan ( <a href="https://habrahabr.ru/users/Ahan/">@Ahan</a> ), Nikolai, Ivan ( <a href="https://habrahabr.ru/users/vprov/">@VProv</a> ).  Ivan and Ahmedkhan are students of the Moscow Institute of Physics and Technology and underwent an internship, while Nikolai works at QIWI. <br>  Thanks for attention! </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/339454/">https://habr.com/ru/post/339454/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../339442/index.html">How the clipboard works in Windows</a></li>
<li><a href="../339444/index.html">‚ÄúColleagues, I like everything, but ...‚Äù or how to build work with the customer</a></li>
<li><a href="../339446/index.html">Trading robot for web designers</a></li>
<li><a href="../339448/index.html">Design for iPhone X</a></li>
<li><a href="../339450/index.html">Parse BGP NOTIFICATION by RFC</a></li>
<li><a href="../339456/index.html">ZFS and KVM. @home</a></li>
<li><a href="../339458/index.html">Uneducated youth: an attempt to summarize and some personal</a></li>
<li><a href="../339460/index.html">When did Phoenix kill Reils?</a></li>
<li><a href="../339464/index.html">Photogrammetry research</a></li>
<li><a href="../339468/index.html">Google Chrome distribution knows who downloaded it</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>