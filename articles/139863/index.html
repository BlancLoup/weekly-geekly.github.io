<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recommender systems: SVD Part I</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue to talk about recommender systems. Last time we made the first attempt to determine the similarity between users and the similarity betwee...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recommender systems: SVD Part I</h1><div class="post__text post__text-html js-mediator-article">  We continue to talk about recommender systems.  Last time we made the first attempt to determine the similarity between users and the similarity between products.  Today we will come to the same task on the other hand - we will try to train the factors that characterize users and products.  If Vasya from the <a href="http://habrahabr.ru/company/surfingbird/blog/139518/">previous post</a> likes films about tractors and doesn't like films about pigs, and Peter, on the contrary, it would be great to learn to understand which films are ‚Äúabout piglets‚Äù and recommend them to Peter, and which films are ‚Äúabout tractors‚Äù, and recommend them to Vasya. <br><br><img src="http://theinternetcrashed.com/files/2010/10/Solid-State-Society-44.jpg" alt="image"><br><a name="habracut"></a><br>  I remind once again (and, apparently, I will remind until the end of time) that we have a matrix consisting of ratings (likes, purchase facts, etc.) that users (matrix rows) assigned to products (matrix columns).  Let's look at the matrix <img src="https://habrastorage.org/getpro/habr/post_images/f62/51e/c9e/f6251ec9ee67f3241437207b258d0a90.png" alt="image">  In which are recorded known to us ratings.  As a rule, one user will not be able to evaluate a significant proportion of products, and there will hardly be many products that a significant proportion of users are ready to evaluate.  This means that the matrix <i>R is</i> sparse (sparse);  Is it possible to use this somehow? <br><br>  The main tool for us will be the so-called <i>singular decomposition of the</i> matrix <i>R</i> : <br><img src="https://habrastorage.org/getpro/habr/post_images/660/3be/bfa/6603bebfab27e94c0ee6fe8d1cfccaa7.png" alt="image"><br>  Singular decomposition is a fairly simple but very powerful tool.  Actually, this is one of the main practical results of linear algebra, and the result is not very new (the properties of SVD were studied at the latest in the 1930s), and it is even more surprising when university courses in linear algebra are quite detailed. Some other aspects completely bypass the SVD side. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      If you have three minutes, you can enjoy a vivid example of mathematical humor;  humor, of course, is mathematic, so it's not that funny, but the essence is clearly stated, and the fans will surely like the music ... hmm, on a habr, it would probably be better to say that the fans of <i>Fallout</i> will like it: <br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/JEYLfIVvR9I%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253,15700255,15700259&amp;usg=ALkJrhg2mGDLH-IZvsaj9UsSkpCK6Fq1Hw" frameborder="0" allowfullscreen=""></iframe><br><br>  In case there are still no three minutes, I will single out the property that is most important for us: if <i>R</i> is a large matrix <img src="https://habrastorage.org/getpro/habr/post_images/25c/880/a65/25c880a654cea774d32445dc20156377.png" alt="image">  but of small rank <i>f</i> (in particular, sparse matrices are often of small rank), it can be decomposed into a product of the matrix <img src="https://habrastorage.org/getpro/habr/post_images/2b8/b8b/edf/2b8b8bedfbf580c7bd70d427bcbf80bf.png" alt="image">  and matrices <img src="https://habrastorage.org/getpro/habr/post_images/483/7f2/5c5/4837f25c57616012077afd7fb11f305b.png" alt="image">  thus sharply reducing the number of parameters, from <i>NM</i> to ( <i>N</i> + <i>M</i> ) <i>f</i> (to understand that this is a big progress, imagine that, as is usually the case in practice, <i>N</i> and <i>M</i> are measured in hundreds of thousands and millions, and <i>f is</i> less than ten ). <br><br>  SVD is very widely used in machine learning;  in fact, if you want to bring something closer, it is possible that SVD will meet you somewhere along the way.  A classic example of using SVD is noise reduction, for example, in images.  Consider a (black and white) image as a matrix of size <i>X</i> <img src="https://habrastorage.org/getpro/habr/post_images/25c/880/a65/25c880a654cea774d32445dc20156377.png" alt="image">  whose elements are the intensities of each pixel.  Now we will try to select <i>f</i> columns of pixels from the image, find them ‚Äúrepresentative‚Äù and present each of the remaining columns as a linear combination of these.  I will not go into mathematics right now, but as a result, when you find the optimal representations of each column, it turns out that you presented the original matrix in the form of a work <img src="https://habrastorage.org/getpro/habr/post_images/d73/cad/449/d73cad449c4b5637e26bdfe842c6f363.png" alt="image">  size matrices <img src="https://habrastorage.org/getpro/habr/post_images/2b8/b8b/edf/2b8b8bedfbf580c7bd70d427bcbf80bf.png" alt="image">  and <img src="https://habrastorage.org/getpro/habr/post_images/483/7f2/5c5/4837f25c57616012077afd7fb11f305b.png" alt="image">  , that is, they just approximated the matrix <i>X with a</i> matrix of small rank <i>f</i> . <br><br>  The main property of SVD is that SVD gives the <i>optimal</i> approximation, if in matrix <i>D</i> you just leave exactly <i>f of the</i> first diagonal elements, and the rest are zero: <br><img src="https://habrastorage.org/getpro/habr/post_images/843/2e5/6c0/8432e56c04a2fbf57fe1f95de97c4eb5.png" alt="image"><br>  In the diagonal matrix <i>D</i> , which is in the middle of the singular value decomposition, the elements are ordered by size: <img src="https://habrastorage.org/getpro/habr/post_images/853/c37/d05/853c37d054aacbc8b1f057dd7b6b75db.png" alt="image">  , so to nullify the last elements is to nullify the smallest elements. <br><br>  If you choose a good <i>f</i> , then as a result, the image and weight will lose, and the noise on it will be less.  And <i>f</i> in this case can be selected on the basis of the size of the singular values ‚Äã‚Äãof the matrix, i.e.  those diagonal elements of the matrix <i>D</i> : it is desirable to discard as much as possible, but at the same time as small as possible of such elements. <br><br>  But we digress.  In the case of recommender systems, it turns out that we represent each user with a vector of <i>f</i> factors <img src="https://habrastorage.org/getpro/habr/post_images/6c8/525/cc8/6c8525cc809870cc5c209d8e6e02a49e.png" alt="image">  and present each product as a vector of <i>f</i> factors <img src="https://habrastorage.org/getpro/habr/post_images/239/5c7/7ef/2395c77ef02bbee7cf4ee885ee43ee63.png" alt="image">  and then, to predict the rating of user <i>i to</i> product <i>j</i> , we take their scalar product <img src="https://habrastorage.org/getpro/habr/post_images/1da/3b3/271/1da3b3271c61cbe0a7c42bbf9bb65779.png" alt="image">  .  It can be said that a vector of user factors shows how much a user likes or dislikes a particular factor, and a vector of product factors shows how much one or another factor in a product is expressed.  Linear algebra, on the other hand, tells us that for a sparse rating matrix such decomposition is often possible and has meaningful meaning. <br><br>  It may turn out, by the way, that some factors will be easily understood by the human mind: for films, something like ‚Äúcomedy ‚Äì drama‚Äù, ‚Äúaction share‚Äù, ‚Äúromance share‚Äù, etc., can be distinguished, and the factors of users accordingly, they will show how appropriate the characteristics of the film to their taste.  But nothing substantive can stand out - there are no guarantees here, formally we simply juggle with numbers. <br><br>  Next time we will turn this basic idea - to bring the rating matrix of a small rank closer by SVD - into a well-posed optimization task and learn how to solve it. </div><p>Source: <a href="https://habr.com/ru/post/139863/">https://habr.com/ru/post/139863/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../139854/index.html">"Yandex" introduced the navigation service</a></li>
<li><a href="../139857/index.html">Search engine indexing in Evernote</a></li>
<li><a href="../139858/index.html">Asterisker's Notes - Asterisk based PBX</a></li>
<li><a href="../139860/index.html">DevBar - a pub for employees of the IT-industry and not only (St. Petersburg)</a></li>
<li><a href="../139862/index.html">Amazon Web Services + Ubuntu Server Cloud Guest</a></li>
<li><a href="../139867/index.html">Microscopic 3D printing at 5 meters per second</a></li>
<li><a href="../139868/index.html">Clarification of the great quad-core confusion with Apple A5X</a></li>
<li><a href="../139870/index.html">Once again about the skiplist ...</a></li>
<li><a href="../139871/index.html">Photo gallery on Django using Google Picasa as a hosting</a></li>
<li><a href="../139873/index.html">Tele2 and Radio Research Institutes begin two-week LTE testing in Omsk</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>