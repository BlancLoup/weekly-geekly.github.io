<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Nine Elasticsearch rakes that I stepped on</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="‚ÄúA trained person is also stepping on a rake. 
 But on the other hand, there is a pen. ‚Äù 

 Elasticsearch is a great tool, but each tool requires not ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Nine Elasticsearch rakes that I stepped on</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/ap/2k/jc/ap2kjcsehhaliahrmgg6a3r27xw.jpeg" alt="Illustration by Anton Gudim"><br><br><br>  <i>‚ÄúA trained person is also stepping on a rake.</i> <i><br></i>  <i>But on the other hand, there is a pen. ‚Äù</i> <br><br>  Elasticsearch is a great tool, but each tool requires not only <a href="https://habr.com/company/yamoney/blog/328018/">adjustment</a> and <a href="https://habr.com/company/yamoney/blog/358550/">maintenance</a> , but also attention to detail.  Some are insignificant and lie on the surface, while others are hidden so deeply that it will take more than a day, not a dozen coffee cups and more than one kilometer of nerves to search.  In this article I will tell about nine wonderful rakes in the setting of the elastic, which I stepped on. <br><a name="habracut"></a><br>  I will arrange a rake descending evidence.  From those that can be foreseen and circumvented at the stage of setting up and entering a cluster in the production state, to very strange, but bringing the most extensive experience (and the stars in the eyes). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h2>  Data nodes must be the same. </h2><br>  ‚ÄúCluster works at the speed of the slowest data node‚Äù is a suffered axiom.  But there is another obvious point that is not related to performance: elastic does not think in disk space, but in shards, and tries to evenly distribute them among data nodes.  If there is more space on one of the data nodes than on others, then it will be useless to stand idle. <br><br><h2>  Deprecation.log </h2><br>  It may happen that someone does not use the most modern means of sending data to the elastic, which does not know how to set the Content-Type when executing requests.  In this list, for example, heka, or when logs leave devices with their built-in tools).  In this case, the deprecation.  The log begins to grow at an awesome rate, and for each request, the following lines appear in it: <br><br><pre><code class="hljs markdown">[<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,659</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,670</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,671</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,673</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController</span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [<span class="hljs-string"><span class="hljs-string">Content-Type</span></span>] header. [<span class="hljs-string"><span class="hljs-string">2018-07-07T14:10:26,677</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">WARN </span></span>][<span class="hljs-string"><span class="hljs-string">oedrRestController </span></span>] Content type detection for rest requests is deprecated. Specify the content type using the [Content-Type] header.</code> </pre> <br>  Requests come, on average, every 5-10 ms - and each time a new line is added to the log.  This adversely affects the performance of the disk subsystem and increases iowait.  Deprecation.log can be turned off, but this is not very reasonable.  To collect the elastic logs into it, but not to allow littering, I disable only the logs of the oedrRestController class. <br><br>  To do this, add the following structure to logs4j2.properties: <br><br><pre> <code class="hljs pgsql">logger.restcontroller.name = org.elasticsearch.deprecation.rest.RestController logger.restcontroller.<span class="hljs-keyword"><span class="hljs-keyword">level</span></span> = error</code> </pre><br>  It will raise the logs of this class to the error level, and they will stop falling into deprecation.log. <br><br><h2>  .kibana </h2><br>  What does the normal cluster installation process look like?  We put the nodes, combine them into a cluster, put the x-pack (who needs it), and of course, Kibana.  We start, check that everything works and Kibana sees the cluster, and continue tuning.  The problem is that on a freshly installed cluster, the default template looks like this: <br><br><pre> <code class="hljs json">{ <span class="hljs-attr"><span class="hljs-attr">"default"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"order"</span></span>: <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-attr"><span class="hljs-attr">"template"</span></span>: <span class="hljs-string"><span class="hljs-string">"*"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"settings"</span></span>: { <span class="hljs-attr"><span class="hljs-attr">"number_of_shards"</span></span>: <span class="hljs-string"><span class="hljs-string">"1"</span></span>, <span class="hljs-attr"><span class="hljs-attr">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"0"</span></span> } }, <span class="hljs-attr"><span class="hljs-attr">"mappings"</span></span>: {}, <span class="hljs-attr"><span class="hljs-attr">"aliases"</span></span>: {} }</code> </pre> <br>  And the .kibana index where all settings are stored is created in a single copy. <br><br>  Somehow there was a case when due to a hardware failure, one of the data nodes in the cluster was killed.  He quickly came to a consistent state by raising replicas of the shard from neighboring data-nodes, but, by a happy coincidence, it was on this data-node that the only shard with an index of .kibana was located.  The situation is stalemate - the cluster is alive, in working condition, and Kibana is in red-status, and my phone is torn from the calls of employees who urgently need their logs. <br><br>  All this is solved simply.  Nothing has fallen yet: <br><br><pre> <code class="hljs objectivec">XPUT .kibana/_settings { <span class="hljs-string"><span class="hljs-string">"index"</span></span>: { <span class="hljs-string"><span class="hljs-string">"number_of_replicas"</span></span>: <span class="hljs-string"><span class="hljs-string">"&lt;__&gt;"</span></span> } }</code> </pre> <br><h2>  XMX / XMS </h2><br>  The <a href="https://www.elastic.co/guide/en/elasticsearch/guide/master/_limiting_memory_usage.html">documentation</a> says - "No more than 32 GB", and rightly so.  But it is also correct that you do not need to set it in the service settings. <br><pre> <code class="hljs diff"><span class="hljs-deletion"><span class="hljs-deletion">-Xms32g -Xmx32g</span></span></code> </pre> <br>  Because it is already more than 32 gigabytes, and here we run into an interesting nuance of Java memory.  Above a certain limit, Java ceases to use compressed pointers and begins to consume an unreasonably large amount of memory.  It‚Äôs easy to check if compressed pointers use a Java machine running Elasticsearch.  We look at the service log: <br><br><pre> <code class="hljs markdown">[<span class="hljs-string"><span class="hljs-string">2018-07-29T15:04:22,041</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">INFO</span></span>][<span class="hljs-string"><span class="hljs-string">oeeNodeEnvironment</span></span>][<span class="hljs-symbol"><span class="hljs-symbol">log-elastic-hot3</span></span>] heap size [31.6gb], compressed ordinary object pointers [true]</code> </pre> <br>  The amount of memory that should not be exceeded depends, among other things, on the version of Java being used.  To calculate the exact amount in your case - see the <a href="https://www.elastic.co/guide/en/elasticsearch/guide/master/heap-sizing.html">documentation</a> . <br><br>  I now have all the elastic data-nodes installed: <br><br><pre> <code class="hljs diff"><span class="hljs-deletion"><span class="hljs-deletion">-Xms32766m -Xmx32766m</span></span></code> </pre> <br>  It seems to be a banal fact, and the documentation is well described, but I regularly encounter installations with Elasticsearch, where this moment is missed, and Xms / Xmx are exposed in 32g. <br><br><h2>  / var / lib / elasticsearch </h2><br>  This is the default path for storing data in elasticsearch.  yml: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch</code> </pre> <br>  There I usually mount one large RAID array, and here's why: we specify ES several ways to store data, for example, like this: <br><br><pre> <code class="hljs kotlin">path.<span class="hljs-keyword"><span class="hljs-keyword">data</span></span>: /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data1, /<span class="hljs-keyword"><span class="hljs-keyword">var</span></span>/lib/elasticsearch/data2</code> </pre> <br>  In data1 and data2, different disks or raid arrays are mounted.  But elastic does not balance and does not distribute the load between these paths.  First, he fills one section, then he starts writing to another, so the load on the storage will be uneven.  Knowing this, I made an unequivocal decision - I combined all the disks in RAID0 / 1 and mounted it in the path specified in path.data. <br><br><h2>  available_processors </h2><br>  And no, I mean not the processors on the ingest nodes.  If you look at the properties of the running node (via the _nodes API), you can see something like this: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>. { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-87-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span> }</code> </pre> <br>  It is seen that the node is running on a host with 28 cores, and the elastic correctly determined their number and started at all.  But if there are more than 32 cores, then sometimes it‚Äôs like this: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"os"</span></span>: { <span class="hljs-string"><span class="hljs-string">"refresh_interval_in_millis"</span></span>: <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-string"><span class="hljs-string">"name"</span></span>: <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"arch"</span></span>: <span class="hljs-string"><span class="hljs-string">"amd64"</span></span>, <span class="hljs-string"><span class="hljs-string">"version"</span></span>: <span class="hljs-string"><span class="hljs-string">"4.4.0-116-generic"</span></span>, <span class="hljs-string"><span class="hljs-string">"available_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">72</span></span>, <span class="hljs-string"><span class="hljs-string">"allocated_processors"</span></span>: <span class="hljs-number"><span class="hljs-number">32</span></span> }</code> </pre> <br>  It is necessary to forcibly set the number of processors available to the service - this has a good effect on node performance. <br><br><pre> <code class="hljs">processors: 72</code> </pre> <br><h2>  thread_pool.bulk.queue_size </h2><br>  In the section about thread_pool.bulk.rejected of the previous <a href="https://habr.com/company/yamoney/blog/358550/">article there</a> was such a metric - a count of the number of refusals for requests to add data. <br><br>  I wrote that the growth of this indicator is a very bad sign, and the developers recommend not setting up thread pools, but adding new nodes to the cluster - supposedly, this solves performance problems.  But the rules are needed in order to sometimes break them.  Yes, and it does not always work to ‚Äúthrow a problem with iron,‚Äù so one of the measures to deal with failures in bulk requests is to increase the size of this queue. <br><br>  By default, the queue settings look like this: <br><br><pre> <code class="hljs objectivec"><span class="hljs-string"><span class="hljs-string">"thread_pool"</span></span>: { <span class="hljs-string"><span class="hljs-string">"bulk"</span></span>: { <span class="hljs-string"><span class="hljs-string">"type"</span></span>: <span class="hljs-string"><span class="hljs-string">"fixed"</span></span>, <span class="hljs-string"><span class="hljs-string">"min"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"max"</span></span>: <span class="hljs-number"><span class="hljs-number">28</span></span>, <span class="hljs-string"><span class="hljs-string">"queue_size"</span></span>: <span class="hljs-number"><span class="hljs-number">200</span></span> } }</code> </pre> <br>  The algorithm is as follows: <br><br><ol><li>  We collect statistics on the average queue size during the day (the instantaneous value is stored in thread_pool.bulk.queue); </li><li>  Carefully increase the queue_size to a size slightly larger than the average size of the active queue - because a failure occurs when it is exceeded; </li><li>  Increasing the size of the pool is not necessary, but acceptable. </li></ol><br>  To do this, add something like this to the host‚Äôs settings (you will, of course, have your own values): <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.size</span></span>: 32 <span class="hljs-selector-tag"><span class="hljs-selector-tag">thread_pool</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.bulk</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.queue_size</span></span>: 500</code> </pre> <br>  And after restarting the node, be sure to monitor the load, I / O, memory consumption.  and all that is possible to roll back the settings, if necessary. <br><br>  <i>Important: these settings make sense only on the nodes working on receiving new data.</i> <br><br><h2>  Pre-index creation </h2><br>  As I said in the first <a href="https://habr.com/company/yamoney/blog/328018/">article of the</a> cycle, we use Elasticsearch to store the logs of all microservices.  The point is simple - one index stores the logs of one component in one day. <br><br>  From this it follows that new indices are created every day by the number of microservices - so earlier every night the elastic fell into clinch for about 8 minutes, until a hundred new indices were created, several hundred new shards, the disk load schedule went off the shelf, the queues grew to send logs to elastic on hosts, and Zabbix flourished with alerts like a Christmas tree. <br><br>  To avoid this, a common Python script was written for preliminary index creation.  The script works like this: it finds the indexes for today, extracts their mappings and creates new indexes with the same mappings, but one day in advance.  Works on cron, runs in those hours when the Elastic is least loaded.  The script uses the elasticsearch library and is available on <a href="https://github.com/adel-s/ElasticSearch/blob/master/es_precreate_indices.py">GitHub</a> . <br><br><h2>  Transparent Huge Pages </h2><br>  Once we discovered that the elastic nodes that are working to receive data began to hang under load during peak hours.  And with very strange symptoms: the use of all processor cores drops to zero, but nevertheless the service hangs in memory, regularly listens to the port, does nothing, does not respond to requests, and after some time falls out of the cluster.  The systemctl restart service does not respond.  Only good old kill ‚àí9 helps. <br><br>  This is not caught by standard monitoring tools; on the charts, until the moment of the fall, the nominal picture is empty in the service logs.  The memory dump of the java-machine at this moment could not be done either. <br><br>  But, as they say, "we are professionals, so after some time they googled the decision."  A similar problem was highlighted in the <a href="https://discuss.elastic.co/t/elasticsearch-5-4-2-process-periodically-dying-with-high-cpu-load-and-kernel-message-pgtable-generic-c-33-bad-pmd/92239/2">discussion.elastic.co</a> thread and turned out to be a bug in the kernel related to tranparent huge pages.  It was decided to turn off the thp in the kernel using the sysfsutils package. <br><br>  Check if transparent huge pages are turned on for you: <br><br><pre> <code class="hljs pgsql">cat /sys/kernel/mm/transparent_hugepage/enabled <span class="hljs-keyword"><span class="hljs-keyword">always</span></span> madvise [never]</code> </pre> <br>  If there is always [always] - you are potentially in danger. <br><br><h2>  Conclusion </h2><br>  These are the main rakes (in fact there were, of course, more), which I happened to step on in one and a half years of work as an administrator of the Elasticsearch cluster.  I hope this information will be useful to you on the difficult and mysterious path to the ideal cluster Elasticsearch. <br><br>  And for the illustration, thanks to Anton Gudim - there is still a lot of good in his <a href="https://www.instagram.com/gudim_public/">instagram</a> . </div><p>Source: <a href="https://habr.com/ru/post/419041/">https://habr.com/ru/post/419041/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../419029/index.html">Fortnite has become a social phenomenon. Parents increasingly hire trainers for their children and play with them</a></li>
<li><a href="../419033/index.html">A quick note on running vue.js in the kubernetes cluster</a></li>
<li><a href="../419035/index.html">The book "Head First Agile. Flexible project management ¬ª</a></li>
<li><a href="../419037/index.html">PPPOS implementation on stm32f4-discovery</a></li>
<li><a href="../419039/index.html">"Sberbank" presented a virtual woman and a contactless beer tap</a></li>
<li><a href="../419043/index.html">The elusive problem of frame timing</a></li>
<li><a href="../419045/index.html">Yandex reopens the set in the Interface Development School. Examples of lectures and homework from the previous set</a></li>
<li><a href="../419047/index.html">Reddit hacked, database with passwords and email for 2005-2007 leaked</a></li>
<li><a href="../419049/index.html">GeekBrains launches free online educational marathon "Find Yourself in Digital"</a></li>
<li><a href="../419051/index.html">How "Flant" helps newcomers</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>