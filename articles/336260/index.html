<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Swift Mutexes and Capture</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Translation of an article by Matt Gallager . 

 This article will focus on the lack of threading (threading) and thread synchronization tools in Swift...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Swift Mutexes and Capture</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/fbb/bce/eb1/fbbbceeb15c940ba9d81ea140aa98700.png"><br><br>  <i>Translation of an <a href="https://www.cocoawithlove.com/blog/2016/06/02/threads-and-mutexes.html">article by Matt Gallager</a> .</i> <br><br>  This article will focus on the lack of threading (threading) and thread synchronization tools in Swift.  We will discuss the proposal to implement ‚Äúmultithreading‚Äù (concurrency) in Swift and how, before this opportunity appears, streaming execution in Swift will imply the use of traditional mutexes and the shared mutable state. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It is not particularly difficult to use a mutex in Swift, but against this background I would like to highlight the subtle nuances of performance in Swift - dynamic memory allocation during the capture of closures.  We want our mutex to be fast, but passing a circuit for execution inside a mutex can reduce performance by 10 times due to the memory overhead.  Let's look at several ways to solve this problem. <br><a name="habracut"></a><br><h2>  Absence of stream execution (threading) in Swift </h2><br>  When Swift was first announced in June 2014, he had two obvious omissions: <br><br><ul><li>  error processing, </li><li>  streaming execution and synchronization of threads. </li></ul><br>  Error handling was implemented in Swift 2 and was one of the key features of this release. <br><br>  And streaming execution for the most part is still ignored by Swift.  Instead of language-based streaming support, Swift includes the <code>Dispatch</code> module (libdispatch, aka Grand Central Dispatch) on all platforms, and implicitly suggests that we use <code>Dispatch</code> instead of expecting help from the language. <br><br>  Delegating the responsibility of the library that comes with the delivery seems particularly strange compared to other modern languages, such as Go and Rust, in which streaming execution primitives and strict thread safety (respectively) have become the main features of their languages.  Even the <code>@synchronized</code> and <code>atomic</code> properties in Objective-C seem like a generous offer compared to the lack of something similar in Swift. <br><br>  What is the reason for such an obvious omission in this language? <br><br><h2>  Future "multithreading" in Swift </h2><br>  The answer is briefly discussed in the <a href="">proposal for the introduction of "multithreading" in the Swift repository</a> . <br><br>  <i>I mention this proposal to emphasize that Swift developers in the future would like to do something regarding multi-threading, but please keep in mind what Swift developer Joe Groff says: ‚Äúthis document is just a proposal, not an official statement of direction development. "</i> <br><br>  This sentence appeared to describe a situation where, for example, in <a href="http://homes.cs.washington.edu/~djg/papers/cycthreads.pdf">Cyclone</a> or <a href="http://blog.rust-lang.org/2015/04/10/Fearless-Concurrency.html">Rust,</a> links cannot be divided between the execution threads.  Regardless of whether the result is similar to these languages, it seems that Swift plans to eliminate the common memory of streams, with the exception of types implementing <code>Copyable</code> and transmitted through strictly controlled channels (in the sentence they are called <code>Stream</code> 's).  A kind of coroutine will also appear (in a sentence called <code>Task</code> 's), which will behave like asynchronous dispatch blocks that can be paused / resumed. <br><br>  Further in the sentence it is stated that in <b>libraries</b> on top of the <code>Stream / Task / Copyable</code> primitives the most common <b>language</b> means of stream execution can be implemented (by analogy with <code>chan</code> in Go, <code>async / await</code> in .NET, <code>actor</code> in Erlang). <br><br>  Sounds good, but when to expect multithreading in Swift?  In Swift 4?  In Swift 5?  Not soon. <br><br>  Thus, now it does not help us, but rather even hinders. <br><br><h2>  The impact of future functions on the current library </h2><br>  The problem is that Swift avoids the use of simple multi-threaded primitives in a language, or thread-safe versions of language functions on the grounds that they will be replaced or eliminated by some future means. <br><br>  You can find this evidence by reading the Swift-Evolution mailing list: <br><br><ul><li>  <a href="https://github.com/apple/swift/pull/1454">References to objects (both strong and weak) are not defined "if there is read / write, write / write or anything / destroy data in the race variable"</a> .  There is no intention to change this behavior or to suggest an integrated ‚Äúatomic‚Äù approach, since this is ‚Äúone of the few vague rules of behavior that we adopt‚Äù.  A possible ‚Äúcorrection‚Äù of this indefinite behavior will be a new multithreading model. </li><li>  The resulting types (or other throws (throws) other than functional interfaces) would be useful for numerous ‚Äúcontinuation passing style‚Äù algorithms and would be carefully discussed, but ultimately they will be ignored until until Swift ‚Äúprovides proper language support [for coroutines or asynchronous promises]‚Äù as part of changes in multithreading. </li></ul><br><h2>  We are trying to find a fast general purpose mutex </h2><br>  In short: if we need multi-threaded behavior, then we need to build it ourselves using existing streaming tools and mutex properties. <br><br>  Standard <a href="http://stackoverflow.com/questions/24045895/what-is-the-swift-equivalent-to-objective-cs-synchronized">mutex advice in Swift</a> : use <code>DispatchQueue</code> and call <code>sync</code> for it. <br><br>  I like libdispatch, but in most cases using <code>DispatchQueue.sync</code> as a mutex is the slowest way to fix the problem, more than <b>an order of magnitude</b> slower than other solutions due to the inevitable closure capture cost passed to the <code>sync</code> function.  This is due to the fact that the closure of the mutex must capture the surrounding state (in particular, capture the link to the protected resource), and this capture implies the use of the context of the closure located in the dynamic memory.  Until Swift is able to optimize insulating (non-escaping) closures in the stack, the only way to avoid the extra cost of putting closures into dynamic memory is to make sure they are embedded.  Unfortunately, this is not possible within module boundaries, such as the Dispatch module boundaries.  This makes <code>DispatchQueue.sync</code> unnecessarily slow Swift mutex. <br><br>  The next proposed option in frequency can be considered <code>objc_sync_enter</code> / <code>objc_sync_exit</code> .  Being 2-3 times faster than libdispatch, it is still a bit slower than the ideal (because it is always a re-entrant mutex), and depends on runtime Objective-C (therefore limited to the Apple platform). <br><br>  The fastest option for a mutex, <code>OSSpinLock</code> is more than 20 times faster than <code>dispatch_sync</code> .  In addition to the general limitations of spinlock (high CPU load, if several threads try to enter at the same time), <a href="https://lists.swift.org/pipermail/swift-dev/Week-of-Mon-20151214/000372.html">iOS has serious problems that make it completely unsuitable for use on this platform.</a>  <a href="https://lists.swift.org/pipermail/swift-dev/Week-of-Mon-20151214/000372.html">Accordingly, it can only be used on Mac</a> . <br><br>  If you are targeting iOS 10 or macOS 10.12, or something newer, you can use <code>os_unfair_lock_t</code> .  This performance decision should be close to <code>OSSpinLock</code> , while devoid of its most serious problems.  However, this lock is not a FIFO.  Instead, a mutex is granted to an arbitrary peer (hence, unfiar).  You need to decide whether this is a problem for your program, although in general it means that this option should not be your first choice for a general purpose mutex. <br><br>  All these problems make <code>pthread_mutex_lock</code> / <code>pthread_mutex_unlock</code> only reasonable, productive and portable option. <br><br><h2>  Capture Mutexes and Pitfalls </h2><br>  Like most things in a pure C language, <code>pthread_mutex_t</code> has a rather clumsy interface that helps to use the Swift wrapper (especially for building and automatic cleaning).  In addition, it is useful to have a ‚Äúscoped‚Äù mutex - which accepts the function and executes it inside the mutex, providing a balanced ‚Äúlock‚Äù and ‚Äúunlocking‚Äù from either side of the function. <br><br>  <code>PThreadMutex</code> call our wrapper <code>PThreadMutex</code> .  The following is the implementation of the simple scoped-mutex function in this wrapper: <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync</span></span></span><span class="hljs-function">&lt;R&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">()</span></span></span></span></span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span>) -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span> { pthread_mutex_lock(&amp;m) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { pthread_mutex_unlock(&amp;m) } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> execute() }</code> </pre> <br>  It should work quickly, but it is not.  Do you see why? <br><br>  The problem arises from the implementation of reusable functions, such as the one presented in a separate <code>CwlUtils</code> module.  This leads to exactly the same problem as in the case of <code>DispatchQueue.sync</code> : locking by grab leads to the allocation of dynamic memory.  Due to the overhead during this process, the function will work more than 10 times slower than needed (3.124 seconds for 10 million calls, compared to the ideal 0.263 seconds). <br><br>  What exactly is "captured"?  Let's consider the following example: <br><br><pre> <code class="hljs 1c">mutex.sync { doSomething(<span class="hljs-meta"><span class="hljs-meta">&amp;protectedMutableState) }</span></span></code> </pre> <br>  To do something useful inside a mutex, the reference to <code>protectedMutableState</code> must be stored in a "closure context", which is data that is in dynamic memory. <br><br>  This may seem harmless enough (in the end, the seizure is what the closures do).  But if the <code>sync</code> function cannot be built into what calls it (because it is in another module or file, and the optimization of the entire module is turned off), then the dynamic memory will be allocated during the capture. <br><br>  And we don't want that.  To avoid this, pass the closure to the appropriate parameter instead of capturing it. <br><br>  <b>WARNING</b> : The following few code examples are becoming more and more ridiculous, and in most cases I suggest not following them.  I do this to demonstrate the depth of the problem.  Read the chapter ‚ÄúAnother Approach‚Äù to see what I use in practice. <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync_2</span></span></span><span class="hljs-function">&lt;T&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">_</span></span></span></span><span class="hljs-function"><span class="hljs-params"> p: </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span><span class="hljs-function"><span class="hljs-params"> T, execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"> T)</span></span></span></span></span></span> -&gt; <span class="hljs-type"><span class="hljs-type">Void</span></span>) { pthread_mutex_lock(&amp;m) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { pthread_mutex_unlock(&amp;m) } execute(&amp;p) }</code> </pre> <br>  That's better ... now the function works at full speed (0.282 seconds for a test of 10 million calls). <br><br>  We solved the problem using the values ‚Äã‚Äãpassed by the functions.  A similar problem occurs with the return of the result.  Next feature: <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync_3</span></span></span><span class="hljs-function">&lt;T, R&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">_</span></span></span></span><span class="hljs-function"><span class="hljs-params"> p: </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span><span class="hljs-function"><span class="hljs-params"> T, execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"> T)</span></span></span></span></span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span>) -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span> { pthread_mutex_lock(&amp;m) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { pthread_mutex_unlock(&amp;m) } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> execute(&amp;p) }</code> </pre> <br>  shows the same low speed of the original, even when the closure does not capture anything (at 1.371 seconds the speed drops even lower).  To handle its result, this closure performs dynamic memory allocation. <br><br>  We can fix this by adding an <code>inout</code> parameter to the result. <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync_4</span></span></span><span class="hljs-function">&lt;T, U&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">_</span></span></span></span><span class="hljs-function"><span class="hljs-params"> p1: </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span><span class="hljs-function"><span class="hljs-params"> T, </span></span><span class="hljs-number"><span class="hljs-function"><span class="hljs-params"><span class="hljs-number">_</span></span></span></span><span class="hljs-function"><span class="hljs-params"> p2: </span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span><span class="hljs-function"><span class="hljs-params"> U, execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">(</span></span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"> T, </span></span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"><span class="hljs-keyword">inout</span></span></span></span></span><span class="hljs-function"><span class="hljs-params"><span class="hljs-params"> U)</span></span></span></span></span></span> -&gt; <span class="hljs-type"><span class="hljs-type">Void</span></span>) -&gt; <span class="hljs-type"><span class="hljs-type">Void</span></span> { pthread_mutex_lock(&amp;m) execute(&amp;p, &amp;p2) pthread_mutex_unlock(&amp;m) }</code> </pre><br>  and cause so <br><br><pre> <code class="hljs perl">// ,  <span class="hljs-string"><span class="hljs-string">`mutableState`</span></span>  <span class="hljs-string"><span class="hljs-string">`result`</span></span>  ,       mutex.sync_4(&amp;mutableState, &amp;result) { $1 = doSomething($0) }</code> </pre> <br>  We are back to full speed, or close enough to it (0.307 seconds for 10 million calls). <br><br><h2>  Another approach </h2><br>  One advantage of capturing a closure is how easy it seems.  The elements inside the grip have the same <b>names</b> inside and outside the closure, and the connection between them is obvious.  When we avoid capturing a closure, and instead try to pass all the values ‚Äã‚Äãas parameters, we have to either rename all of our variables or give them shadow names ‚Äî which does not contribute to the simplicity of understanding ‚Äî and we still risk accidentally capturing the variable again degrading performance. <br><br>  Let's put everything aside and solve the problem differently. <br><br>  We can create a free <code>sync</code> function in our file that takes a mutex as a parameter: <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync</span></span></span><span class="hljs-function">&lt;R&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(mutex: PThreadMutex, execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">()</span></span></span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">throws</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">rethrows</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span> { pthread_mutex_lock(&amp;mutex.m) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { pthread_mutex_unlock(&amp;mutex.m) } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> execute() }</code> </pre><br>  If you put the function in the file from which it will be called, then everything <b>almost</b> works.  We get rid of the cost of dynamic memory allocation, while the speed of execution drops from 3.043 to 0.374 seconds.  But we still have not reached a level of 0.263 seconds, as with a direct call to <code>pthread_mutex_lock</code> / <code>pthread_mutex_unlock</code> .  What's wrong again? <br><br>  It turns out that despite the presence of a private function in the same file, where Swift can fully inline this function, Swift does not eliminate the redundant retention and release of the <code>PThreadMutex</code> parameter (the type of which is <code>class</code> so that <code>pthread_mutex_t</code> does not break during copying). <br><br>  We can force the compiler to avoid these deductions and exemptions by making the function an extension of <code>PThreadMutex</code> , rather than a free function: <br><br><pre> <code class="hljs swift"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">extension</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">PThreadMutex</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync</span></span></span><span class="hljs-function">&lt;R&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">()</span></span></span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">throws</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">rethrows</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span> { pthread_mutex_lock(&amp;m) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { pthread_mutex_unlock(&amp;m) } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> execute() } }</code> </pre> <br>  This causes Swift to handle the <code>self</code> parameter as <code>@guaranteed</code> , eliminating hold / release costs, and we finally reach a value of 0.264 seconds. <br><br><h2>  Semaphores, not mutexes? </h2><br>  Why not use <code>dispatch_semaphore_t</code> ?  The advantage of <code>dispatch_semaphore_wait</code> and <code>dispatch_semaphore_signal</code> is that they do not require a closure - these are separate, unscoped calls. <br><br>  You can use <code>dispatch_semaphore_t</code> to create constructs like a mutex: <br><br><pre> <code class="hljs swift"><span class="hljs-keyword"><span class="hljs-keyword">public</span></span> <span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">struct</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">DispatchSemaphoreWrapper</span></span></span><span class="hljs-class"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">let</span></span> s = <span class="hljs-type"><span class="hljs-type">DispatchSemaphore</span></span>(value: <span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">init</span></span>() {} <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">func</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sync</span></span></span><span class="hljs-function">&lt;R&gt;</span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(execute: </span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params"><span class="hljs-params">()</span></span></span></span></span></span> <span class="hljs-keyword"><span class="hljs-keyword">throws</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">rethrows</span></span> -&gt; <span class="hljs-type"><span class="hljs-type">R</span></span> { <span class="hljs-number"><span class="hljs-number">_</span></span> = s.wait(timeout: <span class="hljs-type"><span class="hljs-type">DispatchTime</span></span>.distantFuture) <span class="hljs-keyword"><span class="hljs-keyword">defer</span></span> { s.signal() } <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-keyword"><span class="hljs-keyword">try</span></span> execute() } }</code> </pre> <br>  It turns out that this is about a third <b>faster than the</b> <code>pthread_mutex_lock</code> / <code>pthread_mutex_unlock</code> mutex (0.168 seconds versus 0.244).  But, despite the increase in speed, using a semaphore for a mutex is not the best option for a common mutex. <br><br>  Semaphores are prone to a series of errors and problems.  The most serious of these are forms of <a href="https://en.wikipedia.org/wiki/Priority_inversion">priority inversion</a> .  Priority inversion is the same type of problem that caused <code>OSSpinLock</code> be used for iOS, but the problem for semaphores is a bit more complicated. <br><br>  With spinlock, priority inversion means: <br><br><ol><li>  The high priority thread is active, spinning (spinning), and waiting for the lock to be held by the lower priority thread. </li><li>  A low priority thread never removes a lock because it is depleted by a higher priority thread. </li></ol><br>  With a semaphore, priority inversion means: <br><br><ol><li>  A high priority thread is waiting for a semaphore. </li><li>  The medium priority stream does not depend on the semaphore. </li><li>  It is expected that the low priority stream signals the semaphore that the high priority stream can continue. </li></ol><br>  A medium priority thread will deplete a low priority (this is normal for thread priority).  But since the high-priority stream is waiting for the low-priority stream to signal with a semaphore, the high-priority stream is <b>also being</b> depleted by the medium-priority one.  Ideally, this should not happen. <br><br>  If the correct mutex was used instead of the semaphore, then the priority of the high-priority thread will be transferred to the stream with a lower priority while the high-priority waits for the mutex held by the low-priority thread - this allows the low-priority thread to complete its work and unlock the high-priority thread.  However, semaphores are not held by the stream, so priority transmission cannot occur. <br><br>  Ultimately, semaphores are a good way to bind notifications of terminations between threads (something that is not easy with mutexes), but the construction of semaphores is complex and carries risks, so their use should be limited to situations where you know all the threads in advance and their priorities - when it is known that the priority of the waiting thread is equal to or <b>lower than the</b> priority of the signaling flow. <br><br>  All this may seem a bit confusing - since you probably don‚Äôt intentionally create threads with different priorities in your programs.  However, Cocoa frameworks add a bit of complexity: they use dispatch queues everywhere, and each queue has a ‚ÄúQoS class‚Äù.  And this may lead to the fact that the queue will work with a different thread priority.  If you do not know the order of each task in the program (including the user interface and other tasks that are queued using Cocoa frameworks), then you may suddenly encounter a multithreaded priority situation.  Better to avoid it. <br><br><h2>  Application </h2><br>  <i>A project containing the implementations of <code>PThreadMutex</code> and <code>DispatchSemaphore</code> is available on <a href="https://github.com/mattgallagher/CwlUtils">Github</a> .</i> <br><br>  The <a href="https://github.com/mattgallagher/CwlUtils/blob/master/Sources/CwlUtils/CwlMutex.swift%3Fts%3D3">CwlMutex.swift</a> file <a href="https://github.com/mattgallagher/CwlUtils/blob/master/Sources/CwlUtils/CwlMutex.swift%3Fts%3D3">is</a> completely self-contained, so you can simply copy it, if that's all you need. <br><br>  Alternatively, the <a href="">ReadMe.md</a> file contains detailed information about cloning the entire repository and adding the framework creating it to your projects. <br><br><h2>  Conclusion </h2><br>  The <a href="">pthread_mutex_t</a> remains the best and safest mutex option in Swift for both Mac and iOS.  In the future, Swift is likely to acquire the ability to optimize non-escaping closures in the stack, or inline beyond the boundaries of the modules.  Any of these innovations will eliminate the inherent problems with <a href="">Dispatch.sync</a> , probably making it a better option.  But for now, it's too inefficient. <br><br>  While semaphores and other ‚Äúlight‚Äù locks are valid approaches in some scenarios, they are not general purpose mutexes and imply additional considerations and risks when designing. <br><br>  Regardless of the choice of machine mutexes, you need to be careful when inline for maximum performance, otherwise an excessive number of seizures with closures can slow down mutexes 10 times.  In the current version of Swift, this may mean copying and pasting code into the file in which it is used. <br><br>  Streaming, inlining, and optimization are all topics in which we can expect significant changes beyond Swift 3. However, current Swift users have to work in Swift 2.3 and Swift 3 - and this article describes the current behavior in these versions when trying to get maximum performance when using a scoped mutex. <br><br><h2>  Addition: performance metrics </h2><br>  10 million times a simple cycle was driven away: inputting a mutex, increasing the counter and outputting a mutex.  The ‚Äúslow‚Äù versions of DispatchSemaphore and PThreadMutex were compiled as part of a dynamic structure, separate from the test code. <br><br>  Results: <br><br><table><tbody><tr><th>  Mutex option </th><th>  Seconds (Swift 2.3) </th><th>  Seconds (Swift 3) </th></tr><tr><td>  PThreadMutex.sync (closure capture) </td><td>  3,043 </td><td>  3.124 </td></tr><tr><td>  DispatchQueue.sync </td><td>  2,330 </td><td>  3,530 </td></tr><tr><td>  PThreadMutex.sync_3 (return result) </td><td>  1,371 </td><td>  1,364 </td></tr><tr><td>  objc_sync_enter </td><td>  0.869 </td><td>  0.833 </td></tr><tr><td>  sync (PThreadMutex) (function in the same file) </td><td>  0.374 </td><td>  0.387 </td></tr><tr><td>  PThreadMutex.sync_4 (Dual inout parameters) </td><td>  0.307 </td><td>  0.310 </td></tr><tr><td>  PThreadMutex.sync_2 (inout single parameter) </td><td>  0.282 </td><td>  0.284 </td></tr><tr><td>  PThreadMutex.sync (inline non-capture) </td><td>  0.264 </td><td>  0.265 </td></tr><tr><td>  Direct calls to pthread_mutex_lock / unlock </td><td>  0.263 </td><td>  0.263 </td></tr><tr><td>  OSSpinLockLock </td><td>  0.092 </td><td>  0.108 </td></tr></tbody></table><br>  The test code used is part of the associated <a href="https://github.com/mattgallagher/CwlUtils">CwlUtils</a> project, but the test file containing these performance tests (CwlMutexPerformanceTests.swift) is by default not connected to the test module and must be turned on intentionally. </div><p>Source: <a href="https://habr.com/ru/post/336260/">https://habr.com/ru/post/336260/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../336250/index.html">How to work with designers</a></li>
<li><a href="../336252/index.html">Comparative load testing of Lua-connectors for Tarantool from NGINX</a></li>
<li><a href="../336254/index.html">Features of the development of mobile MMO RTS. Part 6</a></li>
<li><a href="../336256/index.html">Russian girls in Data Science. Part 1</a></li>
<li><a href="../336258/index.html">Reporting in 1C: Data Composition System (ACS), idea and architecture</a></li>
<li><a href="../336262/index.html">Configuring IDE Clion and Cmake to work with STM32 and C ++</a></li>
<li><a href="../336264/index.html">One step closer to C ++ 20. Results of the meeting in Toronto</a></li>
<li><a href="../336266/index.html">Advanced Jekyll</a></li>
<li><a href="../336268/index.html">ReactiveX 2.0 with examples, or grokay reactive programming 2.0. Part 1: Observable vs Flowable, Backpressure</a></li>
<li><a href="../336270/index.html">Brief reaction to Xored material</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>