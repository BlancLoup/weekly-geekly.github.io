<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Anatomy of an incident, or how to work on reducing downtime</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Sooner or later in any project it is time to work on the stability / availability of your service. For some services at the initial stage, the speed o...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Anatomy of an incident, or how to work on reducing downtime</h1><div class="post__text post__text-html js-mediator-article"><div style="text-align:center;"><img src="https://habrastorage.org/webt/_c/bv/ts/_cbvtskjzvfzxtmqpwfhau4vt_g.png"></div><br><p>  Sooner or later in any project it is time to work on the stability / availability of your service.  For some services at the initial stage, the speed of developing features is more important, at this moment the team is not fully formed, and the technologies are not chosen very carefully.  For other services (most often b2b technology) to gain customer confidence, the need to ensure high uptime arises with the first public release.  But suppose that the moment X has nevertheless arrived and you have begun to worry about how long your service "lies" in the reporting period.  Under the cut, I propose to see what makes up the downtime, and how best to work on reducing it. </p><a name="habracut"></a><br><h2 id="pokazateli">  Indicators </h2><br><p>  Obviously, before you improve something, you need to understand the current state.  Therefore, if we started to reduce downtime, we should first start measuring it. </p><br><p>  We will not discuss here in detail how to do it specifically, the pros and cons of different approaches, but the process looks something like this: </p><br><ul><li>  rely on near-business metrics (errors in the service, service response time, $ / second, signups / second and so forth) </li><li>  determine what is good and what is bad </li><li>  the transition is good-&gt; bad is the beginning of the incident </li><li>  transition is bad-&gt; good - end of the incident </li><li>  time from beginning to end - the duration of the incident (cap with us) </li><li>  the sum of the duration of incidents for the period (month / quarter / year) - downtime (downtime) </li><li>  (100 - &lt;idle time&gt; / &lt;period duration&gt; * 100) = availability percentage for the period </li></ul><br><p>  When talking about uptime / downtime, they often mention another indicator: </p><br><p>  <strong>MTTR</strong> (mean time to repair) - the average time from the beginning of the incident to its end. <br>  Problems with it begin right from the first word in the abbreviation.  Considering that all incidents are different, averaging the duration cannot tell us anything about the system. </p><br><p>  This time we will not average anything, but just see what happens during the incident. </p><br><h2 id="anatomiya-incidenta">  Anatomy of an incident </h2><br><p>  Let's see which significant stages can be distinguished during the incident: </p><br><img src="https://habrastorage.org/webt/ux/vx/be/uxvxbem_ictsovnxnjmjsocquti.png"><br><ul><li>  <strong>detection</strong> - the interval between the first error that we gave to the user, before the SMS arrived on duty </li><li>  <strong>reaction</strong> - from receiving a problem notification to the moment when a person began to solve this problem (usually at that moment the event in the monitoring is transferred to the Acknowledged state) </li><li>  <strong>investigation</strong> - from the beginning of work on the problem to the moment when the cause of the incident is clear and we know what needs to be done to restore the work. </li><li>  <strong>elimination</strong> - recovery time, for example, roll back the release, promote the new <del>  master </del>  primary database server </li></ul><br><p>  Perhaps our model is incomplete and there are some other stages, but I propose to introduce them only after realizing how this will help us in practice.  For now we will consider each stage in more detail. </p><br><h2 id="detection">  Detection </h2><br><p>  Why do we spend time on detecting an emergency?  Why not send a notification on the first error the user received?  In fact, I know many companies that tried to do this, but this idea was rejected in just a few hours, for which several dozens of SMS messages were received.  I think that there is not a single more or less large service that does not have a constant "background" stream of errors.  Not all of them are a sign that something has broken, there are also bugs in the software, invalid data obtained from the form and insufficient validation, and so on. </p><br><p>  As a result, the level of errors (or other metrics) that exceeds daily fluctuations is used as a criterion for opening an incident.  This is exactly what leads to the notification of responsible employees after the actual start of the problem. </p><br><p>  But back to our original task - reducing the duration of incidents.  How can we shorten the detection time?  Faster notify?  Come up with super logic detection of anomalies? </p><br><p>  I propose to do nothing so far, but look at the following stages, since in reality they are interrelated. </p><br><h2 id="reaction">  Reaction </h2><br><p>  Here we have a purely human factor.  We assume that the monitoring coped with the detection of the problem and we successfully woke up the engineer on duty (the entire escalation also worked at the previous stage). </p><br><p>  Consider the "worst" case, we do not have a dedicated duty service, and an alert overtakes a peacefully sleeping admin.  His actions: </p><br><ul><li>  react to SMS: a wife with sensitive hearing helps here, various phone applications that enhance the effect of receiving an SMS (1-5 minutes) </li><li>  decide that out of bed he will still get out: if the alerts are configured incorrectly, the person can wait 2 minutes "and what if suddenly resolve?"  and fall asleep (1-15 minutes) </li><li>  get to the laptop, open your eyes, wake up, get to the monitoring, press Ack: (1-15 minutes) </li></ul><br><p>  As a result, in the worst case, we get 35 minutes of reaction.  According to my observations, this reaction time seems to be true. </p><br><p>  Since at this stage we are dealing with people, it is necessary to act extremely carefully and thought out.  In no case do not need to write a regulation, according to which a person who has just woken up should move!  Let's just create the conditions. </p><br><p>  Let's first remove the engineer‚Äôs doubts that the problem will end on its own.  This is done very simply: <strong>to make the alert criterion insensitive to minor problems and notify if the incident lasts for any significant time</strong> .  Yes, we have just increased the duration of the "detection" stage, but let's look at an example: </p><br><ul><li>  increase detection time by 5 minutes </li><li>  the number of incidents decreases: all short bursts of errors usually fit into 1 minute.  These short incidents must be recorded, but without notifying people.  Often, in total, they give a very long downtime, but you can deal with them during working hours.  For this task, you will need a high level of detail in monitoring, since the problem has already ended, and diagnostic tools for the most part do not store history. </li><li>  if a person is forced to respond to alerts once a month or less, and not every other day, he will respond to it more adequately and not treat it as a routine </li><li>  delayed notification allows a person not to think: if the SMS has arrived, then everything is serious and will not correct itself </li></ul><br><p>  Potentially, this approach will reduce the total reaction time by 15+ minutes.  If this reaction time does not suit you, you should think about the duty service. </p><br><h2 id="investigation">  Investigation </h2><br><p>  Perhaps this is the most difficult stage of the accident, when you need to understand what is happening and what to do.  In reality, this stage is very often combined with the stage of taking action, since the process usually goes like this: </p><br><ul><li>  we look into monitoring, logs (if monitoring is not enough), we run some other diagnostic tools </li><li>  hypothesize </li><li>  check hypotheses, either by metrics or by performing some actions (restarts all in a row :) </li><li>  we estimate the results of changes </li><li>  Communicate with colleagues if your knowledge of a particular subsystem is not enough <br>  and so on until the enlightenment or the end of the incident. </li></ul><br><p>  This stage is usually the most significant in the total duration of the incident.  How to reduce it? <br>  Here everything is not very clear, there are several vectors: </p><br><ul><li>  <strong>Simplify infrastructure</strong> : Imagine how fast people are joking who have one database and one service. </li><li>  <strong>spreading knowledge in a team</strong> : ideally, if people‚Äôs communication is not going on during the incident, but during their daily work (people‚Äôs communication is generally a very long process) </li><li>  <strong>monitoring</strong> : many people think that monitoring only works at the "detection" stage, but in fact it can act as an optimization of the hypothesis testing process ("does the database work properly?", "has my service rested on resources?") and also as a transport spread knowledge in a team.  <strong>"Seryoga, check if there are errors in the log X about deadlocks?"</strong>  <strong>can be turned into a trigger, the description of which will be a link to the wiki with the instruction</strong> . </li></ul><br><h2 id="elimination">  Elimination </h2><br><p>  As I said above, this stage often merges with the previous one.  But it happens that the reason is immediately clear, but the recovery will be very long.  For example, you have a dead server with <del>  master </del>  primary (I won‚Äôt get used to it :) for a long time :), but you never prompted a replica, that is, you‚Äôll read the documentation, roll out a new config of applications, etc. </p><br><p>  Naturally, after each significant incident, you need to figure out how to prevent this from happening again or greatly speed up the recovery.  But let's see which areas we can try to work proactively: </p><br><ul><li>  <strong>infrastructure management toolkit</strong> : if in order to fix everything you need to roll out a new config, but this is done in at least 20 minutes - this is your limitation.  Try to come up with scenarios that can happen and a way to accelerate some urgent processes.  For example, in ansible, you have configured serial (parallel execution of tasks) = 3, but if you are still lying, you can roll with serial = 30, you need to teach everyone to redefine it (similarly about rolling update strategy in kubernetes). </li><li>  <strong>teachings</strong> : if you know the probable failure and recovery scenarios you do not have automated, you must have an instruction that <strong>must be tested</strong> .  Schedule downtime (if needed), hold the exercises.  Often, at this stage, such cases are automated, as in the process of the exercise, it turns out most of the pitfalls of even the most complex at first glance procedures. </li><li>  <strong>interaction with contractors</strong> : you must know in advance what you will do if your hoster becomes ill.  Often, an awareness of the likelihood of a problem and the cost of closing risks leads to the conclusion - ‚Äúwe‚Äôll just wait for recovery‚Äù.  But on the other hand, engineers and business will be ready for such a scenario.  For example, you can work through the issue of switching your traffic to a pre-prepared stub, notify users of a pre-prepared letter, and so on.  Or vice versa, you make an instruction, according to which we give the hoster 30 minutes to restore, and then we start moving to another DC, where we already have a replica of the database, but we need to expand everything else.  And here again, the teachings, note the time to move and so on. </li></ul><br><h2 id="mtbf-mean-time-between-failures">  MTBF (Mean Time Between Failures) </h2><br><p>  Another common indicator that is mentioned when discussing uptime.  I propose again not to average anything, but simply to talk about the number of incidents that occur during the time interval. </p><br><p>  This is where the question of how you take care of the resiliency of your service comes to the fore: </p><br><ul><li>  is there a single point of failure (SPOF) in the infrastructure, what is the probability of failure? </li><li>  How confident are you that there are no SPOFs that you do not know about?  (this is exactly the problem that is solved with the help of <a href="https://en.wikipedia.org/wiki/Chaos_Monkey">chaos monkey</a> ) </li><li>  Do load balancers work well for failures?  ( <a href="https://habr.com/company/okmeter/blog/423085/">my balancing report</a> ) </li><li>  How resilient is the network? </li><li>  How reliable is the datacenter? </li></ul><br><p>  Sometimes, in order to calculate / predict all this, a ‚Äúrisk map‚Äù is made, where each scenario (which could naturally be assumed, always contains those that we do not yet know) has the likelihood + impact effect (short / long time, data loss, reputational loss). , etc).  Then, according to such a map, they systematically work, closing, in the first place, highly probable and serious scenarios. </p><br><p>  Another technique that can be used is the classification of past incidents.  Now there is a lot of talk about the fact that it is very useful to write "post mortem" incidents, where the causes of the problem, the actions of people are analyzed, and possible future actions are worked out.  But in order to quickly look at the causes of all accidents over the past period, it is convenient to sum up their duration with grouping into ‚Äúproblem classes‚Äù and where downtime is most likely to take action: </p><br><ul><li> <strong>human errors</strong> : reduce the number of manual actions in production, different protection against operator error </li><li>  <strong>unsuccessful releases</strong> : it is worth improving testing (including load) </li><li>  <strong>errors in applications</strong> : repair leaks, crashes and other "freezes" </li><li>  <strong>network</strong> : buy equipment, set up, hire networkers, change contractor </li><li>  <strong>database</strong> : hire a DBA, take care of fault tolerance, buy better hardware </li><li>  <strong>DC</strong> : think about reserve or move </li><li>  <strong>external impacts</strong> (ddos, locks, certificate reviews, domains): buy antiddos, stock up with proxy, monitor domains / certificates for the validity period, have several certificates from different CA. </li></ul><br><p>  That is, if you do not even try to predict possible scenarios of problems, then it is definitely worth working with incidents that have already happened. </p><br><h2 id="itogo">  Total </h2><br><p>  All incidents are different: </p><br><img src="https://habrastorage.org/webt/hq/h2/c_/hqh2c_hykjlgpf1jbyznyvnbf4y.png"><br><p>  The algorithm for working on increasing uptime is very similar to any other optimization: </p><br><pre><code class="hljs erlang-repl"> -&gt;  -&gt;   -&gt;  </code> </pre> <br><p>  From my own experience, I can say that in order to significantly improve uptime, it‚Äôs enough just to start following it and analyzing the causes of incidents.  It usually happens that the most simple changes bring the most significant effect. </p><br><p>  <em><a href="https://okmeter.io/%3Futm_source%3Dhabr%26utm_medium%3Dhabr-post%26utm_campaign%3Dblog%26utm_content%3Dincident_anatomy">Our monitoring service</a> helps not only with the "detection" stage, but also greatly reduces the "investigation" (customers will confirm)</em> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/422973/">https://habr.com/ru/post/422973/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../422961/index.html">Sony Xperia Ear Duo Assistant Headset Receives Important Update</a></li>
<li><a href="../422965/index.html">UniSharping: converting C # code to Java and Python</a></li>
<li><a href="../422967/index.html">Habr.com. Transparency report</a></li>
<li><a href="../422969/index.html">Anomaly Frango, Outbreak</a></li>
<li><a href="../422971/index.html">Rails Expert System</a></li>
<li><a href="../422977/index.html">Mikhail Bessmeltsev with a colleague developed new algorithms for graphics vectorization</a></li>
<li><a href="../422979/index.html">American analogue of the GDPR: what you need to know about the act of the CCPA</a></li>
<li><a href="../422981/index.html">The simplest implementation of the Entity Component System</a></li>
<li><a href="../422985/index.html">Quick start a web project (BE - Java Spring, FE - React Redux, interaction - Rest, WebSocket)</a></li>
<li><a href="../422987/index.html">And again the 256th day of the year</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>