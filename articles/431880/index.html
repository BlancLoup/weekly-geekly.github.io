<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Doom of SceneKit. Yandex experience with 3D graphics in iOS</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="- I'm too young to die. 


 SceneKit is a high-level three-dimensional graphics framework in iOS that helps create animated scenes and effects. It inc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Doom of SceneKit. Yandex experience with 3D graphics in iOS</h1><div class="post__text post__text-html js-mediator-article"><p>  <i>- I'm too young to die.</i> </p><br><p>  SceneKit is a high-level three-dimensional graphics framework in iOS that helps create animated scenes and effects.  It includes a physics engine, a particle generator and a set of simple actions for 3D objects that allow you to describe the scene in terms of content ‚Äî geometry, materials, lighting, cameras ‚Äî and animate it through a description of the changes for these objects. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/de8/eb3/a60/de8eb3a6080bb48be4f407c07d1b122a.gif"></p><br><p>  Today we will take a careful, slightly stern look at SceneKit, but first we turn to the basics and see what the 3D scene is and what needs to be done to create it. <a name="habracut"></a></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1fa/7d7/138/1fa7d71381bc0f217fe6ddff8a08e77f.png" alt="The simplest scene of three nodes with geometry in them."><br>  <em>The simplest scene of three nodes with geometry in them</em> </p><br><p>  First you need to create the basic structure of the scene, which consists of <a href="https://developer.apple.com/documentation/scenekit/scnnode">nodes</a> or nodes of the scene.  Each node can contain both geometry and other nodes.  Geometry can be as simple as a ball, a cube, or a pyramid, or more complex, created in external editors. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/a9c/dd6/3ae/a9cdd63aeb3c662009d721feff5a2fa2.png" alt="Overlay materials"><br>  <em>Overlay materials</em> </p><br><p>  Then for this geometry it is necessary to specify <a href="https://developer.apple.com/documentation/scenekit/scnmaterial/">materials</a> that will determine the basic representation of the objects.  Each material sets its own lighting model and, depending on it, uses a different set of <a href="https://developer.apple.com/documentation/scenekit/scnmaterialproperty/">properties</a> .  Each such property is usually a color or texture, but in addition to these commonly used options, there is also the possibility to use <a href="https://developer.apple.com/documentation/quartzcore/calayer">CALayer</a> , <a href="https://developer.apple.com/documentation/avfoundation/avplayer/">AVPlayer</a> , and <a href="https://developer.apple.com/documentation/spritekit/skscene/">SKScene</a> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/0c1/ca5/831/0c1ca58318327b07263caf4f9d67ea63.png" alt="Add light sources"><br>  <em>Add light sources</em> </p><br><p>  After that, you need to add <a href="https://developer.apple.com/documentation/scenekit/scnlight">light sources</a> that determine how well objects are visible in one or another part of the scene.  They, by analogy with geometry, must lie inside some kind of node.  SceneKit supports many different <a href="https://developer.apple.com/documentation/scenekit/scnlight/lighttype">types of lighting</a> , as well as several <a href="https://developer.apple.com/documentation/scenekit/scnshadowmode">types of shadows</a> . </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/c16/a8e/f8e/c16a8ef8ebcfb7242eef1d8142b70dea.png" alt="Bokeh effect &quot;out of the box&quot;"><br>  <em>Bokeh effect "out of the box"</em> </p><br><p>  Then you need to create a <a href="https://developer.apple.com/documentation/scenekit/scncamera">camera</a> (and put it in a separate node) and set basic parameters for it.  There are a lot of them, but with the help of them you can create cool effects.  Out of the box, bokeh (or blur), HDR with adaptation, glow, SSAO and hue / saturation modifications are supported. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/76f/62c/908/76f62c908bda6280380c922e3a1e8ca3.gif" alt="Simple animations in SceneKit"><br>  <em>Simple animations in SceneKit</em> </p><br><p>  Finally, SceneKit includes a simple set of actions for 3D objects that allow you to set scene changes over time.  SceneKit also supports <a href="https://developer.apple.com/documentation/scenekit/scnaction/1523984-javascriptaction">actions described in JavaScript</a> , but this is a topic for a separate article. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/08c/0dc/2e4/08c0dc2e44e627e369af7dea6f18d27d.gif" alt="The interaction of the particle generator with the physics engine can lead to a tornado!"><br>  <em>The interaction of the particle generator with the physics engine can lead to a tornado!</em> </p><br><p>  In addition to graphics, the main features of SceneKit are the particle generator and the advanced physics engine, which allows you to set the actual physical properties of both ordinary objects and particles from the generator. </p><br><p>  About all these chips a large number of detailed tutorials is written.  But in the development process, we practically did not use these opportunities ... </p><br><h2 id="hey-not-too-rough">  <strong>Hey, not too rough</strong> </h2><br><blockquote>  <em>Once I wrote a lighting model for 3D games better than real sunlight, giving an acceptable FPS on the Nvidia 8800, but I decided not to release the engine, since God is cute to me and I don‚Äôt want to show its incompetence in this matter.</em> <br>  <em>- John Carmack</em> </blockquote><p>  We start the detailed study with a fairly simple task, which arises in almost everyone who works quite seriously with SceneKit: how to load a model with a complex geometry and connected materials, lighting, and even animations? </p><br><p>  There are several ways, and they all have their pros and cons: </p><br><ol><li><p>  SCNScene (named :) - gets the scene from the bundle, </p><br></li><li><p>  SCNScene (url: options :) - loads the scene by URL, </p><br></li><li><p>  SCNScene (mdlAsset :) - converts a scene from different formats, </p><br></li><li><p>  SCNReferenceNode (url :) - lazily loading the scene. </p><br></li></ol><br><h3 id="poluchaem-scenu-iz-bandla">  We get the scene from the bundle </h3><br><p>  You can use the <a href="https://developer.apple.com/documentation/scenekit/scnscene/1523355-init">standard method</a> : put our model in dae or scn format in the scnassets bundle and load it from there by analogy with UIImage (named :). </p><br><p>  But what if you want to control the updating of models yourself, without releasing an update in the App Store every time you need to change a couple of textures?  Or imagine that you need to support user-created maps and models.  Or - that you simply do not want to increase the size of the application, since the 3D graphics in it is not the main functionality. </p><br><h3 id="zagruzhaem-scenu-po-url">  Load the scene by URL </h3><br><p>  You can use <a href="https://developer.apple.com/documentation/scenekit/scnscene/1522660-init">the scene constructor from the</a> scn-file <a href="https://developer.apple.com/documentation/scenekit/scnscene/1522660-init">URL</a> .  This method supports downloading not only from the file system, but also from the network, but in the latter case, you can forget about compression.  Plus, you will need to convert the model to scn format in advance.  You can, of course, use dae, but with it comes a set of restrictions.  For example - the lack of physically based rendering. </p><br><p>  The main advantage of this method is that it allows the flexibility to customize <a href="https://developer.apple.com/documentation/scenekit/scnscenesource/loadingoption">the import settings</a> .  You can, for example, modify the life cycle of animations and make them repeat indefinitely.  You can explicitly specify the source for loading external resources like textures, you can convert the orientation and scale of the scene, create missing normals for the geometry, merge all the geometry of the scene into one large node, or discard all elements of the scene that do not conform to the standard format. </p><br><h3 id="konvertiruem-scenu-iz-raznyh-formatov">  Convert a scene from different formats. </h3><br><p>  The third option is to use a <a href="https://developer.apple.com/documentation/scenekit/scnscene/1419833-init">constructor with MDLAsset</a> .  That is, we first create the <a href="https://developer.apple.com/documentation/modelio/mdlasset/">MDLAsset</a> that is available in the ModelIO framework and then pass it to the designer for the scene. </p><br><p>  This option is good because it allows you to download many different formats.  Officially, MDLAsset can load obj, ply, stl and usd formats, but after running out a list of all possible formats related to computer graphics, I found four more: abc, bsp, vox and md3, but they may not be fully supported or in all systems, and for them you need to check the correctness of the import. </p><br><p>  It is also necessary to take into account that this method has an overhead for conversion, and use it very carefully. </p><br><p>  These methods have one common pitfall: they return SCNScene, not SCNNode.  The only way to add content to an already existing scene is to copy all the child nodes and - this step can be easily skipped - animations from the root node (they, for example, can appear there when working with dae).  In addition, you need to take into account that there can be only one environment texture in the scene (if you do not use custom shaders for reflections). </p><br><h3 id="lenivo-zagruzhaem-scenu">  Lazily loading the scene </h3><br><p>  The fourth option is to use the <a href="https://developer.apple.com/documentation/scenekit/scnreferencenode">SCNReferenceNode</a> .  It returns not the scene, but the node, which can lazily (or on demand) load the entire scene hierarchy into itself.  Thus, this method is similar to the first, but it hides within itself all the problems with copying. </p><br><p>  He has one thing: the global parameters of the scene are lost. </p><br><p>  It turns out that this is the easiest and fastest way to load your model, but if you need file-tuning, the first method will be better. </p><br><p>  As a result, we stopped at the first option, since it was most convenient for us to work in the scn format, and for the designers to convert it from the dae format.  In addition, we needed fine-tuning animations when loading. </p><br><h3 id="vovse-ne-prezhdevremennye-optimizacii">  Not premature optimizations at all </h3><br><p>  Having fiddled with this process long enough, I can give you some tips. </p><br><p>  The main advice is to convert files to scn in advance.  Then you can, by opening the file in Xcode's built-in scene editor, see exactly how your object will look in SceneKit. </p><br><p> In addition, the scn-file is actually just a binary representation of the scene, so loading from it will take the least time.  For the same dae, you first need to parse the xml, then convert all the meshes, animations and materials.  Moreover, the conversion of animations and materials is a potential source of problems.  We recall the lack of PBR support in dae: it turns out that if you want to use it, you have to change the type of all materials after the conversion and manually set the appropriate textures. </p><br><p>  With this operation, you can get a very useful side effect: significant compression of textures.  It is enough to open them in the "View" and export, changing the format to heic.  On average, this simple operation saved 5 megabytes per model. </p><br><p>  Also, if you download a scene from the Internet, I can advise you to download it in the archive, unpack it and pass the URL of the unpacked scn-file.  This will save you and the user extra megabytes - which, in turn, will speed up the download, as well as reduce the number of points of failure.  Agree: to make a separate request for each external resource, and even on the mobile Internet - not the best way to increase reliability. </p><br><h2 id="hurt-me-plenty">  <strong>Hurt me plenty</strong> </h2><br><blockquote>  <em>When I travel by car, I often hear the hard disk of the universe crackling, loading the next street.</em> <br>  <em>- John Carmack</em> </blockquote><p>  So, when the work on loading and importing models is put on stream, a new task arises: adding various effects and possibilities to the scene.  And believe me, there is something to tell about.  We begin by going through the various collections in SceneKit. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1e3/c0e/ffd/1e3c0effdc9c72b19d6e3d0a19a13796.png" alt="The counters in SceneKit are counted immediately after physics. And before rendering the frame"><br>  <em>The counters in SceneKit are counted immediately after physics.</em>  <em>And before rendering the frame</em> </p><br><p>  Constraints, you say?  What are the counters?  Few people know, and even more so, and talks about it, but SceneKit has its own set of constraints.  And although they are not as flexible as the UIkit frameworks, with them you can still do a lot of interesting things. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/006/c01/3a3/006c013a36d0bc0f615ca4f4378e4767.gif" alt="SCNReplicatorConstraint"><br>  <em>SCNReplicatorConstraint</em> </p><br><p>  Let's start with a simple construction - <a href="https://developer.apple.com/documentation/scenekit/scnreplicatorconstraint">SCNReplicatorConstraint</a> .  All that he does is duplicate the position, rotation and size of another object with additional offset sets.  Like all other constraints, it is possible to change the force and set the increment flag.  Best of all, both parameters can be shown on this count. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/dd7/98d/a46/dd798da468df11e71ab53c7a41e187c5.gif" alt="Reduced force 10 times"><br>  <em>Reduced force 10 times</em> </p><br><p>  <a href="https://developer.apple.com/documentation/scenekit/scnconstraint/1468692-influencefactor">Strength</a> influences how much a transformation is applied to an object.  And once the position of the target object changes every frame, the shadow object approaches one-tenth of the difference in distance.  Because of this, a lag effect appears. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/535/ec5/0bc/535ec50bce2b4e57f30c71226bc783da.gif" alt="Removed incrementality and reduced force 10 times."><br>  <em>Removed incrementality and reduced force 10 times.</em> </p><br><p>  <a href="https://developer.apple.com/documentation/scenekit/scnconstraint/2867541-isincremental">Incrementality</a> , in turn, affects whether the constraint is canceled after rendering.  Suppose we turned it off.  Then we see that the frame is applied on each frame before rendering, and after rendering it is canceled, and each frame is repeated this way.  As a result, combining these two parameters, you can get a rather interesting effect of a clock hand. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/74d/5c5/cfd/74d5c5cfd7bde6252adf7855705ec5ad.gif" alt="The plane always faces the camera."><br>  <em>The plane always faces the camera.</em> </p><br><p>  Let us turn to a more interesting construction: the so-called billboard. </p><br><p>  Suppose it is necessary that some object is always to us "face".  To do this, you just need to use <a href="https://developer.apple.com/documentation/scenekit/scnbillboardconstraint">SCNBillboardConstraint</a> , specify which axis the object can rotate around.  Further, before calculating each frame (after the step with physics), the positions and orientations of all objects will be updated to suit all constraints. </p><br><p>  Here you can mention <a href="https://developer.apple.com/documentation/scenekit/scnlookatconstraint">Look At Constraint</a> : it is similar to a billboard, only the object can be placed facing any other object of the scene instead of the current camera. </p><br><p>  What can be done with them?  Of course, most often these frameworks are used to draw trees or small objects.  Also at the expense of them create special effects like fire or explosion.  In addition, with their help, you can force the camera to follow the object on the scene. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/077/6ed/96d/0776ed96d0d6116c12ababd108bfe180.gif" alt="Keeps distance between objects"><br>  <em>Keeps distance between objects</em> </p><br><p>  <a href="https://developer.apple.com/documentation/scenekit/scndistanceconstraint">SCNDistanceConstraint</a> allows <a href="https://developer.apple.com/documentation/scenekit/scndistanceconstraint">you</a> to set the minimum and / or maximum distance to the position of another object.  And yes, with it you can make a snake.  :) This constraint can also be used to bind a camera to a character, although the camera position is usually more difficult to set, and it is not an easy task to describe it with some constraints.  The same effect can be achieved by adding a spring in the physics engine, but this spring can be supplemented with a constraint in case you need to avoid problems with excessive stretching or compression of the spring. </p><br><p>  Many have seen it in some Hitman, Fallout, or Skyrim: you are dragging a body behind you, it touches an obstacle - and it starts behaving as if a demon has entered it.  This would help to avoid such bugs. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/f8f/578/aca/f8f578aca5e9d78034dcef18062349b8.gif" alt="SCNSliderConstraint"><br>  <em>SCNSliderConstraint</em> </p><br><p>  <a href="https://developer.apple.com/documentation/scenekit/scnsliderconstraint">SCNSliderConstraint</a> allows <a href="https://developer.apple.com/documentation/scenekit/scnsliderconstraint">you</a> to set the minimum distance between a given object and physical bodies with a suitable collision mask.  Quite a funny constraint, but again, they try to simulate it through physical interaction.  The basic idea is to set the radius of the dead zone with physical bodies for an object that does not have a physical body. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/357/930/7df/3579307dfcf69c026734b0c35d7c6f04.gif" alt="Inverse kinematics at work"><br>  <em>Inverse kinematics at work</em> </p><br><p>  <a href="https://developer.apple.com/documentation/scenekit/scnikconstraint">SCNIKConstraint</a> is the most interesting, but also the most complex, using the so-called inverse kinematics.  Using a chain of parent nodes, inverse kinematics iteratively tries to bring the node to which you apply this constraint to the required point.  In fact, it allows you not to think about the position in which the shoulder and forearm should be located, but simply to set the position of the hand and the possible angles of rotation of the connecting nodes.  The rest will be considered for you.  The main drawback of this constraint is that it allows you to set only the position of the hand, but not its orientation, and the restrictions on the angles can be made global, without breaking down the axes. </p><br><p>  So, we got acquainted in detail with the foundations and what they can do.  Let's continue to explore interesting effects.  We will deal with the effect of shadows. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/5d9/d62/4a4/5d9d624a480d96c5654a8df80be14115.gif" alt="Here is the plane, but it is not"><br>  <em>Here is the plane, but it is not</em> </p><br><p>  It would seem, what could be simpler in an engine that supports shadows, than creating shadows?  But sometimes the shadows need to be dropped on a completely transparent plane.  This is very useful in ARKit, since the image of the camera is displayed behind the plane, and the shadow should be cast somewhere.  The trick turns out to be quite simple: you must first turn on the pending shadows and turn off recording to all components near the plane in the material tab, and the shadow will continue to be superimposed on it.  The only problem is that this plane will overlap the objects behind it. </p><br><p>  But shadows are not the only poorly studied effect in SceneKit.  Let's now deal with mirrors. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/32d/75b/1ba/32d75b1ba50036e814d21bacb0104523.png" alt="SCNFloor mirror - what could be simpler"><br>  <em>SCNFloor mirror - what could be simpler</em> </p><br><p>  Everyone who has been playing with SceneKit probably knows about scnfloor, which adds mirror reflections to the floor.  But for some reason, very few people use it for honest mirror reflections, because you can put your modelka over the floor geometry, tilt it a bit and turn it ... into a regular mirror. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1c4/bdf/289/1c4bdf289fd13dd45489765512bc2d2a.png"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/fdd/950/d98/fdd950d9889d35a4d8e66b871c0c2767.png" alt="Glass drips and curved mirror"><br>  <em>Glass drips and curved mirror</em> </p><br><p>  But, what is even less known, for this gender, you can put a normal map.  Due to this, in turn, you can create many different interesting effects, such as the effect of streaks or a curved mirror. </p><br><h2 id="ultra-violence">  <strong>Ultra-violence</strong> </h2><br><blockquote>  <em>Once I kissed a girl with open eyes.</em>  <em>The near clipping plane cut her face.</em>  <em>Since then, I kiss only with my eyes closed.</em> <br>  <em>- John Carmack</em> </blockquote><p>  Shadows, mirrors - interesting effects.  But there is one effect that with skillful use may be even more interesting - video texture. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/998/047/b54/998047b5496741e28e66d6faeb04f4a1.gif"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/8b7/91c/a6b/8b791ca6b8f464f5bd7401bdc07225ac.gif" alt="Normal video and video with height map"><br>  <em>Normal video and video with height map</em> </p><br><p>  You may need them just to show the video inside the game.  But it is much more interesting that with the help of video textures you can modify the geometry.  To do this, you need to put a video texture with a height map in the <a href="https://developer.apple.com/documentation/scenekit/scnmaterial/2867516-displacement">displacement</a> property of your material and use the material on a plane with a sufficiently large <a href="https://developer.apple.com/documentation/scenekit/scnplane/1523991-widthsegmentcount">number of segments</a> .  It remains to understand how to put it there. </p><br><p>  I mentioned in the description of the scene creation process that you can use <a href="https://developer.apple.com/documentation/spritekit/skscene">SKScene</a> as the material <a href="https://developer.apple.com/documentation/spritekit/skscene">property</a> , and this is the SpriteKit scene.  SpriteKit - it's like SceneKit, but for 2D graphics.  It has support for displaying video with <a href="https://developer.apple.com/documentation/spritekit/skvideonode">SKVideoNode</a> .  You only need to put SKVideoNode in SKScene, and SKScene in SCNMaterialProperty, and everything will be ready. </p><br><p>  But by exporting the resulting 3D scene and opening it somewhere else, we will see a black square.  Having rummaged in the scn-file, I found the reason.  It turns out that when saving a video clip, it does not save the video URL.  It would seem, you take and rule.  But not everything is so simple: the scn-file is the so-called binary plist, which contains the result of the work of NSKeyedArchiver.  And the material, which is the SpriteKit's scene, is the same binary plist, which, it turns out, already lies inside another binary plist!  It is good that there are only two nesting levels. </p><br><p>  Well, now we‚Äôll move on not to an effect, but to a tool that allows you to create any kind of effects.  These are shader modifiers. </p><br><p>  Before you modify something, you need to understand what we are modifying.  A shader by definition is a program for a GPU that is run for each vertex and for each pixel.  Thus, a shader is a program that determines how an object looks on the screen. </p><br><p>  Well, the modifier shader allows you to change the results of the work of standard shaders to GLSL or Metal Shading Language.  They are also available in a visual editor, which allows you to see changes in the modifier in real time. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/663/9d1/9a7/6639d19a72901a50caea2b969816a1ae.png"></p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/1d1/94a/660/1d194a660488fe33aa392c0573834ca4.png" alt="Fur and Parallax Mapping"><br>  <em>Fur and Parallax Mapping</em> </p><br><p>  Using the modifier shader, you can create sophisticated visual effects.  For example, a couple of the most famous effects: Fur and <a href="https://ru.wikipedia.org/wiki/Parallax_mapping">Parallax Mapping</a> . </p><br><pre><code class="objectivec hljs"><span class="hljs-meta"><span class="hljs-meta">#pragma arguments texture2d bg; texture2d height; float depth; float layers; #pragma transparent #pragma body constexpr sampler sm = sampler(filter::linear, s_address::repeat, t_address::repeat); float3 bitangent = cross(_surface.tangent, _surface.normal); float2 direction = float2(-dot(_surface.view.rgb, _surface.tangent), dot(_surface.view.rgb, _surface.bitangent)); _output.color.rgba = float4(0); for(int i = 0; i </span><span class="hljs-meta-string"><span class="hljs-meta"><span class="hljs-meta-string">&lt; int(floor(layers)); i++) { float coeff = float(i) / floor(layers); float2 defaultCoords = _surface.diffuseTexcoord + direction * (1 - coeff) * depth; float2 adjustment = float2(scn_frame.sinTime + defaultCoords.x, scn_frame.cosTime) * depth * coeff * 0.1; float2 coords = defaultCoords + adjustment; _output.color.rgb += bg.sample(sm, coords).rgb * coeff * (height.sample(sm, coords).r + 0.1) * (1.0 - coeff); _output.color.a += (height.sample(sm, coords).r + 0.1) * (1.0 - coeff); } return _output;</span></span></span></span></code> </pre> <br><p><img src="https://habrastorage.org/getpro/habr/post_images/7aa/b5f/b0e/7aab5fb0e907d2a7290f0f31e415e4be.gif" alt="Ray Casting with caustics in real time."><br>  <em>Ray Casting with caustics in real time</em> </p><br><p>  What is even more interesting, no one bothers to completely throw out the results of their work and write your renderer.  For example, you can try to implement Ray Casting in shaders.  And it all works fast enough to provide 30 FPS even on such complex calculations.  But this is a topic for a separate report.  Come to <a href="https://mobiusconf.com/talks/nch5tml8wg6scacc2ss88/">Mobius</a> ! </p><br><h2 id="nightmare">  <strong>Nightmare!</strong> </h2><br><blockquote>  <em>I do not like to blink, because the closed eyelids abruptly load the GPU for the BDPT due to lack of lighting.</em> <br>  <em>- John Carmack</em> </blockquote><p>  So, we have a bunch of objects with cool effects.  Now it remains to learn how to write them.  To do this, we turn to a more complex topic: how we learned to record video directly from SceneKit without an external UI, and how we optimized this record dozens of times. </p><br><p>  Let's first turn to the simplest solution: <a href="https://developer.apple.com/documentation/replaykit">ReplayKit</a> .  Find out why it does not fit.  Generally speaking, this solution allows you to create a screen entry in a few lines of code and save it through the system preview.  But.  It has a big minus - it records everything, the whole UI, including all the buttons on the screen.  It was our first decision, but for obvious reasons it was impossible to let it go in production: users had to share the videotape, and share it not from a system preview. </p><br><p>  We found ourselves in a situation where the solution had to be written from scratch.  Quite from scratch.  So, let's see how you can create your own video in iOS and record your frames there.  It's pretty simple: </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/bd5/ce3/0f8/bd5ce30f8b449cba0a5fbe8d68d2a420.png" alt="Recording process"><br>  <em>Recording process</em> </p><br><p>  You need to create an entity that will record files - <a href="https://developer.apple.com/documentation/avfoundation/avassetwriter">AVAssetWriter</a> , add a video stream to it - <a href="https://developer.apple.com/documentation/avfoundation/avassetwriterinput">AVAssetWriterInput</a> , and create an adapter for this stream that will convert our pixel buffer to the format required by the stream - <a href="https://developer.apple.com/documentation/avfoundation/avassetwriterinputpixelbufferadaptor">AVAssetWriterPixelBufferAdaptor</a> . </p><br><p>  Just in case, I remind you that a <a href="https://developer.apple.com/documentation/corevideo/cvpixelbuffer-q2e">pixel buffer</a> is an entity that is a piece of memory where data for pixels are somehow written.  In essence, this is a low-level view of a picture. </p><br><p>  But how to get this pixelbuffer?  The solution is simple.  SCNView has a great <a href="https://developer.apple.com/documentation/scenekit/scnview/1524031-snapshot">.snapshot ()</a> function that returns a UIImage.  We just need to create a pixelbuffer from this UIImage. </p><br><pre> <code class="objectivec hljs">var unsafePixelBuffer: CVPixelBuffer? CVPixelBufferPoolCreatePixelBuffer(<span class="hljs-literal"><span class="hljs-literal">NULL</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.pixelBufferPool, &amp;unsafePixelBuffer) guard let pixelBuffer = maybePixelBuffer <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } CVPixelBufferLockBaseAddress(pixelBuffer, <span class="hljs-number"><span class="hljs-number">0</span></span>) let data = CVPixelBufferGetBaseAddress(pixelBuffer) let rgbColorSpace = <span class="hljs-built_in"><span class="hljs-built_in">CGColorSpaceCreateDeviceRGB</span></span>() let bitmapInfo = <span class="hljs-built_in"><span class="hljs-built_in">CGBitmapInfo</span></span>(rawValue: <span class="hljs-built_in"><span class="hljs-built_in">CGBitmapInfo</span></span>.byteOrder32Little.rawValue | <span class="hljs-built_in"><span class="hljs-built_in">CGImageAlphaInfo</span></span>.premultipliedFirst.rawValue) let rowBytes = <span class="hljs-built_in"><span class="hljs-built_in">NSUInteger</span></span>(CVPixelBufferGetBytesPerRow(pixelBuffer)) let context = <span class="hljs-built_in"><span class="hljs-built_in">CGContext</span></span>( data: data, width: image.width, height: image.height, bitsPerComponent: <span class="hljs-number"><span class="hljs-number">8</span></span>, bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer), space: rgbColorSpace, bitmapInfo: bitmapInfo.rawValue ) context?.draw(image, <span class="hljs-keyword"><span class="hljs-keyword">in</span></span>: <span class="hljs-built_in"><span class="hljs-built_in">CGRect</span></span>(x: <span class="hljs-number"><span class="hljs-number">0</span></span>, y: <span class="hljs-number"><span class="hljs-number">0</span></span>, width: image.width, height: image.height)) CVPixelBufferUnlockBaseAddress(pixelBuffer, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.appendPixelBuffer(pixelBuffer, withPresentationTime: presentationTime)</code> </pre> <br><p>  We just allocate memory space, describe the format of these pixels, block the buffer for the change, get the memory address, create a context at the received address, where we describe how the pixels are packed, how many lines are in the picture and what color space we use.  Then we copy there the pixels from the UIImage, knowing the final format, and remove the lock for the change. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ba0/e25/5a0/ba0e255a03515ee7e299bfa3d2757526.png"></p><br><p>  Now we need to do this every frame.  To do this, we create a display link that will call a callback on each frame, where we, in turn, will call the snapshot method and create a pixel buffer from the image.  It's simple! </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ab9/6fa/841/ab96fa8410458c0854d2c6b74bfb488c.gif"></p><br><p>  And no.  Such a solution even on powerful phones causes terrible lags and FPS drawdowns.  Let's do the optimization. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/b13/562/6e6/b135626e6d19559cb931b0d62e20e0d3.png"></p><br><p>  Suppose we do not need 60 FPS.  We will even be pleased with the 25th.  But what is the easiest way to achieve this result?  Of course, you just need to bring all this to the background thread.  Moreover, according to the developers, this function is thread-safe. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/701/d7c/a26/701d7ca267899b726966bcefacd2921e.gif"></p><br><p>  Hmm, lag has become less, but the video has ceased to be recorded ... </p><br><p>  It's simple.  As they say, if you have a problem and you solve it with several threads, you will have 2 problems. </p><br><p>  If you try to record a pixelbuffer with a timestamp lower than the last recorded one, then the entire video will be invalid. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/643/ed3/f61/643ed3f61dd84093f38c4447952d2a8b.png"></p><br><p>  Let's not then write down the new buffer until the previous entry ends. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/335/c9f/544/335c9f544b1bfd080a34cfe03847235a.gif"></p><br><p>  Hmm, it got much better.  But still, why did lags appear from the beginning? </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/ef1/b9d/838/ef1b9d8381334a5b0d6c9c4952eea020.png"></p><br><p>  It turns out that the <a href="https://developer.apple.com/documentation/scenekit/scnview/1524031-snapshot">.snapshot ()</a> function, with which we get the image from the screen, creates a new renderer for each call, draws a frame from scratch and returns it, and not the image that is on the screen.  This leads to funny effects.  For example, a physical simulation happens twice as fast. </p><br><p>  But wait - why are we trying to render a new frame every time?  Surely somewhere you can find the buffer that is displayed on the screen.  And indeed, there is access to such a buffer, but it is quite non-trivial.  We need to get <a href="https://developer.apple.com/documentation/quartzcore/cametaldrawable">CAMetalDrawable</a> from Metal. </p><br><p>  Unfortunately, it's not so easy to get to Metal directly from SCNView for a fairly understandable reason - you can choose the type of API in SceneKit yourself, but if you look under the hood and look at the <a href="https://developer.apple.com/documentation/uikit/uiview/1622436-layer">layer</a> , you can see what it is like in the case of Metal, <a href="https://developer.apple.com/documentation/quartzcore/cametallayer">CAMetalLayer</a> . </p><br><p>  But even here failure awaits us: in CAMetalLayer, the only way to interact with the view is the function nextDrawable, which returns not occupied by CAMetalDrawable.  It is assumed that you will write data to it and call the present function on it, which will display it on the screen. </p><br><p>  The solution actually exists.  The fact is that after disappearing from the screen, the buffer is not deployed, but only placed back into the pool.  Indeed, why allocate memory each time, if two or three buffers are enough: one is shown on the screen, the second for rendering and the third, for example, for post-processing, if you have one. </p><br><p>  It turns out that after displaying the buffer, the data from it will not disappear anywhere and you can safely and securely access it. </p><br><p>  And if we, as a successor, begin in response to each call of nextDrawable () to save it, we get almost what we need.  The problem is that the saved CAMetalDrawable is the one in which the image is being drawn right now. </p><br><p>  The jump to the real solution is very simple - we keep both the current Drawable and the previous one. </p><br><p>  And here it is, ready - direct memory access through CAMetalDrawable. </p><br><pre> <code class="objectivec hljs">var unsafePixelBuffer: CVPixelBuffer? CVPixelBufferPoolCreatePixelBuffer(<span class="hljs-literal"><span class="hljs-literal">NULL</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.pixelBufferPool, &amp;unsafePixelBuffer) guard let pixelBuffer = maybePixelBuffer <span class="hljs-keyword"><span class="hljs-keyword">else</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> } CVPixelBufferLockBaseAddress(pixelBuffer, <span class="hljs-number"><span class="hljs-number">0</span></span>) let data = CVPixelBufferGetBaseAddress(pixelBuffer) let width: <span class="hljs-built_in"><span class="hljs-built_in">NSUInteger</span></span> = lastDrawable.texture.width let height: <span class="hljs-built_in"><span class="hljs-built_in">NSUInteger</span></span> = lastDrawable.texture.height let rowBytes: <span class="hljs-built_in"><span class="hljs-built_in">NSUInteger</span></span> = <span class="hljs-built_in"><span class="hljs-built_in">NSUInteger</span></span>(CVPixelBufferGetBytesPerRow(pixelBuffer) lastDrawable.texture.getBytes( data, bytesPerRow: rowBytes, fromRegion: <span class="hljs-built_in"><span class="hljs-built_in">MTLRegionMake2D</span></span>(<span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, width, height), mipmapLevel: <span class="hljs-number"><span class="hljs-number">0</span></span> ) CVPixelBufferUnlockBaseAddress(pixelBuffer, <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">self</span></span>.appendPixelBuffer(pixelBuffer, withPresentationTime: presentationTime)</code> </pre> <br><p>  So, now we do not create a context and draw a UIImage in it, but copy one piece of memory into another.  The question arises: what about the pixel format? .. </p><br><p>  It does not coincide with <a href="https://developer.apple.com/documentation/coregraphics/1408837-cgcolorspacecreatedevicergb">deviceColorSpace</a> ... And it does not coincide with <a href="https://developer.apple.com/documentation/corevideo/kcvpixelformattype_32bgra">frequently used</a> color spaces ... </p><br><p>  This is exactly the point where <a href="https://github.com/svtek/SceneKitVideoRecorder/issues/3">the</a> author of one of the public hearths broke, who performs the same task.  All the rest did not even get here. </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/198/181/bc7/198181bc7d908bca2ec56df495010106.gif"></p><br><p>  Well, all these tricks - for the eerie filter? </p><br><p>  Well, I do not!  In the article about ARKit, you can find a mention of the fact that the image from the camera does not use the standard color space, but the extended one.  And even presented the matrix of transformation of color space.  But why engage in transformation, if you can try to record directly in this format?  It remains to find out which format of the 60 available ... </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/75d/802/781/75d8027810eb481eaa1775bd122cb79a.gif"></p><br><p>  And then I took up the search.  I recorded three videos in different streams with different formats, replacing them with each recording. </p><br><p>  As a result, at about the fortieth format, we get its name.  It turns out that this is none other than <a href="https://developer.apple.com/documentation/corevideo/1563591-pixel_format_identifiers/kcvpixelformattype_30rgblepackedwidegamut">kCVPixelFormatType_30RGBLEPackedWideGamut</a> .  How could I not guess? </p><br><p><img src="https://habrastorage.org/getpro/habr/post_images/d6e/2e6/31e/d6e2e631e3dfdc690c85222a51561446.gif"></p><br><p>  But my joy continued until the first tester.  I had no words.  How?  I just spent a lot of time searching for the right format.  It's good that the problem was localized quickly - the bug was reproduced stably and only on 6s and 6s Plus.  Almost immediately after that, I remembered that wide-gamut-enabled displays started to be installed only in the seventh iPhones. </p><br><p>  Having changed wide-gamut to good old 32RGBA, I get a working record!  It remains to understand how to determine that the device supports wide-gamut.  There are still iPads with different types of display, and I thought that for sure you can get an ENUM type of display from the system.  Having rummaged in documentation, I found it <a href="https://developer.apple.com/documentation/uikit/uitraitcollection/1771749-displaygamut">‚âà</a> it is <a href="https://developer.apple.com/documentation/uikit/uitraitcollection/1771749-displaygamut">displayGamut</a> in <a href="https://developer.apple.com/documentation/uikit/uitraitcollection">UITraitCollection</a> . </p><br><p>  When I gave the testers an assembly, I received good news from them - everything worked without any lags even on old devices! </p><br><p>  As a conclusion, I want to tell you - be engaged in 3D graphics!  In our application, for which augmented reality is not the main use case, people over the weekend of the city covered more than 2,000 kilometers, watched more than 3,000 objects and recorded more than 1,000 videos with them!  Imagine what you can do if you do it yourself. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/431880/">https://habr.com/ru/post/431880/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431870/index.html">Developer of interactive books with LEDs complained about the theft of ideas by Google employees</a></li>
<li><a href="../431872/index.html">JavaScript Guide, Part 9: ES7, ES8, and ES9 Features Overview</a></li>
<li><a href="../431874/index.html">Imba: JavaScript-compatible language for quick work with DOM</a></li>
<li><a href="../431876/index.html">Angular Application Optimization</a></li>
<li><a href="../431878/index.html">Unknown javascript features</a></li>
<li><a href="../431884/index.html">Microsoft has overtaken Apple by market capitalization: how did this happen?</a></li>
<li><a href="../431886/index.html">Al Lowy has posted his source code collection for Sierra products on eBay</a></li>
<li><a href="../431888/index.html">‚ÄúI think team ideas are the most important thing when developing a product‚Äù</a></li>
<li><a href="../431890/index.html">How to place an order on the exchange of freelancing</a></li>
<li><a href="../431892/index.html">We use Veeam Backup & Replication to test new systems and applications before upgrading</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>