<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Dynamic Pricing Based on LSTM - ANN in the Home Retail</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is no secret that machine learning methods began to penetrate everywhere in various business areas, optimizing, improving and even creating new bus...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Dynamic Pricing Based on LSTM - ANN in the Home Retail</h1><div class="post__text post__text-html js-mediator-article">  It is no secret that machine learning methods began to penetrate everywhere in various business areas, optimizing, improving and even creating new business processes.  One of the important areas is the question of setting the price for a product and here, with enough data, MO helps to do what was previously difficult to achieve - to recover the multifactor demand curve from the data.  Thanks to the restored demand curve, it became possible to build dynamic pricing systems that allow price optimization depending on the pricing goal - to increase revenue or profit.  This article is a compilation of my dissertation work, in which the LSTM-ANN dynamic pricing model was developed and tested in practice for 4 weeks for one of the home goods retailer's products. <br><a name="habracut"></a><br>  I want to immediately note that in this article I will not disclose the name of the company in which the research was conducted (nevertheless, this is one of the companies from the list presented in the premises), instead I will simply call it Retailer. <br><br><h2>  Prerequisites </h2><br>  There is a price leader in the home goods retail market - Leroy Merlin.  Sales volumes of this network allow them to maintain a minimum price strategy for the entire product range, which leads to price pressure on other market players. <br><br>  Revenues and profits of major retailers in St. Petersburg as of December 31, 2017 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <img src="https://pp.userapi.com/c850120/v850120341/11b56/KxDRU-7Yq0c.jpg" alt="image"><br><br>  In this regard, in Retailer, a different pricing approach is used: <br><br><ul><li>  The price is set at the level of the lowest of competitors; </li><li>  Restriction on the price of the bottom: the purchase price + minimum mark-up, reflecting the approximate cost per unit. </li></ul><br>  This approach is a combination of cost method of pricing and price orientation of competitors.  However, it is not perfect - it does not directly take into account consumer demand. <br><br>  Due to the fact that the dynamic pricing model takes into account many factors (demand, seasonality, promotions, prices of competitors), and also allows you to impose restrictions on the proposed price (for example, from the bottom - covering costs), the system potentially gets rid of all one-sidedness and disadvantages of other pricing methods. <br><br><h2>  Data </h2><br>  For the study, the company provided data from January 2015 to July 2017 (920 days / 131 week).  These data included: <br><br><ul><li>  Daily sales, including weekends, for 470 products (16 product groups); </li><li>  Days of promotions in the store; </li><li>  Days in which discounts were provided for the goods; </li><li>  Prices for each of the 470 products; </li><li>  Daily data on the number of checks throughout the network in St. Petersburg; </li><li>  Prices of major competitors for most of the 470 products (data were taken once a week). </li></ul><br>  In addition to this data, I also added calendar dummy variables: <br><br><ul><li>  Season of the year (autumn / winter / summer / spring); </li><li>  Month; </li><li>  Quarter; </li><li>  Day of the week; </li><li>  Holidays; </li></ul><br>  Also, weather variables: <br><br><ul><li>  Rainfall - dummy; </li><li>  Temperature; </li><li>  The temperature deviation from the average in the season. </li></ul><br>  Directly analyzing the daily sales of goods, I found that: <br>  Only about 30% of the goods were sold all the time, all other goods were either introduced for sale later than 2015, or were withdrawn from sale earlier than 2017, which led to a significant restriction on the choice of goods for research and price experiments.  This also leads us to the fact that due to the constant change of goods in the line of the store, it becomes difficult to create an integrated pricing pricing system, however, there are some ways to get around this problem, which will be discussed later. <br><br><h2>  Pricing system </h2><br>  To build a price recommendation system for a product for the next period of time based on a model predicting demand, I came up with the following scheme: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125ba/ZaHF_uL_FD4.jpg" alt="image"><br><br>  Since, having trained the model on the data, we get the model that restored the multifactor demand curve, giving the input different prices of the goods, we will get the expected sales, depending on this price.  Thus, we can optimize the price to achieve the desired result - to maximize the expected revenue or expected profit.  It remains only to train a model that could well predict sales. <br><br>  What did not work <br>  After selecting one of the products for research, I used XGBoost before going directly to the LSTM model. <br><br>  I did this in the hope that XGBoost will help me to throw away a lot of unnecessary factors (this happens automatically), and the ones that I have left to use for the LSTM model.  I used this approach deliberately, because in order to avoid unnecessary questions in my dissertation defense, I wanted to get a strong and, at the same time, a simple justification of the choice of factors for the model, on the one hand, and on the other, simplification of development.  In addition, I received a ready-made, draft model on which one could quickly try out different ideas in a study.  And after that, going to a final understanding of what will work and what does not, make the final LSTM model. <br><br>  To understand the problem of forecasting, here is the daily sales schedule for the first selected product: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125c4/rwun1M2dbAI.jpg" alt="image"><br><br>  The entire time series of sales on the chart was divided into average sales for the period, so as not to disclose the real values, but keep the look. <br><br>  In general, a lot of noise, while there are pronounced bursts - this is the conduct of promotions at the network level. <br><br>  Since for me it was the first experience in building machine learning models, I had to spend quite a lot of time on various articles and documentation in order for me to eventually get something done. <br><br>  The initial list of factors that presumably affect sales: <br><br><ul><li>  Data on the daily sales of other products from this group, total sales in the group in pieces and the number of checks for all the stores of the chain in St. Petersburg with lags 1, 2, 3, 7, 14, 21, 28; </li><li>  Data on prices of other products from the group; </li><li>  The ratio of the price of the investigated product with the prices of other products from the group; </li><li>  The lowest price among all competitors (the data were taken once a week, and I made an assumption that such prices will be valid for the next week); </li><li>  The ratio of the price of the investigated product with the lowest price from competitors; </li><li>  Sales lags by group (in units); </li><li>  Simple average and RSI based on lags of sales of group products, total sales in the group and the number of checks. </li></ul><br>  A total of 380 factors turned out.  (2.42 observations per factor).  Thus, the problem of clipping is not significant factors was really high, but XGBoost helped to cope with this, significantly cutting the number of factors to 23 (40 observations per factor). <br><br>  The best result I could achieve using greed search is as follows: <br><br><img src="https://pp.userapi.com/c850120/v850120780/126b5/CfMx4e8ESwc.jpg" alt="image"><br>  R ^ 2-adj = 0.4 on the test sample <br><br>  The data were divided into training and test samples without mixing (since this is a time series).  As a metric, I used the R ^ 2 indicator adjusted deliberately, since the presentation of the final results of the work was to be carried out before the commission, including  consisting of business representatives, therefore, it was used as the most famous and easy to understand. <br><br>  The final results diminished my belief in success, because the result of R ^ 2-adj 0.4 meant only that the prediction system would not be able to predict demand the next day well enough, and the price recommendation would differ little from the ‚Äúfinger to the sky‚Äù system. <br><br>  Additionally, I decided to check how much XGBoost will be effective for predicting daily sales for a group of products (in jokes) and predicting the number of checks in the whole network. <br><br>  Sales by product group: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125df/mvImj91seQs.jpg" alt="image"><br>  R ^ 2-adj = 0.71 <br><br>  Checks: <br><br><img src="https://pp.userapi.com/c850120/v850120780/125e9/9cv4Fstpybc.jpg" alt="image"><br>  R ^ 2-adj = 0.86 <br><br>  I think the reason that sales data for a specific product could not be predicted is clear from the graphs presented - noise.  Individual sales of goods turned out to be too susceptible to randomness, so the regression construction method was not effective.  At the same time, by aggregating the data, we removed the influence of randomness and obtained good predictive capabilities. <br><br>  In order to finally make sure that it is a meaningless exercise to predict demand for one day ahead, I used the SARIMAX model (statsmodels package for python) for day sales: <br><br><img src="https://pp.userapi.com/c850120/v850120780/12605/d2HKylyPqKY.jpg" alt="image"><br><br><img src="https://pp.userapi.com/c850120/v850120341/11b45/FNzCYjeSuMM.jpg" alt="image"><br><br>  In fact, the results are no different from those obtained using XGBoost, which suggests that the use of a complex model in this case is not justified. <br><br>  At the same time, I also want to note that neither for XGBoost nor for SARIMAX were the weather factors significant. <br><br><h2>  Construction of the final model </h2><br>  The solution to the quality prediction problem was to aggregate the data into a weekly level.  This reduced the effect of random factors, however, significantly reduced the amount of observed data: if the daily data was 920, then weekly only 131. The situation worsened by the fact that the number of factors remained almost unchanged (dummies were excluded on the days of the week), but the number of observations of the target variable shrunk dramatically. <br><br>  In addition, my task was complicated by the fact that at that time, the company decided to change the product for which the experiment will be carried out using the model, so I had to develop a model from scratch. <br><br>  The change of goods occurred on goods with pronounced seasonality: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b3d/ytgwPwTAb9k.jpg" alt="image"><br><br>  Due to the transition to weekly sales, there was a logical question: is it adequate to use the LSTM model in general on such a small amount of data?  I decided to find out in practice and, first of all, reduce the number of factors (even if it carried the potential damage in reducing relevant information).  I threw out all the factors that are calculated on the basis of sales lags (average, RSI), weather factors (on the daily data, the weather did not matter, and the transfer to the weekly level, especially, lost some sense).  After that, I traditionally used XGBoost to cut off other non-significant factors.  Later, I additionally cut a few more factors, based on the LSTM model, just by eliminating the factors one by one, teaching the model again and comparing the results. <br><br>  The final list of factors is as follows: <br><br><ul><li>  The ratio of price per kilogram of the investigated product and primer CERESIT ST 17 10 l .; </li><li>  The ratio of the price of the investigated product and the product and primer CERESIT ST 17 10 l; </li><li>  The ratio of the price of the investigated product and the primer EURO PRIMER 3 liters; </li><li>  The ratio of the price of the investigated product and the minimum price of competitors; </li><li>  Dummy variables for three network-level promotions; </li><li>  Dummy variable spring, summer, autumn seasons; </li><li>  Lags 1 - 5 week sales of the investigated product. </li></ul><br>  Only 15 factors (9 observations per factor). <br><br>  The final LSTM model was written using Keras, it included 2 hidden layers (25 and 20 neurons, respectively), and the activator was sigmoid. <br><br>  The code for the final LSTM model using Keras: <br><br><pre><code class="python hljs">model = Sequential() model.add(LSTM(<span class="hljs-number"><span class="hljs-number">25</span></span>, return_sequences=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, input_shape=(<span class="hljs-number"><span class="hljs-number">1</span></span>, trainX.shape[<span class="hljs-number"><span class="hljs-number">2</span></span>]))) model.add(LSTM(<span class="hljs-number"><span class="hljs-number">20</span></span>)) model.add(Dense(<span class="hljs-number"><span class="hljs-number">1</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'sigmoid'</span></span>)) model.compile(loss=<span class="hljs-string"><span class="hljs-string">'mean_squared_error'</span></span>, optimizer=<span class="hljs-string"><span class="hljs-string">'adam'</span></span>) model.fit(trainX, trainY, epochs=<span class="hljs-number"><span class="hljs-number">40</span></span>, batch_size=<span class="hljs-number"><span class="hljs-number">1</span></span>, verbose=<span class="hljs-number"><span class="hljs-number">2</span></span>) model.save(<span class="hljs-string"><span class="hljs-string">'LSTM_W.h5'</span></span>)</code> </pre> <br>  Result: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b4d/p2WlWsInzls.jpg" alt="image"><br><br><img src="https://pp.userapi.com/c850120/v850120780/12618/brScDD1suqE.jpg" alt="image"><br><br>  The quality of the prediction on the test sample looked quite convincingly on the metric, however, in my opinion, it did not reach the ideal, because, despite a fairly accurate definition of the average sales level, bursts in individual weeks could be quite deviate from ‚Äú average level of sales, which gave a strong deviation of the sales forecast from reality on certain days (up to 50%).  However, I have already used this model directly to experiment in practice. <br><br>  It is also interesting to see what the restored price demand curve looks like.  To do this, I drove the model across the price range and, based on the predicted sales, built the demand curve: <br><br><img src="https://pp.userapi.com/c850120/v850120341/11b34/bxcLRJ0MYwc.jpg" alt="image"><br><br><h2>  Experiment </h2><br>  Each week, the network provided data on sales for the previous week in St. Petersburg, as well as prices from competitors.  Based on these data, I optimized the price to maximize the expected profit, saying the price that the network should set for the next week, which she did.  This went on for 4 weeks (the deadline was agreed with the retailer). <br><br>  Profit maximization was carried out with restrictions: the minimum price was the purchase price + fix.  surcharge, the maximum price was limited by the price of the primer of the same manufacturer, only in a 10l pack. <br><br>  The results of the experiment are presented in the tables below (all figures are divided by a certain value in order not to reveal the absolute values): <br><br>  Sales Prediction: <br><br><img src="https://pp.userapi.com/c850120/v850120780/1264e/YRDIUbPG9xQ.jpg" alt="image"><br><br>  Profit Prediction: <br><br><img src="https://pp.userapi.com/c850120/v850120780/12656/vcdgOqaU4B4.jpg" alt="image"><br><br>  In order to assess the impact of the new pricing system on sales, I compared sales for the same period, only in previous years. <br><br>  Summary results for 4 weeks: <br><br><img src="https://pp.userapi.com/c850120/v850120780/1265e/n4R8YcCFEqg.jpg" alt="image"><br><br>  As a result, we get a double picture: absolutely unrealistic predictions in terms of sales, but, at the same time, purely positive results in terms of economic indicators (both in profit and revenue). <br><br>  The explanation, in my opinion, is that in this case, the model, incorrectly predicting sales, nevertheless caught the right thought - the price elasticity for this product was below 1, which means that the price could be increased, without fear of falling sales, which we saw (sales in units remained at about the same level as last year and the year before). <br><br>  But do not forget that 4 weeks is a short-term period and the experiment was conducted on only one product.  In the long run, the overpricing of goods in the store usually leads to a drop in sales in the whole store.  To confirm my guess on this score, I decided, using XGBoost, to check whether consumers have a ‚Äúmemory‚Äù of prices for previous periods (if in the past it was more expensive ‚Äúin general‚Äù than its competitors, the consumer goes to competitors).  Those.  will the average price level of the group for the last 1, 3 and 6 months provide for sales by groups of goods? <br><br><img src="https://pp.userapi.com/c850120/v850120780/12698/nJHagmFwIwA.jpg" alt="image"><br><br>  Indeed, the guess was confirmed: one way or another, the average price level for previous periods affects sales in the current period.  This means that it is not enough to optimize the price in the current period for a single product - it is also necessary to take into account the general price level in the long term.  That, in general, leads to a situation where tactics (profit maximization now) contradicts strategy (competitive survival).  This, however, is already better off to marketers. <br><br>  Given the results and experience in my opinion, the most optimal, pricing system based on sales forecast could look like this: <br><br><ol><li>  Rising from the commodity nomenclature to the half of the step above is to carry out cluster analysis and group conditional screwdrivers by similarity and forecast sales and set the price not for a single screwdriver, but for this subgroup - this way we avoid the problem of permanently removing and adding commodity nomenclatures. </li><li>  Carrying out price optimization in the complex - not only for individual subgroups of goods, but also taking into account long-term effects.  For this, you can use a model that predicts sales in the whole network, good, it turned out to be impressively accurate even in day sales. </li></ol><br>  Summing up the work done, I would like to say that for me, as an inexperienced person, in development in general and in MO methods in particular, it was difficult, however, everything turned out to be feasible.  It was also interesting to check for yourself how these methods are applicable in reality.  Having read many articles before, my eyes were burning from the fact that I would try to do everything on my own and I was in anticipation that I would get excellent results.  The practice turned out to be harsh - a small amount of goods with a long sales history, noisy daytime data, mistakes in predicting sales volumes, the use of complex models is not always justified.  Nevertheless, I got an unforgettable experience and learned what it means to use analytics in practice. <br><br>  ‚Üí Based on the work done, I prepared a project in my <a href="https://github.com/feodal01/LSTM_Project">repository</a> <br><br>  In the repository, you will find a dataset generated based on dependencies taken from real data, as well as a python script that allows you to conduct a virtual experiment on this generated data, suggesting you try your luck and outrun the model for the resulting profit by setting the price for the item.  All you need is to download and run the script. <br><br>  I hope my experience will help in determining the boundaries of the use of MO methods and will show that patience and perseverance can achieve results, even if you are not a professional in some field. </div><p>Source: <a href="https://habr.com/ru/post/421429/">https://habr.com/ru/post/421429/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../421417/index.html">Coffee machine for coffee independent or Wacaco mobile coffee machine</a></li>
<li><a href="../421419/index.html">An ode to ‚Äúfoamed‚Äù nickel, non-existent sapphires and the Soviet deputy minister: the iconic OTTO SX-P1 in Japan, the USA and the USSR</a></li>
<li><a href="../421421/index.html">LAppS: Half a million 1KB-WebSocket messages per second with TLS on one CPU</a></li>
<li><a href="../421423/index.html">Enterprise DevOps: as in the big company collect microservices</a></li>
<li><a href="../421425/index.html">Tame and Consolidate: The Story of Moving to the Oracle Supercluster</a></li>
<li><a href="../421431/index.html">Time Management, or Effective Chaos Management</a></li>
<li><a href="../421433/index.html">There are exactly a day left before the server starts</a></li>
<li><a href="../421435/index.html">‚ÄúWhy are we doing all this?‚Äù - the creator of Prisma and the former leader of VK projects about his new secret project</a></li>
<li><a href="../421439/index.html">The Japanese presented a prototype processor for an ex-flop supercomputer: how does a chip work?</a></li>
<li><a href="../421441/index.html">Embox begins climbing Elbrus</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>