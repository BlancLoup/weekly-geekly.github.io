<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Python Testing with pytest. Chapter 2, Writing Test Functions</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Back Next 


 You will learn how to organize tests in classes, modules and directories. Then I will show you how to use markers to mark which tests yo...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">🔎</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">📜</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">⬆️</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">⬇️</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Python Testing with pytest. Chapter 2, Writing Test Functions</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/jl/jn/bb/jljnbbjr-ejh473xy_eccsmknpk.png">  <a href="https://habr.com/ru/post/448782/">Back</a> <a href="https://habr.com/ru/post/448786/">Next</a> <img src="https://habrastorage.org/webt/rw/dy/-g/rwdy-grsvbpcetjttrmecdkxtlk.png"></p><br><p>  <em>You will learn how to organize tests in classes, modules and directories.</em>  <em>Then I will show you how to use markers to mark which tests you want to run, and discuss how embedded markers can help you skip tests and mark tests, waiting for failure.</em>  <em>Finally, I will talk about test parameterization, which allows tests to be invoked with different data.</em> </p><br><p><img src="https://habrastorage.org/webt/hd/--/9w/hd--9w134j0rxhmxftrflbbdopy.png"></p><a name="habracut"></a><br><p>  The examples in this book are written using Python 3.6 and pytest 3.2.  pytest 3.2 supports Python 2.6, 2.7 and Python 3.3+. </p><br><blockquote>  The source code for the Tasks project, as well as for all the tests shown in this book, is available via the <a href="https://pragprog.com/titles/bopytest/source_code" title="https://pragprog.com/titles/bopytest/source_code">link</a> on the book's web page at <a href="https://pragprog.com/titles/bopytest" title="https://pragprog.com/titles/bopytest">pragprog.com</a> .  You do not need to download the source code to understand the test code;  The test code is presented in a convenient form in the examples.  But to follow along with the project objectives, or to adapt test examples to test your own project (your hands are untied!), You should go to the book’s web page and download the work.  In the same place, on the book’s web page there is a link for <a href="https://pragprog.com/titles/bopytest/errata" title="https://pragprog.com/titles/bopytest/errata">errata</a> messages and a <a href="https://forums.pragprog.com/forums/438" title="https://forums.pragprog.com/forums/438">discussion forum</a> . </blockquote><p>  Under the spoiler is a list of articles in this series. </p><br><div class="spoiler">  <b class="spoiler_title">Table of contents</b> <div class="spoiler_text"><ul><li>  <a href="https://habr.com/ru/post/426699/"><strong>Introduction</strong></a> </li><li>  <a href="https://habr.com/ru/post/448782/"><strong>Chapter 1: Getting Started with pytest</strong></a> </li><li>  <a href="https://habr.com/ru/post/448788/"><strong>Chapter 2: Writing Test Functions</strong></a> (This article) </li><li>  <a href="https://habr.com/ru/post/448786/"><strong>Chapter 3: Pytest Fixtures</strong></a> </li><li>  <a href="https://habr.com/ru/post/448792/"><strong>Chapter 4: Builtin Fixtures</strong></a> </li><li>  <a href="https://habr.com/ru/post/448794/"><strong>Chapter 5: Plugins</strong></a> </li><li>  <a href="https://habr.com/ru/post/448796/"><strong>Chapter 6: Configuration</strong></a> </li><li>  <a href="https://habr.com/ru/post/448798/"><strong>Chapter 7: Using pytest with other tools</strong></a> </li></ul></div></div><br><p>  In the previous chapter, you started pytest.  You saw how to run it with files and directories and how many of the options worked.  In this chapter, you will learn how to write test functions in the context of testing a Python package.  If you are using pytest to test anything other than the Python package, most of this chapter will be useful. </p><br><p>  We will write tests for the Tasks package.  Before we do this, I’ll talk about the structure of the Python distribution package and the tests for it, as well as how to make the tests see the test package.  Then I will show you how to use assert in tests, how tests handle unexpected exceptions and test expected exceptions. </p><br><p>  In the end, we will have a lot of tests.  This way you will learn how to organize tests into classes, modules and directories.  Then I will show you how to use markers to mark which tests you want to run, and discuss how embedded markers can help you skip tests and mark tests, waiting for failure.  Finally, I will talk about test parameterization, which allows tests to be invoked with different data. </p><br><blockquote> <strong><em>Note of translator:</em></strong> <strong><em>If you are using Python version 3.5 or 3.6,</em></strong> then when you run tests in Chapter 2, you may receive messages like this <br><img src="https://habrastorage.org/webt/57/eq/4x/57eq4xsrdjfiyxn9g9ihresujc8.jpeg"><br>  This problem is treated by correcting <code>...\code\tasks_proj\src\tasks\tasksdb_tinydb.py</code> and reinstalling the tasks package. <br><pre> <code class="plaintext hljs">$ cd /path/to/code $ pip install ./tasks_proj/`</code> </pre> <br><br>  You <code>eids</code> to <code>doc_ids</code> named parameters on <code>doc_ids</code> and <code>eid</code> on <code>doc_id</code> in the module <code>...\code\tasks_proj\src\tasks\tasksdb_tinydb.py</code> <br><br>  Explanations See <code>#83783</code> <a href="https://pragprog.com/titles/bopytest/errata">here</a> </blockquote><br><h2 id="testirovanie-paketa">  Package testing </h2><br><p>  To learn how to write test functions for the Python package, we will use the sample Tasks project, as described in the Tasks project on page xii.  Tasks is a Python package that includes a command line tool with the same name as a task. </p><br><p>  Appendix 4 “Packaging and Distributing Python Projects” on page 175 includes an explanation of how to distribute your projects locally within a small team or globally through PyPI, so I will not understand in detail how to do this;  however, let's quickly consider what is in the “Tasks” project and how different files fit into the testing history of this project. </p><br><p>  The following is the file structure of the Tasks project: </p><br><pre> <code class="plaintext hljs">tasks_proj/ ├── CHANGELOG.rst ├── LICENSE ├── MANIFEST.in ├── README.rst ├── setup.py ├── src │ └── tasks │ ├── __init__.py │ ├── api.py │ ├── cli.py │ ├── config.py │ ├── tasksdb_pymongo.py │ └── tasksdb_tinydb.py └── tests ├── conftest.py ├── pytest.ini ├── func │ ├── __init__.py │ ├── test_add.py │ └── ... └── unit ├── __init__.py ├── test_task.py └── ...</code> </pre> <br><p>  I included the complete project list (with the exception of the complete list of test files) to indicate how the tests fit into the rest of the project, and point out several files that are key to testing, namely <em>conftest.py, pytest.ini</em> , various <em><code>__init__.py</code></em> files and <em>setup.py</em> . </p><br><p>  All tests are stored in <em>tests</em> and separately from the package source files in <em>src</em> .  This is not a pytest requirement, but it is best practice. </p><br><p>  All top-level files, <em>CHANGELOG.rst, LICENSE, README.rst, MANIFEST.in</em> , and <em>setup.py</em> , are discussed in more detail in Appendix 4, Packaging and Distributing Python Projects, on page 175. Although <em>setup.py is</em> important for building the distribution from the package, as well as to be able to install the package locally so that the package is available for import. </p><br><p>  Functional and unit tests are divided into their own directories.  This is an arbitrary decision and not required.  However, organizing test files into multiple directories makes it easy to run a subset of tests.  I like to separate the functional and unit tests, because functional tests should break only if we intentionally change the functionality of the system, while the unit tests can break during refactoring or implementation changes. </p><br><p>  The project contains two types of <code>__init__.py</code> files: those found in the <code>src/</code> directory and those that are in <code>tests/</code> .  The <code>src/tasks/__init__.py</code> tells Python that the directory is a package.  It also acts as the main interface for the package when someone uses <code>import tasks</code> .  It contains code for importing certain functions from <code>api.py</code> , so <code>cli.py</code> and our test files can access the package functions, for example, <code>tasks.add()</code> , instead of running <code>task.api.add ()</code> .  The <code>tests/func/__init__.py</code> and <code>tests/unit/__init__.py</code> are empty.  They tell the pytest to go up one directory to find the root of the test directory and the <code>pytest.ini</code> file. </p><br><p>  The <code>pytest.ini</code> file is optional.  It contains the overall pytest configuration for the entire project.  Your project should have no more than one of them.  It may contain directives that change the behavior of pytest, for example, setting options for a list of parameters that will always be used.  You will learn all about <code>pytest.ini</code> in Chapter 6, “Configuration,” on page 113. </p><br><p>  The conftest.py file is also optional.  It is considered pytest as a “local plugin” and may contain hook functions and fixtures.  <em>Hook functions</em> are a way to insert code into the pytest part of the process to change how pytest works.  Fixtures are the setup and teardown functions that run before and after test functions and can be used to represent the resources and data used by tests.  (Fixtures are discussed in chapter 3, pytest Fixtures, on page 49 and chapter 4, Builtin Fixtures, on page 71, and hook functions are discussed in chapter 5 “Plugins” on page 95.) Hook functions and fixtures, which are used in tests in several subdirectories should be contained in tests / conftest.py.  You can have multiple conftest.py files;  for example, you can have one in tests and one for each tests subdirectory. </p><br><p>  If you have not done so already, you can download a copy of the source code for this project from <a href="https://pragprog.com/titles/bopytest/source_code">the</a> book’s <a href="https://pragprog.com/titles/bopytest/source_code">website</a>  Alternatively, you can work on your project with a similar structure. </p><br><p>  Here is test_task.py: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / unit / test_task.py</strong> </blockquote><br><pre> <code class="plaintext hljs">"""Test the Task data type.""" # -*- coding: utf-8 -*- from tasks import Task def test_asdict(): """_asdict()   .""" t_task = Task('do something', 'okken', True, 21) t_dict = t_task._asdict() expected = {'summary': 'do something', 'owner': 'okken', 'done': True, 'id': 21} assert t_dict == expected def test_replace(): """replace ()      .""" t_before = Task('finish book', 'brian', False) t_after = t_before._replace(id=10, done=True) t_expected = Task('finish book', 'brian', True, 10) assert t_after == t_expected def test_defaults(): """        .""" t1 = Task() t2 = Task(None, None, False, None) assert t1 == t2 def test_member_access(): """ .field  namedtuple.""" t = Task('buy milk', 'brian') assert t.summary == 'buy milk' assert t.owner == 'brian' assert (t.done, t.id) == (False, None)</code> </pre> <br><p>  This <em>test</em> import statement is specified in the <em>test_task.py</em> file: </p><br><pre> <code class="plaintext hljs">from tasks import Task</code> </pre> <br><p>  The best way to allow tests to import tasks or import something from tasks is to set tasks locally with pip.  This is possible because there is a setup.py file for directly invoking pip. </p><br><p>  Install tasks by running <code>pip install .</code>  or <code>pip install -e .</code>  from the tasks_proj directory.  Or another option to run <code>pip install -e tasks_proj</code> from the directory one level higher: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code $ pip install ./tasks_proj/ $ pip install --no-cache-dir ./tasks_proj/ Processing ./tasks_proj Collecting click (from tasks==0.1.0) Downloading click-6.7-py2.py3-none-any.whl (71kB) ... Collecting tinydb (from tasks==0.1.0) Downloading tinydb-3.4.0.tar.gz Collecting six (from tasks==0.1.0) Downloading six-1.10.0-py2.py3-none-any.whl Installing collected packages: click, tinydb, six, tasks Running setup.py install for tinydb ... done Running setup.py install for tasks ... done Successfully installed click-6.7 six-1.10.0 tasks-0.1.0 tinydb-3.4.0</code> </pre> <br><p>  If you only want to run tests for tasks, this command will do.  If you want to be able to change the source code during the installation of tasks, you need to use the installation with the -e option (for editable “editable”): </p><br><pre> <code class="plaintext hljs">$ pip install -e ./tasks_proj/ Obtaining file:///path/to/code/tasks_proj Requirement already satisfied: click in /path/to/venv/lib/python3.6/site-packages (from tasks==0.1.0) Requirement already satisfied: tinydb in /path/to/venv/lib/python3.6/site-packages (from tasks==0.1.0) Requirement already satisfied: six in /path/to/venv/lib/python3.6/site-packages (from tasks==0.1.0) Installing collected packages: tasks Found existing installation: tasks 0.1.0 Uninstalling tasks-0.1.0: Successfully uninstalled tasks-0.1.0 Running setup.py develop for tasks Successfully installed tasks</code> </pre> <br><p>  Now let's try running the tests: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch2/tasks_proj/tests/unit $ pytest test_task.py ===================== test session starts ====================== collected 4 items test_task.py .... =================== 4 passed in 0.01 seconds ===================</code> </pre> <br><p>  Import worked!  The rest of the tests can now safely use import tasks.  Now we will write some tests. </p><br><h2 id="ispolzovanie-operatorov-assert">  Using assert statements </h2><br><p>  When you write test functions, the usual Python assert statement is your primary tool for reporting test failures.  The simplicity of this in pytest is brilliant.  This is what makes many developers use pytest on top of other frameworks. </p><br><p>  If you used any other testing platform, you probably saw the various assert helper functions.  For example, the following is a list of some assert forms and assert helper functions: </p><br><table><thead><tr><th>  <strong>pytest</strong> </th><th>  <strong>unittest</strong> </th></tr></thead><tbody><tr><td>  assert something </td><td>  assertTrue (something) </td></tr><tr><td>  assert a == b </td><td>  assertEqual (a, b) </td></tr><tr><td>  assert a &lt;= b </td><td>  assertLessEqual (a, b) </td></tr><tr><td>  ... </td><td>  ... </td></tr></tbody></table><br><p>  With pytest, you can use assert &lt;expression&gt; with any expression.  If the expression is evaluated as False, when it is converted to bool, the test will end with an error. </p><br><p>  pytest includes a feature called assert rewriting, which intercepts assert calls and replaces them with something that can tell you more about why your statements failed.  Let's see how useful this rewriting is if you look at a few assertion errors: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / unit / test_task_fail.py</strong> </blockquote><br><pre> <code class="plaintext hljs">""" the Task type    .""" from tasks import Task def test_task_equality(): """     .""" t1 = Task('sit there', 'brian') t2 = Task('do something', 'okken') assert t1 == t2 def test_dict_equality(): """ ,   dicts,    .""" t1_dict = Task('make sandwich', 'okken')._asdict() t2_dict = Task('make sandwich', 'okkem')._asdict() assert t1_dict == t2_dict</code> </pre> <br><p>  All these tests fail, but the information in the trace is interesting: </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\unit&gt;pytest test_task_fail.py ============================= test session starts ============================= collected 2 items test_task_fail.py FF ================================== FAILURES =================================== _____________________________ test_task_equality ______________________________ def test_task_equality(): """Different tasks should not be equal.""" t1 = Task('sit there', 'brian') t2 = Task('do something', 'okken') &gt; assert t1 == t2 E AssertionError: assert Task(summary=...alse, id=None) == Task(summary='...alse, id=None) E At index 0 diff: 'sit there' != 'do something' E Use -v to get the full diff test_task_fail.py:9: AssertionError _____________________________ test_dict_equality ______________________________ def test_dict_equality(): """Different tasks compared as dicts should not be equal.""" t1_dict = Task('make sandwich', 'okken')._asdict() t2_dict = Task('make sandwich', 'okkem')._asdict() &gt; assert t1_dict == t2_dict E AssertionError: assert OrderedDict([...('id', None)]) == OrderedDict([(...('id', None)]) E Omitting 3 identical items, use -vv to show E Differing items: E {'owner': 'okken'} != {'owner': 'okkem'} E Use -v to get the full diff test_task_fail.py:16: AssertionError ========================== 2 failed in 0.30 seconds ===========================</code> </pre> <br><p>  Wow!  This is a lot of information.  For each failed test, the exact error string is displayed using the&gt; Failure Pointer.  Lines E show additional information about assert failure to help you understand what went wrong. </p><br><p>  I deliberately put two mismatches in <code>test_task_equality()</code> , but only the first one was shown in the previous code.  Let's try again with the <code>-v</code> flag, as suggested in the error message: </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\unit&gt;pytest -v test_task_fail.py ============================= test session starts ============================= collected 2 items test_task_fail.py::test_task_equality FAILED test_task_fail.py::test_dict_equality FAILED ================================== FAILURES =================================== _____________________________ test_task_equality ______________________________ def test_task_equality(): """Different tasks should not be equal.""" t1 = Task('sit there', 'brian') t2 = Task('do something', 'okken') &gt; assert t1 == t2 E AssertionError: assert Task(summary=...alse, id=None) == Task(summary='...alse, id=None) E At index 0 diff: 'sit there' != 'do something' E Full diff: E - Task(summary='sit there', owner='brian', done=False, id=None) E ? ^^^ ^^^ ^^^^ E + Task(summary='do something', owner='okken', done=False, id=None) E ? +++ ^^^ ^^^ ^^^^ test_task_fail.py:9: AssertionError _____________________________ test_dict_equality ______________________________ def test_dict_equality(): """Different tasks compared as dicts should not be equal.""" t1_dict = Task('make sandwich', 'okken')._asdict() t2_dict = Task('make sandwich', 'okkem')._asdict() &gt; assert t1_dict == t2_dict E AssertionError: assert OrderedDict([...('id', None)]) == OrderedDict([(...('id', None)]) E Omitting 3 identical items, use -vv to show E Differing items: E {'owner': 'okken'} != {'owner': 'okkem'} E Full diff: E {'summary': 'make sandwich', E - 'owner': 'okken', E ? ^... E E ...Full output truncated (5 lines hidden), use '-vv' to show test_task_fail.py:16: AssertionError ========================== 2 failed in 0.28 seconds ===========================</code> </pre> <br><p>  Well, I think it's damn cool!  pytest not only could find both differences, but also showed us exactly where these differences are.  In this example, only equality assert is used;  <a href="http://doc.pytest.org/en/latest/example/reportingdemo.html">There</a> are many more variations of the assert operator on the <a href="http://doc.pytest.org/en/latest/example/reportingdemo.html">pytest.org</a> website with amazing trace debugging information. </p><br><h2 id="ozhidanie-isklyucheniy-expected-exception">  Expectation Expected </h2><br><p>  Exceptions can occur in several places in the Tasks API.  Let's quickly take a look at the functions found in <em>tasks / api.py</em> : </p><br><pre> <code class="plaintext hljs">def add(task): # type: (Task) -\&gt; int def get(task_id): # type: (int) -\&gt; Task def list_tasks(owner=None): # type: (str|None) -\&gt; list of Task def count(): # type: (None) -\&gt; int def update(task_id, task): # type: (int, Task) -\&gt; None def delete(task_id): # type: (int) -\&gt; None def delete_all(): # type: () -\&gt; None def unique_id(): # type: () -\&gt; int def start_tasks_db(db_path, db_type): # type: (str, str) -\&gt; None def stop_tasks_db(): # type: () -\&gt; None</code> </pre> <br><p>  There is an agreement between the CLI code in <em>cli.py</em> and the API code in <em>api.py</em> regarding which types will be passed to the API functions.  API calls are where I expect exceptions to be raised if the type is incorrect.  To make sure that these functions raise exceptions, if they are called incorrectly, use the wrong type in the test function to intentionally raise TypeError exceptions and use them with pytest.raises (expected exception), for example: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / func / test_api_exceptions.py</strong> </blockquote><br><pre> <code class="plaintext hljs">"""    -   API.""" import pytest import tasks def test_add_raises(): """add()       param.""" with pytest.raises(TypeError): tasks.add(task='not a Task object')</code> </pre> <br><p>  In <code>test_add_raises()</code> , with <code>pytest.raises(TypeError)</code> : the operator reports that everything that is in the next block of code should throw a TypeError exception.  If an exception is not raised, the test fails.  If the test causes another exception, it fails. </p><br><p>  We just checked the exception type in <code>test_add_raises()</code> .  You can also check the exclusion options.  For <code>start_tasks_db(db_path, db_type)</code> , not only <em>db_type</em> should be a string, it really should be either 'tiny' or 'mongo'.  You can check to make sure that the exception message is correct by adding excinfo: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / func / test_api_exceptions.py</strong> </blockquote><br><pre> <code class="plaintext hljs">def test_start_tasks_db_raises(): """,     .""" with pytest.raises(ValueError) as excinfo: tasks.start_tasks_db('some/great/path', 'mysql') exception_msg = excinfo.value.args[0] assert exception_msg == "db_type must be a 'tiny' or 'mongo'"</code> </pre> <br><p>  This allows us to look more closely at this exception.  The variable name after as (in this case excinfo) is filled with information about the exception and is of type ExceptionInfo. </p><br><p>  In our case, we want to make sure that the first (and only) exclusion parameter matches the string. </p><br><h2 id="marking-test-functions">  Marking Test Functions </h2><br><p>  pytest provides a cool mechanism for putting markers in test functions.  A test can have more than one marker, and a marker can be in several tests. </p><br><p>  Markers will make sense to you after you see them in action.  Suppose we want to run a subset of our tests as a quick “smoke test” to get an idea of ​​whether there is some serious gap in the system.  Smoke tests by convention are not comprehensive, thorough test suites, but a selected subset that can be quickly launched and give the developer a decent understanding of the health of all parts of the system. </p><br><p>  To add a smoke test suite to the Tasks project, you need to add <code>@mark.pytest.smoke</code> for some tests.  Let's add it to several <code>test_api_exceptions.py</code> tests (note that the <em>smoke</em> and <em>get</em> markers are not built into pytest; I just made them up): </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / func / test_api_exceptions.py</strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.smoke def test_list_raises(): """list()       param.""" with pytest.raises(TypeError): tasks.list_tasks(owner=123) @pytest.mark.get @pytest.mark.smoke def test_get_raises(): """get()       param.""" with pytest.raises(TypeError): tasks.get(task_id='123')</code> </pre> <br><p>  Now let's run only those tests that are marked with <code>-m marker_name</code> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests&gt;cd func (venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v -m "smoke" test_api_exceptions.py ============================= test session starts ============================= collected 7 items test_api_exceptions.py::test_list_raises PASSED test_api_exceptions.py::test_get_raises PASSED ============================= 5 tests deselected ============================== =================== 2 passed, 5 deselected in 0.18 seconds ==================== (venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt; (venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v -m "get" test_api_exceptions.py ============================= test session starts ============================= collected 7 items test_api_exceptions.py::test_get_raises PASSED ============================= 6 tests deselected ============================== =================== 1 passed, 6 deselected in 0.13 seconds ====================</code> </pre> <br><p>  Remember that <code>-v</code> short for <code>--verbose</code> and allows us to see the names of the tests that are being run.  Using -m 'smoke' runs both tests marked @ pytest.mark.smoke. </p><br><p>  Using <code>-m</code> 'get' will run one test labeled <code>@pytest.mark.get</code> .  Pretty simple. </p><br><p>  Everything becomes wonder and wonder!  The expression after <code>-m</code> can use <code>and</code> , <code>or</code> and <code>not</code> combine several markers: </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v -m "smoke and get" test_api_exceptions.py ============================= test session starts ============================= collected 7 items test_api_exceptions.py::test_get_raises PASSED ============================= 6 tests deselected ============================== =================== 1 passed, 6 deselected in 0.13 seconds ====================</code> </pre> <br><p>  We conducted this test only with <code>smoke</code> and <code>get</code> markers.  We can use and <code>not</code> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v -m "smoke and not get" test_api_exceptions.py ============================= test session starts ============================= collected 7 items test_api_exceptions.py::test_list_raises PASSED ============================= 6 tests deselected ============================== =================== 1 passed, 6 deselected in 0.13 seconds ====================</code> </pre> <br><p>  The addition of <code>-m 'smoke and not get'</code> chose a test that was marked using <code>@pytest.mark.smoke</code> , but not <code>@pytest.mark.get</code> . </p><br><h3 id="zapolnenie-smoke-test">  Filling Smoke Test </h3><br><p>  Previous tests do not seem to be a sensible set of <code>smoke test</code> tests.  We did not actually touch the database and did not add any tasks.  Of course the <code>smoke test</code> would have to do this. </p><br><p>  Let's add some tests that consider adding a task, and use one of them as part of our <em>smoke</em> test suite: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / func / test_add.py</strong> </blockquote><br><pre> <code class="plaintext hljs">"""  API tasks.add ().""" import pytest import tasks from tasks import Task def test_add_returns_valid_id(): """tasks.add(valid task)    .""" # GIVEN an initialized tasks db # WHEN a new task is added # THEN returned task_id is of type int new_task = Task('do something') task_id = tasks.add(new_task) assert isinstance(task_id, int) @pytest.mark.smoke def test_added_task_has_id_set(): """,   task_id  tasks.add().""" # GIVEN an initialized tasks db # AND a new task is added new_task = Task('sit in chair', owner='me', done=True) task_id = tasks.add(new_task) # WHEN task is retrieved task_from_db = tasks.get(task_id) # THEN task_id matches id field assert task_from_db.id == task_id</code> </pre> <br><p>  Both of these tests have a GIVEN comment on the initialized database of tasks, but there is no initialized database in the test.  We can define fixture to initialize the database before the test and clean it after the test: </p><br><blockquote>  <strong>ch2 / tasks_proj / tests / func / test_add.py</strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.fixture(autouse=True) def initialized_tasks_db(tmpdir): """Connect to db before testing, disconnect after.""" # Setup : start db tasks.start_tasks_db(str(tmpdir), 'tiny') yield #    # Teardown : stop db tasks.stop_tasks_db()</code> </pre> <br><p>  The fixture, tmpdir, used in this example is builtin (builtin fixture).  You will learn all about embedded fixtures in chapter 4, Builtin Fixtures, on page 71, and you will learn about writing your own fixtures and how they work in chapter 3, pytest Fixtures, on page 49, including the autouse parameter used here. </p><br><p>  The autouse used in our test shows that all tests in this file will use fixture.  The pre- <code>yield</code> code is executed before each test;  code after <code>yield</code> is executed after the test.  If desired, yield can return data to the test.  You will look at all this and much more in subsequent chapters, but here we need to somehow set up the database for testing, so I can’t wait any longer, and I must show you this device (fixture of course!).  (pytest also supports the old-fashioned setup and teardown functions, such as those used in <strong>unittest</strong> and <strong>nose</strong> , but they are not so interesting. However, if you are still interested, they are described in Appendix 5, xUnit Fixtures, on page 183.) </p><br><p>  Let's postpone the discussion of fixtures and move on to the beginning of the project and launch our <em>smoke test suite</em> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;cd .. (venv33) ...\bopytest-code\code\ch2\tasks_proj\tests&gt;cd .. (venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest -v -m "smoke" ============================= test session starts ============================= collected 56 items tests/func/test_add.py::test_added_task_has_id_set PASSED tests/func/test_api_exceptions.py::test_list_raises PASSED tests/func/test_api_exceptions.py::test_get_raises PASSED ============================= 53 tests deselected ============================= =================== 3 passed, 53 deselected in 0.49 seconds ===================</code> </pre> <br><p>  It shows that labeled tests from different files can be run together. </p><br><h2 id="propusk-testov-skipping-tests">  Skipping Tests </h2><br><p>  Although the markers discussed in labeling verification methods were the names of your choice on page 31, pytest includes some useful built-in markers: <code>skip</code> , <code>skipif</code> , and <code>xfail</code> .  In this section, I will talk about <code>skip</code> and <code>skipif</code> , and in the next one, <code>-xfail</code> . </p><br><p>  Markers <code>skip</code> and <code>skipif</code> allow you to skip tests that do not need to perform.  For example, let's say we did not know how <code>tasks.unique_id()</code> should work.  Should each call return a different number?    ,       ? </p><br><p> -,    (,        <code>initialized_tasks_db</code> ;     ): </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_unique_id_1.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">"""Test tasks.unique_id().""" import pytest import tasks def test_unique_id(): """ unique_id ()     .""" id_1 = tasks.unique_id() id_2 = tasks.unique_id() assert id_1 != id_2</code> </pre> <br><p>    : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest test_unique_id_1.py ============================= test session starts ============================= collected 1 item test_unique_id_1.py F ================================== FAILURES =================================== _______________________________ test_unique_id ________________________________ def test_unique_id(): """Calling unique_id() twice should return different numbers.""" id_1 = tasks.unique_id() id_2 = tasks.unique_id() &gt; assert id_1 != id_2 E assert 1 != 1 test_unique_id_1.py:11: AssertionError ========================== 1 failed in 0.30 seconds ===========================</code> </pre> <br><p>  Hm  ,  .   API  ,  ,  docstring  """Return an integer that does not exist in the db.""",   <em>  ,     DB</em> .      .       ,   : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_unique_id_2.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.skip(reason='misunderstood the API') def test_unique_id_1(): """ unique_id ()     .""" id_1 = tasks.unique_id() id_2 = tasks.unique_id() assert id_1 != id_2 def test_unique_id_2(): """unique_id()    id.""" ids = [] ids.append(tasks.add(Task('one'))) ids.append(tasks.add(Task('two'))) ids.append(tasks.add(Task('three'))) #   id uid = tasks.unique_id() # ,        assert uid not in ids</code> </pre> <br><p>  ,   ,   ,   <code>@pytest..skip()</code>    . </p><br><p>  : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_unique_id_2.py ============================= test session starts ============================= collected 2 items test_unique_id_2.py::test_unique_id_1 SKIPPED test_unique_id_2.py::test_unique_id_2 PASSED ===================== 1 passed, 1 skipped in 0.19 seconds =====================</code> </pre> <br><p>  ,   -   ,       ,         0.2.0 .           skipif: </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_unique_id_3.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.skipif(tasks.__version__ &lt; '0.2.0', reason='not supported until version 0.2.0') def test_unique_id_1(): """ unique_id ()     .""" id_1 = tasks.unique_id() id_2 = tasks.unique_id() assert id_1 != id_2</code> </pre> <br><p> ,     <code>skipif()</code> ,      Python.   ,  ,    .      <em>skip</em> ,    <em>skipif</em> .     <em>skip</em> ,     <em>skipif</em> .      ( <em>reason</em> )   <em>skip</em> , <em>skipif</em>  <em>xfail</em> .    : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest test_unique_id_3.py ============================= test session starts ============================= collected 2 items test_unique_id_3.py s. ===================== 1 passed, 1 skipped in 0.20 seconds =====================</code> </pre> <br><p> <code>s.</code> ,     (skipped),    (passed).   ,    -  <code>-v</code> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_unique_id_3.py ============================= test session starts ============================= collected 2 items test_unique_id_3.py::test_unique_id_1 SKIPPED test_unique_id_3.py::test_unique_id_2 PASSED ===================== 1 passed, 1 skipped in 0.19 seconds =====================</code> </pre> <br><p>       .        <code>-rs</code> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -rs test_unique_id_3.py ============================= test session starts ============================= collected 2 items test_unique_id_3.py s. =========================== short test summary info =========================== SKIP [1] func\test_unique_id_3.py:8: not supported until version 0.2.0 ===================== 1 passed, 1 skipped in 0.22 seconds =====================</code> </pre> <br><p>  <code>-r chars</code>    : </p><br><pre> <code class="plaintext hljs">$ pytest --help ... -r chars show extra test summary info as specified by chars (     ,  ) (f)ailed, (E)error, (s)skipped, (x)failed, (X)passed, (p)passed, (P)passed with output, (a)all except pP. ...</code> </pre> <br><p>        ,           . </p><br><h2 id="markirovka-testov-ozhidayuschih-sboya">     </h2><br><p>    <code>skip</code>  <code>skipif</code>    ,   .    <code>xfail</code>   pytest   ,  ,    .     <code>unique_id ()</code> ,   <code>xfail</code> : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_unique_id_4.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.xfail(tasks.__version__ &lt; '0.2.0', reason='not supported until version 0.2.0') def test_unique_id_1(): """ unique_id()     .""" id_1 = tasks.unique_id() id_2 = tasks.unique_id() assert id_1 != id_2 @pytest.mark.xfail() def test_unique_id_is_a_duck(): """ xfail.""" uid = tasks.unique_id() assert uid == 'a duck' @pytest.mark.xfail() def test_unique_id_not_a_duck(): """ xpass.""" uid = tasks.unique_id() assert uid != 'a duck'</code> </pre> <br><p> Running this shows: </p><br><p>    ,   ,   <code>xfail</code> .         == vs.! =.      . </p><br><p>   : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest test_unique_id_4.py ============================= test session starts ============================= collected 4 items test_unique_id_4.py xxX. =============== 1 passed, 2 xfailed, 1 xpassed in 0.36 seconds ================</code> </pre> <br><p> X  XFAIL,   «  ( <em>expected to fail</em> )».  X   XPASS  «,    ,   ( <em>expected to fail but passed.</em> )». </p><br><p> <code>--verbose</code>    : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_unique_id_4.py ============================= test session starts ============================= collected 4 items test_unique_id_4.py::test_unique_id_1 xfail test_unique_id_4.py::test_unique_id_is_a_duck xfail test_unique_id_4.py::test_unique_id_not_a_duck XPASS test_unique_id_4.py::test_unique_id_2 PASSED =============== 1 passed, 2 xfailed, 1 xpassed in 0.36 seconds ================</code> </pre> <br><p>    <em>pytest</em> ,  ,  ,    <code>xfail</code> ,   FAIL.    <em>pytest.ini</em> : </p><br><pre> <code class="plaintext hljs">[pytest] xfail_strict=true</code> </pre> <br><p>    <em>pytest.ini</em>    6, ,  . 113. </p><br><h2 id="vypolnenie-podmnozhestva-testov">    </h2><br><p>    ,         ​​    .       .        , ,          .      ,       .        .    . </p><br><h3 id="a-single-directory"> A Single Directory </h3><br><p>       ,      <em>pytest</em> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest tests\func --tb=no ============================= test session starts ============================= collected 50 items tests\func\test_add.py .. tests\func\test_add_variety.py ................................ tests\func\test_api_exceptions.py ....... tests\func\test_unique_id_1.py F tests\func\test_unique_id_2.py s. tests\func\test_unique_id_3.py s. tests\func\test_unique_id_4.py xxX. ==== 1 failed, 44 passed, 2 skipped, 2 xfailed, 1 xpassed in 1.75 seconds =====</code> </pre> <br><p>     ,   <code>-v</code>      ,   . </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest -v tests\func --tb=no ============================= test session starts =============================</code> </pre> <br><p>  ... </p><br><pre> <code class="plaintext hljs">collected 50 items tests\func\test_add.py::test_add_returns_valid_id PASSED tests\func\test_add.py::test_added_task_has_id_set PASSED tests\func\test_add_variety.py::test_add_1 PASSED tests\func\test_add_variety.py::test_add_2[task0] PASSED tests\func\test_add_variety.py::test_add_2[task1] PASSED tests\func\test_add_variety.py::test_add_2[task2] PASSED tests\func\test_add_variety.py::test_add_2[task3] PASSED tests\func\test_add_variety.py::test_add_3[sleep-None-False] PASSED ... tests\func\test_unique_id_2.py::test_unique_id_1 SKIPPED tests\func\test_unique_id_2.py::test_unique_id_2 PASSED ... tests\func\test_unique_id_4.py::test_unique_id_1 xfail tests\func\test_unique_id_4.py::test_unique_id_is_a_duck xfail tests\func\test_unique_id_4.py::test_unique_id_not_a_duck XPASS tests\func\test_unique_id_4.py::test_unique_id_2 PASSED ==== 1 failed, 44 passed, 2 skipped, 2 xfailed, 1 xpassed in 2.05 seconds =====</code> </pre> <br><p>   ,      . </p><br><h3 id="odinochnyy-test-filemodule">   File/Module </h3><br><p>   ,  ,          pytest: </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch2/tasks_proj $ pytest tests/func/test_add.py =========================== test session starts =========================== collected 2 items tests/func/test_add.py .. ======================== 2 passed in 0.05 seconds =========================</code> </pre> <br><p>        . </p><br><h3 id="odinochnaya-testovaya-funkciya">    </h3><br><p>     ,  <code>::</code>    : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch2/tasks_proj $ pytest -v tests/func/test_add.py::test_add_returns_valid_id =========================== test session starts =========================== collected 3 items tests/func/test_add.py::test_add_returns_valid_id PASSED ======================== 1 passed in 0.02 seconds =========================</code> </pre> <br><p>  <code>-v</code> ,  ,    . </p><br><h3 id="odinochnyy-test-class">  Test Class </h3><br><p> Here's an example: </p><br><p>   —    ,     . <br>  Here is an example: </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/</strong> <code>test_api_exceptions.py</code> </blockquote><br><pre> <code class="plaintext hljs">class TestUpdate(): """    tasks.update().""" def test_bad_id(self): """non-int id   excption.""" with pytest.raises(TypeError): tasks.update(task_id={'dict instead': 1}, task=tasks.Task()) def test_bad_task(self): """A non-Task task   excption.""" with pytest.raises(TypeError): tasks.update(task_id=1, task='not a task')</code> </pre> <br><p>      ,     <code>update()</code> ,     .     ,   ,        <code>::</code> ,      : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest -v tests/func/test_api_exceptions.py::TestUpdate ============================= test session starts ============================= collected 2 items tests\func\test_api_exceptions.py::TestUpdate::test_bad_id PASSED tests\func\test_api_exceptions.py::TestUpdate::test_bad_task PASSED ========================== 2 passed in 0.12 seconds ===========================</code> </pre> <br><h3 id="a-single-test-method-of-a-test-class"> A Single Test Method of a Test Class </h3><br><p>        ,     —     <code>::</code>   : </p><br><pre> <code class="plaintext hljs">$ cd /path/to/code/ch2/tasks_proj $ pytest -v tests/func/test_api_exceptions.py::TestUpdate::test_bad_id ===================== test session starts ====================== collected 1 item tests/func/test_api_exceptions.py::TestUpdate::test_bad_id PASSED =================== 1 passed in 0.03 seconds ===================</code> </pre> <br><blockquote> <strong> ,   </strong> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
     ,        , , ,      .   ,        <code>pytest -v</code> . </blockquote><br><h3 id="nabor-testov-na-osnove-bazovogo-imeni-testa">        </h3><br><p>  <code>-k</code>      ,         .       <code>and</code> , <code>or</code>  <code>not</code>  . ,        <code>_raises</code> : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest -v -k _raises ============================= test session starts ============================= collected 56 items tests/func/test_api_exceptions.py::test_add_raises PASSED tests/func/test_api_exceptions.py::test_list_raises PASSED tests/func/test_api_exceptions.py::test_get_raises PASSED tests/func/test_api_exceptions.py::test_delete_raises PASSED tests/func/test_api_exceptions.py::test_start_tasks_db_raises PASSED ============================= 51 tests deselected ============================= =================== 5 passed, 51 deselected in 0.54 seconds ===================</code> </pre> <br><p>    <code>and</code>  <code>not</code>    <code>test_delete_raises()</code>  : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj&gt;pytest -v -k "_raises and not delete" ============================= test session starts ============================= collected 56 items tests/func/test_api_exceptions.py::test_add_raises PASSED tests/func/test_api_exceptions.py::test_list_raises PASSED tests/func/test_api_exceptions.py::test_get_raises PASSED tests/func/test_api_exceptions.py::test_start_tasks_db_raises PASSED ============================= 52 tests deselected ============================= =================== 4 passed, 52 deselected in 0.44 seconds ===================</code> </pre> <br><p>     ,     , ,         <code>-k</code>     .     ,          ,         . </p><br><h2 id="parametrized-testing-parametrizovannoe-testirovanie"> [Parametrized Testing]:   </h2><br><p>         ,     ,       .                  .  -               pytest,  -    . </p><br><p>    ,     ,      <code>add()</code> : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">"""  API tasks.add().""" import pytest import tasks from tasks import Task def test_add_1(): """tasks.get ()  id,   add() works.""" task = Task('breathe', 'BRIAN', True) task_id = tasks.add(task) t_from_db = tasks.get(task_id) # ,  ,    assert equivalent(t_from_db, task) def equivalent(t1, t2): """   .""" #  ,   id return ((t1.summary == t2.summary) and (t1.owner == t2.owner) and (t1.done == t2.done)) @pytest.fixture(autouse=True) def initialized_tasks_db(tmpdir): """    ,  .""" tasks.start_tasks_db(str(tmpdir), 'tiny') yield tasks.stop_tasks_db()</code> </pre> <br><p>    tasks   <code>id</code>   <code>None</code> .           <code>id</code> .       <code>==</code> ,  ,        .   <code>equivalent()</code>  ,   <code>id</code> .  <code>autouse</code> ,  ,    .  ,   : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_1 ============================= test session starts ============================= collected 1 item test_add_variety.py::test_add_1 PASSED ========================== 1 passed in 0.69 seconds ===========================</code> </pre> <br><p>   .   ,      .  ,       ?  No problems.    <code>@pytest.mark.parametrize(argnames, argvalues)</code>          , : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.parametrize('task', [Task('sleep', done=True), Task('wake', 'brian'), Task('breathe', 'BRIAN', True), Task('exercise', 'BrIaN', False)]) def test_add_2(task): """    .""" task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert equivalent(t_from_db, task)</code> </pre> <br><p>   <code>parametrize()</code> —        — 'task',   .   —   ,         Task. pytest               : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_2 ============================= test session starts ============================= collected 4 items test_add_variety.py::test_add_2[task0] PASSED test_add_variety.py::test_add_2[task1] PASSED test_add_variety.py::test_add_2[task2] PASSED test_add_variety.py::test_add_2[task3] PASSED ========================== 4 passed in 0.69 seconds ===========================</code> </pre> <br><p>  <code>parametrize()</code>    .      ,  ,      : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.parametrize('summary, owner, done', [('sleep', None, False), ('wake', 'brian', False), ('breathe', 'BRIAN', True), ('eat eggs', 'BrIaN', False), ]) def test_add_3(summary, owner, done): """    .""" task = Task(summary, owner, done) task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert equivalent(t_from_db, task)</code> </pre> <br><p>   ,        pytest,       ,      : </p><br><pre> <code class="plaintext hljs">(venv35) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_3 ============================= test session starts ============================= platform win32 -- Python 3.5.2, pytest-3.5.1, py-1.5.3, pluggy-0.6.0 -- cachedir: ..\.pytest_cache rootdir: ...\bopytest-code\code\ch2\tasks_proj\tests, inifile: pytest.ini collected 4 items test_add_variety.py::test_add_3[sleep-None-False] PASSED [ 25%] test_add_variety.py::test_add_3[wake-brian-False] PASSED [ 50%] test_add_variety.py::test_add_3[breathe-BRIAN-True] PASSED [ 75%] test_add_variety.py::test_add_3[eat eggs-BrIaN-False] PASSED [100%] ========================== 4 passed in 0.37 seconds ===========================</code> </pre> <br><p>  ,      ,     pytest,    : </p><br><pre> <code class="plaintext hljs">(venv35) c:\BOOK\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_3[sleep-None-False] ============================= test session starts ============================= test_add_variety.py::test_add_3[sleep-None-False] PASSED [100%] ========================== 1 passed in 0.22 seconds ===========================</code> </pre> <br><p>   ,     : </p><br><pre> <code class="plaintext hljs">(venv35) c:\BOOK\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v "test_add_variety.py::test_add_3[eat eggs-BrIaN-False]" ============================= test session starts ============================= collected 1 item test_add_variety.py::test_add_3[eat eggs-BrIaN-False] PASSED [100%] ========================== 1 passed in 0.56 seconds ===========================</code> </pre> <br><p>      ,        : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">tasks_to_try = (Task('sleep', done=True), Task('wake', 'brian'), Task('wake', 'brian'), Task('breathe', 'BRIAN', True), Task('exercise', 'BrIaN', False)) @pytest.mark.parametrize('task', tasks_to_try) def test_add_4(task): """ .""" task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert equivalent(t_from_db, task)</code> </pre> <br><p>      .     : </p><br><pre> <code class="plaintext hljs">(venv35) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_4 ============================= test session starts ============================= collected 5 items test_add_variety.py::test_add_4[task0] PASSED [ 20%] test_add_variety.py::test_add_4[task1] PASSED [ 40%] test_add_variety.py::test_add_4[task2] PASSED [ 60%] test_add_variety.py::test_add_4[task3] PASSED [ 80%] test_add_variety.py::test_add_4[task4] PASSED [100%] ========================== 5 passed in 0.34 seconds ===========================</code> </pre> <br><p>      ,     .    ,      ids  <code>parametrize()</code> ,          .  <code>ids</code>       ,     . ,         <code>tasks_to_try</code> ,       : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">task_ids = ['Task({},{},{})'.format(t.summary, t.owner, t.done) for t in tasks_to_try] @pytest.mark.parametrize('task', tasks_to_try, ids=task_ids) def test_add_5(task): """Demonstrate ids.""" task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert equivalent(t_from_db, task)</code> </pre> <br><p>     ,   : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::test_add_5 ============================= test session starts ============================= collected 5 items test_add_variety.py::test_add_5[Task(sleep,None,True)] PASSED test_add_variety.py::test_add_5[Task(wake,brian,False)0] PASSED test_add_variety.py::test_add_5[Task(wake,brian,False)1] PASSED test_add_variety.py::test_add_5[Task(breathe,BRIAN,True)] PASSED test_add_variety.py::test_add_5[Task(exercise,BrIaN,False)] PASSED ========================== 5 passed in 0.45 seconds ===========================</code> </pre> <br><p>        : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v "test_add_variety.py::test_add_5[Task(exercise,BrIaN,False)]" ============================= test session starts ============================= collected 1 item test_add_variety.py::test_add_5[Task(exercise,BrIaN,False)] PASSED ========================== 1 passed in 0.21 seconds ===========================</code> </pre> <br><p>       ;          shell.     <code>parametrize()</code>  .               : </p><br><blockquote> <strong>ch2/tasks_proj/tests/func/ <code>test_add_variety.py</code></strong> </blockquote><br><pre> <code class="plaintext hljs">@pytest.mark.parametrize('task', tasks_to_try, ids=task_ids) class TestAdd(): """   .""" def test_equivalent(self, task): """ ,   .""" task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert equivalent(t_from_db, task) def test_valid_id(self, task): """          .""" task_id = tasks.add(task) t_from_db = tasks.get(task_id) assert t_from_db.id == task_id</code> </pre> <br><p>    : </p><br><pre> <code class="plaintext hljs">(venv33) ...\bopytest-code\code\ch2\tasks_proj\tests\func&gt;pytest -v test_add_variety.py::TestAdd ============================= test session starts ============================= collected 10 items test_add_variety.py::TestAdd::test_equivalent[Task(sleep,None,True)] PASSED test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)0] PASSED test_add_variety.py::TestAdd::test_equivalent[Task(wake,brian,False)1] PASSED test_add_variety.py::TestAdd::test_equivalent[Task(breathe,BRIAN,True)] PASSED test_add_variety.py::TestAdd::test_equivalent[Task(exercise,BrIaN,False)] PASSED test_add_variety.py::TestAdd::test_valid_id[Task(sleep,None,True)] PASSED test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)0] PASSED test_add_variety.py::TestAdd::test_valid_id[Task(wake,brian,False)1] PASSED test_add_variety.py::TestAdd::test_valid_id[Task(breathe,BRIAN,True)] PASSED test_add_variety.py::TestAdd::test_valid_id[Task(exercise,BrIaN,False)] PASSED ========================== 10 passed in 1.16 seconds ==========================</code> </pre> <br><p>     ,            <code>@pytest.mark.parametrize()</code> .       <code>pytest.param(&lt;value\&gt;, id="something")</code> : </p><br><p>  : </p><br><pre> <code class="plaintext hljs">(venv35) ...\bopytest-code\code\ch2\tasks_proj\tests\func $ pytest -v test_add_variety.py::test_add_6 ======================================== test session starts ========================================= collected 3 items test_add_variety.py::test_add_6[just summary] PASSED [ 33%] test_add_variety.py::test_add_6[summary\owner] PASSED [ 66%] test_add_variety.py::test_add_6[summary\owner\done] PASSED [100%] ================================ 3 passed, 6 warnings in 0.35 seconds ================================</code> </pre> <br><p>  ,  <code>id</code>       . </p><br><h2 id="uprazhneniya">  Exercises </h2><br><ol><li>     , <code>task_proj</code> ,  <a href="https://pragprog.com/titles/bopytest/source_code">-  </a>  ,         <code>pip install /path/to/tasks_proj</code> . </li><li>   . </li><li>  <em>pytest</em>   . </li><li>  <em>pytest</em>   ,  <code>tasks_proj/tests/func</code> .  pytest     ,     .     .  ,    ? </li><li>  xfail      ,     pytest   tests    . </li><li>      <code>tasks.count()</code> ,   .    API  ,     ,  ,    . </li><li>          ?       <code>test_api_exceptions.py</code> . ,      . (   <code>api.py</code>   .) </li></ol><br><h2 id="chto-dalshe">  What's next </h2><br><p>         <em>pytest</em> . ,   ,   ,       .         <code>initialized_tasks_db</code> .     /        . </p><br><p>      ,           .           pytest. </p><br><p><img src="https://habrastorage.org/webt/jl/jn/bb/jljnbbjr-ejh473xy_eccsmknpk.png">  <a href="https://habr.com/ru/post/448782/">Back</a> <a href="https://habr.com/ru/post/448786/">Next</a> <img src="https://habrastorage.org/webt/rw/dy/-g/rwdy-grsvbpcetjttrmecdkxtlk.png"></p></div><p>Source: <a href="https://habr.com/ru/post/448788/">https://habr.com/ru/post/448788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../448768/index.html">Multi-level lighting control: fault tolerance solutions and products</a></li>
<li><a href="../448774/index.html">SQL to CSV using DBMS_SQL</a></li>
<li><a href="../448780/index.html">What gives software for recruiting money</a></li>
<li><a href="../448782/index.html">Python Testing with pytest. Getting started with pytest, Chapter 1</a></li>
<li><a href="../448786/index.html">Python Testing with pytest. CHAPTER 3 pytest Fixtures</a></li>
<li><a href="../448790/index.html">SpaceVIL - cross-platform GUI framework for development on .Net Core, .Net Standard and JVM</a></li>
<li><a href="../448792/index.html">Python Testing with pytest. Builtin Fixtures, Chapter 4</a></li>
<li><a href="../448794/index.html">Python Testing with pytest. Plugins, CHAPTER 5</a></li>
<li><a href="../448796/index.html">Python Testing with pytest. Configuration, CHAPTER 6</a></li>
<li><a href="../448798/index.html">Python Testing with pytest. Using pytest with other tools, CHAPTER 7</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>