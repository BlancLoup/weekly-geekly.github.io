<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Big Data Training: Spark MLlib</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! 



 Last time we met with the Apache Spark tool, which has recently become almost the most popular tool for processing big data, and in par...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Big Data Training: Spark MLlib</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr! <br><br><img src="https://habrastorage.org/getpro/habr/post_images/42f/e81/f34/42fe81f3404ede93b648612bdc45c465.png" alt="image"><br><br>  <a href="http://habrahabr.ru/post/250811/">Last time</a> we met with the <b>Apache Spark</b> tool, which has recently become almost the most popular tool for processing big data, and in particular, <b>Large Scale Machine Learning</b> .  Today we will take a closer look at the <b>MlLib</b> library, namely, we will show how to solve machine learning problems ‚Äî classification, regression, clustering, and also collaborative filtering.  In addition, we will show how it is possible to investigate the signs in order to select and isolate new ones (the so-called <b>Feature Engineering</b> , which we <a href="http://habrahabr.ru/post/248129/">talked about earlier</a> , and <a href="http://habrahabr.ru/post/249759/">not once</a> ). <br><a name="habracut"></a><br><h3>  Plan </h3><br>  First of all, we will look at how to store the objects of our training sample, how to calculate the basic statistics of signs, then the machine learning algorithms (classification, regression, clustering) and finally, consider an example of building a recommender system - the so-called.  collaborative filtering methods, or more precisely, one of the most common ALS algorithms. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h3>  Vectors </h3><br>  For simple <b>"dense"</b> vectors there is a special class <b>Vectors.dense</b> : <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.linalg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Vectors my_vec = Vectors.dence ([<span class="hljs-number"><span class="hljs-number">1.12</span></span>, <span class="hljs-number"><span class="hljs-number">4.10</span></span>, <span class="hljs-number"><span class="hljs-number">1.5</span></span>, <span class="hljs-number"><span class="hljs-number">-2.7</span></span>, <span class="hljs-number"><span class="hljs-number">3.5</span></span>, <span class="hljs-number"><span class="hljs-number">10.7</span></span>, <span class="hljs-number"><span class="hljs-number">0.7</span></span>])</code> </pre> <br>  For <b>‚Äúsparse‚Äù</b> vectors, the <b>Vectors.sparse</b> class is <b>used</b> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.linalg <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Vectors my_vec = Vectors.sparse(<span class="hljs-number"><span class="hljs-number">10</span></span>, [<span class="hljs-number"><span class="hljs-number">0</span></span>,<span class="hljs-number"><span class="hljs-number">2</span></span>,<span class="hljs-number"><span class="hljs-number">4</span></span>,<span class="hljs-number"><span class="hljs-number">9</span></span>], [<span class="hljs-number"><span class="hljs-number">-1.2</span></span>, <span class="hljs-number"><span class="hljs-number">3.05</span></span>, <span class="hljs-number"><span class="hljs-number">-4.08</span></span>, <span class="hljs-number"><span class="hljs-number">0.46</span></span>])</code> </pre><br>  Here, the first argument is the number of features (the length of the vector), followed by a list ‚Äî the numbers of nonzero features, and after ‚Äî the values ‚Äã‚Äãof the features themselves. <br><br><h3>  Marked Vectors </h3><br>  For marked points in Spark there is a special <b>LabeledPoint</b> class: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.regression <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> LabeledPoint my_point = LabeledPoint(<span class="hljs-number"><span class="hljs-number">1.0</span></span>, my_vec)</code> </pre><br>  Where in the <b>LabeledPoint</b> class we have <b>LabeledPoint.features</b> - any of the vectors described above, and <b>LabeledPoint.label</b> is, respectively, a label that can take any real value in the case of a regression task and the value <b>[0.0,1.0,2.0, ...]</b> - for classification tasks <br><br><h3>  Work with signs </h3><br>  It is no secret that often, in order to build a good machine learning algorithm, it is enough just to look at the signs, select the most relevant ones or come up with new ones.  For this purpose, in the Spark class <b>Statistics</b> , with which you can do all these things, for example: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.stat <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> Statistics summary = Statistics.colStats(features) <span class="hljs-comment"><span class="hljs-comment"># meas of features summary.mean # non zeros features summary.numNonzeros # variance summary.variance # correlations of features Statistics.corr(features)</span></span></code> </pre><br>  In addition, Spark has a huge number of additional features like sampling, generation of standard features (like TF-IDF for texts), as well as such an important thing as scaling of features (the reader is offered to read this documentation in the article after reading this article).  For the latter, there is a special class <b>Scaler</b> : <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.feature <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> StandardScaler scaler = StandardScaler(withMean=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>, withStd=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>).fit(features) scaler.transform (features.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x.toArray()))</code> </pre><br>  The only thing that is important to remember is that in the case of sparse vectors this does not work and the scaling strategy must be thought out for a specific task.  We now turn directly to the problems of machine learning. <br><br><h3>  Classification and Regression </h3><br><h4>  Linear methods </h4><br>  <a href="http://habrahabr.ru/post/248779/">As always, the</a> most common methods are linear classifiers.  Learning a linear classifier is reduced to the problem of convex minimization of the functional of the weights vector.  The difference lies in the choice of the loss function, the regularization function, the number of iterations, and many other parameters.  For example, consider below the logistic loss function (and, respectively, the so-called logistic regression method), 500 iterations and L2 - regularization. <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyspark.mllib.classification <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> cls model = cls.LogisticRegressionWithSGD.train(train, iterations=<span class="hljs-number"><span class="hljs-number">500</span></span>, regType=<span class="hljs-string"><span class="hljs-string">"l2"</span></span>)</code> </pre><br>  Similarly, linear regression is done: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> pyspark.mllib.regression <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> regr model = regr.RidgeRegressionWithSGD.train(train)</code> </pre><br><h4>  Naive bayes </h4><br>  In this case, the learning algorithm takes as input only 2 parameters ‚Äî the learning sample itself and the smoothing parameter: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.classification <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> NaiveBayes model = NaiveBayes.train(train, <span class="hljs-number"><span class="hljs-number">8.5</span></span>) model.predict(test.features)</code> </pre><br><h4>  Decisive trees </h4><br>  In Spark, as in many other packages, regression and classification trees are implemented.  The learning algorithm takes as input multiple parameters, such as multiple classes, maximum tree depth.  Also, the algorithm must specify which categories have categorical features, as well as many other parameters.  However, one of the most important of them in training trees is the so-called <b>impurity</b> ‚Äî the criterion for calculating the so-called <b>information gain</b> , which can usually take the following values: <b>entropy</b> and <b>gini</b> ‚Äî for classification problems, <b>variance</b> ‚Äî for regression problems.  For example, consider a binary classification with the parameters defined below: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> DecisionTree model = DecisionTree.trainClassifier(train, numClasses=<span class="hljs-number"><span class="hljs-number">2</span></span>, impurity=<span class="hljs-string"><span class="hljs-string">'gini'</span></span>, maxDepth=<span class="hljs-number"><span class="hljs-number">5</span></span>) model.predict(test.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: x.features))</code> </pre><br><h4>  Random forest </h4><br>  Random forests, as is known, are among the universal algorithms and one would expect that they will be implemented in this tool.  They use the trees described above.  There are also exactly the <b>trainClassifier</b> and <b>trainRegression</b> methods for teaching the classifier and the regression function, respectively.  One of the most important parameters is the number of trees in the forest, the <b>impurity</b> already known to us, as well as the <b>featureSubsetStrategy</b> , the number of signs that are considered when splitting a tree on the next node (for more information about the values, see the documentation).  Accordingly, the following is an example of a binary classification using 50 trees: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.tree <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> RandomForest model = RandomForest.trainClassifier(train, numClasses=<span class="hljs-number"><span class="hljs-number">2</span></span>, numTrees=<span class="hljs-number"><span class="hljs-number">50</span></span>, featureSubsetStrategy=<span class="hljs-string"><span class="hljs-string">"auto"</span></span>, impurity=<span class="hljs-string"><span class="hljs-string">'gini'</span></span>, maxDepth=<span class="hljs-number"><span class="hljs-number">20</span></span>, seed=<span class="hljs-number"><span class="hljs-number">12</span></span>) model.predict(test.map(<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x:x.features))</code> </pre><br><h3>  Clustering </h3><br>  As elsewhere, in the park, the well-known <b>KMeans</b> algorithm is <b>implemented</b> , whose training takes input directly, the number of clusters, the number of iterations, and the strategy for selecting initial cluster centers (the <b>initializationMode</b> parameter, which by default has a value of <b>k-means</b> , and also can value <b>random</b> ): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.clustering <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> KMeans clusters = KMeans.train(features, <span class="hljs-number"><span class="hljs-number">3</span></span>, maxIterations=<span class="hljs-number"><span class="hljs-number">100</span></span>, runs=<span class="hljs-number"><span class="hljs-number">5</span></span>, initializationMode=<span class="hljs-string"><span class="hljs-string">"random"</span></span>) clusters.predict(x.features))</code> </pre><br><h3>  Collaborative filtering </h3><br>  Considering that the most famous example of using Big Data is a recommender system, it would be strange if the simplest algorithms were not implemented in many packages.  This also applies to Spark.  It implements the <b>ALS (Alternative Least Square)</b> algorithm - perhaps one of the most well-known collaborative filtering algorithms.  The description of the algorithm itself deserves a separate article.  Here we just say in a nutshell that the algorithm actually deals with decomposition of the response matrix (the rows of which are users, and the columns are products) into <b>product</b> matrices <b>‚Äî a topic</b> and a <b>topic-user</b> , where the topics are some hidden variables, the meaning of which is often not clear (the beauty of the <b>ALS</b> algorithm is just to find the topics themselves and their values).  The essence of these topics is that each user and each movie are now characterized by a set of features, and the scalar product of these vectors is the assessment of the movie of a particular user.  The training set for this algorithm is specified in the form of a table <b>userID -&gt; productID -&gt; rating</b> .  After that, the model is trained using ALS (which, like other algorithms, accepts many parameters for input, which the reader is offered to read about on their own): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">from</span></span> pyspark.mllib.recommendation <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ALS model = ALS.train (ratings, <span class="hljs-number"><span class="hljs-number">20</span></span>, <span class="hljs-number"><span class="hljs-number">60</span></span>) predictions = model.predictAll(ratings.map (<span class="hljs-keyword"><span class="hljs-keyword">lambda</span></span> x: (x[<span class="hljs-number"><span class="hljs-number">0</span></span>],x[<span class="hljs-number"><span class="hljs-number">1</span></span>])))</code> </pre><br><h3>  Conclusion </h3><br>  So, we briefly reviewed the <b>MlLib</b> library from the Apache Spark framework, which was developed for distributed processing of big data.  Recall that the main advantage of this tool, <a href="http://habrahabr.ru/post/250811/">as discussed earlier</a> , is that the data can be cached in RAM, which allows you to significantly speed up the calculations in the case of iterative algorithms, which are most of the machine learning algorithms. </div><p>Source: <a href="https://habr.com/ru/post/251471/">https://habr.com/ru/post/251471/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../251461/index.html">A year and a half of working with SAP hybris: normal flight. The most important thing you need to know about developing on eCommerce platforms</a></li>
<li><a href="../251463/index.html">The geolocation saga and how to make a geo-web service on NGINX without a database engine and without programming</a></li>
<li><a href="../251465/index.html">Interfaces in the real world (more examples)</a></li>
<li><a href="../251467/index.html">React v0.13 RC</a></li>
<li><a href="../251469/index.html">How Internet giants have turned the network hardware business upside down</a></li>
<li><a href="../251473/index.html">The site from scratch on a full stack of BEM technologies. Yandex Methodology</a></li>
<li><a href="../251477/index.html">Honest private properties in the prototype</a></li>
<li><a href="../251479/index.html">We write a bot for MMORPG with assembler and draenei. Part 4</a></li>
<li><a href="../251487/index.html">Draining data 180 thousand users FL.ru</a></li>
<li><a href="../251489/index.html">A router from the operator? - No thanks!</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>