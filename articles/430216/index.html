<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>As I understood that I eat a lot of sweets, or the classification of goods by checks in the application</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Task 
 In this article, we want to tell how we created a solution for classifying product names from checks in an application for accounting for expen...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>As I understood that I eat a lot of sweets, or the classification of goods by checks in the application</h1><div class="post__text post__text-html js-mediator-article"><h2>  Task </h2><br>  In this article, we want to tell how we created a solution for classifying product names from checks in an application for accounting for expenses by check and purchase assistant.  We wanted to give users the ability to view purchase statistics collected automatically based on scanned checks, namely, to distribute all goods purchased by the user into categories.  Because forcing the user to independently group products is already the last century.  There are several approaches to solve this problem: you can try to apply clustering algorithms with different methods of vector representation of words or classical classification algorithms.  We didn‚Äôt invent anything new and in this article we just want to share a small guide about a possible solution of the problem, examples of how not to do it, an analysis of why other methods did not work and what problems could be encountered in the process. <br><a name="habracut"></a><br><h2>  Clustering </h2><br>  One of the problems was that the names of the goods we receive from checks are not always easy to decipher, even to a person.  It is unlikely that you will find out what kind of product with the name <a href="https://www.ikea.com/ru/ru/catalog/products/10382555/">‚ÄúUTRUSTA krnsht‚Äù</a> was bought in one of the Russian stores?  True connoisseurs of Swedish design will of course answer us right away: Bracket for the Utrusta oven, but keeping such specialists in the headquarters is quite expensive.  In addition, we did not have a ready-made, marked sample suitable for our data, for which we could train the model.  Therefore, we first describe how, in the absence of data, we applied clustering algorithms for learning and why we did not like it. <br><br>  Such algorithms are based on measurements of distances between objects, which requires their vector representation or the use of a metric to measure word similarity (for example, Levenshtein distance).  At this step, the complexity consists in a meaningful vector representation of the names.  It is problematic to extract properties from the names that will fully and comprehensively describe the product and its relationship with other products. <br><br>  The simplest option is to use Tf-Idf, but in this case, the dimension of the vector space is quite large, and the space itself is discharged.  In addition, this approach does not extract any additional information from the titles.  Thus, in one cluster there can be many products from different categories, united by a common word, such as, for example, ‚Äúpotato‚Äù or ‚Äúsalad‚Äù: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <div style="text-align:center;"><img src="https://habrastorage.org/webt/pb/uh/bd/pbuhbdnf0bwmwvy0ywqwapil018.png"></div><br>  We also cannot control which clusters will be assembled.  The only thing that can be designated is the number of clusters (if algorithms are used that are not based on density peaks in space).  But if you specify too small an amount, then there will be one huge cluster that will contain all the names that could not fit in other clusters.  If you specify a large enough, after the operation of the algorithm, we will have to look through hundreds of clusters and combine them into semantic categories with our hands. <br><br>  The tables below provide information on clusters using the KMeans and Tf-Idf algorithm for vector representation.  From these tables we see that the distances between the centers of the clusters are smaller than the average distance between the objects and the centers of the clusters to which they belong.  Such data can be explained by the fact that there are no apparent density peaks in the space of vectors and the centers of the clusters are located around the circumference, where most of the objects are located outside the boundary of this circle.  In addition, one cluster is formed, which contains most of the vectors.  Most likely in this cluster are collected names that contain words that occur more frequently than others among all products from different categories. <br><br><table><caption>  Table 1. Distances between clusters. </caption><tbody><tr><th>  Cluster </th><th>  C1 </th><th>  C2 </th><th>  C3 </th><th>  C4 </th><th>  C5 </th><th>  C6 </th><th>  C7 </th><th>  C8 </th><th>  C9 </th></tr><tr><th>  C1 </th><td>  0.0 </td><td>  0.502 </td><td>  0.354 </td><td>  0.475 </td><td>  0.481 </td><td>  0.527 </td><td>  0.498 </td><td>  0.501 </td><td>  0.524 </td></tr><tr><th>  C2 </th><td>  0.502 </td><td>  0.0 </td><td>  0.614 </td><td>  0.685 </td><td>  0.696 </td><td>  0.728 </td><td>  0.706 </td><td>  0.709 </td><td>  0.725 </td></tr><tr><th>  C3 </th><td>  0.354 </td><td>  0.614 </td><td>  0.0 </td><td>  0.590 </td><td>  0.597 </td><td>  0.635 </td><td>  0.610 </td><td>  0.613 </td><td>  0.632 </td></tr><tr><th>  C4 </th><td>  0.475 </td><td>  0.685 </td><td>  0.590 </td><td>  0.0 </td><td>  0.673 </td><td>  0.709 </td><td>  0.683 </td><td>  0.687 </td><td>  0.699 </td></tr><tr><th>  C5 </th><td>  0.481 </td><td>  0.696 </td><td>  0.597 </td><td>  0.673 </td><td>  0.0 </td><td>  0.715 </td><td>  0.692 </td><td>  0.694 </td><td>  0.711 </td></tr><tr><th>  C6 </th><td>  0.527 </td><td>  0.727 </td><td>  0.635 </td><td>  0.709 </td><td>  0.715 </td><td>  0.0 </td><td>  0.726 </td><td>  0.728 </td><td>  0.741 </td></tr><tr><th>  C7 </th><td>  0.498 </td><td>  0.706 </td><td>  0.610 </td><td>  0.683 </td><td>  0.692 </td><td>  0.725 </td><td>  0.0 </td><td>  0.707 </td><td>  0.714 </td></tr><tr><th>  C8 </th><td>  0.501 </td><td>  0.709 </td><td>  0.612 </td><td>  0.687 </td><td>  0.694 </td><td>  0.728 </td><td>  0.707 </td><td>  0.0 </td><td>  0.725 </td></tr><tr><th>  C9 </th><td>  0.524 </td><td>  0.725 </td><td>  0.632 </td><td>  0.699 </td><td>  0.711 </td><td>  0.741 </td><td>  0.714 </td><td>  0.725 </td><td>  0.0 </td></tr></tbody></table><br><table><caption>  Table 2. Clusters in brief </caption><tbody><tr><th>  Cluster </th><th>  Number of objects </th><th>  Average distance </th><th>  Minimum distance </th><th>  Max distance </th></tr><tr><th>  C1 </th><td>  62530 </td><td>  0.999 </td><td>  0.041 </td><td>  1.001 </td></tr><tr><th>  C2 </th><td>  2159 </td><td>  0.864 </td><td>  0.527 </td><td>  0.964 </td></tr><tr><th>  C3 </th><td>  1099 </td><td>  0.934 </td><td>  0.756 </td><td>  0.993 </td></tr><tr><th>  C4 </th><td>  1292 </td><td>  0.879 </td><td>  0.733 </td><td>  0.980 </td></tr><tr><th>  C5 </th><td>  746 </td><td>  0.875 </td><td>  0.731 </td><td>  0.965 </td></tr><tr><th>  C6 </th><td>  2451 </td><td>  0.847 </td><td>  0.719 </td><td>  0.994 </td></tr><tr><th>  C7 </th><td>  1133 </td><td>  0.866 </td><td>  0.724 </td><td>  0.986 </td></tr><tr><th>  C8 </th><td>  876 </td><td>  0.863 </td><td>  0.704 </td><td>  0.999 </td></tr><tr><th>  C9 </th><td>  1879 </td><td>  0.849 </td><td>  0.526 </td><td>  0.981 </td></tr></tbody></table><br><br>  But in some places the clusters are quite decent, as, for example, in the image below - there almost all products belong to cat food. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/oh/uy/5p/ohuy5p_bgiignp9ah_gokvvbzs4.png"></div><br><br>  Doc2Vec is another one of the algorithms that allow you to represent texts in vector form.  When using this approach, each name will be described by a vector of smaller dimension than when using Tf-Idf.  In the resulting vector space, similar texts will be close to each other, and various texts will be far away. <br><br>  This approach can solve the problem of large dimensionality and space dimensioning, which is obtained by the Tf-Idf method.  For this algorithm, we used the simplest version of tokenization: we broke the name into separate words and took their initial forms.  He was trained on the data in this way: <br><br><pre><code class="python hljs">max_epochs = <span class="hljs-number"><span class="hljs-number">100</span></span> vec_size = <span class="hljs-number"><span class="hljs-number">20</span></span> alpha = <span class="hljs-number"><span class="hljs-number">0.025</span></span> model = doc2vec.Doc2Vec(vector_size=vec_size, alpha=alpha, min_alpha=<span class="hljs-number"><span class="hljs-number">0.00025</span></span>, min_count=<span class="hljs-number"><span class="hljs-number">1</span></span>, dm =<span class="hljs-number"><span class="hljs-number">1</span></span>) model.build_vocab(train_corpus) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> epoch <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> range(max_epochs): print(<span class="hljs-string"><span class="hljs-string">'iteration {0}'</span></span>.format(epoch)) model.train(train_corpus, total_examples=model.corpus_count, epochs=model.iter) <span class="hljs-comment"><span class="hljs-comment"># decrease the learning rate model.alpha -= 0.0002 # fix the learning rate, no decay model.min_alpha = model.epochs</span></span></code> </pre> <br>  But with this approach, we obtained vectors that do not carry information about the name - random values ‚Äã‚Äãcan be used with the same success.  Here is one example of the operation of the algorithm: the image shows goods similar to the algorithm‚Äôs opinion to ‚ÄúBorodino bread forms n 0 0 K‚Äù. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qd/m9/lp/qdm9lplwaidbilivtpb3jjskopo.png"></div><br>  Perhaps the problem is in the length and context of the names: the pass in the name "__ club. Banana 200ml" can be like yogurt, juice or a large jar of cream.  You can achieve a better result by using a different approach to tokenization of names.  We had no experience using this method and by the time the first attempts failed, we had already found a couple of marked sets with product names, so we decided to leave this method for a while and go to the classification algorithms. <br><br><h2>  Classification </h2><br><h3>  Data preprocessing </h3><br>  The names of goods from checks come to us in a not always clear form: Latin and Cyrillic words are mixed in words.  For example, the letter ‚Äúa‚Äù can be replaced by ‚Äúa‚Äù Latin, and this increases the number of unique names - for example, the words ‚Äúmilk‚Äù and ‚Äúmilk‚Äù will be considered different.  The names also contain many other misprints and abbreviations. <br><br>  We studied our base and found common mistakes in the names.  At this stage, we were treated with regular expressions, with the help of which we cleaned up the names and led them to a certain general form.  When using this approach, the result is increased by approximately 7%.  In conjunction with a simple version of the SGD Classifier based on Huber's loss function with twisted parameters, we obtained an accuracy of 81% for F1 (average accuracy for all product categories). <br><br><pre> <code class="python hljs">sgd_model = SGDClassifier() parameters_sgd = { <span class="hljs-string"><span class="hljs-string">'max_iter'</span></span>:[<span class="hljs-number"><span class="hljs-number">100</span></span>], <span class="hljs-string"><span class="hljs-string">'loss'</span></span>:[<span class="hljs-string"><span class="hljs-string">'modified_huber'</span></span>], <span class="hljs-string"><span class="hljs-string">'class_weight'</span></span>:[<span class="hljs-string"><span class="hljs-string">'balanced'</span></span>], <span class="hljs-string"><span class="hljs-string">'penalty'</span></span>:[<span class="hljs-string"><span class="hljs-string">'l2'</span></span>], <span class="hljs-string"><span class="hljs-string">'alpha'</span></span>:[<span class="hljs-number"><span class="hljs-number">0.0001</span></span>] } sgd_cv = GridSearchCV(sgd_model, parameters_sgd,n_jobs=<span class="hljs-number"><span class="hljs-number">-1</span></span>) sgd_cv.fit(tf_idf_data, prod_cat) sgd_cv.best_score_, sgd_cv.best_params_</code> </pre> <br>  Also, do not forget that some categories of people buy more often than others: for example, ‚ÄúTea and sweets‚Äù and ‚ÄúVegetables and fruits‚Äù are much more popular than ‚ÄúServices‚Äù and ‚ÄúCosmetics‚Äù.  With such a distribution of data, it is better to use algorithms that allow you to specify weights (degree of importance) for each class.  Class weight can be determined inversely with the value equal to the ratio of the number of products in the class to the total number of products.  But you should not think about it, because in the implementation of these algorithms, it is possible to automatically determine the weight of categories. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/ty/o9/pk/tyo9pkfhw-ibby5jopobc5okotc.png"></div><br><h2>  Getting new data for training </h2><br>  Our application required slightly different categories than those used in the competition, and the names of the products from our database were significantly different from those presented in the contest.  Therefore, we needed to mark the goods from our checks.  We tried to do it on our own, but realized that even if we connected our whole team, it would take a lot of time.  Therefore, we decided to use <a href="https://toloka.yandex.ru/">‚ÄúToloka‚Äù of</a> Yandex. <br><br>  There we used the following assignment form: <br><br><ul><li>  in each cell we had a product, the category of which you need to define </li><li>  its presumable category defined by one of our previous models </li><li>  answer field (if the proposed category was incorrect) </li></ul><br>  We created a detailed instruction with examples that explained the features of each category, and also used quality control methods: a set with reference answers that were shown along with the usual tasks (we implemented the reference answers ourselves, marking out several hundred products).  According to the results of the answers to these tasks, users who were incorrectly marking the data were eliminated.  However, for the whole project we banned only three users from 600+. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/fx/ib/xt/fxibxt-v5h7vouodjoffwtl6egy.png"></div><br>  With the new data, we received a model that better suited our data, and the accuracy increased slightly (by ~ 11%) and received 92% already. <br><br><h2>  Final model </h2><br>  We began the classification process with a combination of data from several datasets with Kaggle - 74%, after which we improved the preprocessing - 81%, collected a new data set - 92% and finally improved the classification process: initially, using logistic regression, we obtain preliminary probabilities of goods belonging to categories, based on the names of the goods, SGD gave greater accuracy in percent, but still had great values ‚Äã‚Äãon the loss functions, which had a bad effect on the results of the final classifier.  Further, we combine the data with other data on the product (the price of the product, the store in which it was purchased, statistics on the store, check and other meta-information), and XGBoost learns all this data volume, which gave an accuracy of 98% (increase another 6%).  As it turned out, the greatest contribution was made by the quality of the training sample. <br><br><h2>  Run on server </h2><br>  In order to speed up the deployment, we raised a simple server on Flask to Docker.  There was one method that took from the server the goods that need to be categorized and returned the goods with categories.  Thus, we easily integrated into the existing system, the center of which was Tomcat, and we did not have to make changes to the architecture - we just added one more block to it. <br><br><h2>  Release </h2><br>  A few weeks ago, we posted a release categorized on Google Play (it will appear on the App Store after some time).  It turned out like this: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/h1/tj/ie/h1tjiekixykrb_liuwftfnd8ufy.png" width="50%"></div><br>  In the next releases we plan to add the possibility of correcting categories, which will allow us to quickly collect categorization errors and retrain the categorization model (as long as we do it ourselves). <br><br>  Mentioned Kaggle competitions: <br><br>  <a href="https://www.kaggle.com/c/receipt-categorisation">www.kaggle.com/c/receipt-categorisation</a> <br>  <a href="https://www.kaggle.com/c/market-basket-analysis">www.kaggle.com/c/market-basket-analysis</a> <br>  <a href="https://www.kaggle.com/c/prod-price-prediction">www.kaggle.com/c/prod-price-prediction</a> </div><p>Source: <a href="https://habr.com/ru/post/430216/">https://habr.com/ru/post/430216/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../430204/index.html">MIT course "Computer Systems Security". Lecture 18: "Private Internet Browsing", part 1</a></li>
<li><a href="../430206/index.html">MIT course "Computer Systems Security". Lecture 18: "Private Internet Browsing", part 2</a></li>
<li><a href="../430208/index.html">MIT course "Computer Systems Security". Lecture 18: "Private Internet Browsing", part 3</a></li>
<li><a href="../430210/index.html">Puzzle "Test My Patience" by Check Point Security Academy</a></li>
<li><a href="../430212/index.html">OpenSceneGraph: Scene Geometry Basics</a></li>
<li><a href="../430218/index.html">Energy Optimization STM32: A Practical Guide</a></li>
<li><a href="../430220/index.html">How to turn a "hundred" usb hub into a "smart" managed and save at the same time $ 300</a></li>
<li><a href="../430222/index.html">Senior Engineer in search of work. How I passed 20 interviews with HR and what I think about it</a></li>
<li><a href="../430224/index.html">Schizotypal Disorder: An Inside Look</a></li>
<li><a href="../430226/index.html">From var b to interview</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>