<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Differentiable programming</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="With four parameters, I can ask an elephant, and with five I can make him wiggle his trunk. 
 - John Von Neumann 

 The idea of ​​" differentiated pro...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">🔎</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">📜</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">⬆️</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">⬇️</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Differentiable programming</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/fv/8k/s8/fv8ks8cvj64dyqlstu1vi9znbv8.jpeg"></p><br><blockquote>  With four parameters, I can ask an elephant, and with five I can make him wiggle his trunk. <br>  - John Von Neumann </blockquote><p>  The idea of ​​" <a href="https://en.wikipedia.org/wiki/Differentiable_programming">differentiated programming</a> " is very popular in the world of machine learning.  For many, it is not clear whether this term represents a real shift in how researchers understand machine learning, or is it just a (another) rebranding of “deep learning”.  This post explains what new is differentiable programming (or ∂P) in the machine learning table. </p><br><p>  Most importantly, differentiated programming is a shift opposite to the direction of deep learning;  from increasingly heavily parameterized models to simpler ones, which make more use of the structure of the problem. </p><br><p>  Next, we leaf through the canvas of uninteresting text, we want to find out what autodifferentiation is and even populate from the catapult! </p><a name="habracut"></a><br><h2 id="brute-force-with-benefits">  Brute Force with Benefits </h2><br><p>  Differentiability is the basic idea that makes deep learning so successful.  Where a brute force search of even a few hundred model parameters would be too expensive, gradients allow a pseudo-random walk around interesting parts of the parameter space and find a good set.  By performing such a seemingly naive algorithm, we get a good generality, but it’s far from obvious that we need to differentiate, say, working with sequences in language translation, but everything turns out to be simple, show us a little ingenuity. </p><br><p>  What about biological neurons and <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mo>&amp;#x3C3;</mo></mrow><mo stretchy=&quot;false&quot;>(</mo><mi>W</mi><mtext>&amp;#xA0;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="21.431ex" height="2.66ex" viewBox="0 -832 9227 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-3D" x="775" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-3C3" x="1831" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-28" x="2403" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-57" x="2792" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-74" x="4091" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-69" x="4452" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-6D" x="4798" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-65" x="5676" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-73" x="6143" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-78" x="6612" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-2B" x="7407" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-62" x="8408" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-29" x="8837" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mo>σ</mo></mrow><mo stretchy="false">(</mo><mi>W</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>i</mi><mi>m</mi><mi>e</mi><mi>s</mi><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-1"> y = σ (W \ times x + b) </script>  ?  There is nothing special about this formula;  This is a simple and flexible example of a high-parametric non-linear function.  In fact, this is probably the worst such feature in most cases.  A single layer of the neural network can, in principle, classify images of cats, but only using a relatively uninteresting trick acting as a reference table.  <strong>Works smoothly!</strong>  - but the fine print warns that you may need more parameters than atoms in the universe.  To actually make this thing work, you need to code the problem structure in the model — this is where it starts to look more like traditional programming. </p><br><p>  For example, <em><a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B2%25D1%2591%25D1%2580%25D1%2582%25D0%25BE%25D1%2587%25D0%25BD%25D0%25B0%25D1%258F_%25D0%25BD%25D0%25B5%25D0%25B9%25D1%2580%25D0%25BE%25D0%25BD%25D0%25BD%25D0%25B0%25D1%258F_%25D1%2581%25D0%25B5%25D1%2582%25D1%258C">ConvNets</a></em> have a huge advantage over the perceptron, because they work with <a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)">image cores</a> that are known to use translational invariance.  The face - it is the face, regardless of whether it is displayed in the upper left corner of the image or in the center, but where the perceptron should have studied this case in each particular case, the core can immediately respond to any part of the image.  It is difficult to analyze convolutional networks in statistical terms, but it is much easier to treat them as an automatically tuned version of what image processing experts wrote by hand.  The core image is the first and simplest differentiated program. </p><br><h2 id="encoding-structure-redux">  Encoding Structure, Redux </h2><br><p>  ML toolboxes increasingly support algorithmic differentiation (AD), which allows us to differentiate models using cycles, branching, and recursion — or any program built on a set of differentiated mathematical primitives.  This led to a more complex architecture: NLP models are increasingly similar to classical grammar parsers with <a href="https://arxiv.org/abs/1603.06021">stack-augmented</a> models, and you can even differentiate an analogue <a href="https://arxiv.org/pdf/1410.5401.pdf">of the Turing machine</a> or <a href="https://arxiv.org/abs/1605.06640">programming language interpreter</a> . </p><br><p>  The last step taken by differentiated programming is no longer to consider the multiplication of matrices, convolutions, and <em>RNN</em> as fundamental building blocks of deep learning, but only as special cases.  We can apply deep learning methods to any parameterized differentiable function. <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-2-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.418ex" height="2.66ex" viewBox="0 -832 1902 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-28" x="550" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-78" x="940" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-29" x="1512" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-2"> f (x) </script>  .  Functions as complex as physical simulators or ray tracers can also be differentiated and optimized.  Even <a href="https://arxiv.org/abs/1803.00745">quantum computing</a> can fit into this structure. </p><br><p><img src="https://habrastorage.org/webt/8i/pk/fa/8ipkfaovyvmrkrgev-dnc85atm8.png"></p><br><p>  Scientists have long used mechanistic models that are between explicit programming and machine learning.  Differential equations with free parameters used in physics, epidemiology or pharmacodynamics are equivalent in all, except terminology, <a href="https://julialang.org/blog/2019/01/fluxdiffeq">neural networks</a> .  They are simply more limited in what functions they can represent, which makes it easier to get a good result. </p><br><p>  Indeed, powerful progress is this: all-pervasive differentiability means that all these methods are connected together like <em>lego</em> bricks.  Instead of always writing new ML programs, we can incorporate existing programs by adapting <a href="https://arxiv.org/abs/1611.01652">physics engines in models of robotics</a> based on deep learning.  Where modern reinforcement learning algorithms have to build a detailed model of the external world based only on what they will be rewarded for (sounds like <a href="https://twitter.com/emilecontal/status/1089011610566385664">brute force</a> ), we can instead simply omit a detailed, accurate knowledge of physical systems before learning even begins. </p><br><p>  Even the most mature areas of deep learning do not stand aside;  After the convolution kernel, the next next step for image models is a <a href="https://people.csail.mit.edu/tzumao/diffrt/">differentiated ray tracer</a> .  3D rendering contains a lot of structural knowledge about how scenes are displayed in pixels, and this can also be in our melting pot.  Let's say a model makes decisions in a simulated environment, displayed as pixels, which the model uses as input.  In principle, we can now make the whole cycle differentiable, which will allow us to directly see the influence of the environment on the model's decisions and vice versa.  This can significantly increase the power of a realistic simulated environment for training models, such as cars with automatic driving. </p><br><p> As in science, hybrid models can be more efficient and allow some of the trade-offs between deep learning and explicit programming.  For example, a UAV flight path planner may have a neural network component that can make only marginally corrective changes to a reliable explicit program, making its overall behavior analyzed, while at the same time adapting to empirical data.  This is also good for interpretability: the parameters of mechanistic models and simulations usually have clear physical interpretations, so if the model evaluates the parameters inside, it makes a clear statement about what it thinks is happening outside. </p><br><p>  If this is all so wonderful, why hasn’t everyone abandoned and didn’t rush to learn to differentiate?  Unfortunately, the limitations of <a href="https://julialang.org/blog/2017/12/ml%26pl">existing frameworks</a> make it difficult to build models of such complexity, and it is impossible to reuse the wealth of knowledge embedded in the existing scientific code.  The need to re-implement physical engines from scratch in a very limited modeling language turns a ten-line script into a multi-year research project.  But advances in language and <a href="https://julialang.org/blog/2018/12/ml-language-compiler">compilation technology</a> , especially <a href="https://arxiv.org/abs/1810.07951">automatic differentiation</a> , bring us closer to the holy grail: "just differentiate my game engine, please." </p><br><h2 id="itak-chto-takoe-differenciruemoe-programmirovanie">  So what is differentiated programming? </h2><br><p>  Differentiated programming applies deep learning methods to complex existing programs, taking advantage of the vast amount of knowledge embedded in them: deep learning, statistics, programming and science — everything that models the world around us and what can be thrust into <a href="https://arxiv.org/pdf/1702.00748.pdf">a particle accelerator</a> .  This will improve current models and allow ML to be applied in areas where its current constraints — either interpretability or computational and data requirements — make it impractical alone. </p><br><h2 id="differenciruemye-problemy-upravleniya">  Differentiated control problems </h2><br><p>  Further, we show that ∂P can lead to some simple but classic control problems in which we usually use a black box for <strong>reinforcement learning (RL)</strong> .  ∂P-models not only teach much more effective management strategies, but also teach several orders of magnitude faster.  <a href="https://github.com/FluxML/model-zoo/tree/cdda5cad3e87b216fa67069a5ca84a3016f2a604/games/differentiable-programming">The code is available</a> for study - in most cases, it learns in a few seconds on any laptop. </p><br><h2 id="sleduyte-za-gradientom">  Follow the Gradient </h2><br><p>  Differentiation is almost every step of deep learning;  for this function <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-3-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy=&quot;false&quot;>(</mo><mi>x</mi><mo stretchy=&quot;false&quot;>)</mo></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="8.672ex" height="2.66ex" viewBox="0 -832 3733.6 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-79" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-3D" x="775" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="1831" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-28" x="2382" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-78" x="2771" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-29" x="3344" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>y</mi><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-3"> y = f (x) </script>  we use a gradient <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-4-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>y</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>x</mi></mrow></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="10.061ex" height="2.419ex" viewBox="0 -780.1 4332 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-79" x="523" y="0"></use></g><g transform="translate(3236,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-78" x="523" y="0"></use></g></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mi>y</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mi>x</mi></mrow></math></span></span><script type="math/tex" id="MathJax-Element-4"> \ frac {dy} {dx} </script> to figure out how changing <code>x</code> will affect <code>y</code> .  Despite the mathematical essence, gradients are in fact a very general and intuitive concept.  Forget the formulas you had to look at in school;  let's do something more fun, like bullet stuff. </p><br><p><img src="https://habrastorage.org/webt/hy/uu/qr/hyuuqrkomuhvvl8h_wxnqlzkfas.gif"></p><br><p>  When we drop things with a pre-emption, our <code>x</code> (input) is a setting (say, counterweight size or angle of release), and <code>y</code> is the distance the projectile travels before landing.  If you are trying to aim, the gradient tells you something very useful - to increase or decrease a certain parameter.  To maximize distance, just follow the gradient. </p><br><p>  Good, but how do we get the right parameter?  But with the help of a clever thing called <a href="">algorithmic differentiation</a> , which allows you to differentiate not only simple formulas that you have learned at school, but also programs of any complexity — for example, our <a href="">simulator Trebushet</a> .  As a result, we can take a simple simulator written in Julia and the DiffEq <a href="">diffuro</a> package without deep study, and get the gradients for it in one function call. </p><br><pre> <code class="julia hljs"><span class="hljs-comment"><span class="hljs-comment"># what you did in school gradient(x -&gt; 3x^2 + 2x + 1, 5) # (32,) # something a little more advanced gradient((wind, angle, weight) -&gt; Trebuchet.shoot(wind, angle, weight), -2, 45, 200) # (4.02, -0.99, 0.051)</span></span></code> </pre> <br><h2 id="throwing-stuff">  Throwing stuff </h2><br><p>  We need to aim it at the target, using gradients to fine tune the angle of release;  Similar things are common under the name of parameter estimates, and <a href="https://julialang.org/blog/2019/01/fluxdiffeq">we have already considered similar examples</a> .  We can make the task more interesting by going to the meta-method: instead of aiming the trebuchet at one goal, we optimize a neural network that can target it at any goal.  Here's how it works: the neural network accepts two inputs, the target distance in meters and the current wind speed.  The network puts out the required settings (the weight of the counterweight and the angle of disengagement), which are fed into the simulator, which calculates the distance traveled.  Then we compare with our goal and move along the whole chain to adjust the weight of the network.  Our “data set” is a randomly selected set of targets and wind speeds. </p><br><p><img src="https://habrastorage.org/webt/nc/gz/7w/ncgz7wwykokoutzvfgyrm4xbspm.png"></p><br><p>  A good feature of this simple model is that learning takes place quickly, because we have expressed exactly what we want from the model, in a completely differentiated way.  Initially it looks like this: </p><br><p><img src="https://habrastorage.org/webt/au/8a/-d/au8a-dfq1ud9_fhdcrbvzapfedi.gif"></p><br><p>  After about five minutes of training (on the same processor core of my laptop), it looks like this: </p><br><p><img src="https://habrastorage.org/webt/ta/wu/lr/tawulrzggnp09s7qwcyykbhqeqs.gif"></p><br><p>  If you want to influence the trajectory, increase the wind speed: </p><br><p><img src="https://habrastorage.org/webt/qq/9m/dg/qq9mdgrns5hqcpzedeoh1-htrua.gif"></p><br><p>  Deviated by 16 cm, or about 0.3%.  And how about targeting directly?  This is easy to do with a gradient descent, given that we have gradients.  However, this is a slow iterative process that takes about 100 ms each time.  On the contrary, the work of the neural network takes 5 µs (twenty thousand times faster) with a small loss of accuracy.  This trick “approximate inversion of functions through gradients” is very general, and it can be used not only with dynamic systems, but also with <a href="https://github.com/lengstrom/fast-style-transfer">a fast style transfer algorithm</a> . </p><br><p>  This is the simplest possible management problem that we use mainly for illustrative purposes.  But we can apply the same methods, in more advanced ways, to classical RL problems. </p><br><h2 id="cart-meet-pole">  Cart, meet pole </h2><br><p>  A more recognizable control problem is the <a href="https://gym.openai.com/envs/CartPole-v0/">CartPole</a> , “Hello world” for reinforcement learning.  The challenge is to learn how to balance a vertical pillar, pushing its base left or right.  Our installation is generally similar to the Trebuchet case: <a href="">the Julia implementation</a> allows us to directly consider the reward received by the medium as a loss.  ∂P allows us to seamlessly switch from a simple model to an RL model. </p><br><p><img src="https://habrastorage.org/webt/ik/wp/0y/ikwp0y4qhogsybm3rakwiyxwfuc.png"></p><br><p>  An insightful reader may notice a snag.  The action space for a card board — offset to the left or right — is discrete and, therefore, not differentiable.  We solve this problem by introducing differentiable discretization, defined <a href="">as follows</a> : </p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display"><span class="MathJax_SVG" id="MathJax-Element-5-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><merror><mtext>f&amp;#xA0;(x)&amp;#xA0;=&amp;#xA0;\&amp;#xA0;left&amp;#xA0;\&amp;#xA0;{\&amp;#xA0;begin&amp;#xA0;{matrix}&amp;#xA0;\,&amp;#xA0;1,&amp;#xA0;\,&amp;#xA0;x&amp;#xA0;\&amp;#xA0;geqslant0&amp;#xA0;\\&amp;#xA0;-1,&amp;#xA0;\,&amp;#xA0;x&amp;#xA0;&amp;lt;0&amp;#xA0;\&amp;#xA0;end&amp;#xA0;{matrix}&amp;#xA0;\&amp;#xA0;right.</mtext></merror></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><span class="noError" aria-hidden="true" style="display: inline-block;">f&nbsp;(x)&nbsp;=&nbsp;\&nbsp;left&nbsp;\&nbsp;{\&nbsp;begin&nbsp;{matrix}&nbsp;\,&nbsp;1,&nbsp;\,&nbsp;x&nbsp;\&nbsp;geqslant0&nbsp;\\&nbsp;-1,&nbsp;\,&nbsp;x&nbsp;&lt;0&nbsp;\&nbsp;end&nbsp;{matrix}&nbsp;\&nbsp;right.</span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><merror><mtext>f&nbsp;(x)&nbsp;=&nbsp;\&nbsp;left&nbsp;\&nbsp;{\&nbsp;begin&nbsp;{matrix}&nbsp;\,&nbsp;1,&nbsp;\,&nbsp;x&nbsp;\&nbsp;geqslant0&nbsp;\\&nbsp;-1,&nbsp;\,&nbsp;x&nbsp;&lt;0&nbsp;\&nbsp;end&nbsp;{matrix}&nbsp;\&nbsp;right.</mtext></merror></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-5"> f (x) = \ left \ {\ begin {matrix} \, 1, \, x \ geqslant0 \\ -1, \, x <0 \ end {matrix} \ right. </script></p><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-6-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mtext>&amp;#xA0;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>f</mi></mrow><mrow class=&quot;MJX-TeXAtom-ORD&quot;><mi>d</mi><mi>x</mi></mrow><mo>=</mo><mn>1</mn></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="14.445ex" height="2.419ex" viewBox="0 -780.1 6219.6 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="250" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-72" x="800" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-61" x="1252" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-63" x="1781" y="0"></use><g transform="translate(2215,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="523" y="0"></use></g><g transform="translate(3289,0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-64" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-78" x="523" y="0"></use></g><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-3D" x="4662" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMAIN-31" x="5719" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtext>&nbsp;</mtext><mi>f</mi><mi>r</mi><mi>a</mi><mi>c</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mi>f</mi></mrow><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mi>x</mi></mrow><mo>=</mo><mn>1</mn></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-6"> \ frac {df} {dx} = 1 </script></p><br><p>  In other words, we force the gradient to behave as if <math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax_SVG" id="MathJax-Element-7-Frame" tabindex="0" style="font-size: 100%; display: inline-block; position: relative;" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;><mi>f</mi></math>" role="presentation"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.279ex" height="2.419ex" viewBox="0 -780.1 550.5 1041.5" role="img" focusable="false" style="vertical-align: -0.607ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/post/459562/&amp;xid=17259,15700002,15700022,15700186,15700191,15700256,15700259,15700262&amp;usg=ALkJrhjdCY6j2gKCws4dWZGNZxCAzqss-Q#MJMATHI-66" x="0" y="0"></use></g></svg><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-7"> f </script>  was an identical function.  Considering how much the mathematical idea of ​​differentiability is already used in ML, it may not be surprising that we can simply cheat here;  for learning, all we need is a signal to inform our pseudo-random crawl of parameter space, and the rest is details.  The results speak for themselves.  In cases where RL methods need to be trained in hundreds of episodes, before solving a problem, the ∂P models need only about 5 episodes to finally win. </p><br><p><img src="https://habrastorage.org/webt/7t/eq/hj/7teqhjn1f4rdicydv4iiqjasirm.gif"></p><br><h2 id="the-pendulum--backprop-through-time">  The Pendulum &amp; Backprop through Time </h2><br><p>  An important goal for RL <em>(reinforcement learning)</em> is to handle deferred remuneration when an action does not help us improve the results several steps in a row.  When the environment is differentiable, ∂P allows you to train the agent back-propagating in time, as in a recurrent network!  In this case, the state of the environment becomes a “hidden state” that changes between time steps. </p><br><p><img src="https://habrastorage.org/webt/uy/od/mb/uyodmbo3grxrwkyawrgfol0nujo.png"></p><br><p>  To demonstrate this technique, consider a model of a <a href="https://github.com/openai/gym/wiki/Pendulum-v0">pendulum</a> , where the task is to swing the pendulum until it rises vertically and to keep it in unstable equilibrium.  This is tricky for RL models;  After about 20 episodes of training, the problem is solved, but often the path to the solution is clearly suboptimal.  In contrast, BPTT can beat the <a href="https://github.com/openai/gym/wiki/Leaderboard">ranking of</a> RL <a href="https://github.com/openai/gym/wiki/Leaderboard">leaders</a> in one training episode.  It is instructive to observe how this episode unfolds;  at the beginning of the recording, the strategy is random, and the model improves over time.  The pace of learning is almost alarming. </p><br><p><img src="https://habrastorage.org/webt/jm/wm/nh/jmwmnhr9nts4ki257ywtittvgck.gif"></p><br><p>  The model is well suited to handle any starting angle and has something close to the optimal strategy.  When restarting, the model looks like this. </p><br><p><img src="https://habrastorage.org/webt/bu/7f/t3/bu7ft3bzofjrdumbfqvvwqr0nz8.gif"></p><br><p>  This is just the beginning;  we will achieve real success by applying <em>DP</em> to environments with which RL is generally too difficult to work with, where rich simulations and models already exist (as in most engineering and natural sciences), and where interpretability is an important factor (as in medicine). </p><br><h2 id="the-map-is-not-the-territory">  The map is not the territory </h2><br><p>  The limitation of these toy models is that they equate the simulated learning environment with the testing environment;  Of course, the real world is not differentiable.  In a more realistic model, the simulation gives us a rough pattern of behavior, which is refined by the data.  This data informs, say, a simulated wind effect, which, in turn, improves the quality of the gradients that the simulator transmits to the controller.  Models can even be part of the controller’s straight aisle, allowing it to refine its predictions without having to learn system dynamics from scratch.  Exploring these new architectures will make exciting future work. </p><br><h2 id="coda">  Coda </h2><br><p>  The basic idea is that differentiated programming, in which we simply write an arbitrary numerical program and optimize it using gradients, is a powerful way to create more advanced models and architectures that are similar to deep learning, especially when we have a large library of differentiable programs at hand. .  The models described are just previews, but we hope that they will give an idea of ​​how these ideas can be implemented in a more realistic way. </p><br><p>  Just as functional programming involves the reasoning and expression of algorithms using functional patterns, differentiable programming involves expressing algorithms using differentiable patterns.  The deep learning community has already developed many such design patterns, for example, to handle management problems or a consistent and tree-like data structure.  As the region grows older, much more will be invented, and as a result, against the background of these programs, probably even the most advanced of the existing deep learning architectures will look rude and backward. </p><br><h3 id="ssylki">  Links </h3><br><ul><li>  <a href="https://fluxml.ai/">Sources</a> </li><li>  <a href="https://en.wikipedia.org/wiki/Differentiable_programming">Wikipedia</a> </li><li>  <a href="https://habr.com/ru/post/63055/">Autodifferentiation;</a>  <a href="https://habr.com/ru/post/63055/">article on Habré</a> </li><li>  <a href="">Autodifferentiation;</a>  <a href="">ready package</a> </li><li>  <a href="https://github.com/YermolenkoIgor/Julia_tutorial_rus/blob/master/intro/AutoDiff.ipynb">Autodifferentiation;</a>  <a href="https://github.com/YermolenkoIgor/Julia_tutorial_rus/blob/master/intro/AutoDiff.ipynb">do it yourself</a> </li></ul><br><p><img src="https://habrastorage.org/webt/f2/4o/pr/f24oprfnnrnguwwb03xcyo__xx4.gif"></p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/459562/">https://habr.com/ru/post/459562/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../459550/index.html">Backup Part 5: Testing Bacula and Veeam Backup for Linux</a></li>
<li><a href="../459552/index.html">How to lose access to the live system, simply by fumbling the source code</a></li>
<li><a href="../459554/index.html">Watch for file changes with Alerting OpenDistro for Elasticsearch</a></li>
<li><a href="../459558/index.html">How to start using User Mode in Linux</a></li>
<li><a href="../459560/index.html">Capabilities of container data centers: a ready switching node in Myanmar for 50 days</a></li>
<li><a href="../459564/index.html">What developers need to know about business</a></li>
<li><a href="../459568/index.html">Vertical writing in modern IT</a></li>
<li><a href="../459570/index.html">Beeline shows ads Google bot. Bot displeased</a></li>
<li><a href="../459576/index.html">Useful Google Chrome Extensions for Programmers</a></li>
<li><a href="../459578/index.html">Project Management System by Open Core Model in the Public Sector</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>