<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>The myth of RAM and O (1)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Stockholm City Library. Photo minotauria . 


 In this article I want to tell you that evaluating the time to access memory as O (1) is a very bad ide...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>The myth of RAM and O (1)</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/files/1af/8bc/292/1af8bc29229d45f9af56eb728fe759c5.jpg"><br>  <em>Stockholm City Library.</em>  <em>Photo <a href="https://www.instagram.com/minotauria/">minotauria</a> .</em> </p><br><p>  In this article I want to tell you that evaluating the time to access memory as O (1) is a very bad idea, and instead we should use O (‚àöN).  First, we consider the practical side of the issue, then the mathematical side, based on theoretical physics, and then we consider the consequences and conclusions. </p><br><h1>  Introduction </h1><br><p>  If you studied computer science and analysis of algorithmic complexity, then you know that the passage through the linked list is O (N), the binary search is O (log (N)), and the search for an element in the hash table is O (1).  What if I tell you that all this is not true?  What if the pass through the linked list is actually O (N‚àöN), and the search in the hash table is O (‚àöN)? </p><br><p>  Do not believe?  I will convince you now.  I will show that memory access is not O (1), but O (‚àöN).  This result is valid both in theory and in practice.  Let's start with the practice. </p><br><h1>  Measure </h1><br><p>  Let's first define the definitions.  The ‚ÄúO‚Äù notation is great for many things, from memory usage to running instructions.  In this article, we O (f (N)) will mean that f (N) is the upper bound (worst case) for the <em>time</em> it takes to gain access to N bytes of memory (or, respectively, N equal-sized elements ).  I use Big O to analyze time, <em>but not operations</em> , and this is important.  We will see that the CPU is waiting for a long slow memory.  Personally, I don‚Äôt care what the processor is doing while waiting.  I only care about the time, how long this or that task is completed, so I limit myself to the definition above. <a name="habracut"></a></p><br><p>  Another note: RAM in the header means random access (random memory accesses) as a whole, and not some specific type of memory.  I consider the time to access information in memory, be it a cache, DRAM or swap. </p><br><p>  <a href="https://github.com/emilk/ram_bench">Here is a simple program</a> that runs through a coherent list of size N. Dimensions - from 64 elements to 420 million elements.  Each list node contains a 64-bit pointer and 64 bits of data.  Nodes are mixed in memory, so each memory access is arbitrary.  I measure the passage through the list several times, and then mark on the chart the time it took to access the item.  We should get a flat graph of the form O (1).  Here is what happens in reality: </p><br><p><img src="https://habrastorage.org/files/4a1/66f/d9e/4a166fd9e095459092018f312e53b385.png"><br>  <em>The difficulty of accessing an item in the linked list.</em>  <em>Accessing an arbitrary item in a 100-megabyte list is about 100 times slower than accessing an item in a 10-kilobyte list.</em> </p><br><p>  Notice that this chart uses a logarithmic scale on both axes, so the difference is really huge.  From about one nanosecond per element, we have reached a whole microsecond!  But why?  The answer, of course, is cache.  System memory (RAM) is actually quite slow, and to compensate, smart computer designers add a hierarchy of faster, closer and more expensive caches to speed up operations.  My computer has three cache levels: L1, L2, L3, 32 kb, 256 kb and 4 mb in size, respectively.  I have 8 gigabytes of RAM, but when I ran this experiment, I had only 6 free gigabytes, so in the latter I started swapping to disk (SSD).  Here is the same graph, but with the size of the caches. </p><br><p><img src="https://habrastorage.org/files/ce8/bbe/031/ce8bbe0316b941ffaa413a45a9195258.png"><br>  <em>The vertical lines indicate L1 = 32 kb, L2 = 256 kb, L3 = 4 mb and 6 gigabytes of free memory.</em> </p><br><p>  This graph shows the importance of caches.  Each cache layer is several times faster than the previous one.  This is the reality of modern CPU architecture, be it a smartphone, laptop or mainframe.  But where is the general pattern?  Can a simple equation be placed on this graph?  It turns out we can! </p><br><p>  Let's take a closer look, and note that between 1 megabyte and 100 megabytes is about a 10-fold slowdown.  And the same between 100 megabytes and 10 gigabytes.  It seems that every 100-fold increase in used memory gives a 10-fold slowdown.  Let's add this to the chart. </p><br><p><img src="https://habrastorage.org/files/ec7/454/e69/ec7454e693c34242894b88785a3ef3ce.png"><br>  <em>The blue line is O (‚àöN).</em> </p><br><p>  The blue line is a graph indicating O (‚àöN) cost of each memory access.  Seems great, right?  Of course, this is my particular car, and your picture may look different.  However, the equation is very easy to remember, so maybe it should be used as a rough rule. </p><br><p>  You probably ask, but what's next, right of the schedule?  Does the increase continue or does the schedule go flat?  Well, it becomes flat for a while, while the SSD has enough free space, after which the program will have to switch to the HDD, then to the disk server, then to the far data center, and so on.  Each jump will create a new flat area, but the general trend for improvement, I think, will continue.  I did not continue my experiment due to lack of time and lack of access to a large data center. </p><br><p>  ‚ÄúBut the empirical method cannot be used to define the boundaries of Big-O,‚Äù you say.  Of course!  Perhaps, there is a theoretical border at a delay at memory access? </p><br><h1>  Round Library </h1><br><p>  Let me describe a thought experiment.  Suppose you are a librarian working in a circular library.  Your table is in the center.  The time you need to get any book is limited by the distance you need to go.  And in the worst case, this is the radius, because you need to reach the very edge of the library. </p><br><p>  Suppose your sister works in another similar library, but she has (at the library, not sister :), - approx.  lane.) radius twice.  Sometimes she needs to go twice as much.  But its library has 4 times more space than yours, and it has 4 times more books.  The number of books is proportional to the square of the radius: N‚àù r¬≤.  And since the time T of access to the book is proportional to the radius, then N‚àù T¬≤ or T‚àù‚àöN or T = O (‚àöN). </p><br><p>  This is a rough analogy with the central processor, which needs to get data from its library - RAM.  Of course, the speed of the ‚Äúlibrarian‚Äù is important, but here we are limited by the speed of light.  For example, in one cycle of a 3-gigahertz processor, light travels a distance of 10 cm. So, for a trip back and forth, any instantly available memory should be no more than 5 centimeters from the processor. </p><br><p>  Well, how much information can we place within a certain distance r from the processor?  Above, we talked about a round flat library, but what if it is spherical?  The amount of memory that will fit in the sphere will be proportional to the cube of the radius - r¬≥.  In reality, computers are quite flat.  This is partly a question of form factor and partly a question of cooling.  Maybe someday we will learn how to build and cool three-dimensional memory blocks, but for now the practical limitation of the amount of information N within the radius r will be N ‚àù r¬≤.  This is also true for very distant repositories, such as data centers (which are distributed over a two-dimensional surface of the planet). </p><br><p>  But is it possible, theoretically, to improve the picture?  To do this, learn a little about black holes and quantum physics! </p><br><h1>  Theoretical boundary </h1><br><p>  The amount of information that can be placed in a sphere of radius r can be calculated using <a href="https://en.wikipedia.org/wiki/Bekenstein_bound">the Bekenstein boundary</a> .  This amount is directly proportional to the radius and mass: N ‚àù r ¬∑ m.  How massive can a sphere be?  Well, what is the densest in the universe?  Black hole!  <a href="https://en.wikipedia.org/wiki/Black_hole">It turns out</a> that the mass of a black hole is proportional to the radius: m ‚àù r.  That is, the amount of information that can be placed in a sphere of radius r is N ‚àù r¬≤.  We came to the conclusion that the amount of information is limited by the area of ‚Äã‚Äãthe sphere, and not the volume! </p><br><p>  In short: if you try to shove a very large L1 cache into the processor, it will eventually collapse into a black hole, which will prevent the user from returning the result of the calculation. </p><br><p>  It turns out that N ‚àù r¬≤ is not only practical, but also a theoretical border!  That is, the laws of physics set a limit on the speed of access to memory: in order to get N data bits, you need to transmit a message at a distance proportional to O (‚àöN).  In other words, every 100-fold increase in the task leads to a 10-fold increase in the time it takes to access one element.  And this is exactly what our experiment showed! </p><br><h1>  A bit of history </h1><br><p>  In the past, processors were much slower than memory.  On average, a memory search was faster than the calculation itself.  A common practice was to keep in memory tables for sines and logarithms.  But times have changed.  Processor performance grew much faster than memory speed.  Modern processors most of their time just waiting for the memory.  That is why there are so many cache levels.  I believe that this trend will continue for a long time, so it is important to rethink old truths. </p><br><p> You can say that the whole essence of Big-O is in abstraction.  It is necessary to get rid of such details of the architecture as a memory delay.  This is true, but I argue that <strong>O (1) is an incorrect abstraction</strong> .  In particular, Big-O is needed for abstraction of constant factors, but memory access is not a constant operation.  Neither in theory nor in practice. </p><br><p>  In the past, any access to memory in computers was equally expensive, so O (1).  But this is no longer the case, and for quite some time.  I think it's time to think about it differently, forget about memory access for O (1) and replace it with O (‚àöN). </p><br><h1>  Effects </h1><br><p>  The cost of accessing the memory depends on the size that is requested - O (‚àöN), where N is the size of the memory that is accessed with each request.  This means that if the same list or table is accessed, the following statement is true: </p><br><p>  Pass through the coherent list is O (N‚àöN) operation.  Binary search is O (‚àöN).  Getting from an associative array is O (‚àöN).  In fact, <strong>any arbitrary search in any database is at best O (‚àöN).</strong> </p><br><p>  It is worth noting that the actions performed between operations are important.  If your program works with a memory of size N, then any random request to memory will be O (‚àöN).  So the pass through the list of size K will cost O (K‚àöN).  When re-passing (immediately, without recourse to another memory) cost will be O (K‚àöK).  Hence an important conclusion: <strong>if you need to access the same memory area several times, then minimize the intervals between calls</strong> . </p><br><p>  If you make a pass through an array of size K, then the cost will be O (‚àöN + K), because only the first call will be arbitrary.  The second pass will be O (K).  Hence another important conclusion: <strong>if you plan to make a pass, then use an array</strong> . </p><br><p>  There is one big problem: many languages ‚Äã‚Äãdo not support real arrays.  Languages ‚Äã‚Äãlike Java and many scripting languages ‚Äã‚Äãstore all objects in dynamic memory, and the array there is actually an array of pointers.  If you pass through such an array, then the performance will be no better than when passing through a linked list.  <strong>Passing through an array of objects in Java is O (K‚àöN)</strong> .  This can be compensated by creating objects in the correct order, then the memory allocator, <em>I hope</em> , will place them in memory in order.  But if you need to create objects at different times or shuffle them, then nothing happens. </p><br><h1>  Conclusion </h1><br><p>  Memory access methods are very important.  One should always try to access memory in a predictable way, and minimize arbitrary memory accesses.  There is nothing new here, of course, but it is worth repeating.  I hope you will adopt a new tool for thinking about the cache: <strong>memory access costs O (‚àöN).</strong>  The next time you evaluate the complexity, think about this idea. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/309394/">https://habr.com/ru/post/309394/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../309382/index.html">Elixir: Preparing parsing correctly - yecc and leex</a></li>
<li><a href="../309384/index.html">Compensation of lags for weapons in MechWarrior Online</a></li>
<li><a href="../309386/index.html">OSPF (Quagga), Shorewall and Policy Routing: inactive route problem</a></li>
<li><a href="../309388/index.html">Bad code kills</a></li>
<li><a href="../309390/index.html">Selection of equipment for corporate cloud storage</a></li>
<li><a href="../309400/index.html">The book ‚ÄúHTML5 and CSS3. Website development for any browsers and devices. 2nd ed. "</a></li>
<li><a href="../309402/index.html">Microsoft StorSimple virtual array. Part 2</a></li>
<li><a href="../309404/index.html">Why the 35-year-old SoftBank company again spoils its reputation because of the ‚Äúcrazy‚Äù deal with ARM Holdings</a></li>
<li><a href="../309406/index.html">Anniversary Edition Intercepter-NG 1.0</a></li>
<li><a href="../309408/index.html">Manual Start Timer</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>