<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Comparing best APIs for filtering obscene content</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Full testing of several APIs for filtering images of various categories, such as nudity, pornography, and dismemberment. 



 The person immediately r...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Comparing best APIs for filtering obscene content</h1><div class="post__text post__text-html js-mediator-article">  Full testing of several APIs for filtering images of various categories, such as nudity, pornography, and dismemberment. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/61a/0bd/57a/61a0bd57a558ed9e069c9da24e8696fc.jpg"><br><br>  The person immediately realizes that a certain image is inappropriate, that is, NSFW (Not Safe For Work).  But for artificial intelligence, things are not so clear.  Many companies are now trying to develop effective means to automatically filter such content. <br><a name="habracut"></a><br>  I wanted to understand what is the current state of the market.  Compare the effectiveness of existing APIs for filtering images in the following categories: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Frank nudity </li><li>  Suggestive nudity (that is suggestive of outright nudity - approx. Lane.) </li><li>  Pornography / intercourse </li><li>  Imitation / animated porn </li><li>  Dismemberment (gore) / violence </li></ul><br>  <b>Tl; DR:</b> If you just want to know the best API, you can go directly to the comparison at the end of the article. <br><br><h1>  Experimental Conditions </h1><br>  <b>Data set</b>  For evaluation, I collected my NSFW dataset with an equal number of pictures in each NSFW subcategory.  The data set consists of 120 images with 20 positive NSFW images for each of the five mentioned categories, and 20 SFW images.  I decided not to use the publicly available set of YACVID 180, since it is mainly based on using nudity as a measure of NSFW content. <br><br>  Collecting NSFW pictures is tedious; this is a very long and painful task, which explains the small number of images. <br><br>  <b>The dataset is available for download <a href="https://drive.google.com/folderview%3Fid%3D18SY4oyZgTD_dh8-dc0wmsl1GvMsA7woY">here</a> .</b>  <b><u>[Warning: contains candid content]</u></b> <b><br><br></b>  <b><a href="https://docs.google.com/spreadsheets/d/1fEOJfTLmQdtRvllw1e8LXJ6vXAjQQj68iF4gWCJy1JM/edit%3Fusp%3Dsharing">Here is a</a> table with the raw results for each API and each image in the data set.</b> <br><br><h1>  Metrics </h1><br>  Each of the classifiers is evaluated according to generally accepted metrics: <br><br><h3>  True positive: TP </h3><br>  If the classifier calls something NSFW and it is actually NSFW. <br><br><h3>  True negative: TN </h3><br>  If the classifier calls something SFW, and this is actually SFW. <br><br><h3>  False Positive: FP </h3><br>  If the classifier is called something NSFW, and this is actually SFW. <br><br><h3>  False negative: FN </h3><br>  If the classifier calls something SFW, which was actually NSFW. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/462/ef5/45c/462ef545c6e968b3c54ba38417511dce.png"><br><br><h3>  Accuracy </h3><br>  If the model makes a prediction, can you trust it? <br><br><h3>  Accuracy (precision) </h3><br>  If the model says that the image is NSFW, how often is the prediction correct? <br><br><h3>  Recall </h3><br>  If all samples are NSFW, how much does she identify? <br><br><h3>  F1 score </h3><br>  It is a mixture of infallibility and recall, often similar to accuracy. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ec7/740/69d/ec774069de39f1d76591109e457f0277.png"></div><br><br>  The following content moderation APIs were evaluated: <br><br><ul><li>  <a href="https://aws.amazon.com/rekognition/" rel="noopener"><em>Amazon Rekognition</em></a> </li><li>  <a href="https://cloud.google.com/vision/" rel="noopener"><em>Google</em></a> </li><li>  <a href="https://azure.microsoft.com/en-in/services/cognitive-services/content-moderator/" rel="noopener"><em>Microsoft</em></a> </li><li>  <a href="https://algorithmia.com/algorithms/spullara/YahooOpenNSFW" rel="noopener"><em>Yahoo</em></a> </li><li>  <a href="https://algorithmia.com/algorithms/sfw/NudityDetectioni2v" rel="noopener"><em>Algorithmia</em></a> </li><li>  <a href="https://clarifai.com/models/nsfw-image-recognition-model-e9576d86d2004ed1a38ba0cf39ecb4b1" rel="noopener"><em>Clarifai</em></a> </li><li>  <a href="https://deepai.org/machine-learning-model/nsfw-detector" rel="noopener"><em>Deepai</em></a> </li><li>  <a href="https://imagga.com/solutions/adult-content-moderation.html" rel="noopener"><em>Imagga</em></a> </li><li>  <a href="https://nanonets.com/content-moderation-api/" rel="noopener"><em>Nanonets</em></a> </li><li>  <a href="https://sightengine.com/" rel="noopener"><em>Sightengine</em></a> </li><li>  <a href="https://xmoderator.com/" rel="noopener"><em>X-Moderator</em></a> </li></ul><br><h1>  Productivity by category </h1><br>  At first, I evaluated each API in all NSFW categories. <br><br><h1>  Pornography / intercourse </h1><br>  The Google and Sightengine APIs are really good here.  They are the only truly recognized all pornographic images.  Nanonets and Algorithmia are slightly behind with a score of 90%.  Microsoft and Imagga showed the worst performance in this category. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/6d9/faf/a65/6d9fafa658b2f527824eb6ae241fa41e.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/d6e/e79/167/d6ee791670e7dfa0425dee823d1ea08e.png"><br><br>  Images that are easy to identify are clearly pornographic.  All APIs correctly recognized the images above.  Most of them predicted NSFW with very high confidence. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a01/ca1/939/a01ca19394689c2cac96803ed7c3de26.png"><br><br>  Images that are difficult to identify contain partially closed or blurred objects, making it difficult to work.  In the worst case, 11 out of 12 systems made a mistake with the image.  The effectiveness in recognizing pornography varies greatly depending on the intensity of porn and how well the content is visible. <br><br><h1>  Frank nudity </h1><br>  Most APIs do an amazing job with many of the pictures in this category, showing a detection rate of 100%.  Even the most inefficient APIs (Clarifai and Algorithmia) showed 90%.  The definition of nudity has always been the subject of debate.  As can be seen from the results, systems usually fail in doubtful cases when there is a possibility that the image is still SFW. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c36/6fc/13f/c366fc13f13bf5565f65f7daf4737909.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/45c/436/44d/45c43644d9fd473d51f4f0ca7b6b4e9a.png"><br><br>  In simple images clearly visible nudity.  Anyone would call them NSFW without question.  No API made a mistake, and the average score was 0.99. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/83f/c7b/9cd/83fc7b9cde287f7b712fda75471f35b9.png"><br><br>  On controversial images the API was wrong.  Maybe the reason is that each of them has sensitivity settings. <br><br><h1>  Suggestive nudity </h1><br>  Google won again with a detection rate of 100%.  Sightengine and Nanonets performed better than others with 95% and 90%, respectively.  Automatic systems recognize suggestive nudity almost as easily as explicit.  They make a mistake in the pictures, which usually look like SFW, with only some signs of nudity. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/40f/cb1/3ba/40fcb13ba029230b0d414a206758eb1a.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/a92/bee/4be/a92bee4be11339ccb569a2d85aa4b977.png"><br><br>  Again, no API was mistaken on explicit NSFW images. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a6b/b00/f25/a6bb00f254a26a747a6ce0157f2805a4.png"><br><br>  In the suggestive nudity the API disagreed.  As in sheer nudity, they had different thresholds of tolerance.  I myself am not sure whether to recognize these SFW pictures or not. <br><br><h1>  Imitation / animated porn </h1><br>  All APIs did an exceptionally good job here and found 100% of imitation porn examples.  The only exception was Imagga, which missed one image.  I wonder why APIs work so well on this task?  Apparently, it is easier for algorithms to identify artificially created images than natural ones. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/855/a76/266/855a762662284a0e4838897912937e4d.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/331/258/a8c/331258a8c34a1d0fffda26bf69dd1824.png"><br><br>  All APIs have shown excellent results and high confidence ratings. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fbf/8cf/83a/fbf8cf83a46e0519d3d509df9fb4701d.png"></div><br><br>  The only image in which Imagga was mistaken can be interpreted as not porn, if you do not look at it for a long time. <br><br><h1>  Dismemberment </h1><br>  This is one of the most difficult categories, since the average detection efficiency through the API was less than 50%.  Clarifai and Sightengine outdid the competition by correctly finding 100% of the images in this category. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/cda/6f8/054/cda6f8054f52cd5e5ca122275aea8311.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/1d4/386/c13/1d4386c13acf272b29576b739008e2b3.png"><br><br>  The API best coped with medical images, but even on the lightest of them, 4 out of 12 systems were wrong. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/271/abb/d23/271abbd23e9a356933a5a3a961cdb18b.png"><br><br>  Difficult images have nothing in common.  However, people will very easily call these pictures bloody.  This probably means that the reason for the poor performance is the lack of available data for training. <br><br><h1>  Safe pictures </h1><br>  Images that are not identifiable as NSFWs are considered safe.  Data collection in itself is difficult, because these pictures must be close to the NSFW in order to evaluate the API well.  One can argue whether all these images are SFW or not.  Here, Sightengine and Google showed the worst result, which explains their excellent performance in other categories.  They just call all the dubious NSFW pictures.  Imagga, on the other hand, did a good job here because it doesn‚Äôt call anything NSFW.  X-Moderator also showed itself very well. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d5/930/9a1/9d59309a1e0a0797d96f7aa99d4ee25f.png"></div><br><br><img src="https://habrastorage.org/getpro/habr/post_images/489/e3f/1b1/489e3f1b10bb850cc201cd92ab76cd64.png"><br>  <b>Links to original images: <a href="https://drive.google.com/open%3Fid%3D1MJKxx0irLVWLd0g4Mvf4CMv76rXBT7oo" rel="noopener">SFW15</a> , <a href="https://drive.google.com/open%3Fid%3D1lCY1Ivwjb5lDnvWge6_qsbsqx2OFfSt1" rel="noopener">SFW12</a> , <a href="https://drive.google.com/open%3Fid%3D1SE1LnhENoNZYT2OiU1yyVA3d-2y01YNA" rel="noopener">SFW6</a> , <a href="https://drive.google.com/open%3Fid%3D1Mhac-551FPJIdDBYrP3UrquGLi0lSnPF" rel="noopener">SFW4</a></b> <br><br>  Only small patches of skin are displayed on the lungs for defining images, and people also easily identify them as SFW.  Only one or two systems recognized them correctly. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/47b/e0d/00c/47be0d00cc4bcf66e0e300468d579099.png"><br>  <b>Links to original images: <a href="https://drive.google.com/open%3Fid%3D1357lms8figMDZYCblKWC-7iUz20kTdiG" rel="noopener">SFW17</a> , <a href="https://drive.google.com/open%3Fid%3D1nyE_AqnUzWlrs_Bph2Coz32y6LfGQJqP" rel="noopener">SFW18</a> , <a href="https://drive.google.com/open%3Fid%3D1VhNgEuFZA_jtm1mvUPG3OtqGXqN9Wng8" rel="noopener">SFW10</a> , <a href="https://drive.google.com/open%3Fid%3D1C4cT6Mxzsn5HSPr0M42nhyIQzp1jYSnh" rel="noopener">SFW3</a></b> <br><br>  On all difficult-to-identify SFW images, larger areas of skin are displayed or this is anime (systems tend to consider anime pornography).  Most APIs considered images with a large bare body area as SFW.  The question is, is this an SFW? <br><br><h1>  General comparison </h1><br>  Looking at the effectiveness of the API in all categories of NSFW, as well as their effectiveness in recognizing SFW correctly, we can conclude that the best result for F1 and the best average accuracy for the system Nanonets: it works well in all categories in a stable way.  The Google system shows an exceptionally good result in the NSFW categories, but too often marks safe pictures as NSFW, therefore, received a penalty on the F1 metric. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/fa1/a3b/d17/fa1a3bd175db605cc97027a9f531da1a.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c5f/2ff/ee9/c5f2ffee941e3d59df38ea602594da6d.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/153/576/32c/15357632cb346d53743f183a95752995.png"></div><br><br><h1>  By developers </h1><br>  I compared the top 5 systems in terms of accuracy and the F1 score to evaluate differences in their performance.  The larger the area of ‚Äã‚Äãthe radar chart, the better. <br><br><h3>  1. Nanonets </h3><br>  The Nanonets system did not rank first in any category.  However, this is the most balanced solution.  The weakest point where you can still work on it is the accuracy of SFW recognition.  It is too sensitive to any naked part of the body. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/9d7/b93/04f/9d7b9304f0170027f64cb161a505006e.png"></div><br><br><h3>  2. Google </h3><br>  Google is the best in most NSFW categories, but worst of all in SFW detection.  I want to note that I took a sample for testing from Google, that is, it ‚Äúshould know‚Äù these images.  This may be the reason for really good performance in most categories. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/b25/27a/348/b2527a3486ea533f49db93f28871a5f5.png"></div><br><br><h3>  3. Clarifai </h3><br>  Clarifai really shines in defining the dismemberment, being ahead of most other APIs, the system is also well balanced and works well in most categories.  But she lacks accuracy in identifying suggestive nudity and pornography. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/760/cd3/f67/760cd3f6718b5455262636fbf5a5f048.png"></div><br><br><h3>  4. X-Moderator </h3><br>  X-Moderator is another well-balanced API.  In addition to dismemberment, it is well identifies most of the other types of NSFW.  The accuracy is 100% in the definition of SFW, which distinguishes this system from competitors. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/36b/dad/79c/36bdad79ca6aca4ab821d6ced6c10fef.png"></div><br><br><h3>  5. Sightengine </h3><br>  Like Google, Sightengine showed an almost perfect result in identifying NSFW.  However, she did not recognize a single image of a dismemberment. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/8fe/231/303/8fe231303a92ca3ec3d1b8b265474f53.png"></div><br><br><h1>  Prices </h1><br>  Another criterion in choosing an API is price.  The prices of all companies are compared below.  Most APIs offer a free trial with limited use.  The only completely free API from Yahoo, but it needs to be hosted on your hosting, this API is not included in this table. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/c1b/0c2/74e/c1b0c274ef386df2a58a1ff46ea24875.png"></div><br><br>  <i>Amazon, Microsoft, Nanonets and DeepAI offer the lowest price of $ 1,000 per month for a million API calls.</i> <br><br><h1>  What is the best content moderation API? </h1><br>  The subjective nature of the NSFW content makes it difficult to determine the winner. <br><br>  <b>For social media of a general theme, which is more focused on content distribution and needs a balanced classifier, I would prefer the Nanonets API at the highest F1 grade for the classifier.</b> <br><br>  If the application is focused on children, I would be reinsured and chose the Google API for its exemplary performance in all categories of NSFW, even with the loss of some of the normal content. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/d97/47f/99a/d9747f99ad72f6a43359821477bce7c3.png"></div><br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/ba8/59e/729/ba859e7291c0dc6ebf907a97f87d2f64.png"></div><br><br><h1>  What is NSFW really? </h1><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/78c/bae/afa/78cbaeafab83b61961daa9b8a0a31dea.jpg"></div><br><br>  Having spent a lot of time on this problem, I realized one key thing: in fact, the definition of NSFW is very vague.  Each person will have their own definition.  What is considered acceptable is largely dependent on what your service provides.  Partial nudity is acceptable in a dating application, but not residency.  And in the medical journal the opposite.  Indeed, the gray area is suggestive nudity where it is impossible to get the right answer. </div><p>Source: <a href="https://habr.com/ru/post/431628/">https://habr.com/ru/post/431628/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../431618/index.html">Harry Potter and the difficulties of translation: ROSMEN and MAKHAON against the original</a></li>
<li><a href="../431620/index.html">‚ÄúI became a therapist or psychologist‚Äù - Vitaly Friedman about UX testing and mobile interfaces</a></li>
<li><a href="../431622/index.html">View Flutter Live at the Wrike office on December 4th</a></li>
<li><a href="../431624/index.html">You need to eliminate not bugs, but the reason for their appearance: case from the game developer</a></li>
<li><a href="../431626/index.html">Microsoft will equip VR-glasses US soldiers</a></li>
<li><a href="../431630/index.html">Laser safety clearly, or why you should not look into the laser beam</a></li>
<li><a href="../431636/index.html">How to celebrate the day of information security</a></li>
<li><a href="../431638/index.html">How we got into the IB department of a large company and started working in it. Diary of two young and promising professionals</a></li>
<li><a href="../431642/index.html">Beekeepers against microcontrollers or about the benefits of errors</a></li>
<li><a href="../431644/index.html">Straight line with TM. v4.0</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>