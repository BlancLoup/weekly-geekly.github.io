<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Big Data from A to Z. Part 5.1: Hive - SQL engine over MapReduce</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! We continue our series of articles on data analysis tools and methods. The following 2 articles in our series will be devoted to Hive, a too...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Big Data from A to Z. Part 5.1: Hive - SQL engine over MapReduce</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  We continue our series of articles on data analysis tools and methods.  The following 2 articles in our series will be devoted to Hive, a tool for SQL lovers.  In previous articles we looked at <a href="https://habrahabr.ru/company/dca/blog/267361/">the MapReduce paradigm</a> , and the <a href="https://habrahabr.ru/company/dca/blog/270453/">techniques and strategies for working with it</a> .  Perhaps many readers some of the solutions to problems with MapReduce seemed somewhat cumbersome.  Indeed, almost 50 years after the invention of SQL, it seems rather strange to write more than one line of code to solve problems like ‚Äúcount me the amount of transactions broken down by region‚Äù. <br><br>  On the other hand, classical DBMS, such as Postgres, MySQL or Oracle, do not have such flexibility in scaling when processing large data arrays and, upon reaching a larger volume, further support becomes a big headache. <br><br> <a href="https://habrahabr.ru/company/dca/blog/283212/"><img src="https://habrastorage.org/getpro/habr/post_images/967/15f/f26/96715ff269ee89fb0a3241a71b6ce9cb.png"></a> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Actually, <a href="https://hive.apache.org/">Apache Hive</a> was coined to combine these two advantages: <br><br><ul><li>  Scalable MapReduce </li><li>  Ease of use of SQL for data samples. </li></ul><br>  Under the cat we will tell how this is achieved, how to start working with Hive, and what are the restrictions on its use. <br><a name="habracut"></a><br><h2>  general description </h2><br>  Hive appeared in the depths of the company Facebook in 2007, and a year later the source code of hive was opened and transferred under the control of the apache software foundation.  Initially, hive was a set of scripts over hadoop streaming (see <a href="https://habrahabr.ru/company/dca/blog/268277/">the 2nd article of</a> our cycle), later it developed into a full-fledged framework for querying data over MapReduce. <br><br>  The current version of apache hive (2.0) is an advanced framework that can work not only on top of the Map / Reduce framework, but also on top of <a href="http://spark.apache.org/">Spark</a> (about Spark we will have separate articles in the cycle), as well as <a href="https://tez.apache.org/">Apache Tez</a> . <br><br>  Apache hive is used in production by companies such as Facebook, Grooveshark, Last.Fm, and many others.  We at <a href="http://datacentric.ru/">Data-Centric alliance</a> use HIve as the main repository of logs for our advertising platform. <br><br><h2>  Architecture </h2><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/607/31d/b11/60731db1124ac0f5bccca2b15b798d24.png"></div><br>  Hive is an engine that turns SQL queries into chains of map-reduce tasks.  The engine includes components such as Parser (parses incoming SQL queries), Optimimer (optimizes the query for greater efficiency), Planner (schedules tasks for execution) Executor (runs tasks on the MapReduce framework. <br><br>  Hive also requires a metadata repository.  The fact is that SQL assumes work with such objects as a database, a table, columns, rows, cells, and so on.  Since the data itself that is used by hive is simply stored as files on hdfs, it is necessary to store somewhere the correspondence between the hive objects and the actual files. <br><br>  As metastorage, a regular relational DBMS is used, such as MySQL, PostgreSQL or Oracle. <br><br><h2>  Command line interface </h2><br>  In order to try working with hive, the easiest way is to use its command line.  A modern utility for working with hive is called <b>beeline</b> (hello to our partners from the operator of the same name :)) To do this, on any machine in the hadoop cluster (see <a href="https://habrahabr.ru/company/dca/blog/268277/">our tutorial on hadoop</a> ) with the hive installed, just type a command. <br><br><pre><code class="hljs perl">$beeline</code> </pre> <br>  Next, you need to establish a connection with the hive-server: <br><br><pre> <code class="hljs haskell"><span class="hljs-title"><span class="hljs-title">beeline</span></span>&gt; !connect jdbc:hive2://localhost:<span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span> root root <span class="hljs-type"><span class="hljs-type">Connecting</span></span> to jdbc:hive2://localhost:10000/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span> <span class="hljs-type"><span class="hljs-type">Connected</span></span> to: <span class="hljs-type"><span class="hljs-type">Apache</span></span> <span class="hljs-type"><span class="hljs-type">Hive</span></span> (<span class="hljs-title"><span class="hljs-title">version</span></span> 1.1.0-<span class="hljs-title"><span class="hljs-title">cdh5</span></span>.7.0) <span class="hljs-type"><span class="hljs-type">Driver</span></span>: <span class="hljs-type"><span class="hljs-type">Hive</span></span> <span class="hljs-type"><span class="hljs-type">JDBC</span></span> (<span class="hljs-title"><span class="hljs-title">version</span></span> 1.1.0-<span class="hljs-title"><span class="hljs-title">cdh5</span></span>.7.0) <span class="hljs-type"><span class="hljs-type">Transaction</span></span> isolation: <span class="hljs-type"><span class="hljs-type">TRANSACTION_REPEATABLE_READ</span></span> 0: jdbc:hive2://localhost:10000/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>&gt;</code> </pre> <br>  <code>root root</code> is the username and password in this context.  After this, you will receive a command prompt in which you can enter the <code>hive</code> . <br><br>  It is also sometimes convenient to not enter sql queries in the beeline command line, but to save and edit them in a file, and then execute all the queries from the file.  To do this, run beeline with the database connection parameters and the -f parameter indicating the name of the file containing the queries: <br><br><pre> <code class="hljs pgsql">beeline -u jdbc:hive2://localhost:<span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span> -n root -p root -f sorted.<span class="hljs-keyword"><span class="hljs-keyword">sql</span></span></code> </pre> <br><h2>  Data Units </h2><br>  When working with hive, you can select the following objects with which hive operates: <br><br><ol><li>  Database </li><li>  Table </li><li>  Partition (partition) </li><li>  Bucket </li></ol><br>  Let us examine each of them in more detail: <br><br><h3>  Database </h3><br>  <b>The database</b> represents an analogue of the database in relational DBMS.  The database is a <b>namespace</b> containing tables.  The command to create a new database looks like this: <br><br><pre> <code class="hljs pgsql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DATABASE</span></span>|<span class="hljs-keyword"><span class="hljs-keyword">SCHEMA</span></span> [<span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span>] &lt;<span class="hljs-keyword"><span class="hljs-keyword">database</span></span> <span class="hljs-type"><span class="hljs-type">name</span></span>&gt;</code> </pre> <br>  Database and Schema in this context is the same.  The optional <b>IF NOT EXISTS</b> additive, as it is not difficult to guess, creates the database only if it does not already exist. <br><br>  An example of creating a database: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DATABASE</span></span> userdb;</code> </pre> <br>  To switch to the appropriate database, use the USE command: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">USE</span></span> userdb;</code> </pre> <br><br><h3>  Table </h3><br>  A table in hive is an analogue of a table in a classical relational database.  The main difference - that the data hive'ovskih tables are kept simple in the form of regular files on hdfs.  These can be plain text csv files, binary sequence files, more complex column paruqet files, and other formats.  But in any case, the data over which the hive table is configured is very easy to read and not from the hive. <br><br>  Tables in hive are of two types: <br><br>  <b>A classic table</b> , to which data is added using hive.  Here is an example of creating such a table ( <a href="http://www.tutorialspoint.com/hive/hive_create_table.htm">source of example</a> ): <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> employee ( eid <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">name</span></span> <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, salary <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, destination <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">COMMENT</span></span> <span class="hljs-string"><span class="hljs-string">'Employee details'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">ROW</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DELIMITED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FIELDS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">LINES</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\n'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">STORED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> TEXTFILE;</code> </pre> <br>  Here we have created a table, the data in which will be stored in the form of ordinary csv-files, the columns of which are separated by tabs.  After that, the data in the table can be downloaded.  Let our user in the home folder on hdfs have (I remind you that you can download the file with the help of <b>hadoop fs -put</b> ) sample.txt file of the form: <br><br><pre> <code class="hljs pgsql"><span class="hljs-number"><span class="hljs-number">1201</span></span> Gopal <span class="hljs-number"><span class="hljs-number">45000</span></span> Technical manager <span class="hljs-number"><span class="hljs-number">1202</span></span> Manisha <span class="hljs-number"><span class="hljs-number">45000</span></span> Proof reader <span class="hljs-number"><span class="hljs-number">1203</span></span> Masthanvali <span class="hljs-number"><span class="hljs-number">40000</span></span> Technical writer <span class="hljs-number"><span class="hljs-number">1204</span></span> Kiran <span class="hljs-number"><span class="hljs-number">40000</span></span> Hr <span class="hljs-keyword"><span class="hljs-keyword">Admin</span></span> <span class="hljs-number"><span class="hljs-number">1205</span></span> Kranthi <span class="hljs-number"><span class="hljs-number">30000</span></span> Op <span class="hljs-keyword"><span class="hljs-keyword">Admin</span></span></code> </pre> <br>  We can upload data using the following command: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">LOAD</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DATA</span></span> INPATH <span class="hljs-string"><span class="hljs-string">'/user/root/sample.txt'</span></span> OVERWRITE <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> employee;</code> </pre> <br>  After hive will <b>move the</b> data stored in our file to the hive storage.  You can verify this by reading the data directly from the file in the hive repository in hdfs: <br><br><pre> <code class="hljs cs">[<span class="hljs-meta"><span class="hljs-meta">root@quickstart ~</span></span>]<span class="hljs-meta"><span class="hljs-meta"># hadoop fs -text /user/hive/warehouse/userdb.db/employee/* 1201  Gopal       45000    Technical manager 1202  Manisha     45000    Proof reader 1203  Masthanvali 40000    Technical writer 1204  Kiran       40000    Hr Admin 1205  Kranthi     30000    Op Admin</span></span></code> </pre> <br>  Classic tables can also be created as a result of a select query to other tables: <br><br><pre> <code class="hljs pgsql"><span class="hljs-number"><span class="hljs-number">0</span></span>: jdbc:hive2://localhost:<span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>&gt; <span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> big_salary <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> employee <span class="hljs-keyword"><span class="hljs-keyword">WHERE</span></span> salary &gt; <span class="hljs-number"><span class="hljs-number">40000</span></span>; <span class="hljs-number"><span class="hljs-number">0</span></span>: jdbc:hive2://localhost:<span class="hljs-number"><span class="hljs-number">10000</span></span>/<span class="hljs-keyword"><span class="hljs-keyword">default</span></span>&gt; <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> * <span class="hljs-keyword"><span class="hljs-keyword">FROM</span></span> big_salary; +<span class="hljs-comment"><span class="hljs-comment">-----------------+------------------+--------------------+-------------------------+--+ | big_salary.eid  | big_salary.name  | big_salary.salary  | big_salary.destination  | +-----------------+------------------+--------------------+-------------------------+--+ | 1201            | Gopal            | 45000              | Technical manager       | | 1202            | Manisha          | 45000              | Proof reader            | +-----------------+------------------+--------------------+-------------------------+--+</span></span></code> </pre> <br>  By the way, SELECT to create a table in this case already starts the mapreduce task. <br><br>  <b>External table</b> , the data in which is loaded by external systems, without the participation of hive.  To work with external tables when creating a table, you need to specify the <b>EXTERNAL</b> keyword, and also specify the path to the folder where the files are stored: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXTERNAL</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> employee_external ( eid <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">name</span></span> <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, salary <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, destination <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">COMMENT</span></span> <span class="hljs-string"><span class="hljs-string">'Employee details'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">ROW</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DELIMITED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FIELDS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">LINES</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\n'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">STORED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> TEXTFILE LOCATION <span class="hljs-string"><span class="hljs-string">'/user/root/external_files/'</span></span>;</code> </pre> <br>  After that, the table can be used in the same way as regular hive tables.  The most convenient thing about this is that you can simply copy the file to the correct daddy in hdfs, and hive will automatically pick up new files when querying the corresponding table.  This is very useful when working with logs for example. <br><br><h3>  Partition (partition) </h3><br>  Since hive is an engine for translating SQL queries into mapreduce tasks, usually even the simplest queries to the table lead to a complete scan of the data in this table.  In order to avoid complete scanning of data on some of the columns of the table, it is possible to partition this table.  This means that data relating to different values ‚Äã‚Äãwill be physically stored in different folders on HDFS. <br><br>  To create a partitioned table, you must specify which columns will be used for partitioning: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">IF</span></span> <span class="hljs-keyword"><span class="hljs-keyword">NOT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">EXISTS</span></span> employee_partitioned ( eid <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">name</span></span> <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, salary <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, destination <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">COMMENT</span></span> <span class="hljs-string"><span class="hljs-string">'Employee details'</span></span> PARTITIONED <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> (birth_year <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, birth_month <span class="hljs-keyword"><span class="hljs-keyword">string</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">ROW</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DELIMITED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FIELDS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">LINES</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\n'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">STORED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> TEXTFILE;</code> </pre> <br>  When filling data into such a table, it is necessary to explicitly indicate in which partition we fill the data: <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">LOAD</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DATA</span></span> INPATH <span class="hljs-string"><span class="hljs-string">'/user/root/sample.txt'</span></span> OVERWRITE <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> employee_partitioned <span class="hljs-keyword"><span class="hljs-keyword">PARTITION</span></span> (birth_year=<span class="hljs-number"><span class="hljs-number">1998</span></span>, birth_month=<span class="hljs-string"><span class="hljs-string">'May'</span></span>);</code> </pre> <br>  Now let's see how the directory structure looks like: <br><br><pre> <code class="hljs pgsql">[root@quickstart ~]# hadoop fs -ls /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/ <span class="hljs-built_in"><span class="hljs-built_in">Found</span></span> <span class="hljs-number"><span class="hljs-number">1</span></span> items drwxrwxrwx   - root supergroup          <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">03</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/birth_year=<span class="hljs-number"><span class="hljs-number">1998</span></span> [root@quickstart ~]# hadoop fs -ls -R /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/ drwxrwxrwx   - root supergroup          <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">03</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/birth_year=<span class="hljs-number"><span class="hljs-number">1998</span></span> drwxrwxrwx   - root supergroup          <span class="hljs-number"><span class="hljs-number">0</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">03</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/birth_year=<span class="hljs-number"><span class="hljs-number">1998</span></span>/birth_month=May -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup        <span class="hljs-number"><span class="hljs-number">161</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">15</span></span>:<span class="hljs-number"><span class="hljs-number">03</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_partitioned/birth_year=<span class="hljs-number"><span class="hljs-number">1998</span></span>/birth_month=May/sample.txt</code> </pre> <br>  It can be seen that the directory structure looks like this, that each partition corresponds to a separate folder on hdfs.  Now, if we run any queries, in WHERE clauses, the constraints on partition values ‚Äã‚Äã- mapreduce will only take input data from the corresponding folders. <br><br>  In the case of External tables, partitioning works in a similar way, but a similar directory structure will have to be created manually. <br><br>  Partitioning is very convenient for example for separating logs by dates, as a rule, any requests for statistics contain a restriction by dates.  This can significantly reduce the time of the request. <br><br><h3>  Bucket </h3><br>  Partitioning helps to reduce processing time, if usually when queries are known restrictions on the values ‚Äã‚Äãof a column.  However, it is not always applicable.  For example, if the number of values ‚Äã‚Äãin a column is very large.  For example, it can be a user ID in a system containing several million users. <br><br>  In this case, the division of the table into buckets will help us.  The rows of the table for which the value matches the hash function calculated by a specific column fall into one bakt. <br><br>  For any work with baketated tables, you must not forget to include support for buckets in the hive (otherwise, hive will work with them as with ordinary tables): <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">set</span></span> hive.enforce.bucketing=<span class="hljs-literal"><span class="hljs-literal">true</span></span>;</code> </pre> <br>  CLUSTERED BY is used to create a table broken into buckets. <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">set</span></span> hive.enforce.bucketing=<span class="hljs-literal"><span class="hljs-literal">true</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">CREATE</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> employee_bucketed ( eid <span class="hljs-built_in"><span class="hljs-built_in">int</span></span>, <span class="hljs-keyword"><span class="hljs-keyword">name</span></span> <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, salary <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>, destination <span class="hljs-keyword"><span class="hljs-keyword">String</span></span>) CLUSTERED <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span>(eid) <span class="hljs-keyword"><span class="hljs-keyword">INTO</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span> BUCKETS <span class="hljs-keyword"><span class="hljs-keyword">ROW</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FORMAT</span></span> <span class="hljs-keyword"><span class="hljs-keyword">DELIMITED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">FIELDS</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\t'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">LINES</span></span> <span class="hljs-keyword"><span class="hljs-keyword">TERMINATED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">BY</span></span> <span class="hljs-string"><span class="hljs-string">'\n'</span></span> <span class="hljs-keyword"><span class="hljs-keyword">STORED</span></span> <span class="hljs-keyword"><span class="hljs-keyword">AS</span></span> TEXTFILE;</code> </pre> <br>  Since the Load command is used to simply transfer data to the hive storage - in this case, it is not suitable for loading, since the data must be preprocessed, properly breaking them into buckets.  Therefore, they need to be loaded using the INSERT command from another table (for example, from an external table): <br><br><pre> <code class="hljs sql"><span class="hljs-keyword"><span class="hljs-keyword">set</span></span> hive.enforce.bucketing=<span class="hljs-literal"><span class="hljs-literal">true</span></span>; FROM employee_external <span class="hljs-keyword"><span class="hljs-keyword">INSERT</span></span> OVERWRITE <span class="hljs-keyword"><span class="hljs-keyword">TABLE</span></span> employee_bucketed <span class="hljs-keyword"><span class="hljs-keyword">SELECT</span></span> *;</code> </pre> <br>  After executing the command, make sure that the data is really broken into 10 parts: <br><br><pre> <code class="hljs pgsql">[root@quickstart ~]# hadoop fs -ls /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed <span class="hljs-built_in"><span class="hljs-built_in">Found</span></span> <span class="hljs-number"><span class="hljs-number">10</span></span> items -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000000</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000001</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000002</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000003</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000004</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000005</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000006</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000007</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000008</span></span>_0 -rwxrwxrwx   <span class="hljs-number"><span class="hljs-number">1</span></span> root supergroup   <span class="hljs-number"><span class="hljs-number">31555556</span></span> <span class="hljs-number"><span class="hljs-number">2016</span></span><span class="hljs-number"><span class="hljs-number">-05</span></span><span class="hljs-number"><span class="hljs-number">-08</span></span> <span class="hljs-number"><span class="hljs-number">16</span></span>:<span class="hljs-number"><span class="hljs-number">04</span></span> /<span class="hljs-keyword"><span class="hljs-keyword">user</span></span>/hive/warehouse/employee_bucketed/<span class="hljs-number"><span class="hljs-number">000009</span></span>_0</code> </pre> <br>  Now, when querying for data related to a specific user, we will not need to scan the entire table, but only 1/10 of this table. <br><br><h2>  Hive checklist </h2><br>  Now we have disassembled all the objects that hive operates on.  Once the tables are created - you can work with them, since with the usual database tables.  However, you should not forget that hive is still the engine for launching mapreduce tasks over regular files, and it is not a complete replacement for a classical DBMS.  Reckless use of such heavy commands as a JOIN can lead to very long tasks.  Therefore, before building your architecture based on hive, you need to think a few times.  Here is a small checklist for using hive: <br><br><ul><li>  There is a lot of data that needs to be processed and they are not stored on the disk of one machine (otherwise it is better to think about classic SQL systems). <br><br></li><li>  The data is mostly only added and rarely updated (if updates are frequent - it may be worth considering using Hbase, for example, see our <a href="https://habrahabr.ru/company/dca/blog/280700/">previous material</a> . <br><br></li><li>  The data has a well-structured structure and is well divided into columns. <br><br></li><li>  Data processing patterns are well described by a declarative query language (SQL). <br><br></li><li>  The response time to the request is not critical (since hive works on the basis of MapReduce ‚Äî you should not expect interactivity). </li></ul><br><h2>  Conclusion </h2><br>  In this article, we sorted out the hive architecture, the data units that operate on hive, and gave examples on how to create and populate the hive tables.  In the next article of the cycle, we will look at the advanced features of hive, including: <br><br><ul><li>  Transactional model </li><li>  Indices </li><li>  User-defined functions </li><li>  Integration of hive with non-hdfs data warehouses </li></ul><br><br>  <a href="https://www.youtube.com/channel/UCOvuB83CWNZ0yz8qeNpWIIQ">Youtube Channel about data analysis</a> <br><br>  Links to previous articles in the cycle: <br><br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/267361/">Big Data from A to Z. Part 1: Principles of working with big data, the MapReduce paradigm</a> <br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/268277/">Big Data from A to Z. Part 2: Hadoop</a> <br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/270453/">Big Data from A to Z. Part 3: Techniques and strategies for developing MapReduce-applications</a> <br>  ¬ª <a href="https://habrahabr.ru/company/dca/blog/280700/">Big Data from A to Z. Part 4: Hbase</a> </div><p>Source: <a href="https://habr.com/ru/post/283212/">https://habr.com/ru/post/283212/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../283198/index.html">About the 1C: Enterprise Mobile Platform</a></li>
<li><a href="../283202/index.html">Laboratory: Rust - let's talk about Rust in Kaspersky Lab on May 17</a></li>
<li><a href="../283204/index.html">We build a pre-built dylib into the application.</a></li>
<li><a href="../283208/index.html">Using Matalysis in computer games (part 3)</a></li>
<li><a href="../283210/index.html">We are looking for vulnerabilities using google</a></li>
<li><a href="../283214/index.html">AngularJS and Ruby on Rails Single Page Application Building Architecture</a></li>
<li><a href="../283216/index.html">The law ‚ÄúOn Personal Data‚Äù and the practice of its application in Russian reality. Part 2</a></li>
<li><a href="../283218/index.html">Yandex Translate for Visual Studio Code</a></li>
<li><a href="../283220/index.html">Data transfer: fantastic speed and new methods</a></li>
<li><a href="../283222/index.html">Access to the Firebird database from YII2</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>