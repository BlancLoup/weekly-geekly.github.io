<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Feel Neural Network or Neural Network Designer</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I have been interested in neural networks for a long time, but only from the point of view of the viewer - I followed the new opportunities that they ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">🔎</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">📜</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">⬆️</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">⬇️</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Feel Neural Network or Neural Network Designer</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/webt/ev/ze/on/evzeonp9st8jbej0jg8vicbnd0u.png"><br><br>  I have been interested in neural networks for a long time, but only from the point of view of the viewer - I followed the new opportunities that they offer in comparison with the usual programming.  But never climbed either into theory or into practice.  And suddenly (after the sensational news about AlphaZero) I wanted to make my neural network.  After seeing a few lessons on this topic on YouTube, I got a little idea of ​​the theory and moved on to practice.  In the end, I did even better than my neural network.  It turned out the designer of the neural networks and visual aid for them (that is, you can see what is happening inside the neural network).  Here's what it looks like: <br><br><img src="https://habrastorage.org/webt/y1/lh/xj/y1lhxjd_yxgdocd6ld0ei8biw3m.png"><br><a name="habracut"></a><br>  And now a little more.  With this constructor, you can create a network of direct distribution (Feedforward neural network) up to 8 hidden layers (plus a layer of inputs and a layer of outputs, totaling 10 layers (usually 4 layers is more than enough)) in each layer up to 30 neurons because all this is simultaneously displayed on the screen, if there are requests in the comments, I will release a version without limitations and visualization).  The activation function of all neurons is sigmoid based on the logistic function.  It is also possible to train the resulting networks by the method of back propagation of error by gradient descent according to given examples.  And, most importantly, you can look at each neuron in each individual case (what value it transmits further, its displacement (correction, bias) - neurons with negative displacement are white, with positive displacement are bright green), connections of neurons depending on their weight are marked red - positive, blue - negative, and also differ in thickness - the greater the module weight, the thicker.  And if you hover the mouse on a neuron, you can still see what signal it comes to, and what exactly is its offset.  This is useful to understand how a particular network works or to show students how the direct distribution networks work.  But the most important thing is that you can save your network to a file and share it with the world. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Next, there will be instructions for using the program, embedding the networks created in your projects, as well as the analysis of several networks that are bundled. <br><br><h2>  How to use the constructor </h2><br>  To get started, <a href="https://yadi.sk/d/2wUAYMiU3TfaE9">download the archive from here</a> . <br><br>  Unpack to the root of the <code>D:\</code> drive <br><br>  Run <code>NeuroNet.exe</code> <br><br>  You can try to “Download” any network, look at it, click on “Training”, see its accuracy, poke arrows to the left, to the right (on the sides), to view various options of input (left column of neurons) and output (right) data, Click "Stop" and try to enter your input data (any values ​​from 0 to 1 are allowed, consider this when creating your networks and normalize the input and output data). <br><br>  Now how to build your network.  First of all, you need to define the network architecture (the number of neurons in each layer, separated by commas), click “Build” (or “Demolish” first, then build, if you have another network displayed on the screen), click “Teaching sample”, “Delete all »And enter your teaching examples, according to the instructions on the screen.  You can also indicate the input and output small square images (maximum 5x5 pixels), from which the normalized brightness values ​​of the pixels will be determined (disregarding their color), for which you need to click on "in" and "out", respectively.  Click "Add an example", repeat the procedure the desired number of times.  Click “Finish”, “Training” and how accuracy will be satisfactory (usually 98%), click “Stop” icon in the form of a floppy disk (save), give the network a name and be happy that you created the neural network yourself.  Additionally, you can set the learning speed with the “More Accurate / Faster” slider, and also visualize not every 50th step, but every 10th or 300th step, as you like. <br><br><h2>  Integration of established networks into your projects </h2><br>  To use my neural networks in my own projects, I created a separate application, <code>doNet.exe</code> , which needs to be run with parameters: “ <code>D:\NeuroNet\doNet.exe &lt; &gt; &lt;   &gt;</code> ”, wait for the application to finish, after which read the output from <code>D:\NeuroNet\temp.txt</code> <br><br>  For example, a 4-5.exe application is created using the 4-5 network (about this and other networks below).  This application describes in detail how to properly run doNet.exe <br><br><h2>  Parsing the bundled networks </h2><br>  Let's start with the classics - "XOR (Half-rated)."  Among others, in particular, this task — addition modulo 2 — in 1969 was cited as an example of the limitations of neural networks (namely, single-layer perceptrons).  In general, there are two inputs (with values ​​either 0 or 1 each), our task is to answer 1 if the values ​​of the inputs are different, 0 - if they are the same. <br><br><img src="https://habrastorage.org/webt/bp/2o/nk/bp2onksk08gvfsgkrx1r0paqade.png"><br><br>  Further "Quantity-units".  Three inputs (0 or 1 on each).  It is required to count how many units were filed.  Implemented as a classification task - four outputs for each answer variant (0,1,2,3 units).  At which output the maximum value, respectively, the answer is the same. <br><br><img src="https://habrastorage.org/webt/iu/nj/ki/iunjkiqbgwho4urowk1zpnb3hfy.png"><br><br>  "Multiplication" - Two inputs (real from 0 to 1), the output of their product. <br><br><img src="https://habrastorage.org/webt/xr/gw/2k/xrgw2kuxc4oiqkxc5g3azkyjmvm.png"><br><br>  “4-5” - The normalized brightness values ​​of the pixels of the 4x4 image are fed to the input, and the output has the normalized brightness values ​​of the pixels of the 5x5 image. <br><br><img src="https://habrastorage.org/webt/gd/e9/ym/gde9ymjns-sym-ky0kzh7unmdpi.png"><br><br>  The network conceived how to increase the quality of a large image by 25%, and the interesting filter for a photo came out: <br><br><img src="https://habrastorage.org/webt/xy/8d/xq/xy8dxqm-9afcncvu9hfwkskfgde.png"><br><br>  <b>UPD:</b> The NeuroNet2.exe application was added to the archive (the same constructor, but without visualization (thanks to which it works 2 times faster) and restrictions on the number of neurons in the layer (up to 1024 instead of 30), also in the training set, input and output can be submitted square pictures up to 32x32).  Also added to the training schedule.  Neural networks can now use (and integrate into their projects (even on the server)) and those who do not know their theory!  In the semi-automatic mode (after training, manually submit values ​​to the input and get the result on the screen) they can be used even without programming knowledge! <br><br><img src="https://habrastorage.org/webt/pe/ao/9w/peao9we8plxi9blywvcpwuulsiy.png"><br><br>  That's all, waiting for comments. <br><br>  <i>PS If an error occurs, try registering with the administrator using regsvr32 comdlg32 files, which are also in the archive.</i> </div><p>Source: <a href="https://habr.com/ru/post/351922/">https://habr.com/ru/post/351922/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../351912/index.html">Gartner Data & Analytics Summit 2018</a></li>
<li><a href="../351914/index.html">Marvin Minsky "The Emotion Machine": Chapter 2 "Impressors"</a></li>
<li><a href="../351916/index.html">Application architecture on the Akili framework</a></li>
<li><a href="../351918/index.html">Who's there? In the European Union offered to hide the data of the owners of domain names</a></li>
<li><a href="../351920/index.html">Android Support Library 28. What's new?</a></li>
<li><a href="../351924/index.html">What do we know about the terrain loss of machine learning?</a></li>
<li><a href="../351926/index.html">Welcome to Front-end MeetUp in Raiffeisenbank. UPD: Broadcast Mitap</a></li>
<li><a href="../351928/index.html">Conference DEFCON 22. "Mass scanning of the Internet through open ports." Robert Graham, Paul McMillan, Dan Tantler</a></li>
<li><a href="../351930/index.html">How to stay in the TOP when changing search algorithms (a guide for novice SEOs)</a></li>
<li><a href="../351932/index.html">Installing Debian with a root on an encrypted ZFS mirror</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>