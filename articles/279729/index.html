<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Deep Reinforcement Learning (or what you bought DeepMind for)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I continue to talk about the success of DeepMind. This post is about their first public achievement: an algorithm that learns to play Atari games with...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Deep Reinforcement Learning (or what you bought DeepMind for)</h1><div class="post__text post__text-html js-mediator-article">  I continue to talk about the success of DeepMind.  This post is about their first public achievement: an algorithm that learns to play Atari games without knowing anything about games except pixels on the screen. <br><br>  Here, in fact, the main artifact (if you have not seen this video, look necessarily, it explodes the brain) <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/EfGD2qveGdQ%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700248,15700253&amp;usg=ALkJrhga_OpyUMsF9ShJFJxnJlGVEO8BwA" frameborder="0" allowfullscreen=""></iframe><br>  That's about as much publicly known about the company when it is bought for half a billion dollars. <br><a name="habracut"></a><br>  <em>Disclaimer: The post is written on the basis of fairly edited chat logs <a href="http://closedcircles.com/%3Finvite%3D0c36b64786b4574a8d4b65bc9787ee123863ca9a">closedcircles.com</a> , hence the style of presentation, and the availability of clarifying questions.</em> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Seminal paper, how it works - <a href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf</a> <br><br><h1>  So, quickly and on the fingers about Q-learning </h1><br>  Suppose we are in some kind of environment that has the current state <em>s</em> , and at each moment we can perform the action <em>a</em> from the discrete set.  This action takes the system to a new state (stochastically, i.e. there is anything random in the system) and can give us a reward or end game <br><br>  The concept of cumulative return over time is introduced - this is the total amount of reward that we can get from the current moment to the end of the game, with future rewards decreasing by <em>y</em> for each time tick, i.e.  the next time is reward - this is <em>y * r</em> , after another - <em>y <sup>2</sup> * r</em> <br><br>  So, the optimal behavior can be described by a certain function <em>Q * (s, a)</em> , which returns a single number. <br>  The main thing is to understand what it means! <br><br>  It means the following: if we are in the state <em>s</em> , and we take action <em>a</em> , what will be our cumulative reward until the end of the game, if after the action <em>a</em> we act in an optimal way? <br>  If such a function is given from above, with its help it is easy to play optimally - try Q * (s, a) for all a, choose the maximum one. <br><br>  Is everyone clear?  I'll go put the tea. <br>  Okay, your problems. <br><br>  Next, there is a key recurrence equation for this imaginary Q * function, the so-called Bellman Equation <br><img src="https://habrastorage.org/files/4b3/c12/72d/4b3c1272d1a448d5a8be76d0149d2bff.png" alt="image"><br>  That is so beautiful. <br>  What he says! <br><br>  He says that if we know with what probabilities <em>s</em> goes into the next state <em>s`</em> due to the action <em>a</em> ... <br>  Then <em>Q *</em> for the current state <em>s</em> should be equal to the maximum from the choice of the next action in the next step. <br><br>  Well, they say, if we know the future, then to understand what the reward will be on the current team when performing action <em>a</em> , we can look into this future, see what actions are optimal there and thus say what will be a cumulative reward. <br><br>  This expression, of course, can be represented as an iterative step of improving the function. <br>  That is, once again, if we know from which state into which system probabilistically passes in the case of certain actions, we can use this to improve our approximation of the function <em>Q.</em> <br><br>  Oh, the tea is boiling. <br><br>  <em>Listen, I mean what I didn‚Äôt understand at all - how can she then play all Atari games?</em> <br>  She does not play all the games at once.  She trains the Q function for each game. <br><br>  So.  The idea itself is simple.  Let's introduce this Q-function as CNN!  (in the sense of convolutional neural network) <br>  This is just the usual one, with pixels on the inputs and with the number of neurons in the output equal to the number of possible actions.  More precisely, the network receives pixels of several past emulator frames (4 frames in the original implementation), that is, the network can track moving objects. <br>  On each neuron of the last layer of the network - <em>Q (s, a)</em> for the desired <em>a</em> . <br><br><h1>  How to train this network? </h1><br>  Here is a network version in the current step, with some weights.  A new step happened, we learned a new transition from <em>s</em> to <em>s'</em> as a result of <em>a</em> and whether it was reward. <br>  Let us keep all such stupid <em>(s, s', a, r)</em> save, all that we saw for the game. <br><br>  Choose a few random ones, and use the equation above to tweak the weights of the network a little, so that they predict the maximum of what the old weights predicted on the next stage. <br><br>  This moment is difficult to describe in words, but just to understand: <br>  We calculate the maximum of the predictions of the past network for <em>s-s' by</em> iterating through all the actions one step at a time, and let us say we train in order to immediately predict what we got by looking at the options and maximum. <br>  And actually almost everything. <br><br>  As a result, we remember in memory of the algorithm all transitions between the steats that the system saw and train the neural network, which stupidly in the picture predicts where the next action will lead us. <br><br>  The neural network itself does not have a memory and a state at all, the entire system memory is in trained weights.  If anything, muscular such memory. <br>  (why it is necessary to train on a random dial-up from history, and not on the latter - so that a positive feedback loop does not happen, that is, so that the system does not adapt to the consequences of a particular recent choice, but to the data as a whole) <br><br>  Small details - they still use the so-called e-greedy policy. <br>  At each step, with probability <em>e</em> , a stupidly random action will be done, and with probability <em>(1-e) the</em> Q-function predicted by the current estimate. <br>  From time <em>e</em> , of course, decreases. <br><br><img src="https://habrastorage.org/files/e21/72b/1ec/e2172b1ec1e146c381cb17eb6e4edb05.png" alt="image"><br><br>  Here is the pseudocode algorithm. <br>  And that's all.  Do not do anything else. <br><br>  Only train CNN to predict the Q function along the way.  On an array of states, iteratively. <br><br>  <em>I wonder how long it is learning</em> <br>  Tens and hundreds of hours <br><br>  <em>in, and also tell about experience replay (they worked out common phrases there, but in the code everything is too simple).</em> <br>  Experience replay is their name for the fact that you need to memorize all state transitions that were during the work and train them all. <br>  <em>Yes, very cool.</em>  <em>As I understand it, they didn‚Äôt really bother with the choice of the ‚Äúforgetting‚Äù strategy (they‚Äôre stupidly ‚Äúforgetting‚Äù the old one by the fact of rewriting it).</em> <br>  Yeah, how much memory is enough, and remember so much. <br><br>  <em>about preprocessing - I correctly understand that the point of packing four (three) images in "fi" is stupidly saving computing, and not completely trying to somehow work with local animation / direction of movement, etc.?</em> <br>  They give the input past 4 frames of history. <br>  It seems to me that it is independent of frame skipping to save resources, so this allows the network to look at the animation, yes. <br><br>  <em><a href="http://deepqa.tilda.ws/page78823.html">By the</a> way, during <a href="http://deepqa.tilda.ws/page78823.html">Diphak</a> in one team, the guys reduced the number of frames to 3, with no apparent deterioration in the results</em> <br>  They write in the article that everywhere was ok with 4 (and this is better than 3 for training time), except for Space Invaders. <br>  In them, the laser shot every 4 frames and therefore became invisible with gaps.  Made for them 3. <br><br>  <em>Vooot, and finally a stupid question while we discuss the input frames and the Main Formula with Gamma Discount Rivord</em> <br>  <em>where did they get reward from?</em> <br>  From the game.  Stupid scores.  In the original version, they simply looked at the sign of change scores. <br><br>  <em>"from the game" - is it from the emulator with an additional emulator hack?</em> <br>  The short answer to the question and at the same time to the unasked questions about the frames on the screen and how to press the buttons - use ready-made libraries like <a href="http://www.arcadelearningenvironment.org/">http://www.arcadelearningenvironment.org</a> , otherwise your research on DQN will quickly stop. <br>  How good that we live in the XX1 century!  Both the rovers and end-of-game signals will be extracted, for example, by this very ALE from more than 50 atari games. <br><br>  It is very interesting to communicate with the outside world, I think.  In order for this entire deep reinforcement learning to occur, a lot of things must come together.  First of all, there must be games. <br><br><h2>  Games - the main engine of progress (and apparently porn yet) </h2><br>  Since there are games, we have a very powerful GPU. <br>  Since there are games on Atari, there is a powerful community about the emulator and other infrastructure. <br>  Since there are games and people play them, we have created other games - excellent training programs, perfectly varying degrees of difficulty.  The further, the more realistic. <br>  Do not save yourself. <br><br>  For people who like the code to read more than paperies, this is a good implementation - <a href="https://github.com/spragunr/deep_q_rl">https://github.com/spragunr/deep_q_rl</a> <br></div><p>Source: <a href="https://habr.com/ru/post/279729/">https://habr.com/ru/post/279729/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../279717/index.html">Express analysis of malicious files for educational purposes</a></li>
<li><a href="../279721/index.html">Setting up email notification templates in 3CX Phone System</a></li>
<li><a href="../279723/index.html">"Favorite smartphone chemist technologist" or the unification of the desktop of your gadget</a></li>
<li><a href="../279725/index.html">The evolution of SDN: the way to a great programmable future</a></li>
<li><a href="../279727/index.html">ActiveResource, prefix and nested resources</a></li>
<li><a href="../279731/index.html">Qt 5.6 framework release</a></li>
<li><a href="../279733/index.html">ECMA-262 standard (JavaScript) in pictures, part 2</a></li>
<li><a href="../279735/index.html">Productivity in the development of Office Add-ins</a></li>
<li><a href="../279737/index.html">Replacing jitter exceptions</a></li>
<li><a href="../279739/index.html">Processing ‚Äúvideo 360‚Äù, image cleaning: algorithm and its implementation in C #</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>