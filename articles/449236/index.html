<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Ok, google: how to get captcha?</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. My name is Ibadov Ilkin, I am a student of the Ural Federal University. 

 In this article, I want to talk about my experience with the automat...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Ok, google: how to get captcha?</h1><div class="post__text post__text-html js-mediator-article">  Hello.  My name is Ibadov Ilkin, I am a student of the Ural Federal University. <br><br>  In this article, I want to talk about my experience with the automated solution of the google company ‚ÄúGoogle‚Äù - ‚ÄúreCAPTCHA‚Äù.  I would like to warn the reader in advance that at the time of writing this article, the prototype is not working as effectively as it may seem from the title, however, the result shows that the approach being implemented is able to solve the problem posed. <br><a name="habracut"></a><br>  Probably everyone in his life came across captcha: enter text from a picture, solve a simple expression or a complex equation, choose cars, fire hydrants, pedestrian crossings ... Protecting resources from automated systems is necessary and plays a significant role in security: captcha protects from DDoS attacks , automatic registrations and postings, parsing, prevents from spam and selection of passwords to accounts. <br><br><img src="https://habrastorage.org/webt/kw/la/yu/kwlayu1_wxfwcrknvshirpv8dtw.png"><br>  <font color="#999999"><i>The registration form on "Habr√©" could be with such a captcha.</i></font> 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      With the development of machine learning technologies, the performance of a captcha may be at risk.  In this article, I describe the key points of the program, which can solve the problem of manual image selection in Google reCAPTCHA (to my delight, not always). <br><br>  To get a captcha, you need to solve such tasks as: determining the required captcha class, detecting and classifying objects, detecting captcha cells, imitating human activity in captcha solution (cursor movement, click). <br><br>  To search for objects in an image, trained neural networks are used that can be downloaded to a computer and recognize objects in images or videos.  But to solve a captcha, it is not enough just to detect objects: it is necessary to determine the position of the cells and find out which cells you need to select (or not select the cells at all).  For this, computer vision tools are used: in this work, this is the well-known <a href="https://opencv.org/">OpenCV library</a> . <br><br>  In order to find the objects in the image, first, the image itself is required.  I get a screenshot of a part of the screen using the ‚Äú <a href="https://pyautogui.readthedocs.io/">PyAutoGUI</a> ‚Äù module with sufficient dimensions for detecting objects.  In the remainder of the screen, I bring up windows for debugging and monitoring the processes of the program. <br><br><h2>  Object Detection </h2><br>  Detection and classification of objects is what the neural network does.  The library that allows us to work with neural networks is called ‚Äú <a href="https://www.tensorflow.org/">Tensorflow</a> ‚Äù (developed by Google).  Today, <a href="">there are many different trained models for</a> your choice <a href="">on different data</a> , which means that they can all return different detection results: some models will be better at finding objects, and some are worse. <br><br>  In this paper I use the ssd_mobilenet_v1_coco model.  The selected model is trained on the <a href="http://cocodataset.org/"><abbr title="Common Objects in Context">COCO</abbr></a> data set, which identifies 90 different classes (from people and cars to a toothbrush and a comb).  Now there are other models that are trained on the same data, but with different parameters.  In addition, this model has optimal performance and accuracy, which is important for a desktop computer.  The source reports that the processing time for a single frame of 300 x 300 pixels is 30 milliseconds.  On the "Nvidia GeForce GTX TITAN X". <br><br>  The result of the neural network is a set of arrays: <br><br><ul><li>  with the list of classes of detected objects (their identifiers); </li><li>  with a list of assessments of detected objects (in percent); </li><li>  with the list of coordinates of the detected objects ("boxes"). </li></ul><br>  The indices of the elements in these arrays correspond to each other, that is: the third element in the array of object classes corresponds to the third element in the array of ‚Äúboxes‚Äù of detected objects and the third element in the array of object ratings. <br><br><img src="https://habrastorage.org/webt/g4/po/kt/g4poktayba4n3mwspffduto6d1k.png"><br>  <font color="#999999"><i>The selected model allows you to detect objects from 90 classes in real time.</i></font> <br><br><h2>  Cell Detection </h2><br>  ‚ÄúOpenCV‚Äù provides us with the ability to operate on entities called ‚Äú <a href="https://docs.opencv.org/3.1.0/d4/d73/tutorial_py_contours_begin.html">outlines</a> ‚Äù: They can be found only with the function ‚ÄúfindContours ()‚Äù from the library ‚ÄúOpenCV‚Äù.  The input of such a function is to submit a binary image, which can be obtained <a href="https://docs.opencv.org/3.4.3/d7/d4d/tutorial_py_thresholding.html">by the threshold conversion function</a> : <br><br><pre><code class="python hljs">_retval, binImage = cv2.threshold(image,<span class="hljs-number"><span class="hljs-number">254</span></span>,<span class="hljs-number"><span class="hljs-number">255</span></span>,cv2.THRESH_BINARY) contours = cv2.findContours(binImage, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[<span class="hljs-number"><span class="hljs-number">0</span></span>]</code> </pre> <br>  Having established the extreme values ‚Äã‚Äãof the parameters of the threshold conversion function, we also get rid of various kinds of noise.  Also, to minimize the number of unnecessary small elements and noise, you can apply <a href="https://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B0%25D1%2582%25D0%25B5%25D0%25BC%25D0%25B0%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BE%25D1%2580%25D1%2584%25D0%25BE%25D0%25BB%25D0%25BE%25D0%25B3%25D0%25B8%25D1%258F">morphological transformations</a> : erosion (compression) and capacity (expansion) functions.  These functions are also included in the "OpenCV".  After transformations, contours are selected, the number of vertices of which is equal to four (having previously performed an <a href="https://docs.opencv.org/3.4.0/d3/dc0/group__imgproc__shape.html">approximation</a> function over the contours). <br><br><img src="https://habrastorage.org/webt/mo/d3/j7/mod3j7vkdav-bchdnbxl2lycz18.png"><br>  <font color="#999999"><i>In the first window, the result of the threshold conversion.</i></font>  <font color="#999999"><i>In the second - an example of morphological transformation.</i></font>  <font color="#999999"><i>In the third window, the cells and captcha cap are already selected: they are highlighted in color by software.</i></font> <br><br>  After all the transformations, the outlines that are not cells still fall into the final array with cells.  In order to weed out unnecessary noise, I select by the values ‚Äã‚Äãof the length (perimeter) and the area of ‚Äã‚Äãthe contours. <br><br>  It was experimentally revealed that the values ‚Äã‚Äãof the circuits of interest are in the range from 360 to 900 units.  This value is selected on the screen with a diagonal of 15.6 inches and a resolution of 1366 x 768 pixels.  Further, the indicated contour values ‚Äã‚Äãcan be calculated depending on the size of the user's screen, but there is no such anchoring in the prototype being created. <br><br>  The main advantage of the chosen approach to the detection of cells is that we do not care how the grid will look and how many cells will be shown on the captcha page: 8, 9 or 16. <br><br><img src="https://habrastorage.org/webt/oo/er/tt/ooerttikg_xree5wlre88cljjtq.png"><br>  <font color="#999999"><i>The image shows varieties of captcha nets.</i></font>  <font color="#999999"><i>Note that the distance between the cells is different.</i></font>  <font color="#999999"><i>Separate cells from each other allows morphological compression.</i></font> <br><br>  An additional advantage of detecting contours is the fact that OpenCV allows us to detect their centers (we need them to determine the coordinates of movement and mouse click). <br><br><h2>  Selecting cells for selection </h2><br>  Having an array with pure contours of captcha cells without unnecessary noise contours, we can cycle through each cell of a captcha (‚Äúcontour‚Äù in the terminology of ‚ÄúOpenCV‚Äù) and check it for the fact of intersection with the detected ‚Äúbox‚Äù of the object received from the neural network. <br><br>  To establish this fact, the translation of the detected ‚Äúbox‚Äù into a cell-like contour was used.  But this approach turned out to be wrong, because the case when the object is located inside a cell is not considered to be an intersection.  Naturally, such cells did not stand out in the captcha. <br><br>  The problem was solved by redrawing the contour of each cell (with white fill) on a black sheet.  Similarly, a binary image of a frame with an object was obtained.  The question arises - how now to establish the fact of the intersection of a cell with a filled object frame?  In each iteration of the array with cells, a disjunction (logical or) is performed on the two binary images.  As a result, we get a new binary image in which the intersected sections will be highlighted.  That is, if there are such areas, then the cell and the frame of the object intersect.  Programmatically, such a test can be done using the ‚Äú <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.any.html">.any ()</a> ‚Äù method: it will return ‚ÄúTrue‚Äù if there is at least one element in the array equal to one or ‚ÄúFalse‚Äù if there are no units. <br><br><img src="https://habrastorage.org/webt/cg/8u/pb/cg8upb6izzlfukmfunjgjrzwl2y.gif"><br>  <font color="#999999"><i>The ‚Äúany ()‚Äù function for the ‚ÄúLogical OR‚Äù image in this case will return true and thereby establish the fact of the intersection of the cell with the frame area of ‚Äã‚Äãthe detected object.</i></font> <br><br><h2>  Control </h2><br>  Cursor control in ‚ÄúPython‚Äù becomes available thanks to the ‚Äúwin32api‚Äù module (however, later it turned out that ‚ÄúPyAutoGUI‚Äù already imported into the project also knows how to do this).  Pressing and releasing the left mouse button, as well as moving the cursor to the desired coordinates, is performed by the corresponding functions of the win32api module.  But in the prototype, they were wrapped in user functions in order to provide visual observation of the movement of the cursor.  This has a negative impact on performance and was implemented solely for demonstration. <br><br>  In the development process, the idea of ‚Äã‚Äãselecting cells in a random order arose.  It is possible that this has no practical sense (for obvious reasons, ‚ÄúGoogle‚Äù does not give us comments and descriptions of the captcha mechanisms), but moving the cursor over the cells in a chaotic manner looks more fun. <br><br><img src="https://habrastorage.org/webt/p0/zh/g8/p0zhg8ii2gn0a76yj1h0vp5q5mg.gif"><br>  <font color="#999999"><i>On animation, the result is ‚Äúrandom.shuffle (boxesForSelect)‚Äù.</i></font> <br><br><h2>  Text recognising </h2><br>  In order to combine all the existing developments into a single whole, one more link is required: the block of recognition of the class required from the captcha.  We already know how to recognize and distinguish different objects in the image, we can click on arbitrary captcha cells, but we don‚Äôt know which cells to click on.  One of the ways to solve this problem is to recognize the text from the captcha cap.  First of all, I tried to implement text recognition using the <a href="https://github.com/tesseract-ocr">Tesseract-OCR</a> optical character recognition tool. <br><br>  In recent versions, it is possible to install language packs directly in the installer window (previously this was done manually).  After installing and importing Tesseract-OCR into my project, I tried to recognize the text from the captcha cap. <br><br>  The result, unfortunately, did not impress me at all.  I decided that the text in the header is highlighted in bold and consistent style for a reason, so I tried to apply various transformations to the image: binarization, shrinking, expanding, blurring, distorting and resizing operations.  Unfortunately, this did not give a good result: in the best cases, only part of the class letters was determined, and when the result was satisfactory, I applied the same transformations, but for other caps (with different text), and the result was again bad. <br><br><img src="https://habrastorage.org/webt/mm/5o/zf/mm5ozfhoq5cmrwiitaig_di7ntu.png"><br>  <font color="#999999"><i>Recognizing the ‚ÄúTesseract-OCR‚Äù header usually resulted in unsatisfactory results.</i></font> <br><br>  It is impossible to say unequivocally that Tesseract-OCR does not recognize the text well, it is not so: the tool does much better with other images (not captcha caps). <br><br>  I decided to use a third-party service that offered an API for working with it free of charge (registration and receiving a key to an email address is required).  The service has a limit of 500 recognitions per day, but over the entire development period I have not encountered any problems with restrictions.  On the contrary: I submitted to the service the original image of the cap (without applying absolutely no transformations) and I was pleasantly impressed with the result. <br><br>  The words from the service were returned with almost no errors (usually even those written in small print).  Moreover, they were returned in a very convenient format ‚Äî broken down by line with newline characters.  In all the images I was interested only in the second line, so I addressed it directly.  This could not but rejoice, since such a format freed me from the need to prepare a line: I did not have to cut the beginning or end of the whole text, do trims, replacements, work with regular expressions and perform other operations on the line aimed at selecting one word (and sometimes two!) - a nice bonus! <br><br><pre> <code class="python hljs">text = serviceResponse[<span class="hljs-string"><span class="hljs-string">'ParsedResults'</span></span>][<span class="hljs-number"><span class="hljs-number">0</span></span>][<span class="hljs-string"><span class="hljs-string">'ParsedText'</span></span>] <span class="hljs-comment"><span class="hljs-comment">#   JSON lines = text.splitlines() #   print("Recognized " + lines[1]) #  !</span></span></code> </pre> <br>  The service that recognized the text almost never made a mistake with the class name, but I still decided to leave part of the class name for a possible error.  This is optional, but I noticed that ‚ÄúTesseract-OCR‚Äù in some cases incorrectly recognized the end of a word starting from the middle.  In addition, this approach eliminates the application error, in the case of a long class name or a two-word name (in this case, the service returns not 3, but 4 lines, and I cannot find an entry in the second line for the full name of the class). <br><br><img src="https://habrastorage.org/webt/6o/3v/nq/6o3vnqamplfdhd9byanlv4c55xw.png"><br>  <font color="#999999"><i>The third-party service recognizes the class name well without any transformations above the image.</i></font> <br><br><h2>  Fusion of developments </h2><br>  Get the text from the caps - a little.  It should be compared with the identifiers of the existing classes of the model, because in the class array the neural network returns exactly the class identifier, and not its name, as it may seem.  When training a model, as a rule, a file is created in which the names of the classes and their identifiers are matched (aka ‚Äúlabel map‚Äù).  I decided to do it easier and specify the class identifiers manually, since the captcha still requires classes in Russian (by the way, this can be changed): <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">if</span></span> <span class="hljs-string"><span class="hljs-string">""</span></span> <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> query: <span class="hljs-comment"><span class="hljs-comment">#       classNum = 3 #   "label map"  elif "" in query: classNum = 10 elif "" in query: classNum = 11 ...</span></span></code> </pre> <br>  Everything described above is reproduced in the main program loop: the object's frames, cells, their intersections are determined, cursor movements and clicks are made.  When a cap is detected, text is recognized.  If the neural network cannot detect the required class, then an arbitrary image shift is performed up to 5 times (that is, the input data changes to the neural network), and if detection has not yet occurred, then click on the ‚ÄúPass / Confirm‚Äù button (its position is detected similarly to cell detection and caps). <br><br>  If you often decide captcha, you could observe a picture when the selected cell disappears, and a new one appears slowly and slowly in its place.  Since the prototype was programmed to instantly go to the next page after selecting all the cells, I decided to make 3 second pauses in order to eliminate pressing the ‚ÄúNext‚Äù button without detecting objects on the slowly appearing cell. <br><br>  The article would not be complete if it did not contain a description of the most important thing - a tick of the successful passage of a captcha.  I decided that a simple <a href="https://docs.opencv.org/2.4/doc/tutorials/imgproc/histograms/template_matching/template_matching.html">pattern matching</a> can do this task.  It is worth noting that pattern matching is far from the best way to detect objects.  For example, I had to set the detection sensitivity to ‚Äú0.01‚Äù so that the function would stop seeing ticks in everything, but I saw it when there was a tick.  Similarly, I acted with an empty chekbos, which meets the user and from which the captcha begins (there were no problems with sensitivity). <br><br><h2>  Result </h2><br>  The result of all the actions described was the application, which I tested on the " <a href="https://toster.ru/">Toaster</a> ": <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/wfl1K0bqBWQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  We have to admit that the video was not shot on the first attempt, since I often encountered the need to select classes that are not available in the model (for example, pedestrian crossings, stairs or shop windows). <br><br>  ‚ÄúGoogle reCAPTCHA‚Äù returns a certain value to the site, indicating how ‚ÄúYou are a robot‚Äù, and site administrators, in turn, can set a threshold for passing this value.  It is possible that the ‚ÄúToaster‚Äù had a relatively low threshold for captcha passing.  This explains the rather easy passage of a captcha program, despite the fact that it was mistaken twice without seeing the traffic light from the first page and the fire hydrant from the fourth page of the captcha. <br><br>  In addition to the Toaster, experiments were conducted on the official <a href="https://www.google.com/recaptcha/api2/demo">reCAPTCHA demonstration page</a> .  As a result, it is noticed that after multiple erroneous detections (and non-detections), captcha becomes extremely difficult for even a person: new classes are required (like tractors and palm trees), cells without objects appear in the samples (almost monotonous colors) and the number of pages increases that need to go through. <br><br>  This was especially noticeable when I decided to try clicking on random cells in case of undetected objects (due to their absence in the model).  Therefore, we can say for sure that random clicks will not lead to the solution of the problem.  To get rid of such a ‚Äúblockage‚Äù by the examiner, the Internet connection was reconnected and the browser data was cleared, because it was impossible to pass such a test - it was almost endless! <br><br><img src="https://habrastorage.org/webt/xk/xe/5u/xkxe5uanfylidi1jgteyb3svyoc.png"><br>  <font color="#999999"><i>If you doubt your humanity, such an outcome is possible.</i></font> <br><br><h2>  Development </h2><br>  If the article and the application cause interest in the reader, I will be happy to continue its implementation, tests and further description in a more detailed form. <br><br>  We are talking about finding classes that are not part of the current network, it will significantly improve the efficiency of the application.  At the moment there is an urgent need to recognize at least such as classes like: pedestrian crossings, shop windows and chimneys - I will tell you how to retrain the model.  During development, I made a small list of the most common classes: <br><br><ul><li>  pedestrian crossings; </li><li>  fire hydrants; </li><li>  shop windows; </li><li>  chimneys; </li><li>  cars; </li><li>  buses; </li><li>  traffic lights; </li><li>  bicycles; </li><li>  vehicles; </li><li>  stairs; </li><li>  marks. </li></ul><br>  Improvements in the quality of object detection can also be achieved by using several models at the same time: this can degrade performance but increase accuracy. <br><br>  Another way to improve the quality of object detection is to change the image input to the neural network: in the video you can see that if objects are not detected, I repeatedly make an arbitrary image displacement (within 10 pixels horizontally and vertically), and often this operation allows you to see objects that were previously were not detected. <br><br>  An image magnification from a small square to a large one (up to 300 x 300 pixels) also leads to the detection of undetected objects. <br><br><img src="https://habrastorage.org/webt/01/sd/eh/01sdehtdkgz_a-5gtsxh1-auhoq.png"><br>  <font color="#999999"><i>No objects were found on the left: the original square with a side of 100 pixels.</i></font>  <font color="#999999"><i>On the right, the bus is found: an enlarged square up to 300 x 300 pixels.</i></font> <br><br>  Another interesting transformation is the removal of the white grid over the image by means of ‚ÄúOpenCV‚Äù: it is possible that the fire hydrant in the video was not detected for this reason (this class is present in the neural network). <br><br><img src="https://habrastorage.org/webt/m_/7p/zu/m_7pzu93h3jqw3doy3flbtlymee.png"><br>  <font color="#999999"><i>On the left, the original image, and on the right, the modified one in the graphical editor: the grid is removed, the cells are moved towards each other.</i></font> <br><br><h2>  Results </h2><br>  With this article I wanted to tell you that captcha is probably not the best protection against bots, and it is quite possible that soon there will be a need for new means of protection against automated systems. <br><br>  The developed prototype, even if it is in an incomplete state, demonstrates that with the presence of the required classes in the neural network model and the application of transformations over images, it is possible to achieve automation of the process that should not be automated. <br><br>  Also, I would like to draw Google‚Äôs attention to the fact that in addition to the captcha circumvention method described in this article, there is also <a href="https://vc.ru/services/54846-uncaptcha-2-servis-dlya-obhoda-recaptcha-ot-google-s-pomoshchyu-servisa-google">another way</a> in which the audio sample is <abbr title="The process of translating audio into text">transcribed</abbr> .  In my opinion, now it is necessary to take measures related to improving the quality of software products and algorithms against robots. <br><br>  From the content and essence of the material it may seem that I do not like ‚ÄúGoogle‚Äù and in particular ‚ÄúreCAPTCHA‚Äù, but this is far from the case, and if there is a next implementation, I will tell why. <br><br>  Developed and demonstrated in order to improve the level of education and improve methods aimed at ensuring the security of information. <br><br>  Thanks for attention. </div><p>Source: <a href="https://habr.com/ru/post/449236/">https://habr.com/ru/post/449236/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../449216/index.html">Cheating automated surveillance cameras</a></li>
<li><a href="../44922/index.html">Image Optimization, Part 3: 4 steps to reduce file size</a></li>
<li><a href="../449220/index.html">DrumHero: As I did the first game in my life</a></li>
<li><a href="../449224/index.html">About the bias of artificial intelligence</a></li>
<li><a href="../449232/index.html">Control of solar power consumption by computer / server</a></li>
<li><a href="../44924/index.html">Human possibilities are endless</a></li>
<li><a href="../449240/index.html">The story of one young service Daida (art by subscription)</a></li>
<li><a href="../449244/index.html">Medium - the first decentralized Internet provider in Russia</a></li>
<li><a href="../449246/index.html">AX200 - Intel's Wi-Fi 6</a></li>
<li><a href="../449252/index.html">Zombie projects - merge user data even after his death</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>