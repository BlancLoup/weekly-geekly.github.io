<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Factor modeling using neural network</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The article discusses factor modeling using the neural network factorization method and the error back-propagation algorithm. This factorization metho...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Factor modeling using neural network</h1><div class="post__text post__text-html js-mediator-article">  <i>The article discusses factor modeling using the neural network factorization method and the error back-propagation algorithm.</i>  <i>This factorization method is an alternative to classical factor analysis.</i>  <i>This method has been improved to conduct factor rotation and obtain an interpretable solution.</i>  <i>The factor structure obtained using this method of factorization, are in accordance with the results of factor modeling by other methods.</i> <br><a name="habracut"></a><br><p> <font><b>Introduction</b></font>  <font>The classical factor analysis [1] allows, on the basis of a sample of various indicators, to form factor indicators with the required accuracy describing the original object and reducing the dimensionality of the problem by moving to them.</font>  <font>Factor indicators are a linear combination of inputs.</font>  <font>Thus, factor models are linear.</font> </p><br><p>  <font>The neural network allows you to approximate the mapping between the source and target indicators.</font>  <font>Moreover, approximable mappings can be nonlinear.</font>  <font>A two-layer perceptron allows one to approximate any Boolean function of Boolean variables [2].</font>  <font>A two-level neural network can approximate</font> <font>any continuous function</font> <font>in a uniform metric with any given error</font> <font><i>Œµ</i></font> <font>&gt; 0</font> <font><img src="https://habrastorage.org/files/0f1/a7e/8fc/0f1a7e8fcfc24dc282b5348e17d3655d.gif" width="100" height="23"></font>  <font>, and in the mean square metric, any measurable function defined on a bounded set [3, 4, 5, 6].</font> </p><br><p>  <font>To restore the regularities between the parameters, a special neural network learning algorithm is used: the back error propagation algorithm [7].</font>  <font>From the mathematical point of view, this algorithm is a gradient optimization method.</font> </p><br><p>  <font>The essence of this method for constructing factor models is that a mathematical model of a neural network with a linear transfer function is used to identify patterns between the parameters.</font>  <font>The values ‚Äã‚Äãof the factor variables are determined to be equal to the values ‚Äã‚Äãof the output signals of the neurons of the hidden layer of the neural network.</font>  <font>Thus, the neural network performs classical factor analysis, i.e.</font>  <font>builds linear combinations of source parameters [8, 9, 10].</font> </p><br><p>  <font>This paper proposes an improved algorithm for backpropagating an error by introducing an additional term to the error function to construct an interpretable factor structure and solve the problem of factor rotation based on a neural network.</font> </p><br><p>  <font><b>Mathematical model of a neuron.</b></font>  <font>The state of the neuron is described by a set of variables:</font> </p><br><p>  <font>input weights</font> <font><img src="https://habrastorage.org/files/631/cd9/49f/631cd949f7a74efba0225ba24f9e81bc.gif" width="120" height="23"></font>  <font>where</font> <font><i>m</i></font> <font>is the number of input signals</font> <font><img src="https://habrastorage.org/files/9a7/d1f/94f/9a7d1f94f551450792a40a1cc1d6f46f.gif" width="22" height="21"></font>  <font>;</font> </p><br><p>  <font>free member</font> <font><img src="https://habrastorage.org/files/252/5e5/572/2525e55724de4fc8905027920b1fde63.gif" width="27" height="21"></font>  <font>in the calculation of the output signal.</font>  <font>The signal at the output of the neuron is calculated by the formula:</font> </p><br><p><img src="https://habrastorage.org/files/434/b84/edd/434b84eddcff4b0dadddff778741cf63.gif" width="63" height="19">  <font>where</font> <font><img src="https://habrastorage.org/files/f4c/91f/134/f4c91f13453745529c01002cc7346cab.gif" width="108" height="33"></font>  <font>- the weighted sum of the signals at the inputs of the neuron,</font> </p><br><p>  <font><i>œÉ</i></font> <font>is the transfer function of the neuron, for example, sigmoidal function</font> <font><img src="https://habrastorage.org/files/6df/a3f/6e7/6dfa3f6e75ee4f3fb949ddea77a20a93.gif" width="155" height="38"></font>  <font>.</font> </p><br><p>  <font><b>Neural network.</b></font>  <font>Separate neurons are combined into layers.</font>  <font>The output signals of neurons from one layer arrive at the input to the neurons of the next layer, the model of the so-called multilayer perceptron (Fig. 1).</font>  <font>In the software implementation of the author's neural network, the concept of descendant neurons and ancestor neurons is introduced.</font>  <font>All neurons that have an input signal from a given neuron are its descendants or passive neurons or axons.</font>  <font>All neurons forming the input signals of a given neuron are its ancestors or active neurons or dendrites.</font> </p><br><br><p><img src="https://habrastorage.org/files/b23/9fb/097/b239fb0970d34016baa95d0dd7f02683.gif" width="303" height="189"></p><br><br><p>  <font><b>Fig.</b></font>  <font><b>1</b></font> <font><b>.</b></font>  <font>Simple neural network (input neurons, hidden neurons, output neuron).</font> </p><br><p>  <font><b>Error propagation algorithm.</b></font>  <font>The back-propagation error algorithm for training a neural network corresponds to minimizing the error function</font> <font><i>E</i></font> <font><i>(</i></font> <font><i>w</i></font> <sub><font><i>ij</i></font></sub> <font><i>)</i></font> <font>.</font>  <font>As such an error function, the sum of squared deviations of the network output signals from the required ones can be used:</font> </p><br><p><img src="https://habrastorage.org/files/08e/69e/0a8/08e69e0a806548fba06daaced634b8e1.gif" width="118" height="41">  <font>,</font> </p><br><p>  <font>Where</font> <font><img src="https://habrastorage.org/files/232/fdb/9f6/232fdb9f633449e8a4a03baea191b3a0.gif" width="21" height="21"></font>  <font>- the output value of the</font> <font><i>i-</i></font> <font>th neuron of the output layer,</font> </p><br><p><img src="https://habrastorage.org/files/01d/3f6/64a/01d3f664af3c45768e4f0e87b06ba5c3.gif" width="21" height="24">  - the <font>required value of the</font> <font><i>i-</i></font> <font>th neuron of the output layer.</font> </p><br><p>  <font>In this algorithm, the learning iteration consists of three procedures:</font> </p><br><ol><li><p>  <font>The propagation of the signal and the calculation of the signals at the output of each neuron.</font> </p><br></li><li><p>  <font>Error calculation for each neuron.</font> </p><br></li><li><p>  <font>Changing the weights of connections.</font> </p><br></li></ol><br><p>  <font>By repeatedly cycling the sets of signals at the input and output and back propagation of an error, the neural network is trained.</font>  <font>For a multilayer perceptron and a certain type of neuron transfer function, the convergence of this method was proved for a certain type of error function [11].</font> </p><br><p>  <font><b>Calculation of errors.</b></font>  <font>If the transfer function of neurons is sigmoidal, then the errors for neurons of different layers are calculated using the following formulas.</font> </p><br><p>  <font>Error calculations for neurons of the output layer are made according to the formula:</font> </p><br><p><img src="https://habrastorage.org/files/6eb/223/733/6eb223733fba4d8f89c29e560637c8f2.gif" width="204" height="24">  <font>,</font> </p><br><p>  <font>Where</font> <font><img src="https://habrastorage.org/files/751/99c/e56/75199ce56ec34bf9bd66295bf3f2f905.gif" width="23" height="24"></font>  <font>- the desired value at the output of the</font> <font><i>jth</i></font> <font>neuron of the output layer</font> <font><i>L</i></font> <font>,</font> </p><br><p><img src="https://habrastorage.org/files/797/1f1/f56/7971f1f5601b408ca2489fa015b7970e.gif" width="23" height="21">  - the <font>signal at the output of the</font> <font><i>j-</i></font> <font>th neuron of the output layer</font> <font><i>L</i></font> <font>,</font> </p><br><p>  <font><i>L</i></font> <font>is the depth of the neural network,</font> </p><br><p>  <font>Errors for the neurons of the remaining layers are calculated by the formula:</font> </p><br><p><img src="https://habrastorage.org/files/2ec/73f/406/2ec73f4065a34eb8a09e91f52df78bbe.gif" width="313" height="38">  <font>,</font> </p><br><p>  <font>where</font> <font><i>i</i></font> <font>- indices of neurons, the descendants of the neuron,</font> </p><br><p><img src="https://habrastorage.org/files/498/8b5/673/4988b5673f15423b859ae43940f7f850.gif" width="42" height="24">  - the <font>signal at the output of the</font> <font><i>j-</i></font> <font>th neuron layer</font> <font><i>l</i></font> <font>,</font> </p><br><p><img src="https://habrastorage.org/files/593/64e/628/59364e628a9d404cb537ef7619b56e54.gif" width="61" height="24">  - the <font>connection between the</font> <font><i>j-</i></font> <font>th</font> <font>neuron of the</font> <font><i>l-</i></font> <font>th layer and the</font> <font><i>i-</i></font> <font>th</font> <font>neuron of the (</font> <font><i>l</i></font> <font>+1) -th layer.</font> </p><br><p>  <font><b>Changing the threshold levels of neurons and weights of connections.</b></font>  <font>To change the link weights, use the following formula:</font> </p><br><p><img src="https://habrastorage.org/files/14f/88e/6d0/14f88e6d0a454ff29cdf0e710b67a97b.gif" width="328" height="22"></p><br><p><img src="https://habrastorage.org/files/580/4d6/f21/5804d6f213e244bfbe2c32221c4515b1.gif" width="277" height="21">  <font>,</font> </p><br><p><img src="https://habrastorage.org/files/9a8/70a/b75/9a870ab75a624be7a4649152d23baa67.gif" width="312" height="22">  <font>,</font> </p><br><p><img src="https://habrastorage.org/files/61a/84d/ecd/61a84decd2f94462a9a4f64c473a5310.gif" width="274" height="21">  <font>,</font> </p><br><p>  <font>where</font> <font><i>i</i></font> <font>is the index of the active neuron (neuron of the source of input signals of passive neurons),</font> </p><br><p>  <font><i>j</i></font> <font>- passive neuron index,</font> </p><br><p>  <font><i>n</i></font> <font>is the</font> <font>number of the learning iteration,</font> </p><br><p>  <font><i>Œ±</i></font> <font>is the coefficient of inertia for smoothing sharp jumps when moving along the surface of the objective function,</font> </p><br><p>  <font>0 &lt;</font> <font><i>Œ∑</i></font> <font>&lt;1</font> <font>is the multiplier setting the speed of "movement".</font> </p><br><p>  <font><b>Method of building a factor model.</b></font>  <font>Factor analysis is based on the following linear model linking the original indicators</font> <font><img src="https://habrastorage.org/files/be9/ff0/427/be9ff042735843bca256ddab3a59f3ce.gif" width="21" height="21"></font>  <font>and factors</font> <font><img src="https://habrastorage.org/files/2c0/5a9/e78/2c05a9e78e714987ae79f57b90235efc.gif" width="24" height="21"></font>  <font>:</font> </p><br><p><img src="https://habrastorage.org/files/3c6/0bc/977/3c60bc977e024c95b026151fd07c3eab.gif" width="255" height="21"></p><br><p>  <font><i>m</i></font> <font>is the number of variables</font> </p><br><p>  <font><i>g</i></font> <font>- the number of factors</font> </p><br><p><img src="https://habrastorage.org/files/be9/ff0/427/be9ff042735843bca256ddab3a59f3ce.gif" width="21" height="21">  - <font>source variables</font> </p><br><p><img src="https://habrastorage.org/files/2c0/5a9/e78/2c05a9e78e714987ae79f57b90235efc.gif" width="24" height="21">  - <font>common factors</font> </p><br><p><img src="https://habrastorage.org/files/43a/38b/c01/43a38bc01f7e4d9f8691233f09a3776e.gif" width="22" height="21">  - <font>specific factors.</font> </p><br><p>  <font>In the matrix form, the linear model of factor analysis is written in the form:</font> </p><br><p><img src="https://habrastorage.org/files/75b/1e6/5dd/75b1e65dd008471899cae16986e9c907.gif" width="98" height="18">  <font>,</font> </p><br><p>  <font>Where</font> <font><img src="https://habrastorage.org/files/26d/a2f/34a/26da2f34aa224960a6e08f0fac2a92b2.gif" width="59" height="29"></font>  <font>- dimension matrix</font> <font><img src="https://habrastorage.org/files/679/6c3/32d/6796c332dbde44ff9be637dc45df122b.gif" width="39" height="18"></font>  <font>values ‚Äã‚Äãof</font> <font><i>m</i></font> <font>parameters for</font> <font><i>n</i></font> <font>objects,</font> </p><br><p><img src="https://habrastorage.org/files/d56/777/444/d5677744491445c1ad31cfa271458b3b.gif" width="60" height="29">  - <font>dimension matrix</font> <font><img src="https://habrastorage.org/files/cf5/231/44f/cf523144fedf4b98b76daa5f694cbf21.gif" width="38" height="18"></font>  <font>the values ‚Äã‚Äãof</font> <font><i>g</i></font> <font>factors for</font> <font><i>n</i></font> <font>objects,</font> </p><br><p><img src="https://habrastorage.org/files/fae/5c6/697/fae5c6697ab34e8ab9959e28f135db07.gif" width="62" height="29">  - <font>dimension matrix</font> <font><img src="https://habrastorage.org/files/12a/738/08f/12a73808f8054c34831ec8529dc199b3.gif" width="39" height="18"></font>  <font>values ‚Äã‚Äãof</font> <font><i>m</i></font> <font>specific factors for</font> <font><i>n</i></font> <font>objects,</font> </p><br><p><img src="https://habrastorage.org/files/b3f/8b7/849/b3f8b7849e74449caee9ed2d80855aeb.gif" width="60" height="29">  - <font>matrix of factorial dimension display</font> <font><img src="https://habrastorage.org/files/705/94b/cab/70594bcab9604ec9881c24ed618cdfa7.gif" width="41" height="18"></font>  <font>weights,</font> </p><br><p><img src="https://habrastorage.org/files/bd4/962/ca5/bd4962ca55904660ad24b01897e9814e.gif" width="65" height="29">  - <font>diagonal dimension matrix</font> <font><img src="https://habrastorage.org/files/279/45b/763/27945b7634f84699bf38fa1fe5d7a72b.gif" width="43" height="18"></font>  <font>weighting factors specific factors.</font> </p><br><p>  <font>In this method of building a factor model, the latent characteristics are assigned to the neurons of the hidden layer.</font>  <font>At the same time, the number of neurons in the hidden layer is assumed to be smaller than the number of neurons in the input layer for the implementation of factor compression of the input information.</font>  <font>To estimate the number of neurons in the hidden layer, you can apply the Kaiser rule of classical factor analysis.</font>  <font>The neurons of the input and output layer are assigned the initial characteristics of the objects of study.</font>  <font>When the transfer function of neurons is linear, this configuration of the neural network corresponds to the classical factor analysis (Fig. 2).</font> </p>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p><img src="https://habrastorage.org/files/007/733/50c/00773350c2b0421db8f7d04ab30c23ee.gif" width="304" height="143"></p><br><br><p>  <font><b>Fig.</b></font>  <font><b>2</b></font>  <font>Scheme of the neural network of classical factor analysis (the number of neurons in the input layer is equal to the number of neurons in the output layer, the number of neurons in the hidden layer is less than the number of neurons in the input layer).</font> </p><br><br><p>  <font>Using neural network training, weights of input connections of neurons of the hidden and output layer are calculated, which correspond to the elements of the inverse and direct factor display</font> <font><img src="https://habrastorage.org/files/860/ac7/4f4/860ac74f4dbf4bf4b8dbcf1f12caaa58.gif" width="25" height="21"></font>  <font>.</font>  <font>Weights of neurons are searched in the interval [-1, 1].</font>  <font>The filling of factors with the initial variables is determined using the values ‚Äã‚Äãof the elements of the factorial display and the selected threshold level of significance.</font> <font><img src="https://habrastorage.org/files/c22/9ce/8dc/c229ce8dc4c94fafa97ac17a7e136336.gif" width="71" height="19"></font>  <font>.</font>  <font>The variable</font> <font><i>i</i></font> <font>enters the factor</font> <font><i>j</i></font> <font>if</font> <font><img src="https://habrastorage.org/files/41c/9e2/c1c/41c9e2c1c75946228f65ff7d5e5d28e9.gif" width="90" height="22"></font>  <font>.</font> </p><br><br><p>  <font>To reveal the relationship between the factor model and the neural network, we use the formulas for obtaining the output signal of the neurons of the hidden layer.</font> </p><br><p>  <font>Denote the output of the</font> <font><i>jth</i></font> <font>neuron of the hidden layer</font> <font><img src="https://habrastorage.org/files/4e4/479/450/4e447945042e401ea54bfbc4dac8c3fc.gif" width="26" height="21"></font>  <font>.</font>  <font>The output signal of the</font> <font><i>i-</i></font> <font>th neuron of the input layer is denoted by</font> <font><img src="https://habrastorage.org/files/038/284/19b/03828419beb641b6baa59105c03a8a7e.gif" width="21" height="21"></font>  <font>.</font>  <font>As a transfer function, we will use a linear function.</font> <font><img src="https://habrastorage.org/files/31b/8e2/7ef/31b8e27ef8fc446f8d6e52d0e3a911ca.gif" width="98" height="19"></font>  <font>.</font> </p><br><p>  <font>As a result</font> </p><br><p><img src="https://habrastorage.org/files/124/359/346/12435934679f447ebcb5aed00e4595c7.gif" width="335" height="44">  <font>,</font> </p><br><p>  <font>where</font> <font><i>m</i></font> <font>is the number of neurons in the input layer;</font> </p><br><p><img src="https://habrastorage.org/files/673/b03/ee2/673b03ee2c1742f8b2b8659f2363ceb9.gif" width="46" height="24">  - the <font>connection between the</font> <font><i>i-</i></font> <font>th</font> <font>neuron of the</font> <font><i>s-</i></font> <font>th layer and the</font> <font><i>j-</i></font> <font>th</font> <font>neuron of the</font> <font><i>t</i></font> <font>-th layer,</font> </p><br><p><img src="https://habrastorage.org/files/876/e39/bae/876e39baecdc4bf8aa89f532f51b3de6.gif" width="30" height="24">  - <font>threshold level of the</font> <font><i>i</i></font> <font>-th neuron of the</font> <font><i>s</i></font> <font>-th layer.</font> </p><br><p>  <font>Similarly for the output layer:</font> </p><br><p><img src="https://habrastorage.org/files/b30/12d/f72/b3012df72dae4e1ba60a3ba4caa132b7.gif" width="337" height="44">  <font>,</font> </p><br><p>  <font>Where</font> <font><img src="https://habrastorage.org/files/782/369/393/7823693931b34e74bcca71dc7ee5faa2.gif" width="21" height="24"></font>  <font>- the output value of the</font> <font><i>i-</i></font> <font>th neuron of the output layer,</font> </p><br><p>  <font><i>g</i></font> <font>is the number of neurons in the hidden layer.</font> </p><br><p>  <font>The resulting linear relationship of variables corresponds to the classical model of factor analysis, in which factors are linear combinations of source variables.</font>  <font>The task of finding factor mappings and factor values ‚Äã‚Äãis reduced to the task of finding weights of connections and threshold levels of a neural network.</font>  <font>Since the factor mapping and factor values ‚Äã‚Äãare unknown, a network with an intermediate layer is required.</font>  <font>The network as a whole performs the identical transformation, i.e.</font>  <font>the output signal on the</font> <font><i>i-</i></font> <font>th neuron of the input layer is equal to the output signal of the</font> <font><i>i</i></font> <font>-th neuron of the output layer.</font>  <font>Separate parts of the network (input and output parts) correspond to direct and inverse factor mapping.</font> </p><br><p>  <font><i><b>Theorem</b></i></font> <font>.</font> </p><br><p>  <font>Let be</font> <font><img src="https://habrastorage.org/files/953/832/f0e/953832f0e84a450f802c75a035e5184a.gif" width="53" height="24"></font>  <font>and</font> <font><img src="https://habrastorage.org/files/c68/ee7/fcb/c68ee7fcbbaa480283138125b9954d28.gif" width="53" height="24"></font>  <font>- the weight of the input signals of the output and hidden layer of the neural network with a linear transfer function.</font>  <font>The number of neurons on the output layer is equal to the number of neurons in the input layer.</font>  <font>A neural network consists of an input, hidden and output layer and performs the same transformation for any input signal (the vector of input signals of the network is equal to the vector of output signals).</font> </p><br><p>  <font>Then the following equality holds</font> <font>:</font> </p><br><p><img src="https://habrastorage.org/files/d3c/25d/493/d3c25d4933cf42daa1813d2299286ebe.gif" width="138" height="41">  <font>,</font> </p><br><p>  <font>Where</font> <font><img src="https://habrastorage.org/files/d0c/83a/412/d0c83a412be24891a6055b567f3ac4cf.gif" width="54" height="24"></font>  <font>- the</font> <font>connection between the</font> <font><font><i>i</i></font> <font>-</font> <font>th neuron of the</font> <font><i>s</i></font> <font>-</font> <font>th layer and the</font> <font><i>j</i></font> <font>-</font> <font>th neuron of the</font> <font><i>t</i></font> <font>-</font> <font>th layer</font> <font>,</font></font> </p><p>  <font><font><i>g</i></font> <font>is the number of neurons in the hidden layer.</font></font> </p> <font><br></font> <p>  <font><font><i><b>Proof</b></i></font> <font>:</font></font> </p> <font><br></font> <p>  <font><font>Denote</font></font> <font><font><img src="https://habrastorage.org/files/627/8b8/1ab/6278b81ab2d34575be0cc056b62310e8.gif" width="26" height="21"></font></font>  <font><font>- the output signal of the</font> <font><i>i-</i></font> <font>th neuron of the</font> <font><i>k</i></font> <font>-th layer,</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/7ec/7dd/048/7ec7dd0484d440189ef1b79586e52dbe.gif" width="26" height="21"></font>  <font>- <font><i>i</i></font> <font>-th output signal of neurons of the first layer.</font></font> </p> <font><br></font> <p>  <font><font>To search for neural network weights, the following condition must be met:</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/240/d59/cee/240d59ceeba74f8a9e0abe86bb3c6d06.gif" width="53" height="21"></font>  <font><font>, the output signal on the</font> <font><i>i-</i></font> <font>th neuron of the input layer is equal to the output signal of the</font> <font><i>i</i></font> <font>-th neuron of the output layer.</font></font>  <font><font>From this condition follows an auxiliary condition:</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/416/fad/ca6/416fadca608b407a84bc03136ee332a2.gif" width="74" height="21"></font>  <font><font>, the change in the</font> <font><i>i-</i></font> <font>th input network signal is equal to the change in the</font> <font><i>i-</i></font> <font>th output signal.</font></font>  <font><font>The following equalities are true:</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/ac3/94a/2c6/ac394a2c61a7436aaa78eac3a7b0c509.gif" width="99" height="24"></font>  <font><font>,</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/c91/7d5/c5e/c917d5c5ecb74521ad0d9ac2fdf8abaf.gif" width="99" height="24"></font>  <font><font>where</font></font> <font><font><img src="https://habrastorage.org/files/7d6/0b3/eee/7d60b3eee79a4beebfbc0dbc7b6ebdef.gif" width="26" height="24"></font></font>  <font><font>and</font></font> <font><font><img src="https://habrastorage.org/files/a01/8a9/0f5/a018a90f5602436f9e42a7a740cfb97c.gif" width="26" height="24"></font></font>  <font><font>- input and output signal to change,</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/ee6/a1a/cea/ee6a1acead21475d8014a603276378f5.gif" width="53" height="24"></font>  <font><font>.</font></font> </p> <font><br></font> <p>  <font><font>Suppose that only the</font> <font><i>i</i></font> <font>-th input signal was</font> <font>changed</font> <font>.</font></font> </p> <font><br></font> <p>  <font><font>From these conditions follows:</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/373/d69/a82/373d69a8201b4dedabf40444a2804078.gif" width="461" height="44"></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/9d2/b83/a96/9d2b83a966594bbfadf33e51b74b738c.gif" width="835" height="80"></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/949/3a1/e4a/9493a1e4a7ab42b7b63dea39ba0ab941.gif" width="835" height="44"><img src="https://habrastorage.org/files/58f/094/f67/58f094f6706b4e4c9841a3407747572b.gif" width="192" height="41"></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/8a8/d46/1b7/8a8d461b75fe410599e2b465142455f6.gif" width="184" height="41"></font>  <font><font>;</font></font> </p> <font><br></font> <p>  <font><font>Insofar as</font></font> <font><font><img src="https://habrastorage.org/files/416/fad/ca6/416fadca608b407a84bc03136ee332a2.gif" width="74" height="21"></font></font>  <font><font>then</font></font> <font><font><img src="https://habrastorage.org/files/c90/3a3/3cd/c903a33cd516463f94fe7c045cade895.gif" width="136" height="41"></font></font>  <font><font>must be performed for all</font> <font><i>i</i></font> <font>, since the choice of the</font> <font><i>i-</i></font> <font>th input signal was arbitrary.</font></font> </p> <font><br><br></font> <p>  <font><font>The theorem is proved.</font></font> </p> <font><br></font> <p>  <font><font>Weights</font></font> <font><font><img src="https://habrastorage.org/files/953/832/f0e/953832f0e84a450f802c75a035e5184a.gif" width="53" height="24"></font></font>  <font><font>and</font></font> <font><font><img src="https://habrastorage.org/files/c68/ee7/fcb/c68ee7fcbbaa480283138125b9954d28.gif" width="53" height="24"></font></font>  <font><font>input signals of the output and hidden layer of the neural network with a linear transfer function correspond to the coefficients of the direct and inverse factor display.</font></font>  <font><font>The more accurate a neural network with factorial compression of information performs the identity transformation, the more accurately the equality of the theorem will be fulfilled, which corresponds to the fact that the composition of the direct and inverse factorial transformation must give the identity transformation.</font></font>  <font><font>We prove the corresponding theorem.</font></font> </p> <font><br><br><br><br></font> <p>  <font><font><i><b>Theorem</b></i></font> <font>.</font></font> </p> <font><br></font> <p>  <font><font><i>Let be</i></font></font> <font><font><img src="https://habrastorage.org/files/953/832/f0e/953832f0e84a450f802c75a035e5184a.gif" width="53" height="24"></font></font>  <font><font>and</font></font> <font><font><img src="https://habrastorage.org/files/c68/ee7/fcb/c68ee7fcbbaa480283138125b9954d28.gif" width="53" height="24"></font></font>  <font><font>- the weight of the input signals of the output and hidden layer of the neural network with a linear transfer function.</font></font>  <font><font>The number of neurons on the output layer is equal to the number of neurons in the input layer.</font></font>  <font><font>The neural network consists of input, hidden and output layer.</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/2f6/0af/76d/2f60af76da6b49f8b0239927d62c036c.gif" width="28" height="18"></font>  <font>- the <font>average discrepancy of the signal between the input and output of the network per one input (output) neuron,</font></font> </p> <font><br></font> <p> <font><img src="https://habrastorage.org/files/df2/d0d/671/df2d0d67148940a9bb1a61239ed0f04a.gif" width="31" height="18"></font>  <font>- <font>discrepancy of equality</font></font> <font><font><img src="https://habrastorage.org/files/d3c/25d/493/d3c25d4933cf42daa1813d2299286ebe.gif" width="138" height="41"></font></font>  <font><font>i.e.</font></font> <font><font><img src="https://habrastorage.org/files/b33/7c6/5b2/b337c65b2bd84a6d90be8ba2f92df2ba.gif" width="171" height="41"></font></font>  <font><font>,</font></font> </p> <font><br></font> <p>  <font><font>Where</font></font> <font><font><img src="https://habrastorage.org/files/d0c/83a/412/d0c83a412be24891a6055b567f3ac4cf.gif" width="54" height="24"></font></font>  <font><font>- the</font> <font>connection between the</font> <font><font><i>i</i></font> <font>-</font> <font>th neuron of the</font> <font><i>s</i></font> <font>-</font> <font>th layer and the</font> <font><i>j</i></font> <font>-</font> <font>th neuron of the</font> <font><i>t</i></font> <font>-</font> <font>th layer</font> <font>,</font></font></font> </p><p>  <font><font><font><i>g</i></font> <font>is the number of neurons in the hidden layer.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Then the smaller</font></font></font> <font><font><font><img src="https://habrastorage.org/files/2f6/0af/76d/2f60af76da6b49f8b0239927d62c036c.gif" width="28" height="18"></font></font></font>  <font><font><font>the less</font></font></font> <font><font><font><img src="https://habrastorage.org/files/df2/d0d/671/df2d0d67148940a9bb1a61239ed0f04a.gif" width="31" height="18"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font><i><b>Proof</b></i></font> <font>:</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>In the previous theorem, the following equality of</font> <font>the signal value by one variable at the output is</font> <font>proved</font></font></font> <font><font><font><img src="https://habrastorage.org/files/b09/087/a81/b09087a81c3d4079a5db5e2c0b4d9908.gif" width="26" height="21"></font></font></font>  <font><font><font>from the increment of the signal on the same variable at the input</font></font></font> <font><font><font><img src="https://habrastorage.org/files/879/a1e/251/879a1e251ca34e7dabbb57d41165a078.gif" width="36" height="21"></font></font></font>  <font><font><font>:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/74a/fab/542/74afab5422ad4183a5dbd0e10ee30520.gif" width="209" height="41"></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Insofar as</font></font></font> <font><font><img src="https://habrastorage.org/files/aef/6cc/b0f/aef6ccb0f33a46dd97d446c1ed738eb1.gif" width="469" height="24"></font></font>  <font><font><font>,</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Where</font></font></font> <font><font><font><img src="https://habrastorage.org/files/e17/73c/8c2/e1773c8c253c461c93ed17a3f5960d6f.gif" width="34" height="20"></font></font></font>  <font><font><font>- the initial discrepancy of signals between the input and output of the network before the change of the</font> <font><i>i-</i></font> <font>th input signal,</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>that</font></font></font> <font><font><font><img src="https://habrastorage.org/files/fc0/681/cb4/fc0681cb4e5742d9a1e03d7f9ceeb4d6.gif" width="409" height="48"></font></font></font>  <font><font><font>.</font></font></font>  <font><font><font>This means a monotonous relationship between</font></font></font> <font><font><font><img src="https://habrastorage.org/files/df2/d0d/671/df2d0d67148940a9bb1a61239ed0f04a.gif" width="31" height="18"></font></font></font>  <font><font><font>and</font></font></font> <font><font><font><img src="https://habrastorage.org/files/2f6/0af/76d/2f60af76da6b49f8b0239927d62c036c.gif" width="28" height="18"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>The theorem is proved.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>For the construction of nonlinear principal components, the antisymmetric sigmoidal function can be chosen as the transfer function:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/56f/04b/9dd/56f04b9dd8394a4199c6ced94dc88d3e.gif" width="175" height="38"></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>In any case, regardless of the type of transfer function, to obtain an interpretable factor mapping, an additional term is introduced into the overall objective function of squared residuals, which meets the criterion of the varimax of the classical factor analysis - it maximizes the dispersion of variable loads attributable to all factors:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/3a9/de6/3eb/3a9de63eb6334e5690eaaad828b674cc.gif" width="287" height="54"></font></font>  <font><font><font>,</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/aa0/4e8/420/aa04e84204e04831a4757e397129fd56.gif" width="117" height="52"></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Accounting for the varimax criterion leads to the appearance of additional terms when the weights of the neural network change on the output layer:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/6c1/9bb/0c7/6c19bb0c75f14e4c85a149dcba4c3d09.gif" width="383" height="72"></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Another option to obtain an interpretable factor map can be the use of a special interpretability criterion [12].</font></font></font>  <font><font><font>This criterion is that only one factor load for a fixed variable should be close to 1, while the rest should be close to 0. It is proposed to implement the empirical interpretation criterion as follows: among the factor loads for a fixed variable, the maximum modulo .</font></font></font>  <font><font><font>All factor loads other than the maximum decrease in absolute value by</font> <font><i>Œ≥</i></font> <font>, while the maximum</font> <font>load</font> <font>increases by</font> <font><i>Œ≥</i></font> <font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>A similar account of the main conditions of the factor analysis for the sum of the squares of the variable factor loads attributable to all factors</font></font></font> <font><font><font><img src="https://habrastorage.org/files/14b/fe7/abd/14bfe7abdf1e49c88fd257ae8b1d960d.gif" width="115" height="50"></font></font></font>  <font><font><font>and search for factor loads in the allowable interval</font></font></font> <font><font><font><img src="https://habrastorage.org/files/359/963/41e/35996341ecf546b78c24a4327ff4a4df.gif" width="113" height="24"></font></font></font>  <font><font><font>leads to amendments to changes in the weights of the neural network for the output layer.</font></font></font>  <font><font><font>In case of violation of these conditions, it is proposed to use the penalty function.</font></font></font> <font><font><font><img src="https://habrastorage.org/files/39a/e37/333/39ae37333af24e088186ebc1feb9b7b1.gif" width="134" height="51"></font></font></font>  <font><font><font>corresponding to minimizing the weights of neurons.</font></font></font>  <font><font><font>Then</font></font></font> <font><font><font><img src="https://habrastorage.org/files/601/f95/043/601f95043f624519ae37e97051a8b840.gif" width="206" height="44"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>To standardize the input values ‚Äã‚Äãof the neural network, a linear transformation is used:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/7e7/d3e/a47/7e7d3ea47e6a474cbee4ad602d5063c6.gif" width="75" height="18"></font></font>  <font><font><font>which translates the range of the original</font> <font><i>x</i></font> <font>values</font> <font>from [</font> <font><i>min</i></font> <font>,</font> <font><i>max</i></font> <font>] to [</font> <font>s</font> <font>,</font> <font>t</font> <font>].</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Then</font></font></font> <font><font><font><img src="https://habrastorage.org/files/5eb/eae/e2c/5ebeaee2cc0c4d38a5603b0c159cf1c4.gif" width="97" height="38"></font></font></font>  <font><font><font>,</font></font></font> <font><font><font><img src="https://habrastorage.org/files/53e/0dc/cc7/53e0dccc7df74844af6f41eed838bd60.gif" width="115" height="38"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>For the inverse transformation of the output values ‚Äã‚Äãof the neural network from the range [</font> <font>s</font> <font>, t] to [</font> <font><i>min</i></font> <font>,</font> <font><i>max</i></font> <font>], the transformation is used</font></font></font> <font><font><font><img src="https://habrastorage.org/files/83b/791/c5f/83b791c5fd8e4ff09f6590d2557fd492.gif" width="77" height="38"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>As an interval [</font> <font>s</font> <font>,</font> <font>t</font> <font>] for antisymmetric sigmoidal function</font></font></font> <font><font><font><img src="https://habrastorage.org/files/56f/04b/9dd/56f04b9dd8394a4199c6ced94dc88d3e.gif" width="175" height="38"></font></font></font>  <font><font><font>interval can be selected [-0.85, 0.85].</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>Then</font></font></font> <font><font><font><img src="https://habrastorage.org/files/ae6/61a/1ba/ae661a1baf944354a52355ea48f460c3.gif" width="97" height="38"></font></font></font>  <font><font><font>,</font></font></font> <font><font><font><img src="https://habrastorage.org/files/679/d08/287/679d0828700b40459cdc9a040b9ed5f2.gif" width="70" height="18"></font></font></font>  <font><font><font>.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font><b>Numerical experiment.</b></font></font></font>  <font><font><font>The initial parameters were taken 15 biophysical indicators for 131 persons with hypertension at the initial stage:</font></font></font> </p> <font><font><br></font></font> <ol><li><p>  <font><font><font><i>weight,</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>body mass index (BMI)</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>respiratory rate (RR),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>segmented neutrophils (C),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>lymphocytes (L),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>of course, the systolic size of the left ventricle (DAC),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>of course, the systolic volume of the left ventricle (CSR),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>of course, the diastolic size of the left ventricle (MDC),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>of course, the diastolic volume of the left ventricle (CDW),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>stroke volume (PP),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>cardiac output (MOS),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>total peripheral vascular resistance (OPS),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>Hildebrandt Index (HI)</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>left ventricular ejection fraction (EF),</i></font></font></font> </p> <font><font><br></font></font> </li><li><p>  <font><font><font><i>left ventricular shortening fraction (FU).</i></font></font></font> </p> <font><font><br></font></font> </li></ol> <font><font><br></font></font> <p>  <font><font><font>When teaching a neural network on the initial stage arterial hypertension data containing 131 patterns and 15 variables with an antisymmetric sigmoidal transfer function and 5 neurons on a hidden layer, the error per variable was no more than 10% of the range of values ‚Äã‚Äãof the variable in the sample.</font></font></font>  <font><font><font>The graph of the convergence of the learning process is presented in Figure 3. Under the iteration of learning there is one epoch of learning, when the network is substituted for the entire set of learning patterns.</font></font></font>  <font><font><font>The total error on the training sample is understood as the sum of errors for all patterns of the training set on one training iteration.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>To test the effectiveness of neural network training, the initial set of input-output values ‚Äã‚Äãwas divided into 2 independent subsets: training and test.</font></font></font>  <font><font><font>Training was conducted on the training set, and verification - on the test.</font></font></font>  <font><font><font>The neural network error on the test set is an indicator of how accurately the neural network learned.</font></font></font>  <font><font><font>The relative volume of the test set was estimated by the formula [7]:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/452/750/c65/452750c659d4428fb9ee8c99e33f720b.gif" width="116" height="41"></font></font>  <font><font><font>,</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>where</font> <font><i>W</i></font> <font><i>is the</i></font> <font>number of input parameters.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>When</font> <font><i>W</i></font> <font>= 15,</font></font></font> <font><font><font><img src="https://habrastorage.org/files/d54/eed/7e7/d54eed7e714749e6a8dc0c3b11bddbc6.gif" width="74" height="21"></font></font></font>  <font><font><font>.</font></font></font>  <font><font><font>With 131 patterns, there are 20 patterns per test set.</font></font></font> </p> <font><font><br></font></font> <p>  <font><font><font>The graph of the change in the total error for the test set patterns during verification at each epoch of the learning process is presented in Figure 4. The total error on the test set is the sum of errors for the 20 test set patterns during the verification process at each learning epoch, i.e.</font></font></font>  <font><font><font>when a full set of training set patterns was used for training, but the test set was not involved in the training.</font></font></font>  <font><font><font>At each epoch, the relative error for the test set is larger than the relative error for the training set.</font></font></font>  <font><font><font>In the limit, when the error for the training set begins to converge, the effect of retraining is possible, i.e.</font></font></font>  <font><font><font>the value of the error during verification on the test set does not begin to decrease, but this is due to the fact that intermediate points between points of the training set in a multidimensional space are poorly approximated by a recoverable dependence of the neural network.</font></font></font>  <font><font><font>Figure 4 and its graph of error variation on the test set shows that there is no retraining effect and the amount of the training set is sufficient for the number of initial indicators equal to 15. The graphs show only minor error fluctuations during further training on the training set in the process of error convergence for the training set.</font></font></font> </p> <font><font><br><br></font></font> <p>  <font><font><font>From the graph it is clear that the effect of overtraining is not observed, further training leads only to a small fluctuation of the total error on the test set.</font></font></font> </p> <font><font><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/4e5/50c/168/4e550c16832d43979d760b6dae2594a9.gif" width="425" height="254"></font></font> </p> <font><font><br></font></font> <p>  <font><font><font><b>Fig.</b></font></font></font>  <font><font><font><b>3</b></font></font></font>  <font><font><font>The graph of the change in the total error on the training set (131 patterns, 15 variables).</font></font></font> </p> <font><font><br><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/6dd/8eb/6ac/6dd8eb6aca0a4b04837c1b0aebf1dc81.gif" width="428" height="262"></font></font> </p> <font><font><br></font></font> <p>  <font><font><font><b>Fig.</b></font></font></font>  <font><font><font><b>4</b></font></font></font>  <font><font><font>Graph of the total error on the test set (20 patterns, 15 variables).</font></font></font> </p> <font><font><br><br></font></font> <p> <font><font><img src="https://habrastorage.org/files/7bd/706/ddc/7bd706ddc68743e4939e4fea9c1f2ab3.gif" width="489" height="249"></font></font> </p> <font><font><br></font></font> <p>  <font><font><font><b>Fig.</b></font></font></font>  <font><font><font><b>5</b></font></font></font>  <font><font><font>Eigenvalues ‚Äã‚Äãof source variables.</font></font></font> </p> <font><font><br><br></font></font> <p>  <font><font><font>The average errors per 15 variables for one pattern on the training and test sets are 1.28 and 1.54.</font></font></font>  <font><font><font>With the initial range of parameters [-0.85, 0.85], the error per one variable for the training and test sets is 5 and 6%.</font></font></font>  <font><font><font>For example, for the ‚Äú</font> <font><i>weight</i></font> <font>‚Äù</font> <font>parameter</font> <font>, the largest weight was equal to 116</font> <font><i>kg</i></font> <font>, the smallest 45</font> <font><i>kg</i></font> <font>, with a range of 71</font> <font><i>kg</i></font> <font>, an error of 6% corresponds to 4.26</font> <font><i>kg</i></font> <font>.</font></font></font>  <font><font><font>This indicates a good ability of the neural network to generalize.</font></font></font>  <font><font><font>Since an error of 6% for 15 input parameters and 131 examples for learning is less than a theoretical estimate of an error of 10% when 15 * 10 examples are required for training, we can talk about the sufficiency of the training set.</font></font></font>  <font><font><font>It is known that between a really sufficient size of the training set and theoretical estimates there can be a large gap [7].</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>          ,               1.         5.          ,                        .               ,       ,         .           .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font> ,     ,     ,       ¬´¬ª       ,            .       1, 2, 3.</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>           10%             .       [</font> <font>11</font> <font>].</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font><b> 1.</b></font> <font>  ¬´¬ª</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>(   + ¬´¬ª )</font></font></font> </p> <font><font><br><img src="https://habrastorage.org/files/52e/651/622/52e6516222154e1599597755c16f6300.png"><br></font></font> <p> <font><font><font><b> 2.</b></font> <font>      (¬´¬ª )</font></font></font> </p> <font><font><br><img src="https://habrastorage.org/files/e2d/a95/650/e2da9565075a43018f8801ba0f3a2c96.png"><br><br></font></font> <p> <font><font><font><b> 3.</b></font> <font>      ( )</font></font></font> </p> <font><font><br><img src="https://habrastorage.org/files/a06/715/56d/a0671556ddc84bf18e3de9454148250b.png"><br><br></font></font> <p> <font><font><font>     ,  ¬´¬ª .       :      0,      1.        ,   ,   ,         .                 ,     ,            .                ,           .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>      ,      (</font> <font>4</font> <font>).</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font><b></b></font> <font><b>4</b></font> <font><b>.</b></font> <font> ,     </font></font></font> </p> <font><font><br><img src="https://habrastorage.org/files/ab0/e34/c02/ab0e34c022464d739efaaef694335d2f.png"><br></font></font> <p> <font><font><font>   </font></font></font> </p> <font><font><br><br></font></font> <p> <font><font><font>      ,     .     </font> <font>[</font> <font>1</font> <font>3, 14]:</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>1.       ,  ,     .  , ,     .  ,              ,            . ,   ,    .   ,    .          .           ,     , , , .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>2.                    (, )   .      .  ,           .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>3. ,      .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>4. ,       ,       .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>5.  ,      ,       .           .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>     . ,            3.  4  5   ,            .       .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>   [13]      .       ,    ,      . ,           ,   .       (- )            .       ,      ,      .       ,   ,    ,       .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font><b>.</b></font> <font>               .            .      ,          :   ,      .      ,         .</font></font></font> </p> <font><font><br></font></font> <p> <font><font><font>            .    ,        ,     ,       ¬´¬ª       .</font></font></font> </p> <font><font><br></font></font> <h1> <font><font></font></font> </h1> <font><font><br></font></font> <ol><li><p> <font><font><font> .</font> <font><i> .</i></font> <font>.  . . . ; . . . . ‚Äî .: , 1980.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font> .</font> <font><i>  .</i></font> <font>//     . .: , 1994.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Hornik K., Stinchcombe M., White H.</font> <font><i>Multilayer Feedforward Networks are Universal Approximators.</i></font> <font>// Neural Networks, 1989, v.2, N.5.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Cybenko G.</font> <font><i>Approximation by Superpositions of a Sigmoidal Function.</i></font> <font>// Mathematics of Control, Signals and Systems, 1989, v.2.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Funahashi K.</font> <font><i>On the Approximate Realization of Continuous Mappings by Neural Networks.</i></font> <font>// Neural Networks.</font> <font>1989,</font> <font>v</font> <font>.2,</font> <font>N</font> <font>.3, 4.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font> ..</font> <font><i>       .</i></font> <font>//     / . . . ‚Äì , 1998. ‚Äì .1, ‚Ññ1. ‚Äì . 11-24.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font></font> <font>C</font> <font>.</font> <font><i> :  .</i></font> <font>.  . . . , . . . 2- ., . ‚Äî .:   , 2008, 1103 .</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font> .</font> <font><i>    </i></font> <font>. .:   , 2002, 344 .</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Gorban A., Kegl B., Wunsch D., Zinovyev A.,</font> <font><i>Principal Manifolds for Data Visualisation and Dimension Reduction</i></font> <font>// Springer, Berlin ‚Äì Heidelberg ‚Äì New York, 2007.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Kruger U., Antory D., Hahn J., Irwin GW, McCullough G.</font> <font><i>Introduction of a nonlinearity measure for principal component models.</i></font> <font>// Computers &amp; Chemical Engineering, 29 (11-12), 2355‚Äì2362 (2005)</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font>Jain AK, Mao J., Mohiuddin KM</font> <font><i>Artificial Neural Networks: A Tutorial.</i></font> <font>Computer, March, 1996, pp. 31-44.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font> ..,  ..</font> <font><i>   .</i></font> <font>//    . 2015. ‚Ññ 2. . 75-84.</font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Goltyapin V.V., Shoovin V.A. </font></font></font> <font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Oblique factorial model of first stage arterial hypertension. </font></font></i></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">// Bulletin of Omsk University. </font><font style="vertical-align: inherit;">2010. No. 4. </font></font></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">120-128.</font></font></font></font></font> </p> <font><font><br></font></font> </li><li><p> <font><font><font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Chovin V.A. </font></font></font> <font><i><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Confirmatory factor model of arterial hypertension. </font></font></i></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">// Computer research and modeling. </font><font style="vertical-align: inherit;">2012. V. 4. No. 4. </font></font></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">c </font></font></font> <font><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">. </font><font style="vertical-align: inherit;">885-894.</font></font></font></font></font> </p> <font><font><br></font></font> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/312420/">https://habr.com/ru/post/312420/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../312408/index.html">Tame iron Lenovo and wonder nuances XClarity</a></li>
<li><a href="../312412/index.html">Why do we need the project "Design of state systems"</a></li>
<li><a href="../312414/index.html">It's time for business to take selfies seriously.</a></li>
<li><a href="../312416/index.html">Innovations are not introduced there</a></li>
<li><a href="../312418/index.html">Development for Sailfish OS: FLUX architecture in QML by the example of an application for memorizing literary terms</a></li>
<li><a href="../312422/index.html">Create good tables</a></li>
<li><a href="../312424/index.html">Multi-Tech Base Stations let you deploy a LoRaWAN network in a couple of clicks</a></li>
<li><a href="../312426/index.html">Software-defined modular collocation - why is it needed?</a></li>
<li><a href="../312428/index.html">Microsoft fixed vulnerabilities in their products</a></li>
<li><a href="../312430/index.html">How to write less code for MR, or why the world needs another query language? History of Yandex Query Language</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>