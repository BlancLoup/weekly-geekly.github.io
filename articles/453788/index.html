<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neural networks prefer textures and how to deal with it.</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Recently, several articles criticized ImageNet, perhaps the most well-known set of images used to train neural networks. 


 In the first article, App...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neural networks prefer textures and how to deal with it.</h1><div class="post__text post__text-html js-mediator-article"><p><img src="https://habrastorage.org/webt/59/ck/a6/59cka6w8edkhs0-jitjtc_dicg8.png"></p><br><p>  Recently, several articles criticized ImageNet, perhaps the most well-known set of images used to train neural networks. </p><br><p>  In the first article, <a href="https://arxiv.org/pdf/1904.00760.pdf">Approximating CNNs with bag-of-local features models works surprisingly well on ImageNet, the</a> authors take a model similar to bag-of-words and use fragments from the image as the ‚Äúwords‚Äù.  These fragments can be up to 9x9 pixels.  And at the same time, on such a model, where any information about the spatial arrangement of these fragments is completely absent, the authors obtain accuracy from 70 to 86% (for example, the accuracy of the usual ResNet-50 is ~ 93%). </p><br><p>  In the second article, <a href="https://arxiv.org/pdf/1811.12231.pdf">ImageNet-trained CNNs are biased along texture, the</a> authors come to the conclusion that the ImageNet data set itself and how people and neural networks perceive images are to blame, and suggest using the new dataset - Stylized-ImageNet. </p><br><p>  More details about what people see in pictures, and what neural networks see <a name="habracut"></a></p><br><h3 id="imagenet">  ImageNet </h3><br><p>  The ImageNet data set was created in 2006 by the efforts of Professor Fei-Fei Li and continues to evolve to this day.  At the moment, it contains about 14 million images belonging to more than 20 thousand different categories. </p><br><p>  Since 2010, a subset of this data set, known as ImageNet 1K with ~ 1 million images and a thousand classes, has been used in the ILSVRC (ImageNet Large Scale Visual Recognition Challenge) competition.  At this competition in 2012, AlexNet ‚Äúshot‚Äù, a convolutional neural network that reached top-1 accuracy of 60% and top-5 at 80%. <br>  It is on this subset of dataset that people from the academic environment are measured by <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">their SOTA</a> when they offer new network architectures. </p><br><p>  A little about the learning process on this dataset.  It will be about the training protocol on ImageNet in the academic environment.  That is, when we are shown in the article the results of some SE block, ResNeXt or DenseNet network, the process looks like this: the network is trained for 90 epochs, the learning rate decreases by 30 and 60 epoch, each time 10 times, as an optimizer A normal SGD with a small weight decay is selected, only RandomCrop and HorizontalFlip are used from augmentations, the image is usually resized to 224x224 pixels. </p><br><p>  Here is an example <a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py">pytorch script</a> for learning on ImageNet. </p><br><h3 id="bagnet">  BagNet </h3><br><p>  Returning to the previously mentioned articles.  In the first of these, the authors wanted a model that would be easier to interpret than ordinary deep networks.  Inspired by the idea of ‚Äã‚Äãbag-of-feature models, they create their own family of models - BagNets.  Using the usual ResNet-50 network as a basis. </p><br><p>  Replacing some 3x3 convolutions in 1x1 in ResNet-50, they achieve that the <a href="https://en.wikipedia.org/wiki/Receptive_field">receptive field of</a> neurons on the last convolutional layer is significantly reduced, up to 9x9 pixels.  Thus, they limit the information available to a single neuron to a very small fragment of the entire image - a patch of several pixels.  It should be noted that for the untouched ResNet-50, the size of the receptive field <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/receptive_field">is more than 400 pixels,</a> which completely covers the image, which is usually resized to 224x224 pixels. </p><br><p>  This patch is the <strong>maximum</strong> fragment of the image from which the model could extract spatial data.  At the end of the model, all the data was simply summed up and the model could not in any way know where each patch is relative to the other patches. <br>  In total, three variants of networks with receptive field 9x9, 17x17 and 33x33 were tested.  And, despite the complete lack of spatial information, such models were able to achieve good accuracy in the classification on ImageNet.  Top-5 accuracy for patches 9x9 was 70%, for 17x17 - 80%, for 33x33 - 86%.  For comparison, the ResNet-50 top-5 accuracy is approximately 93%. </p><br><p><img src="https://habrastorage.org/webt/hq/ld/s3/hqlds3eqhc0jzjusdbrgmzvwxua.png" alt="BagNet"></p><br><p>  The structure of the model is shown in the figure above.  Each patch of qxqx3 pixels cut from the image is converted into a vector 2048 in length by the network. Then this vector is fed to the input of the linear classifier, which gives scores for each of the 1000 classes.  By collecting the scores of each patch in a 2d array, you can get a heatmap for each class and each pixel of the original image.  The final scores for the image were obtained by summing the heatmap of each class. </p><br><p>  Examples of heatmaps for some classes: </p><br><p><img src="https://habrastorage.org/webt/d7/wb/5f/d7wb5fbpcbugnrcma1qrnn9vyc0.png" alt="HeatMaps"><br>  As you can see, the biggest contribution to the benefit of one or another class is made by patches located at the edges of the objects.  Patches from the background are almost ignored.  So far everything is going fine. </p><br><p>  Look at the most informative patches: </p><br><p><img src="https://habrastorage.org/webt/vr/x0/er/vrx0erm0vpyodspgnvhejnowzjy.png" alt="InformativePatches"></p><br><p>  For example, the authors took four classes.  For each of them, we chose 2x7 most significant patches (that is, patches where the score of this class was the highest).  The top row of 7 patches is taken from the images of the corresponding class only, the bottom one - from the entire image sample. </p><br><p>  With these pictures you can see remarkable.  For example, for the tench class (tench, fish) fingers are a characteristic feature.  Yes, ordinary human fingers on a green background.  And all because almost all the images with this class there is a fisherman who, in fact, holds this fish in his hands, bragging about the trophy. </p><br><div class="spoiler">  <b class="spoiler_title">Examples from ImageNet</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/7z/me/lk/7zmelkx_dofjucas3n-ejexol9o.png"></p></div></div><br><p>  For laptop computers, character keys are the letter keys.  Typewriter keys are also counted in this class. </p><br><p>  For a book cover a characteristic feature are letters on a colored background.  Let it even be the inscription on a T-shirt or on the package. </p><br><p>  It would seem that this problem should not bother us.  Since it is inherent only in a narrow class of networks with a very limited receptive field.  But then, the authors considered a correlation between the logits (network outputs before the final softmax) assigned to each BagNet class with different receptive field, and the logits from VGG-16, which has a sufficiently large receptive field.  And they found it quite high. </p><br><div class="spoiler">  <b class="spoiler_title">Correlation between BagNets and VGG-16</b> <div class="spoiler_text"><p><img src="https://habrastorage.org/webt/9d/m2/lb/9dm2lbnzbzbzxb9gdnp_rjb02z4.png" alt="Logits"></p></div></div><br><p>  The authors wondered if BagNet contained any hints about how other networks make decisions. </p><br><p>  For one of the tests, they used such a technique as Image Scrambling.  Which consisted in using <a href="https://arxiv.org/abs/1505.07376">a texture generator based on gram matrices</a> to create a picture where the textures are saved, but spatial information is missing. </p><br><p><img src="https://habrastorage.org/webt/qn/mf/0t/qnmf0t-0eegse5rrjs6gze13r94.png" alt="ScrambledImages"></p><br><p> VGG-16, trained in ordinary full-fledged pictures, coped with such scrambled pictures pretty well.  Its top-5 accuracy dropped from 90% to 80%.  That is, even the networks with rather large receptive field still preferred to memorize textures and ignore spatial information.  Therefore, their accuracy and did not fall heavily on scrambled images. </p><br><p>  The authors conducted a series of experiments where they compared which parts of the images are most significant for BagNet and other networks (VGG-16, ResNet-50, ResNet-152 and DenseNet-169).  Everything hinted that the rest of the networks, like BagNet, when making decisions, rely on small fragments of images and make approximately the same mistakes.  This was especially noticeable for not very deep networks like VGG. </p><br><p>  This tendency of networks to make decisions based on textures, unlike us - people who prefer the form (see figure below), prompted the authors of the second article to create a new data set based on ImageNet. </p><br><h3 id="stylized-imagenet">  Stylized ImageNet </h3><br><p>  First of all, the authors of the article using style transfer created a set of images where the form (spatial data) and textures in one image contradicted each other.  And compared the results of people and deep convolutional networks of different architectures on a synthesized data set of 16 classes. </p><br><p><img src="https://habrastorage.org/webt/st/zm/kr/stzmkr4kpjew5lfwqv-gpqmnutu.png" alt="CatVsElephant"></p><br><p>  In the extreme right figure, people see a cat, a network - an elephant. </p><br><p><img src="https://habrastorage.org/webt/rg/4m/ax/rg4max4fx9sjww7zgclnqbcohjw.png"></p><br><p>  Comparison of results of people and neural networks. </p><br><p>  As you can see, when referring an object to a particular class, people relied on the shape of objects, neural networks - on textures.  In the picture above, people saw a cat, a network - an elephant. </p><br><blockquote>  Yes, here you can find fault with the fact that the networks are also somewhat right, and this, for example, could be an elephant, photographed from close range, with a tattoo of a beloved cat.  But the fact that the networks, when making decisions, do not behave like people, the authors considered the problem and began to search for ways to solve it. </blockquote><p>  As mentioned above, relying only on textures, the network is able to achieve a good result at 86% top-5 accuracy.  And this is not about several classes where textures help to properly classify images, but about most classes. </p><br><p>  The problem is precisely in ImageNet itself, since it will be shown later that the network is able to learn the shape, but does not, because there are enough textures on this data set, and the neurons responsible for the textures are on shallow layers, which are much easier to train. </p><br><p>  Using this time a somewhat different mechanism <a href="https://arxiv.org/abs/1703.06868">AdaIN fast style transfer, the</a> authors created a new data set - Stylized ImageNet.  The shape of the objects was taken from ImageNet, and a set of textures from this <a href="https://www.kaggle.com/c/painter-by-numbers/">competition on Kaggle</a> .  The script for generation is available <a href="https://github.com/rgeirhos/Stylized-ImageNet">by reference</a> . </p><br><p><img src="https://habrastorage.org/webt/tq/lr/lb/tqlrlbyodw62dy8labuioctg7c0.png"></p><br><p>  Further, for brevity, ImageNet will be denoted as <strong>IN</strong> , Stylized ImageNet as <strong>SIN</strong> . </p><br><p>  The authors took ResNet-50 and three BagNet with different receptive field and trained on a separate model for each of the data sets. </p><br><p>  And that's what they did: </p><br><p><img src="https://habrastorage.org/webt/rm/bm/k7/rmbmk7uvvm9gigwy5xkh6fvfghm.png"></p><br><p>  What we see here.  ResNet-50, trained on IN, is completely incapacitated on SIN.  What partly confirms the fact that when training for IN, the network overrides textures and ignores the shape of objects.  At the same time, ResNet-50 trained in SIN does an excellent job with both SIN and IN.  That is, if you deprive it of a simple path, the network goes along the difficult path - it teaches the form of objects. <br>  BagNet is finally beginning to behave as expected, especially on small patches, as it has nothing to catch on - textural information is simply not available in the SIN. </p><br><p>  In those sixteen classes mentioned earlier, ResNet-50, trained in SIN, began to give answers more similar to those given by people: </p><br><p><img src="https://habrastorage.org/webt/wd/ft/l4/wdftl4dzwh-2st0brezxk513w4c.png"></p><br><p>  In addition to simple training ResNet-50 on SIN, the authors tried to train the network on a mixed set of SIN and IN, including fine-tuning separately on pure IN. </p><br><p><img src="https://habrastorage.org/webt/va/xy/jp/vaxyjpywpya8-vyrt548ikz52wg.png"></p><br><p>  As you can see, when using SIN + IN for training, the results improved not only on the main task - image classification on ImageNet, but also on the task of detecting objects on the PASCAL VOC 2007 data set. </p><br><p>  In addition, the networks trained on the SIN have become more resistant to different noise in the data. </p><br><h3 id="zaklyuchenie">  Conclusion </h3><br><p>  Even now, in 2019, after seven years past with AlexNet's success, when neural networks are widely used in computer vision, when ImageNet 1K de facto became the standard for evaluating the performance of models in an academic environment, the mechanism of how neural networks make decisions is not quite clear. .  And how does this affect the data sets on which these networks were trained. </p><br><p>  The authors of the first article attempted to shed light on how such decisions are made in networks with a bag-of-features-based architecture with a limited receptive field that is easier to interpret.  And, comparing the answers from BagNet and the usual deep neural networks, they came to the conclusion that the decision-making processes in them are quite similar. </p><br><p>  The authors of the second article compared the way people and neural networks perceive images in which the form and textures contradict each other.  And proposed to reduce the differences in perception to use a new data set - Stylized ImageNet.  Having received as a bonus a gain in accuracy of classification on ImageNet and detection on third-party data sets. </p><br><p>  The main conclusion can be drawn as follows: the networks that study in the pictures, having the ability to memorize the higher-level spatial properties of objects, prefer an easier way to achieve the goal ‚Äî overfit to textures.  If the data set on which they train allows it. </p><br><p>  In addition to academic interest, the problem of overfitting on textures is also important for all of us who use pre-trained models for transfer learning in their tasks. <br>  An important consequence for us of all this is that you shouldn‚Äôt trust the model models weights that were pre-trained on ImageNet, since for most of them fairly simple augmentations were used that in no way contribute to getting rid of overfitting.  And it is better, if there are opportunities, to have models, trained with more serious augmentations, or with Stylized ImageNet + ImageNet in the stash.  To always have the opportunity to compare which of them is better suited for our current task. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/453788/">https://habr.com/ru/post/453788/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../453778/index.html">How we created an online bank for business. Part One: Rebranding</a></li>
<li><a href="../45378/index.html">Pre-project documentation: what is it and why is it so important?</a></li>
<li><a href="../453780/index.html">How to choose a Grandstream SIP phone - in general and in particular?</a></li>
<li><a href="../453782/index.html">Infinite UIScrollView</a></li>
<li><a href="../453784/index.html">As a DevOps specialist, automating</a></li>
<li><a href="../45379/index.html">Inflation in home networks.</a></li>
<li><a href="../453790/index.html">‚ÄúCustomer left is forever?‚Äù How to count customer churn in SaaS and what‚Äôs wrong with basic metrics</a></li>
<li><a href="../453792/index.html">Recommender systems: ideas, approaches, tasks</a></li>
<li><a href="../453794/index.html">10 useful tips on the implementation of Pixel Perfect design in Front-end development (for example, working with the Sketch editor)</a></li>
<li><a href="../453796/index.html">Do people need mathematics?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>