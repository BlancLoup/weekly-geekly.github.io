<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Predicting Titanic Passenger Survival with Azure Machine Learning</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Thank you very much for preparing the article for Kirill Malev from Merku. For more than 3 years, Kirill has been engaged in the practical application...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Predicting Titanic Passenger Survival with Azure Machine Learning</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  <em>Thank you very much for preparing the article for Kirill Malev from Merku.</em>  <em>For more than 3 years, Kirill has been engaged in the practical application of machine learning for different amounts of data.</em>  <em>The company solves problems in the field of predicting customer churn and natural language processing, paying great attention to the commercialization of the results.</em>  <em>Graduated from the Bologna University and NSTU</em> <em><br></em> </blockquote><br>  Today we will tell you about how to use in practice the Azure cloud platform to solve machine learning tasks to solve machine learning problems using the popular prediction problem for surviving passengers of Titanic. <br><br>  We all remember the famous picture about the owl, so in this article all the steps are commented on in detail.  If you do not understand any step, you can ask questions in the comments. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/206/111/37f/20611137fbd5d9c08399aaedb7e6b9fd.jpg"></div><br><a name="habracut"></a><br>  Big Data and machine learning are already approaching a plateau of productivity, according to a fresh Gartner <a href="http://www.datanami.com/2015/08/26/why-gartner-dropped-big-data-off-the-hype-curve/">report</a> .  This means that the market has a sufficient understanding of how to apply big data processing technologies and people are more used to this topic than to 3D organ printing or the idea of ‚Äã‚Äãcolonizing Mars. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Habrahabr has already <a href="http://habrahabr.ru/company/microsoft/blog/236823/">published</a> <a href="http://habrahabr.ru/company/microsoft/blog/254637/">articles</a> that talk about using the Azure cloud platform for machine learning.  They talk about how everything works, how you can use this platform and its advantages and disadvantages.  We will complement this collection with two practical examples. <br><br>  In both examples, we will use the Kaggle Data Scientist competition platform.  Kaggle is the place where companies post data, identify problems that they would like to solve, after which engineers from all over the world compete to solve problems exactly.  As a rule, for the best results, the winners at the publication of their decision receive cash prizes. <br>  At the same time, in addition to commercial competitions, Kaggle also has disassembled tasks with provided data that are used for training.  For them, the ‚Äúknowledge‚Äù is directly indicated as a prize and these competitions are not completed. <br><br>  We will begin our introduction to Azure ML by participating in the most famous of them: Titanic: Machine Learning from Disaster.  We upload the data, analyze it using the R language models and the built-in Azure ML functions, get the results and upload them to Kaggle. <br><br>  We will move from simple to complex: first we will get the result of analyzing the data using the code on R, then we will use the analysis tools built into the platform itself.  The solution on R is given at the end of the article under the cut and it is specifically based on the official Kaggle training materials, so that beginners can, if they wish, become familiar with how they can get exactly the same results from scratch. <br><br>  The main goal of this article is to use the functionality of the cloud solution to solve the classification problem.  If you don‚Äôt understand the syntax or content of the R code, you can read a small introduction to solving data analysis problems using the example of the Titanic problem in R language in Trevor Stevens <a href="http://trevorstephens.com/post/73461351896/titanic-getting-started-with-r-part-4-feature">blog</a> or take an interactive hour course on <a href="https://www.datacamp.com/courses/kaggle-tutorial-on-machine-learing-the-sinking-of-the-titanic">datacamp</a> .  Any questions about the models themselves are welcome in the comments. <br><br><h4>  <b>Getting started with the cloud</b> </h4><br>  To work with Azure, you need a Microsoft account.  Registration takes 5 minutes, in addition, if you have not yet registered with Azure, you should pay close attention to <a href="https://www.microsoft.com/ru-ru/ms-start">Microsoft BizSpark</a> , a startup support program that provides extended bonuses during registration.  If you do not fall under the terms of this program, then at registration you will be allocated a trial $ 200, which can be spent on any Azure cloud resources.  To test Azure ML and repeat the article is more than enough. <br><br>  If you have an account, you can enter <a href="http://azure.microsoft.com/ru-ru/services/machine-learning/">the</a> Azure ML <a href="http://azure.microsoft.com/ru-ru/services/machine-learning/">section</a> . <br>  Selecting ‚ÄúMachine Learning‚Äù from the list of Azure services on the left will prompt you to create a workspace (which will contain models and project files called ‚Äúexperiments‚Äù).  You must immediately connect to the data storage account or create a new one.  It is needed in order to store intermediate results of data processing or upload results. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f3e/521/2d5/f3e5212d58c6fc774bbb110d2acb348a.png"><br><br>  After creating a workspace, you can create a project in it.  To do this, you will need to click the "create" button: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/81a/890/b6d/81a890b6d5c2fb10bdcc88878ee666f4.png"><br><br>  And then those who are accustomed to learn a new platform or framework with examples are waiting for a great gift - ready examples of projects with which you can quickly understand how the service works.  In case of problems, the answers to most questions can be found in user <a href="http://azure.microsoft.com/en-us/documentation/services/machine-learning/">-</a> friendly <a href="http://azure.microsoft.com/en-us/documentation/services/machine-learning/">online documentation</a> .  On the practice of working with Azure, all the necessary information can really be found there. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d9f/647/2fb/d9f6472fbd3078f760f44f33fbf227dd.png"><br><br>  In the library of ready-made experiments there is an example of launching a model on R, which looks like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2b7/cd2/428/2b7cd2428e7708f3670219484c580ffb.png"><br><br>  These blocks, connected by arrows, are the process of analyzing the data in this project (or experiment, if you use the terminology of Azure ML).  Using this visualization greatly facilitates the understanding of data analysis and the <a href="https://en.wikipedia.org/wiki/Cross_Industry_Standard_Process_for_Data_Mining">CRISP-DM</a> methodology. <br><br>  The essence of this methodology is as follows: <br><br><ol><li>  Data is loaded on Wednesday </li><li>  Useful features are selected / created from the data. </li><li>  The model is trained on the selected features. </li><li>  Using a separate data set, model quality is assessed. </li><li>  If the quality is unsatisfactory, then steps 2-4 are repeated; if satisfactory, then the model is used for its intended purpose. </li></ol><br>  For our article in the experiment creation dialog, we will select the very first version of the ‚ÄúBlank Experiment‚Äù, and then transferred 2 blocks to the workspace: ‚ÄúExecute R Script‚Äù from the ‚ÄúR language models‚Äù section on the left and ‚ÄúWriter‚Äù from Data Input And Output. <br>  The model with which we begin will look much more modest than the one given in the initial example: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/539/544/b95/539544b9582e5a8af8e436859822f1db.png"><br><br>  In the block ‚ÄúExecute R Script‚Äù we put the code that downloads the data, highlights the new properties of the objects, trains the model and makes the prediction.  The only alternative to running the script on the local machine was that we replaced the line <br><br><pre><code class="python hljs">write.csv(my_solution,file=<span class="hljs-string"><span class="hljs-string">"my_solution.csv"</span></span> , row.names=FALSE)</code> </pre> <br>  on <br><br><pre> <code class="python hljs">maml.mapOutputPort(<span class="hljs-string"><span class="hljs-string">"my_solution"</span></span>)</code> </pre> <br>  to be able to save the solution in the cloud and then download it (this is explained below). <br><br>  At the end of the R code, we had a line that shows the importance of various parameters.  The output of scripts is available in the second ‚Äúexit‚Äù (number ‚Äú2‚Äù in the screenshot) from the block by clicking on the ‚ÄúVizualize‚Äù menu item: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/550/ef5/b17/550ef5b177dcdc541309a3ca41324e97.png"><br><br>  So, the code works, we saw which variables were the most important for the survival of the Titanic passengers, but how to get the result and upload it to Kaggle? <br><br>  The first exit from the block, in which we put our predictions with the help of the line, is responsible for this. <br><br><pre> <code class="python hljs">maml.mapOutputPort(<span class="hljs-string"><span class="hljs-string">"my_solution"</span></span>)</code> </pre> <br>  It allows you to redirect the output from the R code to the Writer object, which writes our data set to the data warehouse.  As settings, we specified the name of the repository that our experiment uses (habrahabrdata1) and the path to the container in which we wanted to record our result: saved-datasets / kaggle-R-titanic-dataset.csv <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5b6/90b/10c/5b690b10c59ca4e3620c8c8c8751e7de.png"><br><br>  For convenience, we have previously created this separate storage in order not to lose data among the Azure ML service data (they can be viewed in the experimentouput repository).  By the way, please note that when creating a repository you cannot use underscore ‚Äú_‚Äù or capital letters. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/cd3/fb2/9d4/cd3fb29d42b40e89ea5b819cb2f0b341.png"><br><br>  Predictions of our model are downloaded immediately in csv-format from the cloud storage service.  When sending this model we got the result: <b>0.78469</b> <br><br>  <b>Use Azure Machine Learning Tools</b> <br><br>  Now that we‚Äôve learned a little about the interface and how Azure ML works and made sure that everything works, we can use more of the functions built into the cloud to work with the data: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/getpro/habr/post_images/523/1ea/b44/5231eab44bb50d9856c7c102b9e70605.jpg"></div><br><br>  To begin with, we upload data to the cloud for training and evaluation of results.  To do this, go to the "Datasets" section and load the previously downloaded .csv files: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5f8/acc/59f/5f8acc59f48404fd59ad79cf870ffeab.png"><br><br>  As a result, our data sets will look like this: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f2a/7c8/fab/f2a7c8fab4e657c97c7a27e675855b68.png"><br><br>  Accordingly, we can rewrite our script, which previously did all the work: downloaded the data, carried out their initial processing, divided it into test and training sets, trained the model and evaluated the test set. <br><br>  Moving from simple to complex: now the code in R will be responsible only for data processing.  We will upload data and divide it into a set for training and quality assessment using the cloud. <br>  To do this, we append two lines to the top of the script: <br><br><pre> <code class="python hljs">train &lt;- maml.mapInputPort(<span class="hljs-number"><span class="hljs-number">1</span></span>) <span class="hljs-comment"><span class="hljs-comment"># class: data.frame test &lt;- maml.mapInputPort(2) # class: data.frame</span></span></code> </pre> <br>  and after the code that processes the signs we make the output: <br><br><pre> <code class="python hljs">maml.mapOutputPort(<span class="hljs-string"><span class="hljs-string">"all_data"</span></span>)</code> </pre> <br>  Now the basic scheme of the experiment will look like this: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/1xlic-p873x2xyea97p2_6e8n"><br><br>  And the condition of splitting into a set for training and a test set will look like this: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/77-tqu-oivw__9uin7lhww19"><br><br>  At the same time, at output 1 we will have a test set (because the splitting condition is fulfilled for it) and at output 2, for training. <br><br>  Now we are ready to evaluate using the AUC criterion the efficiency of the binary classification algorithms built into the cloud.  For the basis of this experiment, we took the example of <a href="https://europewest.studio.azureml.net/Home/ViewWorkspace/85063f151a98432db5860db75a39564f%3F"><b>Compare Binary Classifiers</b></a> . <br><br>  Checking one algorithm is as follows: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/1r6moi3e1-_cg7x8ze9i45e5-"><br><br>  Each algorithm receives as input a sample of the kaggle_titanic_train for selecting the best parameters of the algorithms.  These parameters are recruited using the Sweep Parameters block (for more information about it, see the <a href="https://azure.microsoft.com/en-us/documentation/articles/machine-learning-algorithm-parameters-optimize/">article</a> about iterating over parameters), which allows you to iterate over all the parameters in a given range, across the grid of all parameters or use a random pass.  In the settings of the Sweep Parameters, you can set the evaluation criteria.  We set AUC as a more suitable criterion for us. <br><br>  After selecting the parameters, the best model obtained is estimated using a separate part of the sample.  Results for models with the best parameters are displayed at the end of the whole experiment: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/1sluv1q5xu45b5zuq375hzkuv"><br><br>  Clicking on the first exit from the last block ‚ÄúExecute R script‚Äù we get the result: <br><br>  The best result showed Two-Class SVM.  We can look at the best parameters by clicking on the output from the Sweep Parameters block: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/3f7rcgpo852gzk_38504bnhq"><br><br>  As a result, to determine whether passengers will survive or not on the test data, we can run the model with the optimal parameters: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/vydnrak_z747cpe3c0egn987"><br><br>  After creating an experiment with determining the best model, the new model will look quite simple: <br><br><img src="http://litehtmlconv.azurewebsites.net/api/pics/t5bjtz__3597wni3qgdzgnnd"><br><br>  It uses the same blocks as all our experiments before.  The Train Model model receives the entire kaggle_titanic_train dataset, and for evaluation (prediction), the Score Model block is used, to which the kaggle_titanic_test dataset is sent with all the necessary attributes (which we calculated using R).  Then, from the entire set, only columns with passenger ID and prediction will be left to survive or not, and the result is stored in Blob storage. <br><br>  When you send the results of this model to Kaggle, you get the value of <b>0.69856</b> , which is less than the value we obtained using the method of decision trees, doing all the work in R. <br><br>  However, if you train a similar algorithm from Azure ML (Two Class Decision Forest) with similar parameters (number of trees: 100), then when you send the result to Kaggle it will improve by 0.00957 and will be equal to <b>0.79426</b> . <br><br>  Thus, the ‚Äúmagic‚Äù search of parameters does not cancel a more thorough manual search and the work of an expert who can get better results. <br><br>  <b>Conclusion</b> <br><br>  We considered the possibility of using the Azure ML cloud environment to participate in Kaggle data analysis competitions: both as an environment for executing code on R and partially using embedded cloud tools (leaving R primary processing and generation of new features). <br><br>  This environment can be used to apply machine learning, especially when the analysis results need to be left in the Hadoop cluster (Microsoft provides its implementation) or published as a web service. <br><br>  In the case of a positive response, we will also publish in this blog an equally detailed example of the publication of a machine learning model as a web service. <br><br><div class="spoiler">  <b class="spoiler_title">Complete solution of the problem on R</b> <div class="spoiler_text"><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># All data, both training and test set # Assign the training set train &lt;- read.csv(url("http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv")) # Assign the testing set test &lt;- read.csv(url("http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv")) test$Survived &lt;- NA all_data = rbind (train,test) # Passenger on row 62 and 830 do not have a value for embarkment. # Since many passengers embarked at Southampton, we give them the value S. # We code all embarkment codes as factors. all_data$Embarked[c(62,830)] = "S" all_data$Embarked &lt;- factor(all_data$Embarked) # Passenger on row 1044 has an NA Fare value. Let's replace it with the median fare value. all_data$Fare[1044] &lt;- median(all_data$Fare, na.rm=TRUE) #Getting Passenger Title all_data$Name &lt;- as.character(all_data$Name) all_data$Title &lt;- sapply(all_data$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]}) all_data$Title &lt;- sub(' ', '', all_data$Title) all_data$Title[all_data$Title %in% c('Mme', 'Mlle')] &lt;- 'Mlle' all_data$Title[all_data$Title %in% c('Capt', 'Don', 'Major', 'Sir')] &lt;- 'Sir' all_data$Title[all_data$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] &lt;- 'Lady' all_data$Title &lt;- factor(all_data$Title) all_data$FamilySize &lt;- all_data$SibSp + all_data$Parch + 1 library(rpart) # How to fill in missing Age values? # We make a prediction of a passengers Age using the other variables and a decision tree model. # This time you give method="anova" since you are predicting a continuous variable. predicted_age &lt;- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + FamilySize, data=all_data[!is.na(all_data$Age),], method="anova") all_data$Age[is.na(all_data$Age)] &lt;- predict(predicted_age, all_data[is.na(all_data$Age),]) # Split the data back into a train set and a test set train &lt;- all_data[1:891,] test &lt;- all_data[892:1309,] library(randomForest) # Train set and test set str(train) str(test) # Set seed for reproducibility set.seed(111) # Apply the Random Forest Algorithm my_forest &lt;- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + FamilySize + Title, data=train, importance = TRUE, ntree=1000) # Make your prediction using the test set my_prediction &lt;- predict(my_forest, test, "class") # Create a data frame with two columns: PassengerId &amp; Survived. Survived contains your predictions my_solution &lt;- data.frame(PassengerId = test$PassengerId, Survived = my_prediction) # Write your solution away to a csv file with the name my_solution.csv write.csv(my_solution,file="my_solution.csv" , row.names=FALSE) varImpPlot(my_forest)</span></span></code> </pre><br><br></div></div><br>  <b>Bibliography</b> <br><ul><li>  <a href="http://habrahabr.ru/company/microsoft/blog/236823/">Introduction to machine learning and a quick start with Azure ML</a> </li><li>  <a href="http://habrahabr.ru/company/microsoft/blog/254637/">Azure Machine Learning for Data Scientist</a> </li></ul><br></div><p>Source: <a href="https://habr.com/ru/post/268039/">https://habr.com/ru/post/268039/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../268029/index.html">Security vulnerability detected in WinRAR software</a></li>
<li><a href="../268031/index.html">Apiway - a new way of client-server data transport</a></li>
<li><a href="../268033/index.html">We balance the mechanics in games</a></li>
<li><a href="../268035/index.html">Bad "Spring" or as the reasons for the delays were looking for</a></li>
<li><a href="../268037/index.html">Preparing ASP.NET5, issue number 4 - details about routing</a></li>
<li><a href="../268047/index.html">Asterisk. This time, as a background music broadcasting system with the possibility of emergency notification</a></li>
<li><a href="../268055/index.html">Support for geolocation and corrected sketches of the express panel in the assembly Vivaldi 1.0.288.3</a></li>
<li><a href="../268057/index.html">CUSTIS Open Seminars: Graduation Season</a></li>
<li><a href="../268061/index.html">Democracy and Program Committee</a></li>
<li><a href="../268063/index.html">Friday format: How to write code that no one can accompany</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>