<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>As I made the fastest resize of images. Part 3, fixed-point numbers</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="I continue to talk in detail about optimization techniques that allowed me to write the fastest image resize on modern x86 processors. This time it wi...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>As I made the fastest resize of images. Part 3, fixed-point numbers</h1><div class="post__text post__text-html js-mediator-article"><p>  I continue to talk in detail about optimization techniques that allowed me to write the fastest image resize on modern x86 processors.  This time it will be about converting floating-point calculations to calculations with integers.  First, I will tell you a little bit of theory of how this works.  Then go back to the real code, including the SIMD version. </p><br><p>  In the previous parts: </p><br><p>  ‚Üí <a href="https://habrahabr.ru/post/321744/">Part 0</a> <br>  ‚Üí <a href="https://habrahabr.ru/post/322352/">Part 1, general optimizations</a> <br>  ‚Üí <a href="https://habrahabr.ru/post/326900/">Part 2, SIMD</a> </p><a name="habracut"></a><br><h2 id="celye-chisla-i-plavayuschie-tochki">  Integers and Floating Points </h2><br><p>  I think everyone knows that there are two basic ways to represent numbers in computer memory.  Here are their main features: </p><br><p>  Whole numbers </p><br><ul><li>  Store exact value </li><li>  Have a relatively small range of values. </li><li>  The difference between two adjacent values ‚Äã‚Äãis always 1 </li><li>  Convenient for storage </li><li>  1 - 0.1 = 0 </li></ul><br><p>  Floating point numbers </p><br><ul><li>  Store approximate value with a certain accuracy </li><li>  The range of values ‚Äã‚Äãis very large </li><li>  The difference between neighboring values ‚Äã‚Äãmay be greater than the number of photons in the universe. </li><li>  Convenient for intermediate calculations. </li><li>  1 - 0.1 = 0.900000000000000022204460 ... </li></ul><br><p>  Numbers with a floating point are stored in memory in the form of two values ‚Äã‚Äã- mantissa and exponent.  The mantissa is the bits that store the value itself (almost as whole numbers), and the exponent says how many digits the mantissa value is shifted.  To find out the true value of a number, you need to multiply the mantissa by the bit depth of the exponent: m ¬∑ 2·µâ.  The digit capacity in this case is 2, since  binary numeral system. </p><br><p><img src="https://habrastorage.org/web/232/8e4/74c/2328e474c3394e22bfd2d1560f67995b.png"></p><br><p>  It seems that this system is quite complex compared to the integer representation of a number.  However, on modern processors, most operations (such as multiplication and addition) on floating-point numbers are performed in the same number of cycles as operations on integers.  The complexity of the operations only leads to an increase in the number of transistors in the processors, but not the number of cycles.  But besides the speed of the operations themselves, there are several other factors that need to be considered in the context of performance. </p><br><ul><li><p>  As mentioned above, integers are more often used for storage, and floating point for calculations.  This means that you often need to convert from one view to another.  This is of course fast, but depending on the task it can affect performance.  At the end of the <a href="https://habrahabr.ru/post/322352/">first part,</a> I described a case in which such conversion was the most expensive calculation operation. </p><br></li><li><p>  The minimum size of floating-point types for the x86 architecture is 32 bits.  But for storing integers, you can often get by with 16 bits, and in some cases eight.  This is not critical for scalar computing, when the processor at one time is busy executing one instruction.  However, in the case of vector calculations, it allows two or four times more operations per clock cycle. </p><br></li><li>  At the end of the <a href="https://habrahabr.ru/post/326900/">second part</a> , it was clear that when the AVX2 code is running, the processor slows down the clock frequency.  According to my observations, this happens only when working with floating point numbers.  If integer AVX2 commands are used, the processor continues to operate at maximum frequency.  Of course, this is a rather specific point, the behavior can easily change from generation to generation, and depending on the purpose of the processor.  However, the output does not change: floating-point numbers may run slower than integers, even with the same performance per clock. </li></ul><br><h2 id="fiksirovannaya-tochka">  Fixed point </h2><br><p>  It can be seen that the integers have the potential for higher performance and you can try to translate arithmetic on them.  But, of course, replacing float with int everywhere will not work.  When resizing, many calculations are carried out in the range from 0 to 1. That is,  in integer representation, it will be just zero. </p><br><p>  Here <a href="https://ru.wikipedia.org/wiki/%25D0%25A7%25D0%25B8%25D1%2581%25D0%25BB%25D0%25BE_%25D1%2581_%25D1%2584%25D0%25B8%25D0%25BA%25D1%2581%25D0%25B8%25D1%2580%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25BD%25D0%25BE%25D0%25B9_%25D0%25B7%25D0%25B0%25D0%25BF%25D1%258F%25D1%2582%25D0%25BE%25D0%25B9">numbers with a fixed point</a> come to the rescue.  Strictly speaking, integers are also fixed-point numbers, the point of which is fixed after the youngest binary digit.  But it is possible to speculatively move it, say 8 bits, and assume that the unit is actually 1/256.  Then 256 is one, 512 is a two, and 384 is 1.5.  What does this give?  In this form, you can record not only the integer part of the number, but also the real part.  A fairly common example of fixed-point numbers is the currency data type, which some programming languages ‚Äã‚Äãhave.  It holds an integer amount of kopecks or cents, and to get rubles or dollars, you need to divide the value by one hundred. </p><br><p>  So again: fixed-point numbers are numbers multiplied by some constant to increase the accuracy of calculations.  Unlike floating-point numbers, this constant is not stored in the number itself, but can be hard-wired into the implementation of the algorithm, or be calculated in the process. </p><br><p>  In general, working with fixed-point numbers is not difficult, but there are a few things that need to be remembered. </p><br><ul><li><p>  The range of values ‚Äã‚Äãdecreases with increasing accuracy of the fractional part.  If you move a fixed point one bit to the left, the accuracy will double, but the range of values ‚Äã‚Äãwill be halved.  You should always try to move the point as far as possible to the left, but avoid overflows.  Therefore, before starting to transfer calculations to a fixed point, you need to determine the range of maximum values ‚Äã‚Äãthat will be involved in the calculations.  For example, if the range of values ‚Äã‚Äãis from -128 to 384, then the number of bits needed to represent the values ‚Äã‚Äã(including the sign) will be 10. If the 16-bit data type is used, then only 6 bits will remain for accuracy. </p><br></li><li><p>  The addition and subtraction operations for fixed-point numbers work as usual.  Multiplication by an integer too. </p><br></li><li>  When multiplying two numbers with a fixed point, the number of digits responsible for accuracy doubles.  In the same way as the number of digits responsible for storing the whole part doubles.  That is, if after multiplication you need to get the original accuracy, the result will have to be shifted by the number of bits of precision.  Or you can not move, if it will be more convenient to read, but you have to keep this fact in mind. </li></ul><br><p><img src="https://habrastorage.org/web/adf/6bd/8c3/adf6bd8c37474ee1be4f366af9326635.png"></p><br><h2 id="podschet-tochnosti">  Counting accuracy </h2><br><p>  Before translating the code into integers, it would be nice to calculate how many bits can be allocated for accuracy.  Moreover, this calculation makes sense for each operation.  And it‚Äôs better to start from the end: </p><br><pre><code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span> ss, ss0, ss1; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (xx = <span class="hljs-number"><span class="hljs-number">0</span></span>; xx &lt; imOut-&gt;xsize; xx++) { ss0 = ss1 = ss2 = <span class="hljs-number"><span class="hljs-number">0.5</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (y = ymin; y &lt; ymax; y++) { ss0 = ss0 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">0</span></span>]) * k[y-ymin]; ss1 = ss1 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">1</span></span>]) * k[y-ymin]; ss2 = ss2 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">2</span></span>]) * k[y-ymin]; } imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">0</span></span>] = clip8(ss0); imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">1</span></span>] = clip8(ss1); imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">2</span></span>] = clip8(ss2); }</code> </pre> <br><p>  The <code>ss0</code> - <code>ss2</code> contain the sum of the products of the coefficients per pixel.  Pixels have values ‚Äã‚Äãin the range [0, 255], and about the sum of the coefficients it is known that it is equal to one.  That is, the final value of <code>ss0</code> - <code>ss2</code> will also be in the range [0, 255].  But this is only the final!  In general, some coefficients may be negative, and, as a result, the sum of positive coefficients may be greater than one (look at the <a href="https://habrahabr.ru/post/321744/">filter charts</a> from the zero article).  Therefore, intermediate values ‚Äã‚Äãmay slightly fall outside the range [0,255].  One bit for negative numbers and one more for possible overflows above would be enough in this case.  Total to store the value will need 10 bits [-512,511].  Since it is logical to make the batteries at least 32-bit, then the storage of the accuracy of the batteries (let's call it <code>PRECISION_BITS</code> ) is 22 bits. </p><br><p>  It remains to deal with the multiplication of pixels by the coefficients.  I have already said that when multiplying a number with a fixed point by an integer, no additional transformations are necessary.  In this case, the integer is the pixel values.  This means that the coefficients of accuracy can be the same as that of batteries - 22 bits. </p><br><h2 id="skalyarnye-vychisleniya-s-fiksirovannoy-tochkoy">  Fixed-point scalar computing </h2><br><p>  This is surprising, but in the code above you need to change only one line to translate it to work with a fixed point.  At the very beginning, the batteries are assigned a value of 0.5.  In the new number system this will correspond to the value <code>1 &lt;&lt; (PRECISION_BITS - 1)</code> .  That is, the unit is shifted one digit less than the accuracy.  0.5 new units. </p><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ss, ss0, ss1; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (xx = <span class="hljs-number"><span class="hljs-number">0</span></span>; xx &lt; imOut-&gt;xsize; xx++) { ss0 = ss1 = ss2 = <span class="hljs-number"><span class="hljs-number">1</span></span> &lt;&lt; (PRECISION_BITS <span class="hljs-number"><span class="hljs-number">-1</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (y = ymin; y &lt; ymax; y++) { ss0 = ss0 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">0</span></span>]) * k[y-ymin]; ss1 = ss1 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">1</span></span>]) * k[y-ymin]; ss2 = ss2 + ((UINT8) imIn-&gt;image[y][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">2</span></span>]) * k[y-ymin]; } imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">0</span></span>] = clip8(ss0); imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">1</span></span>] = clip8(ss1); imOut-&gt;image[yy][xx*<span class="hljs-number"><span class="hljs-number">4</span></span>+<span class="hljs-number"><span class="hljs-number">2</span></span>] = clip8(ss2); }</code> </pre> <br><p>  All other calculations remain unchanged, which indirectly indicates that we are on the right track.  After all, a change in the concept did not introduce difficulties in implementation, but you can still count on performance gains. </p><br><p>  But the function <code>clip8</code> , limiting the final pixel value in the range [0, 255], will change a lot.  It was: </p><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">inline</span></span></span><span class="hljs-function"> UINT8 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clip8</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">float</span></span></span></span><span class="hljs-function"><span class="hljs-params"> in)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> out = (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>) in; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (out &gt;= <span class="hljs-number"><span class="hljs-number">255</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">255</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (out &lt;= <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (UINT8) out; }</code> </pre> <br><p>  It became: </p><br><pre> <code class="cpp hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">static</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">inline</span></span></span><span class="hljs-function"> UINT8 </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">clip8</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(</span></span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">int</span></span></span></span><span class="hljs-function"><span class="hljs-params"> in)</span></span></span><span class="hljs-function"> </span></span>{ <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (in &gt;= (<span class="hljs-number"><span class="hljs-number">1</span></span> &lt;&lt; PRECISION_BITS &lt;&lt; <span class="hljs-number"><span class="hljs-number">8</span></span>)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">255</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">if</span></span> (in &lt;= <span class="hljs-number"><span class="hljs-number">0</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> (UINT8) (in &gt;&gt; PRECISION_BITS); }</code> </pre> <br><p>  First, the accepted value changes - now it's a 32-bit int.  Secondly, it is not immediately cast to the integer type (earlier it was in the first line).  Now, instead, you can compare <code>in</code> with the value <code>1 &lt;&lt; PRECISION_BITS &lt;&lt; 8</code> .  This value is 256 in our number system with a fixed point, because it is shifted by the number of bits in the fractional part and another 8 bits.  And as you know, <code>1 &lt;&lt; 8</code> - this is just 256. And already at the end, if all the comparisons gave a negative result, the value is really reduced to an ordinary whole, without any points.  The result is the usual shift by the number of bits of precision. </p><br><p>  Now to a fixed point you need to bring the coefficients.  Let me remind you that initially the coefficients are floating point numbers from -1 to 1. And the sum of all coefficients for calculating one pixel is equal to one.  I am sure that there is no point in using integer arithmetic to actually calculate the coefficients.  First, the calculation of coefficients takes much less time than their use.  Secondly, trigonometric functions are used inside some filters.  Therefore, it seems to me correct to calculate the floating point coefficients, and only then convert them to a fixed one, multiplying by <code>(1 &lt;&lt; PRECISION_BITS)</code> . </p><br><pre> <code class="cpp hljs"><span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (x = <span class="hljs-number"><span class="hljs-number">0</span></span>; x &lt; xsize * kmax; x++) { kk[x] = (<span class="hljs-keyword"><span class="hljs-keyword">int</span></span>) (prekk[x] * (<span class="hljs-number"><span class="hljs-number">1</span></span> &lt;&lt; PRECISION_BITS)); }</code> </pre> <br><p>  What does this give?  Here are the latest results for scalar calculations that were obtained on floating point numbers: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0.03009 s 136.10 Mpx/s to 320x200 bic 0.05187 s 78.97 Mpx/s to 320x200 lzs 0.08113 s 50.49 Mpx/s to 2048x1280 bil 0.14017 s 29.22 Mpx/s to 2048x1280 bic 0.17750 s 23.08 Mpx/s to 2048x1280 lzs 0.22597 s 18.13 Mpx/s to 5478x3424 bil 0.58726 s 6.97 Mpx/s to 5478x3424 bic 0.74648 s 5.49 Mpx/s to 5478x3424 lzs 0.90867 s 4.51 Mpx/s</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/57e8925b3bff7ff79eb8e4625f43e3e363b87bc9">commit 57e8925</a> .</i> </p><br><p>  Here are the results for a fixed point: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0.02079 s 196.99 Mpx/s 44.7 % to 320x200 bic 0.03459 s 118.41 Mpx/s 50.0 % to 320x200 lzs 0.05649 s 72.50 Mpx/s 43.6 % to 2048x1280 bil 0.10483 s 39.07 Mpx/s 33.7 % to 2048x1280 bic 0.13362 s 30.66 Mpx/s 32.8 % to 2048x1280 lzs 0.17210 s 23.80 Mpx/s 31.3 % to 5478x3424 bil 0.46706 s 8.77 Mpx/s 25.7 % to 5478x3424 bic 0.59492 s 6.88 Mpx/s 25.5 % to 5478x3424 lzs 0.72819 s 5.62 Mpx/s 24.8 %</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/15d0573919238722ba0ca006c7420d641e11e846">commit 15d0573</a> .</i> </p><br><p>  As you can see, everything was not in vain and the increase is very serious.  Most of all, it is noticeable on a strong decrease, where there were more operations to convert pixel values. </p><br><h2 id="simd-vychisleniya-s-fiksirovannoy-tochkoy">  Fixed Point SIMD </h2><br><p>  Translation of a <a href="https://habrahabr.ru/post/326900/">SIMD code</a> from the third part to calculations with a fixed point can be divided into 4 stages: </p><br><ul><li>  SSE4 vertical pass translation </li><li>  horizontal passage translation SSE4 </li><li>  AVX2 vertical pass translation </li><li>  horizontal pass translation AVX2 </li></ul><br><p>  These stages will be pretty monotonous, so it makes sense to carefully consider only one.  Here is an example of a vertical passage SSE4 on floating point numbers. </p><br><pre> <code class="cpp hljs">ImagingResampleVerticalConvolution8u(UINT32 *lineOut, Imaging imIn, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ymin, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ymax, <span class="hljs-keyword"><span class="hljs-keyword">float</span></span> *k) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> y, xx = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (; xx &lt; imIn-&gt;xsize; xx++) { __m128 sss = _mm_set1_ps(<span class="hljs-number"><span class="hljs-number">0.5</span></span>); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (y = ymin; y &lt; ymax; y++) { __m128i pix = _mm_cvtepu8_epi32(*(__m128i *) &amp;imIn-&gt;image32[y][xx]); __m128 mmk = _mm_set1_ps(k[y - ymin]); __m128 mul = _mm_mul_ps(_mm_cvtepi32_ps(pix), mmk); sss = _mm_add_ps(sss, mul); } __m128i ssi = _mm_cvtps_epi32(sss); ssi = _mm_packs_epi32(ssi, ssi); lineOut[xx] = _mm_cvtsi128_si32(_mm_packus_epi16(ssi, ssi)); } }</code> </pre> <br><p>  The data type <code>__m128</code> stores 4 floating point numbers.  It is no longer needed, it should be replaced with <code>__m128i</code> .  The analogue of the <code>_mm_set1_ps</code> function is <code>_mm_set1_epi32</code> .  The <code>_mm_cvtepi32_ps</code> and <code>_mm_cvtps_epi32</code> conversion <code>_mm_cvtepi32_ps</code> <code>_mm_cvtps_epi32</code> no longer needed; instead, the result at the end will need to be shifted to <code>PRECISION_BITS</code> to the right.  Difficulties can arise only with the <code>_mm_mul_ps</code> function, because there is no direct analogue for it, but if you search, <code>_mm_mullo_epi32</code> is <code>_mm_mullo_epi32</code> .  The fact is that multiplying two 32-bit numbers gives a 64-bit number.  Lo means that the lower 32 bits of the result will be returned, which is exactly what is needed.  Completely all code will look like this: </p><br><pre> <code class="cpp hljs">ImagingResampleVerticalConvolution8u(UINT32 *lineOut, Imaging imIn, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ymin, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> ymax, <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> *intk) { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> y, xx = <span class="hljs-number"><span class="hljs-number">0</span></span>; <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (; xx &lt; imIn-&gt;xsize; xx++) { __m128i sss = _mm_set1_epi32(<span class="hljs-number"><span class="hljs-number">1</span></span> &lt;&lt; (PRECISION_BITS <span class="hljs-number"><span class="hljs-number">-1</span></span>)); <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> (y = ymin; y &lt; ymax; y++) { __m128i pix = _mm_cvtepu8_epi32(*(__m128i *) &amp;imIn-&gt;image32[y][xx]); __m128i mmk = _mm_set1_epi32(intk[y - ymin]); __m128i mul = _mm_mullo_epi32(pix, mmk); sss = _mm_add_epi32(sss, mul); } sss = _mm_srai_epi32(sss, PRECISION_BITS); sss = _mm_packs_epi32(sss, sss); lineOut[xx] = _mm_cvtsi128_si32(_mm_packus_epi16(sss, sss)); } }</code> </pre> <br><p>  Now you can compare the results obtained for the SSE4 version on floating point numbers: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0.01151 s 355.87 Mpx/s to 320x200 bic 0.02005 s 204.27 Mpx/s to 320x200 lzs 0.03421 s 119.73 Mpx/s to 2048x1280 bil 0.04450 s 92.05 Mpx/s to 2048x1280 bic 0.05951 s 68.83 Mpx/s to 2048x1280 lzs 0.07804 s 52.49 Mpx/s to 5478x3424 bil 0.18615 s 22.00 Mpx/s to 5478x3424 bic 0.24039 s 17.04 Mpx/s to 5478x3424 lzs 0.30674 s 13.35 Mpx/s</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/8d0412b4969db192b0671e78eb72db65899ec968">commit 8d0412b</a> .</i> </p><br><p>  With the results obtained for SS4 on fixed-point numbers: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0.01253 s 326.82 Mpx/s -8.1 % to 320x200 bic 0.02239 s 182.94 Mpx/s -10.5 % to 320x200 lzs 0.03663 s 111.83 Mpx/s -6.6 % to 2048x1280 bil 0.04712 s 86.92 Mpx/s -5.6 % to 2048x1280 bic 0.06731 s 60.86 Mpx/s -11.6 % to 2048x1280 lzs 0.08176 s 50.10 Mpx/s -4.5 % to 5478x3424 bil 0.19010 s 21.55 Mpx/s -2.1 % to 5478x3424 bic 0.25013 s 16.38 Mpx/s -3.9 % to 5478x3424 lzs 0.31413 s 13.04 Mpx/s -2.4 %</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/7d8df663944808d198da2699e9d8342b1bbbddbc">commit 7d8df66</a> .</i> </p><br><p>  And here I was in for a surprise.  For a long time I tried to understand what went wrong.  At some point I went to watch the timings of each instruction that was used in the cycle and the solution was exactly here. </p><br><p>  Now in <a href="https://software.intel.com/sites/landingpage/IntrinsicsGuide/">the Intel Intrinsics Guide,</a> this is not visible, because it is constantly updated and from time to time it deletes data for old processors from it.  But when I dealt with this issue, the <code>_mm_mullo_epi32</code> operation had the following timings table: </p><br><pre> <code class="markdown hljs">Architecture Latency Throughput Broadwell 10 2 Haswell 10 2 Ivy Bridge 5 1</code> </pre> <br><p>  Now compare with the timings of a similar <code>_mm_mul_ps</code> for floating point numbers: </p><br><pre> <code class="markdown hljs">Architecture Latency Throughput Broadwell 3 0.5 Haswell 5 0.5 Ivy Bridge 5 1</code> </pre> <br><p>  Here you can see that, starting from the architecture of Haswell, Intel scored on the vector multiplication of whole 32-bit numbers.  And it is exactly whole and exactly 32-bit ones, because all other variants of multiplications continue to grow faster from architecture to architecture. </p><br><p>  Interestingly, for the AVX2 version of the code, this is not observed and the negative effect of increased delay does not prevail over the positive effect of switching to fixed-point calculations.  And the performance gain for fixed-point numbers is ‚âà10%.  There are two reasons for this: </p><br><ul><li>  As I said at the very beginning, the processor does not slow down the frequency when it executes integer AVX2 instructions, unlike AVX2 floating point instructions. </li><li>  The AVX2 version processes 2 times more data at a time, and therefore the multiplication instruction is performed 2 times less often for the same amount of data.  This means that the negative effect of large delays is 2 times less noticeable. </li></ul><br><h2 id="poiski-svyatogo-graalya">  The Quest for the Holy Grail </h2><br><p>  I prepared integer calculations for Pillow version 3.3.  And I tried to release the Pillow and Pillow-SIMD versions more or less synchronously and with the same improvements.  And it was very disappointing that the transition to integers gave a tangible increase in Pillow, but it did not give much or did not give any in Pillow-SIMD.  Then, in the release, I was able to slightly compensate for the lag by deploying cycles.  This made it possible to improve the instruction pipeline and slightly eliminated the effect of slow multiplication.  But about this I would like to tell in the last article of this series. </p><br><p>  If you look at how the performance of the regular Pillow version has changed, it will be seen that Pillow 3.3 was a good gain due to integer calculations.  And in Pillow 3.4, everything remained almost at the same level. </p><br><p><img src="https://habrastorage.org/web/881/76f/ad7/88176fad701f46ee9bd15e2688ebdb16.png"></p><br><p>  Whereas for Pillow-SIMD the situation is reversed: version 3.3 was almost slower than the previous one.  But in 3.4 there was a significant leap, which allows me to say now that Pillow-SIMD is currently the fastest implementation of resizing on a CPU. </p><br><p><img src="https://habrastorage.org/web/c13/3b5/c44/c133b5c440e74c9da0d583cecaac2d9a.png"></p><br><p>  To achieve such improvements in Pillow-SIMD 3.4, it was necessary to get rid of the vector 32-bit multiplication of integers.  But how?  Translate all calculations to 16 bits?  It is easy to calculate that in this case, the coefficients ( <code>PRECISION_BITS</code> ) would be 16‚àí8‚àí2 = 6 bits, that is only 64 values.  In practice, much less, because the sum of all coefficients must be equal to one (that is, 64).  And the number of coefficients depends on the size of the filter window and the scale of reduction (details in <a href="https://habrahabr.ru/post/321744/">part 0</a> ).  When the image is reduced 10 times with the Lanczos filter, the coefficients themselves will be sixty.  The calculations in 16 bits were obviously not accurate enough, you had to think of something else. </p><br><p>  The thought gave me no peace: for some reason, Intel decided to cut down the multiplication in such a strange way.  And other developers do not lament about this on the Internet, but continue to successfully solve their problems.  Maybe 32-bit multiplication is really not necessary for working with graphics, I just don‚Äôt see how to do without it. </p><br><p>  I watched <code>_mm_mullo_epi16</code> .  It was possible to try cleverly select the bit width of the coefficients so that the result of the convolution was 32-bit, but the result of multiplying the pixel value by the coefficient remained within 16 bits.  Then the accuracy of the coefficient itself would have remained 7 bits (one bit went to the sign).  It was significantly better than 6 bits for the sum of all coefficients.  I was going to implement it when I accidentally stumbled upon another solution. </p><br><h2 id="a-chto-esli-by-dlya-svertok-byla-specialnaya-instrukciya">  What if there was a special instruction for the bundles? </h2><br><p>  Imagine you are solving a problem, considering the tools that you have from different angles, and then accidentally stumble upon one that was specifically devised for this task. </p><br><p>  What is the difficulty of multiplication?  Storage of the result of multiplication requires two times more bits than operands.  Therefore, we have to choose: the upper or lower part of the result must be obtained.  Precision suffers from this, and only a fraction of the significant bits are used by the operands.  What if you could get the whole result of multiplication?  Then it would take twice as many bits, that is, for example, two registers with the result.  But what if, after multiplying, add these two registers?  All the same, you will need to add the result of multiplication, this is the meaning of convolution.  Then an instruction would be obtained that takes X pairs of operands to multiply, multiplies them, gets X products, then adds adjacent ones and gives X / 2 sums of products at the output.  And, oddly enough, this instruction was found already in SSE2!  It is called <code>_mm_madd_epi16</code> .  And the delay in it is two times lower than that of <code>_mm_mullo_epi32</code> , and it performs 3 times more operations. </p><br><p>  Once again: there are two registers at the input, each with eight 16-bit signed integers.  These numbers are multiplied in pairs, somewhere in the mind eight eight-bit multiplication results are stored.  Neighboring multiplication results are added together and four signed 32-bit numbers are obtained.  Eight multiplications and four additions with one quick instruction instead of four multiplications slow.  Virtually no loss of accuracy. </p><br><p>  The only problem is that the adjacent results of multiplications add up, and for pixels these will be adjacent channels.  If you apply the command to the forehead, the red channel of the first pixel will fold with the green of the first pixel, and the blue of the first pixel will fold with the alpha channel.  The same for the second pixel.  And for bundles you need to add the red channel of the first pixel with the red channel of the second and so on.  That is, the values ‚Äã‚Äãneed to be slightly mixed before applying this instruction. </p><br><h2 id="perehod-na-16-bitnye-koefficienty">  Transition to 16-bit coefficients </h2><br><p>  Unfortunately, simply replacing the <code>int</code> type with <code>INT16</code> not enough: the coefficients can go beyond this type.  At the beginning, I said that the exponent (the accuracy of a number or the position of a virtual fixed point, as you wish) can be set both in the algorithm itself and calculated in the process.  And here is exactly the case when, depending on different input data, you will need to select different exponents.  And for this calculation will need the value of the maximum of the coefficients. </p><br><pre> <code class="cpp hljs"><span class="hljs-meta"><span class="hljs-meta">#</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> MAX_COEFS_PRECISION (16 - 1) #</span><span class="hljs-meta-keyword"><span class="hljs-meta"><span class="hljs-meta-keyword">define</span></span></span><span class="hljs-meta"> PRECISION_BITS (32 - 8 - 2) coefs_precision = 0; while ( maxkk &lt; (1 &lt;&lt; (MAX_COEFS_PRECISION-1)) &amp;&amp; (coefs_precision &lt; PRECISION_BITS) ) { maxkk *= 2; coefs_precision += 1; };</span></span></code> </pre> <br><p>  That is, it is necessary to ensure that, on the one hand, the value of the maximum coefficient does not exceed 16 bits (because it will be presented in a 16-bit form), and on the other, the value of the whole convolution does not exceed 32 bits (this condition is satisfied <code>coefs_precision &lt; PRECISION_BITS</code> ). </p><br><p>  It seems to me that I already tired enough with the code, so I will not analyze what exactly needs to be changed and how to mix the pixels so that you can apply the <code>_mm_madd_epi16</code> instruction.  Those who are interested can, as always, see the changes in the githaba commits and ask questions in the comments.  I will give the results of the SSE4 version on 16-bit coefficients relative to the SSE4 version on floating point numbers: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0,00844 s 485.20 Mpx/s 36,4 % to 320x200 bic 0,01289 s 317.79 Mpx/s 55,5 % to 320x200 lzs 0,01903 s 215.24 Mpx/s 79,8 % to 2048x1280 bil 0,04481 s 91.41 Mpx/s -0,7 % to 2048x1280 bic 0,05419 s 75.59 Mpx/s 9,8 % to 2048x1280 lzs 0,06930 s 59.11 Mpx/s 12,6 % to 5478x3424 bil 0,19939 s 20.54 Mpx/s -6,6 % to 5478x3424 bic 0,24559 s 16.68 Mpx/s -2,1 % to 5478x3424 lzs 0,29152 s 14.05 Mpx/s 5,2 %</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/9b9a91ff8719f0fa1f716b114276e769706de63e">commit 9b9a91f</a> .</i> </p><br><p>  And the results of the AVX2 version on 16-bit coefficients relative to the AVX2 version on floating-point numbers: </p><br><pre> <code class="markdown hljs">Scale 2560√ó1600 RGB image to 320x200 bil 0.00682 s 600.15 Mpx/s 34.6 % to 320x200 bic 0.00990 s 413.86 Mpx/s 50.5 % to 320x200 lzs 0.01424 s 287.54 Mpx/s 60.6 % to 2048x1280 bil 0.03889 s 105.31 Mpx/s 7.6 % to 2048x1280 bic 0.04519 s 90.64 Mpx/s 11.3 % to 2048x1280 lzs 0.05226 s 78.38 Mpx/s 18.2 % to 5478x3424 bil 0.15195 s 26.96 Mpx/s 6.7 % to 5478x3424 bic 0.16977 s 24.13 Mpx/s 17.8 % to 5478x3424 lzs 0.20229 s 20.25 Mpx/s 15.6 %</code> </pre> <br><p>  <i>Result for <a href="https://github.com/uploadcare/pillow-simd/commit/3ad471834607acf98788cffebf49fdf948d2791c">commit 3ad4718</a> .</i> </p><br><h2 id="itogo">  Total </h2><br><p>  Thus, the transition to integer calculations gave a gain for both the scalar code and SIMD.  There is a slight performance regression in the SSE4 version when enlarging images using some filters.  But the fact is that the code presented here is quite different from what was included in the Pillow-SIMD 3.3 or 3.4 version - this is a kind of vinaigrette, an intermediate link, where some optimizations are applied, but others are not.  In real versions, there was no performance degradation. </p><br><p>  If you look back and remember the very first version, it turns out that the current code is 10-12 times faster on the same hardware.  What took 2 seconds can now be performed 5 times per second!  But if you look at the <a href="https://python-pillow.org/pillow-perf/">official benchmarks</a> , you can see that the actual performance of the Pillow-SIMD 3.4 with the AVX2 is still 2 times higher than it turned out by the end of this article.  So, there is a reason and material for the next part. </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/334790/">https://habr.com/ru/post/334790/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../334778/index.html">Basics of computer networks. Subject number 8. Link Aggregation Protocol: Etherchannel</a></li>
<li><a href="../334780/index.html">Register for the webinar "How to safely and profitably protect the company from unknown threats and cryptographers"</a></li>
<li><a href="../334782/index.html">As another large courier company, the personal data of its customers handed out</a></li>
<li><a href="../334786/index.html">MMO from scratch. Part 2. Building functionality + Diamond Square algorithm</a></li>
<li><a href="../334788/index.html">Offshore and foreign trade transactions: advantages and pitfalls</a></li>
<li><a href="../334792/index.html">Microsoft did not isolate Windows Defender in the sandbox, so I did</a></li>
<li><a href="../334794/index.html">Bluetooth mesh - network architecture and security</a></li>
<li><a href="../334796/index.html">How Android works, part 1</a></li>
<li><a href="../334798/index.html">1C and ETL</a></li>
<li><a href="../334800/index.html">DataGrip 2017.2: Supports Redshift and Azure, several databases in PostgreSQL, transaction control and more</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>