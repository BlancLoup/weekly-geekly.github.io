<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Introduction to reinforcement training: from a multi-armed gangster to a full-fledged RL agent</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hi, Habr! Reinforcement training is one of the most promising areas of machine learning. With its help, artificial intelligence today is able to solve...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Introduction to reinforcement training: from a multi-armed gangster to a full-fledged RL agent</h1><div class="post__text post__text-html js-mediator-article">  Hi, Habr!  Reinforcement training is one of the most promising areas of machine learning.  With its help, artificial intelligence today is able to solve a wide range of tasks: from robotics and video games to modeling the behavior of customers and healthcare.  In this introductory article we will explore the main idea of ‚Äã‚Äãreinforcement learning and build our own self-learning bot from scratch. <br><br><img src="https://habrastorage.org/webt/q9/gx/s6/q9gxs6zyezpxccjaexi8erd6lu4.jpeg"><br><a name="habracut"></a><br><h3>  Introduction </h3><br>  The main difference between learning with reinforcement (reinforcement learning) and classical machine learning is that artificial intelligence is trained in the process of interaction with the environment, and not on historical data.  Combining the ability of neural networks to restore complex interconnections and self-learning agent (system) in reinforcement learning, the machines achieved tremendous success, having won first in several <a href="https://deepmind.com/research/dqn/">Atari video games</a> , and then the world <a href="https://deepmind.com/research/alphago/">go</a> champion. <br><br>  If you are accustomed to working with the tasks of teaching with a teacher, then in case of reinforcement learning there is a slightly different logic.  Instead of creating an algorithm that learns to recruit ‚Äúfactor-correct answer‚Äù pairs, in reinforcement training, you must teach the agent to interact with the environment, independently generating these pairs.  Then he will be trained on them through the system of observations (observations), wins (reward) and actions (actions). 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      It is obvious that now at each moment in time we do not have a constant correct answer, so the task becomes a bit more cunning.  In this series of articles, we will create and train reinforcement training agents.  Let's start with the simplest version of the agent, so that the basic idea of ‚Äã‚Äãreinforcement learning is extremely clear, and then we move on to more complex tasks. <br><br><h3>  Multi-armed bandit </h3><br>  The simplest example of the task of training with reinforcements is the problem of a multi-armed gangster (it is widely covered in Habr√©, in particular, <a href="https://habrahabr.ru/company/surfingbird/blog/168611/">here</a> and <a href="https://habrahabr.ru/company/smartengines/blog/328764/">here</a> ).  In our formulation of the problem there are n slot machines, in each of which the probability of winning is fixed.  Then the agent's goal is to find the slot machine with the highest expected winnings and always choose it.  For simplicity, we will have only four slot machines from which to choose. <br><br>  In truth, this task can with a stretch be attributed to reinforcement learning, since the following properties are characteristic of tasks from this class: <br><br><ul><li>  <b>Different actions lead to different wins.</b>  For example, when searching for treasures in a maze, turning left may mean a bunch of diamonds, and turning right will mean a hole of poisonous snakes. </li><li>  <b>The agent receives the gain with a delay in time.</b>  This means that, turning left in the maze, we do not immediately realize that this is the right choice. </li><li>  <b>Winning depends on the current state of the system.</b>  Continuing the example above, turning left may be correct in the current part of the maze, but not necessarily in the rest. </li></ul><br>  In the problem of a multi-armed gang there is neither the second nor the third condition, which simplifies it significantly and allows us to concentrate only on identifying the optimal action from all possible options.  In the language of reinforcement learning, this means finding a ‚Äúrule of conduct‚Äù (policy).  We will use a method called policy gradients, in which the neural network updates its behavior rule as follows: the agent performs an action, receives feedback from the environment and on its basis adjusts the weights of the model through a gradient descent. <br><br>  In the field of training with reinforcements, there is another approach, in which agents teach value functions.  Instead of finding the optimal action in the current state, the agent learns to predict how profitable it is to be in this state and perform this action.  Both approaches give good results, but the logic of the policy gradient is more obvious. <br><br><h3>  Policy gradient </h3><br>  As we have already found out, in our case the expected gain of each of the gaming machines does not depend on the current state of the environment.  It turns out that our neural network will consist only of a set of scales, each of which corresponds to a single gaming machine.  These weights will determine which handle you need to pull to get the maximum gain.  For example, if all weights are initialized to 1, then the agent will be equally optimistic about winning in all slot machines. <br><br>  To update the weights of the model, we will use the e-greedy line of conduct.  This means that in most cases the agent will choose an action that maximizes the expected gain, but sometimes (with a probability equal to <i>e</i> ) the action will be random.  This will ensure that all possible options are selected, which will allow the neural network to ‚Äúlearn‚Äù more about each of them. <br><br>  Having performed one of the actions, the agent receives feedback from the system: 1 or -1, depending on whether he won.  This value is then used to calculate the loss function: <br><br><p></p><p><math></math><span class="MathJax_Preview" style="color: inherit; display: none;"></span><div class="MathJax_SVG_Display" style="text-align: center;"><span class="MathJax_SVG" id="MathJax-Element-1-Frame" tabindex="0" data-mathml="<math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>&amp;#x2212;</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=&quot;false&quot;>(</mo><mi>n</mi><mo stretchy=&quot;false&quot;>)</mo><mo>&amp;#x2217;</mo><mi>A</mi></math>" role="presentation" style="font-size: 100%; display: inline-block; position: relative;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="19.877ex" height="2.66ex" viewBox="0 -832 8558 1145.2" role="img" focusable="false" style="vertical-align: -0.728ex;" aria-hidden="true"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-4C" x="0" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-6F" x="681" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-73" x="1167" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-73" x="1636" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMAIN-3D" x="2383" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMAIN-2212" x="3440" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-6C" x="4218" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-6F" x="4517" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-67" x="5002" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMAIN-28" x="5483" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-6E" x="5872" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMAIN-29" x="6473" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMAIN-2217" x="7084" y="0"></use><use xlink:href="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://habr.com/ru/company/newprolab/blog/343834/&amp;xid=17259,15700022,15700186,15700190,15700248,15700253&amp;usg=ALkJrhgfGmPfppzfDxBvk7lQ9YLXtgPWlg#MJMATHI-41" x="7807" y="0"></use></g></svg><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>L</mi><mi>o</mi><mi>s</mi><mi>s</mi><mo>=</mo><mo>‚àí</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo><mo>‚àó</mo><mi>A</mi></math></span></span></div><script type="math/tex;mode=display" id="MathJax-Element-1"> Loss = -log (n) * A </script></p><br>  <i>A (advantage)</i> is an important element of all learning algorithms with reinforcement.  It shows how perfect the action is better than a certain baseline.  In the future, we will use a more complex baseline, but for now we take it to be 0, that is, <i>A</i> will simply be equal to the reward for each action (1 or -1).  <i>n</i> is the rule of behavior, the weight of the neural network corresponding to the handle of the slot machine, which we have chosen at the current step. <br><br>  It is intuitively clear that the loss function should take such values ‚Äã‚Äãthat the weights of the actions that led to the gain increase, and those that lead to the loss decrease.  As a result, the weights will be updated, and the agent will increasingly choose the gaming machine with the highest fixed probability of winning, until finally he will always choose it. <br><br><h3>  Algorithm implementation </h3><br>  <b>Bandits</b>  First, we will create our gangsters (in everyday life the slot machine is called a gangster).  In our example, they will be 4. The <i>pullBandit</i> function generates a random number from the standard normal distribution, and then compares it with the value of the gangster and returns the result of the game.  The farther on the list is the gangster, the greater the likelihood that the agent will win by choosing him.  Thus, we want our agent to learn to <b>always</b> choose the last gangster. <br><br><pre><code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-comment"><span class="hljs-comment">#  .  ‚Ññ4    . bandits = [0.2,0,-0.2,-5] num_bandits = len(bandits) def pullBandit(bandit): #   result = np.random.randn(1) if result &gt; bandit: # return 1 else: # return -1</span></span></code> </pre> <br>  <b>Agent.</b>  The piece of code below creates our simple agent, which consists of a set of values ‚Äã‚Äãfor thugs.  Each value corresponds to a win / loss depending on the choice of a gangster.  To update agent weights, we use a policy gradient, that is, we choose actions that minimize the loss function: <br><br><pre> <code class="python hljs">tf.reset_default_graph() <span class="hljs-comment"><span class="hljs-comment"># 2   feed-forward  .     . weights = tf.Variable(tf.ones([num_bandits])) chosen_action = tf.argmax(weights,0) # 6    .        ,        . reward_holder = tf.placeholder(shape=[1],dtype=tf.float32) action_holder = tf.placeholder(shape=[1],dtype=tf.int32) responsible_weight = tf.slice(weights,action_holder,[1]) loss = -(tf.log(responsible_weight)*reward_holder) optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) update = optimizer.minimize(loss)</span></span></code> </pre> <br>  <b>Agent training.</b>  We will train the agent, by choosing certain actions and receiving winnings / losses.  Using the obtained values, we will know exactly how to update the weights of the model in order to more often choose gangsters with a large expected gain: <br><br><pre> <code class="python hljs">total_episodes = <span class="hljs-number"><span class="hljs-number">1000</span></span> <span class="hljs-comment"><span class="hljs-comment">#   total_reward = np.zeros(num_bandits) #     0 e = 0.1 #   init = tf.global_variables_initializer() #  tensorflow with tf.Session() as sess: sess.run(init) i = 0 while i &lt; total_episodes: #        if np.random.rand(1) &lt; e: action = np.random.randint(num_bandits) else: action = sess.run(chosen_action) #  ,     reward = pullBandit(bandits[action]) #  _,resp,ww = sess.run([update,responsible_weight,weights], feed_dict={reward_holder:[reward],action_holder:[action]}) #     total_reward[action] += reward if i % 50 == 0: print("     " + str(num_bandits) + " bandits: " + str(total_reward)) i+=1 print(" ,   ‚Ññ" + str(np.argmax(ww)+1) + " ...") if np.argmax(ww) == np.argmax(-np.array(bandits)): print("...  !") else: print("...   !")</span></span></code> </pre> <br>  Result: <br><br><pre> <code class="python hljs">   <span class="hljs-number"><span class="hljs-number">4</span></span>   : [<span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">45.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">91.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-3.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">138.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">2.</span></span> <span class="hljs-number"><span class="hljs-number">183.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">2.</span></span> <span class="hljs-number"><span class="hljs-number">229.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">277.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">4.</span></span> <span class="hljs-number"><span class="hljs-number">323.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">2.</span></span> <span class="hljs-number"><span class="hljs-number">370.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">416.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">464.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-3.</span></span> <span class="hljs-number"><span class="hljs-number">-1.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">508.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-7.</span></span> <span class="hljs-number"><span class="hljs-number">1.</span></span> <span class="hljs-number"><span class="hljs-number">4.</span></span> <span class="hljs-number"><span class="hljs-number">549.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-9.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">3.</span></span> <span class="hljs-number"><span class="hljs-number">593.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-9.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">4.</span></span> <span class="hljs-number"><span class="hljs-number">640.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-9.</span></span> <span class="hljs-number"><span class="hljs-number">0.</span></span> <span class="hljs-number"><span class="hljs-number">5.</span></span> <span class="hljs-number"><span class="hljs-number">687.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-9.</span></span> <span class="hljs-number"><span class="hljs-number">-2.</span></span> <span class="hljs-number"><span class="hljs-number">5.</span></span> <span class="hljs-number"><span class="hljs-number">735.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-9.</span></span> <span class="hljs-number"><span class="hljs-number">-4.</span></span> <span class="hljs-number"><span class="hljs-number">7.</span></span> <span class="hljs-number"><span class="hljs-number">781.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-10.</span></span> <span class="hljs-number"><span class="hljs-number">-4.</span></span> <span class="hljs-number"><span class="hljs-number">8.</span></span> <span class="hljs-number"><span class="hljs-number">829.</span></span>]    <span class="hljs-number"><span class="hljs-number">4</span></span>   : [ <span class="hljs-number"><span class="hljs-number">-13.</span></span> <span class="hljs-number"><span class="hljs-number">-4.</span></span> <span class="hljs-number"><span class="hljs-number">7.</span></span> <span class="hljs-number"><span class="hljs-number">875.</span></span>]  ,   ‚Ññ<span class="hljs-number"><span class="hljs-number">4</span></span> ... ...  !</code> </pre> <br>  Full Jupyter Notebook can be downloaded <a href="https://gist.github.com/awjuliani/902fe41c3a9efe27299e72aee1b3158c">here</a> . <br><br><h3>  The solution to the full-fledged learning problem with reinforcement </h3><br>  Now, when we know how to create an agent capable of choosing the optimal solution from several possible ones, let's move on to a more complex task, which will be an example of full-fledged reinforcement learning: evaluating the current state of the system, the agent must choose actions that maximize the gain not only now but in the future. <br><br>  Systems in which reinforcement learning can be solved are called Markov Decision Processes (Markov Decision Processes, MDP).  Such systems are characterized by winnings and actions that ensure the transition from one state to another, and these winnings depend on the current state of the system and the decision that the agent takes in this state.  Winnings can be obtained with a delay in time. <br><br>  Formally, the Markov decision-making process can be defined as follows.  The MDP consists of a set of all possible states <i>S</i> and actions <i>A</i> , and at each moment of time it is in the state <i>s</i> and performs the action <i>a</i> of these sets.  Thus, a tuple is given <i>(s, a)</i> and for it <i>T (s, a)</i> is defined - the probability of transition to the new state <i>s'</i> and <i>R (s, a)</i> is the gain.  As a result, at any time in the MDP, the agent is in the state <i>s</i> , decides <i>a</i> and receives the new state <i>s'</i> and the gain <i>r</i> in response. <br><br>  For example, even the process of opening the door can be represented as a Markov decision-making process.  The state will be our view of the door, as well as the location of our body and the door in the world.  All possible body movements that we can do are set <i>A</i> , and winning is the successful opening of the door.  Certain actions (for example, a step in the direction of the door) bring us closer to achieving the goal, but they themselves do not bring any gain, since it is provided only by directly opening the door.  As a result, the agent must perform such actions, which sooner or later lead to the solution of the problem. <br><br><h3>  The task of stabilizing an inverted pendulum </h3><br>  Let's use OpenAI Gym - a platform for developing and training AI bots using games and algorithmic tests and take the classic task from there: the task of stabilizing an inverted pendulum or Cart-Pole.  In our case, the essence of the task is to keep the rod in a vertical position as long as possible, moving the cart horizontally: <br><img src="https://habrastorage.org/webt/gw/7u/vy/gw7uvynwumvekyrdz70jo13lt_u.gif"><br><br>  Unlike the multi-armed gangster problem, this system has: <br><br><ul><li>  <b>Observations.</b>  The agent must know where the rod is now and at what angle.  This observation will be used by the neural network to assess the likelihood of a particular action. </li><li>  <b>Delayed winnings</b>  It is necessary to move the cart in such a way that it is beneficial both at the moment and in the future.  To do this, we will compare the pair ‚Äúobservation - action‚Äù with the adjusted value of the gain.  Adjustment is performed by a function that weighs actions by time. </li></ul><br>  To take into account the time lag, we need to use the policy gradient method with some corrections.  First, it is now necessary to update an agent that has more than one observation per unit of time.  To do this, we will collect all the observations in the buffer, and then use them simultaneously to update the model weights.  This set of observations per unit of time is then compared with the <i>discounted</i> gain. <br><br>  Thus, every action of the agent will be made taking into account not only the instant win, but all subsequent ones.  Also now we will use the adjusted gain as an estimate of the <i>A (advantage)</i> element in the loss function. <br><br><h3>  Algorithm implementation </h3><br>  Import the libraries and load the Cart-Pole task environment: <br><br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> tf <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> tensorflow.contrib.slim <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> slim <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> gym <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> matplotlib.pyplot <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> plt %matplotlib inline <span class="hljs-keyword"><span class="hljs-keyword">try</span></span>: xrange = xrange <span class="hljs-keyword"><span class="hljs-keyword">except</span></span>: xrange = range env = gym.make(<span class="hljs-string"><span class="hljs-string">'CartPole-v0'</span></span>) <span class="hljs-comment"><span class="hljs-comment">#  </span></span></code> </pre> <br>  <b>Agent.</b>  First, create a function that will discount all subsequent wins to the current moment: <br><br><pre> <code class="python hljs">gamma = <span class="hljs-number"><span class="hljs-number">0.99</span></span> <span class="hljs-comment"><span class="hljs-comment">#   def discount_rewards(r): """     ,    """ discounted_r = np.zeros_like(r) running_add = 0 for t in reversed(xrange(0, r.size)): running_add = running_add * gamma + r[t] discounted_r[t] = running_add return discounted_r</span></span></code> </pre> <br>  Now create our agent: <br><br><pre> <code class="python hljs"><span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">class</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">agent</span></span></span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">()</span></span></span><span class="hljs-class">:</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">__init__</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(self, lr, s_size,a_size,h_size)</span></span></span><span class="hljs-function">:</span></span> <span class="hljs-comment"><span class="hljs-comment">#  feed-forward  . #       self.state_in= tf.placeholder(shape=[None,s_size],dtype=tf.float32) hidden = slim.fully_connected(self.state_in,h_size, biases_initializer=None,activation_fn=tf.nn.relu) self.output = slim.fully_connected(hidden,a_size, activation_fn=tf.nn.softmax,biases_initializer=None) self.chosen_action = tf.argmax(self.output,1) #   # 6    . #      #   , #       . self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32) self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32) self.indexes = tf.range(0, tf.shape(self.output)[0])*tf.shape(self.output)[1] + self.action_holder self.responsible_outputs = tf.gather(tf.reshape(self.output, [-1]), self.indexes) #  self.loss = -tf.reduce_mean(tf.log(self.responsible_outputs)* self.reward_holder) tvars = tf.trainable_variables() self.gradient_holders = [] for idx,var in enumerate(tvars): placeholder = tf.placeholder(tf.float32,name=str(idx)+'_holder') self.gradient_holders.append(placeholder) self.gradients = tf.gradients(self.loss,tvars) optimizer = tf.train.AdamOptimizer(learning_rate=lr) self.update_batch = optimizer.apply_gradients(zip(self.gradient_holders, tvars))</span></span></code> </pre> <br>  <b>Agent training.</b>  Now, finally, let's get to the agent's training: <br><br><pre> <code class="python hljs">tf.reset_default_graph() <span class="hljs-comment"><span class="hljs-comment">#  tensorflow myAgent = agent(lr=1e-2,s_size=4,a_size=2,h_size=8) #  total_episodes = 5000 #   max_ep = 999 update_frequency = 5 init = tf.global_variables_initializer() #  tensorflow with tf.Session() as sess: sess.run(init) i = 0 total_reward = [] total_lenght = [] gradBuffer = sess.run(tf.trainable_variables()) for ix,grad in enumerate(gradBuffer): gradBuffer[ix] = grad * 0 while i &lt; total_episodes: s = env.reset() running_reward = 0 ep_history = [] for j in range(max_ep): #    ,   a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]}) a = np.random.choice(a_dist[0],p=a_dist[0]) a = np.argmax(a_dist == a) s1,r,d,_ = env.step(a) #     ep_history.append([s,a,r,s1]) s = s1 running_reward += r if d == True: #  ep_history = np.array(ep_history) ep_history[:,2] = discount_rewards(ep_history[:,2]) feed_dict = {myAgent.reward_holder:ep_history[:,2], myAgent.action_holder:ep_history[:,1], myAgent.state_in:np.vstack(ep_history[:,0])} grads = sess.run(myAgent.gradients, feed_dict=feed_dict) for idx,grad in enumerate(grads): gradBuffer[idx] += grad if i % update_frequency == 0 and i != 0: feed_dict = dictionary = dict(zip(myAgent.gradient_holders, gradBuffer)) _ = sess.run(myAgent.update_batch, feed_dict=feed_dict) for ix,grad in enumerate(gradBuffer): gradBuffer[ix] = grad * 0 total_reward.append(running_reward) total_lenght.append(j) break #   if i % 100 == 0: print(np.mean(total_reward[-100:])) i += 1</span></span></code> </pre> <br>  Result: <br><br><pre> <code class="python hljs"><span class="hljs-number"><span class="hljs-number">16.0</span></span> <span class="hljs-number"><span class="hljs-number">21.47</span></span> <span class="hljs-number"><span class="hljs-number">25.57</span></span> <span class="hljs-number"><span class="hljs-number">38.03</span></span> <span class="hljs-number"><span class="hljs-number">43.59</span></span> <span class="hljs-number"><span class="hljs-number">53.05</span></span> <span class="hljs-number"><span class="hljs-number">67.38</span></span> <span class="hljs-number"><span class="hljs-number">90.44</span></span> <span class="hljs-number"><span class="hljs-number">120.19</span></span> <span class="hljs-number"><span class="hljs-number">131.75</span></span> <span class="hljs-number"><span class="hljs-number">162.65</span></span> <span class="hljs-number"><span class="hljs-number">156.48</span></span> <span class="hljs-number"><span class="hljs-number">168.18</span></span> <span class="hljs-number"><span class="hljs-number">181.43</span></span></code> </pre> <br>  You can see the complete Jupyter Notebook <a href="https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb">here</a> .  See you in the next articles, where we will continue to study reinforced learning! </div><p>Source: <a href="https://habr.com/ru/post/343834/">https://habr.com/ru/post/343834/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../343824/index.html">MySQL and partitioning</a></li>
<li><a href="../343826/index.html">Russia's first mitap on Apache Ignite, December 12</a></li>
<li><a href="../343828/index.html">Writing a simple Linux kernel module</a></li>
<li><a href="../343830/index.html">Dependency injection in .Net Mark Siman 1 - Dependencies between application layers</a></li>
<li><a href="../343832/index.html">Designing a system for reading data from input devices (Part Two)</a></li>
<li><a href="../343836/index.html">EU GDPR: Compliance with the requirements of regulators in the field of cloud computing</a></li>
<li><a href="../343838/index.html">Analyze it - Lenta.ru</a></li>
<li><a href="../343840/index.html">Renga development team: how we reached the idyll, working without managers</a></li>
<li><a href="../343842/index.html">WD (Western Digital) plans to create a 40 TB disk, but is this enough?</a></li>
<li><a href="../343844/index.html">How programmers with PVS-Studio looked for project errors</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>