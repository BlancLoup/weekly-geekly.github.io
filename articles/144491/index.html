<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Who's there? - Identification of the person by voice</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello, dear reader! 

 I bring to your attention an interesting and informative article about a particular method of recognizing the speaker. Just a c...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Who's there? - Identification of the person by voice</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage2/cc1/8c0/6d0/cc18c06d072410212ac3527e8ea3ebb1.jpg"><br><br>  Hello, dear reader! <br><br>  I bring to your attention an interesting and informative article about a particular method of recognizing the speaker.  Just a couple of months ago, I came across an <a href="http://habrahabr.ru/post/140828/">article</a> on the use of chalk-cepstral coefficients for speech recognition.  She did not find a response, probably because of insufficient structuring, although the material in it is very interesting.  I will take the responsibility to bring this material in an accessible form and continue the topic of speech recognition in Habr√©. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Under the cut, I will describe the whole process of identifying a person by voice, from recording and processing the sound to directly determining the identity of the speaker. <a name="habracut"></a><br><br><h4>  Sound recording </h4><br>  Our story begins with the recording of an analog signal from an external source using a microphone.  As a result of this operation, we obtain a set of values ‚Äã‚Äãthat correspond to a change in the amplitude of a sound with time.  This coding principle is called <a href="http://ru.wikipedia.org/wiki/%25D0%2598%25D0%25BC%25D0%25BF%25D1%2583%25D0%25BB%25D1%258C%25D1%2581%25D0%25BD%25D0%25BE-%25D0%25BA%25D0%25BE%25D0%25B4%25D0%25BE%25D0%25B2%25D0%25B0%25D1%258F_%25D0%25BC%25D0%25BE%25D0%25B4%25D1%2583%25D0%25BB%25D1%258F%25D1%2586%25D0%25B8%25D1%258F">pulse code modulation</a> aka PCM (Pulse-code modulation).  As you might guess, the ‚Äúraw‚Äù data obtained from the audio stream is not yet suitable for our purposes.  The first step is to convert the naughty bits into a set of meaningful values ‚Äã‚Äã- signal amplitudes.  [1, p.  31] As input, I will use an uncompressed 16-bit signed (PCM-signed) wav file with a sampling frequency of 16 kHz. <br><br><pre><code class="cs hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">double</span></span></span><span class="hljs-function">[] </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">readAmplitudeValues</span></span></span><span class="hljs-function">(</span><span class="hljs-params"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-params"><span class="hljs-keyword">bool</span></span></span></span><span class="hljs-function"><span class="hljs-params"> isBigEndian</span></span></span><span class="hljs-function">)</span></span> { <span class="hljs-keyword"><span class="hljs-keyword">int</span></span> MSB, LSB; <span class="hljs-comment"><span class="hljs-comment">//     byte[] buffer = ReadDataFromExternalSource(); //   - double[] data = new double[buffer.length / 2]; for (int i = 0; i &lt; buffer.length; i += 2) { if(isBigEndian) //       { //    MSB MSB = buffer[2 * i]; //    LSB LSB = buffer[2 * i + 1]; } else { //  LSB = buffer[2 * i]; MSB = buffer[2 * i + 1]; } //   ,   16-   //       - 2^15 data[i] = ((MSB &lt;&lt; 8) || LSB) / 32768; } return data; }</span></span></code> </pre> <br>  Refresh the knowledge about the order of bytes can be on <a href="http://ru.wikipedia.org/wiki/%25D0%259F%25D0%25BE%25D1%2580%25D1%258F%25D0%25B4%25D0%25BE%25D0%25BA_%25D0%25B1%25D0%25B0%25D0%25B9%25D1%2582%25D0%25BE%25D0%25B2">Wikipedia</a> . <br><br><h4>  Sound processing </h4><br>  The obtained amplitude values ‚Äã‚Äãmay not coincide even for two identical records due to external noise, different input loudness and other factors.  <a href="http://ru.wikipedia.org/wiki/%25D0%259D%25D0%25BE%25D1%2580%25D0%25BC%25D0%25B0%25D0%25BB%25D0%25B8%25D0%25B7%25D0%25B0%25D1%2586%25D0%25B8%25D1%258F_%25D0%25B7%25D0%25B2%25D1%2583%25D0%25BA%25D0%25B0">Normalization is</a> used to bring the sounds to a ‚Äúcommon denominator‚Äù.  The idea of ‚Äã‚Äãpeak normalization is simple: divide all amplitude values ‚Äã‚Äãby the maximum (within a given sound file).  Thus, we equated speech patterns recorded at different loudness, putting everything in the scale from -1 to 1. It is important that after such a transformation, any sound completely fills a given interval. <br><br>  Normalization, in my opinion, is the simplest and most effective algorithm for sound preprocessing.  There are also a lot of others: ‚Äúcutting off‚Äù frequencies above or below a given one, smoothing, etc. <br><br><h4>  Divide and rule </h4><br>  Even when working with sound with a minimum sufficient sampling rate (16 kHz), the size of the unique characteristics for the second sound sample is simply huge - 16000 amplitude values.  It is not possible to perform any complicated operations on such data volumes.  In addition, it is not entirely clear how to compare objects with different numbers of unique features. <br><br>  To begin with, we will reduce the computational complexity of the problem, breaking it up into smaller subtasks.  With this move we kill two birds with one stone, because by setting a fixed size of the subtasks and averaging the results of calculations for all the tasks, we will get a predetermined number of signs for classification. <br><img src="http://habrastorage.org/storage2/414/d03/cf3/414d03cf34160212ad032cabe5f56fde.png"><br>  The figure shows the "cutting" of the sound signal into frames of length N with half overlap.  The need for overlap caused by the distortion of the sound if the frames were located nearby.  Although in practice this technique is often neglected to save computational resources.  Following the recommendations [1, p.  28], choose a frame length of 128 ms, as a compromise between accuracy (long frames) and speed (short frames).  The rest of the speech, which does not occupy a full frame, can be filled with zeros to the desired size, or simply discarded. <br><br>  To eliminate undesirable effects in the further processing of frames, multiply each element of the frame by a special weight function (‚Äúwindow‚Äù).  The result will be the selection of the central part of the frame and the smooth attenuation of the amplitudes at its edges.  This is necessary to achieve the best results in the <a href="http://www.dsplib.ru/content/dft/dft.html">Fourier transform</a> , since it is oriented towards an infinitely repetitive signal.  Accordingly, our frame should dock with itself and as smoothly as possible.  There are a <a href="http://en.wikipedia.org/wiki/Window_function">great many</a> windows.  We will use the Hamming window. <br><img src="http://habrastorage.org/storage2/38c/bf6/fe5/38cbf6fe51ae1d5663de59dd2068c3b4.png"><br>  <i>n is the sequence number of the element in the frame for which the new amplitude value is calculated</i> <i><br></i>  <i>N - as before, the frame length (the number of signal values ‚Äã‚Äãmeasured during the period)</i> <br><br><h4>  Discrete Fourier Transform </h4><br>  The next step is to obtain a short-term spectrogram of each frame separately.  For these purposes we use the <a href="http://ru.wikipedia.org/wiki/%25D0%2594%25D0%25B8%25D1%2581%25D0%25BA%25D1%2580%25D0%25B5%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B5_%25D0%25BF%25D1%2580%25D0%25B5%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D0%25B7%25D0%25BE%25D0%25B2%25D0%25B0%25D0%25BD%25D0%25B8%25D0%25B5_%25D0%25A4%25D1%2583%25D1%2580%25D1%258C%25D0%25B5">discrete Fourier transform</a> . <br><img src="http://habrastorage.org/storage2/fd8/9fa/dad/fd89fadad17dab81b8507f289a5f8a52.png"><br>  <i>N - as before, the frame length (the number of signal values ‚Äã‚Äãmeasured during the period)</i> <i><br></i>  <i>x <sub>n</sub> - the amplitude of the n-th signal</i> <i><br></i>  <i>X <sub>k</sub> - N complex amplitudes of sinusoidal signals composing the original signal</i> <br><br>  In addition, we will square each value of <i>X <sub>k</sub></i> for further logarithmization. <br><br><h4>  Go to chalk scale </h4><br>  To date, the most successful are voice recognition systems that use knowledge about the structure of the hearing aid.  A few words about this <a href="http://habrahabr.ru/post/64681/">are on Habr√©</a> .  In short, the ear interprets sounds not linearly, but on a logarithmic scale.  Until now, we have performed all the operations on the ‚ÄúHertz‚Äù, now we will move on to the ‚ÄúMelam‚Äù.  To visualize the dependence will help drawing. <br><img src="http://habrastorage.org/storage2/b2e/116/b02/b2e116b0214feab318e7efb01352d029.png"><br>  As can be seen, the chalk scale behaves linearly up to 1000 Hz, and then it shows a logarithmic nature.  The transition to a new scale is described by a simple dependency. <br><img src="http://habrastorage.org/storage2/d8e/07b/093/d8e07b093abf60333dfee2c73000e9dc.png"><br>  <i>m is the frequency in chalk</i> <i><br></i>  <i>f - frequency in hertz</i> <br><br><h4>  Getting feature vector </h4><br>  Now we are closer than ever to our goal.  The feature vector will consist of those very small cepstral coefficients.  We calculate them by the formula [2] <br><img src="http://habrastorage.org/storage2/3eb/1ae/f07/3eb1aef07d8dfe638a346c71a5f849c0.png"><br>  <i>c <sub>n</sub> - chalk-cepstral coefficient n</i> <i><br></i>  <i>S <sub>k</sub> - the amplitude of the k-th value in the frame in chalk</i> <i><br></i>  <i>K - advance the specified number of chalk-core coefficients</i> <i><br></i>  <i>n ‚àà [1, K]</i> <br><br>  As a rule, the number <i>K is</i> chosen equal to 20 and start counting from 1 due to the fact that the coefficient <i>c <sub>0</sub></i> carries little information about the speaker, since it is, in fact, averaging the amplitudes of the input signal.  [2] <br><br><h4>  So who did say all the same? </h4><br>  The last stage is the classification of the speaker.  The classification is made by calculating the measure of similarity of the test data and those already known.  The measure of similarity is expressed by the distance from the vector of signs of the test signal to the vector of signs already classified.  We will be interested in the simplest solution - the <a href="http://en.wikipedia.org/wiki/Manhattan_distance">distance of city blocks</a> . <br><img src="http://habrastorage.org/storage2/f92/644/bc2/f92644bc288885963de29645db73ea7c.png"><br>  This solution is more suitable for vectors of discrete nature, in contrast to the Euclidean distance. <br><br>  An attentive reader certainly remembers that the author at the beginning of the article mentioned the averaging of features of speech frames.  So, filling this gap, I conclude the article with a description of the algorithm for finding the average feature vector for several frames and several speech patterns. <br><br><h4>  Clustering </h4><br>  Finding a feature vector for one sample is not difficult: such a vector is represented as an arithmetic average of the vectors characterizing individual frames of speech.  To improve the recognition accuracy, it is simply necessary to average the results not only between frames, but also to take into account the performance of several speech samples.  Having several voice recordings, it is reasonable not to average the indicators to one vector, but to clusterize, for example, using the <a href="http://ru.wikipedia.org/wiki/K-means">k-</a> means method. <br><br><h4>  Results </h4><br>  Thus, I talked about a simple but effective system for identifying a person by voice.  Summarizing, the recognition process is constructed as follows: <br><ol><li>  We collect several speech training samples, the more - the better. </li><li>  We find for each of them a characteristic feature vector. </li><li>  For samples with a known author, we perform clustering with one center (averaging) or several.  Acceptable results begin with the use of 4 centers for each speaker.  [2] </li><li>  In the identification mode, we find the distance from the test vector to the cluster centers studied during the training.  Which cluster test speech will be closer - to such a speaker and refer the sample. </li><li>  It is possible to experimentally establish even a certain confidence interval ‚Äî the maximum distance at which the test sample may be located from the center of the cluster.  If this value is exceeded, classify the sample as unknown. </li></ol><br><br>  I always welcome helpful comments about improving the material.  Thanks for attention. <br><br><h4>  Literature: </h4><br><ol><li>  <a href="http://marf.sourceforge.net/docs/marf/0.3.0.6/report.pdf">Modular Audio Recognition Framework v.0.3.0.6 (0.3.0final) and its Applications</a> </li><li>  <a href="http://www.assembla.com/spaces/strojno_ucenje_lab/documents/bikvce70wr3r6zeje5avnr/download/p141omfccu.pdf">Speaker identification using mel frequency cepstral coefficients</a> </li></ol></div><p>Source: <a href="https://habr.com/ru/post/144491/">https://habr.com/ru/post/144491/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../144486/index.html">Mind games. We understand with Intel HD graphics. And play?</a></li>
<li><a href="../144487/index.html">Steps to optimize capture pages</a></li>
<li><a href="../144488/index.html">We write REST application on Sinatra and we fasten Redactor. Part 2</a></li>
<li><a href="../144489/index.html">thn.gs - in order not to remember</a></li>
<li><a href="../144490/index.html">Practical SMM or how NOT to conduct contests in social networks</a></li>
<li><a href="../144492/index.html">Google Drive may lose its logo</a></li>
<li><a href="../144494/index.html">Facebook launches its photo sharing application. Instagram has nothing to do with it</a></li>
<li><a href="../144495/index.html">How I bought a 40W laser engraver in China + some theory of CO2 lasers</a></li>
<li><a href="../144497/index.html">No one reads the rules.</a></li>
<li><a href="../144498/index.html">Three helpful Rails console tips</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>