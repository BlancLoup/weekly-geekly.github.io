<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Multilayer perceptron (with an example in PHP)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Reading Habr for materials on neural networks and in general on the topic of artificial intelligence, I found a post about a single-layer perceptron a...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Multilayer perceptron (with an example in PHP)</h1><div class="post__text post__text-html js-mediator-article">  Reading Habr for materials on neural networks and in general on the topic of artificial intelligence, I found a post about a <a href="http://habrahabr.ru/blogs/artificial_intelligence/40659/">single-layer perceptron</a> and decided out of curiosity to start studying neural networks with it, and then expand the experience to a multi-layer perceptron.  What and narrate. <a name="habracut"></a><br><br><h4>  Theory </h4><br>  The multilayer perceptron is well described in the <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE%25D1%2581%25D0%25BB%25D0%25BE%25D0%25B9%25D0%25BD%25D1%258B%25D0%25B9_%25D0%25BF%25D0%25B5%25D1%2580%25D1%2586%25D0%25B5%25D0%25BF%25D1%2582%25D1%2580%25D0%25BE%25D0%25BD_%25D0%25A0%25D1%2583%25D0%25BC%25D0%25B5%25D0%25BB%25D1%258C%25D1%2585%25D0%25B0%25D1%2580%25D1%2582%25D0%25B0">Wiki</a> , but only the structure is described there, but we will try it in practice, along with the learning algorithm.  By the way, it is <a href="http://ru.wikipedia.org/wiki/%25D0%259C%25D0%25B5%25D1%2582%25D0%25BE%25D0%25B4_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">also described in the Wiki</a> , although comparing it with several other sources (books and <a href="http://www.aiportal.ru/">aiportal.ru</a> ), I found several problem areas here and there. <br>  So, the multilayer perceptron is a neural network consisting of layers, each of which consists of elements - neurons (more precisely, their models).  These elements are of three types: sensory (input, S), associative (trained ‚Äúhidden‚Äù layers, A) and responsive (output, R).  This type of perceptrons is called multi-layered not because it consists of several layers, because the input and output layers can not be made out in code at all, but because it contains <b>several</b> (usually no more than two or three) <b>trained (A) layers</b> . <br>  A neuron model (we will call it just a neuron) is a network element that has several inputs, each of which has weight.  A neuron, receiving a signal, multiplies the signals by weights and sums the resulting values, and then transmits the result to another neuron or to the output of the network.  Here, too, the multilayer perceptron is different.  Its function is <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25B3%25D0%25BC%25D0%25BE%25D0%25B8%25D0%25B4">sigmoid</a> , it gives values ‚Äã‚Äãon the interval from 0 to 1. Several functions belong to <a href="http://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25B8%25D0%25B3%25D0%25BC%25D0%25BE%25D0%25B8%25D0%25B4">sigmoids</a> , we will mean a <a href="http://ru.wikipedia.org/wiki/%25D0%259B%25D0%25BE%25D0%25B3%25D0%25B8%25D1%2581%25D1%2582%25D0%25B8%25D1%2587%25D0%25B5%25D1%2581%25D0%25BA%25D0%25B0%25D1%258F_%25D1%2584%25D1%2583%25D0%25BD%25D0%25BA%25D1%2586%25D0%25B8%25D1%258F">logistic function</a> .  During the examination of the method, you will understand why it is so good. <br>  Several layers that can be trained (more precisely, adjust) allow you to approximate very complex nonlinear functions, that is, their scope is wider than single-layer ones. <br><br><h4>  We try in practice </h4><br>  We will immediately shift the theory to practice so that we can remember better and everyone could try. <br>  I recommend reading the <a href="http://habrahabr.ru/blogs/artificial_intelligence/40659/">above post</a> , if you are not in neural networks, of course. <br>  So, take a simple task - to recognize the numbers without turning and distortion.  In such a task, a multi-layer perceptron will be sufficient; moreover, it is less sensitive to noise. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h5>  Network structure </h5><br>  The network will have two hidden layers, 5 times smaller in size than the input.  That is, if we have 20 inputs, then on the hidden layers there will be 4 neurons each.  In the case of this task, I allow myself the courage to select the number of layers and neurons empirically.  Layers take 2, with an increase in the number of layers, the result does not improve. <br><br><h5>  Learning algorithm </h5><br>  The training of neural networks of the chosen type is carried out using <a href="http://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">the error back-propagation algorithm</a> .  That is, if, when we answer, the layers transmit a signal to the output of the network, then we will compare the response of the neural network with the correct one and calculate the error, which then goes up through the network - from the outputs to the inputs. <br><br>  We will estimate the network error as half the sum of squares of the differences in the signals at the outputs.  More simply: we divide in half the sum of i such expressions here: (ti - oi) ^ 2, where ti is the value of the i-th signal in the correct answer, and oi is the value of the i-th output of the neural network.  That is, we summarize the squares of the errors on the inputs and divide everything in half.  If this error (in the example code is $ d) is large enough (does not fit into the accuracy we need), we rule the weights of the neurons. <br><br>  Formulas for the amendment of the scales are fully posted on the <a href="http://ru.wikipedia.org/wiki/%25D0%2590%25D0%25BB%25D0%25B3%25D0%25BE%25D1%2580%25D0%25B8%25D1%2582%25D0%25BC_%25D0%25BE%25D0%25B1%25D1%2580%25D0%25B0%25D1%2582%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D1%2580%25D0%25B0%25D1%2581%25D0%25BF%25D1%2580%25D0%25BE%25D1%2581%25D1%2582%25D1%2580%25D0%25B0%25D0%25BD%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F_%25D0%25BE%25D1%2588%25D0%25B8%25D0%25B1%25D0%25BA%25D0%25B8">wiki</a> , I will not repost.  I just want to note that I tried to literally repeat the formulas so that it would be clear how it is in practice.  Here comes the advantage of choosing the activation function ‚Äî it has a simple derivative (œÉ '(x) = œÉ (x) * (1-œÉ (x))), and it is used to correct the weights.  The weights of each layer are adjusted separately.  That is, layer by layer from the last to the first.  And here I made a mistake, at first I corrected the weights on each example separately, and the neural network learned to solve only one ‚Äúexample‚Äù.  It is correct in such an algorithm to give all the examples of a training sample to the inputs in turn, this is called an <b>epoch</b> .  And only with the end of an epoch to count an error (total for all examples of the sample) and to adjust weights. <br><br>  During training, jumps in errors are possible, but it happens.  The selection of the coefficients Œ± (determines the effect of weights on training) and Œ∑ (determines the influence of the correction value Œ¥) is very important - the rate of convergence and falling into local extrema depend on it.  I consider Œ± = 0.7 and Œ∑ = 0.001 to be the most universal, although try to play with them: an increase in Œ± and Œ∑ speeds up learning, but we can fly a minimum. <br><br>  Next, lay out <a href="http://zalil.ru/30770156">an example in PHP</a> .  The code is far from ideal, but it performs its tasks. </div><p>Source: <a href="https://habr.com/ru/post/116613/">https://habr.com/ru/post/116613/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../116608/index.html">In the project OpenStreetMap there is a serious decline, the dissolution is inevitable</a></li>
<li><a href="../116609/index.html">To all girls undeservedly injured on March 8! :)</a></li>
<li><a href="../116610/index.html">Social networks + LETS = New development horizons</a></li>
<li><a href="../116611/index.html">siteTarget - a new counter that measures calls for a website</a></li>
<li><a href="../116612/index.html">Karma and Zarya</a></li>
<li><a href="../116614/index.html">Quick start in MODX Revolution</a></li>
<li><a href="../116615/index.html">There is always one more feature to succeed.</a></li>
<li><a href="../116617/index.html">Creative with meaning</a></li>
<li><a href="../116618/index.html">New version of NuGet 1.2 Packet Manager released</a></li>
<li><a href="../116620/index.html">Beauty contest for system administrators</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>