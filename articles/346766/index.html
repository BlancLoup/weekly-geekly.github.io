<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Recognizing motion gestures on Android using Tensorflow</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Introduction 
 Today, there are many different ways to interact with smartphones: touch-screen, hardware buttons, fingerprint scanner, video camera (f...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Recognizing motion gestures on Android using Tensorflow</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/537/b2a/cad/537b2acad137f4d8a45dd01e217efe04.jpg" alt="image"><br><br><h2>  Introduction </h2><br>  Today, there are many different ways to interact with smartphones: touch-screen, hardware buttons, fingerprint scanner, video camera (for example, facial recognition system), D-PAD, buttons on the headset, and so on.  But what about the use of motion gestures? <br><br>  For example, quickly moving the phone to the right or left while holding it in your hand can very accurately reflect the intention to move to the next or previous song in the playlist.  Or you can quickly turn the phone upside down and then back to update the application content.  Implementing such an interaction looks promising and literally adds a new dimension to the UX.  This article describes how to implement this using machine learning and the Tensorflow library for Android. <br><a name="habracut"></a><br><h2>  Description </h2><br>  Let's define the ultimate goal.  I would like the smartphone to recognize fast movements left and right. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I would also like the implementation to be in the Android library and it could be easily integrated into any other application. <br><br>  Gestures can be recorded on a smartphone using several sensors: accelerometer, gyroscope, magnetometer, and others.  Later, a set of recorded gestures can be used in machine learning algorithms for recognition. <br><br>  A special Android application will be developed for data recording.  Preprocessing and training will be done on a PC in Jupyter Notebook using the Python language and the TensorFlow library.  Gesture recognition will be implemented in a demonstration application using learning outcomes.  In the end, we will develop an Android-ready gesture recognition library that can be easily integrated into other applications. <br><br>  Our implementation plan: <br><br><ul><li>  Collect data on the phone </li><li>  Develop and train a neural network </li><li>  Export neural network to smartphone </li><li>  Develop a test application for Android </li><li>  Develop Android library </li></ul><br><h2>  Implementation </h2><br><h3>  Data preparation </h3><br>  To begin with, let's decide which sensors and what type of data our gestures can describe.  It seems that both the accelerometer and the gyroscope must be used to accurately describe these gestures. <br><br>  The accelerometer obviously measures acceleration and, accordingly, movement: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/889/c4c/ce8/889c4cce88152fdef8605c19269a5f19.png" alt="image"><br><br>  The accelerometer has an interesting nuance - it measures not only the acceleration of the phone itself, but also the <a href="https://ru.wikipedia.org/wiki/%25D0%25A3%25D1%2581%25D0%25BA%25D0%25BE%25D1%2580%25D0%25B5%25D0%25BD%25D0%25B8%25D0%25B5_%25D1%2581%25D0%25B2%25D0%25BE%25D0%25B1%25D0%25BE%25D0%25B4%25D0%25BD%25D0%25BE%25D0%25B3%25D0%25BE_%25D0%25BF%25D0%25B0%25D0%25B4%25D0%25B5%25D0%25BD%25D0%25B8%25D1%258F">acceleration of gravity</a> which is approximately equal to 9.8 m / s <sup>2</sup> .  This means that the magnitude of the vector of acceleration lying on the table phone will be equal to 9.8.  Such values ‚Äã‚Äãcannot be used directly and must be subtracted from the value of the gravitational acceleration vector.  This is not an easy task because it requires joint processing of the magnetometer and accelerometer data.  Fortunately, Android has a special ‚ÄúLinear Accelerometer‚Äù sensor that performs the necessary calculations and returns the correct values. <br><br>  A gyroscope, on the other hand, measures rotation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/5d7/61c/142/5d761c142aff2b09fa0164b4f83ce4b0.jpg" alt="image"><br><br>  Let's try to determine which values ‚Äã‚Äãwill correlate with our gestures.  Obviously, in an accelerometer (meaning a linear accelerometer), the values ‚Äã‚Äãof X and Y will describe gestures to a sufficiently large degree.  The value of the Z accelerometer is unlikely to depend on our gestures. <br><br>  As for the gyro sensor, it seems that gestures slightly affect the Z axis. However, to simplify the implementation, I propose not to include it in the calculation.  In this case, our gesture detector recognizes the movement of the phone not only in the hand, but also along the horizontal line - for example, on the table.  But this is not too big a problem. <br><br>  Thus, we need to develop an Android application that can record accelerometer data. <br><br>  I developed such an <a href="https://github.com/ryanchyshyn/motion_gestures_detection/tree/master/GesturesManager">application</a> .  Here is a screenshot of the recorded right gesture: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/7de/fce/006/7defce006c9dfe6c94abe23fca4d1a45.jpg" alt="image"><br><br>  As you can see, the X and Y axes react very strongly to the gesture.  The Z axis also reacts, but, as we decided, it will not be included in the processing. <br><br>  Here is the ‚Äúleft‚Äù gesture: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/8b1/730/8ee/8b17308ee0afbe7594f94bb40ff5876e.jpg" alt="image"><br><br>  Notice that the X values ‚Äã‚Äãare almost the opposite of the values ‚Äã‚Äãfrom the previous gesture. <br><br>  Another thing to mention is the sampling rate of the data.  It reflects how often the data is updated and directly affects the amount of data during a time interval. <br><br>  Another thing to consider is the duration of the gestures.  This value, like many others, should be chosen empirically.  I found that the duration of gestures lasts no more than 1 second, but to make the value more suitable for calculations, I rounded it to 1.28 seconds. <br><br>  The selected refresh rate is 128 points for 1.28 seconds, which gives a delay of 10 milliseconds (1.28 / 128).  This value must be passed to the <a href="https://developer.android.com/reference/android/hardware/SensorManager.html">registerListener</a> method. <br><br>  The idea is to train the neural network to recognize such signals in the data stream from the accelerometer. <br><br>  So, next we need to write a lot of gesture samples to the files.  Of course, the same type of gestures (right or left) must be marked with the same tag.  It is difficult to say in advance how many samples are necessary for network training, but this can be determined as a result of training. <br><br>  Tapn somewhere on the graph, you highlight the sample - i.e.  plot of 128 pixels in length: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/df7/105/6af/df71056af0f5aefaab1ae51c676ed2d9.jpg" alt="image"><br><br>  Now the ‚Äúsave‚Äù button will be active.  Clicking on it will automatically save the sample in a file in the working directory to a file with the name of the form ‚Äú{label} _ {timestamp} .log‚Äù. The working directory can be selected in the application menu. <br><br>  Also note that after saving the current sample, the next one will be automatically selected.  The next gesture is selected using a very simple algorithm: find the first entry an absolute value of X which is greater than 3, then rewind 20 points. <br><br>  This automation allows us to quickly save a lot of samples.  I recorded 500 samples per gesture.  Saved data must be copied to a PC for further processing.  (Processing and learning directly on the phone looks interesting, but TensorFlow for Android currently does not support learning). <br><br>  In the picture presented earlier, the data range is approximately ¬± 6.  However, if you wave your phone stronger, it can reach ¬± ‚Äã‚Äã10.  It is better to normalize the data so that the range is ¬± 1, which is much better suited to the format of the neural network data.  To do this, I simply divided all data into a constant - in my case 9. <br><br>  The next step that needs to be done before learning begins is filtering the data to eliminate high-frequency vibrations.  Such fluctuations are not related to our gestures. <br><br>  There are many ways to filter data.  One of them is the <a href="https://ru.wikipedia.org/wiki/%25D0%25A1%25D0%25BA%25D0%25BE%25D0%25BB%25D1%258C%25D0%25B7%25D1%258F%25D1%2589%25D0%25B0%25D1%258F_%25D1%2581%25D1%2580%25D0%25B5%25D0%25B4%25D0%25BD%25D1%258F%25D1%258F">Moving Average</a> filter.  Here is an example of how it works: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/429/cbe/0ae/429cbe0ae565f9f5f8a5d4cba8a2131e.jpg" alt="image"><br><br>  Note that the maximum X values ‚Äã‚Äãof the data are now half the original.  Since we will perform the same data filtering in real time during recognition, this should not be a problem. <br><br>  The final step to improve learning is data augmentation.  This process extends the original data set by performing some manipulations.  In our case, I just moved the data left and right to a few points: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/3d7/598/f21/3d7598f21dd49b82a73e17b7c4600a8f.jpg" alt="image"><br><br><h3>  Neural Network Design </h3><br>  Designing a neural network is not an easy task and requires some experience and intuition.  On the other hand, neural networks are well studied for some types of tasks, and you can simply adapt an existing network.  Our task is very similar to the task of classifying images;  the input can be viewed as an image with a height of 1 pixel (and this is the case - the first operation converts the input two-dimensional data [128 columns x 2 channels] into three-dimensional data [1 row x 128 columns x 2 channels]). <br><br>  Thus, the input of the neural network is an array [128, 2]. <br><br>  The output of the neural network is a vector with a length equal to the number of labels.  In our case, these are 2 numbers with a floating double-precision data type. <br><br>  Below is a diagram of the neural network: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/a21/be5/cc4/a21be5cc4ced8fb849db18e4bd5d7d11.jpg" alt="image"><br><br>  And the detailed scheme obtained in TensorBoard: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/837/68c/0e6/83768c0e605dd8928c66cad7dd0a986c.jpg" alt="image"><br><br>  This diagram contains some auxiliary nodes that are necessary only for training.  Later I will show a clean, optimized picture. <br><br><h3>  Training </h3><br>  Training will be conducted on a PC in a <a href="http://jupyter.org/">Jupyter Notebook</a> environment using Python and the TensorFlow library.  You can run Notebook in <a href="http://conda.io/docs/user-guide/install/download.html">Conda</a> using the following <a href="">configuration file</a> .  Here are some hyper learning options: <br><br><pre><code class="hljs css">: <span class="hljs-selector-tag"><span class="hljs-selector-tag">Adam</span></span>   : 3   (<span class="hljs-selector-tag"><span class="hljs-selector-tag">learning</span></span> <span class="hljs-selector-tag"><span class="hljs-selector-tag">rate</span></span>): 0<span class="hljs-selector-class"><span class="hljs-selector-class">.0001</span></span></code> </pre> <br>  The data set is divided into training and verification in the ratio of 7 to 3. <br><br>  The quality of training can be monitored using the values ‚Äã‚Äãof accuracy of training and testing.  Learning accuracy should approach, but not reach 1. A value too low will indicate poor and inaccurate recognition, and a too high value will result in retraining the model and may lead to some artifacts during recognition, for example, recognition with a non-zero value for data without gestures.  Good testing accuracy is proof that a trained model can recognize data that it has never seen before. <br><br>  Learning Protocol: <br><br><pre> <code class="hljs cs">(<span class="hljs-string"><span class="hljs-string">'Epoch: '</span></span>, <span class="hljs-number"><span class="hljs-number">0</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Loss: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.054878365</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Accuracy: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.99829739</span></span>) (<span class="hljs-string"><span class="hljs-string">'Epoch: '</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Loss: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.0045060506</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Accuracy: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.99971622</span></span>) (<span class="hljs-string"><span class="hljs-string">'Epoch: '</span></span>, <span class="hljs-number"><span class="hljs-number">2</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Loss: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.00088313385</span></span>, <span class="hljs-string"><span class="hljs-string">' Training Accuracy: '</span></span>, <span class="hljs-number"><span class="hljs-number">0.99981081</span></span>) (<span class="hljs-string"><span class="hljs-string">'Testing Accuracy:'</span></span>, <span class="hljs-number"><span class="hljs-number">0.99954832</span></span>)</code> </pre><br>  The TensorFlow graph and its associated data can be saved to files using the following methods: <br><br><pre> <code class="python hljs">saver = tf.train.Saver() <span class="hljs-keyword"><span class="hljs-keyword">with</span></span> tf.Session() <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> session: session.run(tf.global_variables_initializer()) <span class="hljs-comment"><span class="hljs-comment"># save the graph tf.train.write_graph(session.graph_def, '.', 'session.pb', False) for epoch in range(training_epochs): # train saver.save(session, './session.ckpt')</span></span></code> </pre><br>  The full code can be found <a href="https://github.com/ryanchyshyn/motion_gestures_detection/blob/master/Python/training.ipynb">here</a> . <br><br><h3>  Neural Network Export </h3><br>  How to save TensorFlow data was shown in the previous section.  The graph is stored in the session.pb file, and the training data (weights, etc.) are saved in several ‚Äúsession.ckpt‚Äù files.  These files can be quite large: <br><br><pre> <code class="hljs css"><span class="hljs-selector-tag"><span class="hljs-selector-tag">session</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.ckpt</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.data-00000-of-00001</span></span> 3385232 <span class="hljs-selector-tag"><span class="hljs-selector-tag">session</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.ckpt</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.index</span></span> 895 <span class="hljs-selector-tag"><span class="hljs-selector-tag">session</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.ckpt</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.meta</span></span> 65920 <span class="hljs-selector-tag"><span class="hljs-selector-tag">session</span></span><span class="hljs-selector-class"><span class="hljs-selector-class">.pb</span></span> 47732</code> </pre><br>  The graph and learning data can be frozen and converted into one file suitable for working on a mobile device. <br><br>  To freeze it, copy the tensorflow / python / tools / freeze_graph.py file to the script directory and run the following command: <br><br><pre> <code class="hljs tex">python freeze_graph.py --input_graph=session.pb <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--input_binary=True <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--input_checkpoint=session.ckpt <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--output_graph=frozen.pb <span class="hljs-tag"><span class="hljs-tag">\</span><span class="hljs-name"><span class="hljs-tag"><span class="hljs-name"> </span></span></span></span>--output_node_names=labels_output</code> </pre><br>  where output_graph is the output file and output_node_names is the name of the output node.  This value is specified in the Python code. <br><br>  The resulting file is smaller than the previous ones, but still large enough: <br><br> <code>frozen.pb 1130835 <br></code> <br><br>  Here is what this model looks like in TensorBoard: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/edc/36f/41b/edc36f41badcc3266330246e8c98f6bd.jpg" alt="image"><br><br>  To obtain such an image, copy the tensorflow / python / tools / import_pb_to_tensorboard.py file to the script directory and run: <br><br><pre> <code class="hljs pgsql">python import_pb_to_tensorboard.py <span class="hljs-comment"><span class="hljs-comment">--model_dir=frozen.pb --log_dir=tmp</span></span></code> </pre><br>  where frozen.pb is the model file. <br><br>  Now run TensorBoard: <br><br><pre> <code class="hljs pgsql">tensorboard <span class="hljs-comment"><span class="hljs-comment">--logdir=tmp</span></span></code> </pre><br>  There are several ways to <a href="https://www.tensorflow.org/versions/master/mobile/prepare_models">optimize the model</a> for the mobile environment.  To run the following commands, you need to compile TensorFlow from <a href="https://www.tensorflow.org/install/install_sources">sources</a> : <br><br>  1. Remove unused nodes and general optimization.  Run: <br><br><pre> <code class="hljs scala">bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=mydata/frozen.pb --out_graph=mydata/frozen_optimized.pb --inputs=<span class="hljs-symbol"><span class="hljs-symbol">'x_inpu</span></span>t' --outputs=<span class="hljs-symbol"><span class="hljs-symbol">'labels_outpu</span></span>t' --transforms=<span class="hljs-symbol"><span class="hljs-symbol">'strip_unused_nodes</span></span>(<span class="hljs-class"><span class="hljs-keyword"><span class="hljs-class"><span class="hljs-keyword">type</span></span></span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">=float</span></span></span><span class="hljs-class">, </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">shape=</span></span></span><span class="hljs-class">"128,2") </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">remove_nodes</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">op=</span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">Identity</span></span></span></span><span class="hljs-class"><span class="hljs-params">, op=</span></span><span class="hljs-type"><span class="hljs-class"><span class="hljs-params"><span class="hljs-type">CheckNumerics</span></span></span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">round_weights</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">num_steps=256</span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">fold_constants</span></span></span><span class="hljs-class">(</span><span class="hljs-params"><span class="hljs-class"><span class="hljs-params">ignore_errors=true</span></span></span><span class="hljs-class">) </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">fold_batch_norms</span></span></span><span class="hljs-class"> </span><span class="hljs-title"><span class="hljs-class"><span class="hljs-title">fold_old_batch_norms</span></span></span><span class="hljs-class">'</span></span></code> </pre><br>  Result: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/c6e/39c/d93/c6e39cd9333a5c7226ee6d73add7ccf8.jpg" alt="image"><br><br>  2. Quantization (converting a floating-point data format into an 8-bit integer format).  Run: <br><br><pre> <code class="hljs cs">bazel-bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=mydata/frozen_optimized.pb --out_graph=mydata/frozen_optimized_quant.pb --inputs=<span class="hljs-string"><span class="hljs-string">'x_input'</span></span> --outputs=<span class="hljs-string"><span class="hljs-string">'labels_output'</span></span> --transforms=<span class="hljs-string"><span class="hljs-string">'quantize_weights strip_unused_nodes'</span></span></code> </pre><br>  As a result, the output file is 287,129 bytes in size compared to the original 3.5 MB.  This file can be used in TensorFlw for Android. <br><br><h3>  Android Demo Application </h3><br>  To perform signal recognition in the Android application, you need to connect the TensorFlow for Android library to the project.  Add a library to the gradle dependencies: <br><br><pre> <code class="hljs nginx"><span class="hljs-section"><span class="hljs-section">dependencies</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">implementation</span></span> <span class="hljs-string"><span class="hljs-string">'org.tensorflow:tensorflow-android:1.4.0'</span></span> }</code> </pre><br>  Now you can access the TensorFlow API through the TensorFlowInferenceInterface class.  First, put the file "frozen_optimized_quant.pb" in the "assets" directory of your application (i.e. "app / src / main / assets") and load it in the code (for example, when starting the Activity, however, as usual, it‚Äôs better to produce any I / O operations in the background thread): <br><br><pre> <code class="java hljs">inferenceInterface = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> TensorFlowInferenceInterface(getAssets(), ‚Äúfile:<span class="hljs-comment"><span class="hljs-comment">///android_asset/frozen_optimized_quant.pb‚Äù);</span></span></code> </pre><br>  Notice how the model file is specified. <br><br>  Finally, recognition can be performed: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] data = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[<span class="hljs-number"><span class="hljs-number">128</span></span> * <span class="hljs-number"><span class="hljs-number">2</span></span>]; String[] labels = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> String[]{<span class="hljs-string"><span class="hljs-string">"Right"</span></span>, <span class="hljs-string"><span class="hljs-string">"Left"</span></span>}; <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[] outputScores = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> <span class="hljs-keyword"><span class="hljs-keyword">float</span></span>[labels.length]; <span class="hljs-comment"><span class="hljs-comment">// populate data array with accelerometer data inferenceInterface.feed("x_input", data, new long[] {1, 128, 2}); inferenceInterface.run(new String[]{‚Äúlabels_output‚Äù}); inferenceInterface.fetch("labels_output", outputScores);</span></span></code> </pre><br>  The data is fed to the input of our ‚Äúblack box‚Äù in the form of a one-dimensional array containing serial data X and Y of the accelerometer, that is, the data format [x1, y1, x2, y2, x3, y3, ..., x128, y128]. <br><br>  At the output, we have two floating-point numbers in the range 0 ... 1, the values ‚Äã‚Äãof which are the correspondence of the input data to the ‚Äúleft‚Äù or ‚Äúright‚Äù gestures.  Note that the sum of these values ‚Äã‚Äãis 1. Thus, for example, if the input signal does not match either the left or the right gesture, then the output will be close to [0.5, 0.5].  For simplicity, it is better to convert these values ‚Äã‚Äãto absolute values ‚Äã‚Äã0 ... 1, using simple mathematics. <br><br>  In addition, do not forget to perform filtering and normalization of data before running recognition. <br><br>  Here is a screenshot of the testing window of the demo application: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/078/7e9/8e7/0787e98e7e83ab639d533cce3ae869f0.jpg" alt="image"><br><br>  where the red and green lines represent the pre-processed signal in real time.  Yellow and blue lines refer to ‚Äúcorrected‚Äù ‚Äúright‚Äù and ‚Äúleft‚Äù gestures, respectively.  ‚ÄúTime‚Äù is the processing time of one sample, and it is rather low, which allows recognition in real time (two milliseconds means that processing can be performed at a speed of 500 Hz, we set up an accelerometer to update at 100 Hz). <br><br>  As you can see, there are some nuances.  First, there are some nonzero recognition values, even for an ‚Äúempty‚Äù signal.  Secondly, each gesture has a long-lasting ‚Äútrue‚Äù recognition in the center with a value close to 1.0, and a slight opposite recognition at the edges. <br><br>  It seems that additional processing is required to perform accurate, actual gesture recognition. <br><br><h3>  Android library </h3><br>  I implemented recognition using TensorFlow along with additional processing of the output in a separate library for Android.  Library and demo application are <a href="https://github.com/ryanchyshyn/motion_gestures_detection/tree/master/MotionGesturesDemo">here</a> . <br><br>  To use the library in your application, add a dependency on the library to the gradle file: <br><br><pre> <code class="hljs cs">repositories { maven { url <span class="hljs-string"><span class="hljs-string">"https://dl.bintray.com/rii/maven/"</span></span> } } dependencies { ... implementation <span class="hljs-string"><span class="hljs-string">'uk.co.lemberg:motiondetectionlib:1.0.0'</span></span> }</code> </pre><br>  create a MotionDetector listener: <br><br><pre> <code class="java hljs"><span class="hljs-keyword"><span class="hljs-keyword">private</span></span> <span class="hljs-keyword"><span class="hljs-keyword">final</span></span> MotionDetector.Listener gestureListener = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MotionDetector.Listener() { <span class="hljs-meta"><span class="hljs-meta">@Override</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">public</span></span></span><span class="hljs-function"> </span><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">void</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">onGestureRecognized</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(MotionDetector.GestureType gestureType)</span></span></span><span class="hljs-function"> </span></span>{ Log.d(TAG, <span class="hljs-string"><span class="hljs-string">"Gesture detected: "</span></span> + gestureType); } };</code> </pre><br>  and enable recognition: <br><br><pre> <code class="java hljs">MotionDetector motionDetector = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> MotionDetector(context, gestureListener); motionDetector.start();</code> </pre><br><h2>  Conclusion </h2><br>  We have gone through all the stages of developing and implementing motion gesture recognition in an Android application using the TensorFlow library: collecting and pre-processing data, developing and training a neural network, and developing a test application and a ready-to-use library for Android.  The described approach can be used for any other recognition or classification tasks.  The resulting library can be integrated into any other Android application to make it possible to control it using motion gestures. <br><br>  I hope you found this article useful, you can also watch the video review below. <br>  If you have a project idea, but do not know where to start, <a href="https://lemberg.co.uk/contact-us">we are always here to help you</a> . <br><br><iframe width="560" height="315" src="https://www.youtube.com/embed/On-plyyFR48" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br>  PS In fact, I am also the author of the original English version of the article, which was published on <a href="http://blog.lemberg.co.uk/">blog.lemberg.co.uk</a> , so I can answer technical questions. </div><p>Source: <a href="https://habr.com/ru/post/346766/">https://habr.com/ru/post/346766/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346756/index.html">How far the unmanned vehicle will reach? Ask a question to an Intel expert</a></li>
<li><a href="../346758/index.html">Burn after reading. We make one-time links to naked Nginx</a></li>
<li><a href="../346760/index.html">TeamLead - our all: the most popular reports from HighLoad ++ 2017. Part 1</a></li>
<li><a href="../346762/index.html">The magic of the word "blockchain" and other materials in our collection</a></li>
<li><a href="../346764/index.html">January fixing holes in the .NET Framework and Core</a></li>
<li><a href="../346768/index.html">Two-factor authentication for Cisco Meraki Client VPN using Token2 TOTPRadius</a></li>
<li><a href="../346770/index.html">Undocumented CSS Techniques</a></li>
<li><a href="../346772/index.html">Using the KOMPAS-3D API ‚Üí Lesson 6 ‚Üí Saving a document in various formats</a></li>
<li><a href="../346774/index.html">JavaScript exceeded everyone</a></li>
<li><a href="../346776/index.html">Hyper-V - a child of marketing or a real alternative?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>