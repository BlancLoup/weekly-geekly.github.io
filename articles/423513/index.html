<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>We save on RAID-controller, or how to feed</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In our century of cloud services, AWS Lambda and other shared hosting sites of completely intangible computing resources, sometimes you want a little ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>We save on RAID-controller, or how to feed</h1><div class="post__text post__text-html js-mediator-article"><img align="right" src="https://habrastorage.org/webt/tk/12/jd/tk12jdebhacj1uexhpnwt0bbzo0.jpeg">  In our century of cloud services, AWS Lambda and other <s>shared hosting sites of</s> completely intangible computing resources, sometimes you want a little bit of your own.  In addition to the desire, sometimes there are also needs to thoughtfully twist one or another software product with minimal expenses on the platform.  It is almost always possible to find some surplus materiel, sometimes it even turns out to put everything together and turn it on.  If these surpluses are CPUs for at least 4-6 cores and 64GB of memory - generally excellent, you can take ESXi and work with anything.  One problem: with VMWare's disk capacity on the domestic hardware, nothing at all.  The performance of local single HDDs is low, and losing the contents of an individual spherical screw in a vacuum in the 21st century is like hey.  Let's try to connect something over the network. <br><br>  TL; DR&gt; combining, balancing, rr limit, that's it. <br><a name="habracut"></a><br>  Actually, the text further is not about the fact that it is generally possible or some kind of know-how.  The Internet is full of articles for dummies (here we tick, then Next, Next, Done) on how to submit iSCSI disk capacity.  I am writing just to eliminate the ‚Äúmistakes of the survivors‚Äù and share moments when ‚Äúeverything goes wrong‚Äù (and it goes, Murphy was right), and when you try to load the solution, it just falls. <br><br>  So, we will try to extinguish our ‚Äúhome hypervisor‚Äù with an external disk array connected via the network.  Since everything revolves around us ‚Äúcheaply‚Äù, let it be FreeNAS and 4 SATA-disks, which are serviced by the average 3 GHz 45-nm percent.  We look at Ebay, and for money comparable to a used RAID controller, we are dragging a pair of i350-T4 network cards from there.  These are Intel's four-port gigabit adapters.  According to them, we will associate the storage with the hypervisor. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Count a little bit.  The average data transfer rate of an average SATA disk is 160-180 MB / s with an interface width of 6 Gbit / s.  In fact, the actual data transfer rate from the HDD does not exceed 2 Gb / s.  This is not such a big number, considering that we are planning to use 4 gigabit ports on the connection (how exactly to turn 4x1 Gbit into 4 Gbit - we will discuss further).  Everything is much worse with random access speeds - here everything falls almost to the level of diskettes. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/7y/cn/re/7ycnrez1dwgdr_nsgkti4zjdsjw.png"></div><br>  Considering that the disk load profile of many guest OSes is far from linear, I would like to see more cheerful figures.  To correct the situation in the hypervisor file system (VMFS v6), the block size is 1 MB, which contributes to the consolidation of many random operations and speeds up access to data on virtual disks.  But even with this, a single physical disk will not be enough to handle I / O operations from all the "guests". <br><br>  Immediately make a reservation - everything further makes sense if you have more than two adapters for a ‚Äúnetwork of storage‚Äù.  ESXi with a free single-processor license can connect, in addition to local drives, to two types of storages - NFS and iSCSI.  NFS assumes file-level access and is also good in its own way.  On it, you can deploy guests, undemanding to disk performance.  Backing them up is a pleasure, because  You can open the same NFS ball somewhere else and copy snapshots vm.  In general, with one network interface (if it‚Äôs not 10GE, of course) - NFS is your choice. <br><br>  ISCSI has several advantages over NFS.  In order to realize them fully, we have already prepared ourselves - laying 4 gigabit ports for the storage network.  How does network bandwidth typically expand at a known interface speed?  That's right, aggregation.  But for the complete utilization of the aggregated channel, a number of conditions are needed, and this is more suitable for the communication of switches between themselves or for the network hyperlink of the hypervisor.  The implementation of the iSCSI protocol provides for such a function as multipathing (literally, many ways) - the ability to connect the same volume through different network interfaces.  Of course, there is some load balancing, although the main purpose is the fault tolerance of the storage network.  (To be fair, NFSv4.1 supports session trunking based on the ultimate magic like RDMA and MPTCP, but this is an attempt to shift the problems of file access <s>from a sore head to a healthy one</s> to lower levels.) <br><br>  So, for the beginning we will publish our target.  We believe that FreeNAS is installed, the IP-address of the management regularly ships us the web-interface, we cut the array and zvol on it in full accordance with our inner convictions.  In our case, these are 4 x 500GB disks, combined in raidz1 (which gives only 1.3 TiB of effective capacity), and zvol is 1 TB in size exactly.  Let's configure the i350 network interfaces, for simplicity we assume that everyone will belong to different subnets. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3f/ci/-b/3fci-bzni4siodupm_admdf8nfe.png"></div><br>  Then we configure the iSCSI-ball using the ‚ÄúNext, Next, Done‚Äù method.  When setting up the portal, do not forget to add there all the network interfaces allocated for iSCSI.  It should look something like the pictures. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/uk/4w/hu/uk4whuf8w2gxvts-vxu-3oupyn8.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/yq/pp/wg/yqppwgzsqzxkxkffbatxbgxiiy0.png"></div><br>  A little more attention will need to be paid to setting the extent ‚Äî when presenting a volume, it is necessary to force the block size to 512 bytes.  Without this, the ESXi initiator refused to recognize the presented volumes at all.  For fidelity, it is better to disable the forwarding of the sizes of the nat block (which is not on the zvol and can not be) and enable Xen support mode. <br>  With FreeNAS, for now. <br><br>  On the ESXi side, it's a bit more complicated with network configuration.  Again, we assume that the hypervisor itself is installed and is also controlled by a separate port.  You will need to allocate 4 VM Kernel interfaces belonging to 4m different port-groups in 4 different virtual switches.  Each of these switches is allocated its own physical port of uplink.  We take vmk addresses, of course, in the corresponding subnets, in the same way as configuring the storage ports.  The procedure for setting addresses is generally important - either we connect port-to-port cards without a switch, or we give different links to different networks (well, this is an adult), therefore the physical correspondence of the ports matters. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/qe/hq/ox/qehqoxnpinkpxs1fy2ajiamu7om.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/cn/vr/q1/cnvrq1slbat9sqajuqy0wvudadi.png"></div><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/3x/ml/1b/3xml1bfwyrzwqqkzmin3pwz4ltw.png"></div><br>  When setting up a network for iSCSI, we pay special attention to the MTU parameter.  This is exactly the case when ‚Äúsize matters‚Äù - we take the maximum that allows all components of the network to be installed.  If the cards are directly connected, you can specify mtu 9000 on both sides, on ESXi and FreeNAS.  However, normal switches will support this value.  We ping, we see that our network is normal, and the packets of the required size pass.  Fine.  We set fire to the initiator. <br><br>  Turn on iSCSI, add IP addresses to the dynamic configuration section (Storage -&gt; Adapters -&gt; Configure iSCSI -&gt; Dynamic targets).  After saving, the iSCSI portals will be polled at these addresses, the initiator will determine that the same volume is behind each of them, and connect to it at all available addresses (that same multipath).  Next we need to create a datastore on the device that appears. <br><br>  After that, you can roll out a virtual machine and measure what we did. <br><br><div style="text-align:center;"><img src="https://habrastorage.org/webt/4t/hf/bd/4thfbd-txhgteetxgb6zs3gggjo.png"></div><br>  Not so impressive results.  Open the storage console, display the current network status and run tests. <br><br> <code>root@freenas:~ # systat -ifstat</code> <br> <div class="spoiler">  <b class="spoiler_title">What we see?</b> <div class="spoiler_text"><pre> <code class="hljs sql"> /0 /1 /2 /3 /4 /5 /6 /7 /8 /9 /10 <span class="hljs-keyword"><span class="hljs-keyword">Load</span></span> Average <span class="hljs-keyword"><span class="hljs-keyword">Interface</span></span> Traffic Peak Total lo0 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0.319</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.893</span></span> KB/s <span class="hljs-number"><span class="hljs-number">3.041</span></span> MB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">0.319</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.893</span></span> KB/s <span class="hljs-number"><span class="hljs-number">3.041</span></span> MB alc0 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0.478</span></span> KB/s <span class="hljs-number"><span class="hljs-number">1.233</span></span> KB/s <span class="hljs-number"><span class="hljs-number">3.934</span></span> MB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">0.412</span></span> KB/s <span class="hljs-number"><span class="hljs-number">1.083</span></span> KB/s <span class="hljs-number"><span class="hljs-number">2.207</span></span> MB igb3 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0.046</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.105</span></span> KB/s <span class="hljs-number"><span class="hljs-number">181.434</span></span> KB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">0.073</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.196</span></span> KB/s <span class="hljs-number"><span class="hljs-number">578.396</span></span> KB igb2 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0.046</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.105</span></span> KB/s <span class="hljs-number"><span class="hljs-number">120.963</span></span> KB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">0.096</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.174</span></span> KB/s <span class="hljs-number"><span class="hljs-number">517.221</span></span> KB igb1 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">4.964</span></span> MB/s <span class="hljs-number"><span class="hljs-number">121.255</span></span> MB/s <span class="hljs-number"><span class="hljs-number">10.837</span></span> GB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">6.426</span></span> MB/s <span class="hljs-number"><span class="hljs-number">120.881</span></span> MB/s <span class="hljs-number"><span class="hljs-number">3.003</span></span> GB igb0 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">0.046</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.105</span></span> KB/s <span class="hljs-number"><span class="hljs-number">139.123</span></span> KB <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">0.073</span></span> KB/s <span class="hljs-number"><span class="hljs-number">0.210</span></span> KB/s <span class="hljs-number"><span class="hljs-number">869.938</span></span> KB</code> </pre><br></div></div><br>  Only one of the four network ports (igb1) was utilized.  This happens because the balancing mechanism provided by default for multipath selects the same adapter with each data packet.  We need to use everything. <br>  Connect to the hypervisor via SSH and command. <br>  First, let's look at the id of the moon with multipath, and how it works: <br><br> <code>[root@localhost:~] esxcfg-mpath -b <br> naa.6589cfc000000b478db42ca922bb9308 : FreeNAS iSCSI Disk (naa.6589cfc000000b478db42ca922bb9308) <br> <br> [root@localhost:~] esxcli storage nmp device list -d naa.6589cfc000000b478db42ca922bb9308 | grep PSP <br> Path Selection Policy: VMW_PSP_MRU</code> <br> <br>  The path selection policy is MRU, I mean most recently used.  All data goes to the same port, re-selecting the path occurs only when the network connection is unavailable.  Change to round-robin, in which all interfaces change in turn after some number of operations: <br><br> <code>[root@localhost:~] esxcli storage nmp device set -d naa.6589cfc000000b478db42ca922bb9308 -P VMW_PSP_RR</code> <br> <br>  Reboot ESXi, open monitoring, run tests.  We see that the load is distributed evenly across the network adapters (at a minimum, peak values, too much), the test results are also more fun. <br><br><img align="right" src="https://habrastorage.org/webt/pg/au/ov/pgauovsykzwhnis_ustnqqp-d_y.png"><pre> <code class="hljs delphi"> <span class="hljs-keyword"><span class="hljs-keyword">Interface</span></span> Peak igb3 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">43.233</span></span> MB/s <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">46.170</span></span> MB/s igb2 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">42.806</span></span> MB/s <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">45.773</span></span> MB/s igb1 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">43.495</span></span> MB/s <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">45.489</span></span> MB/s igb0 <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> <span class="hljs-number"><span class="hljs-number">43.208</span></span> MB/s <span class="hljs-keyword"><span class="hljs-keyword">out</span></span> <span class="hljs-number"><span class="hljs-number">46.079</span></span> MB/s</code> </pre> <br>  There are some deviations in the ports, this is due to the limits of the Path Selection Policy - the number of operations or bytes, after which the switch to another port occurs.  The default is 1000 IOPS, that is, if the data exchange went within 999 operations, it will go through one network port.  You can change, compare and select the appropriate value.  You can not change, the default is enough for most tasks. <br><br>  We make measurements, we test, we work.  The results obtained significantly exceed the capabilities of a single disk, so now our virtual machines can not be pushed by elbows on I / O operations.  The resulting speeds and the fault tolerance of the array will depend on the hardware and how the volume is configured. </div><p>Source: <a href="https://habr.com/ru/post/423513/">https://habr.com/ru/post/423513/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../423501/index.html">Search and create a visual style design project</a></li>
<li><a href="../423503/index.html">My experience of moving, living and studying in Germany</a></li>
<li><a href="../423505/index.html">Zuckerberg sells Facebook shares for $ 13 billion so that ‚Äúour children never have to be sick‚Äù</a></li>
<li><a href="../423507/index.html">How to save memory on browser tabs, but not to lose their contents. Experience team Yandex. Browser</a></li>
<li><a href="../423511/index.html">Interception of the installation of Firefox and Chrome in Windows 10</a></li>
<li><a href="../423515/index.html">Is DRY good or can it break the SOLID O?</a></li>
<li><a href="../423519/index.html">Wandering monster: how to get rid of problems on the map</a></li>
<li><a href="../423521/index.html">Seven rules of thumb for experimenting with websites</a></li>
<li><a href="../423523/index.html">Combined sensor, with preference and poetess</a></li>
<li><a href="../423527/index.html">Children's apps massively collect personal data and pass it on to third parties.</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>