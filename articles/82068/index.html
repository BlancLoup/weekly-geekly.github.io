<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Notes on NLP (Part 10)</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="(The first parts: 1 2 3 4 5 6 7 8 9 ). As stated in the well-known advertising, "you did not wait, but we came" :) 

 During the time that has passed ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Notes on NLP (Part 10)</h1><div class="post__text post__text-html js-mediator-article">  (The first parts: <a href="http://habrahabr.ru/blogs/artificial_intelligence/79790/">1</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79819/">2</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79830/">3</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79853/">4</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79882/">5</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79923/">6</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/79962/">7</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/80081/">8</a> <a href="http://habrahabr.ru/blogs/artificial_intelligence/80268/">9</a> ).  As stated in the well-known advertising, "you did not wait, but we came" :) <br><br>  During the time that has passed since the publication of the ninth part, I read one good book on the topic (there are a couple more in the to-read list), many articles, and also talked to several experts.  Accordingly, a new volume of material has accumulated that deserves a separate note.  As usual, I introduce others, in parallel, I structure knowledge for myself. <br><br>  I apologize right away: this part is difficult to read and understand.  Well, yes, as they say, not all the Shrovetide cat.  Difficult tasks correspond to complex texts :) <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h1>  A little about machine translation: statistics against the rules </h1>  I'll start with my favorite holivar.  Which is better - statistical models or models based on explicit grammar rules?  Not bad writes about this Yorick Wilks in the book <a href="http://www.amazon.com/Machine-Translation-Its-Scope-Limits/dp/0387727736">Machine Translation: Its Scope and Limits</a> .  The book deals with the task of machine translation.  It is clear that indiscriminately the structure of sentences does not go far in translation, so the topics of syntactic analysis and translation are closely related. <br><br>  In general, Wilkes sees the development of the industry.  Initially, everything was based on the rules.  Then, in the early 90s, a group of guys from IBM came up with the idea of ‚Äã‚Äãapplying pure statistics (without any rules at all), and got results that were much better than expected.  However, Wilkes says that good results initially do not guarantee improvements in the future, since the ‚Äútheoretical ceiling‚Äù of the technology may not be so high.  And in general, in accordance with the "Wilkes law", <i>any theory, even the most abnormal, allows you to get good results in machine translation</i> .  There is another ‚ÄúWilkes second law‚Äù: a <i>successful machine translation system usually does not work on the stated principles</i> . <br><br>  That is, if we take the current state of the dinosaur SYSTRAN, then it turns out that calling this system ‚Äúrule-based‚Äù is not entirely correct, it uses a bunch of different ‚Äúcrutches‚Äù and specific algorithms.  Similarly, the Google &amp; IBM name systems quickly moved out of the ‚Äúpurely statistical‚Äù category.  Say, IBM was initially denied even the morphology modules for individual languages, assuming that everything should be derived ‚Äúon the fly‚Äù from the body of texts.  Now they don‚Äôt.  Yes, we should also note: ‚Äúpure‚Äù statistical analysis of a text in one way or another boils down to the statistics of following one word after another.  If we consider the "trigrams", that is, three consecutive words as the basic element of the analysis, the modern computer is even more or less coping (although for the early 90s and trigrams were very heavy).  Longer strings of words lead to a space of options beyond the capabilities of any current equipment. <br><br>  The book draws attention to the fact that Google also does not reach fanaticism.  For example, the SYSTRAN system for some areas has evolved over decades, and it will not be possible to quickly catch up with any alternative algorithm.  Therefore, Google for certain pairs of languages ‚Äã‚Äãreally uses SYSTRAN, and not its newfangled algorithms.  There are some more good comments on this topic.  For example, the author notes that the statistical translator from English to French is based on a good parallel corpus of English-French texts (minutes of meetings of the Canadian parliament).  Moreover, for rare pairs of languages ‚Äã‚Äã(and in this context, almost all other pairs are rare), it is very difficult to find such an extensive corpus.  Of course, ‚ÄúWar and Peace‚Äù can be found in dozens of languages, but these artistic translations are unlikely to fit the role of elements of a parallel corpus. <br><br>  In general, Wilkes believes that: (1) naked statistics, as well as bare rules, are not very promising (good machine translation requires a knowledge base about text objects and the outside world);  (2) future systems will be hybrid, but it is not quite clear how to make these hybrids;  (3) even purely statistical algorithms must be ‚Äúsmarter‚Äù.  Here is another such observation.  Now on the statistics built speech recognition system.  The system is initially ‚Äútrained‚Äù with input data, and then it can be used in practice.  It is estimated that if children would need as much instructional data for learning speech as computer discriminators, language training would take more than 100 years around the clock. <br><br><h1>  Towards hybridization </h1> Machine translation is a complex and multifaceted topic.  So let's better go back to the parsing, that is, to parse the sentences. <br><br>  I was already accused of unjustified dislike for statistical methods.  I think this is not entirely fair: from my point of view, the ideal direction for developing a syntactic (or syntactic-semantic) analyzer is the automatic, that is, based on statistical methods, the isolation of the parsing rules from the existing tribank.  Just in this direction, my understanding of the subject has progressed markedly over the past month. <br><br>  First, let's talk a little about the tribanks themselves (I remind you, we discuss almost exclusively dependency treebanks).  It seems to me that the creation of a sample bank of disassembled sentences is a very correct task.  Obviously, tagged phrases allow you to perform much more interesting types of text analysis, rather than unmarked sentences.  The amount of work to create a tribank is not so great.  In essence, the compilation of a tribank is an analysis of proposals for members, that is, a job with which each of us is familiar with the school.  Authors of the Finnish <a href="http://bionlp.utu.fi/fintreebank.html">Turku dependency treebank</a> believe that on the basis of a three-bank of ten thousand sentences, you can write a full-fledged parser.  OK, if every day, without straining, to disassemble 10 sentences, such a tribank can be done in three years.  And if you work in three, then for a year.  Is it a lot?  The number of tribanks in the world is growing, this is a fact.  And many of them are available to anyone, often for free. <br><br>  With bitterness, you can see that with Russian, as usual, everything is difficult.  There is a rather large tribank SynTagRus (about 42 thousand of disassembled sentences).  I did not get in touch with its authors, but from the point of view of an outside observer, everything is somehow opaque to them.  ‚ÄúWrite letters, and we may answer.  Maybe we will, and maybe we will not. ‚Äù  Compare this with the freely available Czech and Finnish corps!  I don‚Äôt say that the bank should be free, but it‚Äôs clearly easier to explain the distribution rules than to annotate 42 thousand phrases?  Why is the hard work done, and the simple one is still ‚Äúhanging‚Äù? .. On <a href="http://www.ruscorpora.ru/search-syntax.html">ruscorpora.ru</a> you can search for individual words in the corpus and evaluate the parse trees issued in PDF.  At the same time, the phrase ‚Äúpleases‚Äù: <i>‚ÄúAny offline versions of the case are not yet available, but work is being done in this direction</i> . <i>‚Äù</i>  I am very vividly imagining these ‚Äúongoing work‚Äù: apparently, the old RAR archiver works on an old 80286 computer around the clock, packing gigabytes of trees for later uploading the archive to the site.  And what else to do?  The already mentioned Finnish tribe is stupidly laid out in the archive with explanations, and no one complains. <br><br>  I think it makes sense to talk about two main problems in treebank-based parsing: how to annotate a tribank and how to automatically create a text parser on the basis of a ready-made tribank.  Let's start with the second task.  Suppose the tribank is already there and is ‚Äúsomehow‚Äù annotated.  That is, for each sentence from the tribank there is a ready-made parse graph;  in other words, links between words (which word is associated with) are listed, and for each link its ‚Äútype‚Äù is indicated (for example, ‚Äúsubject‚Äù). <br><br>  In one of the previous parts I mentioned the work in which the tribank was converted into the rules of the XDG grammar for subsequent parsing.  The direction of this is, unfortunately, stalled.  The author explains the reason for this.  Tribank does not give enough restrictions for trees.  That is, the output is obtained by the rules by which you can generate ten different trees for one input sentence.  In general, the author believes that the problem can be solved by ranking according to the ‚Äúgoodness‚Äù of the trees obtained.  Indeed, such studies exist.  Let's see what will come out of them (and maybe we will participate :)).  By the way, on the issue of Google and on pure statistics: one of the trees promoting this direction of tree ranking, Comrade Liang Huang himself worked for a while on Google. <br><br>  Generating rules a la XDG, however, is not the only way to build a parser.  There are more successful projects to date, for example, <a href="http://maltparser.org/">MaltParser</a> .  This thing promises practically fiction: give it a tribank of any language as input, and it will generate the corresponding parser.  And the system works, apparently, not bad.  I remember somewhere I was trying to slander: if the statistics is so good, why did nobody try to write the Pascal statistical analyzer?  :) After all, Pascal is clearly simpler than any natural language!  So, the authors of MaltParser really <a href="http://www.aclweb.org/anthology/W/W09/W09-3807.pdf">managed to solve this problem</a> - they generated the C ++ parser with the help of MaltParser! <br><br>  I have not yet got acquainted with the details of algorithms that convert tribanks into parsers, but in the most general form, the point is that the parsing process is presented as a procedure, depending on the numerical parameters.  Different parameter values ‚Äã‚Äãdirect the parsing procedure for a particular scenario.  Tribank is used as a training set for some standard machine learning algorithm, which is used to select the required set of parameters.  It turns out that "a set of parameters + fixed algorithm = parser". <br><br>  Here I would like to stop and make a few important remarks on the internal, irremovable limitations of MaltParser: <br>  - Statistical methods always try to parse the input sentence, even if it is incorrect.  In principle, this can be perceived in two ways.  If the task is to analyze any phrases that are even incorrectly formed, this is a plus.  If the goal is to create a spelling checker, this is an obvious minus. <br>  - The statistical method is, in fact, a ‚Äúblack box‚Äù.  If we are not satisfied with the analysis of any specific phrase, there is no opportunity to analyze why this happens, and somehow ‚Äúfix‚Äù the algorithm.  You can only change the tribank and re-start the learning algorithm. <br>  - At the output of the parser, only one parse tree is generated, which is considered to be a solution to the parsing problem. <br><br>  In principle, everything is clear with the first two constraints.  But the third point I would like to discuss in more detail.  Here we are faced with another reincarnation of the basic question: <i>what exactly should the parser do?</i> <br><br><h1>  Verge of parsing </h1>  Immediately I warn you, I have not had time to analyze the considerations of colleagues on this issue, so I express my personal opinion. <br><br>  One and only one tree: is it good or bad?  I think it is necessary to separate the tree as a structure of relations ‚Äúin general‚Äù and a tree as a structure of clearly designated types of connections.  In other words, a tree is like (1) a graph with unlabeled edges and like (2) a graph with marked edges. <br><br>  Consider the phrase <i>Ivan came from the guests</i> .  It corresponds to a tree with edges (came, Ivan), (came, from) and (from, guests).  The phrase <i>Ivan came out of courtesy</i> syntactically arranged in the same way, and it corresponds to a similar parse tree.  However, if you add edge labels that describe certain ‚Äútypes of connection‚Äù, the trees will no longer be identical.  <i>Ivan came (from where?) From the guests</i> , but: <i>Ivan came (why?) Out of politeness</i> . <br><br>  Now let's think about how important it is for the parser to be able to generate different trees in structure and different tags.  In other words, how likely is the case when different sentences (separately by structure and separately by tags) correspond to the same sentence? <br><br>  It seems to me that the situation is as follows.  The proposal must correspond to exactly one graph without labels.  The case when there are two or more graphs is possible, but in this case we have a verbal pun - as if with text and subtext.  Do we need such a play on words in practice - the big question.  Honestly, I don‚Äôt think I need it.  In fact, here we are talking about examples of this sort: <br><br><pre>  He saw her in front of his eyes =&gt; he saw (who?) Her (where?) In front of his eyes.
                                      he saw (what?) before her (how?) with his own eyes.
									 
 The Countess was riding in a carriage with a raised backside =&gt; The Countess was riding in a carriage (which one?) With a raised backside.
                                            rode in a carriage (who?) Countess with a raised ass. </pre><br>  Of course, it is difficult for me to say for sure, but according to the sensations, only one parse tree should correspond to the sentence (with unmarked edges).  Whether the parser can independently build such a tree, and what kind of information it needs is a difficult question.  For example, slightly change the phrase with the Countess (forgive what was in my head, I report): The <i>Countess was riding in a carriage with a raised ass</i> .  Here you would notice that the countess has an ass, but not the coach;  therefore, in the correct assortment tree, ‚Äúass‚Äù must be connected with the word ‚Äúcountess‚Äù, and not with the word ‚Äúcarriage‚Äù.  But this requires the parser to have extensive knowledge of anatomy, so the question of the expediency of analyzing such difficulties remains open. <br><br>  Thus, MaltParser is great for unlabeled graphs.  The situation is considerably complicated for graphs with edge labels.  According to the logic of things, a single tree always corresponds to an ideal phrase analysis.  However, in practice, with the growth of our appetites, the capabilities of the parser are sharply reduced.  The real extent of the problem depends on the types of communication between the words of the tribank.  The more sophisticated the communication system, the more difficult it is for the parser.  As a result, the parser tries to sort the sentences ‚Äúin the image and likeness‚Äù of the tribank, that is, using all the methods of connecting words known to it.  (Thus, we come to the first problem mentioned above - the task of annotating the tribank). <br><br>  Do not think that I am talking about some purely theoretical things.  Tribanka markup is not a trivial question.  There is no unified system for marking three banks.  There are some developed approaches that can be used.  And you can not take advantage.  Say, the authors of MaltParser say that by training on different tribanks, they received different quality parsing.  So, Prague Dependency Treebank as a whole gives a worse quality.  And this is explained by the fact that the markup in it is more detailed, there are more types of links.  Accordingly, the parser, which operates with such a solid number of links, has more opportunities to make a mistake. <br><br>  So, here I would like to put the question squarely: what kind of detail should be in the tribank to make the parser useful?  If we confine ourselves to the ‚Äúsubject-predicate-addition‚Äù system, MaltParser (and other statistical analyzers) is enough.  If, however, each word is assigned a whole set of semantic attributes, the problem of ‚Äúunderspecification‚Äù inevitably arises, that is, there is a true lack of data for building a single tree. <br><br>  For example, in a simple marking system, the phrase ‚ÄúI see an onion‚Äù is understood trivially: the subject - predicate - addition. <br>  In a more detailed system, there are two equivalent trees: "I see a bow-weapon" and "I see a bow-food." <br><br>  In principle, you can parse the ‚Äúsimple scenario,‚Äù and then shift the responsibility of identifying clear links and meanings of words to subsequent modules.  But wouldn't it turn out that all the work that we don‚Äôt want to think about was simply transferred to another place, and still it would have to be done somehow? <br><br>  I don't know yet.  I dig further :) </div><p>Source: <a href="https://habr.com/ru/post/82068/">https://habr.com/ru/post/82068/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../82059/index.html">Unsecure Update E107</a></li>
<li><a href="../82061/index.html">About HTML5 video support in modern browsers</a></li>
<li><a href="../82063/index.html">Hivext Platform - a cloud platform for developing Internet applications. Beta testing</a></li>
<li><a href="../82064/index.html">Eh youth! Modding Siemens SL45</a></li>
<li><a href="../82066/index.html">The basics of user stories. Part 1. Introduction</a></li>
<li><a href="../82075/index.html">Friends recommendations for social networks</a></li>
<li><a href="../82077/index.html">State-of-the-art technology</a></li>
<li><a href="../82078/index.html">Levitron</a></li>
<li><a href="../82081/index.html">Moroz and train tickets to QIWI</a></li>
<li><a href="../82082/index.html">Online store inside the banner</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>