<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Neuroclot: Part 4 - Final Model and Product Code</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="A typical day in a neurocooler - chickens often spin in the nest as well. 

 In order to bring, finally, the neurocooler‚Äôs project to its logical conc...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Neuroclot: Part 4 - Final Model and Product Code</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/getpro/habr/post_images/a5f/c10/51e/a5fc1051e661e784e3bdf26bfacd78e0.jpg" alt="image"><br><br>  <i>A typical day in a neurocooler - chickens often spin in the nest as well.</i> <br><br>  In order to bring, finally, the neurocooler‚Äôs project to its logical conclusion, it is necessary to produce a working model and put it on production, so that a number of conditions are met: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  Accuracy of predictions not less than 70-90%; </li><li>  Raspberry pi in the hen house would ideally be able to determine whether the photos belong to classes; </li><li>  It is necessary at least to learn to distinguish all chickens from each other.  The maximum program is also to learn how to count eggs; </li></ul><br>  In this article we will tell you what we got in the end, what models we tried and what amusing things we caught on the road. <br><br>  <b>Articles about neurocooler</b> <br><br><div class="spoiler">  <b class="spoiler_title">Spoiler header</b> <div class="spoiler_text"><ol><li>  <b><a href="https://habrahabr.ru/post/328138/">Intro</a> to learning about neural networks</b> </li><li>  <b><a href="https://habrahabr.ru/post/327978/">Iron, software and config</a> for monitoring chickens</b> </li><li>  <a href="https://habrahabr.ru/post/328940/"><b>A bot</b></a> that posts events from the life of chickens - without a neural network </li><li>  Dataset markup </li><li>  A working <a href="https://habrahabr.ru/post/330738/"><b>model</b></a> for the recognition of chickens in the hen house </li><li>  The result - a working bot that recognizes chickens in the hen house </li></ol></div></div><a name="habracut"></a><br><h2>  <b>0. TL; DR</b> </h2><br>  For the most impatient: <br><br><ul><li>  The accuracy turned out to be about 80%; </li><li>  Dataset can be downloaded <a href="https://drive.google.com/open%3Fid%3D0B9UYhghCktheREdLQ0s2cGNqVUU">here</a> ; </li><li>  ipynb jupyter notebook'a with all the calculations and boiler-plate code with explanations can be downloaded <a href="https://drive.google.com/open%3Fid%3D0B1WwFMq7KtPubjhsazQtUWpiZjg">here</a> , html version <a href="https://drive.google.com/open%3Fid%3D0B1WwFMq7KtPuWjZfT2dJVVdUcU0">here</a> ; </li><li>  Scripts for demonization and deployment on prod - <a href="https://drive.google.com/open%3Fid%3D0B1WwFMq7KtPuOHBWMXdyeml0cVE">here</a> (ipynb); </li></ul><br><h2>  <b>1. Preludes</b> </h2><br>  It also came out very opportunely, because just a few days ago this video came out, under which there is a very useful copy-paste. <br><iframe width="560" height="315" src="https://www.youtube.com/embed/f6Bf3gl4hWY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe><br><br>  From modern tools that adequately fit this goal, only convolutional neural networks come to mind.  <a href="http://spark-in.me/post/neural-chicken-coop-0">Having picked it up</a> (by reference the recursive article about neural networks, if you want to learn) a very thorough time in neural networks, having participated in several competitions (unfortunately already closed) on Kaggle (from the open seals interested in, but there it is rather complicated) and thus mostly passing <a href="http://course.fast.ai/">a course</a> from fast.ai, I outlined something like this: <br><br><ol><li>  Mark a small dataset (and today you really <a href="https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html">don't need a</a> lot of data to build a classifier); </li><li>  Use keras to test as many architectures as possible for a maximum of 1 day; </li><li>  If you can use unallocated photos to increase accuracy (semi-supervised approach); </li><li>  Try to visualize the activations of the inner layers of the neural network; </li></ol><br><h2>  <b>2. Let's see what happened!</b> </h2><br><pre><code class="bash hljs">Layer (<span class="hljs-built_in"><span class="hljs-built_in">type</span></span>) Output Shape Param <span class="hljs-comment"><span class="hljs-comment"># ================================================================= batch_normalization_1 (Batch (None, 700, 400, 3) 2800 _________________________________________________________________ conv2d_1 (Conv2D) (None, 698, 398, 32) 896 _________________________________________________________________ batch_normalization_2 (Batch (None, 698, 398, 32) 2792 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 232, 132, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 230, 130, 64) 18496 _________________________________________________________________ batch_normalization_3 (Batch (None, 230, 130, 64) 920 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 76, 43, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 209152) 0 _________________________________________________________________ dense_1 (Dense) (None, 200) 41830600 _________________________________________________________________ batch_normalization_4 (Batch (None, 200) 800 _________________________________________________________________ dense_2 (Dense) (None, 8) 1608 ================================================================= Total params: 41,858,912 Trainable params: 41,855,256 Non-trainable params: 3,656 _________________________________________________________________</span></span></code> </pre> <br>  <i>The final architecture of the model in a nutshell, which gave about <b>80% accuracy</b></i> <br><br>  To begin, let's see what is happening in the hen house.  To successfully make an applied project, you should always try to control the maximum part of the technological chain.  It is unlikely that the project will succeed if the data comes in the form of ‚Äúhere you are 10 terabytes of video in terrible quality, make us a real-time video recognition model for 10,000 rubles‚Äù or ‚Äúhere you are 50 photos of beer labels‚Äù (humor, but these are examples , based on real ... projects, thank God not mine). <br><br><ul><li>  Chickens constantly climb in the nest, just to sit or out of curiosity, especially the young; </li><li>  Young chickens (white in the photo above) - until a certain point did not lay eggs in the nest, but simply picked other people's eggs; </li><li>  Chickens have fights, but in 95% of cases the rules are observed: i) in the nest 1 chicken ii) during the stay of the chicken in the nest 10-20 unique pictures are made iii) the chicken usually completely covers the eggs with itself iv) the current view of the hens is a product several iterations of the search for the optimal location of the camera; </li><li>  On the day, about 300-400 photos are made, 5-8 eggs are laid (young chickens, due to stupidity and fear, literally rushed under the floor, then after a collection of 22 eggs was found under the floor, they began to nest in the nest); </li><li>  At night, the chickens turn off the light - they go to bed; </li><li>  About 900 photos were tagged; </li></ul><br>  <b>Let's see how many photos came out for each class:</b> <br><img src="https://habrastorage.org/getpro/habr/post_images/f1c/96f/a1d/f1c96fa1d2f71a856cb271faf22e1e53.png" alt="image"><br><br>  Initially, having seen such a situation, I thought that I could even distinguish photos with the number of eggs, but no.  To maximize accuracy on such a small sample, you need to try to do a few things: <br><br><ul><li>  Choose the right data preprocessing algorithms (it is logical that the chickens are spinning in the nest, so you can rotate the photos at least 180 degrees!); </li><li>  Choose an adequate in size and complexity model; </li></ul><br>  With preprocessing is simple - you just need to take the simplest linear model and try 5-10 combinations of parameters - with a sufficient amount of programmed sugar in Keras, this is done by substituting the parameters for such a small dataset for an hour (training 10 epochs of the model takes 50-100 seconds for 1 epoch ).  Not least, this is also done for the purpose of regularization.  Here is an example of one of the best practices: <br><br><pre> <code class="python hljs">genImage = imageGeneratorSugar( featurewise_center = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, samplewise_center = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, featurewise_std_normalization = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, samplewise_std_normalization = <span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, rotation_range = <span class="hljs-number"><span class="hljs-number">90</span></span>, width_shift_range = <span class="hljs-number"><span class="hljs-number">0.05</span></span>, height_shift_range = <span class="hljs-number"><span class="hljs-number">0.05</span></span>, shear_range = <span class="hljs-number"><span class="hljs-number">0.2</span></span>, zoom_range = <span class="hljs-number"><span class="hljs-number">0.2</span></span>, fill_mode=<span class="hljs-string"><span class="hljs-string">'constant'</span></span>, cval=<span class="hljs-number"><span class="hljs-number">0.</span></span>, horizontal_flip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>, vertical_flip=<span class="hljs-keyword"><span class="hljs-keyword">False</span></span>)</code> </pre><br>  You can see the almost complete log of experiments here: <br><br><ul><li>  ipynb jupyter notebook'a with all the calculations and boiler-plate code with explanations can be downloaded <a href="https://drive.google.com/open%3Fid%3D0B1WwFMq7KtPubjhsazQtUWpiZjg">here</a> , html version <a href="https://drive.google.com/open%3Fid%3D0B1WwFMq7KtPuWjZfT2dJVVdUcU0">here</a> ; </li></ul><br>  As for the choice of the model itself, having tested different architectures on a number of datasets, I stopped at about this model (Dropout forgot - but tests with it did not increase accuracy): <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">getTestModelNormalize</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(inputShapeTuple, classNumber)</span></span></span><span class="hljs-function">:</span></span> model = Sequential([ BatchNormalization(axis=<span class="hljs-number"><span class="hljs-number">1</span></span>, input_shape = inputShapeTuple), Convolution2D(<span class="hljs-number"><span class="hljs-number">32</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), BatchNormalization(axis=<span class="hljs-number"><span class="hljs-number">1</span></span>), MaxPooling2D((<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)), Convolution2D(<span class="hljs-number"><span class="hljs-number">64</span></span>, (<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>), activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), BatchNormalization(axis=<span class="hljs-number"><span class="hljs-number">1</span></span>), MaxPooling2D((<span class="hljs-number"><span class="hljs-number">3</span></span>,<span class="hljs-number"><span class="hljs-number">3</span></span>)), Flatten(), Dense(<span class="hljs-number"><span class="hljs-number">200</span></span>, activation=<span class="hljs-string"><span class="hljs-string">'relu'</span></span>), BatchNormalization(), Dense(classNumber, activation=<span class="hljs-string"><span class="hljs-string">'softmax'</span></span>) ]) model.compile(Adam(lr=<span class="hljs-number"><span class="hljs-number">1e-4</span></span>), loss=<span class="hljs-string"><span class="hljs-string">'categorical_crossentropy'</span></span>, metrics=[<span class="hljs-string"><span class="hljs-string">'accuracy'</span></span>]) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> model</code> </pre><br>  Here is its graphic representation: <br><br><img src="https://habrastorage.org/getpro/habr/post_images/89a/e77/8cb/89ae778cb7ed3accba409e5d3e74e314.png" alt="image"><br><br>  <b>Let me explain the composition of the layers and why, in principle, almost any modern CNN looks very similar:</b> <br><br><ol><li>  BatchNormalization - averages incoming data (subtracts the average and divides by the standard deviation), which accelerates the training of the neural network dozens of times.  If you are interested for some reason, go into recursion <a href="http://spark-in.me/post/neural-chicken-coop-0">here</a> or <a href="http://course.fast.ai/">here</a> .  Required to use; </li><li>  Convolution2D - convolutional layers.  In essence, they are filters so that the neural network can learn to recognize abstract images.  In theory, the visualization should have been included in this article, but I didn‚Äôt master it in the end (if you don‚Äôt know how the neural network looks from inside, then here are the links for you <a href="https://t.me/snakers4/932">1</a> 2 <a href="http://yosinski.com/deepvis">3</a> ; </li><li>  MaxPooling2D - averaging of filter values.  Mandatory after convolutional layers; </li><li>  Dropout - essentially needed for regularization.  I did not include it in this specification of the model, because I took the code from my other project and simply forgot it because of the high accuracy of the model; </li><li>  Flatten - then insert a dense layer, otherwise it will not work; </li></ol><br>  The specification of a model with a dropout looks like this (but for some reason it did not give an increase in accuracy at first glance, as I didn‚Äôt pervert then, probably regularization and so was achieved by preprocessing photo + large photos). <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># let's try a model w dropout! def getTestModelNormalizeDropout(inputShapeTuple, classNumber): model = Sequential([ BatchNormalization(axis=1, input_shape = inputShapeTuple), Convolution2D(32, (3,3), activation='relu'), BatchNormalization(axis=1), Dropout(rate=0.3), MaxPooling2D((3,3)), Convolution2D(64, (3,3), activation='relu'), BatchNormalization(axis=1), Dropout(rate=0.1), MaxPooling2D((3,3)), Flatten(), Dense(200, activation='relu'), BatchNormalization(), Dense(classNumber, activation='softmax') ]) model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy']) return model</span></span></code> </pre><br>  Note that categorical_crossentropy is used with softmax.  Adam is used as an optimizer.  I will not stop why I chose them (this combination is essentially a standard), but you can read here and see <a href="https://t.me/snakers4/1034">1</a> , <a href="https://t.me/snakers4/1007">2</a> and <a href="https://t.me/snakers4/975">3 here</a> . <br><br>  <b>The process of finding the optimal model looks like this:</b> <br><br><ol><li>  We try different ways to pre-process data with the simplest model (all of a sudden the weight of the simplest dense model of 3 layers weighs a few gigabytes for large photos, which is not very good) and choose the optimal one; </li><li>  We try different architectures of convolutional (or any other) neural networks, choose the optimal one; </li><li>  When a significant benchmark is reached (50% + accuracy, 75% accuracy - choose from the task, the more classes, the softer the benchmark should be) - you need to analyze what model photos and which classes have problems with; </li><li>  Ensembles and fine-tuning of the outer layers of large neural network architectures - optional - if you want to win the competition; </li><li>  You can try to mix in your sample 20-30% of photos from the test, validation, or simply unpartitioned part of the dataset (semi-supervised, the code works, but I was the one who constantly died in this task, I scored); </li><li>  Repeat to infinity; </li></ol><br><pre> <code class="bash hljs">42/42 [==============================] - 87s - loss: 2.4055 - acc: 0.2783 - val_loss: 4.7899 - val_acc: 0.1771 Epoch 2/15 42/42 [==============================] - 90s - loss: 1.7039 - acc: 0.4049 - val_loss: 2.4489 - val_acc: 0.2011 Epoch 3/15 42/42 [==============================] - 90s - loss: 1.4435 - acc: 0.4827 - val_loss: 2.1080 - val_acc: 0.2402 Epoch 4/15 42/42 [==============================] - 90s - loss: 1.2525 - acc: 0.5311 - val_loss: 2.4556 - val_acc: 0.2179 Epoch 5/15 42/42 [==============================] - 85s - loss: 1.2024 - acc: 0.5549 - val_loss: 2.2180 - val_acc: 0.1955 Epoch 6/15 42/42 [==============================] - 84s - loss: 1.0820 - acc: 0.5858 - val_loss: 1.8620 - val_acc: 0.2849 Epoch 7/15 42/42 [==============================] - 84s - loss: 0.9475 - acc: 0.6535 - val_loss: 2.1256 - val_acc: 0.1955 Epoch 8/15 42/42 [==============================] - 84s - loss: 0.9283 - acc: 0.6665 - val_loss: 1.2578 - val_acc: 0.5642 Epoch 9/15 42/42 [==============================] - 84s - loss: 0.9238 - acc: 0.6792 - val_loss: 1.1639 - val_acc: 0.5698 Epoch 10/15 42/42 [==============================] - 84s - loss: 0.8451 - acc: 0.6963 - val_loss: 1.4899 - val_acc: 0.4581 Epoch 11/15 42/42 [==============================] - 84s - loss: 0.8026 - acc: 0.7183 - val_loss: 0.9561 - val_acc: 0.6480 Epoch 12/15 42/42 [==============================] - 84s - loss: 0.8353 - acc: 0.7064 - val_loss: 1.0533 - val_acc: 0.6145 Epoch 13/15 42/42 [==============================] - 84s - loss: 0.7687 - acc: 0.7380 - val_loss: 0.9039 - val_acc: 0.6760 Epoch 14/15 42/42 [==============================] - 84s - loss: 0.7683 - acc: 0.7287 - val_loss: 1.0038 - val_acc: 0.6704 Epoch 15/15 42/42 [==============================] - 84s - loss: 0.7076 - acc: 0.7451 - val_loss: 0.8953 - val_acc: 0.7039</code> </pre><br>  <i>And this is the hypnotizing progress of the indicator ...</i> <br><br>  By the way, this code snippet will help you if you want to make a semi-supervised model in keras <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># Mix iterator class for pseudo-labelling class MixIterator(object): def __init__(self, iters): self.iters = iters self.multi = type(iters) is list if self.multi: self.N = sum([it[0].N for it in self.iters]) else: self.N = sum([it.N for it in self.iters]) def reset(self): for it in self.iters: it.reset() def __iter__(self): return self def next(self, *args, **kwargs): if self.multi: nexts = [[next(it) for it in o] for o in self.iters] n0s = np.concatenate([n[0] for n in o]) n1s = np.concatenate([n[1] for n in o]) return (n0, n1) else: nexts = [next(it) for it in self.iters] n0 = np.concatenate([n[0] for n in nexts]) n1 = np.concatenate([n[1] for n in nexts]) return (n0, n1) mi = MixIterator([batches, test_batches, val_batches) bn_model.fit_generator(mi, mi.N, nb_epoch=8, validation_data=(conv_val_feat, val_labels))</span></span></code> </pre><br>  But if you go back to our chickens, then after step 3, after analyzing the results of the model, I saw a funny pattern. <br><br>  <b>Here is the ratio of the correct predictions to the number of photos</b> <br><img src="https://habrastorage.org/getpro/habr/post_images/e67/ee6/110/e67ee611011ea76095efc5e45a9ce8a6.png" alt="image"><br><br>  But the main source of incorrect classifications <br><br><img src="https://habrastorage.org/getpro/habr/post_images/915/e85/865/915e85865fbd0ecd0ec55e9df59b6ddf.png" alt="image"><br><br>  Strangely enough, on such a small sample it turned out that the neural network separates chickens from eggs well, but finds eggs with difficulty.  I was able to build a separate model, which counted the eggs with 50% accuracy, but did not continue to train it, perhaps it is easier to consider eggs <a href="http://www.pyimagesearch.com/2014/07/21/detecting-circles-images-using-opencv-hough-circles/">so</a> . <br><br>  Having placed all the eggs in one class, I ended up getting a model with about 80% accuracy on a small dataset. <br><br><h2>  <b>3. What will go on prod?</b> </h2><br>  The last stroke was deployed models for the prod.  Here, of course, you need to save the weight and write a couple of functions that will spin in the daemon in the background.  But here came the <a href="https://github.com/llSourcell/how_to_deploy_a_keras_model_to_production">paste</a> from the video at the beginning of the article. <br><br>  The whole code of such a combat deployment looks like this (here tensor flow is used as a backend). <br><br>  That's basically it. <br><br><pre> <code class="python hljs"><span class="hljs-comment"><span class="hljs-comment"># dependencies import numpy as np import keras.models from keras.models import model_from_json from scipy.misc import imread, imresize,imshow import tensorflow as tf‚Äã‚Äã In [3]: def init(model_file,weights_file): json_file = open(model_file,'r') loaded_model_json = json_file.read() json_file.close() loaded_model = model_from_json(loaded_model_json) #load woeights into new model loaded_model.load_weights(weights_file) print("Loaded Model from disk") #compile and evaluate loaded model loaded_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy']) #loss,accuracy = model.evaluate(X_test,y_test) #print('loss:', loss) #print('accuracy:', accuracy) graph = tf.get_default_graph()‚Äã return loaded_model,graph‚Äã loaded_model,graph = init('model.json','model7_20_epochs.h5')‚Äã In [4]: def predict(img_file, model): # here you should read the image img = imread(img_file,mode='RGB') img = imresize(img,(400,800)) #convert to a 4D tensor to feed into our model img = img.reshape(1,400,800,3) # print ("debug2") #in our computation graph with graph.as_default(): #perform the prediction out = model.predict(img) print(out) print(np.argmax(out,axis=1)) # print ("debug3") #convert the response to a string response = (np.argmax(out,axis=1)) return response In [6]: chick_dict = {0: 'back_spot', 1: 'beauty', 2: 'dirty', 3: 'egg', 4: 'empty', 5: 'light_back', 6: 'ordinary', 7: 'psycho', 8: 'red_star', 9: 'red_twin', 10: 'young_long'}‚Äã In [7]: prediction = predict('test.jpg',loaded_model) print (chick_dict[prediction[0]]) [[ 2.34186242e-04 5.02209296e-04 5.61403576e-04 9.51264706e-03 2.03147720e-04 1.70257801e-04 4.71635815e-03 5.06504579e-03 1.84403792e-01 7.92831838e-01 1.79908809e-03]] Out [9] red_twin In [11]: import matplotlib.pyplot as plt img = imread('test.jpg',mode='RGB') plt.imshow(img) plt.show()</span></span></code> </pre><br>  Model predicts faithful chicken) Wait for the final bot, which will post faithful chickens. </div><p>Source: <a href="https://habr.com/ru/post/330738/">https://habr.com/ru/post/330738/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../330726/index.html">How to scale bitcoin blockchain</a></li>
<li><a href="../330728/index.html">The digest of interesting materials for the mobile developer # 207 (June 05-12)</a></li>
<li><a href="../330730/index.html">Win32 / Industroyer: A New Threat to Industrial Control Systems</a></li>
<li><a href="../330732/index.html">A very rude approach to determining a person‚Äôs language (or how to understand a person‚Äôs language from a normal corporate base)</a></li>
<li><a href="../330736/index.html">Crazy Artificial Intelligence</a></li>
<li><a href="../330740/index.html">Neurocoole Part 3. About chick marking</a></li>
<li><a href="../330744/index.html">One programming lesson</a></li>
<li><a href="../330746/index.html">Principles of software testing. Personal translation from the book ‚ÄúThe Art of Testing‚Äù by G. Myers</a></li>
<li><a href="../330748/index.html">DevConf :: BackEnd already this week on June 17th on Saturday, the program is formed</a></li>
<li><a href="../330750/index.html">Junior, who on the first day of work deleted the database from production</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>