<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>WAFL File System ‚Äî NetApp Foundation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="In my first post on this blog, I promised to tell you about NetApp "from the technical side." However, before I talk about most of the features found ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>WAFL File System ‚Äî NetApp Foundation</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/storage/habraeffect/f5/fa/f5fa579425d6c634d3384d9f64729a48.jpg"><br>  In my first post on this blog, I promised to tell you about NetApp "from the technical side."  However, before I talk about most of the features found in NetApp systems, I‚Äôll have to talk about the ‚Äúfoundation‚Äù, what is the basis of any NetApp storage system ‚Äî the special structure of data organization that is traditionally called the ‚ÄúWAFL file system‚Äù - Write Anywhere File Layout - File Structure with Record Everywhere, if translated literally. <br><br>  If you find that ‚Äúfor Habr‚Äù the text is rather dry, then be patient, it will be more interesting, but I cannot tell you about the structure of what underlies the vast majority of practical ‚Äúfeatures‚Äù of NetApp.  In the future, it will be much more likely to refer ‚Äúfor those interested‚Äù to a detailed explanation in the following posts about more practical ‚Äúfeatures‚Äù. <br>  One way or another, but almost everything that NetApp can uniquely grows is from the file system invented in the early 90s by David Hitz and James Lau, co-founders of the Network Appliance startup.  A good argument for how important and useful in the future development may be the initially competent and thoughtful ‚Äúarchitecture‚Äù of the product. <br><a name="habracut"></a><br><br>  But first, why did NetApp need its own file system, what did the existing ones not suit it?  This is what one of the creators of NetApp, co-founder and CTO of Dave Hitz, says about this: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <blockquote>  ‚ÄúInitially, we did not intend to write our file system.  It seemed to us that Berkeley FFS was quite suitable for us.  But several inherent insoluble problems soon forced us to deal with our own file system. <br><br>  Testing the integrity of the file system in the event of an abnormal stop (fsck) with FFS at the time was done unacceptably slowly.  With the increase in the size of the file system, the situation has deteriorated more and more, which made it almost impossible for our idea to combine all disks into a single disk volume with a single space. <br><br>  We wanted to make the device as simple as possible.  To do this, we had to combine all the disks into a single file system.  At that time (we are talking about the beginning of the 90s, approx track), people usually created a separate file system on each individual disk and mounted them together in a common tree, which was inconvenient and non-universal. <br><br>  Using many disks at once, with a shared file system on them, we would need a RAID.  There were two reasons for this.  First, by combining multiple disks into a single file system at once, you risked losing the entire file system as a result of the failure of one of the multiple disks.  Second, the probability of failure increased with an increase in the number of disks.  We needed RAID, and we decided to implement RAID simply as part of our file system. <br><br>  Previously existing file systems worked on top of RAID, and they didn‚Äôt know how the data was placed on the physical level, so they could not optimize their work with this information.  Having built our own file system, which knew all the features of data layout on multiple physical disks, and independently implementing RAID, we were able to optimize its work as much as possible. <br>  That's why, after looking at all this, we decided to write our own file system for our device. ‚Äù </blockquote><br><br>  <i>(who said ‚Äúwith blackjack and whores‚Äù?;)</i> <br><br>  The main principle underlying the functioning of the WAFL file system, which distinguishes it from all existing file systems, may seem a bit paradoxical: a once-written data block in the file is not overwritten.  It can only be deleted (cleared), but NOT OVERWRITTEN. <br>  Thus, any data block on the file system can be either ‚Äúempty‚Äù, and then it can be written, or ‚Äúwritten‚Äù, and then it can be either read or erased when it is no longer referenced by any entry of the overlying structure.  Writing (rewriting) into a block that is already occupied with some kind of data is impossible by the internal logic of the file system. <br>  The necessary changes in the contents of the recorded file are ‚Äúadded‚Äù to it, to the free space of the file system. <br><br><img src="https://habrastorage.org/storage/habraeffect/88/2d/882d97c89d0d51d6f5f13a8b9661bb06.png"><br><br><blockquote>  Consider the steps. <br>  In the first step, we see a file that occupies three blocks on the file system: A, B, and C. <br>  Step 2. The program using this file wants to change the data in its middle, which is stored in block B. Opening the file for writing, it changes this data, but FS, instead of changing the data in an already recorded block B, writes them to a completely empty block in the free blocks area . <br>  Step 3. The file system swaps the pointer of the blocks used by the file to the recorded block B 'from block B. And since nobody else refers to block B, it is released and becomes empty. <br></blockquote><br><br>  This kind of model allows us to obtain two important features of use: <br><ul><li>  Turn random entries on a storage system into sequentals. </li><li>  It is very simple and effective to organize the so-called Snapshots, Snapshots, or instant "snapshots" of the state of the data on the disks. </li></ul><br>  Let us examine these moments in more detail. <br><br>  The structure of the organization of the WAFL file system blocks will be clear to everyone familiar with the file system of the ‚Äúinodes‚Äù (inodes) type, of all the numerous ‚Äúunix‚Äù heirs of Berkeley's FFS. <br>  File data blocks are addressed using a structure called 'inode'. <br>  The inode can indicate both the blocks of the file data itself and the intermediate inodes of the ‚Äúindirect addressing‚Äù, forming a kind of ‚Äútree‚Äù, where in the ‚Äúroot‚Äù is the ‚Äúroot inode‚Äù, and at the very end of the branches there are blocks of the data itself. <br><br><img src="https://habrastorage.org/storage/habraeffect/22/4f/224f25a5a3ebc5f5873748850342fae7.png"><br><br>  Such a scheme is used by the majority of ‚ÄúUnix‚Äù file systems, what is new in WAFL? <br><br>  Since, as I mentioned above, the contents of the already recorded file system data does not change, and new blocks are only added to the ‚Äútree‚Äù (and remain there until at least someone is referring to them), it is obvious that having saved the ‚Äúroot‚Äù of such tree, root inode, at some point in time, we get a complete ‚Äúsnapshot‚Äù of all the data on the disk at that moment.  After all, the contents of any already recorded blocks (for example, with the previous contents of the file) are guaranteed not to change. <br>  By saving a single block containing the root inode, we save all the data to which it somehow refers. <br>  This makes it easy to create snapshots of data states on disks. <br>  Such a ‚Äúsnapshot‚Äù looks like the complete contents of your entire file system at a certain point in time, the one in which the root inode was saved.  Each file on it is available for reading (change it, of course, will not work). <br><br><img src="https://habrastorage.org/storage/habraeffect/ef/38/ef38a29689d93ec10c76d87db4daec4e.png"><br><br><blockquote>  Consider more.  In the first step, we again have a file that occupies three blocks of the file system. <br>  In step 2, we create a snapshot.  As already explained above, this is simply a copy of the links of the active file system to the blocks occupied by the files.  Now each of blocks A, B and C has two links.  One from File1, the second from this file from the snapshot made.  This is similar to a link in the UNIX file system, but the links do not lead to a file, but to data blocks of this file on the file system.  The program changes the data in block B of the file, instead of which new content is recorded in block B '. <br>  Step 3. However, the old block B does not become empty, as it is referenced from the snapshot.  Now, if we want to read the contents of File1 before the changes, we can use the file ~ snap / File1, which will turn to blocks A, B and C. And the new content is available when reading File1 itself - A, B 'and C. Thus we have both old, through its snapshot, and new contents of the file system. <br></blockquote><br><br>  As I said above, such an organization of writing to the file system makes it possible to achieve several important things from the point of view of the system: <br><ul><li>  Turn the random write operation into a much faster and more efficient storage system sequental write operation (since the entire group of entries in the cache, logically meant for different files in different places, FS can be written into one consecutive segment of empty blocks). </li><li>  Record to discs effectively, "full strip". </li></ul><br>  Unusually, in NetApp systems, they also implemented in their file system, which, I remind you, is also itself a RAID-ohm and volume manager, an unusual type of RAID-RAID type 4. This, I recall, is the ‚Äúalternation with parity‚Äù model, similar to the usual RAID-5, but the parity disk is allocated to a separate physical disk (and not ‚Äúsmeared‚Äù throughout the RAID as in type 5). <br><br>  Usually, RAID-4 is rarely used ‚Äúin nature‚Äù, since it has one, but a very serious drawback - its performance, with its ‚Äúnormal‚Äù use, rests on the performance of the parity disk, each write operation on a RAID group has a change operation parity on the disk, which means that no matter how you increase the size of the RAID group, its total performance will still rest on the performance of one disk for writing. <br>  On the other hand, RAID-4, as RAID with a dedicated parity (as opposed to, for example, RAID-5, ‚Äúwith unallocated parity‚Äù) has a very solid advantage, which is the ability to expand the capacity of the RAID by adding disks ‚Äúinstantly‚Äù.  Adding a disk to RAID-4 does not lead to the need to ‚Äúrebuild a RAID‚Äù, immediately after physically inserting the HDD and adding a data disk to an existing RAID, we can start writing to it, extending both the internal WAFL file system and the ones on top of it structures, such as CIFS data volumes, NFS, or LUNs.  For the device, initially focused on ease of maintenance, and non-IT companies as customers, this was a big plus. <br>  However, what to do with speed? <br><br>  It turned out that if we write on RAID-4 consistently, and with ‚Äúfull RAID stripes,‚Äù then there is simply no problem with hitting the parity disk.  A stripe prepared by one write operation is written to all the disks of the RAID group, then it waits for the next stripe to be assembled, and writes it in one sitting. <br><br>  What prevents to do also on any file system? <br>  "Random-ness" records.  The overwhelming majority of records on modern problems are rather chaotic records on disk space.  Since in the classical file system we are forced to rewrite thousands, tens of thousands of data blocks, randomly scattered across the disk space, and usually without any logic from the disk point of view, it becomes extremely difficult to collect the ‚Äúfull stripe‚Äù in the cache.  To do this, we need to either increase the cache space for writing, or increase the time spent by the data in it, which increases the likelihood that, finally, we will at some point collect the stripe we need in this ‚Äúpuzzle‚Äù and will be able to reset it as efficiently as possible. on raid. <br><br>  However, as you remember, in WAFL we don‚Äôt need to drive the pancake heads to re-record a couple (kilo) bytes somewhere in the middle of the file.  Need to change data?  No problem.  Here we have a large empty segment, write everything accumulated at once there, and then move the pointer in the inode to a new place.  All waiting by the recording group of bytes at once, not driving on the pancake head, in one step, sequentially (Sequental), leaks to the disks.  Recording is completed, and the parity value, precomputed for the stripe, is also recorded in one step on the parity disk. <br><br>  Of course, nothing is given for free, and turning a random write into a consistent one can in some cases turn a ‚Äúconsistent‚Äù reading into a ‚Äúrandom‚Äù reading.  However, practice shows that there are not so many successive reads (as well as consecutive records) in practice, and this ‚Äúsmearing‚Äù of a file in a file system affects relatively little on random reading. <br>  In addition, a large reading cache most often successfully copes with this problem. <br>  For the same cases, when the performance of the SEQUENTIAL reading is really important (for example, during backup, which occurs mostly sequentially within a file), a special background optimization process runs on NetApp systems that continuously increases the degree of data ‚Äúconsistency‚Äù (as in unix FS) is called 'contiguous'), while the data, evaluated as being consistently located, is collected into longer consecutive "chunks", which facilitates their further sequential reading  not (sequental read). <br><br>  The second feature of WAFL is an unusual ‚Äújournaling‚Äù scheme.  Here it must be said that for 1993, when WAFL appeared, logging was still quite rare ‚Äúfeatures‚Äù in file systems, and one of the tasks in creating the AWFL was organizing consistent data storage and a quick restart after a failure.  During these years, a ‚Äúdirty‚Äù restart of large file systems on UNIX servers often caused fsck to start for many minutes, and sometimes even hours. <br><br>  In WAFL, a somewhat unusual scheme is used, with the ‚Äújournal‚Äù being moved to a separate physical device - NVRAM. <br><br> <a title="NVRAM5" href=""><img src="https://habrastorage.org/storage/habraeffect/9e/e0/9ee0799b812858a3de1616cf66ecae9d.jpg"></a> <br><br>  NVRAM is an unusual device.  Although outwardly, it really resembles a caching controller familiar today, with RAM and a battery for powering it on a turned off system and storing data in it, the principle of its operation is completely different. <br><br>  The data and commands received by the storage system (for example, NFS operations) are pre-accumulated in NVRAM, after which they are ‚Äúatomically‚Äù transferred to disks, creating so-called Consistency Points (CP), ‚Äúconsistency points‚Äù, ‚Äúconsistent‚Äù CP is also such a special internal ‚Äúsnapshot‚Äù, using the same logical mechanism).  The operation of creating a CP occurs either every few seconds, or filling a certain amount of memory in NVRAM (high watermark).  Between CPs, the state of the file system is completely consistent, and switching to the next CP is instantly and atomically, so the file system is always in a consistent state, which is similar to how the SQL database is organized. <br><br>  From the point of view of the system, the data was either entirely successfully recorded or had not yet left NVRAM and did not hit the disks.  Since the file system does not overwrite its contents already recorded on the disks, it is very simple to organize ‚Äúatomicity‚Äù, a situation where data has already been changed in part of the blocks, but not in part (for example, a software or hardware failure has occurred) is simply impossible.  It is possible that some of the data considered as EMPTY blocks have already been entered, but the ‚Äúpointer‚Äù to the new CP (instant action) that captures the new state of the file system has not yet been moved, it remains in the consistent state of the previous CP, and this is not a problem.  If the system‚Äôs performance is restored, the recording will continue from the moment it was interrupted at the last successful CP and record the data in the battery-powered NVRAM (up to a week without powering the system as a whole), the data on which the process stopped, and then rearrange the CP .  ‚ÄúHalf-written‚Äù data at the time of failure will be ‚Äúat the FS level‚Äù considered ‚Äúempty‚Äù (and the data in NVRAM will not be considered abandoned) until the pointer to the CP is updated. <br>  That is, the operation of the system is as follows: <br><br>  NVRAM - system: It's time to write CP! <br>  System - NVRAM: Well, let's go. <br>  We write. <br>  NVRAM system: How are you doing there?  All is ready? <br>  NVRAM: Done, chef! <br>  System - File System: Well, God bless you!  Plusadin! <br>  File system: (increments its current state pointer to the generated CP) oink! <br><br>  Unsuccessful write option: <br>  NVRAM - system: Everything is lost, chef!  Disks do not respond!  Chef?  Are you here? <br>  System (boot): Phew, was it a shoeto?  NVRAM!  Stand still!  File system - nobody goes anywhere!  Use the last valid CP record before crashing!  NVRAM - now repeat the last write operation on which you interrupted from the very beginning! <br>  NVRAM: Yes, chef! <br><br>  Such an ingenious scheme makes the file system extremely holistic and stable, to the extent that many practical administrators over the years of NetApp systems have never encountered any incidents of integrity violations and the need to ‚Äúrun chkdsk (fsck for the inhabitants of the neighboring OS galaxy; ) ". <br>  In parentheses, I note that the WAFL integrity monitoring system in the system is, of course, if you need them. <br><br>  You can read more about the device and the principles of WAFL operation from the author‚Äôs publication ‚ÄúCreating a Specialized File System for the NFS File Server‚Äù, published in 1994 in the USENIX magazine, and describing the basic principles of building WAFL.  On the website of the Russian distributor of <a href="http://www.netwell.ru/production/techbiblioteka.php">Netwell</a> , in the regularly updated library of translated Best Practices, you can <a href="http://www.netwell.ru/docs/netapp/wp3002_wafl.pdf">download a translation of</a> this document. <br><br>  In the next article I will talk about how WAFL managed to implement data deduplication that does not slow down the storage system. </div><p>Source: <a href="https://habr.com/ru/post/99832/">https://habr.com/ru/post/99832/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../99826/index.html">Model of voluntary payment in conjunction with charity</a></li>
<li><a href="../99827/index.html">Linux basics from the founder of Gentoo. Part 1 (4/4): Glob-substitutions</a></li>
<li><a href="../99828/index.html">QUMO Libro e-book - 10 days of use</a></li>
<li><a href="../99830/index.html">The first experience of using the device for reading electronic books Qumo Libro</a></li>
<li><a href="../99831/index.html">FreeBSD 8.1 has been released. We are waiting for the announcement</a></li>
<li><a href="../99833/index.html">Facebook crossed the milestone of 500 million active users</a></li>
<li><a href="../99838/index.html">Launch of the official site LiveStreet CMS</a></li>
<li><a href="../99840/index.html">Twitter will open its own data center</a></li>
<li><a href="../99842/index.html">New approach to private online sales</a></li>
<li><a href="../99843/index.html">Shell hotkeys</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>