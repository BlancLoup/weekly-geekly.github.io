<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>And once again about GIL in Python</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Foreword 
 The area in which I was lucky to work is the computational electrophysiology of the heart . The physiology of cardiac activity is determine...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>And once again about GIL in Python</h1><div class="post__text post__text-html js-mediator-article"><h4>  Foreword </h4><br>  The area in which I was lucky to work is the <i>computational electrophysiology of the heart</i> .  The physiology of cardiac activity is determined by electrical processes occurring at the level of individual myocardial cells.  These electrical processes create an electric field that is fairly easy to measure.  Moreover, it is very well described in the framework of mathematical models of electrostatics.  This is where a unique opportunity arises to describe mathematically the work of the heart, and therefore to improve the methods of treating many heart diseases. <br><br>  During my work in this area, I have gained some experience in using various computing technologies.  Some questions that may be of interest not only to me, I will try to answer in the framework of this publication. <br><a name="habracut"></a><br><h4>  Scientific Python in Brief </h4><br>  Starting from the first years of university, I tried to find the perfect tool for the rapid development of numerical algorithms.  If we discard a number of frankly marginal technologies, I traveled between C ++ and MATLAB.  This continued until I discovered Scientific Python [1]. <br><br>  Scientific Python is a collection of Python libraries for scientific computing and scientific visualization.  In my work I use the following packages, which cover about 90% of my needs: <br><table><tbody><tr><th>  Title </th><th>  Description </th></tr><tr><td>  Numpy </td><td>  One of the basic libraries allows you to work with multidimensional arrays as with single objects in the MATLAB style. <br>  Includes the implementation of the basic procedures of linear algebra, the Fourier transform, work with random numbers, etc. </td></tr><tr><td>  Scipy </td><td>  The NumPy extension includes the implementation of optimization methods, working with sparse matrices, statistics, etc. </td></tr><tr><td>  Pandas </td><td>  Separate package for analyzing multidimensional data and statistics. </td></tr><tr><td>  Sympy </td><td>  A package of symbolic mathematics. </td></tr><tr><td>  Matplotlib </td><td>  Two-dimensional graphics. </td></tr><tr><td>  Mayavi2 </td><td>  Three-dimensional graphics based on VTK. </td></tr><tr><td>  Spyder </td><td>  Convenient IDE for interactive development of mathematical algorithms. </td></tr></tbody></table>  In Scientific Python, I found a great balance between a convenient high-level abstraction for the rapid development of numerical algorithms and a modern, advanced language.  But, as you know, there are no perfect tools.  And one of the rather critical problems in Python is the problem of parallel computing. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <h4>  Problems of parallel computing in Python. </h4><br>  By parallel computing in this article, I will understand SMP - symmetric multiprocessing with shared memory.  The use of CUDA and systems with separate memory (most commonly used standard MPI) will not touch. <br><br>  The problem is GIL.  GIL (Global Interpreter Lock) is a lock (mutex) that prevents multiple threads from executing the same bytecode.  This lock, unfortunately, is necessary, as the memory management system in CPython is not thread-safe.  Yes, GIL is not a Python language problem, but a CPython interpreter implementation problem.  But, unfortunately, the remaining implementations of Python are not too adapted for creating fast numerical algorithms. <br><br>  Fortunately, there are currently several ways to solve GIL problems.  Consider them. <br><br><h4>  Test task </h4><br>  Two sets of <i>N</i> vectors are given: <i>P = {p <sub>1</sub> , p <sub>2</sub> , ..., p <sub>N</sub> }</i> and <i>Q = {q <sub>1</sub> , q <sub>2</sub> , ..., q <sub>N</sub> }</i> in three-dimensional Euclidean space.  It is necessary to construct a matrix <i>R of</i> dimension <i>N x N</i> , each element of <i>r <sub>i, j of</sub></i> which is calculated by the formula: <br><br><div style="text-align:center;"><img src="https://habrastorage.org/files/41a/420/436/41a4204361134d7d8ffedc5a9468b9e0.png"></div><br>  Roughly speaking, you need to calculate a matrix that uses pairwise distances between all vectors.  This matrix is ‚Äã‚Äãquite often used in real calculations, for example, in the case of RBF interpolation or the solution of differents in the cp method of integral equations. <br><br>  In test experiments, the number of vectors is <i>N</i> = 5000. A 4-core processor was used for calculations.  Results are obtained on average time from 10 starts. <br><br>  Full implementation of test tasks can be glanced on GitHub [2]. <blockquote>  Correct comment in comments from "@chersaya".  This test task is used here as an example.  If you really need to calculate pairwise distances, use the scipy.spatial.distance.cdist function. </blockquote><br><h4>  Parallel implementation in C ++ </h4><br>  To compare the effectiveness of parallel computing in Python, I implemented this task in C ++.  The code of the main function is as follows. <br><br>  Single-processor implementation: <br><br><pre><code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//! Single thread matrix R calculation void spGetR(vector&lt;Vector3D&gt; &amp; p, vector&lt;Vector3D&gt; &amp; q, MatrixMN &amp; R) { for (int i = 0; i &lt; p.size(); i++) { Vector3D &amp; a = p[i]; for (int j = 0; j &lt; q.size(); j++) { Vector3D &amp; b = q[j]; Vector3D r = b - a; R(i, j) = 1 / (1 + sqrt(r * r)); } } }</span></span></code> </pre> <br>  Multiprocessing implementation: <br><br><pre> <code class="cpp hljs"><span class="hljs-comment"><span class="hljs-comment">//! OpenMP matrix R calculations void mpGetR(vector&lt;Vector3D&gt; &amp; p, vector&lt;Vector3D&gt; &amp; q, MatrixMN &amp; R) { #pragma omp parallel for for (int i = 0; i &lt; p.size(); i++) { Vector3D &amp; a = p[i]; for (int j = 0; j &lt; q.size(); j++) { Vector3D &amp; b = q[j]; Vector3D r = b - a; R(i, j) = 1 / (1 + sqrt(r * r)); } } }</span></span></code> </pre><br>  What is interesting here?  Well, first of all, I used a separate Vector3D class to represent the vector in three-dimensional space.  The overloaded operator "*" in this class has the meaning of a scalar product.  To represent a set of vectors, I used std :: vector.  For parallel computing, OpenMP technology was used.  To parallelize the algorithm, it suffices to use the "#pragma omp parallel for" directive. <br><br>  Results: <br><table><tbody><tr><td>  Single-processor C ++ </td><td>  224 ms </td></tr><tr><td>  Multiprocessor C ++ </td><td>  65 ms </td></tr></tbody></table>  Acceleration 3.45 times with parallel calculation, I think it is quite good for a quad-core processor. <br><br><h4>  Parallel Python implementations </h4><br><h5>  1. Native implementation in pure Python </h5><br>  In this test, I wanted to check how much the problem would be solved in pure Python without using any special packages. <br><br>  Solution code: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">sppyGetR</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(p, q)</span></span></span><span class="hljs-function">:</span></span> R = np.empty((p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>])) nP = p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>] nQ = q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(nP): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(nQ): rx = p[i, <span class="hljs-number"><span class="hljs-number">0</span></span>] - q[<span class="hljs-number"><span class="hljs-number">0</span></span>, j] ry = p[i, <span class="hljs-number"><span class="hljs-number">1</span></span>] - q[<span class="hljs-number"><span class="hljs-number">1</span></span>, j] rz = p[i, <span class="hljs-number"><span class="hljs-number">2</span></span>] - q[<span class="hljs-number"><span class="hljs-number">2</span></span>, j] R[i, j] = <span class="hljs-number"><span class="hljs-number">1</span></span> / (<span class="hljs-number"><span class="hljs-number">1</span></span> + sqrt(rx * rx + ry * ry + rz * rz)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> R</code> </pre><br>  Here <i>p</i> , <i>q</i> is the input data in the NumPy format of arrays of dimensions <i>(N, 3)</i> and <i>(3, N)</i> .  And then comes an honest cycle in Python that calculates the elements of the matrix R. <br><br>  Results: <br><table><tbody><tr><td>  Uniprocessor Python </td><td>  57,386 ms </td></tr></tbody></table>  Yes, yes, precisely 57 thousand milliseconds.  Somewhere 256 times slower than a single-processor C ++.  In general, this is not an option for numerical calculations. <br><br><h5>  2 Single-processor NumPy </h5><br>  In general, for calculations on Python using NumPy, sometimes you can not even think about parallelism.  So, for example, the procedure of multiplying two matrices by NumPy will eventually still be performed using low-level high-performance libraries of linear algebra in C ++ (MKL or ATLAS).  But, unfortunately, this is true only for the most typical operations and does not work in the general case.  Our test task, unfortunately, will be performed sequentially. <br><br>  The decision code is as follows: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">spnpGetR</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(p, q)</span></span></span><span class="hljs-function">:</span></span> Rx = p[:, <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>] - q[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>] Ry = p[:, <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">2</span></span>] - q[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">2</span></span>] Rz = p[:, <span class="hljs-number"><span class="hljs-number">2</span></span>:<span class="hljs-number"><span class="hljs-number">3</span></span>] - q[<span class="hljs-number"><span class="hljs-number">2</span></span>:<span class="hljs-number"><span class="hljs-number">3</span></span>] R = <span class="hljs-number"><span class="hljs-number">1</span></span> / (<span class="hljs-number"><span class="hljs-number">1</span></span> + np.sqrt(Rx * Rx + Ry * Ry + Rz * Rz)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> R</code> </pre><br>  Only 4 lines and no cycles!  That's what I love NumPy for. <br><br>  Results: <br><table><tbody><tr><td>  Single NumPy </td><td>  973 ms </td></tr></tbody></table>  About 4.3 times slower than single-processor C ++.  This is quite a good result.  For the vast majority of calculations, this performance is enough.  But all this is still single-processor results.  We go further to multiprocessing. <br><br><h5>  3 Multiprocessor NumPy </h5><br>  As a solution to problems with GIL, it is traditionally proposed to use several independent execution processes instead of several execution threads.  All is good, but there is a problem.  Each process has an independent memory, and we need to transfer a matrix of results to each process.  To solve this problem in Python multiprocessing, the RawArray class is introduced, which provides the ability to split one array of data between processes.  I don‚Äôt know exactly what is the basis of RawArray.  It seems to me that this is memory mapped files. <br><br>  The decision code is as follows: <br><br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mpnpGetR_worker</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(job)</span></span></span><span class="hljs-function">:</span></span> start, stop = job p = np.reshape(np.frombuffer(mp_share.p), (<span class="hljs-number"><span class="hljs-number">-1</span></span>, <span class="hljs-number"><span class="hljs-number">3</span></span>)) q = np.reshape(np.frombuffer(mp_share.q), (<span class="hljs-number"><span class="hljs-number">3</span></span>, <span class="hljs-number"><span class="hljs-number">-1</span></span>)) R = np.reshape(np.frombuffer(mp_share.R), (p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>])) Rx = p[start:stop, <span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>] - q[<span class="hljs-number"><span class="hljs-number">0</span></span>:<span class="hljs-number"><span class="hljs-number">1</span></span>] Ry = p[start:stop, <span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">2</span></span>] - q[<span class="hljs-number"><span class="hljs-number">1</span></span>:<span class="hljs-number"><span class="hljs-number">2</span></span>] Rz = p[start:stop, <span class="hljs-number"><span class="hljs-number">2</span></span>:<span class="hljs-number"><span class="hljs-number">3</span></span>] - q[<span class="hljs-number"><span class="hljs-number">2</span></span>:<span class="hljs-number"><span class="hljs-number">3</span></span>] R[start:stop, :] = <span class="hljs-number"><span class="hljs-number">1</span></span> / (<span class="hljs-number"><span class="hljs-number">1</span></span> + np.sqrt(Rx * Rx + Ry * Ry + Rz * Rz)) <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mpnpGetR</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(p, q)</span></span></span><span class="hljs-function">:</span></span> nP, nQ = p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] sh_p = mp.RawArray(ctypes.c_double, p.ravel()) sh_q = mp.RawArray(ctypes.c_double, q.ravel()) sh_R = mp.RawArray(ctypes.c_double, nP * nQ) nCPU = <span class="hljs-number"><span class="hljs-number">4</span></span> jobs = utils.generateJobs(nP, nCPU) pool = mp.Pool(processes=nCPU, initializer=mp_init, initargs=(sh_p, sh_q, sh_R)) pool.map(mpnpGetR_worker, jobs, chunksize=<span class="hljs-number"><span class="hljs-number">1</span></span>) R = np.reshape(np.frombuffer(sh_R), (nP, nQ)) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> R</code> </pre><br>  We create divided arrays for input data and output matrix, create a pool of processes by the number of cores, divide the problem into subtasks and solve in parallel. <br><br>  Results: <br><table><tbody><tr><td>  Multiprocessor NumPy </td><td>  795 ms </td></tr></tbody></table>  Yes, faster than the single-processor version, but only 1.22 times.  As the number <i>N</i> grows, the efficiency of the solution grows.  But, in general and in general, our test problem is not too adapted for solving within the framework of a set of independent processes with independent memory.  Although for other tasks this option may be quite effective. <br><br>  At this, known to me, solutions for parallel programming using only Python are over.  Further, as we would not like, for release from GIL it is necessary to go down to the C ++ level.  But this is not as scary as it seems. <br><br><h5>  4 cython </h5><br>  Cython [3] is an extension to the Python language that allows you to embed C instructions in Python code.  Thus, we can take the code in Python and by adding a few instructions significantly speed up the narrow places in terms of performance.  Cython modules are converted to C code and then compiled into Python modules.  The code for solving our Cython problem is as follows: <br><br>  Cython uniprocessor: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@cython.boundscheck(False) @cython.wraparound(False) def spcyGetR(pp, pq): pR = np.empty((pp.shape[0], pq.shape[1])) cdef int i, j, k cdef int nP = pp.shape[0] cdef int nQ = pq.shape[1] cdef double[:, :] p = pp cdef double[:, :] q = pq cdef double[:, :] R = pR cdef double rx, ry, rz with nogil: for i in xrange(nP): for j in xrange(nQ): rx = p[i, 0] - q[0, j] ry = p[i, 1] - q[1, j] rz = p[i, 2] - q[2, j] R[i, j] = 1 / (1 + sqrt(rx * rx + ry * ry + rz * rz)) return R</span></span></code> </pre><br>  Multiprocessing Cython: <br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@cython.boundscheck(False) @cython.wraparound(False) def mpcyGetR(pp, pq): pR = np.empty((pp.shape[0], pq.shape[1])) cdef int i, j, k cdef int nP = pp.shape[0] cdef int nQ = pq.shape[1] cdef double[:, :] p = pp cdef double[:, :] q = pq cdef double[:, :] R = pR cdef double rx, ry, rz with nogil, parallel(): for i in prange(nP, schedule='guided'): for j in xrange(nQ): rx = p[i, 0] - q[0, j] ry = p[i, 1] - q[1, j] rz = p[i, 2] - q[2, j] R[i, j] = 1 / (1 + sqrt(rx * rx + ry * ry + rz * rz)) return R</span></span></code> </pre><br>  If we compare this code with the implementation in pure Python, then all we had to do was just to specify the types for the variables used.  GIL is released in one line.  The parallel loop is organized only by the prange instruction instead of xrange.  In my opinion, it is quite easy and beautiful! <br><br>  Results: <br><table><tbody><tr><td>  Cypro uniprocessor </td><td>  255 ms </td></tr><tr><td>  Multiprocessing cython </td><td>  75 ms </td></tr></tbody></table>  Wow  The execution time almost coincides with the execution time in C ++.  The lag is about 1.1 times in both single-processor and multiprocessor versions almost imperceptibly on real tasks. <br><br><h5>  5 numba </h5><br>  Numba [4] is a fairly new library, is in active development.  The idea here is about the same as in Cython - an attempt to go down to the C ++ level in Python code.  But the idea is implemented much more elegantly. <br><br>  Numba is based on LLVM compilers that allow compiling directly during the program execution (JIT compilation).  For example, to compile any procedure in Python, you just need to add the ‚Äújit‚Äù annotation.  Moreover annotations allow you to specify the types of input / output data, which makes the JIT compilation much more efficient. <br>  The task implementation code is as follows. <br><br>  Single CPU Numba: <br><br><pre> <code class="python hljs"><span class="hljs-meta"><span class="hljs-meta">@jit(double[:, :](double[:, :], double[:, :])) def spnbGetR(p, q): nP = p.shape[0] nQ = q.shape[1] R = np.empty((nP, nQ)) for i in xrange(nP): for j in xrange(nQ): rx = p[i, 0] - q[0, j] ry = p[i, 1] - q[1, j] rz = p[i, 2] - q[2, j] R[i, j] = 1 / (1 + sqrt(rx * rx + ry * ry + rz * rz)) return R</span></span></code> </pre><br>  Multiprocessor Numba: <br><pre> <code class="python hljs"><span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">makeWorker</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">()</span></span></span><span class="hljs-function">:</span></span> savethread = pythonapi.PyEval_SaveThread savethread.argtypes = [] savethread.restype = c_void_p restorethread = pythonapi.PyEval_RestoreThread restorethread.argtypes = [c_void_p] restorethread.restype = <span class="hljs-keyword"><span class="hljs-keyword">None</span></span> <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">worker</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(p, q, R, job)</span></span></span><span class="hljs-function">:</span></span> threadstate = savethread() nQ = q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> i <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(job[<span class="hljs-number"><span class="hljs-number">0</span></span>], job[<span class="hljs-number"><span class="hljs-number">1</span></span>]): <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> j <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> xrange(nQ): rx = p[i, <span class="hljs-number"><span class="hljs-number">0</span></span>] - q[<span class="hljs-number"><span class="hljs-number">0</span></span>, j] ry = p[i, <span class="hljs-number"><span class="hljs-number">1</span></span>] - q[<span class="hljs-number"><span class="hljs-number">1</span></span>, j] rz = p[i, <span class="hljs-number"><span class="hljs-number">2</span></span>] - q[<span class="hljs-number"><span class="hljs-number">2</span></span>, j] R[i, j] = <span class="hljs-number"><span class="hljs-number">1</span></span> / (<span class="hljs-number"><span class="hljs-number">1</span></span> + sqrt(rx * rx + ry * ry + rz * rz)) restorethread(threadstate) signature = void(double[:, :], double[:, :], double[:, :], int64[:]) worker_ext = jit(signature, nopython=<span class="hljs-keyword"><span class="hljs-keyword">True</span></span>)(worker) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> worker_ext <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">mpnbGetR</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(p, q)</span></span></span><span class="hljs-function">:</span></span> nP, nQ = p.shape[<span class="hljs-number"><span class="hljs-number">0</span></span>], q.shape[<span class="hljs-number"><span class="hljs-number">1</span></span>] R = np.empty((nP, nQ)) nCPU = utils.getCPUCount() jobs = utils.generateJobs(nP, nCPU) worker_ext = makeWorker() threads = [threading.Thread(target=worker_ext, args=(p, q, R, job)) <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> job <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> jobs] <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> thread <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> threads: thread.start() <span class="hljs-keyword"><span class="hljs-keyword">for</span></span> thread <span class="hljs-keyword"><span class="hljs-keyword">in</span></span> threads: thread.join() <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> R</code> </pre><br>  Compared to pure Python, only one abstract is added to a single-processor solution on Numba!  The multiprocessor version, unfortunately, is not so beautiful.  It is required to organize a pool of threads, in the manual mode to give GIL.  In previous releases, Numba attempted to implement a parallel loop with one instruction, but due to stability problems in subsequent releases, this feature was removed.  I‚Äôm sure this opportunity will be repaired over time. <br><br>  Results of performance: <br><table><tbody><tr><td>  Single CPU Numba </td><td>  359 ms </td></tr><tr><td>  Multiprocessor Numba </td><td>  180 ms </td></tr></tbody></table>  Slightly worse than Cython, but the results are still very decent!  And the decision itself is extremely elegant. <br><br><h4>  findings </h4><br>  I want to illustrate the results with the following diagrams: <br><br><div style="text-align:center;"><img src="http://habrastorage.org/files/543/17a/3b3/54317a3b3f714aaa8693769a5b0adbec.png" alt="image"></div><br>  Fig.  1. Results of uniprocessor calculations <br><br><div style="text-align:center;"><img src="http://habrastorage.org/files/5f4/366/5fc/5f43665fc4314bbf8a77f03a87f16d17.png" alt="image"></div><br>  Fig.  2. Results of multiprocessor computing <br><br>  It seems to me that the problems of GIL in Python for numerical calculations are almost overcome.  So far as a parallel computing technology, I would recommend Cython.  But I would carefully look to Numba. <br><br><h4>  Links </h4><br>  [1] Scientific Python: <a href="http://scipy.org/">scipy.org</a> <br>  [2] Full test source codes: <a href="https://github.com/alec-kalinin/open-nuance">github.com/alec-kalinin/open-nuance</a> <br>  [3] Cython: <a href="http://cython.org/">cython.org</a> <br>  [4] Numba: <a href="http://numba.pydata.org/">numba.pydata.org</a> <br><br>  PS In the comments "@chersaya" correctly indicated another method of parallel computing.  This is the use of the numexpr library.  Numexpr uses its own virtual machine written in C and its own JIT compiler.  This allows it to accept simple mathematical expressions as a string, compile and quickly calculate. <br><br>  Usage example: <br><pre> <code class="python hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> numexpr <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> ne a = np.arange(<span class="hljs-number"><span class="hljs-number">1e6</span></span>) b = np.arange(<span class="hljs-number"><span class="hljs-number">1e6</span></span>) result = ne.evaluate(<span class="hljs-string"><span class="hljs-string">"sin(a) + arcsinh(a/b)"</span></span>)</code> </pre></div><p>Source: <a href="https://habr.com/ru/post/238703/">https://habr.com/ru/post/238703/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../238689/index.html">News from the world of Node: npm 2.0, nvm for Windows, xtpl</a></li>
<li><a href="../238693/index.html">TinyScreen + TinyDuino - an easy way to create smart watches or smart glasses with your own hands</a></li>
<li><a href="../238695/index.html">Swiss scientists are ready to treat paralysis</a></li>
<li><a href="../238699/index.html">Having a point of view as a problem of modern society and IT-industry</a></li>
<li><a href="../238701/index.html">Hadoop. Detailed guide. 3rd edition</a></li>
<li><a href="../238705/index.html">Does Russia need the support of national software developers? Once upon a time in Russia</a></li>
<li><a href="../238707/index.html">Layfkhaki manual testing on mobile phones from 2GIS - Report from the conference SQA Days 15</a></li>
<li><a href="../238709/index.html">Get familiar with the WinJS library controls.</a></li>
<li><a href="../238711/index.html">ZeroNights 2014 - hackquest</a></li>
<li><a href="../238713/index.html">Explore Chinese routers on RT5350</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>