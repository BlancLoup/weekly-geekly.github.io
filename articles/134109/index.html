<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Internet project stability: how to // Reports from the Mail.Ru Technology Forum 2011: text of the report, video, presentation</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="The report ‚Äú Stability of the website: how to ‚Äù is another in a series of transcripts from the Mail.Ru 2011 Technology Forum . For details on how the ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Internet project stability: how to // Reports from the Mail.Ru Technology Forum 2011: text of the report, video, presentation</h1><div class="post__text post__text-html js-mediator-article"><blockquote>  <i>The report ‚Äú <a href="http://techforum.mail.ru/report/2">Stability of the website: how to</a> ‚Äù is another in a series of transcripts from <a href="http://techforum.mail.ru/">the Mail.Ru 2011 Technology Forum</a> .</i>  <i>For details on how the report decryption system works, see the article <a href="http://habrahabr.ru/company/mailru/blog/132953/">‚ÄúReverse‚Äù of the Mail.Ru Technologies Forum: High-tech in event-management</a> .</i>  <i>There, as well as on the Forum website ( <a href="http://techforum.mail.ru/">http://techforum.mail.ru</a> ) - links to transcripts of other reports.</i> </blockquote><br><video>  http://video.mail.ru/corp/glavred/ftmailru/75.html </video><br>  <i>( <a href="">Download</a> video version for mobile devices - iOS / Android H.264 480 √ó 368, size 170 Mb, video bitrate 500 kbps, audio - 64 kbps)</i> <br>  <i>( <a href="">Download</a> video version of higher resolution H.264 624 √ó 480, size 610 Mb, video bitrate 1500 kbps, audio - 128 kbps)</i> <br>  <i>( <a href="http://www.slideshare.net/tfmailru/ss-10214028/download">Download</a> presentation slides, 5.5Mb)</i> <br><br>  Probably, it is not a secret for you that every time when a large site has problems with work, interruptions, this causes a huge number of discussions.  I will try to tell you how to ensure that your site does not fall, or fall even less often.  What we do for this in Mail.Ru, what methods we use. <br><a name="habracut"></a><br><br><h4>  Stability is important </h4>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Stability for a website - is it important in principle?  There are many opinions on this topic - someone thinks that this is important, someone thinks that, among others, this is not the most important factor.  We consider this very important, and for this we have three main reasons. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/fe2/a57/bd2/fe2a57bd2a10cd61814500175e32d31c.png"><br><br>  As you know, most of the services on the Internet are free.  And as soon as your site does not work, it slows down, experiences some problems, your user goes to a competitor. <br><br>  The second reason is that the threshold for switching between services on the Internet is quite low.  You do not need to go anywhere, there is no high switching cost, as is the case with cellular operators, and you can always switch to another site when the service you are used to using does not work.  And, moreover, there is a huge number of auxiliary features that will help such an offended user to move from one site to another, therefore stability is important. <br><br>  The third reason is not ‚Äúabout the Internet‚Äù.  We should all understand that a person is arranged in such a way that negative emotions are always stronger than positive ones, and one accident on your site, as a rule, covers a huge amount of stable operation time, high-quality service, etc. <br><br>  For these three reasons, we believe that the site should work stably, but on average, the Internet does not have a very high level of stability.  According to our internal statistics, the average Russian Internet site has Uptime, i.e.  ‚ÄúWork time‚Äù - 98.6%.  The figure is not terrible, but if you take a closer look, it‚Äôs five days a year when the average Internet site is down.  This is quite a lot of time, and it tells us that the problem exists, and the problem is quite serious.  Of course, large Internet sites work a little better.  A large Internet site accounts for about four hours of ‚Äúnon-work‚Äù per year.  But, nevertheless, these are all large numbers, especially if we recall about five days. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/179/2bd/acf/1792bdacf5db00451d4596d02c23ad64.png"><br><br><h4>  Causes of accidents </h4><br><br>  Based on the statistics of Mail.ru, we have classified the reasons why accidents occur in our country and on other sites on the Russian Internet.  If we talk about quantitative distribution, I mean the number of accidents due to which this or that site‚Äôs functionality does not work, the site is down, etc., half of the sites crash on the Internet are due to the wrong software release, software release with bugs, crooked configs.  Accordingly, in half of the cases, again I want to draw attention - quantitatively, the developers, the system administrators who failed, failed badly, etc., are to blame.  Another 25% of cases occur in software accidents, to which I also include the load on the site, 16% of cases are network crashes, 8% are server and computer crashes on which the sites operate. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d33/30f/924/d3330f92405018ec0288c38421e11a3b.png"><br><br>  If you look at the qualitative distribution, it will be completely different.  As we can see, the worst accidents are network crashes and data center crashes.  It is very hard to protect yourself from them.  They, as a rule, completely shut down the entire resource or even large hosting, many resources do not work at the same time.  They occupy 30% each.  The following are crashes when new software is released.  This is followed by software crashes and, at the very end, equipment crashes, they make up a small percentage. <br><br><h4>  Monitoring </h4><br><br>  In order to talk about how to make your site stable, we first need to discuss how we know something is wrong with our site.  I am sure that all IT professionals who are in this audience will not argue with me about the need for monitoring.  Anyone going to be?  I am also sure that there are people in this audience who do not have monitoring.  Let me briefly list 10 reasons to have monitoring.  I know that you agree that you need monitoring, but let's still go through this list. <br><br><br><img src="https://habrastorage.org/getpro/habr/post_images/42a/25f/3e0/42a25f3e0d1e2c51c5a85c617882b0ce.png"><br><br>  The first reason sounds very simple: you are an excellent programmer, an even better system administrator, but your site will still fall.  Whatever you do, it will happen - tomorrow, after a year - it does not matter.  It will happen in any case, it is impossible to fight this, it will definitely be, so always remember this.  We need monitoring not only to find out that our site has fallen.  We need it in order to take some measures as soon as possible so that the site can work again.  When the site does not work a small amount of time - this is one problem for the user, when the site does not work for 5 days - this is a problem of a different order. <br><br>  You should not learn about the problems of your site from your users, because the time it takes to get your user to write to you, your tech support told you about the problem - it is too long.  During this time, a large number of users goes to your competitors.  The market is designed in such a way that most service providers, both Internet service providers and data center service providers, will not be liable in proportion to the value of your business, therefore, responsibility for your Uptime and for the time when your site is down , lies entirely on you, not on your service provider. <br><br>  As you know, when the site does not work, users begin to discuss: ‚Äúit does not work temporarily‚Äù or ‚Äúcompletely closed‚Äù or ‚Äúsomething happened‚Äù.  This discussion does great damage to your reputation.  Moreover, we noticed an interesting effect that even those users who were not directly affected by the accident, i.e.  they did not go to the site at the time when he was not working, participating in these discussions, they become as if they were involved.  They believe that something happened to them too and this is another problem for which we need to have monitoring, and we need to know when our site is down. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/e0e/964/bbd/e0e964bbd2515572c2eea8fad2b5cffd.png"><br><br>  All our sites are updated, we are constantly refining new features, we are launching new ‚Äúfeatures‚Äù, we are doing new features on our site, and this leads to the fact that we have a complex, large, long development process.  Monitoring is the thing that allows us to know that there are systemic problems in it.  Only through monitoring you can find out which team of your developers or a specific developer constantly creates problems in the stability of the site.  Only through monitoring can we know which module of our software we need to pay extra attention to. <br><br>  The next problem is that the Internet is large and you can work perfectly with your website, it can open perfectly, all functions can work for you, but you will be the only person for whom it works.  Accordingly, monitoring should be done in such a way that it allows you to see problems not only from your technical infrastructure, but also outside.  Now if we talk about Mail.Ru - it is monitored from about 100 points around the globe, so we learn about the problems not only with us, but also about problems with providers, about problems with providers of our providers.  Honestly, the end user doesn‚Äôt care where Mail.Ru doesn‚Äôt work, the phrase "provider provider" doesn‚Äôt say anything to it.  He understands that Mail.Ru does not work.  Therefore, we observe our network infrastructure, we observe the network infrastructure of those operators who provide services to us through our monitoring. <br><br>  Again, good monitoring will save you time, because not only does it warn you that you have problems, it will also say that it is not working at the moment, and it will save you time for understanding and the subsequent solution. <br><br>  You should be aware of the problem at any time.  Only in Russia there are nine time zones, and all over the world you can have users in those places where, say, the day is already, at the time when we have night. <br><br>  Well, the last argument.  As a rule, the cost of developing a site is quite high.  The cost of creating a monitoring compared to the cost of developing the site is almost none.  By creation, I mean customization, because there are a lot of solutions in open access that can be downloaded and delivered for free. <br><br>  Here are 10 reasons why all people in this room, who have their own websites, and who for some reason do not monitor them, should return from the Technology Forum and set up monitoring for themselves. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/4cc/e41/03b/4cce4103ba2f44f15c8385e00bb52e9a.png"><br><br><h4>  What we are monitoring in Mail.Ru </h4><br><br>  In fact, Mail.Ru has about 140 different types of monitoring, I will not list them all, in total we monitor about 150,000 objects on our servers.  We have a monitoring service.  For example, this may be a service response over HTTP.  We have functional monitoring of services, we always check whether letters reach us, whether they are downloaded via POP3, whether the user can add another user as a friend, etc.  We will definitely monitor network availability.  We check whether we are visible around the world.  We monitor the occupancy of our data warehouses, because most of the software is written in such a way that if the disks are full, then this data will simply deteriorate, and we will be forced to recover from the backup.  We also monitor the speed of the site, because there are thresholds at which it doesn‚Äôt matter that your site opens, relatively speaking, in half an hour.  For the user, this will mean that your site simply does not work.  We monitor all statistical emissions.  We collect a huge amount of statistics on Mail.Ru, for example, we know how many letters we send per day, we know how many messages we send per day, how many friendships we have in social networks.  We try to observe these quantitative metrics, and if any of them starts to grow or change rapidly, or some kind of ‚Äúoutlier‚Äù appears, we have it highlighted in monitoring, it is possible then to analyze why it happened, that this can be done, etc.  And these are just a few of the types of monitoring available. <br><br><br>  But one more important one remains.  If you do not have monitoring, it is very bad.  But when you have monitoring, but it doesn‚Äôt work - it‚Äôs terrible, because you get the false impression that your website is actually working now.  Therefore, the most important and very first monitoring in Mail.Ru is monitoring that monitoring is working. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/831/8d8/b68/8318d8b687180648534c643b95b38441.png"><br><br><h4>  Backup and balancing </h4><br><br>  And now let's talk about what we still have to do so that our site does not fall, as we at Mail.Ru approach these issues.  We begin, naturally, with redundancy and balancing. <br><br>  We have a universal table that we apply to our services.  It is quite common, but at least by trying it on one or another of our services, we can understand how secure it is, how much it corresponds to what we want to receive. <br><br>  We believe that for any data processing servers that are not repositories, N + 1 redundancy is sufficient for us.  N + 1 in this example does not mean that we have one spare server for 100 servers, this means that we determine N. For example, in the case of frontend, we have one spare server for every 10 web servers.  The important thing is that in this case we always have some number of servers that we can, if anything, put into battle.  For data warehouses, we always have two online copies of our storage, because hard drives are such a thing that they cannot be trusted.  And we can switch from one storage to another at any time.  Plus, that is often forgotten, we always have an offline copy of our data.  The problem is that there are not only hardware failures, but also software, in which case I simply don‚Äôt need two arrays with broken data, I just need an offline copy to which we can roll back if we need it. <br><br>  The stock taken by the network infrastructure in Mail.Ru is 35%.  This is enough to survive the "peaks" that are associated with some events.  This is enough to survive the influx of attendance that we experience when our competitors are not working.  In general, 35% of the network infrastructure of the average site is enough to survive all those disasters that can happen to it. <br><br>  Separately, I would like to mention that every time I talk about reservations, every time I talk about reserves, I don‚Äôt mean the server that lies somewhere in your warehouse, office, or somewhere else.  Because at the moment when the accident happens, you will need some amount of time to bring the reserve into battle.  Therefore, all the reserves that you have should be automatically brought into battle.  And in the ideal case, as it is done with us, it should always work.  Those.  The ideal reserve is the redundancy of your working servers.  Backup servers should also process some part of the load in the usual time, so that a copy of the software for them is always relevant, their configuration files are always relevant, etc. <br><br>  So, just a couple of words about how we approach the creation of fault-tolerant balancing systems.  I will describe this using the HTTP service as an example.  In fact, you can use this concept in almost any service.  In order to create such a balancer, we need the simplest router with support for the <a href="http://ru.wikipedia.org/wiki/RIP_(%25D1%2581%25D0%25B5%25D1%2582%25D0%25B5%25D0%25B2%25D0%25BE%25D0%25B9_%25D0%25BF%25D1%2580%25D0%25BE%25D1%2582%25D0%25BE%25D0%25BA%25D0%25BE%25D0%25BB)">RIP</a> protocol, we need a means of communication with this router, I propose an open source solution for <a href="http://ru.wikipedia.org/wiki/Quagga">Quagga</a> , but you can use something else. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/831/8d8/b68/8318d8b687180648534c643b95b38441.png"><br><br>  We need the actual balancer and <a href="http://en.wikipedia.org/wiki/IP_Virtual_Server">IPVS</a> traffic encapsulator, it is very well known, a huge number of system administrators know how to work with them.  And the last thing we need is a utility to check the liveliness of servers <a href="http://www.keepalived.org/">keepalived</a> .  It is good because you can customize it completely to your needs.  So, how does this work on the example of a single data center?  In the future, I will also show how this works in the case of several data centers. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/36c/15a/e03/36c15ae03130c39684968fc5287effb0.png"><br><br>  Here we have a group of our servers.  Suppose - this frontend-s.  They respond via HTTP to user requests.  We have the same Keepalived that I described, which polls them and, if they work, it updates the table of these real servers in IPVS.  If the number of real working servers in IPVS is higher than the threshold value that we have determined sufficient for our service, IPVS raises the virtual IP address of your service on the balancer.  In the case of Mail.Ru, you can see, these are four IP addresses, four different groups of balancers.  The same demon who communicates with the router begins to inform him that the route to Mail.Ru runs through this same balancer.  Further, when a user request comes to us, the router, based on its routing table, calls the balancer, the balancer encapsulates this traffic in IP-in-IP (this adds us only 20 bytes to each packet) and sends it to the real server.  And the real server sends it directly, without encapsulation to the router, the router directly to the user. <br><br>  This scheme allows you to process about 600,000 packets per second, which is not bad even for the hardware of popular vendors. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/d3a/6fc/305/d3a6fc3051af8a027cb8206c185cd623.png"><br><br>  The advantage of this solution, as compared with the hardware purchased from the vendor, is that it is completely under your control, you fully understand what is happening there at the moment, you can add, finish, redo everything as you need.  If several servers do not work for us, keepalived will immediately notice this, and he will turn off these servers in the table of real servers of our balancer.  If the entire farm has broken down, i.e.  the number of live servers that currently work for us turned out to be less than the threshold value that we expose, it is noticed by the same keepalived, it reports this to IPVS, and it removes the virtual IP address from our balancer.  As a result, the demon that is responsible for communicating with the router simply kills the route to this balancer, and the user package simply goes to another data center, where we have a working server farm and a working balancer. <br><br>  On the example of several data centers, this sounds a bit more complicated, but the meaning is about the same.  We receive custom requests.  If the RIP-metrics are the same, they are evenly distributed between our balancers, then go to real servers, then go to the router and back to the user.  In case we have some kind of accident, it is noticed by our monitoring, our keepalived, we kill the route on this router, and all user requests go to the router in the neighboring data center, and accordingly, to the balancer in the neighboring data center. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/0de/6f0/78a/0de6f078a6ad8bd753fda32cf9c161bc.png"><br><br>  As you know, any advertisement describes only the benefits, and my presentation is no exception.  There are four things we need to do in order for it to actually work.  We need to fix the RIP update timers in Quagga, they need to be reduced to one second, just so that the user does not wait for a long time until our routing table is updated. <br><br><br>  Second, we need to once again fix Quagga so that your RIP-metric, which your system administrator can set, does not go into the poison reverse package.  Administratively set the metric you may want in case you have several balancers for your system and you need to do some work on one of them.  As I said, IPVS encapsulates packets by adding 20 bytes to each IP header packet.  We will have to patch Keepalived in such a way that it sends not only the most similar to user requests, we need them to be 1500 bytes in size, so that if we have a bag somewhere on the network, it does not fit through the MTU - we it was immediately discovered. <br><br><br>  And finally, in order to achieve the performance that I promised you, namely, 600,000 packets per second, we need to turn off irqbalance on our server, and manually scatter processes across the cores.  Actually, you have to scatter the keepalived process and the interrupt queue of your network card.  It will be great if your network card supports MSI-X and you have more than one queue.  This is how we balance in Mail.Ru. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/b9c/dbe/1c1/b9cdbe1c1f69af49d6d4761be623198a.png"><br><br>  And now let's talk a little about other things.  In order to make the project stable, reliable and all the time working.  we need to make it modular.  This will allow us to break not all at once, but in parts.  You perfectly understand that whatever your site is, your users work only with some functions on it.  Some parts of your site are more popular, some less.  Accordingly, if your software product is modulated, then you will never break down completely.  You will break down in some way, and this will affect fewer of your favorite users.  The second thing is to cache negative responses.  If for some reason any part of your system stops responding to you, do not continue to send there all the requests from your users.  It is necessary to design the system in such a way that after the accident has happened, every tenth, twentieth, thousandth package gets there, depending on your load.  Accordingly, as soon as your service began to respond correctly, you unblock it automatically, and the work is restored. <br><br>  A very good idea that few realize - work with your modules asynchronously.  This can be done both with the help of clientside - with the help of AJAX, and on the server side, this will allow you, if you have problems with the load, while you wait for the decelerating module, to collect all other necessary data.  As soon as more than one server appears on your system, it is a good idea to divide the load by type.  Namely, to have a specified server for your database, for your frontend-a, for your mail service, etc.  This allows you to break again not entirely. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/58c/7b6/984/58c7b6984295476261d56608feb0747f.png"><br><br><h4>  Release Management and Testing </h4><br><br>  Let's talk a little about release management and testing.  I understand that we do not have time to cover the entire release management, especially since you can arrange a whole conference on this topic.  But there is, nevertheless, a number of topics that for Mail.Ru seem to be the most important and most important. <br><br>  First, half of the stability problems of the project are associated with the release of new code.  And this tells us that there should be release management in the project.  By release management, I mean that you, your manager, the team that develops the project should understand ‚Äúwhat we are launching today,‚Äù ‚Äúwhat is included in this release,‚Äù ‚Äúwho is responsible for it,‚Äù ‚Äúwhat load will this release add to production? ‚Äù  The release process should be as automated as possible, because ... we even have one simple saying in Mail.Ru: ‚Äúthe more automated the process, the fewer surprises in production‚Äù.  Starting from the moment when in order to run something on the production servers, you start copying something, correcting some configuration files manually - wait, it will take quite a bit of time until you make a mistake.  If you have an automatic release process, first of all, it guarantees that what you are rolling out for production is exactly what you have just tested.  Secondly, it minimizes the number of errors in the configuration files, under-produced before the production of the library, etc.  That is, the cost of automation of the release is small, and the benefits of this, I think, is very large. <br><br>  Of course, all people are wrong, and we can not always foresee the load we will bring to our service by launching this or that new functionality.  We at Mail.Ru apply split testing for this.  Split testing is when you launch a new Finch for some users only.  First of all, this is wildly liked by the users themselves, because a group of the elect appears, which sees all the newest, most interesting, most interesting bugs.  Secondly, it allows us to look at this group, and if we didn‚Äôt make a mistake with the load, didn‚Äôt we do something that, when running on the entire system, will lead to an accident. <br><br>  And the last thing I want to say a little bit about is to release everything every time.  Suppose there are several developers in your team, and some of them did their task quickly, some of them did the task for a long time.  So, even if your product, which was made by one developer, is not ready for release - release what is.  Even if nobody calls these modules, even if nobody needs them, release them on the day of release anyway.  This allows you to avoid the black day of the system administrator, when a huge piece of code is released, and if something has broken somewhere, it is impossible to figure it out.  Therefore, releasing your product a little bit each time, making this process permanent, you achieve that you will never have one big megarelosis, after which it will take another five days to roll back everything. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/82c/d93/4d2/82cd934d2de98f7e42280e981b83ae46.png"><br><br>  Of course, it is important not only to release everything, it is also important to predict the load.  You must have your load schedules.  You need to know how fast you work.  Your development team should have a person whose responsibility it is to look at these graphs, and to warn every time - ‚Äúguys, it seems we are approaching critical values‚Äù.  We must have thresholds for all response times of our services, because we need to know when to shout about the problem. <br><br>  There are specific projects.  For example, such as the service of virtual postcards.  On holidays, for example, on New Year's Day, on Valentine's Day, the load on such services increases by 20 times.  Accordingly, if you remember this, then you will be ready for this increase in load. <br><br>  Very often, the team is formed in such a way that programmers develop new functionality, and administrators are responsible for the stability of your site - this is a bad organization, it should be a single team, people should understand what they are launching and how it will be reflected in production.  There should not be situations where some people program and know what they are going to launch, while other people are responsible for the stability of the site, and for them every launch is a surprise.  We at Mail.Ru try to have an estimated load metric that our release will cause, we try to predict that this functionality will require so many additional servers from us.  I also recommend that you do it.  Somewhere, probably, with <br>  <nobr>60-tenth</nobr> <br>  times and you learn to guess the load.  This greatly helps you to predict your load, and in a timely manner to increase the capacity that you need. <br><br><img src="https://habrastorage.org/getpro/habr/post_images/f79/5bf/ebd/f795bfebddcaaaa203bbacdf187956a7.png"><br><br><h4>  Accidents </h4><br><br>  Now let's talk about accident planning.  I do not mean subversive activities, I mean, what will we do when we have finally broken everything.  And as we remember, it is inevitable.  So, spend this time before.  how you had an accident, and you will save many hours of the time that you will spend to restore the stability of your site.  Any member of the team should know - if something is broken, then what is its function, what does it do, how is the reserve injected, how are backups rolled out.  Rollout backup should be fully automated.  What do I mean by automation?  Unfortunately, when our equipment breaks down, it does not always warn us in advance.  Sometimes it happens at night, sometimes some people are on vacation.  Accordingly, so that you do not have a situation where your system administrator is somewhere far away, and your data is already broken, make sure that your backup rolls out automatically, at your request, of course.  Provide stubs and "light versions" of your site.    ,   ,      .       ,     -  ,    ¬´internal server error 500¬ª. <br><br> ,  ,         ,     ,   : (  ) ¬´   ,   ,       ,   .    .¬ª <br><br><img src="https://habrastorage.org/getpro/habr/post_images/860/2f6/037/8602f6037b9cb688aa2396788e00c204.png"><br><br><h4>    </h4><br><br>  ,   ,    ,     ,   -  ,     .     , , .     ,         , ¬´  ¬ª.   -   ,  ,   .         ,     ,      ,    ‚Äî    .    , .       ,        ,    . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/2a2/ac9/995/2a2ac99953831c42c6304b59e7353ed7.png"><br><br>   ‚Äî .  ,   ,    ,  -,  - ,   .      ,       ,      ,      ,    ‚Äî   , ,   , ?    - , - , - ,     ,   ,   .  ,     ,   ,         .     . ,     ,      ,           . <br><br>   ,    ,   ,   ¬´ ¬ª.  10%       . - - , - - , -  .  ,  ,      ‚Äî      ,      . <br><br>    ,         ,    ,      ,   ,   ,  ,    ,       .       ‚Äî  ,     -,  . <br><br>  ,      ,     ,        BGP-,      .  ,    Mail.Ru            . ,  ,            ,       ,      ,    .      ,     . ,  ,  ,     ,     . <br><br><img src="https://habrastorage.org/getpro/habr/post_images/66a/b6b/364/66ab6b36461289d824d992c8ca81e7e9.png"><br><br><h4>  Questions </h4><br><br><blockquote> <b>     ‚Äî   ,  ,    .       ,    , .    ,    ?</b> </blockquote><br><br>  -   15,       . <br><br><blockquote> <b>. ,  .   :        ?</b> </blockquote><br><br>       ,           .       ,          ,       .       ,          ,       ..           ,           . <br><br><blockquote> <b>,  ,    . :     ,   ‚Äî    -    ?</b> </blockquote><br><br>     ‚Äî   .    ,   ,        .    , ,   .   - ‚Äî   ,     ,   . <br><br> <b> .      ?  -    ,    -    ?</b> <br><br>      .   -    .    ,    .       . ,         ,       ,        , ,     ‚Äî ¬´  ,  ,    ¬ª,      .      ‚Äî  ,     . <br><br><blockquote> <b>, ¬´ ¬ª.   ,         ,   ,    ,   -    .       -.      ,         -?</b> </blockquote><br><br> -   .   ,        ,      ¬´¬ª. <br><br><blockquote> <b>.    .        ‚Äî  ,             ‚Äî    .          ,  ‚Äî -  .    ?       ..</b> </blockquote><br><br>  I understood.     ,       .  ,    ,      ,     ‚Äî ¬´,    -   ,     ?¬ª.              .         ,         , -               ,       ,    . <br><br><blockquote> <b>  ‚Äî              ‚Äî ¬´,   ¬ª?    ?</b> </blockquote><br><br>       .   ‚Äî ¬´  ‚Äî  ¬ª.  ,     .   ,        ,   ‚Äî   .   :    - ,    ,  ‚Äî     ¬´,  ,   ¬ª,     ,     .       .     ¬´     ,   ¬ª, ¬´  ¬ª, ¬´   ,    ?¬ª  .. <br><br><blockquote> <b>, .    ,    -.</b>  <b>Those.</b> <b>         .       ?     -   ?     ?</b> </blockquote><br><br>       ,      .   ,       , ,    ,      ,        .  ,      -       ‚Äî    .  ,     ‚Äî   -    . ,     ,      ,   .      ‚Äî    ,     .     ,     ,    ,  ,          . <br><br><blockquote> <b> ,  ‚Äî   ?    ...</b> </blockquote><br><br>   ,   .  Various.      ,      ,     ,    ,       . <br><br><blockquote> <b> .      Java ,     ?     ?   .   :  ,    <nobr>500-</nobr> .    ,   -.  ,   ,      ‚Äî ,     ¬´      ¬ª?</b> </blockquote><br><br>   .     ? ,  ,     ,        ,    ‚Äî   ,    - -  .   ,      ‚Äî       .    Java- ‚Äî , , ,  ,      .   ,    ‚Äî    .  ,     mail.ru,    Java-  ,   ,         ,        . <br><br><blockquote> <b>.   .    opensource      . ,     opensource       ,     .</b> </blockquote><br><br>     . 13   - ‚Äî  .   ,    ,     ,           .  mail.ru   ,   ,     .     . <br><br><br><blockquote> <b> ,       .    .         ,      - ,   -  , ?</b> </blockquote><br><br>     .    DoS-   -      ,     .     ,   ,         .   ,  , , 10     .    ,     ,    ,        ,       DoS.      ,   -  ‚Äî , ,     ,  ,     ,     ,    -    ,   . <br><br><blockquote> <b>.     ‚Äî    -     DoS-?</b> </blockquote><br><br>  Not. <br><br><blockquote> <b> ,    ?</b> </blockquote><br><br>     ‚Äî   . <br><br><blockquote> <b> :   Mail.Ru Group          ?</b> </blockquote><br><br> , .        -      -.    -    . <br><br><blockquote> <b>.   ,  ,    ,      ,      ? , :      ,    <nobr>1-</nobr>      ,    ,     .</b> </blockquote><br><br>      ,      Mail.Ru.          ,   ,     24   .  Mail.Ru   .     ..  ,     - - ,               . ,  ,      -     ,   ,      12  -  ,      -   .    ,     ..  c  . <br><br><hr><br> <i>        <a href="http://techforum.mail.ru/">  Mail.Ru 2011</a> ,  16    .</i>  <i>Details about the technology of creating texts of reports based on video recordings can be found here: <a href="http://habrahabr.ru/company/mailru/blog/132953/">‚ÄúWrong side‚Äù of the Mail.Ru Technologies Forum: High-tech in event-management</a> .</i> <i>   (    )     ‚Äì <a href="http://techforum.mail.ru/">techforum.mail.ru</a> .</i>  <i>Text versions of the reports will be published here and on the Forum website every week or less often in a similar format.</i>  <i>Please report in the "lichku" about typos in the text.</i> </div><p>Source: <a href="https://habr.com/ru/post/134109/">https://habr.com/ru/post/134109/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../134100/index.html">Fight without mirrors - 2</a></li>
<li><a href="../134102/index.html">Size of java objects</a></li>
<li><a href="../134106/index.html">Evernote and HTC 7 Mozart</a></li>
<li><a href="../134107/index.html">Photo review of Computer History Museum in Silicon Valley</a></li>
<li><a href="../134108/index.html">News, contests and initiatives for Windows Phone developers</a></li>
<li><a href="../134110/index.html">The European Union launched an investigation into the cartel collusion between digital book publishers and Apple</a></li>
<li><a href="../134111/index.html">Post of Russia</a></li>
<li><a href="../134114/index.html">Preparing .psd for layout</a></li>
<li><a href="../134115/index.html">What is really a terrible figure eight</a></li>
<li><a href="../134117/index.html">Advertisement of Samsung's flexible transparent 3D displays</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>