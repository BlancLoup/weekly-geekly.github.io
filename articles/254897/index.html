<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Lectures of the Technosphere. 1 semester Algorithms for intelligent processing of large amounts of data</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We continue to publish the materials of our educational projects. This time we offer to get acquainted with the lectures of Tekhnosfera on the course ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Lectures of the Technosphere. 1 semester Algorithms for intelligent processing of large amounts of data</h1><div class="post__text post__text-html js-mediator-article">  We continue to publish the materials of our educational projects.  This time we offer to get acquainted with the lectures of <a href="https://sfera-mail.ru/">Tekhnosfera</a> on the course "Algorithms for intelligent processing of large amounts of data."  The goal of the course is the study by students of both classical and modern approaches to solving Data Mining problems based on machine learning algorithms.  Course instructors: Nikolai Anokhin (@anokhinn), Vladimir Gulin (@vgulin) and Pavel Nesterov (@mephistopheies). <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/UJn2fP0ApZ0%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhisSkifxEyXfjj1J1HV9bEH-RXDow" frameborder="0" allowfullscreen=""></iframe><br><br>  The amount of data generated daily by the services of a large Internet company is truly enormous.  The goal of Data Mining‚Äôs dynamically developing discipline in recent years is to develop approaches that effectively process such data to extract business-friendly information.  This information can be used when creating referral and search engines, optimizing advertising services, or making key business decisions. <br><a name="habracut"></a><br><h4>  <b>Lecture 1. Tasks of Data Mining (Nikolai Anokhin)</b> </h4><br>  Overview of data mining tasks.  Standardization of the approach to solving problems of Data Mining.  CRISP-DM process.  Types of data.  Clustering, classification, regression.  The concept of a model and learning algorithm. 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/vtA93yys_20%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhjHWziu4xZOz-SgagoBlHQFAf_l-A" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 2. The clustering problem and the EM-algorithm (Nikolai Anokhin)</b> </h4><br>  Setting the clustering problem.  Distance functions.  Clustering quality criteria.  EM algorithm.  K-means and modifications. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/XEOvF1trvVU%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhgdp18xZ8u14velA5sTcmJKLquZqQ" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 3. Various clustering algorithms (Nikolai Anokhin)</b> </h4><br>  Hierarchical clustering.  Agglomerative and Divisive algorithms.  Different kinds of distances between clusters.  Stepwise-optimal algorithm.  The case of non-Euclidean spaces.  Criteria for choosing the number of clusters: rand, silhouette.  DBSCAN. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/kjbZLMW6Wic%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhiKyt2hQgyOU08jsl9W5KI6MSJNtg" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 4. The task of classification (Nikolai Anokhin)</b> </h4><br>  Setting the tasks of classification and regression.  Decision making theory.  Types of models.  Examples of loss functions.  Retraining  Classification quality metrics.  MDL.  Decisive trees.  Algorithm CART. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/iKki0J6lqrQ%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhjj-SlM4To6VuPzJJKxp-ZnTYW3XA" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 5. Naive Bayes (Nikolai Anokhin)</b> </h4><br>  Conditional probability and Bayes theorem.  Normal distribution.  Naive Bayes: multinomial, binomial, gaussian.  Smoothing.  Generative NB model and Bayesian inference.  Graphic models. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/2AGXdZaOpZc%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhhQCt94UmK2MkpfGyc2YjNA5qdocA" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 6. Linear models (Nikolai Anokhin)</b> </h4><br>  Generalized linear models.  Setting the optimization problem.  Examples of criteria.  Gradient descent.  Regularization.  Maximum Likelihood method.  Logistic regression. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/t0z7nFGek8w%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhjUsnaxTduE5ewEdUrzmV82BhewLg" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 7. Support vector machine (Nikolai Anokhin)</b> </h4><br>  Splitting surface with maximum clearance.  The formulation of the optimization problem for the cases of linearly separable and linearly non-separable classes.  The conjugate problem.  Reference vectors.  KKT conditions.  SVM for classification and regression tasks.  Kernel trick.  Mercer's theorem.  Examples of kernel functions. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/vbuqVpciiaw%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhhYx9q0Ncxx_wpIncldXfO7hbKnhA" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 8. Reducing the dimension of space (Vladimir Gulin)</b> </h4><br>  The problem of curse dimension.  Selection and selection of signs.  Methods for feature extraction.  Principal Component Method (PCA).  Independent Component Method (ICA).  Methods based on autoencoders.  Methods for feature selection.  Methods based on the mutual correlation of signs.  Methods of maximum relevance and minimum redundancy (mRMR).  Methods based on decision trees. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/tq9pO1xQCSQ%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhiRDTgd6LLMU4dK9YHwBaoFeNdiIg" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 9. Algorithmic compositions 1 (Vladimir Gulin)</b> </h4><br>  Combinations of classifiers.  Model decision trees.  A mixture of experts.  Stacking  Stochastic methods for building classifier ensembles.  Bagging.  RSM.  RandomForest Algorithm. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/GvPSMzPSBPY%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhjQ1RcL_UWfWVSArjrZzXs25VFwCQ" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 10. Algorithmic compositions 2 (Vladimir Gulin)</b> </h4><br>  Key ideas of boosting.  Differences between boosting and bagging.  Algorithm AdaBoost.  Gradient boosting.  Meta-algorithms over algorithmic compositions.  BagBoo algorithm. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/5NBNTl0aCng%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhgzoJAIYy4qCCfSDY3rlUdruJua3A" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 11. Neural networks, training with a teacher (Pavel Nesterov)</b> </h4><br>  Biological neuron and neural networks.  McCulloch-Pitts artificial neuron and artificial neural network.  Perceptron Rosenblatt and Rumelhart.  Error propagation algorithm.  The moment of learning, regularization in the neural network, local learning rate, softmax layer.  Various training modes. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/np2bT8EtFSE%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhh8HKZkbaf1ba-CkAZ0K4Gldze1yg" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 12. Neural networks, learning without a teacher (Pavel Nesterov)</b> </h4><br>  Neural network autoencoder.  Stochastic and recurrent neural networks.  Boltzmann machine and Boltzmann limited machine.  Gibbs distribution.  The contrastive divergence algorithm for learning the BSR.  Sampling data from the BSR.  Binary BSR and Gaussian Binary BSR.  Influence of regularization, nonlinear compression of dimensions, feature extraction.  Semantic hashing. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/F1pFQV_R9mM%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhj5Q0Wt3DBSTyCmvD3OJgZZPZj7Yg" frameborder="0" allowfullscreen=""></iframe><br><br><h4>  <b>Lecture 13. Neural networks, deep networks (Pavel Nesterov)</b> </h4><br>  The difficulties of learning a multilayer perceptron.  Pre-training using RBM.  Deep autoencoder, deep multilayered neural network.  Deep belief network and deep Boltzmann machine.  The device of the human eye and visual cortex.  Convolution networks. <br><br><iframe width="560" height="315" src="https://translate.googleusercontent.com/translate_c?depth=1&amp;rurl=translate.google.com&amp;sl=ru&amp;sp=nmt4&amp;tl=en&amp;u=https://www.youtube.com/embed/xneiojewDaQ%3Ffeature%3Doembed&amp;xid=17259,15700021,15700186,15700191,15700253&amp;usg=ALkJrhh7tvpOChDidvbpJ9XrngLDpTuCSQ" frameborder="0" allowfullscreen=""></iframe><br><br>  Previous issues  Technopark: <br><ul><li>  <a href="http://habrahabr.ru/company/mailru/blog/248745/">1 semester</a>  <a href="http://habrahabr.ru/company/mailru/blog/248745/">Web technologies</a> </li><li>  <a href="http://habrahabr.ru/company/mailru/blog/251561/">1 semester</a>  <a href="http://habrahabr.ru/company/mailru/blog/251561/">Algorithms and data structures</a> </li><li>  <a href="http://habrahabr.ru/company/mailru/blog/253095/">1 semester</a>  <a href="http://habrahabr.ru/company/mailru/blog/253095/">C / C ++</a> </li><li>  <a href="http://habrahabr.ru/company/mailru/blog/254073/">Semester 2</a>  <a href="http://habrahabr.ru/company/mailru/blog/254073/">Database</a> </li><li>  <a href="http://habrahabr.ru/company/mailru/blog/254843/">Term 3</a>  <a href="http://habrahabr.ru/company/mailru/blog/254843/">Design of high-load systems</a> </li></ul><br>  Subscribe to the <a href="http://www.youtube.com/user/TPMGTU/">youtube channel</a> Technopark and Technosphere! </div><p>Source: <a href="https://habr.com/ru/post/254897/">https://habr.com/ru/post/254897/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../254885/index.html">Description of digital machines on VHDL</a></li>
<li><a href="../254887/index.html">WPF: Using Attached Property and Behavior</a></li>
<li><a href="../254889/index.html">Matreshka.js 2: From simple to simple</a></li>
<li><a href="../254891/index.html">We facilitate the support of iOS applications. Part 2 - location and network</a></li>
<li><a href="../254895/index.html">The taste and color 2 - not RGB one</a></li>
<li><a href="../254899/index.html">Encryption in NQ Vault turned out to be an ordinary XOR, and this is not the worst</a></li>
<li><a href="../254901/index.html">Universal Nixie-module on IN-12</a></li>
<li><a href="../254905/index.html">State Mandate to Kill the Universe and Counteract This (report at Bitcoin Conference Russia 2015)</a></li>
<li><a href="../254907/index.html">Web Bundle - the case of RarJPEG lives</a></li>
<li><a href="../254909/index.html">Some practical cryptography for .NET for dummies</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>