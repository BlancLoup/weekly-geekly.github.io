<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Scala data analysis. We consider the correlation of the 21st century</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="It is very important to choose the right tool for data analysis. On the Kaggle.com forums, where international competitions in Data Science are held, ...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Scala data analysis. We consider the correlation of the 21st century</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/011/469/e83/011469e830514f2584d20474bc88a8e5.png"><br>  It is very important to choose the right tool for data analysis.  On the <a href="https://www.kaggle.com/">Kaggle.com</a> forums, where international competitions in Data Science are held, people often ask which tool is better.  The first lines of popularity are R and Python.  In the article, we will talk about an alternative data analysis technology stack, made on the basis of the Scala programming language and the <a href="http://spark.apache.org/">Spark</a> distributed computing platform. <br><br>  How did we come to this?  In <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-21-vek">Retail Rocket,</a> we do a lot of machine learning on very large amounts of data.  Previously, for the development of prototypes, we used the IPython + Pyhs2 combination (hive driver for Python) + Pandas + Sklearn.  At the end of the summer of 2014, we made a fundamental <a href="http://www.slideshare.net/rzykov/retail-rocket-sparkrzykov">decision to switch to Spark</a> , since the experiments showed that we would get a 3-4 fold increase in performance on the same server park. <br><a name="habracut"></a><br>  Another plus is that we can use one programming language for modeling and code that will work on the combat servers.  For us, this was a great advantage, because before that we used 4 languages ‚Äã‚Äãsimultaneously: Hive, Pig, Java, Python, for a small team this is a serious problem. <br><br>  Spark well supports working with Python / Scala / Java through an API.  We decided to choose Scala, since Spark was written on it, that is, it is possible to analyze its source code and, if necessary, correct errors, plus - this is the JVM on which the whole Hadoop is running.  Analysis of the programming language forums under Spark reduced to the following: 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      Scala: <br>  + functional; <br>  + native to Spark; <br>  + works on JVM, which means native to Hadoop; <br>  + strict static typing; <br>  - rather complicated input, but the code is readable. <br><br>  Python: <br>  + popular; <br>  + simple; <br>  - dynamic typing; <br>  - performance is worse than Scala. <br><br>  Java: <br>  + popularity; <br>  + native to Hadoop; <br>  - too much code. <br><br>  More details on the choice of programming language for Spark can be found <a href="http://www.quora.com/Is-Scala-a-better-choice-than-Python-for-Apache-Spark">here</a> . <br><br>  I must say that the choice was not easy, because at that moment no one in the team knew Scala. <br>  A well-known fact: to learn how to communicate well in a language, you need to immerse yourself in the language environment and use it as often as possible.  Therefore, for modeling and quick data analysis, we abandoned the Python stack in favor of Scala. <br><br>  The first step was to find a replacement for IPython, the options were as follows: <br>  1) <a href="http://zeppelin-project.org/">Zeppelin</a> - an IPython-like notebook for Spark; <br>  2) ISpark; <br>  3) Spark Notebook; <br>  4) <a href="https://github.com/ibm-et/spark-kernel">Spark IPython Notebook from IBM</a> . <br><br>  So far, the choice has fallen on ISpark, since it is simple - this is IPython for Scala / Spark, it was relatively easy to attach HighCharts and R charts to it. And we didn‚Äôt have any problems connecting it to the Yarn cluster. <br><br>  Our story about the Scala data analysis environment consists of three parts: <br>  1) A simple task on Scala in ISpark, which will be executed locally on Spark. <br>  2) Setup and installation of components for work in ISpark. <br>  3) We write Machine Learning task on Scala, using libraries R. <br>  And if this article is popular, I will write two others.  ;) <br><br><h2>  Task </h2><br>  Let's try to answer the question: does the average purchase check in the online store depend on the client‚Äôs static parameters, which include the settlement, browser type (mobile / Desktop), operating system and browser version?  This can be done with the help of Mutual Information. <br><br><blockquote>  In <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-21-vek">Retail Rocket,</a> we use entropy much where we use our recommender algorithms and analysis: the classical Shannon formula, the Kullback-Leibler discrepancy, mutual information.  We even applied for a report on <a href="http://recsys.acm.org/recsys15/">the RecSys conference</a> on this topic.  These measures are devoted to a separate, albeit small, section in the well-known textbook on machine learning, Murphy. </blockquote><br>  Let's analyze on real data <a href="http://retailrocket.ru/%3Futm_source%3Dhabr-blog%26utm_medium%3Dreferral%26utm_campaign%3Dhabr-21-vek">Retail Rocket</a> .  Previously, I copied the sample from our cluster to my computer as a csv file. <br><br><h2>  Data loading </h2><br>  Here we use ISpark and Spark, running in local mode, that is, all calculations occur locally, the distribution goes to the cores.  Actually everything is written in the comments.  Most importantly, at the output we get RDD (Spark data structure), which is a collection of case classes of type Row, which is defined in the code.  This will allow access to the fields via ".", For example _.categoryId. <br><br>  At the entrance: <pre><code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.rdd.<span class="hljs-type"><span class="hljs-type">RDD</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.apache.spark.sql._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.tribbloid.ispark.display.dsl._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scala.util.<span class="hljs-type"><span class="hljs-type">Try</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> sqlContext = <span class="hljs-keyword"><span class="hljs-keyword">new</span></span> org.apache.spark.sql.<span class="hljs-type"><span class="hljs-type">SQLContext</span></span>(sc) <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> sqlContext.implicits._ <span class="hljs-comment"><span class="hljs-comment">//  CASE class,     dataframe case class Row(categoryId: Long, orderId: String ,cityId: String, osName: String, osFamily: String, uaType: String, uaName: String,aov: Double) //     val   sc (Spark Context),   Ipython  val aov = sc.textFile("file:///Users/rzykov/Downloads/AOVC.csv") //   val dataAov = aov.flatMap { line =&gt; Try { line.split(",") match { case Array(categoryId, orderId, cityId, osName, osFamily, uaType, uaName, aov) =&gt; Row(categoryId.toLong + 100, orderId, cityId, osName, osFamily, osFamily, uaType, aov.toDouble) } }.toOption }</span></span></code> </pre> <br>  At the exit: <br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">MapPartitionsRDD</span></span>[<span class="hljs-number"><span class="hljs-number">4</span></span>] at map at &lt;console&gt;:<span class="hljs-number"><span class="hljs-number">28</span></span></code> </pre> <br>  Now look at the data itself: <br><img src="https://habrastorage.org/files/d09/1a1/742/d091a174203a4fdfaae9bace759e2bab.png"><br>  This line uses the new DataFrame data type added in Spark in version 1.3.0, it is very similar to the similar structure in the pandas library in Python.  toDf picks up our case class Row, thanks to which it gets the names of the fields and their types. <br><br>  For further analysis, you need to select any one category, preferably with a large amount of data.  For this you need to get a list of the most popular categories. <br><br>  At the entrance: <pre> <code class="scala hljs"><span class="hljs-comment"><span class="hljs-comment">//   dataAov.map { x =&gt; x.categoryId } //   categoryId .countByValue() //     categoryId .toSeq .sortBy( - _._2) //       .take(10) //   10 </span></span></code> </pre> <br>  At the output we got an array of tuples (tuple) in the format (categoryId, frequency): <br><pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">ArrayBuffer</span></span>((<span class="hljs-number"><span class="hljs-number">314</span></span>,<span class="hljs-number"><span class="hljs-number">3068</span></span>), (<span class="hljs-number"><span class="hljs-number">132</span></span>,<span class="hljs-number"><span class="hljs-number">2229</span></span>), (<span class="hljs-number"><span class="hljs-number">128</span></span>,<span class="hljs-number"><span class="hljs-number">1770</span></span>), (<span class="hljs-number"><span class="hljs-number">270</span></span>,<span class="hljs-number"><span class="hljs-number">1483</span></span>), (<span class="hljs-number"><span class="hljs-number">139</span></span>,<span class="hljs-number"><span class="hljs-number">1379</span></span>), (<span class="hljs-number"><span class="hljs-number">107</span></span>,<span class="hljs-number"><span class="hljs-number">1366</span></span>), (<span class="hljs-number"><span class="hljs-number">177</span></span>,<span class="hljs-number"><span class="hljs-number">1311</span></span>), (<span class="hljs-number"><span class="hljs-number">226</span></span>,<span class="hljs-number"><span class="hljs-number">1268</span></span>), (<span class="hljs-number"><span class="hljs-number">103</span></span>,<span class="hljs-number"><span class="hljs-number">1259</span></span>), (<span class="hljs-number"><span class="hljs-number">127</span></span>,<span class="hljs-number"><span class="hljs-number">1204</span></span>))</code> </pre> <br>  For further work, I decided to choose the 128th category. <br><br>  Prepare the data: filter the necessary types of operating systems so as not to litter the graphics with garbage. <br><br>  At the entrance: <pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> interestedBrowsers = <span class="hljs-type"><span class="hljs-type">List</span></span>(<span class="hljs-string"><span class="hljs-string">"Android"</span></span>, <span class="hljs-string"><span class="hljs-string">"OS X"</span></span>, <span class="hljs-string"><span class="hljs-string">"iOS"</span></span>, <span class="hljs-string"><span class="hljs-string">"Linux"</span></span>, <span class="hljs-string"><span class="hljs-string">"Windows"</span></span>) <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> osAov = dataAov.filter(x =&gt; interestedBrowsers.contains(x.osFamily)) <span class="hljs-comment"><span class="hljs-comment">//    .filter(_.categoryId == 128) //   .map(x =&gt; (x.osFamily, (x.aov, 1.0))) //      .reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2)) .map{ case(osFamily, (revenue, orders)) =&gt; (osFamily, revenue/orders) } .collect()</span></span></code> </pre> <br>  At the output, an array of tuples (tuple) in OS format, average check: <pre> <code class="scala hljs"><span class="hljs-type"><span class="hljs-type">Array</span></span>((<span class="hljs-type"><span class="hljs-type">OS</span></span> <span class="hljs-type"><span class="hljs-type">X</span></span>,<span class="hljs-number"><span class="hljs-number">4859.827586206897</span></span>), (<span class="hljs-type"><span class="hljs-type">Linux</span></span>,<span class="hljs-number"><span class="hljs-number">3730.4347826086955</span></span>), (iOS,<span class="hljs-number"><span class="hljs-number">3964.6153846153848</span></span>), (<span class="hljs-type"><span class="hljs-type">Android</span></span>,<span class="hljs-number"><span class="hljs-number">3670.8474576271187</span></span>), (<span class="hljs-type"><span class="hljs-type">Windows</span></span>,<span class="hljs-number"><span class="hljs-number">3261.030993042378</span></span>))</code> </pre> <br>  I want to render, let's do it in HighCharts: <br><img src="https://habrastorage.org/files/9cc/4ed/693/9cc4ed693a754df8be25b077351bf239.png"><br>  Theoretically, you can use any HighCharts graphics if they are supported in <a href="https://github.com/quantifind/wisp">Wisp</a> .  All graphics are interactive. <br><br>  Let's try to do the same, but through R. <br>  Run R client: <pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> org.ddahl.rscala._ <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> ru.retailrocket.ispark._ <span class="hljs-function"><span class="hljs-keyword"><span class="hljs-function"><span class="hljs-keyword">def</span></span></span><span class="hljs-function"> </span><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">connect</span></span></span></span>() = <span class="hljs-type"><span class="hljs-type">RClient</span></span>(<span class="hljs-string"><span class="hljs-string">"R"</span></span>, <span class="hljs-literal"><span class="hljs-literal">false</span></span>) <span class="hljs-meta"><span class="hljs-meta">@transient</span></span> <span class="hljs-keyword"><span class="hljs-keyword">val</span></span> r = connect()</code> </pre> <br>  We build the schedule itself: <br><img src="https://habrastorage.org/files/a9f/9c0/060/a9f9c006072842f3a775370a4aa89e32.png"><br>  So you can build any R graphics right in your IPython notepad. <br><br><h2>  Mutual Information </h2><br>  The graphs show that there is a dependency, but will this metric confirm us?  There are many ways to do this.  In our case, we use mutual information ( <a href="http://en.wikipedia.org/wiki/Mutual_information">Mutual Information</a> ) between the values ‚Äã‚Äãin the table.  It measures the mutual dependence between the distributions of two random (discrete) quantities. <br><br>  For discrete distributions, it is calculated by the formula: <br><br><img src="https://habrastorage.org/files/e1f/52f/b34/e1f52fb3459543079e9b8d7052230f97.png"><br><br>  But we are interested in a more practical metric: <a href="http://en.wikipedia.org/wiki/Maximal_information_coefficient">Maximal Information Coefficient</a> (MIC), for the calculation of which for continuous variables one has to go for tricks.  This is how the definition of this parameter sounds. <br><br>  Let D = (x, y) be a set of n ordered pairs of elements of random variables X and Y. This two-dimensional space is divided into X and Y grids, grouping the values ‚Äã‚Äãof x and y into X and Y tilings, respectively (recall the histograms!). <br><br><img src="https://habrastorage.org/files/d4e/4bd/652/d4e4bd652dbc4a9d8d91ff10e88313d9.png"><br><br>  where B (n) is the size of the grid, I ‚àó (D, X, Y) is mutual information on splitting X and Y. The denominator indicates the logarithm, which serves to normalize the MIC to the values ‚Äã‚Äãof the interval [0, 1].  MIC takes continuous values ‚Äã‚Äãin the interval [0,1]: for extreme values ‚Äã‚Äãit is equal to 1, if the dependence is, 0 - if it is not.  What else you can read on this topic is listed at the end of the article, in the list of references. <br><br>  The book <a href="http://www.amazon.com/Machine-Learning-Probabilistic-Perspective-Computation/dp/0262018020/ref%3Dsr_1_1%3Fie%3DUTF8%26qid%3D1430827941%26sr%3D8-1%26keywords%3Dkevin%2Bmachine%2Blearning">MIC</a> (mutual information) is called the correlation of the 21st century.  And that's why!  The graph below shows 6 dependencies (graphs C - H).  For them, the Pearson and MIC correlations were calculated, they are marked with the corresponding letters in the graph on the left.  As we see, the Pearson correlation is almost zero, while the MIC shows the dependence (graphs F, G, E). <br><img src="https://habrastorage.org/files/289/e07/46b/289e0746bc8e4164b3c96891b008841d.png"><br>  Original source: <a href="http://people.cs.ubc.ca/~murphyk/MLbook/figReport-16-Aug-2012/pdfFigures/MICfig4.pdf">people.cs.ubc.ca</a> <a href="http://people.cs.ubc.ca/~murphyk/MLbook/figReport-16-Aug-2012/pdfFigures/MICfig4.pdf"><br></a> <br>  The table below shows a number of metrics that were calculated on different dependencies: random, linear, cubic, etc.  The table shows that MIC behaves very well, detecting non-linear dependencies: <br><img src="https://habrastorage.org/files/4d3/4c7/469/4d34c74698f5420dab3e81cb8d0748f0.png"><br><br>  Another interesting graph illustrates the effects of noise on the MIC: <br><img src="https://habrastorage.org/files/52d/7a8/cd9/52d7a8cd9e5047f08c1ac07fee85a159.png"><br><br>  In our case, we are dealing with the calculation of MIC, when the variable Aov is continuous and all others are discrete with unordered values, for example, the type of browser.  To correctly calculate the MIC, you will need the discretization of the variable Aov.  We will use a ready-made solution from the site <a href="http://www.exploredata.net/Downloads/MINE-Application">exploredata.net</a> .  There is one problem with this solution: it considers that both variables are continuous and expressed in Float values.  Therefore, we will have to trick the code by encoding the values ‚Äã‚Äãof discrete values ‚Äã‚Äãin Float and randomly changing the order of these values.  To do this, you will have to do a lot of iterations with random order (we will do 100), and as a result we will take the maximum MIC value. <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">import</span></span> data.<span class="hljs-type"><span class="hljs-type">VarPairData</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> mine.core.<span class="hljs-type"><span class="hljs-type">MineParameters</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> analysis.<span class="hljs-type"><span class="hljs-type">Analysis</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> analysis.results.<span class="hljs-type"><span class="hljs-type">BriefResult</span></span> <span class="hljs-keyword"><span class="hljs-keyword">import</span></span> scala.util.<span class="hljs-type"><span class="hljs-type">Random</span></span> <span class="hljs-comment"><span class="hljs-comment">//  ,    "" def encode(col: Array[String]): Array[Double] = { val ns = scala.util.Random.shuffle(1 to col.toSet.size) val encMap = col.toSet.zip(ns).toMap col.map{encMap(_).toDouble} } //   MIC def mic(x: Array[Double], y: Array[Double]) = { val data = new VarPairData(x.map(_.toFloat), y.map(_.toFloat)) val params = new MineParameters(0.6.toFloat, 15, 0, null) val res = Analysis.getResult(classOf[BriefResult], data, params) res.getMIC } //          def micMax(x: Array[Double], y: Array[Double], n: Int = 100) = (for{ i &lt;- 1 to 100} yield mic(x, y)).max</span></span></code> </pre> <br>  Well, we are close to the finals, now let's do the calculation: <br><pre> <code class="scala hljs"><span class="hljs-keyword"><span class="hljs-keyword">val</span></span> aov = dataAov.filter(x =&gt; interestedBrowsers.contains(x.osFamily)) <span class="hljs-comment"><span class="hljs-comment">//    .filter(_.categoryId == 128) //   //osFamily var aovMic = aov.map(x =&gt; (x.osFamily, x.aov)).collect() println("osFamily MIC =" + micMax(encode(aovMic.map(_._1)), aovMic.map(_._2))) //orderId aovMic = aov.map(x =&gt; (x.orderId, x.aov)).collect() println("orderId MIC =" + micMax(encode(aovMic.map(_._1)), aovMic.map(_._2))) //cityId aovMic = aov.map(x =&gt; (x.cityId, x.aov)).collect() println("cityId MIC =" + micMax(encode(aovMic.map(_._1)), aovMic.map(_._2))) //uaName aovMic = aov.map(x =&gt; (x.uaName, x.aov)).collect() println("uaName MIC =" + mic(encode(aovMic.map(_._1)), aovMic.map(_._2))) //aov println("aov MIC =" + micMax(aovMic.map(_._2), aovMic.map(_._2))) //random println("random MIC =" + mic(aovMic.map(_ =&gt; math.random*100.0), aovMic.map(_._2)))</span></span></code> </pre> <br>  At the exit: <pre> <code class="scala hljs">osFamily <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.06658</span></span> orderId <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.10074</span></span> cityId <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.07281</span></span> aov <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.99999</span></span> uaName <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.05297</span></span> random <span class="hljs-type"><span class="hljs-type">MIC</span></span> =<span class="hljs-number"><span class="hljs-number">0.10599</span></span></code> </pre> <br>  For the experiment, I added a random variable with a uniform distribution and AOV itself. <br>  As we see, almost all MICs are below a random variable (random MIC), which can be considered the ‚Äúconditional‚Äù decision threshold.  Aov MIC is almost equal to one, which is natural, since the correlation to itself is equal to 1. <br><br>  An interesting question arises: why do we see the dependence in the graphs, and the MIC is zero?  You can come up with many hypotheses, but most likely for the case of os Family everything is pretty simple - the number of Windows machines far exceeds the number of others: <br><img src="https://habrastorage.org/files/234/0ed/847/2340ed847d544435ab9ca45ab1db43bc.png"><br><br><h2>  Conclusion </h2><br>  I hope that Scala will get its popularity among data analysts (Data Scientists).  This is very convenient, since it is possible to work with the standard IPython notebook + to get all the features of Spark.  This code can easily work with terabyte data arrays, for this you just need to change the configuration line in ISpark, specifying the URI of your cluster. <br><br>  By the way, we have open vacancies in this area: <br><ul><li>  <a href="http://retailrocket.ru/vakansii/">Junior Analyst</a> </li><li>  <a href="http://retailrocket.ru/vakansii/">Research analyst</a> </li></ul><br><br>  Useful links: <br>  <a href="http://www.sciencemag.org/content/334/6062/1518.full.pdf%3Fkeytype%3Dref%26siteid%3Dsci%26ijkey%3DcRCIlh2G7AjiA">Scientific article, on the basis of which MIC was developed</a> . <br>  <a href="http://www.kdnuggets.com/2011/12/reshef-mine-mic-maximal-information-coefficient-software-exploredata.html">Note on KDnuggets about mutual information</a> (there is a video). <br>  <a href="http://minepy.sourceforge.net/">C library for calculating MIC with wrappers for Python and MATLAB / OCTAVE</a> . <br>  <a href="http://www.exploredata.net/">The site of the author of a scientific article</a> that developed MIC (there is a module for R and a Java library on the <a href="http://www.exploredata.net/">site</a> ). </div><p>Source: <a href="https://habr.com/ru/post/258543/">https://habr.com/ru/post/258543/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../258533/index.html">Cocoa Developers Club - Club iOS and OS X developers</a></li>
<li><a href="../258535/index.html">How to get on the Spamhaus lists without spamming</a></li>
<li><a href="../258537/index.html">A little bit about Erlang syntax</a></li>
<li><a href="../258539/index.html">Subscribers do not watch almost 30% of the video due to the poor quality of the mobile Internet</a></li>
<li><a href="../258541/index.html">Developer Diaries: collect professional underwater robot</a></li>
<li><a href="../258545/index.html">JPEG 2000, JPEG-XR and WebP in a country of missed opportunities</a></li>
<li><a href="../258549/index.html">How I became the maintainer of one of the Perl modules on CPAN</a></li>
<li><a href="../258553/index.html">Hewlett-Packard Webinars</a></li>
<li><a href="../258555/index.html">How I left the university to develop my own game. Part 1</a></li>
<li><a href="../258557/index.html">Video conferencing: the future is here</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>