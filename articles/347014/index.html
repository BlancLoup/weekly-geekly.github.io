<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Learning to manage Kubernetes securely</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="We recently created a distributed cron job planning system based on Kubernetes , an exciting new container cluster management platform. Now Kubernetes...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Learning to manage Kubernetes securely</h1><div class="post__text post__text-html js-mediator-article"><p>  We recently created a distributed <a href="https://en.wikipedia.org/wiki/Cron">cron</a> job planning system based on <a href="https://kubernetes.io/">Kubernetes</a> , an exciting new container cluster management platform.  Now Kubernetes takes leading positions and offers many interesting solutions.  One of its main advantages is that engineers do not need to know which machines have their applications running. <br>  Distributed systems are truly complex, and managing their services is one of the biggest challenges facing operational teams.  Implementing new software in production and learning how to reliably manage it is a task that should be taken seriously.  To understand why training with Kubernetes is important (and why it is difficult!), We suggest reading about the <a href="https://community.monzo.com/t/resolved-current-account-payments-may-fail-major-outage-27-10-2017/26296/95">fantastic one-hour switching</a> caused by an error in Kubernetes. </p><br><p>  This article explains why we decided to build architecture on Kubernetes.  We will describe how Kubernetes was integrated into the existing infrastructure, give an approach to building (and improving) confidence in the Kubernetes cluster reliability, and consider the abstractions that we implemented over Kubernetes. </p><a name="habracut"></a><br><h2 id="chto-takoe-kubernetes">  What is Kubernetes? </h2><br><p>  <a href="https://kubernetes.io/">Kubernetes</a> is a distributed system for planning the work of programs in a cluster.  You can order Kubernetes to run five copies of the program, and he will dynamically schedule their deployment on the worker nodes.  Containers are deployed automatically, which improves resource utilization and saves money.  Powerful <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">deployment primitives (Deployment Primitives)</a> allow you to gradually roll out new code, and Security Contexts and Network Policies securely launch different projects in the same cluster. </p><br><p>  Kubernetes has many built-in scheduling capabilities.  Scheduling HTTP services (long-running), daemonsets (daemonsets) that run on each node of the cluster, cron-tasks that run every hour, etc.  If you want to learn more, Kelsey Hightower gave several excellent explanations: <a href="https://www.youtube.com/watch%3Fv%3DHlAXp0-M6SY">Kubernetes for sysadmins</a> and <a href="https://vimeo.com/173610242">healthz:</a>  There is also a great <a href="http://slack.k8s.io/">Slack</a> community. </p><br><h2 id="pochemu-kubernetes">  Why Kubernetes? </h2><br><p>  Each infrastructure project (hopefully!) Starts with business needs, and our goal was to increase the reliability and security of the existing distributed cron-tasks system.  Our requirements: </p><br><ul><li>  Build infrastructure and manage it with a relatively small team (only 2 people worked on the project full-time). </li><li>  Schedule about 500 different cron tasks for all 20 nodes. <br>  Here are some reasons why we decided to use Kubernetes for this: </li><li>  We wanted to build the infrastructure on top of an existing open source project. </li><li>  Kubernetes includes a distributed cron scheduler, so we would not have to write it ourselves. </li><li>  Kubernetes is a very active project that regularly accepts contributions. </li><li>  Kubernetes is written in Go, which is easy to learn.  Almost all the fixes in Kubernetes were made by inexperienced programmers of our team. <br>  We used to use <a href="https://github.com/mesos/chronos">Chronos</a> as a cron scheduling system, but it no longer met our reliability requirements.  Currently, Chronos is <a href="https://github.com/mesos/chronos/graphs/commit-activity">practically not supported</a> (1 commit over the past 9 months; the last time a merge request was approved in March 2016).  Since Chronos is not supported, we decided that we should not continue to invest in improving the existing cluster. </li></ul><br><p>  If you are considering Kubernetes, keep in mind: <strong>do not use Kubernetes just because other companies use it</strong> .  Creating a reliable cluster takes an enormous amount of time, and the business example of its use is not always obvious.  Invest your time in a reasonable way. </p><br><h2 id="chto-oznachaet-nadezhnyy">  What does reliable mean? </h2><br><p>  When it comes to operating services, the word ‚Äúreliable‚Äù does not make sense by itself.  To talk about reliability, you first need to set SLO (purpose of service level). <br>  We had three goals: </p><br><ul><li>  99.99% of cron tasks should be scheduled and started within 20 minutes after the scheduled time.  20 minutes is a fairly wide window, but we interviewed internal customers, and none of them asked for higher accuracy. </li><li>  Tasks must be completed up to 99.99% of the time (without completion). </li><li>  Our migration to Kubernetes should not cause any customer related incidents. <br>  This meant several things: </li><li>  Short periods of downtime in the Kubernetes API are acceptable (if the API fell by 10 minutes, this is normal, as long as we can recover within 5 minutes). </li><li>  Scheduling errors (when a running cron task is not performed) are unacceptable.  We took very seriously planning error messages. </li><li>  You need to be careful in working with containers so that tasks are not interrupted too often. </li><li>  Need a good migration plan. </li></ul><br><h2 id="postroenie-klastera-kubernetes">  Kubernetes cluster building </h2><br><p>  Our basic approach to building the first Kubernetes cluster was to create a cluster from scratch, without using tools like <a href="https://kubernetes.io/docs/reference/generated/kubeadm/">kubeadm</a> or <a href="https://github.com/kubernetes/kops">kops</a> , using <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kubernetes The Hard Way</a> as a reference.  We deployed the cluster using Puppet.  Building a cluster from scratch was a good solution for two reasons: we were able to deeply integrate Kubernetes into our architecture and gained a deep understanding of how its internal components work. </p><br><p>  <strong>Building from scratch will allow Kubernetes to integrate into existing infrastructure.</strong> <br>  We wanted seamless integration with existing systems for logging, certificate management, secrets, network security, monitoring, AWS instance management, deployment, database proxy servers, internal DNS servers, configuration management, etc.  Integration of all these systems sometimes required a little creativity, but in general it was easier to integrate than to force kubeadm / kops to do what we wanted. <br>  We already trust existing systems and know how to manage them, so we would like to continue to use them in the new Kubernetes cluster.  For example, secure certificate management is a very complex issue, and we already have a way to issue and manage certificates.  We were able to avoid creating a new CA only for Kubernetes with proper integration. </p><br><p>  <strong>We had to understand how the set parameters influenced the Kubernetes setting.</strong>  For example, there are more than a dozen certificate / certificate authority settings used for authentication.  Understanding the operation of these parameters has made it easier to debug the installation when we are faced with authentication problems. </p><br><h2 id="ukreplenie-doveriya-k-kubernetes">  Build Confidence in Kubernetes </h2><br><p>  At the beginning of working with Kubernetes, none of the team members had previously worked with him (with the exception of some cases for toy projects).  What do you think of such statements: ‚Äúnone of us have ever used Kubernetes‚Äù, ‚Äúwe are sure that Kubernetes is ready to work in production‚Äù? <br><img src="https://habrastorage.org/webt/bc/n9/8n/bcn98nxf680i3x-ig6qejmxpvew.png"></p><br><h2 id="strategiya-0-pogovorite-s-drugimi-kompaniyami">  Strategy 0: talk to other companies </h2><br><p>  We asked about the experience of working with Kubernetes for several employees of other companies.  They all used Kubernetes differently or in different environments (to run HTTP services, on bare hardware, on <a href="https://cloud.google.com/kubernetes-engine/">Google Kubernetes Engine</a> , etc.). <br>  When it comes to a large and complex system, such as Kubernetes, it is important to think seriously about use cases.  You need to conduct experiments, build confidence in your environment and make your own decisions.  <strong>For example, you should not read this article and decide: ‚ÄúWell, Stripe successfully uses Kubernetes, so we will use it!‚Äù</strong> <br>  Here is what we learned from employees of companies working with Kubernetes clusters: </p><br><ul><li>  Prioritize work on cluster reliability (etcd, which stores the state of your Kubernetes cluster.) </li><li>  Some Kubernetes functions are more stable than others, so be careful with alpha functions.  Some companies use stable functions only after they are marked as stable for several issues (for example, if the function became stable in 1.8, they waited for 1.9 or 1.10 before using it). </li><li>  Consider using the Kubernetes hosted system, such as <a href="https://cloud.google.com/kubernetes-engine/">GKE</a> / <a href="https://azure.microsoft.com/en-ca/blog/introducing-azure-container-service-aks-managed-kubernetes-and-azure-container-registry-geo-replication/">AKS</a> / <a href="https://aws.amazon.com/eks/">EKS</a> .  Proper installation and setup of Kubernetes from scratch is a lot of work.  During the start of our work, AWS has not yet managed the Kubernetes service, so this path was not accessible to us. </li><li>  Be careful with the additional delay in the network created by overlay and software defined networks. <br>  The experience of other companies, of course, did not give us a clear idea of ‚Äã‚Äãwhether Kubernetes would be a good choice for us.  But the information allowed to ask yourself the right questions and understand what you should pay attention to when working with Kubernetes. </li></ul><br><h2 id="strategiya-1-prochitayte-kod">  Strategy 1: read the code </h2><br><p> In our plans there was a very strong dependence on one of the components of Kubernetes, the cronjob controller.  This component was alpha at the time, which made us uneasy.  We checked his work in the test cluster, but could not say whether it would work in production? <br>  Fortunately, all the basic functions of the cronjob controller are only 400 lines on Go.  Reading the <a href="">source code</a> quickly showed that: </p><br><ul><li>  The cronjob controller is a stateless service (like any other component of Kubernetes, except for etcd). </li><li> Every ten seconds, this controller calls the syncAll function: <code>go wait.Until(jm.syncAll, 10 * time.Second, stopCh)</code> . </li><li>  The <code>syncAll</code> function retrieves all cron jobs from the Kubernetes API, iterates through this list, determines which tasks should be performed next time, and then starts these tasks. <br>  The basic logic looked relatively easy to understand.  More importantly, we felt that if there was a mistake in this controller, we could fix it ourselves. </li></ul><br><h2 id="strategiya-2-vypolnyayte-nagruzochnoe-testirovanie">  Strategy 2: Perform Load Testing </h2><br><p>  Before creating the cluster, we did some load testing.  We didn‚Äôt worry about how many nodes Kubernetes could manage (in our plans it was about ~ 20 nodes), but really wanted some nodes to process as many cron tasks as possible (about 50 per minute). <br>  We conducted a test on a cluster of 3 nodes, where we created 1000 cron jobs, each of which was performed once a minute.  Each of these tasks just ran <code>bash -c 'echo hello world'</code> .  We chose simple tasks because we wanted to test the possibilities of planning and orchestrating the cluster, and its not the total computational capacity. <br>  Our test cluster could not process 1000 cron jobs per minute.  We noticed that each node would run no more than one item per second, and the cluster could run 200 cron jobs per minute without problems.  Since we wanted to run only about 50 cron jobs per minute, we decided that these restrictions are not blocking and that we could deal with them later if necessary.  Forward! </p><br><h2 id="strategiya-3-prioritet-na-postroenie-i-testirovanie-klastera-etcd-vysokoy-dostupnosti">  Strategy 3: prioritize building and testing high availability etcd cluster </h2><br><p>  One of the most important tasks you need to perform when setting up Kubernetes is running etcd.  Etcd is the heart of your Kubernetes cluster.  It stores data about all the events in your cluster.  All Kubernetes components, except for etcd, are stateless.  If etcd is not running, you will not be able to make changes to your cluster (although the existing services will continue to work!). <br>  This diagram shows exactly how etcd plays the role of the ‚Äúheart‚Äù of your Kubernetes cluster.  The API server is the final stateless point before etcd, each cluster component talks to etcd through the API server. <br><img src="https://habrastorage.org/webt/ti/4i/lb/ti4ilbtdcu0snniqtlusfclonhe.png"><br>  During the work it is necessary to consider two important points: </p><br><ul><li>  <strong>Set up replication so that the cluster does not die if you lose a node.</strong>  Now we have three replicas. </li><li>  Make sure you have enough I / O bandwidth.  Our version of etcd had a problem: one high latency fsync node could initiate continuous leader elections.  This caused the cluster to become unavailable.  We fixed this by making sure that all nodes have more I / O bandwidth than the number of write operations etcd. <br>  Configuring replication is not a ‚Äúset-and-forget‚Äù operation.  We have thoroughly tested the loss of the etcd node and made sure that the cluster can safely recover in case of such problems. <br>  Here are some of the work we did to set up the etcd cluster: </li><li>  Configure replication. </li><li>  Monitoring accessibility etcd (if etcd does not work, we want to know about it right away). </li><li>  Create a few simple tools to easily deploy new etcd nodes and merge them into a cluster. </li><li>  Verification of recovery from backup etcd. </li><li>  Check and confirm that we can rebuild the entire cluster without downtime. <br>  We were happy that we tested early, and here's why.  On Friday morning, in our production cluster, one of the etcd nodes stopped responding to ping.  We received a warning about this, stopped the node, raised a new one, attached it to the cluster, and in the meantime Kubernetes cluster continued to work without incident.  Fantasy. </li></ul><br><h2 id="strategiya-4-postepennaya-migraciya-zadaniy-cron-v-kubernetes">  Strategy 4: Gradually migrating cron jobs to Kubernetes </h2><br><p>  We set a goal to transfer our cron-tasks to Kubernetes without interruptions.  The secret to the success of successful production migrations is not to avoid mistakes (this is impossible), but to design migration so as to reduce the consequences of mistakes. <br>  Fortunately, we have many different cron tasks for migrating to a new cluster.  There were some low priority tasks that we could carry over with little downtime. <br>  Before starting the migration, we created an easy-to-use tool that, if necessary, would allow us to move tasks back and forth between the old and new systems in less than 5 minutes.  This simple tool has greatly reduced the impact of errors.  If, during a move, for example, a dependency that we did not plan would emerge, no harm!  We could just move the task back, fix the problem and try again later. <br>  Here is the general migration strategy we used: </p><br><ul><li>  Set a strict order of tasks for importance., </li><li>  Repeat the move to move over each cron task.  If we find a new problem, we quickly roll back, fix and try again. </li></ul><br><h2 id="strategiya-5-issleduyte-oshibki-kubernetes-i-ispravte-ih">  Strategy 5: investigate Kubernetes errors (and correct them) </h2><br><p>  At the beginning of the project, a rule was established: if Kubernetes does something strange or unexpected, we must investigate, find out the reasons and make corrections. <br>  The investigation of each question takes a long time, but it is very important.  If we do not pay attention to mistakes, then when working in a production environment, we will definitely encounter problems. <br>  After adopting this approach, we discovered (and were able to correct!) Several errors in Kubernetes. <br>  Here are a few issues found during the research: </p><br><ul><li>  Cronjobs with names longer than 52 characters silently do not perform scheduled tasks (corrected <a href="https://github.com/kubernetes/kubernetes/pull/52733">here</a> ). </li><li>  Sometimes Pods <a href="https://github.com/kubernetes/kubernetes/issues/49314">hung</a> in idle state (fixed <a href="https://github.com/kubernetes/kubernetes/pull/50028">here</a> and <a href="https://github.com/kubernetes/kubernetes/pull/50106">here</a> ). </li><li>  Scheduler <a href="https://github.com/kubernetes/kubernetes/issues/50916">crashes every 3 hours</a> (fixed <a href="https://github.com/kubernetes/kubernetes/pull/55262">here</a> ). </li><li>  The hostgw Flannel backend did not replace the outdated route table entries (corrected <a href="https://github.com/coreos/flannel/pull/803">here</a> ). <br>  The correction of these errors allowed us to feel much more confident in using Kubernetes. <br>  Kubernetes definitely has bugs, just like any other software.  In particular, we use the scheduler a lot and often (because our cron jobs constantly create new modules), and using the caching of the scheduler sometimes leads to errors, regressions and failures.  Caching is hard!  But the code base is available, and we were able to handle the errors we encountered. <br>  If you work with a large cluster of Kubernetes, carefully read the <a href="https://kubernetes.io/docs/concepts/architecture/nodes/">documentation for the host controller</a> , carefully consider and test the settings.  Every time we tested changing the configuration of these parameters (for example, <code>--pod-eviction-timeout</code> ), creating network partitions, an amazing <code>--pod-eviction-timeout</code> happened.  It is always better to detect these surprises when testing, rather than at 3 am in production. </li></ul><br><h2 id="strategiya-6-prednamerenno-vyzyvayte-problemy-klastera-kubernetes">  Strategy 6: Deliberately cause Kubernetes cluster problems </h2><br><p>  We used to discuss the exercises on the <a href="https://stripe.com/blog/game-day-exercises-at-stripe">game day</a> in Stripe, and we still do them.  The idea is to come up with situations that you expect to happen in production (for example, the Kubernetes API server crashes).  Then you need to deliberately reproduce these situations in production (during the working day with a warning) so that you can handle them. <br>  Performing exercises to the cluster often revealed problems in monitoring or configuration files.  We were glad to discover (and control!) These problems at an early stage, and not suddenly see after six months. <br>  Here are some exercises from the day of the game that we used: </p><br><ul><li>  Shut down one Kubernetes API server. </li><li>  Shut down all Kubernetes API servers and run them back (to our surprise, it worked very well). </li><li>  Shut down the etcd node. </li><li>  Reduce the number of working nodes in the Kubernetes cluster.  This should result in all pods migrating to other nodes. <br>  We were very pleased to see how well the Kubernetes handled many of the failures that we gave him.  Kubernetes is designed to be fault tolerant.  It has one etcd cluster that stores all states, an API server, which is a simple REST interface for this database and a set of stateless controllers that coordinate cluster management. <br>  If any of the major Kubernetes components (API server, controller manager, or scheduler) crash or restart, the parameters read the corresponding state from etcd and continue to work without problems.  This was the function we hoped for and believed that it would work well.  She actually showed excellent practical results. <br>  Here are a few problems that we discovered during the tests: </li><li>  ‚ÄúStrange, I have not received information about what really happened.  Let's fix our monitoring. ‚Äù </li><li>  ‚ÄúWhen we destroyed the API server instances and returned them, they required human intervention.  We better fix it. " </li><li>  "Sometimes, when we set up etcd failover, the API server runs timings requests until we restart it." <br>  After running these tests, we developed fixes for the problems found: improved monitoring, identified problems with a fixed configuration, and bugs with Kubernetes. </li></ul><br><h2 id="sozdanie-prostyh-v-ispolzovanii-cron-zadach">  Creating easy-to-use cron tasks </h2><br><p>  Let's take a quick look at how we made the Kubernetes based system easy to use. <br>  Our initial goal was to develop a cron job system that our team could easily maintain.  We became confident in choosing Kubernetes, then it was necessary to simplify the configuration and addition of tasks in cron for fellow engineers.  We have developed a simple YAML configuration format so that users do not need to understand the internal functions of Kubernetes to use the system.  The format we developed is: </p><br><pre> <code class="plaintext hljs">name: job-name-here kubernetes: schedule: '15 */2 * * *' command: - ruby - "/path/to/script.rb" resources: requests: cpu: 0.1 memory: 128M limits: memory: 1024M</code> </pre> <br><p>  We didn‚Äôt do anything interesting here, just wrote a simple program to take this format as a basis and get the configuration in cronjob in the Kubernetes format, which we use with <code>kubectl</code> . <br>  We also wrote tests to ensure that the names of tasks are not too long (the names of cron tasks can not exceed 52 characters) and that all names are unique.  Currently, we do not use <a href="https://en.wikipedia.org/wiki/Cgroups">cgroups</a> to limit the amount of memory on most cron tasks, but we plan to address this issue in the future. <br>  The simple format of the configuration file was easy to use, and since we automatically generated job definitions for Chronos and Kubernetes from the same source description, moving a job between any system was very simple.  This was a key part of the smooth operation of gradual migration. </p><br><h2 id="monitoring-kubernetes">  Kubernetes Monitoring </h2><br><p>  Monitoring the internal state of the Kubernetes cluster turned out to be easier than expected.  We use the <a href="https://github.com/kubernetes/kube-state-metrics">kube-state-metrics</a> indicator package for monitoring and the small program Go <a href="">veneur-prometheus</a> to collect metrics in Prometheus.  The kube-state-metrics indicators publish them as statsd indicators for our monitoring system. <br>  For example, here is a chart of the number of waiting containers in our cluster for the last hour.  Waiting means that they are waiting for the work node to be started.  You can see that at 11 am there was a surge.  This is because many of our cron jobs work at the 0th minute of the hour. </p><br><p><img src="https://habrastorage.org/webt/m_/9f/gp/m_9fgp-35gd7dkxgt-jsucgvehy.png"></p><br><p>  We also have a monitor that checks that there are no containers left in the waiting state.  We make sure that each unit starts working on the node within 5 minutes, otherwise we will receive a warning. </p><br><h2 id="buduschie-plany-dlya-kubernetes">  Future plans for Kubernetes </h2><br><p>  The process of setting up a Kubernetes cluster and transferring our cron-tasks to a new cluster took us five months with the participation of three full-time engineers. <br>  We invested in the study of Kubernetes, because we expect that we will be able to use Kubernetes more widely on Stripe. <br>  Here are some principles that apply when working with Kubernetes (or any other complex distributed system): </p><br><ul><li>  Identify a <strong>clear business reason</strong> for Kubernetes projects (and all infrastructure projects!).  Understanding the business case and the needs of our users has greatly simplified the project. </li><li>  Aggressively reduce the volume.  We decided not to use many of the basic features of Kubernetes to simplify the cluster.  This allows you to deliver changes faster.  For example, since the project did not require network interaction, we could use a firewall for all network connections between nodes and postpone thinking about network security in Kubernetes for a future project. </li><li>  Invest a significant amount of time in learning about the proper operation of the Kubernetes cluster.  Thoroughly test all decisions and changes.  Distributed systems are extremely complex, and there is a high probability of deviation from the plan.  Take the example we described earlier: a node controller can kill all containers in a cluster if they lose contact with the API servers. Learning the behavior of Kubernetes after each configuration change takes time and careful focusing. <br>  Without dwelling on these principles, we can confidently use Kubernetes to continue to grow and develop.  For example, we are watching with interest the release of AWS EKS.  We are finishing work on another system to work with a machine learning model, and we are also studying the movement of some HTTP services to Kubernetes.  As Kubernetes is used in production, we plan to contribute to the open source project. </li></ul><br><p>  Original: <a href="https://stripe.com/blog/operating-kubernetes">Learning to operate Kubernetes reliably</a> . </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/347014/">https://habr.com/ru/post/347014/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../346998/index.html">Javascript and horror mutations</a></li>
<li><a href="../347000/index.html">McSema and decompiling into LLVM source code: is it real?</a></li>
<li><a href="../347002/index.html">Studying MBR and GPT structures</a></li>
<li><a href="../347004/index.html">Ukrainian startups are back from Las Vegas. And so</a></li>
<li><a href="../347008/index.html">Platforms for learning experiments with reinforcement and not only</a></li>
<li><a href="../347016/index.html">We increase our premium twice, or how to hack documents signed with a reinforced qualified signature</a></li>
<li><a href="../347018/index.html">Developing Reusable Reusable Components</a></li>
<li><a href="../347020/index.html">Letter to Junior: what I would like to know at the beginning</a></li>
<li><a href="../347022/index.html">Work with problem * .dwg-files in the environment of nanoCAD</a></li>
<li><a href="../347024/index.html">How did I fix the interactive login, or What is there in the guts of // chrome / test / ChromeDriver?</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>