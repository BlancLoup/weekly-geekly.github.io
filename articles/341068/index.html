<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Service alert a million users with RabbitMQ</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Almost at the very beginning of the creation of the platform (some foundation, the framework on which all application solutions are based) of our clou...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Service alert a million users with RabbitMQ</h1><div class="post__text post__text-html js-mediator-article"><p>  Almost at the very beginning of the creation of the platform (some foundation, the framework on which all application solutions are based) of our cloud-based <a href="https://sbis.ru/">VLSI</a> web application, we realized that without a tool to let the user know about any event from the server, it would be quite difficult to live.  We all want to instantly see a new message from a colleague (who is too lazy to walk 10 meters), raising the corporate spirit of news from the management, a very important task from the testing department or receiving rewards (especially money).  But the path of becoming was thorny, so we will tell you a little about the difficulties that we encountered when growing up from 5.0e3 to 1.0e6 simultaneous connections from users. </p><br><img src="https://habrastorage.org/webt/di/pa/yz/dipayzml4jsvzsq8uladrqrh_c8.png"><a name="habracut"></a><br><br><h2 id="nachalo">  Start </h2><br><p>  The solution "in the forehead" can be a periodic survey from the browser server side.  But already then (back in 2013, when people were still shooting videos for Harlem Shake) we had dozens of different services, and their periodic survey could significantly raise our metrics for requests per second.  Therefore, there was a question about an intermediate super-service that would allow any backend of any service to send data to any desired user (the reverse interaction works via HTTP). </p><br><p>  At that time, web sockets have already gained enough popularity, which allows reducing the overall load and, more significantly, minimizing the delay in information delivery to users' browsers.  But, unfortunately, not all browsers that were popular among our favorite users supported web sockets (also not all proxy servers could skip them in the appendix), so support was also needed for more common technologies such as XHR-polling and XHR-streaming. </p><br><p>  The service was needed ‚Äúyesterday‚Äù, and therefore, so that the development strategy did not become <a href="https://youtu.be/bR-sJAI2Mts">‚Äúchick-chik and production‚Äù</a> , they decided to look for existing solutions.  The search led us into the hole to the broker RabbitMQ, which at that time already had plug-ins for working with the web environment: Web STOMP adapter and SockJS.  The first allows you to associate the binary protocol <a href="https://www.amqp.org/">AMQP</a> with a text <a href="https://stomp.github.io/">STOMP</a> , and the second - to establish a websocket-like connection to the user (he himself is concerned about the choice of supported transport).  Hoping for fruitful cooperation, we deployed and tested a cluster of several nodes.  RabbitMQ did not upset, and we decided to build a user alert system based on it.  At first, everything went well, but the development of functionality in our web application and the increase in the number of active users began to tell us that morning does not start with coffee. </p><br><h2 id="paru-slov-o-rabbitmq">  A few words about RabbitMQ </h2><br><p>  <a href="https://www.rabbitmq.com/">RabbitMQ</a> is a multi-protocol message broker that allows you to organize a fault-tolerant cluster with full data replication to several nodes, where each node can serve read and write requests.  It is written in the <a href="http://www.erlang.org/">Erlang</a> language and uses the <a href="https://www.rabbitmq.com/amqp-0-9-1-reference.html">AMQP</a> protocol <a href="https://www.rabbitmq.com/amqp-0-9-1-reference.html">version 0.9</a> as the main one.  Consider a casual scheme of the interaction of objects in AMQP: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ns/kq/xf/nskqxfdzpwsm6j5cak8us_-503i.png" width="550"></a> <br>  <em><b>Fig.</b></em>  <em><b>1. The</b> interaction of objects in the AMQP.</em> </p><br><p>  The queue (queue) stores and gives to consumers (consumer) all incoming messages.  The exchanger is engaged in routing messages (does not store them) on the basis of the created connections (binding) between it and the queues (or <a href="https://www.rabbitmq.com/e2e.html">other exchangers</a> ).  From the RabbitMQ point of view, a queue is an <a href="http://erlang.org/doc/reference_manual/processes.html">Erlang process</a> with a state (where the messages themselves can be cached), and the exchanger is a ‚Äúlink‚Äù to the module with the code where the routing logic lies.  That is, for example, 10 thousand exchangers will consume about 12 MB of memory, when 10 thousand queues are already about 800 MB. </p><br><p>  In <a href="https://www.rabbitmq.com/clustering.html">cluster mode</a> , meta-information is copied to all nodes.  That is, each node contains a complete list of exchangers, queues, their connections, consumers and other objects.  The processes themselves with queues by default are located only on one node, but using policies you can enable replication ( <a href="https://www.rabbitmq.com/ha.html">mirrored queues</a> ), and the data will be copied automatically to the required number of nodes. </p><br><h2 id="naivnaya-realizaciya">  Naive implementation </h2><br><p>  The first implementation was quite simple: </p><br><ul><li>  RabbitMQ cluster of three nodes, to which both backends and clients were connected; </li><li>  At the start, backends created exchangers with the names of their events (including the name of the website, for example, ‚Äúonline.sbis.ru: contacts.new-message‚Äù) and the direct type; </li><li>  Clients connected via SockJS and ‚Äúlistened‚Äù to the events they needed by subscribing (subscribe) to the corresponding exchangers with a key equal to the user ID; </li><li>  At the time of the origin of the event, the backend sent an AMQP message to the appropriate exchanger.  As the routing key, the message had a recipient identifier. </li></ul><br><p>  Schematically, it all looked like this: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/zi/gb/ew/zigbews8lyph3bze85xngrynihg.png" width="300"></a> <br>  <em><b>Fig.</b></em>  <em><b>2.</b> Connection diagram.</em> </p><br><p> <a href=""><img src="https://habrastorage.org/webt/s-/pg/8x/s-pg8xwky76kzvk-rsnklh6fuys.png" width="500"></a> <br>  <em><b>Fig.</b></em>  <em><b>3.</b> Scheme of objects in the broker.</em> </p><br><div class="spoiler">  <b class="spoiler_title">Types of exchangers</b> <div class="spoiler_text"><p>  There are three main types of exchangers in AMQP: fanout, direct and topic. </p><br><p>  The easiest and fastest type is fanout.  Communication with him does not have any parameters, to receive messages, only their presence is important. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/px/v5/q_/pxv5q_sbhwl7owqwdvoqnmtfhbg.png" width="550"></a> </p><br><p>  <b>Fig.</b>  <b>4.</b> Fanout exchanger. </p><br><p>  Here the dotted line shows the connection, and the green line - the path of the message.  It will be copied to all queues. </p><br><p>  For the direct exchanger, when creating a connection, a string key is specified, which participates in the selection of candidates for receiving the message.  The speed of its work is slightly inferior to the speed of the fanout.  When you publish a message, it indicates the routing key (routing key) and, if this key coincides completely with the communication key, then the final queue will receive a message. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/sb/ve/tr/sbvetrkvu-ocoe5fqngagqynp1u.png" width="550"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>5.</b> Direct exchanger.</em> </p><br><p>  Topic treats a key not just as a string, but as a set of tokens separated by a dot.  You can use the asterisk symbol to indicate the unimportance of the value of the token for us or the grid symbol, which replaces zero or more tokens. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/g3/on/av/g3onav-q2cilsn6jnq4hjmckxy4.png" width="550"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>6.</b> Topic exchanger.</em> </p><br><p>  The speed of such an exchanger depends on the complexity of the key and the number of tokens in it, and in the simplest case it is similar to the speed of the direct exchanger. </p><br><p>  There is another type of exchanger - headers, but it is not recommended to use it because of the speed of work, so we omit it. </p></div></div><br><h3 id="rost-kolichestva-sobytiy">  The increase in the number of events </h3><br><p> At that time, the client code in the browser ‚Äúlistened‚Äù to an average of about 10 events per page, but their growth did not take long (now, for example, only about 70 of them are on the main page).  Their unpredictable life adds pain - an event may be required at any time and for any interval of time. </p><br><p>  But the main problem, of course, was created by the fact that when you subscribe to the STOMP for any exchanger, a queue is automatically created, which is already associated with the exchanger.  And if there are 30 such subscriptions, then there will be 30 queues (within one connection), which can not but affect the resources consumed and the frequency of the call to the <a href="https://en.wikipedia.org/wiki/Out_of_memory">OOM</a> killer service. </p><br><p>  To solve this problem, we changed the approach and removed the name of the event from the name of the exchanger in the header of the AMQP message itself.  The structure is simplified and began to look something like this: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/xu/qs/ni/xuqsniggeyteeebrhbrg4axvlz0.png" width="350"></a> <br>  <em><b>Fig.</b></em>  <em><b>7.</b> The structure of the objects after the first improvement.</em> </p><br><p>  Thus, we have removed the dependence on the number of events on the page, and now only one subscription was required by the user.  But I had to sacrifice traffic, since in the new approach, it was possible to understand whether an event is necessary for the user, only after it is delivered to the user. </p><br><h3 id="oblasti-publikaciy">  Publication areas </h3><br><p>  In our system, there are the concepts of "client" - any company that uses our product, and "user" - an employee of the client.  Companies are completely different, and someone may have two or three employees, and someone may have a few thousand.  If you wanted to send a message to all users of the client, you had to go over the entire list in a loop, which could take a long time.  Therefore, it was urgent to do something about it. </p><br><p>  The solution was the creation of three areas of publication: the user, the client and globally everything.  The global area is required to send system information, for example, please reload the page after updating the backends.  Each region was implemented in RabbitMQ as an exchanger with the direct type and, in order not to force the client to make several subscriptions again, another exchanger with the fanout type was added - the user's personal exchanger. </p><br><p> <a href=""><img src="https://habrastorage.org/webt/df/mt/rm/dfmtrmdybmra8x4zxwevpympt-4.png" width="450"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>8.</b> The structure of objects with areas of publication and personal exchanger.</em> </p><br><p>  The personal exchanger contacted the exchangers of the regions with the necessary identifiers, and the user needed to simply connect to it without any parameters.  But now we had to create personal exchangers before users would subscribe, otherwise there would be an error.  Pre-creating objects for all existing users is quite expensive.  Therefore, we decided to create them dynamically at the moment of connection.  To do this, a nugget was made on Nginx (faced RabbitMQ), jerking a helper utility before proxying a broker request.  The utility checked the existence of all necessary and created in the absence.  Later, they made a utility call only for the request <em>/ info</em> , which is mandatory for SockJS. </p><br><h3 id="auto-delete-ocheredi">  Auto-delete queue </h3><br><p>  Initially, we created queues that were automatically deleted after disconnecting (auto-delete), but this led to two big problems: </p><br><ol><li><p>  With a short break in communication, we lost messages that had not yet reached; </p><br></li><li>  A large number of operations were performed in the system for creating / deleting queues and connections, especially before the optimization of the number of subscriptions.  In the cluster mode, such a large workflow sprawled across all nodes and created an increased load.  This phenomenon is called binding churn. </li></ol><br><p>  As a result, they switched to permanent (persistent) queues that are not deleted at the time of disconnecting the connection, but the ‚Äúexpire‚Äù policy was hung on them if the user is not longer than 5 minutes.  (The system does not pursue the goal of storing data that is not received by the user forever, but is developed only for their prompt delivery.) </p><br><p>  The load on the RabbitMQ cluster has decreased, but user growth has continued.  After a pass of 50 thousand simultaneous users, the system approached its border in terms of resource consumption.  We had to change something again. </p><br><h2 id="opovescheniya-20">  Alerts 2.0 </h2><br><p>  After the problems we experienced earlier, we realized that in this case scaling up the RabbitMQ cluster nodes is not a very optimal way, since problems on one node affected the performance of other nodes, and some kind of fault tolerance at the level of data duplication was not required for our system.  But on the other hand for backends it is very convenient to have a single point for connection.  After some discussion, they came to the conclusion that the system should be divided into two layers: 1) RabbitMQ cluster for connecting backends and 2) independent RabbitMQ nodes for connecting users.  As a result, we have the following scheme: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/hr/ij/ye/hrijyewut-0xrxbzdgqxfwzkgfm.png" width="300"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>9.</b> Two-layer wiring diagram.</em> </p><br><p>  Independent nodes were given the name "Web", since users connect to them via web protocols.  These nodes themselves connect to the cluster using the <a href="https://www.rabbitmq.com/federation.html">Federation</a> RabbitMQ <a href="https://www.rabbitmq.com/federation.html">plugin</a> . </p><br><p> <a href=""><img src="https://habrastorage.org/webt/gc/-c/wb/gc-cwbmk_a5oms3wgzb2n3jahia.png" width="600"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>10.</b> The structure of objects in a two-layer scheme.</em> </p><br><p>  Setting up federation on websites is quite simple: </p><br><ol><li>  We create an upstream (upstream, source), it can consist of both a single node and a list of nodes for fault tolerance; </li><li>  We create a policy in which we say that exchangers with such a name template should be synchronized with upstream. </li></ol><br><p>  After that, the Federation will connect to the upstream nodes, create their own queues for them and subscribe them to the necessary exchangers, then start receiving messages and duplicate them into local exchangers. </p><br><p>  Thus, the message published in the exchanger on the cluster, will fall into the same exchangers on all Web sites.  And the latter we can scale almost linearly depending on the number of users.  Great how! </p><br><h3 id="raspredelenie-klientov">  Customer distribution </h3><br><p>  The most ideal scenario would be the even distribution of client connections among all websites, as well as their ‚Äúloyalty‚Äù to only one of them, which would help reduce the overhead of the user.  But do not forget that the nodes fail, and the user does not have to wait until this node is repaired.  In front of N websites we have M dispatchers (Nginx, which proxies the request to the desired backend).  And all we had to do was make sure that the configuration on the dispatchers was identical and configure them to choose the backend by user ID (for example, through the <a href="https://nginx.ru/ru/docs/stream/ngx_stream_upstream_module.html">hash</a> directive). </p><br><p>  The resulting scheme is not without flaws, but in the end it allowed us to serve more users and withstand about 300 thousand simultaneous connections to 8 web sites. </p><br><h2 id="opovescheniya-30-novaya-nadezhda">  Alerts 3.0: New Hope </h2><br><h3 id="federaciya-hvatit-umnichat">  Federation, be smart enough! </h3><br><p>  The Federation is convenient because it creates everything itself, but its goal is also to minimize traffic between nodes.  Therefore, it creates connections for its queues on upstream only with the keys that are on the exchanger under its control.  And in our case, it turned out that the keys were the number of users who visited this or that web site.  During the day, they could run up to 50 thousand.  And when a collapse happened on one of the nodes or it was simply overloaded, its users could ‚Äúinherit‚Äù on other nodes. </p><br><p>  It would seem, what is wrong with that?  But there are 2 points: </p><br><ol><li>  There were a very large number of links on the upstream nodes (on cluster nodes), equal to the total number of active users ‚Äî for any sent message, it was necessary to first search for interested queues of web sites.  With the growth of users, this could not affect the performance. </li><li>  And the cherry on the cake - synchronization.  After the website returned from reboot or restored the lost connection to an upstream, the Federation plugin started the synchronization process, which usually lasted several minutes.  During this time, processor utilization on cluster nodes was high and caused delays in the delivery of messages. </li></ol><br><p>  As a replacement, we decided to look at another similar plugin - Shovel.  His goal is to transfer data from one node to another without unnecessary problems (or between the queues of one node). </p><br><p>  When replacing, we saw the expected increase in network load, but the cost of the processor and the reduction of delays were worth it.  As a result, when switching to a working system, we noticed a sharp ‚Äúweight loss‚Äù of cluster nodes for a couple of gigabytes of memory, and reloading the web site did not create any additional load. </p><br><h3 id="bolshoe-kolichestvo-svyazey-mezhdu-sloyami">  A large number of connections between the layers </h3><br><p>  In addition to the growth in the number of users and services, there has also been an increase in the number of websites that should have been served by our alert system.  If at the beginning there were 3 of them, then this number increased to 10, and no one was going to stop.  As can be understood from the last picture of the structure of objects, in the current implementation the number of connections depends on the number of sites served, and even by a factor of 3 (the number of publishing areas).  Moreover, the Shovel plugin does not work through policies (as Federation can do), and we need to create all the connections manually after creating each publishing area exchanger (the same script that runs when the user connects).  And can we get rid of this dependence and use one connection for all sites?  - Need to! </p><br><p>  Immediately remembered another type of exchanger "out of the box" - headers.  What he is not an assistant?  The principle of its operation is such that when creating a connection to it, a certain set of parameters and their values ‚Äã‚Äãare specified, which are also specified in the message headers, and if they match, the connection is triggered.  In our case, there were two such parameters: the site name and the publish area.  For example, <strong>site = "online.sbis.ru"</strong> and <strong>scope = "user"</strong> .  This pair was set for communication and for the message.  The only thing that stopped was the postscript in the documentation, that the speed of such an exchanger is small compared to others.  But we had few links with him, and the number of messages is not that big.  Tests have shown that he quite cheerfully copes with the load.  We decided to go for it. </p><br><p>  This was the first failure of this magnitude, since the tests were insufficient in duration.  The behavior of the exchanger was very strange, the first few hours everything worked well, and then the processing speed slowly decreased to zero, although the number of links did not change.  I had to quickly write an external script that performed the work of this exchanger.  Pechalka. </p><br><h3 id="a-mozhet-svoy-obmennik">  Or maybe your exchanger? </h3><br><p>  In the course of solving various problems with RabbitMQ often encountered the language of Erlang - it's time to study it better.  As it turned out, implementing your exchanger for RabbitMQ is not that difficult.  And from that moment on we began a new era of working with this broker. </p><br><p>  After studying the code of standard exchangers ( <a href="">fanout</a> , <a href="">direct</a> ), it became clear that the list of resources that need to send a copy of the message is waiting for the route RabbitMQ function.  Standard exchangers extract this list of resources from the internal database of links.  But in our case, the names of the following exchangers are generally calculated based on the incoming message. </p><br><div class="spoiler">  <b class="spoiler_title">We get the function (simplified)</b> <div class="spoiler_text"><pre><code class="erlang hljs"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">route</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(#exchange{name = #resource{virtual_host = VHost}}, #delivery{message = #basic_message{content = Content}})</span></span></span><span class="hljs-function"> -&gt;</span></span> Headers = (Content#content.properties)#'P_basic'.headers, Scope = <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> rabbit_misc:table_lookup(Headers, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"scope"</span></span>&gt;&gt;) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {longstr, Scope} -&gt; Scope; _ -&gt; &lt;&lt;<span class="hljs-string"><span class="hljs-string">"user"</span></span>&gt;&gt; <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>, Sites = <span class="hljs-keyword"><span class="hljs-keyword">case</span></span> rabbit_misc:table_lookup(Headers, &lt;&lt;<span class="hljs-string"><span class="hljs-string">"sites"</span></span>&gt;&gt;) <span class="hljs-keyword"><span class="hljs-keyword">of</span></span> {array, Sites} -&gt; [ Site || {longstr, Site} &lt;- Sites ]; _ -&gt; [] <span class="hljs-keyword"><span class="hljs-keyword">end</span></span>, [ rabbit_misc:r(VHost, exchange, &lt;&lt;Site/binary, <span class="hljs-string"><span class="hljs-string">"."</span></span>, Scope/binary&gt;&gt;) || Site &lt;- Sites ].</code> </pre> </div></div><br><p>  Now we need to arrange everything as a <a href="https://www.rabbitmq.com/plugin-development.html">plug-in for RabbitMQ</a> , which is also quite simple (then there was a version of RabbitMQ 3.5, and it was a bit more complicated).  As a result, everything took off and even worked faster than fanout due to the lack of samples in the database.  Crunchy!  By the way, the type of exchanger called sbis-ep (entry point). </p><br><h3 id="problemy-pri-bolshih-soobscheniyah">  Problems with large messages </h3><br><p>  At one point, the web sites began to "die like flies" due to lack of memory.  With the help of the <strong>rabbitmq_tracing</strong> plugin, <strong>it was</strong> possible to understand that good people arranged some kind of CDN from the alert system and sent 20 MB of data in one message.  But their switching off of joy did not add - the falls continued (although with a lesser frequency).  The biggest messages remained at 400 Kb.  For a simple event, this is also quite a lot, but the system should not die of such.  The first night of acquaintance with the debugging tools for Erlang was productive, and by the morning they were rolling out healing patches.  It turned out that SockJS used the xmerl_ucs module to work with UTF8, which converted the entire binary into lists and worked with them.  As a result, this led to the fact that the 400-kilobyte message during the processing ate more than 16 MB of memory.  And such messages were sent to several users at once.  In the <a href="https://github.com/rabbitmq/sockjs-erlang/pull/10">patch,</a> working with UTF8 was completely redone using only binary strings, after which there were no drops.  You could go to sleep. </p><br><h3 id="otmechaem-otsutstvuyuschih">  Note missing </h3><br><p>  There was another unpleasant problem due to the fact that in RabbitMQ there is no <a href="https://www.rabbitmq.com/ttl.html">Expire</a> parameter for exchangers (only for queues).  When the user came, everything was created for him, but when he left, the resources were not deleted.  For preventive purposes, we had to periodically overload websites.  The first experience of your own exchanger left a wave of positive - can another one with TTL support do this?  Wow!  But what if it is a personal exchanger of the user (he is fanout so far), and during his creation he will also create all the other objects on which he depends?  This is already quite grown-up!  <strong>Let's</strong> call the exchanger <strong>sbis-user</strong> . </p><br><p>  The names of the parent exchangers and linking parameters depend on the user and client identifiers, as well as on behalf of the site on which the user works.  We can transfer them through the parameters (arguments) of the new exchanger when it is created.  Well, add the ability to control the lifetime of the orphan exchanger, too, through the parameter. </p><br><p>  Create exchangers and connections - you don‚Äôt need much work: just call the already existing RabbitMQ functions and there will be happiness.  But for the organization of the mechanism of TTL still need to work.  The fact is that the exchangers do not have their own Erlang process, where we could periodically check the number of current users and remove ourselves.  Therefore, in addition to the implementation of the exchanger module, we also need our own process, which would follow everyone.  To do this, you need to create an Erlang-application (application) - it will run with the start of our new broker plugin and, in fact, contain one workflow. </p><br><p>  The process of removing orphan exchangers could be naively implemented: periodically we receive a list of all exchangers;  for each of them we get the number of links;  if they are not there, then we add them to the list for deletion and delete them at the required interval.  But this approach creates a large load, especially if we have tens or hundreds of thousands of exchangers.  The <a href="">API for exchangers</a> has functions that the broker calls when creating or deleting links.  They are just what will help us make the best decision.  At the time of creating a new link, we always remove the exchanger from the list of orphans, and at the time of deleting the link, we put it there, if there are no other links. </p><br><p>  The last question: what structure do we need to use in order to be able to check the existence of the exchanger in the list, remove it from there, and also look for already expired exchangers at the lowest cost?  After several tests, we stopped at a combination of two standard structures: </p><br><ul><li>  <strong>gb_tree</strong> , is good for finding the minimum value (the exchanger with the shortest expiration date, which is compared with the current time), as the key structure <strong>{ExpireTime, Exchange}</strong> , and the value is 0 (not used). </li><li>  <strong>map</strong> (already worked on Erlang / OTP 18), as an <strong>Exchange</strong> key, and as an <strong>ExpireTime</strong> value, used to check for existence and search for time (for deletion from the first structure). </li></ul><br><p>  In addition, the number of simultaneously deleted exchangers was limited so as not to create peak loads.  Everything, our second plugin is ready to work for the benefit of users. </p><br><h3 id="opravdalas-li-nadezhda">  Is hope justified? </h3><br><p>  Certainly yes.  We get an even simpler, more beautiful and reliable structure: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/ug/3z/fl/ug3zflh3edzhtoden9nkqyf5btu.png" width="650"></a> </p><br><p>  <b>Fig.</b>  <b>11.</b> The structure of objects in the scheme with its own exchangers <em>sbis-ep</em> and <em>sbis-user</em> . </p><br><p>  On cluster nodes we have only one exchanger with fanout type, only one connection between the web site and the cluster, the scheme does not depend on the number of sites, and after the user‚Äôs absence on the site, the resources allocated to him are cleared.  Just a fairy tale, what else can you dream?  In this form, the system served us up to 600 thousand simultaneous connections on already 15 web-sites, there were peaks of 300 thousand outgoing messages per second.  But that was not her limit. </p><br><h2 id="opovescheniya-x-infinity-and-beyond">  X alerts: Infinity and beyond </h2><br><p>  It is time for new adventures, since the growth of users was not going to stop. </p><br><h3 id="vsyo-vnutri">  All inside </h3><br><p>  Already almost all service operations are working inside RabbitMQ (which gives us minimal delays).  The only thing left outside is the creation of a personal user exchanger.  But after all nothing prevents us and it to realize as a plug-in for RabbitMQ, truth?  So it is, we can use the already working Cowboy web server and initiate the creation of exchangers from our next new plugin. </p><br><p>  Since the browser version of SockJS from the request for <strong>/ info</strong> only requires the <strong>websocket</strong> flag, and the server side in general can do without this request, we can not just intercept this request, but also fully process it.  That is, the request for this resource goes to our plugin, and all other requests continue to go only to the RabbitMQ Web STOMP adapter. </p><br><h3 id="64k-portov-hvatit-vsem">  64K ports will be enough for everyone </h3><br><p>  One of the problems that caused a large number of web sites to be raised was the limit on the number of open connections between Nginx and RabbitMQ on one host.  Nginx could not specify the outgoing connection address, and on the side of the Web STOMP plug-in it was impossible to specify several ports.  In addition, it was necessary to see how many users we now live on web sockets, and how many - on XHR-poling and streaming.  Nginx we could not tell this, and had to fence not very good things. </p><br><p>  Then HAProxy came to the rescue, which <a href="https://gist.github.com/sega-yarkin/b81443f63266e41090e8cce0c819b72d">allowed</a> us to: </p><br><ul><li>  Send requests to the correct port depending on the URL (like Nginx); </li><li>  Create a large number of connections to one port (the source option allows you to specify the outgoing address, and we have a lot of them locally, 127.0.0.0/8); </li><li>  Limit the maximum number of connections to protect the service from massive influx of requests and make it more stable; </li><li>  Receive a lot of statistics about the service (the number of current connections for a particular transport, the speed of new arrivals, response time, response codes, the time to wait for a request in the queue, and others); </li><li>  Save resources.  HAProxy consumed 3 times less memory on the same number of connections, and the processor load was 3-5%. </li></ul><br><h3 id="cache-and-compress-it">  Cache and compress it! </h3><br><p>  RabbitMQ comes with a convenient web management console - Management plugin.  It allows service personnel to quickly assess what is happening in the broker.  Also through its API, we collect statistics on the work of the broker.  The problem is that it does not have any caching mechanisms, and it collects all the information with each request.  In RabbitMQ 3.5, there is no pagination output, and a full list of resources is always given.  And by default, auto-update is enabled after 5 seconds.  It turns out that with 40 thousand queues all information on them can be collected for 10 seconds, eating a lot of CPU time, and weigh 15-20 MB each (load on the network).  That is, by default, the user will make requests constantly.  And if there are several users?  Scary to remember. </p><br><p>  Before switching to HAProxy, we could make a cache on Nginx, but now we decided to try Varnish, since we have a bit of data and caching in memory more efficiently.  Enabled caching ( <strong>beresp.ttl</strong> ) of all content for 15 seconds (for statics a few days) and validity of the cache ( <strong>beresp.grace</strong> ) for the same amount, as well as data compression ( <strong>beresp.do_gzip</strong> ). </p><br><p>  As a result, <a href="https://gist.github.com/sega-yarkin/c26bc5fbf8c6afc1fae9dedd8f90c138">received</a> : </p><br><ul><li>  The request to the resource is performed once and no more than 4 times per minute; </li><li>  Compressed data weigh almost 50 times less than the original (270 KB against 13 MB); </li><li>  The data was given in a matter of milliseconds (it seems that Varnish gives overdue data, if the grace interval is not exceeded, and in the background makes a request to the backend for updating them - it looks great); </li><li>  The load on the broker does not depend on the number of users and is computable. </li></ul><br><h3 id="svet-moy-zerkalce-skazhi">  Light my mirror, say </h3><br><p>  After we optimized web-sites and learned how to scale them well, problems began to emerge already on the lower layer, on cluster nodes.  Tens and hundreds of thousands of outgoing messages per second were mainly due to publications in the client area.  For example, in our company there may be 5,000 people online.  If someone sends 60 messages per second to the entire company, then we will eventually have 60x5000 = 300 thousand / sec.  For cluster nodes, the first factor is important - the number of messages published by backends in RabbitMQ.  And at some point it also began to grow.  It turned out that two sites with full replication (mirroring), serving 18 websites, could process no more than 2000 publications per second, and there was no longer any possibility to adequately scale.  The number of backends also increased, and the number of connections from them exceeded 2500. The circuit required changes. </p><br><p>  Since RabbitMQ showed itself to be quite stable, and for the warning system it was not required ‚Äúmany nines after the comma‚Äù, we decided to abandon the cluster.  Moreover, the backends already knew how to work with the list of nodes and repeat the request to another node if one of them failed.  It remains to teach web sites to connect immediately to all existing downstream brokers, which is not a problem at all.  The new type of nodes was given the name route (in the future, routing mechanisms will also be assigned to it), and after switching to it, the scheme began to look like this: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/yw/vg/0q/ywvg0qyznfnpcxffhg2oyvkt7_w.png" width="300"></a> <br>  <em><b>Fig.</b></em>  <em><b>12.</b> Two-layer scheme with non-cluster nodes.</em> </p><br><p>  Now we can scale both layers as needed.  The variant with two route nodes of the same configuration as cluster nodes already processed about 13 thousand publications per second, but this is not the limit for them! </p><br><h3 id="u-vas-odno-novoe-soobschenie-v-chate">  You have one new message in the chat </h3><br><p>  With the development and popularization of chat rooms in our cloud-based application, another problem arose: the mass sending of identical messages to a list of users.  If there are a dozen people in the chat, then everything happens fairly quickly.  But with several hundred users, posting messages in RabbitMQ could take seconds (and also create a lot of work).  We needed a mechanism that would effectively send the message to the list of recipients. </p><br><p>  It immediately became clear that we can transmit this list through the AMQP message headers (as we transmit the list of sites).  The problem was the task of processing it on the Web-sites.  The first solution used the fact that the names of the user's personal exchangers (sbis-user) consist precisely of the site name and user ID, and we can modify our sbis-ep exchanger to handle such cases.  That is, a message comes with a list of sites and users, the code of the exchanger multiplies these lists and gives the result to RabbitMQ, which searches for its own database and sends it to them by copy of the message.  The option was simple and quick to implement, but it had several disadvantages: </p><br><ul><li>  Works with only one publication area (user); </li><li>  It creates a very redundant list, since one website served only about 5% of all users.  In addition, they could only live on one site, for example, out of three on the list. </li></ul><br><p>  Despite all the shortcomings, the solution was sent to work sites and reduced the workload when communicating in large chat rooms.  The only thing that was added - an artificial restriction on the length of the list of users in 50 units.  Otherwise, a large load multiplier could turn out when the backend published a message within a couple of milliseconds, and all the web sites then worked for a few seconds (with great force comes great responsibility, but not all developers know and remember the second). </p><br><h3 id="sdelaem-vsyo-zhe-kak-nado">  We will do all the same </h3><br><p>  Now, when the degree of the last problem is slightly reduced, it is possible to implement the mechanisms in an adequate way.  It would be more correct to have a base of existing users and search by it, instead of generating a large list.  RabbitMQ already has a table in the <a href="http://erlang.org/doc/man/mnesia.html">Mnesia</a> embedded database with a list of all exchangers.  Unfortunately, it is not organized in the best way for our search, so you have to make your own.  The new table will be of type set and will contain the exchanger as the primary key, followed by all other identifiers needed for the search (which list can be expanded if necessary). </p><br><p>  When processing messages, we now build a <a href="http://erlang.org/doc/man/qlc.html">QLC-</a> request containing the necessary lists of values, and give it to the base for execution.  The result will be a list of existing exchangers that meet all criteria.  Everything works correctly, but we immediately met a drop in performance, although we expected to see an acceleration.  We must somehow correct the situation. </p><br><p>  When we do a search on the primary key, it is performed almost instantly, and to search on other columns of the table we have to go through all its records.  But there are secondary indices in Mnesia - let's turn them on and enjoy the speed!  And we are waiting for another disappointment, nothing has changed.  Search with 50 users and 3 sites fulfills for 30 milliseconds.  The problem here is that we are looking for two fields at once (the identifier and the site), and the indices do not help much (since the source data is only about 10 MB).  The fact that for the internal database there are no restrictions on data types comes to the rescue, and we can insert the tuple <strong>{Site, Ident}</strong> as the field value.  Then the search will take place only in one field.  Now the secondary index is working at full strength, but it got worse, 60 milliseconds, how so?  The fact is that we had to expand two lists into one big one, and the search query became huge. </p><br><p>  Well, let's go on the other side.  Mnesia has the function of searching for the value of the index, which should work for the minimum time, but it works for one value, so you need to add a loop.  In addition, we‚Äôll go all out and use the dirty operations in Mnesia to increase performance. </p><br><div class="spoiler">  <b class="spoiler_title">The result is a simple function.</b> <div class="spoiler_text"><pre> <code class="erlang hljs"><span class="hljs-function"><span class="hljs-title"><span class="hljs-function"><span class="hljs-title">user_find_in_index</span></span></span><span class="hljs-params"><span class="hljs-function"><span class="hljs-params">(Sites, Ids, IndexPos)</span></span></span><span class="hljs-function"> -&gt;</span></span> [ element(<span class="hljs-number"><span class="hljs-number">2</span></span>, Item) || Site &lt;- Sites, Id &lt;- Ids, Item &lt;- mnesia:dirty_index_read(user_route, {Site, Id}, IndexPos) ].</code> </pre> </div></div><br><p>  Each index search takes about 6 microseconds, that is, with the same list sizes, we get an answer in less than 1 millisecond (with smaller sizes, the gain is even greater).  This result already suits us. </p><br><h3 id="poslednie-svodki-s-poley">  Latest field reports </h3><br><p>  After all changes we get the following structure of objects: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/uw/c2/c_/uwc2c_c8itzkjd8f1oft3ejzimu.png" width="600"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>13.</b> The final structure of objects.</em> </p><br><p>  The last scheme of connecting nodes and the structure of objects in the broker allowed us to serve <br>  1 million simultaneous web sockets on 21 web sites (there were 1.1 million, but the aggregation of history in the monitoring did not leave a trace from this figure): </p><br><p> <a href=""><img src="https://habrastorage.org/webt/db/_3/zp/db_3zpqtlgcfb1kj3yi4eegoi-i.png"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>14.</b> Total number of connections per work week.</em> </p><br><p>  And also contributed to sending almost 1.3 million messages per second to users with a total traffic of more than 4.2 gigabits / sec: </p><br><p> <a href=""><img src="https://habrastorage.org/webt/zx/z-/sw/zxz-swdj7oann7uad0upwnjhki0.png"></a> </p><br><p>  <em><b>Fig.</b></em>  <em><b>15.</b> Processing peak loads: the number of messages and megabytes of traffic per second.</em> </p><br><h2 id="zaklyuchenie">  Conclusion </h2><br><p>  At the time of the start of building the system for notifying users using RabbitMQ, we could not imagine how large the system would be in the end, and how many messages could be processed.  Let RabbitMQ be capricious from time to time, but allowed us to implement everything with little development costs (especially when we mastered Kung-Fu in writing plug-ins) and reduce implementation time. </p><br><p>  We are happy to share our experience with the community and will be happy to answer all questions. </p><br><p>  <strong>The author of the article is Sergey Yarkin</strong> </p></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/341068/">https://habr.com/ru/post/341068/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../341056/index.html">Who is a fullstack designer</a></li>
<li><a href="../341058/index.html">JAVA 9. What's new?</a></li>
<li><a href="../341060/index.html">UX design: 50 things you probably forgot to do</a></li>
<li><a href="../341064/index.html">We optimize PropertyDrawer under Unity3d</a></li>
<li><a href="../341066/index.html">Cocos2d-x - Event Manager</a></li>
<li><a href="../341070/index.html">Nearly 2018, and we love callbacks</a></li>
<li><a href="../341072/index.html">We work with Bitcoin on Elixir</a></li>
<li><a href="../341074/index.html">Translation and dubbing of a movie at home - Indie Game: The Movie Special Edition</a></li>
<li><a href="../341076/index.html">Ember.js: a great framework for web applications</a></li>
<li><a href="../341080/index.html">How to render frame Unreal Engine</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>