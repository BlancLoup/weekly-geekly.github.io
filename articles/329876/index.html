<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Limit the speed of processing requests in nginx</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Flickr user picture Wonderlane 


 NGINX is great! That's just his documentation on limiting the speed of processing requests seemed to me, how to say...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Limit the speed of processing requests in nginx</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/web/d6f/bca/708/d6fbca7088f14e0e92a724f5eabf62c0.jpeg"><br><p>  <em><a href="https://www.flickr.com/photos/wonderlane/5516928454/">Flickr</a> user picture Wonderlane</em> </p><br><p>  NGINX is great!  That's just his <a href="http_limit_req_module.html">documentation</a> on limiting the speed of processing requests seemed to me, how to say it, somewhat limited.  Therefore, I decided to write this guide on rate limiting and shaping traffic in NGINX. </p><a name="habracut"></a><br><p>  We are going to: </p><br><ul><li>  describe the NGINX directives, </li><li>  deal with accept / reject-logic of NGINX, </li><li>  Visualize the processing of traffic bursts in various settings. </li></ul><br><p>  In addition, I created a <a href="https://github.com/sportebois/nginx-rate-limit-sandbox">GitHub repository</a> and a <a href="https://hub.docker.com/r/sportebois/nginx-rate-limit-sandbox/">Docker image</a> with which you can experiment and reproduce the tests in this article.  It is always easier to learn by doing. </p><br><h2 id="direktivy-nginx-po-ogranicheniyu-skorosti-obrabotki-zaprosov">  NGINX directives on request rate limiting </h2><br><p> In this article we will talk about <a href="http_limit_req_module.html">ngx_http_limit_req_module</a> , which implements the directive <code>limit_req_zone</code> , <code>limit_req</code> , <code>limit_req_status</code> and <code>limit_req_level</code> .  They allow you to control the value of the HTTP request status code for rejected requests, as well as the logging of these failures. </p><br><p>  Most often confused precisely in the logic of the rejection of the request. </p><br><p>  First you need to deal with the directive <code>limit_req</code> , which requires the <code>zone</code> parameter.  It also has <strong>optional</strong> parameters <code>burst</code> and <code>nodelay</code> . </p><br><p>  The following concepts are used here: </p><br><ul><li><p>  <code>zone</code> defines a ‚Äúbucket‚Äù (bucket) - a shared space in which incoming requests are counted.  All requests that fall into one ‚Äúbucket‚Äù will be counted and processed in its section.  This achieves the ability to set restrictions based on URLs, IP addresses, etc. </p><br></li><li><p>  <code>burst</code> is an optional parameter.  When set, it determines the number of requests that can be processed in excess of the established basic speed limit.  It is important to understand that the <em><code>burst</code> is the absolute value of the number of requests, and not the speed</em> . </p><br></li><li>  <code>nodelay</code> is also an optional parameter that is used in conjunction with the <code>burst</code> .  Below we will understand why it is needed. </li></ul><br><h2 id="kakim-obrazom-nginx-prinimaet-reshenie-o-prinyatii-ili-otklonenii-zaprosa">  How does NGINX decide to accept or reject a request? </h2><br><p>  When setting up a zone, its speed is set.  For example, at <code>300r/m</code> 300 requests per minute will be received, and at <code>5r/s</code> - 5 requests per second. </p><br><p>  Examples of directives: </p><br><ul><li> <code>limit_req_zone $request_uri zone=zone1:10m rate=300r/m;</code> </li> <li> <code>limit_req_zone $request_uri zone=zone2:10m rate=5/s;</code> </li> </ul><br><p>  It is important to understand that these two zones have the same limits.  Using the <code>rate</code> parameter, NGINX calculates the frequency and, accordingly, the interval after which a new request can be received.  In this case, NGINX will use an algorithm called a leaky bucket (leaky bucket). </p><br><p>  For NGINX <code>300r/m</code> and <code>5r/s</code> same: it will skip one request every 0.2 s.  In this case, NGINX will set a flag every 0.2 seconds to allow reception of the request.  When a request arrives for this zone, NGINX clears the flag and processes the request.  If another request arrives, and the timer counting the time between packets has not yet been triggered, the request will be rejected with status code 503. If the time has elapsed and the flag has already been set to the enable value, no action will be performed. </p><br><h2 id="nuzhny-li-ogranichenie-skorosti-obrabotki-zaprosov-i-sheyping-trafika">  Do I need to limit the speed of processing requests and shaping traffic? </h2><br><p>  Let's talk about the parameter <code>burst</code> .  Imagine that the flag about which we spoke above can take values ‚Äã‚Äãgreater than one.  In this case, it will reflect the maximum number of requests that NGINX must skip within one burst. </p><br><p>  Now it is no longer a ‚Äúleaky bucket‚Äù, ‚Äútoken bucket‚Äù.  The <code>rate</code> parameter defines the time interval between requests, but we are dealing not with a true / false token, but with a counter from <code>0</code> to <code>1 + burst</code> .  The counter is incremented every time the calculated time interval passes (a timer is triggered), reaching a maximum value of <code>b+1</code> .  Let me remind you again: <code>burst</code> is the number of requests, not the speed of their transmission. </p><br><p>  When a new request arrives, NGINX checks the availability of the token (counter&gt; 0).  If the token is not available, the request is rejected.  Otherwise, the request is accepted and will be processed, and the token is considered consumed (the counter is reduced by one). </p><br><p>  Well, if there are unspent burst tokens, NGINX will accept the request.  <em>But when will he process it?</em> </p><br><p>  We set the limit to 5r / s, while NGINX will accept requests in excess of the norm, if there are available burst tokens, but postpone their processing in such a way as to withstand the set speed.  That is, <strong>these burst requests will be processed with some delay</strong> or will end on timeout. </p><br><p>  In other words, NGINX will not exceed the limit set for the zone, but will place additional requests in the queue and process them with some delay. </p><br><p>  Let's give a simple example: let's say we have a limit of <code>1r/s</code> and the <code>burst</code> is <code>3</code> .  What happens if NGINX receives 5 requests at once? </p><br><ul><li>  The first will be accepted and processed. </li><li>  Since no more than 1 + 3 is allowed, one request will be immediately denied with status code 503. </li><li>  The three remaining ones will be processed one after the other, but not instantly.  NGINX will skip them at a speed of <code>1r/s</code> , staying within the limits of the established limit, and also on the condition that no new requests will be received that also use a quota.  When the queue becomes empty, the burst counter begins to increase again (the marker basket starts to fill). </li></ul><br><p>  In the case of using NGINX as a proxy server, the services behind it will receive requests at a speed of <code>1r/s</code> and will not know anything about the traffic spikes smoothed by the proxy server. </p><br><p>  So, we have just configured the traffic shaping, applying delays to controlling the bursts of requests and aligning the data flow. </p><br><h3 id="nodelay">  nodelay </h3><br><p>  <code>nodelay</code> tells NGINX that it should receive packets within the window defined by the <code>burst</code> value and process them immediately (as well as regular requests). </p><br><p>  As a result, traffic surges will still reach the services located behind NGINX, but these surges will be limited to the <code>burst</code> value. </p><br><h2 id="vizualizaciya-ogranicheniy-skorosti-obrabotki-zaprosov">  Visualization of request processing speed limits </h2><br><p>  Since I believe that the practice helps a lot in memorizing anything, I made a small Docker image with NGINX on board.  There are configured resources for which various options for limiting the speed of query processing are implemented: with a basic limit, with a speed limit using <code>burst</code> , and also with <code>burst</code> and <code>nodelay</code> .  Let's see how they work. </p><br><p>  It uses a fairly simple NGINX configuration (it is also in the Docker image, a link to which can be found at the end of the article): </p><br><pre> <code class="nginx hljs"><span class="hljs-attribute"><span class="hljs-attribute">limit_req_zone</span></span> <span class="hljs-variable"><span class="hljs-variable">$request_uri</span></span> zone=by_uri:<span class="hljs-number"><span class="hljs-number">10m</span></span> rate=30r/m; <span class="hljs-section"><span class="hljs-section">server</span></span> { <span class="hljs-attribute"><span class="hljs-attribute">listen</span></span> <span class="hljs-number"><span class="hljs-number">80</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">location</span></span> /by-uri/burst0 { <span class="hljs-attribute"><span class="hljs-attribute">limit_req</span></span> zone=by_uri; <span class="hljs-attribute"><span class="hljs-attribute">try_files</span></span> <span class="hljs-variable"><span class="hljs-variable">$uri</span></span> /index.html; } <span class="hljs-attribute"><span class="hljs-attribute">location</span></span> /by-uri/burst5 { <span class="hljs-attribute"><span class="hljs-attribute">limit_req</span></span> zone=by_uri burst=<span class="hljs-number"><span class="hljs-number">5</span></span>; <span class="hljs-attribute"><span class="hljs-attribute">try_files</span></span> <span class="hljs-variable"><span class="hljs-variable">$uri</span></span> /index.html; } <span class="hljs-attribute"><span class="hljs-attribute">location</span></span> /by-uri/burst5_nodelay { <span class="hljs-attribute"><span class="hljs-attribute">limit_req</span></span> zone=by_uri burst=<span class="hljs-number"><span class="hljs-number">5</span></span> nodelay; <span class="hljs-attribute"><span class="hljs-attribute">try_files</span></span> <span class="hljs-variable"><span class="hljs-variable">$uri</span></span> /index.html; } }</code> </pre> <br><p>  <em>NGINX test configuration with various options for limiting the speed of processing requests</em> </p><br><p>  In all tests, using this configuration, we send 10 parallel requests simultaneously. </p><br><p>  Let's find out this: </p><br><ul><li>  How many requests will be denied due to speed limit? </li><li>  What is the speed of processing received requests? </li></ul><br><h3 id="delaem-10-parallelnyh-zaprosov-k-resursu-s-ogranicheniem-skorosti-obrabotki-zaprosov">  We make 10 parallel requests to the resource with the speed limit processing requests </h3><br><img src="https://habrastorage.org/web/632/aa8/d6a/632aa8d6a7b549c7b57633ddda623297.gif"><br><p>  <em>10 simultaneous requests to a resource with a request processing speed limit</em> </p><br><p>  In our configuration, allowed 30 requests per minute.  But in this case, 9 out of 10 will be rejected.  If you have carefully read the previous sections, this behavior of NGINX will not come as a surprise to you: <code>30r/m</code> means that only one request will pass in 2 seconds.  In our example, 10 requests come at the same time, one is skipped, and the other nine are rejected, since they are visible to NGINX before the timer that allows the next request is triggered. </p><br><h3 id="ya-perezhivu-nebolshie-vspleski-zaprosov-k-klientamkonechnym-tochkam">  I will survive small bursts of requests to clients / endpoints </h3><br><p>  Good!  Then we add the argument <code>burst=5</code> , which will allow NGINX to skip small bursts of requests to this end point of the zone, with a limit on the speed of processing requests: </p><br><img src="https://habrastorage.org/web/dff/a75/058/dffa750583e14a9eb37d8e2892bb0da0.gif"><br><p>  <em>10 simultaneous requests to the resource with the argument burst = 5</em> </p><br><p>  What happened here?  As was to be expected, 5 additional requests were accepted with the <code>burst</code> argument, and we improved the ratio of received requests to their total number from 1/10 to 6/10 (the rest were rejected).  Here you can clearly see how NGINX updates the token and processes received requests - outgoing speed is limited to <code>30r/m</code> , which equals one request every 2 seconds. </p><br><p>  The response to the first request returns in 0.2 seconds.  The timer is triggered after 2 seconds, one of the pending requests is processed, and the client receives a response.  The total time spent on the road there and back was 2.02 seconds.  After another 2 seconds, the timer works again, giving the opportunity to process the next request, which returns with a total travel time of 4.02 seconds.  And so on and so forth‚Ä¶ </p><br><p>  Thus, the <code>burst</code> argument turns the NGINX query processing speed limit system from a simple threshold filter into a traffic shaper. </p><br><h3 id="moy-server-vyderzhit-dopolnitelnuyu-nagruzku-no-ya-by-hotel-ispolzovat-ogranichenie-skorosti-obrabotki-zaprosov-dlya-predotvrascheniya-ego-peregruzki">  My server will handle the additional load, but I would like to use the request processing rate limit to prevent overloading. </h3><br><p>  In this case, the <code>nodelay</code> argument may be useful.  Let's send the same 10 requests to the endpoint with <code>burst=5 nodelay</code> : </p><br><img src="https://habrastorage.org/web/281/b16/341/281b163411a54eaab4b19fdb0c20761d.gif"><br><p>  <em>10 simultaneous requests to the resource with the argument burst = 5 nodelay</em> </p><br><p>  As expected with <code>burst=5</code> , we will have the same ratio of 200 and 503 state codes.  But outgoing speed is no longer limited to one request every 2 seconds.  As long as burst tokens are available, incoming requests will be received and immediately processed.  The speed of the timer is still important from the point of view of replenishing the number of burst tokens, but the delay does not apply to accepted requests. </p><br><p>  Comment.  In this case, <code>zone</code> uses <code>$request_uri</code> , but all subsequent tests work in the same way for the <code>binary_remote_addr</code> option, in which the speed is limited to the client‚Äôs IP address.  You will have the opportunity to play with these settings, using specially prepared Docker-image. </p><br><h2 id="podvedem-itogi">  Let's sum up </h2><br><p>  Let's try to visualize how NGINX accepts incoming requests and processes them based on the <code>rate</code> , <code>burst</code> and <code>nodelay</code> . </p><br><p>  In order not to complicate, let's display the number of incoming requests (which are then rejected or accepted and processed) on the time scale defined in the zone settings, divided into segments equal to the trigger value of the timer.  The absolute value of the time interval is not significant.  The number of requests that NGINX can process in each step is important. </p><br><p>  Here is the traffic that we will run through different settings for the speed of processing requests: </p><br><img src="https://habrastorage.org/web/27b/967/6ac/27b9676ac8d24e0e8fd80002ea7ea385.png"><br><p>  <em>Incoming requests and request processing speed limit set for the zone</em> </p><br><img src="https://habrastorage.org/web/c65/cb0/c54/c65cb0c5447043b3becbde53bc6aa63a.png"><br><p>  <em>Accepted and rejected requests (burst setting is not set)</em> </p><br><p>  Without <em>burst</em> (that is, with <code>burst=0</code> ) NGINX performs the function of a speed limiter.  Requests are either processed immediately or immediately rejected. </p><br><p>  If we want to allow small bursts of traffic, for example, in order to load capacity within the established limit, then we can add a <code>burst</code> argument, which implies a delay in processing requests received within the available burst tokens: </p><br><img src="https://habrastorage.org/web/9bb/31b/b62/9bb31bb62d1b425795ef1bffe82c6b3b.png"><br><p>  <em>Accepted, delayed and rejected requests (using burst)</em> </p><br><p>  We see that the total number of rejected requests has decreased.  Only those requests exceeding the established speed that came at the moments when there were no available burst tokens were rejected.  With these settings, NGINX performs full-fledged traffic shaping. </p><br><p>  Finally, NGINX can be used to control traffic by limiting the size of a burst of requests (burst), but at the same time bursts of requests will partially reach their handlers (upstream or local), which ultimately leads to less stable outgoing speed, but will improve network latency ( if you can, of course, process these additional requests): </p><br><img src="https://habrastorage.org/web/1fc/ce2/276/1fcce22762434b2ebbf43dc9f7f5bdf0.png"><br><p>  <em>Accepted, processed and rejected requests (burst used with nodelay)</em> </p><br><h2 id="poigrayte-s-ogranicheniem-skorosti-obrabotki-zaprosov">  Play with the speed limit processing requests </h2><br><p>  Now, in order to better consolidate the understanding of the concepts presented, you can study the code, copy the repository and experiment with the prepared Docker-image: </p><br><p>  <a href="https://github.com/sportebois/nginx-rate-limit-sandbox">https://github.com/sportebois/nginx-rate-limit-sandbox</a> . </p><br><p>  References: </p><br><ol><li>  Original: <a href="https://medium.freecodecamp.com/nginx-rate-limiting-in-a-nutshell-128fe9e0126c">NGINX rate-limiting in a nutshell</a> . </li></ol></div>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <p>Source: <a href="https://habr.com/ru/post/329876/">https://habr.com/ru/post/329876/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../329864/index.html">Chip for smart cameras ELISE - one of the most high-tech products in Russia in 2017. Developer fee and camera</a></li>
<li><a href="../329866/index.html">Playing with emotions: Microsoft Cognitive Services + Unity</a></li>
<li><a href="../329870/index.html">Kotlin and Swift. New era in mobile development?</a></li>
<li><a href="../329872/index.html">Deploying and maintaining Redmine, the right way</a></li>
<li><a href="../329874/index.html">Fifth generation of HPE MSA storage: twice the performance for the same price</a></li>
<li><a href="../329878/index.html">The largest Git repository in the world</a></li>
<li><a href="../329880/index.html">JaCarta PKI and OpenVPN for Windows</a></li>
<li><a href="../329882/index.html">9 reasons that prevent you from becoming a timlid</a></li>
<li><a href="../329884/index.html">Machine intelligence in a Gboard keyboard</a></li>
<li><a href="../329886/index.html">Delegates and Lambda Expressions in C # .Net - Cheat Sheet or Briefly</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>