<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Overview of deep convolutional neural network topologies</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="This will be a long post. I have long wanted to write this review, but I was ahead of sim0nsays , and I decided to wait a moment, for example, how the...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Overview of deep convolutional neural network topologies</h1><div class="post__text post__text-html js-mediator-article"><img src="https://habrastorage.org/files/040/6ca/59e/0406ca59e7c243e1bffae413d1d40947.png" align="right" width="320">  This will be a long post.  I have long wanted to write this review, but I was <a href="https://habrahabr.ru/users/sim0nsays/topics/">ahead of</a> <a href="https://habrahabr.ru/users/sim0nsays/" class="user_link">sim0nsays</a> , and I decided to wait a moment, for example, how the <a href="http://image-net.org/challenges/LSVRC/2016/results">results of ImageNet</a> will appear.  Now the moment has come, but imajnet has not brought any surprises, except that Chinese efesbeshniki are in the first place by classification.  Their model, in the <a href="http://mlwave.com/kaggle-ensembling-guide/">best traditions of CGL,</a> is an ensemble of several models (Inception, ResNet, Inception ResNet), and overtakes the past winners by only half a percent (by the way, there is no publication yet, and there is a miserable chance that there is really something new).  By the way, as you can see from the results of imajnet, something went wrong with the addition of layers, as evidenced by the growth in the width of the architecture of the final model.  Maybe from neural networks have already <a href="https://www.youtube.com/watch%3Fv%3DE1XMjIXWKtA">squeezed</a> all that is possible?  Or does NVidia <a href="https://www.overclockers.ru/hardnews/75464/superkompjuter-nvidia-dgx-1-na-gp100-v-kartinkah.html">raise the price of</a> the GPU too much and thus slows down the development of AI?  Is <a href="https://en.wikipedia.org/wiki/AI_winter">winter</a> close?  In general, I will not answer these questions here.  But under the cut you will find a lot of pictures, layers and dances with a tambourine.  It is implied that you are already familiar with the error back-propagation algorithm and understand how the basic building blocks of convolutional neural networks work: convolutions and pooling. <br><br><a name="habracut"></a><br><br><h4>  The transition from neurophysiology to computer vision </h4><br><img src="https://habrastorage.org/files/db8/383/ac0/db8383ac06ad4b92a6b106687773c79c.png" align="left" width="160">  The story should have been started with the pioneers of neural networks (not only artificial) and their contribution: the <a href="https://en.wikipedia.org/wiki/Artificial_neuron">McCulloch-Pitts</a> formal <a href="https://en.wikipedia.org/wiki/Artificial_neuron">neuron</a> model, <a href="https://en.wikipedia.org/wiki/Hebbian_theory">Hebb‚Äôs learning theory</a> , <a href="https://en.wikipedia.org/wiki/Perceptron">Rosenblatt perceptron</a> , <a href="https://en.wikipedia.org/wiki/Paul_Bach-y-Rita">Paul Bach-Rita‚Äôs</a> experiments and others, but I‚Äôll leave <a href="https://habrahabr.ru/company/mailru/blog/252965/">it to readers</a> for independent work.  So I propose to go straight to David Hubel and Torsten Wiesel, the Nobel laureates of 1981.  They received an award for work carried out in 1959 (at the same time, Rosenblatt put his experiments).  Formally, the prize was awarded for "work concerning the principles of information processing in neural structures and mechanisms of brain activity."  It is necessary to immediately warn sensitive readers: then an <a href="https://www.youtube.com/watch%3Fv%3DIOHayh06LJ4">experiment with cats</a> will be described.  The model of the experiment is depicted in the figure below: a bright elongated moving rectangle is shown to the cat on a dark screen at various angles;  The oscilloscope electrode is connected to the occipital part of the brain, where mammals have a visual information processing center.  In the course of the experiment, scientists observed the following effects (you can easily find analogies with modern convolutional networks and recurrent networks): 
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
    <ul><li>  certain areas of the visual cortex are activated only when the line is projected onto a certain part of the retina; </li><li>  the level of activity of the neurons in the area changes as the angle of inclination of the rectangle changes; </li><li>  Some areas are activated only when the object moves in a certain direction. </li></ul><br><br><img src="https://habrastorage.org/files/fcb/f4c/fc6/fcbf4cfc6c0248f9a88e8889627ce5e1.gif"><br><br>  One of the results of the study was a model of the visual system, or topographic map, with the following properties: <br><br><ul><li>  neighboring neurons process signals from neighboring areas of the retina; </li><li>  neurons form a hierarchical structure (image below), where each next level highlights more and more high-level signs (today <a href="https://habrahabr.ru/company/mailru/blog/306916/">we already know how to effectively manipulate these signs</a> ); </li><li>  neurons are organized into so-called columns - computational blocks that transform and transmit information from level to level. </li></ul><br><br><img src="https://habrastorage.org/files/06b/e81/b3a/06be81b3a9e444ba97b5590823912495.png" width="480"><br><br>  The first who tried to put the ideas of Hubel and Wiesel into the program code was Kunihiko Fukushima, who offered two models from 1975 to 1980: the <a href="http://sci-hub.cc/10.1007/bf00342633">cognitron</a> and the <a href="http://sci-hub.cc/10.1007/bf00344251">neocognitron</a> .  These models almost repeated the <a href="http://cns-alumni.bu.edu/~slehar/webstuff/pcave/hubel.html">biological model</a> , today we call simple cells (simple cells) <b>convolutions</b> , and complex cells (complex cells) <b>pooling</b> : these are still the basic building blocks of modern convolutional neural networks.  The model was not taught by the error back-propagation algorithm, but by the original heuristic algorithm in the mode without a teacher.  We can assume that this work was the beginning of neural network computer vision. <br><br><h4>  <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Document recognition</a> (1998) </h4><br><br>  After many years, 1998 came.  Winter has passed.  <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Jan LeKunn</a> , who has long been a postdoc of <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">one of the authors of an</a> article on the <a href="https://en.wikipedia.org/wiki/Yann_LeCun">backpropagation</a> algorithm, publishes a paper (in collaboration with other luminaries of neural networks), in which he mixes ideas of convolutions and pulling with backprop, eventually getting the first working convolutional neural network.  It was introduced into the US mail for index recognition.  This architecture was a standard template for building convolutional networks until recently: convolution alternates with pooling several times, then several fully connected layers.  Such a network consists of 60 thousand parameters.  The main building blocks are convolutions of 5 √ó 5 with a shift of 1 and 2 √ó 2 of shift with a shift of 2. As you already know, convolutions play the role of feature detectors, and pooling (or subsampling) is used to reduce the dimension by exploiting the fact that the <i>images have the property local correlation of</i> pixels - neighboring pixels, as a rule, do not differ much from each other.  Thus, if you get any aggregate from several neighbors, then the loss of information will be insignificant. <br><br><img src="https://habrastorage.org/files/b4d/075/b91/b4d075b91dcc419da0edd52f582f52a1.png"><br><br><h4>  <a href="http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a> (2012) </h4><br><br>  It took another 14 years.  Alex Krizhevsky from the same laboratory where LeKun was a postdoc added the last ingredients to the formula.  Deep learning = model + learning theory + big data + <i>iron</i> .  GPU allowed to significantly increase the number of trained parameters.  The model contains 60 million parameters, three orders of magnitude more; two graphic accelerators were used to train such a model. <br><br><img src="https://habrastorage.org/files/4ba/dec/aa0/4badecaa0ae0422fb0d1f6c4ab4ce28e.png"><br><br><div class="spoiler">  <b class="spoiler_title">Other images of AlexNet</b> <div class="spoiler_text">  Data exchange between GPUs <br><img src="https://habrastorage.org/files/24e/7ab/379/24e7ab379abf49e4941bf7a90c0e32a7.png"><br><br>  Sizes of layers <br><img src="https://habrastorage.org/files/73a/e24/7ba/73ae247bac2e4f4ea2b54a1e49dba79b.png"><br></div></div><br><br>  In terms of network topology, this is almost the same LeNet, just magnified a thousand times.  Several more convolutional layers were added, and the size of the convolution kernels decreases from the input of the network to the output.  This is explained by the fact that at the beginning the pixels are highly correlated, and the receptor area can be safely taken large, we still lose little information.  Next, we apply pooling, <i>thereby increasing the density of uncorrelated plots</i> .  At the next level, it is logical to take a slightly smaller receptor area.  As a result, the authors turned out such a pyramid of bundles of 11 √ó 11 -&gt; 5 √ó 5 -&gt; 3 √ó 3 ... <br><br>  Other tricks have also been applied to avoid retraining, and some of them are now standard for deep networks: <a href="https://arxiv.org/pdf/1207.0580v1.pdf">DropOut</a> (RIP), Data Augmentation, and <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">ReLu</a> .  We will not focus on these tricks, focus on the topology of the model.  I will only add that since 2012, no neural network models have won the Imagnet anymore. <br><br><img src="https://habrastorage.org/files/9f5/b61/ce4/9f5b61ce4fbe4556bfe9f1cca2518493.png"><br><br><h4>  <a href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition</a> (12 Apr 2014) </h4><br><br>  This year there were two interesting articles, this one and Google Inception, which we will discuss below.  The work of the Oxford Laboratory is the last work adhering to the topology pattern laid down by LeCun.  Their VGG-19 model consists of 144 million parameters and adds to the architecture, in addition to 84 million parameters, another simple idea.  Take for example the convolution of 5 √ó 5, this mapping <img src="https://habrastorage.org/files/3b2/898/810/3b28988106804d2690fb4f7ed32143a0.gif">  It contains 25 parameters.  If we replace it with a stack of two layers with convolutions of 3 √ó 3, then we get the same display, but the number of parameters will be less: 3 √ó 3 + 3 √ó 3 = 18, which is 22% less.  If we replace 11 √ó 11 by four 3 √ó 3 convolutions, then this is already 70% less than the parameters. <br><br><img src="https://habrastorage.org/files/e5f/534/577/e5f5345777ef4596add2f46d93731984.png" width="360"><br><br><div class="spoiler">  <b class="spoiler_title">VGG- * Models</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/103/a73/1d4/103a731d40f945509553b26b2a603ebf.png"><br><br>  <a href="http://ethereon.github.io/netscope/">VGG-16 flowing graph</a> <br></div></div><br><br><h4>  <a href="https://arxiv.org/pdf/1312.4400v3.pdf">Network in Network</a> (4 Mar 2014) </h4><br><br><img src="https://habrastorage.org/files/201/b9b/691/201b9b6914614fb5a436211009c6e60b.png"><br><br>  And here begin dancing with a tambourine.  Such a picture was illustrated by the author of the publication of his blog post, where he spoke about ascaded ross han Channel Parameteric pooling.  Consider the main ideas of this article.  Obviously, the convolution operation is a linear transformation of the image patches, then the convolutional layer is a generalized linear model (GLM).  It is assumed that the images are linearly separable.  But why does CNN work?  Everything is simple: a redundant representation is used (a large number of filters) to take into account all variations of one image in the feature space.  The more filters on one layer, the more variations you need to take into account the next layer.  What to do?  Let's replace GLM with something <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D1%2580%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25B4%25D1%2586%25D0%25B0%25D1%2582%25D0%25B0%25D1%258F_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2593%25D0%25B8%25D0%25BB%25D1%258C%25D0%25B1%25D0%25B5%25D1%2580%25D1%2582%25D0%25B0">more efficient,</a> and the <a href="https://ru.wikipedia.org/wiki/%25D0%25A2%25D1%2580%25D0%25B8%25D0%25BD%25D0%25B0%25D0%25B4%25D1%2586%25D0%25B0%25D1%2582%25D0%25B0%25D1%258F_%25D0%25BF%25D1%2580%25D0%25BE%25D0%25B1%25D0%25BB%25D0%25B5%25D0%25BC%25D0%25B0_%25D0%2593%25D0%25B8%25D0%25BB%25D1%258C%25D0%25B1%25D0%25B5%25D1%2580%25D1%2582%25D0%25B0">most effective one is the single-layer perceptron</a> .  This will allow us to more effectively divide the space of signs, thereby reducing their number. <br><br><img src="https://habrastorage.org/files/fcb/e94/59e/fcbe9459e2064f598478ec00544e8cf5.png"><br><br>  I am sure that you have already begun to doubt the expediency of such a decision, we will indeed reduce the number of signs, but significantly increase the number of parameters.  The authors say that the USSR will solve this problem.  It works as follows: at the exit of the convolutional layer, we get a cube of size W √ó H √ó D. For each position (w, h) take all the values ‚Äã‚Äãon D, Cross Channel, and calculate the linear combination, Parametric pooling, and so we will do several times, thereby creating a new volume, and so several times - Cascade.  It turns out that these are simply 1 √ó 1 convolutions with subsequent nonlinearity.  This trick is ideologically the same as in the VGG article about 3 √ó 3 convolutions. It turns out that we first propose replacing ordinary convolution with MLP, but since MLP is very expensive, we replace every fully connected layer with a convolutional layer - this is a Singapore trick .  It turns out that NIN is a deep convolutional neural network, and instead of convolutions there are small convolutional neural networks with a 1 √ó 1 core. In general, while they still did not know which genie they had released. <br><br><img src="https://habrastorage.org/files/7ad/1e3/f17/7ad1e3f170114d6ab454f40d7c408adb.png"><br><br>  The next idea, even more interesting, is to completely abandon the fully connected layers - global average pooling.  Such a network is called a fully convolutional network, since it no longer requires any specific image size to enter and consists only of bundles / pooling.  Suppose there is a classification task into N classes, then instead of fully connected layers, a 1 √ó 1 convolution is used to make a W √ó H √ó N cube from the W √ó H √ó D cube, that is, the 1 √ó 1 convolution can be changed arbitrarily to the depth of the cube signs.  When we have N W √ó H dies, we can calculate the average value of the dice and calculate softmax.  A slightly deeper idea is contained in the authors' claim that earlier convolutional layers acted simply as mechanisms for extracting features, and discriminators were fully connected layers (from there such a pattern, given by LeCunn).  The authors argue that convolutions are discriminators, too, since the network consists only of convolutions and solves the problem of classification.  It is worth noting that in LeNet convolutional networks, 80% of the calculations are on convolutional layers, and 80% of memory consumption is on fully connected. <br><br><img src="https://habrastorage.org/files/58c/0c9/c5f/58c0c9c5fa554378bf87a9cc373e5c4d.png"><br><br>  Later, <a href="https://www.facebook.com/yann.lecun/posts/10152820758292143">LeKun will write</a> on his Facebook: <br><br><blockquote>  In Convolutional Nets, there is no such thing as ‚Äúfully-connected layers‚Äù.  There are only convolution layers with 1 √ó 1 convolution kernels and a full connection table. </blockquote><br><br>  The only pity is that the authors who came up with so many new ideas did not participate in imagnet.  On the other hand, Google jumped in on time and used these ideas in the next publication.  As a result, Google shared the prizes in 2014 with a team from Oxford. <br><br><h4>  <a href="https://arxiv.org/pdf/1409.4842v1.pdf">Going Deeper with Convolutions</a> (17 Sep 2014) </h4><br><br><img src="https://habrastorage.org/files/f5e/67f/f42/f5e67ff4286646b3a6ecf5407f9466f2.png"><br><br>  Google comes into play, they called their network Inception, the name was not chosen by chance, it continues the ideas of previous work about the ‚ÄúNetwork within the Network‚Äù, as well as the well-known meme.  Here is what the authors write: <br><br><blockquote>  In this paper, we‚Äôve been able to follow the guidelines of the computer network. internet meme [1].  In the case of the ‚ÄúInception module‚Äù depth.  In general, it is possible to take a look at the theoretical work of Arora et al [2]. </blockquote><br><br>  An arbitrary increase in the width (the number of neurons in the layers) and the depth (the number of layers) has several disadvantages.  First, an increase in the number of parameters contributes to retraining, and an increase in the number of layers also adds to the problem of gradient decay.  By the way, the last statement will be resolved by ResNet, which will be discussed further.  Secondly, an increase in the number of convolutions in a layer leads to a quadratic increase in computations in this layer.  If new model parameters are used inefficiently, for example, many of them become close to zero, then we just waste our computing power.  Despite these problems, the first author of the article wanted to experiment with deep networks, but with a significantly smaller number of parameters.  To do this, he turned to the article ‚Äú <a href="http://jmlr.org/proceedings/papers/v32/arora14.pdf">Provable Bounds for Learning Some Deep Representations</a> ‚Äù, in which they prove that if the probabilistic distribution of data can be represented as a sparse, deep and wide neural network, then an optimal neural network can be constructed for a given dataset by analyzing the correlations of neurons the previous layer and combining the correlated neurons into groups that will be the neurons of the next layer.  Thus, the neurons of the later layers "look" at the highly intersecting areas of the original image.  Let me remind you how the receptor regions of neurons look at different levels and what signs they extract. <br><br><div class="spoiler">  <b class="spoiler_title">the receptor region of the layer conv1_2 of the network VGG-19</b> <div class="spoiler_text">  Yes, you see almost nothing, since the receptive area is very small, this is the second convolution 3 √ó 3, respectively, the general area is 5 √ó 5. But, having increased, we will see that the feature is just a gradient detector. <br><br><img src="https://habrastorage.org/files/238/731/329/2387313294474789844cb17d7119f543.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">the receptor region of the conv3_3 layer of the VGG-19 network</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/46d/a98/a38/46da98a38c694047a578801ec9e1d8ba.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">the receptor region of the conv4_3 layer of the VGG-19 network</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/7d8/5e7/995/7d85e7995c314c868d5b3176df3cc4ba.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">receptor region of the convG_3 layer of the VGG-19 network</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/d5e/a53/6da/d5ea536da32f4df38719138b63455371.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">receptor region of the pool5 layer of the VGG-19 network</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/424/229/deb/424229deb389451581579a7da42b1bdf.png"><br></div></div><br><br>  In the early layers (closer to the entrance), the correlated neurons will be concentrated in local areas.  This means that if several neurons in one coordinate (w, h) on a plate can learn about the same thing, then in the tensor after the first layer of their activation they will be located in a small region at a certain point (w, h), and will be along the dimension of the filter bank - D. Like this: <br><br><img src="https://habrastorage.org/files/522/2b4/ec6/5222b4ec64794e708805c35d7bc95fb2.png" width="160"><br><br>  How, then, to catch such correlations and turn them into one sign?  The idea of ‚Äã‚Äãa 1 √ó 1 bundle from a previous paper comes to the rescue.  Continuing this idea, it can be assumed that a slightly smaller number of correlated clusters will be slightly larger, for example, 3 √ó 3. The same is true for 5 √ó 5, etc., but Google decided to stop at 5 √ó 5. <br><br><div class="spoiler">  <b class="spoiler_title">convolutions 1 √ó 1</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/89e/0ed/d85/89e0edd85e25439ea312c0ca14526cdd.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">convolutions 3 √ó 3</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/08c/cc6/81a/08ccc681ac564e8fbb7582c600fc2c34.png"><br></div></div><br><br><div class="spoiler">  <b class="spoiler_title">convolutions 5 √ó 5</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/5cb/728/35f/5cb72835f8a44ab0b78831798d6831d2.png"><br></div></div><br><br>  After calculating the feature maps for each convolution, it is necessary to aggregate them in some way, for example, to concatenate filters. <br><br><div class="spoiler">  <b class="spoiler_title">concatenation of signs</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/3a2/418/08f/3a241808f91841e48179f3d0318e4002.png"><br></div></div><br><br>  By the way, in order not to lose the original image from the previous layer, in addition to convolutions, pooling signs are added (we remember that the pooling operation does not entail large losses of information).  Anyway, everybody uses pooling - there is no reason to refuse it.  The whole group of several operations is called an Inception block. <br><br><blockquote>  In addition, since it‚Äôs not possible, then. </blockquote><br><br><img src="https://habrastorage.org/files/31c/247/e82/31c247e82d57421cb4ed3822c91ec350.png"><br><br>  In order to equalize the size of the output tensors, it is also proposed to use a 1 √ó 1 convolution;  In addition, 1 √ó 1 convolutions are also used to reduce the dimension before energy-consuming convolution operations.  As you can see, Google uses 1 √ó 1 convolutions to achieve these goals (the next generation of networks will also exploit these techniques): <br><ul><li>  increasing the dimension (after surgery); </li><li>  reduce the dimension (before the operation); </li><li>  grouping of correlated values ‚Äã‚Äã(the first operation in the block). </li></ul><br><br>  The final model is as follows: <br><br><img src="https://habrastorage.org/files/547/d42/bd4/547d42bd4ecf47d2ba9a7e41bc083e2f.png"><br><br>  In addition to the above, you may notice several additional classifiers at different levels.  The initial idea was that such classifiers would ‚Äúpush‚Äù the gradients to the early layers and thereby reduce the gradient damping effect.  Later, Google will refuse them.  The network itself gradually increases the depth of the tensor and reduces the spatial dimension.  At the end of the article, the authors leave open the question of the effectiveness of such a model, and also hint that they will investigate the possibility of automatically generating network topologies using the above principles. <br><br><h4>  <a href="https://arxiv.org/pdf/1512.00567v3.pdf">Rethinking the Inception Architecture for Computer Vision</a> (Dec 11, 2015) </h4><br><br>  A year later, Google was well prepared for imagnet, rethought the architecture of the insectron, but lost a completely new model, which will be discussed in the next part.  In the new article, the authors explored various architectures in practice and developed four principles for constructing deep convolutional neural networks for computer vision: <br><br><ul><li>  Avoid representational bottlenecks: you should not drastically reduce the dimension of data representation, it should be done smoothly from the beginning of the network to the classifier at the output. </li><li>  High-dimensional representations should be processed locally, increasing the dimension: it is not enough to smoothly reduce the dimension, you should use the principles described in the previous article for the analysis and grouping of correlated segments. </li><li>  Spatial reconciliations can and should be factored into even smaller ones: this will save resources and allow them to increase the size of the network. </li><li>  It is necessary to observe a balance between the depth and width of the network: you should not dramatically increase the depth of the network separately from the width, and vice versa;  you should evenly increase or decrease both dimensions. </li></ul><br><br>  Remember the idea of ‚Äã‚ÄãVGG in 2014, that larger convolutions can be factorized into 3 √ó 3 bundles?  So, Google went even further and factorized all convolutions into N √ó 1 and 1 √ó N. <br><br><img src="https://habrastorage.org/files/24b/60c/f97/24b60cf97509401fa323a5fdd1115172.png" width="360"><br><br><div class="spoiler">  <b class="spoiler_title">Inception block models</b> <div class="spoiler_text">  The first is original. <br><img src="https://habrastorage.org/files/339/7d9/716/3397d97168bd461d9e2ab55688f3adca.png" width="480"><br><br>  First factorized by the principle of VGG. <br><img src="https://habrastorage.org/files/b2d/59d/6cc/b2d59d6cc02b4335a03e2a9e384bfd01.png" width="480"><br><br>  New block model. <br><img src="https://habrastorage.org/files/722/eb3/4a6/722eb34a66c94419ace340b2532fa6e8.png" width="480"><br></div></div><br><br>  As for the bottlenecks, the following scheme is proposed.  Suppose if the input dimension <img src="https://habrastorage.org/files/ea1/c08/800/ea1c088000124618b8f351fde6b14213.gif">  , and we want to get <img src="https://habrastorage.org/files/b90/fdd/0f7/b90fdd0f70544c53a9aca487698b6315.gif">  then we first apply convolution of <img src="https://habrastorage.org/files/180/051/f19/180051f1935c4c5ba46a83f9b98d016a.gif">  filters with step 1 and then affairs pooling.  The total complexity of such an operation <img src="https://habrastorage.org/files/513/839/22c/51383922c39847b59bd7947ae1a12a44.gif">  .  If you do the pooling first, and then the convolution, then the complexity will fall to <img src="https://habrastorage.org/files/ab8/e4c/c0e/ab8e4cc0e2634fe1b9b0b44f5fb27113.gif">  but then the first principle will be violated.  It is proposed to increase the number of parallel branches, to do convolutions with step 2, but at the same time to increase the number of channels twice, then the representative power of the view decreases ‚Äúsmoother‚Äù.  And for manipulating the depth of the tensor, convolutions of 1 √ó 1 are used. <br><br><img src="https://habrastorage.org/files/a73/baf/8fc/a73baf8fc5044117a0f09f213523bc6c.png" width="480"><br><br>  The new model is called Inception V2, and if you add <a href="https://arxiv.org/pdf/1502.03167v3.pdf">batch normalization</a> , then there <a href="http://josephpcohen.com/w/wp-content/uploads/inception-v3.pdf">will be Inception V3</a> .  By the way, additional classifiers were removed, since it turned out that they do not particularly increase the quality, but can act as a regularizer.  But by this time there were already more interesting ways of regularization. <br><br><h4>  <a href="https://arxiv.org/pdf/1512.03385v1.pdf">Deep Residual Learning for Image Recognition</a> (10 Dec 2015) </h4><br><br>  The time has come to study the work of the Chinese division of Microsoft Research, which Google lost in 2015.  It has long been noted that if one simply drains more layers, the quality of such a model grows to a certain limit (see VGG-19), and then begins to fall.  This problem is called the degradation problem, and the networks obtained by running off more layers are plain, or flat networks.  The authors were able to find such a topology in which the quality of the model grows with the addition of new layers. <br><br><img src="https://habrastorage.org/files/604/2aa/2a9/6042aa2a9619420ca623f3d75bad5dff.png"><br><br>  This is achieved by a simple, at first glance, trick, although it leads to unexpected mathematical results.  Thanks to Arnold and Kolmogorov, we and they know that a neural network can approximate almost any function, for example, some complex function <img src="https://habrastorage.org/files/579/54a/dc9/57954adc93b14f289f001a91a5c9a472.gif">  .  Then it is fair that such a network will easily learn the residual-function (in Russian we will call it a residual function): <img src="https://habrastorage.org/files/a76/483/de2/a76483de26334e03b557ee4b20897693.gif">  .  Obviously, our initial objective function will be equal to <img src="https://habrastorage.org/files/ade/4d0/aed/ade4d0aed8214b12b3edaa1613723f15.gif">  .  If we take a certain network, for example, VGG-19, and we attach twenty layers to it, we would like the deep network to behave at least as good as its shallow analogue.  The problem of degradation implies that a complex non-linear function <img src="https://habrastorage.org/files/b6f/b5b/ed6/b6fb5bed6bbf4088817e5bb574d650f2.gif">  , obtained by the flow of several layers, must learn the identical transformation, if the previous layers had reached the limit of quality.  But this does not happen for some reason, perhaps the optimizer simply does not cope in order to adjust the weights so that the complex nonlinear hierarchical model does the identical transformation.  And what if we help her add a shortcut-connection, and it may be easier for the optimizer to make all the weights close to zero, rather than creating an identical transformation.  Surely this reminds you of the ideas of boosting. <br><br><img src="https://habrastorage.org/files/036/377/10c/03637710c63249888426728a84acd083.png"><br><br><div class="spoiler">  <b class="spoiler_title">An example of converting a flat network into a residual network</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/d07/1ab/a6d/d071aba6d8424e01948467fa641e7f37.png"><br></div></div><br><br>  Details of the construction of <a href="https://github.com/Lasagne/Recipes/blob/master/examples/resnet50/ImageNet%2520Pretrained%2520Network%2520(ResNet-50).ipynb">ResNet-52 at theano / lasagne are described here</a> , but here we only mention that 1 √ó 1 convolutions are exploited to increase and decrease the dimension, as bequeathed by Google.  The network is fully convolution. <br><br><img src="https://habrastorage.org/files/222/4f5/d7b/2224f5d7b305474f8c985ca5ce068563.png" width="480"><br><br>  As one of the substantiations of their hypothesis that the residual functions will be close to zero, the authors present a graph of the activation of residual functions depending on the layer and for comparing the activation of layers in flat networks. <br><br><img src="https://habrastorage.org/files/f78/3d8/0f3/f783d80f3a374c94b152dd7670fbe456.png" width="480"><br><br>  And the conclusions from all this are approximately as follows: <br><br><img src="https://habrastorage.org/files/26c/af9/6a2/26caf96a21e74a96a23b9be044109c84.png" width="480"><br><br><div class="spoiler">  <b class="spoiler_title">Compare the depth of ResNet and all previous networks</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/a0c/4d0/d32/a0c4d0d328184fab9a6750a8ab716e13.png"><br><img src="https://habrastorage.org/files/e2c/208/788/e2c20878809447f48f20b9977a342c57.png"><br></div></div><br><br>  Another interesting slide shows the authors that they have made a breakthrough in the depth / quality of networks. ,      ,   ,  19- VGG,   152 . <br><br><img src="https://habrastorage.org/files/99c/cb4/a51/99ccb4a516df4409b77d7b3b6144832c.png" width="480"><br><br>       ,  .       ,    2016    ,         .        . <br><br>    ResNet: <br><ul><li> <a href="http://josephpcohen.com/w/wp-content/uploads/resnet-28-small.pdf">ResNet-56</a> </li><li> <a href="http://ethereon.github.io/netscope/">ResNet-50</a> </li></ul><br><br><h4> <a href="http://arxiv.org/pdf/1602.07261v1.pdf">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</a> (23 Feb 2016) </h4><br><br>      ,    ,        Inception,    ResNet'.   ,  ,    ,     .      ,     ,       .     Inception V4  Inception ResNet. <br><br> Inception V4      ,        Inception-.  ,  ,  4     ,    ,    ‚Äî   ,      . <br><br><div class="spoiler"> <b class="spoiler_title">Inception V4</b> <div class="spoiler_text"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">The general scheme of the network. </font></font><br><img src="https://habrastorage.org/files/ae0/960/247/ae09602479ba40cfa4853c1063d96cd8.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Stem-block. </font></font><br><img src="https://habrastorage.org/files/f82/17b/8d0/f8217b8d0a9b458a8f28011c7bf769be.png"><br><br><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Inception block A.</font></font><br><img src="https://habrastorage.org/files/05c/444/b81/05c444b8195640e298a52b4e7504451b.png"><br></div></div><br><br>      Inception V4  Inception-  ,   shortcut-,   Inception ResNet. <br><br><img src="https://habrastorage.org/files/0b7/f70/4ef/0b7f704efcbc4cdf9f4b0584685273e4.png" width="480"><br><br><div class="spoiler"> <b class="spoiler_title"> ,  ,    ,  .</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/637/baf/49c/637baf49cf7d478699860e4a158b4c07.png"><br></div></div><br><br><h4> <a href="https://arxiv.org/pdf/1603.05027v2.pdf">Identity Mappings in Deep Residual Networks</a> (12 Apr 2016) </h4><br><br>        ResNet,   ,       :     .  ,              .             .          ,       ;      ,       .      ,   1001 . ,   2014  19-    ¬´very deep convolutional neural networks¬ª. <br><br><img src="https://habrastorage.org/files/1f0/269/342/1f026934219c4ce68e87a1075a833ea8.png"><br><br>        .  Let be <br><ul><li><img src="https://habrastorage.org/files/4d2/b53/69d/4d2b5369d1cf4a0a88da845df0dffaaa.gif"> ‚Äî   ; </li><li><img src="https://habrastorage.org/files/b11/e4f/7a1/b11e4f7a18d7457da572011fd4936883.gif"> ‚Äî shortcut connection,        <img src="https://habrastorage.org/files/cc5/7ea/f45/cc57eaf454de47c2b6f18869125cf5f7.gif">  ; </li><li><img src="https://habrastorage.org/files/88e/46e/561/88e46e56126c4aee80b9dad3c744179a.gif"> ‚Äî residual network; </li><li><img src="https://habrastorage.org/files/e6e/818/801/e6e8188018cb4cc3a7c473939117c054.gif"> ‚Äî     ; </li><li><img src="https://habrastorage.org/files/0ab/eda/32f/0abeda32fe6f49318a00fa4af9a1bdf8.gif"> ‚Äî            . </li></ul><br><br>     ,   <img src="https://habrastorage.org/files/7f4/fad/39e/7f4fad39e2fb44c9b88dfeb3f547a268.gif"> ‚Äî    ,    .     <b><i>K</i></b> ,             . ,                .      -  <b><i>k</i></b> ,  ,       :          .        ,          - .   ,          ,    vanishing gradients, ,   <img src="https://habrastorage.org/files/2f3/fba/aef/2f3fbaaefecc463c859bc82f2a39e41c.png" width="100">   ‚Äì1.  ,    .  ,         . <br><br><img src="https://habrastorage.org/files/fd1/cb1/e6d/fd1cb1e6db264c1ba6c653ec83f0db18.png" width="480"><br><br>   ,  -         ?    ,  <img src="https://habrastorage.org/files/50a/3dd/a6f/50a3dda6f81e4b5a9e2f5ce7587e7623.gif">  .       .    ,         .     ,      .          . <br><br>        residual-    ,     ,      ,  . <br><br><div class="spoiler"> <b class="spoiler_title">  </b> <div class="spoiler_text"><img src="https://habrastorage.org/files/c78/e75/dad/c78e75dadfc44f7c9a0f3af70f6d74a9.png" width="480"><br><br><img src="https://habrastorage.org/files/bc6/658/e85/bc6658e857464087933bc1c40034d3a2.png" width="480"><br></div></div><br><br>        ‚Äî   ( <a href="https://en.wikipedia.org/wiki/J%25C3%25BCrgen_Schmidhuber">J√ºrgen Schmidhuber</a> ).      <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM-</a> ,    1997 ,    ,  ResNet    ‚Äî    ,   LSTM,                1.  ,            ‚Äî <a href="https://arxiv.org/pdf/1505.00387v2.pdf">Highway Network</a> . , ,   ,    (gate),      ,     . <br><br><img src="https://habrastorage.org/files/3c2/2e2/381/3c22e238178e473797fe64a357c08550.jpg" width="360"><br><br><h4> <a href="https://arxiv.org/pdf/1604.03640.pdf">Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex</a> (13 Apr 2016) </h4><br><br>     ,     ResNet   RNN.        .  ,       ,      , ,   ,        ,   . <br><br><blockquote> The dark secret of Deep Networks: trying to imitate Recurrent Shallow Networks? <br><br> A radical conjecture would be: the effectiveness of most of the deep feedforward neural networks, including but not limited to ResNet, can be attributed to their ability to approximate recurrent computations that are prevalent in most tasks with larger t than shallow feedforward networks. This may offer a new perspective on the theoretical pursuit of the long-standing question ‚Äúwhy is deep better than shallow‚Äù. <br></blockquote><br><br><h4> <a href="https://arxiv.org/pdf/1605.06431v1.pdf">Residual Networks are Exponential Ensembles of Relatively Shallow Networks</a> (20 May 2016) </h4><br><br>  I'm sure almost all of you are familiar with <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">DropOut, a learning strategy</a> that until recently was the standard for learning deep networks.  It was shown that such a strategy is equivalent to learning an exponentially large number of models and averaging the result of forecasting.  It turns out the construction of the ensemble learning algorithm.  The authors of this article show that ResNet is an ensemble by design.  They show it with a series of interesting experiments.  First, let's take a look at the expanded version of ResNet, which is obtained if we count two consecutive shortcut connections as one edge of a flowing graph. <br><br><img src="https://habrastorage.org/files/940/1a6/2f1/9401a62f169d4af2b7fe62995d08152f.png"><br><br>  You can check this by simply decomposing the formula of the last layer: <br><img src="https://habrastorage.org/files/d54/979/e58/d54979e58e8c42e893dd809b7ac3d39e.png" width="480"><br><br>  As you can see, the network has grown in another dimension, in addition to the width and depth.  The authors say that this is a new third dimension, and they call it multiplicity, in Russian, probably, the closest will be - diversity.  It turns out that thanks to Arnold and Kolmogorov, we know that the network needs to grow in width (proven).  Thanks to Hinton, we know that adding layers improves the lower variational limit of the likelihood of a model, in general, is also good (proven).  And now a new dimension is proposed, that models need to be made diverse, although it is still worth proving. <br><br>  While there is no evidence, the authors offer the following arguments.  After training, in prediction mode, we will remove several layers from the network.  It is obvious that all networks before ResNet after such an operation will simply stop working. <br><br><img src="https://habrastorage.org/files/9cd/a25/548/9cda25548300463584096c14f3e7b736.png"><br><br>  It turns out that the prediction quality of ResNet gradually deteriorates as the number of deleted layers increases.  For example, if you remove half of the layers, the quality drops not so dramatically. <br><br><img src="https://habrastorage.org/files/be2/148/6b5/be21486b5e4341eeac7ab5610651c727.png"><br><br>  It seems that for the sake of trolling, the authors remove several layers from the VGG and from ResNet and compare the quality. <br><br><div class="spoiler">  <b class="spoiler_title">The results are amazing.</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/bf1/aa0/800/bf1aa08001d044b5a44a39408c78ccf0.png"><br></div></div><br><br>  In the next experiment, the authors build the distribution of the absolute values ‚Äã‚Äãof the gradient obtained from subnets of different lengths.  It turns out that for ResNet-50, the effective subnet length is from 5 to 17 layers, they account for almost the entire contribution to the training of the entire model, although they occupy only 45% of all path lengths. <br><br><div class="spoiler">  <b class="spoiler_title">Analysis of the distribution of gradients.</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/deb/928/073/deb9280732a54c5cb0cafad808087bb4.png"><br></div></div><br><br><h4>  Conclusion </h4><br><br>  ResNet continues to actively develop, and various groups both try something new and apply proven patterns from the past.  For example, create ResNet in ResNet. <br><br><ul><li>  <a href="https://arxiv.org/pdf/1606.05262v3.pdf">Convolutional Residual Memory Networks</a> (14 Jul 2016) </li><li>  <a href="https://arxiv.org/pdf/1605.07146v1.pdf">Wide Residual Networks</a> (May 23, 2016) </li><li>  <a href="http://users.cecs.anu.edu.au/~sgould/papers/dicta16-depthdropout.pdf">Depth Dropout: Efficient Training of Convolutional Neural Networks</a> </li><li>  <a href="https://arxiv.org/pdf/1603.08029v1.pdf">Resnet in Resnet: Generalizing Residual Architectures</a> (25 Mar 2016) </li><li>  <a href="https://arxiv.org/pdf/1608.02908v1.pdf">Residual Networks of Residual Networks: Multilevel Residual Networks</a> (9 Aug 2016) </li><li>  <a href="https://arxiv.org/pdf/1609.05672v2.pdf">Multi-Residual Networks</a> (Sep 24, 2016) </li><li>  <a href="https://arxiv.org/pdf/1610.02357v1.pdf">Deep Learning with Separable Convolutions</a> (7 Oct 2016) </li><li>  <a href="https://arxiv.org/pdf/1610.02915v1.pdf">Deep Pyramidal Residual Networks</a> (Oct 10, 2016) </li></ul><br><br>  If you read something that is not mentioned in this review, tell us in the comments.  To keep track of everything with this stream of new articles is simply not realistic.  While this post was moderated on a blog, I managed to add two more new publications to the list.  In one, Google talks about the new version of the insector - Xception, and in the second - the pyramidal ResNet. <br><br>  If you ask someone who is a little versed in neural networks, what types of networks are there, he will say ordinary (fully connected and convolutional) and recurrent.  It seems that a theoretical base will soon appear, within which all types of neural networks will belong to the same class.  There is a universal approximation theorem, which states that any (almost) function can be approximated by a neural network.  A similar theorem for RNN states that any dynamic system can be brought closer by a recurrent network.  And now it turns out that RNN can also be approached by a normal network (it remains to prove this, really).  In general, we live in an interesting time. <br><br><div class="spoiler">  <b class="spoiler_title">We will see.</b> <div class="spoiler_text"><img src="https://habrastorage.org/files/2fa/893/34c/2fa89334cca84acbb0d11c37973d28c6.jpg"><br></div></div></div><p>Source: <a href="https://habr.com/ru/post/311706/">https://habr.com/ru/post/311706/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../311696/index.html">How to disaccustom a puppy-developer to spoil in the code? (Trash story)</a></li>
<li><a href="../311698/index.html">Pico8 - nonexistent game console</a></li>
<li><a href="../311700/index.html">"In all projects, they did exactly what they thought was necessary": Luxoft is about Java and not only</a></li>
<li><a href="../311702/index.html">Can I go to set up the equipment in Spain?</a></li>
<li><a href="../311704/index.html">How to design a super fast site</a></li>
<li><a href="../311708/index.html">How DLCs can increase the number of players: an example of Street Fighter V</a></li>
<li><a href="../311714/index.html">Installing OTRS 5 on a server with Nginx</a></li>
<li><a href="../311716/index.html">RetroBase - an analogue of Retrofit for database queries</a></li>
<li><a href="../311718/index.html">How much does it cost to enter the top cash categories of the app store and is there any point?</a></li>
<li><a href="../311720/index.html">WordCamp Europe in Vienna and the WordPress Development Vector</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>