<!doctype html>
<html class="no-js" lang="en">

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-134931760-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-134931760-1');
  </script>

  <meta charset="utf-8">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <title>Create streaming processing pipeline. Part 1</title>
  <link rel="icon" type="image/x-icon" href="/favicon.ico" />
  <meta name="description" content="Hello. Friends, we are sharing with you the translation of an article prepared especially for the students of the course ‚ÄúData Engineer‚Äù . Go! 



 Ap...">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <link rel="stylesheet" href="../../css/main.css">

  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script>window.jQuery || document.write('<script src="../../js/vendors/jquery-3.3.1.min.js"><\/script>')</script>

  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <script>
       (adsbygoogle = window.adsbygoogle || []).push({
            google_ad_client: "ca-pub-6974184241884155",
            enable_page_level_ads: true
       });
  </script>
</head>

<body>
  <!--[if lte IE 9]>
    <p class="browserupgrade">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience and security.</p>
  <![endif]-->
  <header class="page-header js-page-header">
    <a class="page-header-logo-container" href="https://weekly-geekly.github.io/index.html"></a>
    <div class="page-header-text">Geekly Articles each Day</div>
  </header>
  <nav class="page-headings-container js-page-headings-container"></nav>
  <div class="tools-bar js-tools-bar">
    <!-- <a href="../../search.html" title="Search">üîé</a> -->
    <a class="js-list-of-headings-button" data-state="closed" href="#" title="Headings">üìú</a>
    <a class="js-go-to-top-button" href="#" title="Go to Top">‚¨ÜÔ∏è</a>
    <a class="js-go-to-bottom-button" href="#" title="Go to Bottom">‚¨áÔ∏è</a>
  </div>
  <a href="http://bit.ly/donateToWeeklyGeekly" class="donate-btn">DONATE</a>
  <section class="page js-page"><h1>Create streaming processing pipeline. Part 1</h1><div class="post__text post__text-html js-mediator-article">  Hello.  Friends, we are sharing with you the translation of an article prepared especially for the students of the course <a href="https://otus.pw/lVQx/">‚ÄúData Engineer‚Äù</a> .  Go! <br><br><img src="https://habrastorage.org/webt/fv/mo/wz/fvmowz46naiimojhlq03dhxm38w.png"><br><br><h2>  Apache Beam and DataFlow for real-time pipelines </h2><br>  Today's post is based on a task that I recently did at work.  I was really glad to embody it and describe the work done in the blogpost format, because it gave me the opportunity to work on data-engineering, as well as do something that would be very useful for my team.  Not so long ago, I discovered that our systems store a fairly large amount of user logs associated with one of our products for working with data.  It turned out that no one used this data, so I immediately became interested in what we could find out if we began to analyze it regularly.  However, there were several problems on the way.  The first problem was that the data was stored in many different text files that were not available for instant analysis.  The second problem was that they were stored in a closed system, so I could not use any of my favorite data analysis tools. <a name="habracut"></a>
      <br>
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
        <ins class="adsbygoogle"
          style="display:block; text-align:center;"
          data-ad-layout="in-article"
          data-ad-format="fluid"
          data-ad-client="ca-pub-6974184241884155"
          data-ad-slot="8945601208"></ins>
        <script>
          (adsbygoogle = window.adsbygoogle || []).push({});
        </script>
      <br>
      I had to decide how to make access easier for us and bring at least some value by integrating this data source into some of our user interaction solutions.  After some thought, I decided to design a pipeline to transfer this data to the cloud database so that the team and I could access it and start generating any conclusions.  After I finished Data Engineering in Coursera a while ago, I was eager to use some of the course tools in the project. <br><br>  Thus, placing data in a cloud database seemed like a sensible way to solve my first problem, but what could I do with problem number 2?  Fortunately, there was a way to transfer this data to an environment where I could access tools like Python and the Google Cloud Platform (GCP).  However, it was a long process, so I needed to do something that would allow me to continue developing while I was waiting for the data transfer to finish.  The solution I came to was creating fake data using the <b>Faker</b> library in Python.  I have never used this library before, but quickly realized how useful it is.  Using this approach allowed me to start writing code and testing the pipeline without actual data. <br><br>  Given the above, in this post I will describe how I built the above described pipeline using some of the technologies available in the GCP.  In particular, I will use <b>Apache Beam (version for Python), Dataflow, Pub / Sub and Big Query</b> to collect user logs, convert data and transfer them to a database for further analysis.  In my case, I only needed the Beam package functionality, since my data did not arrive in real time, so Pub / Sub was not required.  However, I will focus on the streaming version, as this is what you may encounter in practice. <br><br><h2>  Introduction to GCP and Apache Beam </h2><br>  The Google Cloud Platform provides a set of really useful tools for processing big data.  Here are some of the tools that I will use: <br><br><ul><li>  <a href="https://cloud.google.com/pubsub/">Pub / Sub</a> is a messaging service that uses the Publisher-Subscriber template (Publisher-Subscriber), which allows us to receive data in real time. </li><li>  <a href="https://cloud.google.com/dataflow/">DataFlow</a> is a service that simplifies the creation of data pipelines and automatically solves tasks such as scaling the infrastructure, which means that we can focus only on writing code for our pipeline. </li><li>  <a href="https://cloud.google.com/bigquery/">BigQuery</a> is a cloud-based data store.  If you are familiar with other databases in SQL, you will not have to deal with BigQuery for a long time. </li><li>  Finally, we will use Apache Beam, namely, focus on the Python version to create our pipeline.  This tool will allow us to create a streaming or batch processing pipeline that integrates with GCP.  It is especially useful for parallel processing and is suitable for extracting, transforming and loading (ETL) tasks, so if we need to move data from one place to another to perform transformations or calculations, the Beam is a good choice. </li></ul><br><br>  A large number of tools are available in the GCP, so it may be difficult to cover them all, including their purpose, but nevertheless, <a href="https://cloud.google.com/terms/services">here is</a> a summary for reference. <br><br><h2>  Visualization of our pipeline </h2><br>  Let's visualize the components of our conveyor in <i>Figure 1</i> .  At a high level, we want to collect user data in real time, process it and transfer it to BigQuery.  Logs are created when users interact with the product by sending requests to the server, which are then logged.  This data can be especially useful for understanding how users interact with our product and whether it works correctly.  In general, the pipeline will contain the following steps: <br><br><ol><li>  These logs of our users are published in the Pub / Sub-section. </li><li>  We connect to Pub / Sub and convert the data to the appropriate format using Python and Beam (steps 3 and 4 in Figure 1). </li><li>  After converting the data, the Beam will then connect to BigQuery and add it to our table (steps 4 and 5 in Figure 1). </li><li>  For the analysis, we can connect to BigQuery using various tools, such as Tableau and Python. </li></ol><br>  Beam makes this process very simple, regardless of whether we have a stream data source or a CSV file, and we want to perform batch processing.  Later you will see that the code contains only the minimal changes necessary to switch between them.  This is one of the benefits of using the Beam. <br><br><img src="https://habrastorage.org/webt/2s/nv/kc/2snvkcz6-jwybsxr2kyz3awipaw.png"><br>  <i>Figure 1: Main Data Pipeline</i> <br><br><h2>  Creating pseudo data with Faker </h2><br>  As I mentioned earlier, due to limited access to data, I decided to create pseudo data in the same format as the actual data.  It was a really useful exercise, since I could write code and test the pipeline while I was waiting for the data.  I suggest looking at the Faker <a href="https://faker.readthedocs.io/en/latest/index.html">documentation</a> if you want to know what else this library has to offer.  Our user data will generally be similar to the example below.  Based on this format, we can generate data line by line to simulate data in real time.  These logs give us information such as date, request type, server response, IP address, etc. <br><br> <code>192.52.197.161 - - [30/Apr/2019:21:11:42] "PUT /tag/category/tag HTTP/1.1" [401] 155 "https://harris-lopez.com/categories/about/" "Mozilla/5.0 (Macintosh; PPC Mac OS X 10_11_2) AppleWebKit/5312 (KHTML, like Gecko) Chrome/34.0.855.0 Safari/5312"</code> <br> <br>  Based on the line above, we want to create our <b>LINE</b> variable using 7 variables in curly brackets below.  We will also use them as variable names in our table schema a bit later. <br><br> <code>LINE = """\ <br> {remote_addr} - - [{time_local}] "{request_type} {request_path} HTTP/1.1" [{status}] {body_bytes_sent} "{http_referer}" "{http_user_agent}"\ <br> """</code> <br> <br>  If we were doing batch processing, the code would be very similar, although we would need to create a set of samples in a certain time range.  To use the faker, we simply create an object and call the methods we need.  In particular, Faker was useful for creating IP addresses, as well as websites.  I used the following methods: <br><br> <code>fake.ipv4() <br> fake.uri_path() <br> fake.uri() <br> fake.user_agent() <br></code> <br><br><pre> <code class="php hljs">from faker import Faker import time import random import os import numpy <span class="hljs-keyword"><span class="hljs-keyword">as</span></span> np from datetime import datetime, timedelta LINE = <span class="hljs-string"><span class="hljs-string">""</span></span><span class="hljs-string"><span class="hljs-string">"\ {remote_addr} - - [{time_local}] "</span></span>{request_type} {request_path} HTTP/<span class="hljs-number"><span class="hljs-number">1.1</span></span><span class="hljs-string"><span class="hljs-string">" [{status}] {body_bytes_sent} "</span></span>{http_referer}<span class="hljs-string"><span class="hljs-string">" "</span></span>{http_user_agent}<span class="hljs-string"><span class="hljs-string">"\ "</span></span><span class="hljs-string"><span class="hljs-string">""</span></span> def generate_log_line(): fake = Faker() now = datetime.now() remote_addr = fake.ipv4() time_local = now.strftime(<span class="hljs-string"><span class="hljs-string">'%d/%b/%Y:%H:%M:%S'</span></span>) request_type = random.choice([<span class="hljs-string"><span class="hljs-string">"GET"</span></span>, <span class="hljs-string"><span class="hljs-string">"POST"</span></span>, <span class="hljs-string"><span class="hljs-string">"PUT"</span></span>]) request_path = <span class="hljs-string"><span class="hljs-string">"/"</span></span> + fake.uri_path() status = np.random.choice([<span class="hljs-number"><span class="hljs-number">200</span></span>, <span class="hljs-number"><span class="hljs-number">401</span></span>, <span class="hljs-number"><span class="hljs-number">404</span></span>], p = [<span class="hljs-number"><span class="hljs-number">0.9</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>, <span class="hljs-number"><span class="hljs-number">0.05</span></span>]) body_bytes_sent = random.choice(range(<span class="hljs-number"><span class="hljs-number">5</span></span>, <span class="hljs-number"><span class="hljs-number">1000</span></span>, <span class="hljs-number"><span class="hljs-number">1</span></span>)) http_referer = fake.uri() http_user_agent = fake.user_agent() log_line = LINE.format( remote_addr=remote_addr, time_local=time_local, request_type=request_type, request_path=request_path, status=status, body_bytes_sent=body_bytes_sent, http_referer=http_referer, http_user_agent=http_user_agent ) <span class="hljs-keyword"><span class="hljs-keyword">return</span></span> log_line</code> </pre> <br><br>  The end of the first part. <br><br>  In the coming days we will share with you the continuation of the article, and now traditionally we are waiting for comments ;-). </div><p>Source: <a href="https://habr.com/ru/post/454186/">https://habr.com/ru/post/454186/</a></p>
<section class="more-articles-navigation-panel js-more-articles-navigation-panel">
<h4>More articles:</h4>
<nav class="list-of-articles-container js-list-of-articles-container"><ul class="list-of-pages js-list-of-pages">
<li><a href="../454178/index.html">An example of calculating the pension IT-person from Moscow</a></li>
<li><a href="../45418/index.html">Do you use code autogeneration based on UML diagrams?</a></li>
<li><a href="../454180/index.html">Schr√∂dinger Cloud Backup</a></li>
<li><a href="../454182/index.html">Full interview with the dean of the Python faculty at GeekBrains - how and why to learn the language to beginners</a></li>
<li><a href="../454184/index.html">KubeCon Europe 2019: How we first visited the main event on Kubernetes</a></li>
<li><a href="../454188/index.html">Alternative recruitment channels</a></li>
<li><a href="../45419/index.html">The .NET Framework will be available on Server Core R2</a></li>
<li><a href="../454190/index.html">What not to do if your phone was stolen</a></li>
<li><a href="../454194/index.html">Complicated Database</a></li>
<li><a href="../454196/index.html">3D printing of electronics on the example of a drone: wires and boards are no longer needed</a></li>
</ul></nav>
</section><br />
<a href="../../allArticles.html"><strong>All Articles</strong></a>
<script src="../../js/main.js"></script>

<!-- Yandex.Metrika counter -->
<script type="text/javascript" >
  (function (d, w, c) {
      (w[c] = w[c] || []).push(function() {
          try {
              w.yaCounter52496797 = new Ya.Metrika({
                  id:52496797,
                  clickmap:true,
                  trackLinks:true,
                  accurateTrackBounce:true,
                  webvisor:true
              });
          } catch(e) { }
      });

      var n = d.getElementsByTagName("script")[0],
          s = d.createElement("script"),
          f = function () { n.parentNode.insertBefore(s, n); };
      s.type = "text/javascript";
      s.async = true;
      s.src = "https://mc.yandex.ru/metrika/watch.js";

      if (w.opera == "[object Opera]") {
          d.addEventListener("DOMContentLoaded", f, false);
      } else { f(); }
  })(document, window, "yandex_metrika_callbacks");
</script>
<noscript><div><img src="https://mc.yandex.ru/watch/52496797" style="position:absolute; left:-9999px;" alt="" /></div></noscript>

<!-- Google Analytics -->
  <script>
    window.ga = function () { ga.q.push(arguments) }; ga.q = []; ga.l = +new Date;
    ga('create', 'UA-134931760-1', 'auto'); ga('send', 'pageview')
  </script>
  <script src="https://www.google-analytics.com/analytics.js" async defer></script>

</section>

  <footer class="page-footer">
    <div class="page-footer-legal-info-container page-footer-element">
      <p>
        Weekly-Geekly | <span class="page-footer-legal-info-year js-page-footer-legal-info-year">2019</span>
      </p>
    </div>
    <div class="page-footer-counters-container page-footer-element">
      <a class="page-footer-counter-clustrmap" href='#'  title='Visit tracker'><img src='https://clustrmaps.com/map_v2.png?cl=698e5a&w=271&t=t&d=6iCFw7uJz0zcOaoxz5k5PcLCJUzv2WG8G5V8M3U6Rc4&co=3a3a3a&ct=ffffff'/></a>
    </div>
  </footer>
</body>

</html>